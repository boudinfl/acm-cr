@inproceedings{10.1145/3351095.3372833,
author = {Wieringa, Maranke},
title = {What to Account for When Accounting for Algorithms: A Systematic Literature Review on Algorithmic Accountability},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372833},
doi = {10.1145/3351095.3372833},
abstract = {As research on algorithms and their impact proliferates, so do calls for scrutiny/accountability of algorithms. A systematic review of the work that has been done in the field of 'algorithmic accountability' has so far been lacking. This contribution puts forth such a systematic review, following the PRISMA statement. 242 English articles from the period 2008 up to and including 2018 were collected and extracted from Web of Science and SCOPUS, using a recursive query design coupled with computational methods. The 242 articles were prioritized and ordered using affinity mapping, resulting in 93 'core articles' which are presented in this contribution. The recursive search strategy made it possible to look beyond the term 'algorithmic accountability'. That is, the query also included terms closely connected to the theme (e.g. ethics and AI, regulation of algorithms). This approach allows for a perspective not just from critical algorithm studies, but an interdisciplinary overview drawing on material from data studies to law, and from computer science to governance studies. To structure the material, Bovens's widely accepted definition of accountability serves as a focal point. The material is analyzed on the five points Bovens identified as integral to accountability: its arguments on (1) the actor, (2) the forum, (3) the relationship between the two, (3) the content and criteria of the account, and finally (5) the consequences which may result from the account. The review makes three contributions. First, an integration of accountability theory in the algorithmic accountability discussion. Second, a cross-sectoral overview of the that same discussion viewed in light of accountability theory which pays extra attention to accountability risks in algorithmic systems. Lastly, it provides a definition of algorithmic accountability based on accountability theory and algorithmic accountability literature.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {1–18},
numpages = {18},
keywords = {algorithmic accountability, accountability theory, data-driven governance, algorithmic systems},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372840,
author = {Green, Ben and Viljoen, Salom\'{e}},
title = {Algorithmic Realism: Expanding the Boundaries of Algorithmic Thought},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372840},
doi = {10.1145/3351095.3372840},
abstract = {Although computer scientists are eager to help address social problems, the field faces a growing awareness that many well-intentioned applications of algorithms in social contexts have led to significant harm. We argue that addressing this gap between the field's desire to do good and the harmful impacts of many of its interventions requires looking to the epistemic and methodological underpinnings of algorithms. We diagnose the dominant mode of algorithmic reasoning as "algorithmic formalism" and describe how formalist orientations lead to harmful algorithmic interventions. Addressing these harms requires pursuing a new mode of algorithmic thinking that is attentive to the internal limits of algorithms and to the social concerns that fall beyond the bounds of algorithmic formalism. To understand what a methodological evolution beyond formalism looks like and what it may achieve, we turn to the twentieth century evolution in American legal thought from legal formalism to legal realism. Drawing on the lessons of legal realism, we propose a new mode of algorithmic thinking---"algorithmic realism"---that provides tools for computer scientists to account for the realities of social life and of algorithmic impacts. These realist approaches, although not foolproof, will better equip computer scientists to reduce algorithmic harms and to reason well about doing good.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {19–31},
numpages = {13},
keywords = {critical algorithm studies, STS, law, epistemology, algorithms},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3373153,
author = {Kang, Sunny Seon},
title = {Algorithmic Accountability in Public Administration: The GDPR Paradox},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373153},
doi = {10.1145/3351095.3373153},
abstract = {The EU General Data Protection Regulation ("GDPR") is often represented as a larger than life behemoth that will fundamentally transform the world of big data. Abstracted from its constituent parts of corresponding rights, responsibilities, and exemptions, the operative scope of the GDPR can be unduly aggrandized, when in reality, it caters to the specific policy objectives of legislators and institutional stakeholders.With much uncertainty ahead on the precise implementation of the GDPR, academic and policy discussions are debating the adequacy of protections for automated decision-making in GDPR Articles 13 (right to be informed of automated treatment), 15 (right of access by the data subject), and 22 (safeguards to profiling). Unfortunately, the literature to date disproportionately focuses on the impact of AI in the private sector, and deflects any extensive review of automated enforcement tools in public administration.Even though the GDPR enacts significant safeguards against automated decisions, it does so with deliberate design: to balance the interests of data protection with the growing demand for algorithms in the administrative state. In order to facilitate inter-agency data flows and sensitive data processing that fuel the predictive power of algorithmic enforcement tools, the GDPR decisively surrenders to the procedural autonomy of Member States to authorize these practices. Yet, due to a dearth of research on the GDPR's stance on government deployed algorithms, it is not widely known that public authorities can benefit from broadly worded exemptions to restrictions on automated decision-making, and even circumvent remedies for data subjects through national legislation.The potential for public authorities to invoke derogations from the GDPR must be contained by the fundamental guarantees of due process, judicial review, and equal treatment. This paper examines the interplay of these principles within the prospect of algorithmic decision-making by public authorities.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {32},
numpages = {1},
keywords = {predictive enforcement tools, general data protection regulation (GDPR), algorithmic accountability, due process, automated decisions},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372873,
author = {Raji, Inioluwa Deborah and Smart, Andrew and White, Rebecca N. and Mitchell, Margaret and Gebru, Timnit and Hutchinson, Ben and Smith-Loud, Jamila and Theron, Daniel and Barnes, Parker},
title = {Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372873},
doi = {10.1145/3351095.3372873},
abstract = {Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source.In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {33–44},
numpages = {12},
keywords = {algorithmic audits, machine learning, accountability, responsible innovation},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372874,
author = {Katell, Michael and Young, Meg and Dailey, Dharma and Herman, Bernease and Guetler, Vivian and Tam, Aaron and Bintz, Corinne and Raz, Daniella and Krafft, P. M.},
title = {Toward Situated Interventions for Algorithmic Equity: Lessons from the Field},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372874},
doi = {10.1145/3351095.3372874},
abstract = {Research to date aimed at the fairness, accountability, and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes. Researchers have given less attention to methods that account for the social and political contexts of specific, situated technical systems at their points of use. Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems. In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit. The main insights we gleaned from our experiences were: (i) many meaningful interventions toward equitable algorithmic systems are non-technical; (ii) community organizations derive the most value from localized materials as opposed to what is "scalable" beyond a particular policy context; (iii) framing harms around algorithmic bias suggests that more accurate data is the solution, at the risk of missing deeper questions about whether some technologies should be used at all. More broadly, we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {45–55},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372870,
author = {Sokol, Kacper and Flach, Peter},
title = {Explainability Fact Sheets: A Framework for Systematic Assessment of Explainable Approaches},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372870},
doi = {10.1145/3351095.3372870},
abstract = {Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {56–67},
numpages = {12},
keywords = {transparency, AI, taxonomy, explainability, work sheet, interpretability, desiderata, fact sheet, ML},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372875,
author = {Kaminski, Margot E. and Malgieri, Gianclaudio},
title = {Multi-Layered Explanations from Algorithmic Impact Assessments in the GDPR},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372875},
doi = {10.1145/3351095.3372875},
abstract = {Impact assessments have received particular attention on both sides of the Atlantic as a tool for implementing algorithmic accountability. The aim of this paper is to address how Data Protection Impact Assessments (DPIAs) (Art. 35) in the European Union (EU)'s General Data Protection Regulation (GDPR) link the GDPR's two approaches to algorithmic accountability---individual rights and systemic governance--- and potentially lead to more accountable and explainable algorithms. We argue that algorithmic explanation should not be understood as a static statement, but as a circular and multi-layered transparency process based on several layers (general information about an algorithm, group-based explanations, and legal justification of individual decisions taken). We argue that the impact assessment process plays a crucial role in connecting internal company heuristics and risk mitigation to outward-facing rights, and in forming the substance of several kinds of explanations.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {68–79},
numpages = {12},
keywords = {general data protection regulation, law, impact assessments},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372830,
author = {Barocas, Solon and Selbst, Andrew D. and Raghavan, Manish},
title = {The Hidden Assumptions behind Counterfactual Explanations and Principal Reasons},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372830},
doi = {10.1145/3351095.3372830},
abstract = {Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established "principal reason" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant---and withholding others.These "feature-highlighting explanations" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear.In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes.We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden.While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world---and the subjective choices necessary to compensate for this---must be understood before these techniques can be usefully implemented.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {80–89},
numpages = {10},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372824,
author = {Lucic, Ana and Haned, Hinda and de Rijke, Maarten},
title = {Why Does My Model Fail? Contrastive Local Explanations for Retail Forecasting},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372824},
doi = {10.1145/3351095.3372824},
abstract = {In various business settings, there is an interest in using more complex machine learning techniques for sales forecasting. It is difficult to convince analysts, along with their superiors, to adopt these techniques since the models are considered to be "black boxes," even if they perform better than current models in use. We examine the impact of contrastive explanations about large errors on users' attitudes towards a "black-box" model. We propose an algorithm, Monte Carlo Bounds for Reasonable Predictions. Given a large error, MC-BRP determines (1) feature values that would result in a reasonable prediction, and (2) general trends between each feature and the target, both based on Monte Carlo simulations. We evaluate on a real dataset with real users by conducting a user study with 75 participants to determine if explanations generated by MC-BRP help users understand why a prediction results in a large error, and if this promotes trust in an automatically-learned model. Our study shows that users are able to answer objective questions about the model's predictions with overall 81.1% accuracy when provided with these contrastive explanations. We show that users who saw MC-BRP explanations understand why the model makes large errors in predictions significantly more than users in the control group. We also conduct an in-depth analysis of the difference in attitudes between Practitioners and Researchers, and confirm that our results hold when conditioning on the users' background.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {90–98},
numpages = {9},
keywords = {explainability, erroneous predictions, interpretability},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372827,
author = {Sendak, Mark and Elish, Madeleine Clare and Gao, Michael and Futoma, Joseph and Ratliff, William and Nichols, Marshall and Bedoya, Armando and Balu, Suresh and O'Brien, Cara},
title = {"The Human Body is a Black Box": Supporting Clinical Decision-Making with Deep Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372827},
doi = {10.1145/3351095.3372827},
abstract = {Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {99–109},
numpages = {11},
keywords = {interpretability, deep learning, medicine, expertise, trust},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3373154,
author = {Kallus, Nathan and Mao, Xiaojie and Zhou, Angela},
title = {Assessing Algorithmic Fairness with Unobserved Protected Class Using Data Combination},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373154},
doi = {10.1145/3351095.3373154},
abstract = {The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts, or performance disparities in general, in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, that includes protected class labels but not decisions or outcomes. We show that a variety of common disparity measures are generally unidentifiable aside for some unrealistic cases, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the sharpest-possible partial identification set of disparities either under no assumptions or when we incorporate mild smoothness constraints. We further provide optimization-based algorithms for computing and visualizing these sets of simultaneously achievable pairwise disparties for assessing disparities that arise between multiple groups, which enables reliable and robust assessments - an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {110},
numpages = {1},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372845,
author = {Black, Emily and Yeom, Samuel and Fredrikson, Matt},
title = {FlipTest: Fairness Testing via Optimal Transport},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372845},
doi = {10.1145/3351095.3372845},
abstract = {We present FlipTest, a black-box technique for uncovering discrimination in classifiers. FlipTest is motivated by the intuitive question: had an individual been of a different protected status, would the model have treated them differently? Rather than relying on causal information to answer this question, FlipTest leverages optimal transport to match individuals in different protected groups, creating similar pairs of in-distribution samples. We show how to use these instances to detect discrimination by constructing a flipset: the set of individuals whose classifier output changes post-translation, which corresponds to the set of people who may be harmed because of their group membership. To shed light on why the model treats a given subgroup differently, FlipTest produces a transparency report: a ranking of features that are most associated with the model's behavior on the flipset. Evaluating the approach on three case studies, we show that this provides a computationally inexpensive way to identify subgroups that may be harmed by model discrimination, including in cases where the model satisfies group fairness criteria.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {111–121},
numpages = {11},
keywords = {fairness, machine learning, disparate impact, optimal transport},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372867,
author = {Marcinkowski, Frank and Kieslich, Kimon and Starke, Christopher and L\"{u}nich, Marco},
title = {Implications of AI (Un-)Fairness in Higher Education Admissions: The Effects of Perceived AI (Un-)Fairness on Exit, Voice and Organizational Reputation},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372867},
doi = {10.1145/3351095.3372867},
abstract = {Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision-makers but, rather, reproduce biases existing within the respective training data. This study adopted a different approach by focusing on individual perceptions of fairness. Specifically, we looked at two different dimensions of perceived fairness: (i) procedural fairness and (ii) distributive fairness. Using cross-sectional survey data (n = 304) from a large German university, we tested whether students' assessments of fairness differ with respect to algorithmic vs. human decision-making (HDM) within the higher education context. Furthermore, we investigated whether fairness perceptions have subsequent effects on three different outcome variables, which are hugely important for universities: (1) exit, (2) voice, and (3) organizational reputation. The results of our survey suggest that participants evaluated ADM higher than HDM in terms of both procedural and distributive fairness. Concerning the subsequent effects of fairness perceptions, we find that (1) distributive fairness as well as procedural fairness perceptions have a negative impact on the intention to protest against an ADM system, whereas (2) only procedural fairness perceptions negatively affect the likelihood of exiting. Finally, (3) distributive fairness, but not procedural fairness perceptions have a positive effect on organizational reputation. For universities aiming to implement ADM systems, it is crucial, therefore, to take possible fairness issues and their further implications into account.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {122–130},
numpages = {9},
keywords = {procedural fairness, distributive fairness, algorithmic decision making, exit, artificial intelligence, reputation, higher education systems, voice},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372879,
author = {Ribeiro, Manoel Horta and Ottoni, Raphael and West, Robert and Almeida, Virg\'{\i}lio A. F. and Meira, Wagner},
title = {Auditing Radicalization Pathways on YouTube},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372879},
doi = {10.1145/3351095.3372879},
abstract = {Non-profits, as well as the media, have hypothesized the existence of a radicalization pipeline on YouTube, claiming that users systematically progress towards more extreme content on the platform. Yet, there is to date no substantial quantitative evidence of this alleged pipeline. To close this gap, we conduct a large-scale audit of user radicalization on YouTube. We analyze 330,925 videos posted on 349 channels, which we broadly classified into four types: Media, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right. According to the aforementioned radicalization hypothesis, channels in the I.D.W. and the Alt-lite serve as gateways to fringe far-right ideology, here represented by Alt-right channels. Processing 72M+ comments, we show that the three channel types indeed increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube's recommendation algorithm, looking at more than 2M video and channel recommendations between May/July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels, while Alt-right videos are reachable only through channel recommendations. Overall, we paint a comprehensive picture of user radicalization on YouTube.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {131–141},
numpages = {11},
keywords = {algorithmic auditing, hate speech, extremism, radicalization},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372863,
author = {Rodolfa, Kit T. and Salomon, Erika and Haynes, Lauren and Mendieta, Iv\'{a}n Higuera and Larson, Jamie and Ghani, Rayid},
title = {Case Study: Predictive Fairness to Reduce Misdemeanor Recidivism through Social Service Interventions},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372863},
doi = {10.1145/3351095.3372863},
abstract = {The criminal justice system is currently ill-equipped to improve outcomes of individuals who cycle in and out of the system with a series of misdemeanor offenses. Often due to constraints of caseload and poor record linkage, prior interactions with an individual may not be considered when an individual comes back into the system, let alone in a proactive manner through the application of diversion programs. The Los Angeles City Attorney's Office recently created a new Recidivism Reduction and Drug Diversion unit (R2D2) tasked with reducing recidivism in this population. Here we describe a collaboration with this new unit as a case study for the incorporation of predictive equity into machine learning based decision making in a resource-constrained setting. The program seeks to improve outcomes by developing individually-tailored social service interventions (i.e., diversions, conditional plea agreements, stayed sentencing, or other favorable case disposition based on appropriate social service linkage rather than traditional sentencing methods) for individuals likely to experience subsequent interactions with the criminal justice system, a time and resource-intensive undertaking that necessitates an ability to focus resources on individuals most likely to be involved in a future case. Seeking to achieve both efficiency (through predictive accuracy) and equity (improving outcomes in traditionally under-served communities and working to mitigate existing disparities in criminal justice outcomes), we discuss the equity outcomes we seek to achieve, describe the corresponding choice of a metric for measuring predictive fairness in this context, and explore a set of options for balancing equity and efficiency when building and selecting machine learning models in an operational public policy setting.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {142–153},
numpages = {12},
keywords = {racial bias, criminal justice, machine learning disparities, algorithmic fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372868,
author = {Malgieri, Gianclaudio},
title = {The Concept of Fairness in the GDPR: A Linguistic and Contextual Interpretation},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372868},
doi = {10.1145/3351095.3372868},
abstract = {There is a growing attention on the notion of fairness in the GDPR in the European legal literature. However, the principle of fairness in the Data Protection framework is still ambiguous and uncertain, as computer science literature and interpretative guidelines reveal.This paper looks for a better understanding of the concept of fairness in the data protection field through two parallel methodological tools: linguistic comparison and contextual interpretation.In terms of linguistic comparison, the paper analyses all translations of the world "fair" in the GDPR in the EU official languages, as the CJEU suggests in CILFIT Case for the interpretation of the EU law.The analysis takes into account also the translation of the notion of fairness in other contiguous fields (e.g. at Article 8 of the EU Charter of fundamental rights or in the Consumer field, e.g. Unfair terms directive or Unfair commercial practice directive).In general, the notion of fairness is translated with several different nuances (in accordance or in discordance with the previous Data protection Directive and with Article 8 of the Charter)In some versions different words are used interchangeably (it is the case of French, Spanish and Portuguese texts), in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR (it is the case of German and Greek version).The analysis reveals three mean semantic notions: correctness (Italian, Swedish, Romanian), loyalty (French, Spanish, Portuguese and the German version of "Treu und Glaube") and equitability (French, Spanish and Portuguese).Interestingly, these three notions have common roots in the Western legal history: the Roman law notion of "bona fide".Taking into account both the value of "bona fide" in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR, the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects.The approach of fairness is effect-based: what is relevant is not the formal respect of procedures (in terms of transparency, lawfulness or accountability), but the substantial mitigation of unfair imbalances that create situations of "vulnerability". Building on these reflections, the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability, within and beyond the GDPR.In sum, the article suggests that the best interpretation of the fairness principles in the GDPR (taking into account both the notion of procedural fairness and of fair balancing) is the mitigation of data subjects' vulnerabilities through specific safeguards and measures.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {154–166},
numpages = {13},
keywords = {data protection, GDPR, linguistic comparison, fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372859,
author = {Barabas, Chelsea and Doyle, Colin and Rubinovitz, JB and Dinakar, Karthik},
title = {Studying up: Reorienting the Study of Algorithmic Fairness around Issues of Power},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372859},
doi = {10.1145/3351095.3372859},
abstract = {Research within the social sciences and humanities has long characterized the work of data science as a sociotechnical process, comprised of a set of logics and techniques that are inseparable from specific social norms, expectations and contexts of development and use. Yet all too often the assumptions and premises underlying data analysis remain unexamined, even in contemporary debates about the fairness of algorithmic systems. This blindspot exists in part because the methodological toolkit used to evaluate the fairness of algorithmic systems remains limited to a narrow set of computational and legal modes of analysis. In this paper, we expand on Elish and Boyd's [17] call for data scientists to develop more robust frameworks for understanding their work as situated practice by examining a specific methodological debate within the field of anthropology, frequently referred to as the practice of "studying up". We reflect on the contributions that the call to "study up" has made in the field of anthropology before making the case that the field of algorithmic fairness would similarly benefit from a reorientation "upward". A case study from our own work illustrates what it looks like to reorient one's research questions "up" in a high-profile debate regarding the fairness of an algorithmic system - namely, pretrial risk assessment in American criminal law. We discuss the limitations of contemporary fairness discourse with regard to pretrial risk assessment before highlighting the insights gained when we reframe our research questions to focus on those who inhabit positions of power and authority within the U.S. court system. Finally, we reflect on the challenges we have encountered in implementing data science projects that "study up". In the process, we surface new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {167–176},
numpages = {10},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372853,
author = {Kulynych, Bogdan and Overdorf, Rebekah and Troncoso, Carmela and G\"{u}rses, Seda},
title = {POTs: Protective Optimization Technologies},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372853},
doi = {10.1145/3351095.3372853},
abstract = {Algorithmic fairness aims to address the economic, moral, social, and political impact that digital systems have on populations through solutions that can be applied by service providers. Fairness frameworks do so, in part, by mapping these problems to a narrow definition and assuming the service providers can be trusted to deploy countermeasures. Not surprisingly, these decisions limit fairness frameworks' ability to capture a variety of harms caused by systems.We characterize fairness limitations using concepts from requirements engineering and from social sciences. We show that the focus on algorithms' inputs and outputs misses harms that arise from systems interacting with the world; that the focus on bias and discrimination omits broader harms on populations and their environments; and that relying on service providers excludes scenarios where they are not cooperative or intentionally adversarial.We propose Protective Optimization Technologies (POTs). POTs, provide means for affected parties to address the negative impacts of systems in the environment, expanding avenues for political contestation. POTs intervene from outside the system, do not require service providers to cooperate, and can serve to correct, shift, or expose harms that systems impose on populations and their environments. We illustrate the potential and limitations of POTs in two case studies: countering road congestion caused by traffic beating applications, and recalibrating credit scoring for loan applicants.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {177–188},
numpages = {12},
keywords = {fairness and accountability, protective optimization technologies},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372872,
author = {Pujol, David and McKenna, Ryan and Kuppam, Satya and Hay, Michael and Machanavajjhala, Ashwin and Miklau, Gerome},
title = {Fair Decision Making Using Privacy-Protected Data},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372872},
doi = {10.1145/3351095.3372872},
abstract = {Data collected about individuals is regularly used to make decisions that impact those same individuals. We consider settings where sensitive personal data is used to decide who will receive resources or benefits. While it is well known that there is a trade-off between protecting privacy and the accuracy of decisions, we initiate a first-of-its-kind study into the impact of formally private mechanisms (based on differential privacy) on fair and equitable decision-making. We empirically investigate novel tradeoffs on two real-world decisions made using U.S. Census data (allocation of federal funds and assignment of voting rights benefits) as well as a classic apportionment problem.Our results show that if decisions are made using an ∈-differentially private version of the data, under strict privacy constraints (smaller ∈), the noise added to achieve privacy may disproportionately impact some groups over others. We propose novel measures of fairness in the context of randomized differentially private algorithms and identify a range of causes of outcome disparities. We also explore improved algorithms to remedy the unfairness observed.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {189–199},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372839,
author = {Slack, Dylan and Friedler, Sorelle A. and Givental, Emile},
title = {Fairness Warnings and Fair-MAML: Learning Fairly with Minimal Data},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372839},
doi = {10.1145/3351095.3372839},
abstract = {Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {200–209},
numpages = {10},
keywords = {covariate shift, meta-learning, fairness, machine learning},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372860,
author = {Bietti, Elettra},
title = {From Ethics Washing to Ethics Bashing: A View on Tech Ethics from within Moral Philosophy},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372860},
doi = {10.1145/3351095.3372860},
abstract = {The word 'ethics' is under siege in technology policy circles. Weaponized in support of deregulation, self-regulation or handsoff governance, "ethics" is increasingly identified with technology companies' self-regulatory efforts and with shallow appearances of ethical behavior. So-called "ethics washing" by tech companies is on the rise, prompting criticism and scrutiny from scholars and the tech community at large. In parallel to the growth of ethics washing, its condemnation has led to a tendency to engage in "ethics bashing." This consists in the trivialization of ethics and moral philosophy now understood as discrete tools or pre-formed social structures such as ethics boards, self-governance schemes or stakeholder groups.The misunderstandings underlying ethics bashing are at least threefold: (a) philosophy and "ethics" are seen as a communications strategy and as a form of instrumentalized cover-up or fa\c{c}ade for unethical behavior, (b) philosophy is understood in opposition and as alternative to political representation and social organizing and (c) the role and importance of moral philosophy is downplayed and portrayed as mere "ivory tower" intellectualization of complex problems that need to be dealt with in practice.This paper argues that the rhetoric of ethics and morality should not be reductively instrumentalized, either by the industry in the form of "ethics washing," or by scholars and policy-makers in the form of "ethics bashing." Grappling with the role of philosophy and ethics requires moving beyond both tendencies and seeing ethics as a mode of inquiry that facilitates the evaluation of competing tech policy strategies. In other words, we must resist narrow reductivism of moral philosophy as instrumentalized performance and renew our faith in its intrinsic moral value as a mode of knowledgeseeking and inquiry. Far from mandating a self-regulatory scheme or a given governance structure, moral philosophy in fact facilitates the questioning and reconsideration of any given practice, situating it within a complex web of legal, political and economic institutions. Moral philosophy indeed can shed new light on human practices by adding needed perspective, explaining the relationship between technology and other worthy goals, situating technology within the human, the social, the political. It has become urgent to start considering technology ethics also from within and not only from outside of ethics.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {210–219},
numpages = {10},
keywords = {moral philosophy, AI, regulation, technology ethics, self-regulation, ethics, technology law},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3373152,
author = {Terzis, Petros},
title = {Onward for the Freedom of Others: Marching beyond the AI Ethics},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373152},
doi = {10.1145/3351095.3373152},
abstract = {The debate on the ethics of Artificial Intelligence brought together different stakeholders including but not limited to academics, policymakers, CEOs, activists, workers' representatives, lobbyists, journalists, and 'moral machines'. Prominent political institutions crafted principles for the 'ethical being' of the AI companies while tech giants were documenting ethics in a series of self-written guidelines. In parallel, a large community started to flourish, focusing on how to technically embed ethical parameters into algorithmic systems. Founded upon the philosophical work of Simone de Beauvoir and Jean-Paul Sartre, this paper explores the philosophical antinomies of the 'AI Ethics' debate as well as the conceptual disorientation of the 'fairness discussion'. By bringing the philosophy of existentialism to the dialogue, this paper attempts to challenge the dialectical monopoly of utilitarianism and sheds fresh light on the -already glaring- AI arena. Why is 'the AI Ethics guidelines' a futile battle doomed to dangerous abstraction? How this battle can harm our sense of collective freedom? Which is the uncomfortable reality that remains obscured by the smoke-gas of the 'AI Ethics' discussion? And eventually, what's the alternative? There seems to be a different pathway for discussing and implementing ethics; A pathway that sets the freedom of others at the epicenter of the battle for a sustainable and open to all future.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {220–229},
numpages = {10},
keywords = {existentialism, philosophy, artificial intelligence, algorithms, ethics},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372844,
author = {Washington, Anne L. and Kuo, Rachel},
title = {Whose Side Are Ethics Codes on? Power, Responsibility and the Social Good},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372844},
doi = {10.1145/3351095.3372844},
abstract = {The moral authority of ethics codes stems from an assumption that they serve a unified society, yet this ignores the political aspects of any shared resource. The sociologist Howard S. Becker challenged researchers to clarify their power and responsibility in the classic essay: Whose Side Are We On. Building on Becker's hierarchy of credibility, we report on a critical discourse analysis of data ethics codes and emerging conceptualizations of beneficence, or the "social good", of data technology. The analysis revealed that ethics codes from corporations and professional associations conflated consumers with society and were largely silent on agency. Interviews with community organizers about social change in the digital era supplement the analysis, surfacing the limits of technical solutions to concerns of marginalized communities. Given evidence that highlights the gulf between the documents and lived experiences, we argue that ethics codes that elevate consumers may simultaneously subordinate the needs of vulnerable populations. Understanding contested digital resources is central to the emerging field of public interest technology. We introduce the concept of digital differential vulnerability to explain disproportionate exposures to harm within data technology and suggest recommendations for future ethics codes..},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {230–240},
numpages = {11},
keywords = {ethics codes, social movements, digital differential vulnerability, digital vulnerability, data science, public interest technology},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375784,
author = {Noriega-Campero, Alejandro and Garcia-Bulle, Bernardo and Cantu, Luis Fernando and Bakker, Michiel A. and Tejerina, Luis and Pentland, Alex},
title = {Algorithmic Targeting of Social Policies: Fairness, Accuracy, and Distributed Governance},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375784},
doi = {10.1145/3351095.3375784},
abstract = {Targeted social policies are the main strategy for poverty alleviation across the developing world. These include targeted cash transfers (CTs), as well as targeted subsidies in health, education, housing, energy, childcare, and others. Due to the scale, diversity, and widespread relevance of targeted social policies like CTs, the algorithmic rules that decide who is eligible to benefit from them---and who is not---are among the most important algorithms operating in the world today. Here we report on a year-long engagement towards improving social targeting systems in a couple of developing countries. We demonstrate that a shift towards the use of AI methods in poverty-based targeting can substantially increase accuracy, extending the coverage of the poor by nearly a million people in two countries, without increasing expenditure. However, we also show that, absent explicit parity constraints, both status quo and AI-based systems induce disparities across population subgroups. Moreover, based on qualitative interviews with local social institutions, we find a lack of consensus on normative standards for prioritization and fairness criteria. Hence, we close by proposing a decision-support platform for distributed governance, which enables a diversity of institutions to customize the use of AI-based insights into their targeting decisions.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {241–251},
numpages = {11},
keywords = {AI for social good, proxy means tests, algorithmic fairness, targeted social programs, cash transfers},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372871,
author = {Abebe, Rediet and Barocas, Solon and Kleinberg, Jon and Levy, Karen and Raghavan, Manish and Robinson, David G.},
title = {Roles for Computing in Social Change},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372871},
doi = {10.1145/3351095.3372871},
abstract = {A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems --- roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined --- changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {252–260},
numpages = {9},
keywords = {inequality, social change, societal implications of AI, discrimination},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372856,
author = {Wagner, Ben and Rozgonyi, Krisztina and Sekwenz, Marie-Therese and Cobbe, Jennifer and Singh, Jatinder},
title = {Regulating Transparency? Facebook, Twitter and the German Network Enforcement Act},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372856},
doi = {10.1145/3351095.3372856},
abstract = {Regulatory regimes designed to ensure transparency often struggle to ensure that transparency is meaningful in practice. This challenge is particularly great when coupled with the widespread usage of dark patterns --- design techniques used to manipulate users. The following article analyses the implementation of the transparency provisions of the German Network Enforcement Act (NetzDG) by Facebook and Twitter, as well as the consequences of these implementations for the effective regulation of online platforms. This question of effective regulation is particularly salient, due to an enforcement action in 2019 by Germany's Federal Office of Justice (BfJ) against Facebook for what the BfJ claim were insufficient compliance with transparency requirements, under NetzDG.This article provides an overview of the transparency requirements of NetzDG and contrasts these with the transparency requirements of other relevant regulations. It will then discuss how transparency concerns not only providing data, but also how the visibility of the data that is made transparent is managed, by deciding how the data is provided and is framed. We will then provide an empirical analysis of the design choices made by Facebook and Twitter, to assess the ways in which their implementations differ. The consequences of these two divergent implementations on interface design and user behaviour are then discussed, through a comparison of the transparency reports and reporting mechanisms used by Facebook and Twitter. As a next step, we will discuss the BfJ's consideration of the design of Facebook's content reporting mechanisms, and what this reveals about their respective interpretations of NetzDG's scope. Finally, in recognising that this situation is one in which a regulator is considering design as part of their action - we develop a wider argument on the potential for regulatory enforcement around dark patterns, and design practices more generally, for which this case is an early, indicative example.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {261–271},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372834,
author = {Toreini, Ehsan and Aitken, Mhairi and Coopamootoo, Kovila and Elliott, Karen and Zelaya, Carlos Gonzalez and van Moorsel, Aad},
title = {The Relationship between Trust in AI and Trustworthy Machine Learning Technologies},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372834},
doi = {10.1145/3351095.3372834},
abstract = {To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {272–283},
numpages = {12},
keywords = {machine learning, artificial intelligence, trustworthiness, trust},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372876,
author = {Venkatasubramanian, Suresh and Alfano, Mark},
title = {The Philosophical Basis of Algorithmic Recourse},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372876},
doi = {10.1145/3351095.3372876},
abstract = {Philosophers have established that certain ethically important values are modally robust in the sense that they systematically deliver correlative benefits across a range of counterfactual scenarios. In this paper, we contend that recourse - the systematic process of reversing unfavorable decisions by algorithms and bureaucracies across a range of counterfactual scenarios - is such a modally robust good. In particular, we argue that two essential components of a good life - temporally extended agency and trust - are underwritten by recourse.We critique existing approaches to the conceptualization, operationalization and implementation of recourse. Based on these criticisms, we suggest a revised approach to recourse and give examples of how it might be implemented - especially for those who are least well off1.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {284–293},
numpages = {10},
keywords = {recourse, robust goods, algorithmic decision making, precarity},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3373157,
author = {Dotan, Ravit and Milli, Smitha},
title = {Value-Laden Disciplinary Shifts in Machine Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373157},
doi = {10.1145/3351095.3373157},
abstract = {As machine learning models are increasingly used for high-stakes decision making, scholars have sought to intervene to ensure that such models do not encode undesirable social and political values. However, little attention thus far has been given to how values influence the machine learning discipline as a whole. How do values influence what the discipline focuses on and the way it develops? If undesirable values are at play at the level of the discipline, then intervening on particular models will not suffice to address the problem. Instead, interventions at the disciplinary-level are required.This paper analyzes the discipline of machine learning through the lens of philosophy of science. We develop a conceptual framework to evaluate the process through which types of machine learning models (e.g. neural networks, support vector machines, graphical models) become predominant. The rise and fall of model-types is often framed as objective progress. However, such disciplinary shifts are more nuanced. First, we argue that the rise of a model-type is self-reinforcing-it influences the way model-types are evaluated. For example, the rise of deep learning was entangled with a greater focus on evaluations in compute-rich and data-rich environments. Second, the way model-types are evaluated encodes loaded social and political values. For example, a greater focus on evaluations in compute-rich and data-rich environments encodes values about centralization of power, privacy, and environmental concerns.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {294},
numpages = {1},
keywords = {deep learning, machine learning, philosophy of science, values in science},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372852,
author = {Zhang, Yunfeng and Liao, Q. Vera and Bellamy, Rachel K. E.},
title = {Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372852},
doi = {10.1145/3351095.3372852},
abstract = {Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {295–305},
numpages = {11},
keywords = {decision support, confidence, trust, explainable AI},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372829,
author = {Jo, Eun Seo and Gebru, Timnit},
title = {Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372829},
doi = {10.1145/3351095.3372829},
abstract = {A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics &amp; privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {306–316},
numpages = {11},
keywords = {sociocultural data, data collection, ML fairness, datasets, machine learning, archives},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372865,
author = {Marda, Vidushi and Narayan, Shivangi},
title = {Data in New Delhi's Predictive Policing System},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372865},
doi = {10.1145/3351095.3372865},
abstract = {In 2015, Delhi Police announced plans for predictive policing. The Crime Mapping, Analytics and Predictive System (CMAPS) would be implemented in India's capital, for live spatial hotspot mapping of crime, criminal behavior patterns and suspect analysis. Four years later, there is little known about the effect of CMAPS due to the lack of public accountability mechanisms and large exceptions for law enforcement under India's Right to Information Act. Through an ethnographic study of Delhi Police's data collection practices, and analysing the institutional and legal reality within which CMAPS will function, this paper presents one of the first accounts of smart policing in India. Through our findings and discussion we show what kinds of biases are present within Delhi Police's data collection practices currently and how they translate and transfer into initiatives like CMAPS. We further discuss what the biases in CMAPS can teach us about future public sector deployment of socio-technical systems in India and other global South geographies. We also offer methodological considerations for studying AI deployments in non-western contexts. We conclude with a set of recommendations for civil society and social justice actors to consider when engaging with opaque systems implemented in the public sector.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {317–324},
numpages = {8},
keywords = {predictive policing, sociotechnical systems, fairness-aware machine learning, interdisciplinary},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372862,
author = {Geiger, R. Stuart and Yu, Kevin and Yang, Yanlai and Dai, Mindy and Qiu, Jie and Tang, Rebekah and Huang, Jenny},
title = {Garbage in, Garbage out? Do Machine Learning Application Papers in Social Computing Report Where Human-Labeled Training Data Comes From?},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372862},
doi = {10.1145/3351095.3372862},
abstract = {Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper's authors labeling the data themselves. Such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. In this paper, we investigate to what extent a sample of machine learning application papers in social computing --- specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data --- give specific details about whether such best practices were followed. Our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: Does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available. We find a wide divergence in whether such practices were followed and documented. Much of machine learning research and education focuses on what is done once a "gold standard" of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {325–336},
numpages = {12},
keywords = {training data, data labeling, human annotation, content analysis, machine learning, meta-research, research integrity},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375783,
author = {Nasr, Milad and Tschantz, Michael Carl},
title = {Bidding Strategies with Gender Nondiscrimination Constraints for Online Ad Auctions},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375783},
doi = {10.1145/3351095.3375783},
abstract = {Interactions between bids to show ads online can lead to an advertiser's ad being shown to more men than women even when the advertiser does not target towards men. We design bidding strategies that advertisers can use to avoid such emergent discrimination without having to modify the auction mechanism. We mathematically analyze the strategies to determine the additional cost to the advertiser for avoiding discrimination, proving our strategies to be optimal in some settings. We use simulations to understand other settings.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {337–347},
numpages = {11},
keywords = {online auctions, targeted advertising, fairness constraints, MDPs},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372848,
author = {Ilvento, Christina and Jagadeesan, Meena and Chawla, Shuchi},
title = {Multi-Category Fairness in Sponsored Search Auctions},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372848},
doi = {10.1145/3351095.3372848},
abstract = {Fairness in advertising is a topic of particular concern motivated by theoretical and empirical observations in both the computer science and economics literature. We examine the problem of fairness in advertising for general purpose platforms that service advertisers from many different categories. First, we propose inter-category and intra-category fairness desiderata that take inspiration from individual fairness and envy-freeness. Second, we investigate the "platform utility" (a proxy for the quality of allocation) achievable by mechanisms satisfying these desiderata. More specifically, we compare the utility of fair mechanisms against the unfair optimum, and show by construction that our fairness desiderata are compatible with utility. Our mechanisms also enjoy nice implementation properties including metric-obliviousness, which allows the platform to produce fair allocations without needing to know the specifics of the fairness requirements.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {348–358},
numpages = {11},
keywords = {envy-freeness, individual fairness, utility, advertisement auctions, algorithmic fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372837,
author = {Sweeney, Chris and Najafian, Maryam},
title = {Reducing Sentiment Polarity for Demographic Attributes in Word Embeddings Using Adversarial Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372837},
doi = {10.1145/3351095.3372837},
abstract = {The use of word embedding models in sentiment analysis has gained a lot of traction in the Natural Language Processing (NLP) community. However, many inherently neutral word vectors describing demographic identity have unintended implicit correlations with negative or positive sentiment, resulting in unfair downstream machine learning algorithms. We leverage adversarial learning to decorrelate demographic identity term word vectors with positive or negative sentiment, and re-embed them into the word embeddings. We show that our method effectively minimizes unfair positive/negative sentiment polarity while retaining the semantic accuracy of the word embeddings. Furthermore, we show that our method effectively reduces unfairness in downstream sentiment regression and can be extended to reduce unfairness in toxicity classification tasks.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {359–368},
numpages = {10},
keywords = {embeddings, fairness, NLP},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372858,
author = {Celis, L. Elisa and Mehrotra, Anay and Vishnoi, Nisheeth K.},
title = {Interventions for Ranking in the Presence of Implicit Bias},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372858},
doi = {10.1145/3351095.3372858},
abstract = {Implicit bias is the unconscious attribution of particular qualities (or lack thereof) to a member from a particular social group (e.g., defined by gender or race). Studies on implicit bias have shown that these unconscious stereotypes can have adverse outcomes in various social contexts, such as job screening, teaching, or policing. Recently, [34] considered a mathematical model for implicit bias and showed the effectiveness of the Rooney Rule as a constraint to improve the utility of the outcome for certain cases of the subset selection problem. Here we study the problem of designing interventions for the generalization of subset selection - ranking - that requires to output an ordered set and is a central primitive in various social and computational contexts. We present a family of simple and interpretable constraints and show that they can optimally mitigate implicit bias for a generalization of the model studied in [34]. Subsequently, we prove that under natural distributional assumptions on the utilities of items, simple, Rooney Rule-like, constraints can also surprisingly recover almost all the utility lost due to implicit biases. Finally, we augment our theoretical results with empirical findings on real-world distributions from the IIT-JEE (2009) dataset and the Semantic Scholar Research corpus.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {369–380},
numpages = {12},
keywords = {interventions, ranking, algorithmic fairness, implicit bias},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372861,
author = {Liu, Lydia T. and Wilson, Ashia and Haghtalab, Nika and Kalai, Adam Tauman and Borgs, Christian and Chayes, Jennifer},
title = {The Disparate Equilibria of Algorithmic Decision Making When Individuals Invest Rationally},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372861},
doi = {10.1145/3351095.3372861},
abstract = {The long-term impact of algorithmic decision making is shaped by the dynamics between the deployed decision rule and individuals' response. Focusing on settings where each individual desires a positive classification---including many important applications such as hiring and school admissions, we study a dynamic learning setting where individuals invest in a positive outcome based on their group's expected gain and the decision rule is updated to maximize institutional benefit. By characterizing the equilibria of these dynamics, we show that natural challenges to desirable long-term outcomes arise due to heterogeneity across groups and the lack of realizability. We consider two interventions, decoupling the decision rule by group and subsidizing the cost of investment. We show that decoupling achieves optimal outcomes in the realizable case but has discrepant effects that may depend on the initial conditions otherwise. In contrast, subsidizing the cost of investment is shown to create better equilibria for the disadvantaged group even in the absence of realizability.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {381–391},
numpages = {11},
keywords = {dynamics, statistical discrimination, fairness, machine learning},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372831,
author = {Harrison, Galen and Hanson, Julia and Jacinto, Christine and Ramirez, Julio and Ur, Blase},
title = {An Empirical Study on the Perceived Fairness of Realistic, Imperfect Machine Learning Models},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372831},
doi = {10.1145/3351095.3372831},
abstract = {There are many competing definitions of what statistical properties make a machine learning model fair. Unfortunately, research has shown that some key properties are mutually exclusive. Realistic models are thus necessarily imperfect, choosing one side of a trade-off or the other. To gauge perceptions of the fairness of such realistic, imperfect models, we conducted a between-subjects experiment with 502 Mechanical Turk workers. Each participant compared two models for deciding whether to grant bail to criminal defendants. The first model equalized one potentially desirable model property, with the other property varying across racial groups. The second model did the opposite. We tested pairwise trade-offs between the following four properties: accuracy; false positive rate; outcomes; and the consideration of race. We also varied which racial group the model disadvantaged. We observed a preference among participants for equalizing the false positive rate between groups over equalizing accuracy. Nonetheless, no preferences were overwhelming, and both sides of each trade-off we tested were strongly preferred by a non-trivial fraction of participants. We observed nuanced distinctions between participants considering a model "unbiased" and considering it "fair." Furthermore, even when a model within a trade-off pair was seen as fair and unbiased by a majority of participants, we did not observe consensus that a machine learning model was preferable to a human judge. Our results highlight challenges for building machine learning models that are perceived as fair and broadly acceptable in realistic situations.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {392–402},
numpages = {11},
keywords = {accountability, machine learning, survey, fairness, data science},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375623,
author = {Liang, Lizhen and Acuna, Daniel E.},
title = {Artificial Mental Phenomena: Psychophysics as a Framework to Detect Perception Biases in AI Models},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375623},
doi = {10.1145/3351095.3375623},
abstract = {Detecting biases in artificial intelligence has become difficult because of the impenetrable nature of deep learning. The central difficulty is in relating unobservable phenomena deep inside models with observable, outside quantities that we can measure from inputs and outputs. For example, can we detect gendered perceptions of occupations (e.g., female librarian, male electrician) using questions to and answers from a word embedding-based system? Current techniques for detecting biases are often customized for a task, dataset, or method, affecting their generalization. In this work, we draw from Psychophysics in Experimental Psychology---meant to relate quantities from the real world (i.e., "Physics") into subjective measures in the mind (i.e., "Psyche")---to propose an intellectually coherent and generalizable framework to detect biases in AI. Specifically, we adapt the two-alternative forced choice task (2AFC) to estimate potential biases and the strength of those biases in black-box models. We successfully reproduce previously-known biased perceptions in word embeddings and sentiment analysis predictions. We discuss how concepts in experimental psychology can be naturally applied to understanding artificial mental phenomena, and how psychophysics can form a useful methodological foundation to study fairness in AI.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {403–412},
numpages = {10},
keywords = {biases in sentiment analysis, artificial psychophysics, two-alternative forced choice task, biases in word embeddings},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3373156,
author = {Castelle, Michael},
title = {The Social Lives of Generative Adversarial Networks},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373156},
doi = {10.1145/3351095.3373156},
abstract = {Generative adversarial networks (GANs) are a genre of deep learning model of significant practical and theoretical interest for their facility in producing photorealistic 'fake' images which are plausibly similar, but not identical, to a corpus of training data. But from the perspective of a sociologist, the distinctive architecture of GANs is highly suggestive. First, a convolutional neural network for classification, on its own, is (at present) popularly considered to be an 'AI'; and a generative neural network is a kind of inversion of such a classification network (i.e. a layered transformation from a vector of numbers to an image, as opposed to a transformation from an image to a vector of numbers). If, then, in the training of GANs, these two 'AIs' interact with each other in a dyadic fashion, shouldn't we consider that form of learning... social? This observation can lead to some surprising associations as we compare and contrast GANs with the theories of the sociologist Pierre Bourdieu, whose concept of the so-called habitus is one which is simultaneously cognitive and social: a productive perception in which classification practices and practical action cannot be fully disentangled.Bourdieu had long been concerned with the reproduction of social stratification: his early works studied formal public schooling in France not as an egalitarian system but instead as one which unintentionally maintained the persistence of class distinctions. It was, he argued, through the cultural inculcation of an embodied and partially unconscious habitus---a "durably installed generative principle of regulated improvisations"---that, he argued, students from the upper classes are given an advantage which is only further reinforced throughout their educational trajectories. For Bourdieu, institutions of schooling instill "deeply interiorized master patterns" of behavior and thought (and classification) which in turn direct the acquisition of subsequent patterns, whose character is determined not simply by this cognitive layering but by their actual use in lived practice, especially early in childhood development.In this work I develop a productive analogy between the GAN architecture and Bourdieu's habitus, in three ways. First, I call attention to the fact that connectionist approaches and Bourdieu's theories were both conceived as revolts against rule-bound paradigms. In the 1980s, Rumelhart and McClelland used a multilayer neural network to learn the phonology of English past-tense verbs because "sometimes we don't follow the rules... language is full of exceptions to the rules"; and in the case of Bourdieu, the habitus was an answer to a long-standing question: "how can behaviour be regulated without being the product of obedience to rules?" Bourdieu strove to transgress what was then seen in the social sciences as a conceptual opposition between structure-based theories of social life and those which emphasized an embodied agency.Second, I suggest that concerns about bias and discrimination in machine learning in recent years can in part be attributed due to the increased use of ML models not just for static classification but for practical action. Similarly, the habitus for Bourdieu is simultaneously durable and transposable: its judgments may be relatively stable, but are capable of being deployed dynamically in novel and varying social situations---or what ML practitioners might call generalizability. We can thus theorize generative models (including GANs) as biased not just in their stereotyped classifications, but through their potential for actively generating new biased data. These generated actions then recursively become part of the social arena Bourdieu called the field, into which new agents are 'born' and for which they may know few alternatives.Finally, it is intriguing that GAN researchers and Bourdieu both extensively use metaphors from game theory. Goodfellow described the GAN architecture as a "two-player minimax game with value function V(G,D)", meaning that there is a single abstract function whose output value the discriminator is trying to maximize and which the generator is trying to minimize; but the dynamic nature of the GAN training process means that convergence to Nash equilibrium is nontrivial. But for Bourdieu, such a utility-based approach to artistic creation could not be more crude when compared to the social reality of art worlds: utilitarianism is, for him, "the degree zero of sociology", by which he means an isolated, inert, and amodal---and therefore not particularly sociological---starting point. Moreover, 19th-century bohemian culture was characterized primarily by its inversion of financial incentives, in which failure is a kind of success, and "selling out" (i.e. maximizing profit) worst of all; and thus the relentless optimization of neural networks may be fundamentally at odds with the "value functions" of many human artists. I conclude that deep learning, while primarily understood as a scientific and technical achievement, may also intentionally or unintentionally constitute a nascent, independent reinvention of social theory.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {413},
numpages = {1},
keywords = {generative adversarial networks, habitus, game theory, sociological theory, bias},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372854,
author = {Moore, Jared},
title = {Towards a More Representative Politics in the Ethics of Computer Science},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372854},
doi = {10.1145/3351095.3372854},
abstract = {Ethics curricula in computer science departments should include a focus on the political action of students. While 'ethics' holds significant sway over current discourse in computer science, recent work, particularly in data science, has shown that this discourse elides the underlying political nature of the problems that it aims to solve. In order to avoid these pitfalls---such as co-option, whitewashing, and assumed universal values---we should recognize and teach the political nature of computing technologies, largely through science and technology studies. Education is an essential focus not just intrinsically, but also because computing students end up joining the companies which have outsize impacts on our lives. At those companies, students both have a responsibility to society and agency beyond just engineering decisions, albeit not uniformly. I propose that we move away from strict ethics curricula and include examples of and calls for political action of students and future engineers. Through such examples---calls to action, practitioner reflections, legislative engagement, direct action---we might allow engineers to better recognize both their diverse agencies and responsibilities.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {414–424},
numpages = {11},
keywords = {politics, science and technology studies, activism, civics},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372832,
author = {Bates, Jo and Cameron, David and Checco, Alessandro and Clough, Paul and Hopfgartner, Frank and Mazumdar, Suvodeep and Sbaffi, Laura and Stordy, Peter and de la Vega de Le\'{o}n, Antonio},
title = {Integrating FATE/Critical Data Studies into Data Science Curricula: Where Are We Going and How Do We Get There?},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372832},
doi = {10.1145/3351095.3372832},
abstract = {There have been multiple calls for integrating topics related to fairness, accountability, transparency, ethics (FATE) and social justice into Data Science curricula, but little exploration of how this might work in practice. This paper presents the findings of a collaborative auto-ethnography (CAE) engaged in by a MSc Data Science teaching team based at University of Sheffield (UK) Information School where FATE/Critical Data Studies (CDS) topics have been a core part of the curriculum since 2015/16. In this paper, we adopt the CAE approach to reflect on our experiences of working at the intersection of disciplines, and our progress and future plans for integrating FATE/CDS into the curriculum. We identify a series of challenges for deeper FATE/CDS integration related to our own competencies and the wider socio-material context of Higher Education in the UK. We conclude with recommendations for ourselves and the wider FATE/CDS orientated Data Science community.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {425–435},
numpages = {11},
keywords = {critical data studies, data science, FATE, higher education},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372866,
author = {Dean, Sarah and Rich, Sarah and Recht, Benjamin},
title = {Recommendations and User Agency: The Reachability of Collaboratively-Filtered Information},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372866},
doi = {10.1145/3351095.3372866},
abstract = {Recommender systems often rely on models which are trained to maximize accuracy in predicting user preferences. When the systems are deployed, these models determine the availability of content and information to different users. The gap between these objectives gives rise to a potential for unintended consequences, contributing to phenomena such as filter bubbles and polarization. In this work, we consider directly the information availability problem through the lens of user recourse. Using ideas of reachability, we propose a computationally efficient audit for top-N linear recommender models. Furthermore, we describe the relationship between model complexity and the effort necessary for users to exert control over their recommendations. We use this insight to provide a novel perspective on the user cold-start problem. Finally, we demonstrate these concepts with an empirical investigation of a state-of-the-art model trained on a widely used movie ratings dataset.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {436–445},
numpages = {10},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372843,
author = {Papakyriakopoulos, Orestis and Hegelich, Simon and Serrano, Juan Carlos Medina and Marco, Fabienne},
title = {Bias in Word Embeddings},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372843},
doi = {10.1145/3351095.3372843},
abstract = {Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings. We develop a new technique for bias detection for gendered languages and use it to compare bias in embeddings trained on Wikipedia and on political social media data. We investigate bias diffusion and prove that existing biases are transferred to further machine learning models. We test two techniques for bias mitigation and show that the generally proposed methodology for debiasing models at the embeddings level is insufficient. Finally, we employ biased word embeddings and illustrate that they can be used for the detection of similar biases in new data. Given that word embeddings are widely used by commercial companies, we discuss the challenges and required actions towards fair algorithmic implementations and applications.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {446–457},
numpages = {12},
keywords = {diffusion, detection, word embeddings, racism, bias, sexism, mitigation, homophobia, fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372849,
author = {S\'{a}nchez-Monedero, Javier and Dencik, Lina and Edwards, Lilian},
title = {What Does It Mean to 'solve' the Problem of Discrimination in Hiring? Social, Technical and Legal Perspectives from the UK on Automated Hiring Systems},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372849},
doi = {10.1145/3351095.3372849},
abstract = {Discriminatory practices in recruitment and hiring are an ongoing issue that is a concern not just for workplace relations, but also for wider understandings of economic justice and inequality. The ability to get and keep a job is a key aspect of participating in society and sustaining livelihoods. Yet the way decisions are made on who is eligible for jobs, and why, are rapidly changing with the advent and growth in uptake of automated hiring systems (AHSs) powered by data-driven tools. Evidence of the extent of this uptake around the globe is scarce, but a recent report estimated that 98% of Fortune 500 companies use Applicant Tracking Systems of some kind in their hiring process, a trend driven by perceived efficiency measures and cost-savings. Key concerns about such AHSs include the lack of transparency and potential limitation of access to jobs for specific profiles. In relation to the latter, however, several of these AHSs claim to detect and mitigate discriminatory practices against protected groups and promote diversity and inclusion at work. Yet whilst these tools have a growing user-base around the world, such claims of 'bias mitigation' are rarely scrutinised and evaluated, and when done so, have almost exclusively been from a US socio-legal perspective.In this paper, we introduce a perspective outside the US by critically examining how three prominent automated hiring systems (AHSs) in regular use in the UK, HireVue, Pymetrics and Applied, understand and attempt to mitigate bias and discrimination. These systems have been chosen as they explicitly claim to address issues of discrimination in hiring and, unlike many of their competitors, provide some information about how their systems work that can inform an analysis. Using publicly available documents, we describe how their tools are designed, validated and audited for bias, highlighting assumptions and limitations, before situating these in the socio-legal context of the UK. The UK has a very different legal background to the US in terms not only of hiring and equality law, but also in terms of data protection (DP) law. We argue that this might be important for addressing concerns about transparency and could mean a challenge to building bias mitigation into AHSs definitively capable of meeting EU legal standards. This is significant as these AHSs, especially those developed in the US, may obscure rather than improve systemic discrimination in the workplace.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {458–468},
numpages = {11},
keywords = {automated hiring, algorithmic decision-making, social justice, fairness, socio-technical systems, GDPR, discrimination},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372828,
author = {Raghavan, Manish and Barocas, Solon and Kleinberg, Jon and Levy, Karen},
title = {Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372828},
doi = {10.1145/3351095.3372828},
abstract = {There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {469–481},
numpages = {13},
keywords = {discrimination law, algorithmic bias, algorithmic hiring},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372846,
author = {Lum, Kristian and Boudin, Chesa and Price, Megan},
title = {The Impact of Overbooking on a Pre-Trial Risk Assessment Tool},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372846},
doi = {10.1145/3351095.3372846},
abstract = {Pre-trial risk assessment tools are used to make recommendations to judges about appropriate conditions of pre-trial supervision for people who have been arrested. Increasingly, there is concern about whether these models are operating fairly, including concerns about whether the models' input factors are fair measures of one's criminal activity. In this paper, we assess the impact of booking charges that do not result in a conviction on a popular risk assessment tool, the Arnold Public Safety Assessment. Using data from a pilot run of the tool in San Francisco, CA, we find that booking charges that do not result in a conviction (i.e. charges that are dropped or end in an acquittal) increased the recommended level of pre-trial supervision in around 27% of cases evaluated by the tool.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {482–491},
numpages = {10},
keywords = {risk assessment, police accountability, overbooking, fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372877,
author = {Bogen, Miranda and Rieke, Aaron and Ahmed, Shazeda},
title = {Awareness in Practice: Tensions in Access to Sensitive Attribute Data for Antidiscrimination},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372877},
doi = {10.1145/3351095.3372877},
abstract = {Organizations cannot address demographic disparities that they cannot see. Recent research on machine learning and fairness has emphasized that awareness of sensitive attributes, such as race and sex, is critical to the development of interventions. However, on the ground, the existence of these data cannot be taken for granted.This paper uses the domains of employment, credit, and healthcare in the United States to surface conditions that have shaped the availability of sensitive attribute data. For each domain, we describe how and when private companies collect or infer sensitive attribute data for antidiscrimination purposes. An inconsistent story emerges: Some companies are required by law to collect sensitive attribute data, while others are prohibited from doing so. Still others, in the absence of legal mandates, have determined that collection and imputation of these data are appropriate to address disparities.This story has important implications for fairness research and its future applications. If companies that mediate access to life opportunities are unable or hesitant to collect or infer sensitive attribute data, then proposed techniques to detect and mitigate bias in machine learning models might never be implemented outside the lab. We conclude that today's legal requirements and corporate practices, while highly inconsistent across domains, offer lessons for how to approach the collection and inference of sensitive data in appropriate circumstances. We urge stakeholders, including machine learning practitioners, to actively help chart a path forward that takes both policy goals and technical needs into account.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {492–500},
numpages = {9},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372826,
author = {Hanna, Alex and Denton, Emily and Smart, Andrew and Smith-Loud, Jamila},
title = {Towards a Critical Race Methodology in Algorithmic Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372826},
doi = {10.1145/3351095.3372826},
abstract = {We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {501–512},
numpages = {12},
keywords = {algorithmic fairness, critical race theory, race and ethnicity},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375674,
author = {Hu, Lily and Kohler-Hausmann, Issa},
title = {What's Sex Got to Do with Machine Learning?},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375674},
doi = {10.1145/3351095.3375674},
abstract = {The debate about fairness in machine learning has largely centered around competing substantive definitions of what fairness or nondiscrimination between groups requires. However, very little attention has been paid to what precisely a group is. Many recent approaches have abandoned observational, or purely statistical, definitions of fairness in favor of definitions that require one to specify a causal model of the data generating process. The implicit ontological assumption of these exercises is that a racial or sex group is a collection of individuals who share a trait or attribute, for example: the group "female" simply consists in grouping individuals who share female-coded sex features. We show this by exploring the formal assumption of modularity in causal models using directed acyclic graphs (DAGs), which hold that the dependencies captured by one causal pathway are invariant to interventions on any other causal pathways. Modeling sex, for example, as a node in a causal model aimed at elucidating fairness questions proposes two substantive claims: 1) There exists a feature, sex-on-its-own, that is an inherent trait of an individual that then (causally) brings about social phenomena external to it in the world; and 2) the relations between sex and its downstream effects can be modified in whichever ways and the former node would still retain the meaning that sex has in our world. Together, these claims suggest sex to be a category that could be different in its (causal) relations with other features of our social world via hypothetical interventions yet still mean what it means in our world. This fundamental stability of categories and causes (unless explicitly intervened on) is essential in the methodology of causal inference, because without it, causal operations can alter the meaning of a category, fundamentally change how it is situated within a causal diagram, and undermine the validity of any inferences drawn on the diagram as corresponding to any real phenomena in the world.We argue that these methods' ontological assumptions about social groups such as sex are conceptual errors. Many of the "effects" that sex purportedly "causes" are in fact constitutive features of sex as a social status. They constitute what it means to be sexed. In other words, together, they give the social meaning of sex features. These social meanings are precisely, we argue, what makes sex discrimination a distinctively morally problematic type of act that differs from mere irrationality or meanness on the basis of a physical feature.Correcting this conceptual error has a number of important implications for how analytical models can be used to detect discrimination. If what makes something discrimination on the basis of a particular social grouping is that the practice acts on what it means to be in that group in a way that we deem wrongful, then what we need from analytical diagrams is a model of what constitutes the social grouping. Such a model would allow us to explain the special moral (and legal) reasons we have to be concerned with the treatment of this category by reference to the empirical social relations and meanings that establish the category as what it is. Only then can we have the normative debate about what is fair or nondiscriminatory vis-\`{a}-vis that group. We suggest that formal diagrams of constitutive relations would present an entirely different path toward reasoning about discrimination (and relatedly, counterfactuals) because they proffer a model of how the meaning of a social group emerges from its constitutive features. Whereas the value of causal diagrams is to guide the construction and testing of sophisticated modular counterfactuals, the value of constitutive diagrams would be to identify a different kind of counterfactual as central to our inquiry into discrimination: one that asks how the social meaning of a group would be changed if its non-modular features were altered.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {513},
numpages = {1},
keywords = {social philosophy, algorithmic fairness, discrimination, causal inference, machine learning, law},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372864,
author = {Binns, Reuben},
title = {On the Apparent Conflict between Individual and Group Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372864},
doi = {10.1145/3351095.3372864},
abstract = {A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {514–524},
numpages = {11},
keywords = {fairness, justice, individual fairness, discrimination, machine learning, statistical parity},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372878,
author = {D'Amour, Alexander and Srinivasan, Hansa and Atwood, James and Baljekar, Pallavi and Sculley, D. and Halpern, Yoni},
title = {Fairness is Not Static: Deeper Understanding of Long Term Fairness via Simulation Studies},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372878},
doi = {10.1145/3351095.3372878},
abstract = {As machine learning becomes increasingly incorporated within high impact decision ecosystems, there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences. Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics. This is understandable; long term dynamics are hard to assess, particularly because they do not align with the traditional supervised ML research framework that uses fixed data sets. To address this structural difficulty in the field, we advocate for the use of simulation as a key tool in studying the fairness of algorithms. We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans, college admissions, and allocation of attention. By analyzing how learning agents interact with these systems in simulation, we are able to extend previous work, showing that static or single-step analyses do not give a complete picture of the long-term consequences of an ML-based decision system. We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research, available at https://github.com/google/ml-fairness-gym.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {525–534},
numpages = {10},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372857,
author = {Hu, Lily and Chen, Yiling},
title = {Fair Classification and Social Welfare},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372857},
doi = {10.1145/3351095.3372857},
abstract = {Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of Δ'e perturbations to a fairness parameter 'e in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of "fairness-to-welfare" solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring "more fair" classifiers does not abide by the Pareto Principle---a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {535–545},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3373155,
author = {Kim, Michael P. and Korolova, Aleksandra and Rothblum, Guy N. and Yona, Gal},
title = {Preference-Informed Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3373155},
doi = {10.1145/3351095.3373155},
abstract = {In this work, we study notions of fairness in decision-making systems when individuals have diverse preferences over the possible outcomes of the decisions. Our starting point is the seminal work of Dwork et al. [ITCS 2012] which introduced a notion of individual fairness (IF): given a task-specific similarity metric, every pair of individuals who are similarly qualified according to the metric should receive similar outcomes. We show that when individuals have diverse preferences over outcomes, requiring IF may unintentionally lead to less-preferred outcomes for the very individuals that IF aims to protect (e.g. a protected minority group). A natural alternative to IF is the classic notion of fair division, envy-freeness (EF): no individual should prefer another individual's outcome over their own. Although EF allows for solutions where all individuals receive a highly-preferred outcome, EF may also be overly-restrictive for the decision-maker. For instance, if many individuals agree on the best outcome, then if any individual receives this outcome, they all must receive it, regardless of each individual's underlying qualifications for the outcome.We introduce and study a new notion of preference-informed individual fairness (PIIF) that is a relaxation of both individual fairness and envy-freeness. At a high-level, PIIF requires that outcomes satisfy IF-style constraints, but allows for deviations provided they are in line with individuals' preferences. We show that PIIF can permit outcomes that are more favorable to individuals than any IF solution, while providing considerably more flexibility to the decision-maker than EF. In addition, we show how to efficiently optimize any convex objective over the outcomes subject to PIIF for a rich class of individual preferences. Finally, we demonstrate the broad applicability of the PIIF framework by extending our definitions and algorithms to the multiple-task targeted advertising setting introduced by Dwork and Ilvento [ITCS 2019].},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {546},
numpages = {1},
keywords = {algorithmic fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375709,
author = {Yang, Kaiyu and Qinami, Klint and Fei-Fei, Li and Deng, Jia and Russakovsky, Olga},
title = {Towards Fairer Datasets: Filtering and Balancing the Distribution of the People Subtree in the ImageNet Hierarchy},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375709},
doi = {10.1145/3351095.3375709},
abstract = {Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {547–558},
numpages = {12},
keywords = {fairness, dataset construction, computer vision, representative datasets},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372835,
author = {Mustafaraj, Eni and Lurie, Emma and Devine, Claire},
title = {The Case for Voter-Centered Audits of Search Engines during Political Elections},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372835},
doi = {10.1145/3351095.3372835},
abstract = {Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's "related searches" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {559–569},
numpages = {11},
keywords = {search engines, bias, voters, elections, Google, algorithm audits},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372841,
author = {Borradaile, Glencora and Burkhardt, Brett and LeClerc, Alexandria},
title = {Whose Tweets Are Surveilled for the Police: An Audit of a Social-Media Monitoring Tool via Log Files},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372841},
doi = {10.1145/3351095.3372841},
abstract = {Social media monitoring by law enforcement is becoming commonplace, but little is known about what software packages for it do. Through public records requests, we obtained log files from the Corvallis (Oregon) Police Department's use of social media monitoring software called DigitalStakeout. These log files include the results of proprietary searches by DigitalStakeout that were running over a period of 13 months and include 7240 social media posts. In this paper, we focus on the Tweets logged in this data and consider the racial and ethnic identity (through manual coding) of the users that are therein flagged by DigitalStakeout. We observe differences in the demographics of the users whose Tweets are flagged by DigitalStakeout compared to the demographics of the Twitter users in the region, however, our sample size is too small to determine significance. Further, the demographics of the Twitter users in the region do not seem to reflect that of the residents of the region, with an apparent higher representation of Black and Hispanic people. We also reconstruct the keywords related to a Narcotics report set up by DigitalStakeout for the Corvallis Police Department and find that these keywords flag Tweets unrelated to narcotics or flag Tweets related to marijuana, a drug that is legal for recreational use in Oregon. Almost all of the keywords have a common meaning unrelated to narcotics (e.g. broken, snow, hop, high) that call into question the utility that such a keyword based search could have to law enforcement.As social media monitoring is increasingly used for law enforcement purposes, racial biases in surveillance may contribute to existing racial disparities in law enforcement practices. We are hopeful that log files obtainable through public records request will shed light on the operation of these surveillance tools. There are challenges in auditing these tools: public records requests may go unfulfilled even if the data is available, social media platforms may not provide comparable data for comparison with surveillance data, demographics can be difficult to ascertain from social media and Institutional Review Boards may not understand how to weigh the ethical considerations involved in this type of research. We include in this paper a discussion of our experience in navigating these issues.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {570–580},
numpages = {11},
keywords = {demographics, police, surveillance, social media monitoring, audit, keywords},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372825,
author = {Rold\'{a}n, Jos\'{e} Mena and Vila, Oriol Pujol and Marca, Jordi Vitri\`{a}},
title = {Dirichlet Uncertainty Wrappers for Actionable Algorithm Accuracy Accountability and Auditability},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372825},
doi = {10.1145/3351095.3372825},
abstract = {Nowadays, the use of machine learning models is becoming a utility in many applications. Companies deliver pre-trained models encapsulated as application programming interfaces (APIs) that developers combine with third-party components and their own models and data to create complex data products to solve specific problems. The complexity of such products and the lack of control and knowledge of the internals of each component used unavoidable cause effects, such as lack of transparency, difficulty in auditability, and the emergence of potential uncontrolled risks. They are effectively black-boxes. Accountability of such solutions is a challenge for the auditors and the machine learning community.In this work, we propose a wrapper that given a black-box model enriches its output prediction with a measure of uncertainty when applied to a target domain. To develop the wrapper, we follow these steps:Modeling the distribution of the output. In a text classification setting, the output is a probability distribution p(y|X, w*) over the different classes to predict, y, given an input text X and the pre-trained model with parameters w*. We model this output by a random variable to measure the variability that the data noise causes in the output. Here we consider the output distribution coming from a Dirichlet probability density function, thus p(y|X, w*) ~ Dir(α).Decomposition of the Dirichlet concentration parameter. To relate the output of the classifier with the concentration parameter in the Dirichlet distribution, we propose a decomposition of the concentration parameter in two terms: α = βy. The role of this scalar β is to control the spread of the distribution around the expected value, i.e. the original prediction y.Training the wrapper. Sentences are represented as the average value of their word embeddings. This representation feeds a neural network that outputs a single regression value that models the parameter β. For each input, we combine β and the black-box prediction to obtain the corresponding distribution for the output ym,i ~ Dir(αi). By using Monte Carlo sampling, we approximate the expected value of the classification probabilities, [EQUATION] and we train the model applying a cross-entropy loss over the predictions and the labels.Obtaining an uncertainty score from the wrapper. To obtain a numerical value for the uncertainty of a prediction, we draw samples from the resulting Dir(α) to evaluate the predictive entropy with [EQUATION], thus obtaining a numerical score for the uncertainty of each prediction.Using uncertainty for rejection. Based on this wrapper, we provide an actionable mechanism to mitigate risk in the form of decision rejection: once equipped with a value for the uncertainty of a given prediction, we can choose not to issue that prediction when the risk or uncertainty in that decision is significant. This results in a rejection system that selects the more confident predictions, discards those more uncertain, and leads to an improvement in the trustability of the resulting system.We showcase the proposed technique and methodology in a practical scenario where we apply a simulated sentiment analysis API based on NLP to different domains. On each experiment, we train a sentiment classifier using text reviews of products in a source domain. We apply the pre-trained black-box to obtain the predictions for the reviews from a target domain. The tuples of review plus black-box predictions are then used for training the wrapper to obtain the uncertainty. Finally, we use the uncertainty score to sort the predictions from more to less uncertain, and we search for a rejection point that maximizes the three performance measures: non-rejected accuracy, and classification and rejection quality.Experiments demonstrate the effectiveness of the uncertainty measure computed by the wrapper and shows its high correlation to bad quality predictions and misclassifications. In all the cases, the uncertainty metric here proposed outperforms traditional uncertainty measures.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {581},
numpages = {1},
keywords = {accuracy, machine learning, uncertainty, black-box models, auditability},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372851,
author = {Coston, Amanda and Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
title = {Counterfactual Risk Assessments, Evaluation, and Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372851},
doi = {10.1145/3351095.3372851},
abstract = {Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome.Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may---and as we show empirically, do---induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {582–593},
numpages = {12},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372869,
author = {Green, Ben},
title = {The False Promise of Risk Assessments: Epistemic Reform and the Limits of Fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372869},
doi = {10.1145/3351095.3372869},
abstract = {Risk assessments have proliferated in the United States criminal justice system. The theory of change motivating their adoption involves two key assumptions: first, that risk assessments will reduce human biases by making objective decisions, and second, that risk assessments will promote criminal justice reform. In this paper I interrogate both of these assumptions, concluding that risk assessments are an ill-advised tool for challenging the centrality and legitimacy of incarceration within the criminal justice system. First, risk assessments fail to provide objectivity, as their use creates numerous sites of discretion. Second, risk assessments provide no guarantee of reducing incarceration; instead, they risk legitimizing the criminal justice system's structural racism. I then consider, via an "epistemic reform," the path forward for criminal justice reform. I reinterpret recent results regarding the "impossibility of fairness" as not simply a tension between mathematical metrics but as evidence of a deeper tension between notions of equality. This expanded frame challenges the formalist, colorblind proceduralism at the heart of the criminal justice system and suggests a more structural approach to reform. Together, this analysis highlights how algorithmic fairness narrows the scope of judgments about justice and how "fair" algorithms can reinforce discrimination.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {594–606},
numpages = {13},
keywords = {fairness, criminal justice system, risk assessment, social justice},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372850,
author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
title = {Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372850},
doi = {10.1145/3351095.3372850},
abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {607–617},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375234,
author = {Singh, Jaspreet and Anand, Avishek},
title = {Model Agnostic Interpretability of Rankers via Intent Modelling},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375234},
doi = {10.1145/3351095.3375234},
abstract = {A key problem in information retrieval is understanding the latent intention of a user's under-specified query. Retrieval models that are able to correctly uncover the query intent often perform well on the document ranking task. In this paper we study the problem of interpretability for text based ranking models by trying to unearth the query intent as understood by complex retrieval models.We propose a model-agnostic approach that attempts to locally approximate a complex ranker by using a simple ranking model in the term space. Given a query and a blackbox ranking model, we propose an approach that systematically exploits preference pairs extracted from the target ranking and document perturbations to identify a set of intent terms and a simple term based ranker that can faithfully and accurately mimic the complex blackbox ranker in that locality. Our results indicate that we can indeed interpret more complex models with high fidelity. We also present a case study on how our approach can be used to interpret recently proposed neural rankers.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {618–628},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372855,
author = {Panigutti, Cecilia and Perotti, Alan and Pedreschi, Dino},
title = {Doctor XAI: An Ontology-Based Approach to Black-Box Sequential Data Classification Explanations},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372855},
doi = {10.1145/3351095.3372855},
abstract = {Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {629–639},
numpages = {11},
keywords = {healthcare data, machine learning, explainable artificial intelligence},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372836,
author = {Hancox-Li, Leif},
title = {Robustness in Machine Learning Explanations: Does It Matter?},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372836},
doi = {10.1145/3351095.3372836},
abstract = {The explainable AI literature contains multiple notions of what an explanation is and what desiderata explanations should satisfy. One implicit source of disagreement is how far the explanations should reflect real patterns in the data or the world. This disagreement underlies debates about other desiderata, such as how robust explanations are to slight perturbations in the input data. I argue that robustness is desirable to the extent that we're concerned about finding real patterns in the world. The import of real patterns differs according to the problem context. In some contexts, non-robust explanations can constitute a moral hazard. By being clear about the extent to which we care about capturing real patterns, we can also determine whether the Rashomon Effect is a boon or a bane.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {640–647},
numpages = {8},
keywords = {philosophy, explanation, epistemology, robustness, machine learning, ethics, objectivity, methodology, artificial intelligence},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375624,
author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, Jos\'{e} M. F. and Eckersley, Peter},
title = {Explainable Machine Learning in Deployment},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375624},
doi = {10.1145/3351095.3375624},
abstract = {Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {648–657},
numpages = {10},
keywords = {transparency, explainability, qualitative study, machine learning, deployed systems},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372847,
author = {Donahue, Kate and Kleinberg, Jon},
title = {Fairness and Utilization in Allocating Resources with Uncertain Demand},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372847},
doi = {10.1145/3351095.3372847},
abstract = {Resource allocation problems are a fundamental domain in which to evaluate the fairness properties of algorithms. The trade-offs between fairness and utilization have a long history in this domain. A recent line of work has considered fairness questions for resource allocation when the demands for the resource are distributed across multiple groups and drawn from probability distributions. In such cases, a natural fairness requirement is that individuals from different groups should have (approximately) equal probabilities of receiving the resource. A largely open question in this area has been to bound the gap between the maximum possible utilization of the resource and the maximum possible utilization subject to this fairness condition.Here, we obtain some of the first provable upper bounds on this gap. We obtain an upper bound for arbitrary distributions, as well as much stronger upper bounds for specific families of distributions that are typically used to model levels of demand. In particular, we find --- somewhat surprisingly --- that there are natural families of distributions (including Exponential and Weibull) for which the gap is non-existent: it is possible to simultaneously achieve maximum utilization and the given notion of fairness. Finally, we show that for power-law distributions, there is a non-trivial gap between the solutions, but this gap can be bounded by a constant factor independent of the parameters of the distribution.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {658–668},
numpages = {11},
keywords = {algorithmic fairness, weibull distribution, uncertainty, power law distribution, resource allocation},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372842,
author = {Elzayn, Hadi and Fish, Benjamin},
title = {The Effects of Competition and Regulation on Error Inequality in Data-Driven Markets},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372842},
doi = {10.1145/3351095.3372842},
abstract = {Recent work has documented instances of unfairness in deployed machine learning models, and significant researcher effort has been dedicated to creating algorithms that intrinsically consider fairness. In this work, we highlight another source of unfairness: market forces that drive differential investment in the data pipeline for differing groups. We develop a high-level model to study this question. First, we show that our model predicts unfairness in a monopoly setting. Then, we show that under all but the most extreme models, competition does not eliminate this tendency, and may even exacerbate it. Finally, we consider two avenues for regulating a machine-learning driven monopolist - relative error inequality and absolute error-bounds - and quantify the price of fairness (and who pays it). These models imply that mitigating fairness concerns may require policy-driven solutions, not only technological ones.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {669–679},
numpages = {11},
keywords = {learning theory, data markets, economics, game theory, industrial organization, algorithmic fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3372838,
author = {Lundgard, Alan},
title = {Measuring Justice in Machine Learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372838},
doi = {10.1145/3351095.3372838},
abstract = {How can we build more just machine learning systems? To answer this question, we need to know both what justice is and how to tell whether one system is more or less just than another. That is, we need both a definition and a measure of justice. Theories of distributive justice hold that justice can be measured (in part) in terms of the fair distribution of benefits and burdens across people in society. Recently, the field known as fair machine learning has turned to John Rawls's theory of distributive justice for inspiration and operationalization. However, philosophers known as capability theorists have long argued that Rawls's theory uses the wrong measure of justice, thereby encoding biases against people with disabilities. If these theorists are right, is it possible to operationalize Rawls's theory in machine learning systems without also encoding its biases? In this paper, I draw on examples from fair machine learning to suggest that the answer to this question is no: the capability theorists' arguments against Rawls's theory carry over into machine learning systems. But capability theorists don't only argue that Rawls's theory uses the wrong measure, they also offer an alternative measure. Which measure of justice is right? And has fair machine learning been using the wrong one?},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {680},
numpages = {1},
keywords = {fairness, bias, philosophy, operationalization, discrimination, machine learning, distributive justice, capability, disability, measure, justice},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375687,
author = {Baker, Dylan and Hanna, Alex and Denton, Emily},
title = {Algorithmically Encoded Identities: Reframing Human Classification},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375687},
doi = {10.1145/3351095.3375687},
abstract = {Our aim with this workshop is to provide a venue within which the FAT* community can thoughtfully engage with identity and the categories which are imposed on people as part of making sense of their identities. Most people have nuanced and deeply personal understandings of what identity categories mean to them; however, sociotechnical systems must, through a set of classification decisions, reduce the nuance and complexity of those identities into discrete categories. The impact of misclassifications can range from the uncomfortable (e.g. displaying ads for items that aren't desirable) to devastating (e.g. being denied medical care; being evaluated as having a high risk of criminal recidivism). However, even the act of being classified can force an individual into categories which feel foreign and othering. Through this workshop, we hope to connect participants' personal understandings of identity to how identity is 'seen' and categorized by sociotechnical systems.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {681},
numpages = {1},
keywords = {infrastructure studies, identity, classification},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375680,
author = {Baxter, Kathy and Schlesinger, Yoav and Aerni, Sarah and Baker, Lewis and Dawson, Julie and Kenthapadi, Krishnaram and Kloumann, Isabel and Wallach, Hanna},
title = {Bridging the Gap from AI Ethics Research to Practice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375680},
doi = {10.1145/3351095.3375680},
abstract = {The study of fairness in machine learning applications has seen significant academic inquiry, research and publication in recent years. Concurrently, technology companies have begun to instantiate nascent program in AI ethics and product ethics more broadly. As a result of these efforts, AI ethics practitioners have piloted new processes to evaluate and ensure fairness in their machine learning applications. In this session, six industry practitioners, hailing from LinkedIn, Yoti, Microsoft, Pymetrics, Facebook, and Salesforce share insights from the work they have undertaken in the area of fairness, what has worked and what has not, lessons learned and best practices instituted as a result.• Krishnaram Kenthapadi presents LinkedIn's fairness-aware reranking for talent search.• Julie Dawson shares how Yoti applies ML fairness research to age estimation in their digital identity platform.• Hanna Wallach contributes how Microsoft is applying fairness principles in practice.• Lewis Baker presents Pymetric's fairness mechanisms in their hiring algorithm.• Isabel Kloumann presents Facebook's fairness assessment framework through a case study of fairness in a content moderation system.• Sarah Aerni contributes how Salesforce is building fairness features into the Einstein AI platform.Building on those insights, we discuss insights and brainstorm modalities through which to build upon the practitioners' work. Opportunities for further research or collaboration are identified, with the goal of developing a shared understanding of experiences and needs of AI ethics practitioners. Ultimately, the aim is to develop a playbook for more ethical and fair AI product development and deployment.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {682},
numpages = {1},
keywords = {ethics, artificial intelligence, fairness, data, practitioners, algorithmic decision-making, ML fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375697,
author = {Pritchard, Helen and Snodgrass, Eric and Morrison, Romi Ron and Britton, Loren and Moll, Joana},
title = {Burn, Dream and Reboot! Speculating Backwards for the Missing Archive on Non-Coercive Computing},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375697},
doi = {10.1145/3351095.3375697},
abstract = {Whether one is speaking of barbed wire, the assembly line or computer operating systems, the history of coercive technologies for the automation of tasks has focused on optimization, determinate outcomes and an ongoing disciplining of components and bodies. Automated technologies of the present emerge and are marked by this lineage of coercive modes of implementation, whose scarred history of techniques of discrimination, exploitation and extraction point to an archive of automated injustices in computing, a history that continues to charge present paradigms and practices of computing.This workshop addresses the history of coercive technologies through attuning to how we perform speculation within practices of computing through a renewed attention to this history. We go backwards into the archive, rather than racing forward and proposing ever new speculative futures of automation. This is because speculative creative approaches are often conceived and positioned as methodological toolkits for addressing computing practices by imagining for/with others for a "future otherwise". We argue that "speculation" as the easy-go-to of designers and artists trying to address automated injustices needs some undoing, as without work it will always be confined within ongoing legacies of coercive modes of computing practice. Instead of creating more just-worlds, the generation of ever-new futures by creative speculation often merely reinforces the project of coercive computing.For this workshop, drawing on queer approaches to resisting futures and informed by activist feminist engagements with archives, we invite participants to temporarily resist imagining futures and instead to speculate backwards. We speculate backwards to various moments, artefacts and practices within computing history. What does it mean to understand techniques of computing and automation as coercive infrastructures? How did so many of the dreams and seeming promises of computing turn into the coercive practices that we see today? Has computing as a practice become so imbued with coercive techniques that we find it hard to imagine otherwise? Together, we will build a speculative understanding and possible archive of non-coercive computing. In the words of Alexis Pauline Gumbs, the emerging archive proposes "how did their dreams make rooms to dream in"... or not, in the case of coercive practices of computing. And "what if she changes her dream?" What if we reboot this dream?1},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {683},
numpages = {1},
keywords = {speculative design, automation, social justice, optimization, trans*feminist technoscience, critical computing},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375686,
author = {Givens, Alexandra Reeve and Morris, Meredith Ringel},
title = {Centering Disability Perspectives in Algorithmic Fairness, Accountability, &amp; Transparency},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375686},
doi = {10.1145/3351095.3375686},
abstract = {It is vital to consider the unique risks and impacts of algorithmic decision-making for people with disabilities. The diverse nature of potential disabilities poses unique challenges for approaches to fairness, accountability, and transparency. Many disabled people choose not to disclose their disabilities, making auditing and accountability tools particularly hard to design and operate. Further, the variety inherent in disability poses challenges for collecting representative training data in any quantity sufficient to better train more inclusive and accountable algorithms.This panel highlights areas of concern, present emerging research efforts, and enlist more researchers and advocates to study the potential impacts of algorithmic decision-making on people with disabilities. A key objective is to surface new research projects and collaborations, including by integrating a critical disability perspective into existing research and advocacy efforts focused on identifying sources of bias and advancing equity.In the technology space, discussion topics will include methods to assess the fairness of current AI systems, and strategies to develop new systems and bias mitigation approaches that ensure fairness for people with disabilities. For example, how do today's currently-deployed AI systems impact people with disabilities? If developing inclusive datasets is part of the solution, how can researchers ethically gather such data, and what risks might centralizing data about disability pose? What new privacy solutions must developers create to reduce the risk of deductive disclosure of identities of people with disabilities in "anonymized" datasets? How can AI models and bias mitigation techniques be developed that handle the unique challenges of disability, i.e., the "long tail" and low incidence of many types of disability - for instance, how do we ensure that data about disability are not treated as outliers? What are the pros and cons of developing custom/personalized AI models for people with disabilities versus ensuring that general models are inclusive?In the law and policy space, the framework for people with disabilities requires specific study. For example, the Americans with Disabilities Act (ADA) requires employers to adopt "reasonable accommodations" for qualified individuals with a disability. But what is a "reasonable accommodation" in the context of machine learning and AI? How will the ADA's unique standards interact with case law and scholarship about algorithmic bias against other protected groups? When the ADA governs what questions employers can ask about a candidate's disability, and HIPAA and the Genetic Information Privacy Act regulate the sharing of health information, how should we think about inferences from data that approximate such questions?Panelists will bring varied perspectives to this conversation, including backgrounds in computer science, disability studies, legal studies, and activism. In addition to their scholarly expertise, several panelists have direct lived experience with disability. The session format will consist of brief position statements from each panelist, followed by questions from the moderator, and then open questions from and discussion with the audience.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {684},
numpages = {1},
keywords = {algorithmic bias, AI FATE, disability studies, accessibility},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375689,
author = {Sassaman, Hannah and Lee, Jennifer and Irvine, Jenessa and Narayan, Shankar},
title = {Creating Community-Based Tech Policy: Case Studies, Lessons Learned, and What Technologists and Communities Can Do Together},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375689},
doi = {10.1145/3351095.3375689},
abstract = {What are the core ways the field of data science can center community voice and power throughout all the processes involved in conceptualizing, creating, and disseminating technology?? What are the most possible and most urgent ways communities can shape the field of algorithmic decision-making to center community power in the next few years? This interactive workshop will highlight some of the following lessons learned through our combined experience engaging with communities challenging technology in Seattle and Philadelphia, cities in the United States. We will discuss the historical context of disproportionate impacts of technology on marginalized and vulnerable communities; case studies including criminal justice risk assessments, face surveillance technologies, and surveillance regulations; and work in small-group and break-out sessions to engage questions about when and where technologists hold power, serve as gatekeepers, and can work in accountable partnership with impacted communities.By the end of the session, we hope that participants will learn how to actively center diverse communities in creating technology by examining successes, challenges, and ongoing work in Seattle and Philadelphia, through the following lessons we have learned:• that communities, policy-makers, and technologists need to work intimately together to lift up each other's' goals• that communities need to gain data justice and data literacy to understand and independently audit how a system is impacting them• that scientific analyses of algorithmic bias are powerful but heard most clearly when lifted up by local community members and stakeholders in decisions where algorithms might be deployed• that anecdotal stories of harm are most impactful on decisionmakers when tied to rigorous scientific analysis and examples from other communities that amplify and ground those stories• that communities and community goals and standards are often not heard in conversations between data scientists and people who deploy algorithms, as well as in decision-makers' conversations about what policy should look like• and that we need to begin to craft what it means for those with the least power in conversations about algorithmic fairness - those judged by those tools - to have far more, or even the most power in the future of their design or implementation.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {685},
numpages = {1},
keywords = {surveillance, case studies, disproportionate impact, bias, algorithms, criminal justice, community-centered},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375692,
author = {Hanna, Alex and Denton, Emily},
title = {CtrlZ.AI Zine Fair: Critical Perspectives},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375692},
doi = {10.1145/3351095.3375692},
abstract = {The FAT* conference has begun the necessary conversation on the normative implications and ethical ramifications of sociotechnical systems. However, many scholars have pointed to the limitations in methodologies and scope of analysis (e.g. [8, 11]). In addition to these critiques, we add in the fact that those who are most affected by this technology do not have the skills, training, or technical aptitude to participate in these conversations. With the exception of the 2018 FAT* tutorial which featured Terrance Wilkerson (who had been labeled as likely to highly recidivate by COMPAS) and his partner, there has been silence from those most impacted by algorithmic unfairness at FAT*. This silence has been deafening, as FAT* conversations - with a few notable exceptions (e.g. [1, 4]) - have failed to discuss anti-racist politics, prison abolition, and social justice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {686},
numpages = {1},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375688,
author = {Allhutter, Doris and Berendt, Bettina},
title = {Deconstructing FAT: Using Memories to Collectively Explore Implicit Assumptions, Values and Context in Practices of Debiasing and Discrimination-Awareness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375688},
doi = {10.1145/3351095.3375688},
abstract = {Research in fairness, accountability, and transparency (FAT) in socio-technical systems needs to take into account how practices of computing are entrenched with power relations in complex and multi-layered ways. Trying to disentangle the way in which structural discrimination and normative computational concepts and methods are intertwined, this frequently raises the question of WHO are the actors that shape technologies and research agendas---who gets to speak and to define bias, (un)fairness, and discrimination? "Deconstructing FAT" is a CRAFT workshop that aims at complicating this question by asking how "we" as researchers in FAT (often unknowingly) mobilize implicit assumptions, values and beliefs that reflect our own embeddedness in power relations, our disciplinary ways of thinking, and our historically, locally, and culturally-informed ways of solving computational problems or approaching our research. This is a vantage point to make visible and analyze the normativity of technical approaches, concepts and methods that are part of the repertoire of FAT research. Inspired by a previous international workshop [1], this CRAFT workshop engages an interdisciplinary panel of FAT researchers in a deconstruction exercise that traces the following issues:(1) FAT research frequently speaks of social bias that is amplified by algorithmic systems, of the problem of discriminatory consequences that is to be solved, and of underprivileged or vulnerable groups that need to be protected. What does this perspectivity imply in terms of the approaches, methods and metrics that are being applied? How do methods of debiasing and discrimination-awareness enact the epistemic power of a perspective of privilege as their norm?(2) FAT research has emphasized the need for multi- or interdisciplinary approaches to get a grip on the complex intertwining of social power relations and the normativity of computational methods, norms and practices. Clearly, multi- and interdisciplinary research includes different normative frameworks and ways of thinking that need to be negotiated. This is complicated by the fact that these frameworks are not fully transparent and ready for reflection. What are the normative implications of interdisciplinary collaboration in FAT research? (3) While many problems of discrimination, marginalization and exploitation can be similar across places, they can also have specific local shapes. How can FAT research e.g. consider historically grown specifics such as the effects of different colonial histories? If these specifics make patterns of discrimination have different and more nuanced dimensions than clear-cut 'redlining', what does this imply?To explore these questions, we use the method of 'mind scripting' which is based in theories of discourse, ideology, memory and affect and aims at investigating hidden patterns of meaning making in written memories of the panelists [2]. The workshop strives to challenge some of the implicit norms and tensions in FAT research and to trigger future directions.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {687},
numpages = {1},
keywords = {discrimination-awareness in machine learning, deconstruction},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375684,
author = {Barry, Marguerite and Kerr, Aphra and Smith, Oliver},
title = {Ethics on the Ground: From Principles to Practice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375684},
doi = {10.1145/3351095.3375684},
abstract = {Surveys of public attitudes show that people believe it is possible to design ethical AI. However the everyday professional development context can offer minimal space for ethical reflection or oversight, creating a significant gap between public expectations and the performance of ethics in practice. This 2-part workshop includes an offsite visit to Telef\'{o}nica Innovation Alpha and uses storytelling and theatre methods to examine how and where ethical reflection happens on the ground. It will explore the gaps in expectations and identify alternative approaches to more effective ethical performance. Bringing social scientists, data scientists, designers, civic rights activists and ethics consultants together to focus on AI/ML in the health context, it will foster critical and creative activities that will bring to the surface the structural, disciplinary, social and epistemological challenges to effective ethical performance in practice. Participants will explore and enact where, when and how meaningful interventions can happen.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {688},
numpages = {1},
keywords = {human-computer interaction, mHealth, ethics guidelines, eHealth, ethics of AI, sociology of expectations, explainable AI, fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375683,
author = {Szymielewicz, Katarzyna and Bacciarelli, Anna and Hidvegi, Fanny and Foryciarz, Agata and P\'{e}nicaud, Soizic and Spielkamp, Matthias},
title = {Where Do Algorithmic Accountability and Explainability Frameworks Take Us in the Real World? From Theory to Practice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375683},
doi = {10.1145/3351095.3375683},
abstract = {This hands-on session takes academic concepts and their formulation in policy initiatives around algorithmic accountability and explainability and tests them against real cases. In small groups we will (1) test selected frameworks on algorithmic accountability and explainability against a concrete case study (that likely constitutes a human rights violation) and (2) test different formats to explain important aspects of an automated decision-making process (such as input data, type of an algorithm used, design decisions and technical parameters, expected outcomes) to various audiences (end users, affected communities, watchdog organisations, public sector agencies and regulators). We invite participants with various backgrounds: researchers, technologists, human rights advocates, public servants and designers.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {689},
numpages = {1},
keywords = {fairness, explanations, algorithmic decision-making, discrimination, accountability, algorithmic impact assessment, philosophy of science, interpretability},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375690,
author = {Ahmad, Muhammad Aurangzeb and Teredesai, Ankur and Eckert, Carly},
title = {Fairness, Accountability, Transparency in AI at Scale: Lessons from National Programs},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375690},
doi = {10.1145/3351095.3375690},
abstract = {The panel aims to elucidate how different national govenmental programs are implementing accountability of machine learning systems in healthcare and how accountability is operationlized in different cultural settings in legislation, policy and deployment. We have representatives from three different govenments, UAE, Singapore and Maldives who will discuss what accountability of AI and machine learning means in their contexts and use cases. We hope to have a fruitful conversation around FAT ML as it is operationalized ccross cultures, national boundries and legislative constraints.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {690},
numpages = {1},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375695,
author = {Williams, Patrick and Kind, Eric},
title = {Hardwiring Discriminatory Police Practices: The Implications of Data-Driven Technological Policing on Minority (Ethnic and Religious) People and Communities},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375695},
doi = {10.1145/3351095.3375695},
abstract = {On data-based policing.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {691},
numpages = {1},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375685,
author = {Wan, Evelyn and de Groot, Aviva and Jameson, Shazade and P\u{a}un, Mara and L\"{u}cking, Phillip and Klumbyte, Goda and L\"{a}mmerhirt, Danny},
title = {Lost in Translation: An Interactive Workshop Mapping Interdisciplinary Translations for Epistemic Justice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375685},
doi = {10.1145/3351095.3375685},
abstract = {There are gaps in understanding in and between those who design systems of AI/ ML, those who critique them, and those positioned between these discourses. This gap can be defined in multiple ways - e.g. methodological, epistemological, linguistic, or cultural. To bridge this gap requires a set of translations: the generation of a collaborative space and a new set of shared sensibilities that traverse disciplinary boundaries. This workshop aims to explore translations across multiple fields, and translations between theory and practice, as well as how interdisciplinary work could generate new operationalizable approaches.We define 'knowledge' as a social product (L. Code) which requires fair and broad epistemic cooperation in its generation, development, and dissemination. As a "marker for truth" (B. Williams) and therefore a basis for action, knowledge circulation sustains the systems of power which produce it in the first place (M. Foucault). Enabled by epistemic credence, authority or knowledge, epistemic power can be an important driver of, but also result from, other (e.g. economic, political) powers.To produce reliable output, our standards and methods should serve us all and exclude no-one. Critical theorists have long revealed failings of epistemic practices, resulting in the marginalization and exclusion of some types of knowledge. How can we cultivate more reflexive epistemic practices in the interdisciplinary research setting of FAT*?We frame this ideal as 'epistemic justice' (M. Geuskens), the positive of 'epistemic injustice', defined by M. Fricker as injustice that exists when people are wronged as a knower or as an epistemic subject. Epistemic justice is the proper use and allocation of epistemic power; the inclusion and balancing of all epistemic sources.As S. Jasanoff reminds us, any authoritative way of seeing must be legitimized in discourse and practice, showing that practices can be developed to value and engage with other viewpoints and possibly reshape our ways of knowing.Our workshop aims to address the following questions: how could critical theory or higher level critiques be translated into and anchored in ML/AI design practices - and vice versa? What kind of cartographies and methodologies are needed in order to identify issues that can act as the basis of collaborative research and design? How can we (un)learn our established ways of thinking for such collaborative work to take place? During the workshop, participants will create, share and explode prototypical workflows of designing, researching and critiquing algorithmic systems. We will identify moments in which translations and interdisciplinary interventions could or should happen in order to build actionable steps and methodological frameworks that advance epistemic justice and are conducive to future interdisciplinary collaboration.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {692},
numpages = {1},
keywords = {epistemic justice, methodologies, critical theory, interdisciplinary collaboration, algorithm development, workflow},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375682,
author = {Goss, Ezra and Hu, Lily and Sabin, Manuel and Teeple, Stephanie},
title = {Manifesting the Sociotechnical: Experimenting with Methods for Social Context and Social Justice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375682},
doi = {10.1145/3351095.3375682},
abstract = {Critiques of 'algorithmic fairness' have counseled against a purely technical approach. Recent work from the FAT* conference has warned specifically about abstracting away the social context that these automated systems are operating within and has suggested that "[fairness work] require[s] technical researchers to learn new skills or partner with social scientists" [Fairness and abstraction in sociotechnical systems, Selbst et al. 2019, FAT* '19]. That "social context" includes groups outside the academy organizing for data and/or tech justice (e.g., Allied Media Projects, Stop LAPD Spying Coalition, data4blacklives, etc). These struggles have deep historical roots but have become prominent in the past several years alongside broader citizen-science efforts. In this CRAFT session we as STEM researchers hope to initiate conversation about methods used by community organizers to analyze power relations present in that social context. We will take this time to learn together and discuss if/how these and other methods, collaborations and efforts can be used to actualize oft-mentioned critiques of algorithmic fairness and move toward a data justice-oriented approach.Many scholars and activists have spoken on how to approach social context when discussing algorithmic fairness interventions. Community organizing and attendant methods for power analysis present one such approach: documenting all stakeholders and entities relevant to an issue and the nature of the power differentials between them. The facilitators for this session are not experts in community organizing theory or practice. Instead, we will share what we have learned from our readings of decades of rich work and writings from community organizers. This session is a collective, interdisciplinary learning experience, open to all who see their interests as relevant to the conversation.We will open with a discussion of community organizing practice: What is community organizing, what are its goals, methods, past and ongoing examples? What disciplines and intellectual lineages does it draw from? We will incorporate key sources we have found helpful for synthesizing this knowledge so that participants can continue exposing themselves to the field after the conference. We will also consider the concept of social power, including power that the algorithmic fairness community holds. Noting that there are many ways to theorize and understand power, we will share the framings that have been most useful to us. We plan to present different tools, models and procedures for doing power analysis in various organizing settings.We will propose to our group that we conduct a power analysis of our own. We have prepared a hypothetical but realistic scenario involving risk assessment in a hospital setting as an example. However, we encourage participants to bring their own experiences to the table, especially if they pertain in any way to data injustice. We also invite participants to bring examples of ongoing organizing efforts with which algorithmic fairness researchers could act in solidarity. Participants will walk away from this session with 1) an understanding of the key terms and sources necessary to gain further exposure to these topics and 2) preliminary experience analyzing power in realistic, grounded scenarios.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {693},
numpages = {1},
keywords = {community organizing, data justice, fairness, power analysis},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375691,
author = {Barocas, Solon and Biega, Asia J. and Fish, Benjamin and Niklas, Jundefineddrzej and Stark, Luke},
title = {When Not to Design, Build, or Deploy},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375691},
doi = {10.1145/3351095.3375691},
abstract = {Recent debate within the FAT* community has focused on how the field conceptualizes the problems it seeks to address, what approach the field should take in attempting to address these problems, and whether the field should even pursue some of the proposed remedies. Questions regarding when not to design, build, or deploy a technology are perhaps the most common expression of this trend. Identifying the problems to address is inextricably linked to the broader question of how to collectively make decisions about what technologies our societies need and want.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {695},
numpages = {1},
keywords = {politics of tech refusal, technology refusal, technology policy},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375667,
author = {Arya, Vijay and Bellamy, Rachel K. E. and Chen, Pin-Yu and Dhurandhar, Amit and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Liao, Q. Vera and Luss, Ronny and Mojsilovi\'{c}, Aleksandra and Mourad, Sami and Pedemonte, Pablo and Raghavendra, Ramya and Richards, John and Sattigeri, Prasanna and Shanmugam, Karthikeyan and Singh, Moninder and Varshney, Kush R. and Wei, Dennis and Zhang, Yunfeng},
title = {AI Explainability 360: Hands-on Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375667},
doi = {10.1145/3351095.3375667},
abstract = {This tutorial will teach participants to use and contribute to a new open-source Python package named AI Explainability 360 (AIX360) (https://aix360.mybluemix.net), a comprehensive and extensible toolkit that supports interpretability and explainability of data and machine learning models.Motivation for the toolkit. The AIX360 toolkit illustrates that there is no single approach to explainability that works best for all situations. There are many ways to explain: data vs. model, direct vs. post-hoc explanation, local vs. global, etc. The toolkit includes ten state of the art algorithms that cover different dimensions of explanations along with proxy explainability metrics. Moreover, one of our prime objectives is for AIX360 to serve as an educational tool even for non-machine learning experts (viz. social scientists, healthcare experts). To this end, the toolkit has an interactive demonstration, highly descriptive Jupyter notebooks covering diverse real-world use cases, and guidance materials, all helping one navigate the complex explainability space.Compared to existing open-source efforts on AI explainability, AIX360 takes a step forward in focusing on a greater diversity of ways of explaining, usability in industry, and software engineering. By integrating these three aspects, we hope that AIX360 will attract researchers in AI explainability and help translate our collective research results for practicing data scientists and developers deploying solutions in a variety of industries. Regarding the first aspect of diversity, Table 1 in [1] compares AIX360 to existing toolkits in terms of the types of explainability methods offered. The table shows that AIX360 not only covers more types of methods but also has metrics which can act as proxies for judging the quality of explanations. Regarding the second aspect of industry usage, AIX360 illustrates how these explainability algorithms can be applied in specific contexts (please see Audience, goals, and outcomes below).In just a few months since its initial release, the AIX360 toolkit already has a vibrant slack community with over 120 members and has been forked almost 80 times accumulating over 400 stars. This response leads us to believe that there is significant interest in the community in learning more about the toolkit and explainability in general.Audience, goals, and outcomes. The presentations in the tutorial will be aimed at an audience with different backgrounds and computer science expertise levels. For all audience members and especially those unfamiliar with Python programming, AIX360 provides an interactive experience (http://aix360.mybluemix.net/data) centered around a credit approval scenario as a gentle and grounded introduction to the concepts and capabilities of the toolkit. We will also teach all participants which type of explainability algorithm is most appropriate for a given use case, not only for those in the toolkit but also from the broader explainability literature. Knowing which explainability algorithms apply to which contexts and understanding when to use them can benefit most people, regardless of their technical background. The second part of the tutorial will consist of three use cases featuring different industry domains and explanation methods. Data scientists and developers can gain hands-on experience with the toolkit by running and modifying Jupyter notebooks, while others will be able to follow along by viewing rendered versions of the notebooks.Here is a rough agenda of the tutorial:1) Overture: Provide a brief introduction to the area of explainability as well as introduce common terms.2) Interactive Web Experience: The AIX360 interactive web experience (http://aix360.mybluemix.net/data) is intended to show a non-computer science audience how different explainability methods may suit different stakeholders in a credit approval scenario (data scientists, loan officers, and bank customers).3) Taxonomy: We will next present a taxonomy that we have created for organizing the space of explanations and guiding practitioners toward an appropriate choice for their applications.4) Installation: We will transition into a Python environment and ask participants to install the AIX360 package on their machines using provided instructions.5) Example Use Cases in Finance, Government, and Healthcare: We will take participants through three use-cases in various application domains in the form of Jupyter notebooks.6) Metrics: We will briefly showcase the two explainability metrics currently available through the toolkit.7) Future Directions: The final segment will be to discuss future directions and how participants can contribute to the toolkit.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {696},
numpages = {1},
keywords = {explainability, open source, interpretability, transparency, taxonomy},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375672,
author = {Rakova, Bogdana and Chowdhury, Rumman and Yang, Jingying},
title = {Assessing the Intersection of Organizational Structure and FAT* Efforts within Industry: Implications Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375672},
doi = {10.1145/3351095.3375672},
abstract = {The work within the Fairness, Accountability, and Transparency of ML (fair-ML) community will positively benefit from appreciating the role of organizational culture and structure in the effective practice of fair-ML efforts of individuals, teams, and initiatives within industry. In this tutorial session we will explore various organizational structures and possible leverage points to effectively intervene in the process of design, development, and deployment of AI systems, towards contributing to positive fair-ML outcomes. We will begin by presenting the results of interviews conducted during an ethnographic study among practitioners working in industry, including themes related to: origination and evolution, common challenges, ethical tensions, and effective enablers. The study was designed through the lens of Industrial Organizational Psychology and aims to create a mapping of the current state of the fair-ML organizational structures inside major AI companies. We also look at the most-desired future state to enable effective work to increase algorithmic accountability, as well as the key elements in the transition from the current to that future state. We investigate drivers for change as well as the tensions between creating an 'ethical' system vs one that is 'ethical' enough. After presenting our preliminary findings, the rest of the tutorial will be highly interactive. Starting with a facilitated activity in break out groups, we will discuss the already identified challenges, best practices, and mitigation strategies. Finally, we hope to create space for productive discussion among AI practitioners in industry, academic researchers within various fields working directly on algorithmic accountability and transparency, advocates for various communities most impacted by technology, and others. Based on the interactive component of the tutorial, facilitators and interested participants will collaborate on further developing the discussed challenges into scenarios and guidelines that will be published as a follow up report.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {697},
numpages = {1},
keywords = {organizational structure, need-finding, empirical study, I/O psychology, fair machine learning},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375673,
author = {Oswald, Marion and Powell, David},
title = {Can an Algorithmic System Be a 'friend' to a Police Officer's Discretion? ACM FAT 2020 Translation Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375673},
doi = {10.1145/3351095.3375673},
abstract = {This tutorial aims to increase understanding of the importance of discretion in police decision-making. It will guide computer scientists, policy-makers, lawyers and others in considering practical and technical issues crucial to avoiding the prejudicial and instead develop algorithms that are supportive - a 'friend'- to legitimate discretionary decision-making. It combines explanation of the relevant law and related literature with discussion based upon deep operational experience in the area of preventative and protective policing work.Autonomy and discretion are fundamental to police work, not only in relation to strategy and policy but for day-to-day operational decisions taken by front line officers. Such discretion 'recognizes the fallibility of interfacing rules with their field of application.' (Hildebrandt 2016). This discretion is not unbounded however and English common law expects discretion to be exercised reasonably and fairly. Conversely, discretion must not be fettered unlawfully, by failing to take a relevant factor into account when making a decision, or by abdicating responsibility to another person, body or 'thing'. Algorithmic systems have the potential to contribute to factors relevant to the decision in question at the point of interaction between their outputs and the real-world outcome for the victim, offender and/or community.Algorithmic decision tools present a number of challenges to legitimate discretionary police decision-making. Unnuanced outputs could be highly influential on the human decision-maker (Cooke and Michie 2012) and may undermine discretionary power to deal with atypical cases and 'un-thought of' factors that rely upon uncodified knowledge (Oswald 2018).Practical and technical considerations will be crucial to developing MLA that are supportive to discretionary decision-making. These include the methodological approach, design of the humancomputer interface having regard the decision-maker's responsibility to give reasons for their decision, the avoidance of unnuanced or over-confident framing of results, understanding of the policing context in which the MLA will operate, and consideration of the implications of organisational culture and processes to the MLA's influence.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {698},
numpages = {1},
keywords = {police, machine learning, discretion, algorithms},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375664,
author = {Gade, Krishna and Geyik, Sahin Cem and Kenthapadi, Krishnaram and Mithal, Varun and Taly, Ankur},
title = {Explainable AI in Industry: Practical Challenges and Lessons Learned: Implications Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375664},
doi = {10.1145/3351095.3375664},
abstract = {Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with the proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI have become far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability [2, 4]. Model explainability is considered a prerequisite for building trust and adoption of AI systems in high stakes domains such as lending and healthcare [1] requiring reliability, safety, and fairness. It is also critical to automated transportation, and other industrial applications with significant socio-economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling.As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale [5, 6, 8]. In fact, the field of explainability in AI/ML is at an inflexion point. There is a tremendous need from the societal, regulatory, commercial, end-user, and model developer perspectives. Consequently, practical and scalable explainability approaches are rapidly becoming available. The challenges for the research community include: (i) achieving consensus on the right notion of model explainability, (ii) identifying and formalizing explainability tasks from the perspectives of various stakeholders, and (iii) designing measures for evaluating explainability techniques.In this tutorial, we will first motivate the need for model interpretability and explainability in AI [3] from various perspectives. We will then provide a brief overview of several explainability techniques and tools. The rest of the tutorial will focus on the real-world application of explainability techniques in industry. We will present case studies spanning several domains such as:• Search and Recommendation systems: Understanding of search and recommendations systems, as well as how retrieval and ranking decisions happen in real-time [7]. Example applications include explanation of decisions made by an AI system towards job recommendations, ranking of potential candidates for job posters, and content recommendations.• Sales: Understanding of sales predictions in terms of customer up-sell/churn.• Fraud Detection: Examining and explaining AI systems that determine whether a content or event is fraudulent.• Lending: How to understand/interpret lending decisions made by an AI system.We will focus on the sociotechnical dimensions, practical challenges, and lessons learned during development and deployment of these systems, which would be beneficial for researchers and practitioners interested in explainable AI. Finally, we will discuss open challenges and research directions for the community.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {699},
numpages = {1},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375670,
author = {Burke, Robin Douglas and Mansoury, Masoud and Sonboli, Nasim},
title = {Experimentation with Fairness-Aware Recommendation Using Librec-Auto: Hands-on Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375670},
doi = {10.1145/3351095.3375670},
abstract = {The field of machine learning fairness has developed metrics, methodologies, and data sets for experimenting with classification algorithms. However, equivalent research is lacking in the area of personalized recommender systems. This 180-minute hands-on tutorial will introduce participants to concepts in fairness-aware recommendation, and metrics and methodologies in evaluating recommendation fairness. Participants will also gain hands-on experience with conducting fairness-aware recommendation experiments with the LibRec recommendation system using the libauto{} scripting platform, and learn the steps required to configure their own experiments, incorporate their own data sets, and design their own algorithms and metrics.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {700},
numpages = {1},
keywords = {evaluation, recommender systems, software, fairness},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375669,
author = {Sen, Indira and Fl\"{o}ck, Fabian and Weller, Katrin and Wei\ss{}, Bernd and Wagner, Claudia},
title = {From the Total Survey Error Framework to an Error Framework for Digital Traces of Humans: Translation Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375669},
doi = {10.1145/3351095.3375669},
abstract = {The digital traces of hundreds of millions of people offer increasingly comprehensive pictures of both individuals and groups on different platforms, but also allow inferences about broader target populations beyond those platforms. Studying the errors that can occur when digital traces are used to learn about humans and social phenomena is essential. Many similar errors also affect survey estimates, which survey designers have been addressing for decades, most notably using the Total Survey Error Framework (TSE). In this tutorial, we first introduce the audience to the concepts and guidelines of the TSE and how they are applied by survey practitioners in the social sciences. Second, we introduce our own conceptual framework to diagnose, understand, and avoid errors that may occur in studies that are based on digital traces of humans.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {701},
numpages = {1},
keywords = {survey methodology, digital traces, representativeness, computational social science, measurement errors},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375665,
author = {Cath, Corinne and Latonero, Mark and Marda, Vidushi and Pakzad, Roya},
title = {Leap of FATE: Human Rights as a Complementary Framework for AI Policy and Practice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375665},
doi = {10.1145/3351095.3375665},
abstract = {The premise of this translation tutorial is that human rights serves as a complementary framework - in addition to Fairness, Accountability, Transparency, and Ethics - for guiding and governing artificial intelligence (AI) and machine learning research and development. Attendees will participate in a case study, which will demonstrate show how a human rights framework, grounded in international law, fundamental values, and global systems of accountability, can offer the technical community a practical approach to addressing global AI risks and harms. This tutorial discusses how human rights frameworks can inform, guide and govern AI policy and practice in a manner that is complementary to Fairness, Accountability, Transparency, and Ethics (FATE) frameworks. Using the case study of researchers developing a facial recognition API at a tech company and its use by a law enforcement client, we will engage the audience to think through the benefits and challenges of applying human rights frameworks to AI system design and deployment. We will do so by providing a brief overview of the international human rights law, and various non-binding human rights frameworks in relation to our current discussions around FATE and then apply them to contemporary debates and case studies},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {702},
numpages = {1},
keywords = {law, policy, governance, ethics, practice, FAT, AI, human rights},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375668,
author = {Duarte, Natasha and Adams, Stan},
title = {Policy 101: An Introduction to Public Policymaking in the EU and US},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375668},
doi = {10.1145/3351095.3375668},
abstract = {Navigating the rules, processes, and venues through which public policy is made can seem daunting. But public participation in these processes is a crucial part of democratic governance. With a general understanding of when, where, and how to engage in policymaking, anyone can become a policy advocate. This tutorial will introduce some of the most common US (federal and state) and EU policymaking processes and provide guidance to experts in other domains (such as data and computer science) who want to get involved in policymaking. We will discuss the practical considerations involved in identifying and choosing among policymaking opportunities and discuss how to maximize the impact of policymaking interventions. This tutorial is intended to be interactive and will be improved by audience participation and questions.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {703},
numpages = {1},
keywords = {advocacy, government, policy, public policy},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375666,
author = {Kaeser-Chen, Christine and Dubois, Elizabeth and Sch\"{u}\"{u}r, Friederike and Moss, Emanuel},
title = {Positionality-Aware Machine Learning: Translation Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375666},
doi = {10.1145/3351095.3375666},
abstract = {Positionality is a person's unique and always partial view of the world which is shaped by social and political contexts. Machine Learning (ML) systems have positionality, too, as a consequence of the choices we make when we develop ML systems. Being positionality-aware is key for ML practitioners to acknowledge and embrace the necessary choices embedded in ML by its creators.When groups form a shared view of the world, or group positionality, they have the power to embed and institutionalize their unique perspectives in artifacts such as standards and ontologies. For example, the international standard for reporting diseases and health conditions (International Classification of Diseases, ICD) is shaped by a distinctly medical, European and North American perspective. It dictates how we collect data, and limits what questions we can ask of data and what ML systems we can develop. Researchers struggle to study the effects of social factors on health outcomes because of what the ICD renders legible (usually in medicalized terms) and what it renders invisible (usually social contexts) in data. The ICD, as with all information infrastructures, promotes and propagates the perspective(s) of its creators. Over time, it establishes what counts as "truth".Positionality, and how it embeds itself in standards, ontologies, and data collection, is the root for bias in our data and algorithms. Every perspective has its limits - there is no view from nowhere. Without an awareness of positionality, the current debate on bias in machine learning is quite limited: adding more data to the set cannot remove bias. Instead, we propose positionality-aware ML, a new workflow focused on continuous evaluation and improvement of the fit between the positionality embedded in ML systems and the scenarios within which it is deployed.To demonstrate how to uncover positionality in standards, ontologies, data, and ML systems, we discuss recent work on online harassment of Canadian journalists and politicians on Twitter. Using legal definitions of hate speech and harassment, Twitter's community standards, and insight from interviews with journalists and politicians, we created standards and annotation guidelines for labeling the intensity of harassment in tweets. We then hand labeled a sample of data and through this process identified instances where positionality impacts choices about how many categories of harassment should exist, how to label boundary cases, and how to interpret messy data. We take three perspectives---technical, systems, socio-technical---that when combined illuminate areas of tension which serve as a signal of misalignment between the positionality embedded in the ML system and the deployment context. We demonstrate how the concept of positionality allows us to delineate sets of use cases that may not be suited for automated, ML solutions. Finally, we discuss strategies for developing positionality-aware ML systems, which embed a positionality appropriate for the application context, and continuously evolve to maintain this contextual fit, with an emphasis on the need for of democratic, egalitarian dialogues between knowledge-producing groups.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {704},
numpages = {1},
keywords = {positionality, categories, artificial intelligence},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375662,
author = {Wexler, James and Pushkarna, Mahima and Robinson, Sara and Bolukbasi, Tolga and Zaldivar, Andrew},
title = {Probing ML Models for Fairness with the What-If Tool and SHAP: Hands-on Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375662},
doi = {10.1145/3351095.3375662},
abstract = {As more and more industries use machine learning, it's important to understand how these models make predictions, and where bias can be introduced in the process. In this tutorial we'll walk through two open source frameworks for analyzing your models from a fairness perspective. We'll start with the What-If Tool, a visualization tool that you can run inside a Python notebook to analyze an ML model. With the What-If Tool, you can identify dataset imbalances, see how individual features impact your model's prediction through partial dependence plots, and analyze human-centered ML models from a fairness perspective using various optimization strategies.Then we'll look at SHAP, a tool for interpreting the output of any machine learning model, and seeing how a model arrived at predictions for individual datapoints. We will then show how to use SHAP and the What-If Tool together. After the tutorial you'll have the skills to get started with both of these tools on your own datasets, and be better equipped to analyze your models from a fairness perspective.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {705},
numpages = {1},
keywords = {fairness, data visualization, machine learning},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375671,
author = {Jacobs, Abigail Z. and Blodgett, Su Lin and Barocas, Solon and Daum\'{e}, Hal and Wallach, Hanna},
title = {The Meaning and Measurement of Bias: Lessons from Natural Language Processing},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375671},
doi = {10.1145/3351095.3375671},
abstract = {The recent interest in identifying and mitigating bias in computational systems has introduced a wide range of different---and occasionally incomparable---proposals for what constitutes bias in such systems. This tutorial introduces the language of measurement modeling from the quantitative social sciences as a framework for examining how social, organizational, and political values enter computational systems and unpacking the varied normative concerns operationalized in different techniques for measuring "bias." We show that this framework helps to clarify the way unobservable theoretical constructs---such as "creditworthiness," "risk to society," or "tweet toxicity"---are turned into measurable quantities and how this process may introduce fairness-related harms. In particular, we demonstrate how to systematically assess the construct validity and reliability of these measurements to detect and characterize specific types of harms, which arise from mismatches between constructs and their operationalizations. We then take a critical look at existing approaches to examining "bias" in NLP models, ranging from work on embedding spaces to machine translation and hate speech detection. We show that measurement modeling can help uncover the implicit constructs that such work aims to capture when measuring "bias." In so doing, we illustrate the limits of current "debiasing" techniques, which have obscured the specific harms whose measurements they implicitly aim to reduce. By introducing the language of measurement modeling, we provide the FAT* community with a framework for making explicit and testing assumptions about unobservable theoretical constructs embedded in computational systems, thereby clarifying and uniting our understandings of fairness-related harms.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {706},
numpages = {1},
keywords = {fairness, word embeddings, construct validity, measurement, bias},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3351095.3375663,
author = {Ganesh, Maya Indira and Dechesne, Francien and Waseem, Zeerak},
title = {Two Computer Scientists and a Cultural Scientist Get Hit by a Driver-Less Car: A Method for Situating Knowledge in the Cross-Disciplinary Study of F-A-T in Machine Learning: Translation Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375663},
doi = {10.1145/3351095.3375663},
abstract = {In a workshop organized in December 2017 in Leiden, the Netherlands, a group of lawyers, computer scientists, artists, activists and social and cultural scientists collectively read a computer science paper about 'improving fairness'. This session was perceived by many participants as eye-opening on how different epistemologies shape approaches to the problem, method and solutions, thus enabling further cross-disciplinary discussions during the rest of the workshop. For many participants it was both refreshing and challenging, in equal measure, to understand how another discipline approached the problem of fairness. Now, as a follow-up we propose a translation tutorial that will engage participants at the FAT* conference in a similar exercise. We will invite participants to work in small groups reading excerpts of academic papers from different disciplinary perspectives on the same theme. We argue that most of us do not read outside our disciplines and thus are not familiar with how the same issues might be framed and addressed by our peers. Thus the purpose will be to have participants reflect on the different genealogies of knowledge in research, and how they erect walls, or generate opportunities for more productive inter-disciplinary work. We argue that addressing, through technical measures or otherwise, matters of ethics, bias and discrimination in AI/ML technologies in society is complicated by the different constructions of knowledge about what ethics (or bias or discrimination) means to different groups of practitioners. In the current academic structure, there are scarce resources to test, build on-or even discard-methods to talk across disciplinary lines. This tutorial is thus proposed to see if this particular method might work.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {707},
numpages = {1},
keywords = {situated knowledge, natural language processing, discrimination, social sciences, science, epistemology, methodology, bias, ethics, cross-disciplinary, humanities},
location = {Barcelona, Spain},
series = {FAT* '20}
}

