@inproceedings{10.1145/3278721.3278805,
author = {Freeman, Richard B. and Furman, Jason},
title = {The Great AI/Robot Jobs Scare: Reality or ... Not Reality of Automation Fear Redux},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278805},
doi = {10.1145/3278721.3278805},
abstract = {This talk will consider the impact of AI/robots on employment, wages and the future of work more broadly. We argue that we should focus on policies that make AI robotics technology broadly inclusive both in terms of consumption and ownership so that billions of people can benefit from higher productivity and get on the path to the coming age of intolerable abundance.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1},
numpages = {1},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278806,
author = {Lin, Patrick},
title = {AI Decisions, Risk, and Ethics: Beyond Value Alignment},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278806},
doi = {10.1145/3278721.3278806},
abstract = {When we think about the values AI should have in order to make right decisions and avoid wrong ones, there's a large but hidden third category to consider: decisions that are not-wrong but also not-right. This is the grey space of judgment calls, and just having good values might not help as much as you'd think here. I'll use autonomous cars as my case study here, with lessons for broader AI: ethical dilemmas can arise in everyday scenarios such as lane positioning and navigation, not just in crazy crash scenarios. This is the space where one good value might conflict with another good value, and there's no "right" answer or even broad consensus on an answer; so, it's important to recognize the hard cases-which are potential limits-in the study of AI ethics.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {2},
numpages = {1},
keywords = {dilemma, ai, risk, cars, decisions, value alignment, ethics, autonomous vehicles},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278739,
author = {Aziz, Haris and Lee, Barton E.},
title = {Sub-Committee Approval Voting and Generalized Justified Representation Axioms},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278739},
doi = {10.1145/3278721.3278739},
abstract = {Social choice is replete with various settings including single-winner voting, multi-winner voting, probabilistic voting, multiple referenda, and public decision making. We study a general model of social choice called sub-committee voting (SCV) that simultaneously generalizes these settings. We then focus on sub-committee voting with approvals and propose extensions of the justified representation axioms that have been considered for proportional representation in approval-based committee voting. We study the properties and relations of these axioms. For each of the axioms, we analyze whether a representative committee exists and also examine the complexity of computing and verifying such a committee.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {3–9},
numpages = {7},
keywords = {preference aggregation, justified representation, computational social choice, approval-based voting, voting},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278761,
author = {Babaei, Mahmoudreza and Kulshrestha, Juhi and Chakraborty, Abhijnan and Benevenuto, Fabr\'{\i}cio and Gummadi, Krishna P. and Weller, Adrian},
title = {Purple Feed: Identifying High Consensus News Posts on Social Media},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278761},
doi = {10.1145/3278721.3278761},
abstract = {Although diverse news stories are actively posted on social media, readers often focus on the news which reinforces their pre-existing views, leading to 'filter bubble' effects. To combat this, some recent systems expose and nudge readers toward stories with different points of view. One example is the Wall Street Journal's 'Blue Feed, Red Feed' system, which presents posts from biased publishers on each side of a topic. However, these systems have had limited success. We present a complementary approach which identifies high consensus 'purple' posts that generate similar reactions from both 'blue' and 'red' readers. We define and operationalize consensus for news posts on Twitter in the context of US politics. We show that high consensus posts can be identified and discuss their empirical properties. We present a method for automatically identifying high and low consensus news posts on Twitter, which can work at scale across many publishers. To do this, we propose a novel category of audience leaning based features, which we show are well suited to this task. Finally, we present our 'Purple Feed' system which highlights high consensus posts from publishers on both sides of the political spectrum.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {10–16},
numpages = {7},
keywords = {consensus, news consumption in social media, audience leaning based features, polarization, purple feed},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278741,
author = {Bisconti Lucidi, Piercosma and Nardi, Daniele},
title = {Companion Robots: The Hallucinatory Danger of Human-Robot Interactions},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278741},
doi = {10.1145/3278721.3278741},
abstract = {The advent of the so-called Companion Robots is raising many ethical concerns among scholars and in the public opinion. Focusing mainly on robots caring for the elderly, in this paper we analyze these concerns to distinguish which are directly ascribable to robotic, and which are instead pre-existent. One of these is the "deception objection", namely the ethical unacceptability of deceiving the user about the simulated nature of the robot's behaviors. We argue on the inconsistency of this charge, as today formulated. After that, we underline the risk, for human-robot interaction, to become a hallucinatory relation where the human would subjectify the robot in a dynamic of meaning-overload. Finally, we analyze the definition of "quasi-other" relating to the notion of "uncanny". The goal of this paper is to argue that the main concern about Companion Robots is the simulation of a human-like interaction in the absence of an autonomous robotic horizon of meaning. In addition, that absence could lead the human to build a hallucinatory reality based on the relation with the robot.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {17–22},
numpages = {6},
keywords = {human-computer interactions, companion robots, quasi-other, philosophy of technology},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278767,
author = {Bj\o{}rgen, Edvard P. and Madsen, Simen and Bj\o{}rknes, Therese S. and Heims\ae{}ter, Fredrik V. and H\r{a}vik, Robin and Linderud, Morten and Longberg, Per-Niklas and Dennis, Louise A. and Slavkovik, Marija},
title = {Cake, Death, and Trolleys: Dilemmas as Benchmarks of Ethical Decision-Making},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278767},
doi = {10.1145/3278721.3278767},
abstract = {Artificial intelligence (AI) systems are becoming part of our lives and societies. The more decisions such systems make for us, the more we need to ensure that the decisions they make have a positive individual and societal ethical impact. How can we estimate how good a system is at making ethical decisions? Benchmarking is used to evaluate how good a machine or a process performs with respect to industry bests. In this paper we argue that (some) ethical dilemmas can be used as benchmarks for estimating the ethical performance of an autonomous system. We advocate that an open source repository of such dilemmas should be maintained. We present a prototype of such a repository available at https://imdb. uib.no/dilemmaz/articles/all1.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {23–29},
numpages = {7},
keywords = {machine ethics, benchmarking},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278750,
author = {Carey, Ryan},
title = {Incorrigibility in the CIRL Framework},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278750},
doi = {10.1145/3278721.3278750},
abstract = {A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. 2015 in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {30–35},
numpages = {6},
keywords = {corrigibility, cooperative inverse reinforcement learning, ai safety, cirl},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278780,
author = {Cave, Stephen and \'{O}h\'{E}igeartaigh, Se\'{a}n S.},
title = {An AI Race for Strategic Advantage: Rhetoric and Risks},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278780},
doi = {10.1145/3278721.3278780},
abstract = {The rhetoric of the race for strategic advantage is increasingly being used with regard to the development of artificial intelligence (AI), sometimes in a military context, but also more broadly. This rhetoric also reflects real shifts in strategy, as industry research groups compete for a limited pool of talented researchers, and nation states such as China announce ambitious goals for global leadership in AI. This paper assesses the potential risks of the AI race narrative and of an actual competitive race to develop AI, such as incentivising corner-cutting on safe-ty and governance, or increasing the risk of conflict. It explores the role of the research community in respond-ing to these risks. And it briefly explores alternative ways in which the rush to develop powerful AI could be framed so as instead to foster collaboration and respon-sible progress.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {36–40},
numpages = {5},
keywords = {arms race, ai risks, international cooperation, artificial intelligence, strategic competition, ai safety, ai narratives, global governance},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278757,
author = {Chan, Hau and Tran-Thanh, Long and Wilder, Bryan and Rice, Eric and Vayanos, Phebe and Tambe, Milind},
title = {Utilizing Housing Resources for Homeless Youth Through the Lens of Multiple Multi-Dimensional Knapsacks},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278757},
doi = {10.1145/3278721.3278757},
abstract = {There are over 1 million homeless youth in the U.S. each year. To reduce homelessness, U.S. Housing and Urban Development (HUD) and housing communities provide housing programs/services to homeless youth with the goal of improving their long-term situation. Housing communities are facing a difficult task of filling their housing programs, with as many youths as possible, subject to resource constraints for meeting the needs of youth. Currently, the assignment is manually done by humans working in the housing communities. In this paper, we consider the problem of assigning homeless youth to housing programs subject to resource constraints. We provide an initial abstract model for this setting and show that the problem of maximizing the total assigned youth to the programs under this model is APX-hard. To solve the problem, we non-trivially formulate it as a multiple multi-dimensional knapsack problem (MMDKP), which is not known to have any approximation algorithm. We provide a first interpretable and easy-to-use greedy algorithm with logarithmic approximation ratio for solving general MMDKP. We conduct experiments on random and realistic instances of the housing assignment settings and show that our algorithm is efficient and effective in solving large instances (up to 1 million youth).},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {41–47},
numpages = {7},
keywords = {multi-dimensional knapsack, knapsack, housing allocation, homeless youth, greedy algorithm, approximation algorithm, simulation},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278740,
author = {Chopra, Amit K. and SIngh, Munindar P.},
title = {Sociotechnical Systems and Ethics in the Large},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278740},
doi = {10.1145/3278721.3278740},
abstract = {Advances in AI techniques and computing platforms have triggered a lively and expanding discourse on ethical decision making by autonomous agents. Much recent work in AI concentrates on the challenges of moral decision making from a decision-theoretic perspective, and especially the representation of various ethical dilemmas. Such approaches may be useful but in general are not productive because moral decision making is as context-driven as other forms of decision making, if not more. In contrast, we consider ethics not from the standpoint of an individual agent but of the wider sociotechnical systems (STS) in which the agent operates. Our contribution in this paper is the conception of ethical STS founded on governance that takes into account stakeholder values, normative constraints on agents, and outcomes (states of the STS) that obtain due to actions taken by agents. An important element of our conception is accountability, which is necessary for adequate consideration of outcomes that prima facie appear ethical or unethical. Focusing on STS provides a basis for tackling the difficult problems of ethics because the norms of an STS give an operational basis for agent decision making.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {48–53},
numpages = {6},
keywords = {moral dilemmas, sociotechnical systems, multiagent systems, norms, accountability, governance, autonomy, decision making},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278765,
author = {d'Aquin, Mathieu and Troullinou, Pinelopi and O'Connor, Noel E. and Cullen, Aindrias and Faller, Gr\'{a}inne and Holden, Louise},
title = {Towards an "Ethics by Design" Methodology for AI Research Projects},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278765},
doi = {10.1145/3278721.3278765},
abstract = {Addressing ethical issues arising from AI research, and by extension from most areas of Data Science, is a core challenge in both the academic and industry worlds. The nature of research and the specific set of technical skills involved imply that AI and Data Science researchers are not equipped to identify and anticipate such issues arising, or to establish solutions at the time a specific research project is being designed. In this paper, we discuss the need for a methodology for ethical research design that involves a broader set of skills from the start of the project. We specifically identify, from the relevant literature, a set of requirements that we argue to be needed for such a methodology. We then explore two case studies where such ethical considerations have been explored in conjunction with the development of specific research projects, in order to validate those assumptions and generalise them into a set of principles guiding an "Ethics by Design" method for conducting AI and Data Science research.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {54–59},
numpages = {6},
keywords = {data science ethics, privacy by design, ai ethics, methodology, data ethics},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278745,
author = {Dignum, Virginia and Baldoni, Matteo and Baroglio, Cristina and Caon, Maurizio and Chatila, Raja and Dennis, Louise and G\'{e}nova, Gonzalo and Haim, Galit and Klie\ss{}, Malte S. and Lopez-Sanchez, Maite and Micalizio, Roberto and Pav\'{o}n, Juan and Slavkovik, Marija and Smakman, Matthijs and van Steenbergen, Marlies and Tedeschi, Stefano and van der Toree, Leon and Villata, Serena and de Wildt, Tristan},
title = {Ethics by Design: Necessity or Curse?},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278745},
doi = {10.1145/3278721.3278745},
abstract = {Ethics by Design concerns the methods, algorithms and tools needed to endow autonomous agents with the capability to reason about the ethical aspects of their decisions, and the methods, tools and formalisms to guarantee that an agent's behavior remains within given moral bounds. In this context some questions arise: How and to what extent can agents understand the social reality in which they operate, and the other intelligences (AI, animals and humans) with which they co-exist? What are the ethical concerns in the emerging new forms of society, and how do we ensure the human dimension is upheld in interactions and decisions by autonomous agents?. But overall, the central question is: "Can we, and should we, build ethically-aware agents?" This paper presents initial conclusions from the thematic day of the same name held at PRIMA2017, on October 2017.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {60–66},
numpages = {7},
keywords = {ethics by design, multi-agent systems, machine ethics},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278729,
author = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
title = {Measuring and Mitigating Unintended Bias in Text Classification},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278729},
doi = {10.1145/3278721.3278729},
abstract = {We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {67–73},
numpages = {7},
keywords = {natural language processing, text classification, fairness, machine learning, algorithmic bias},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278769,
author = {Dyrkolbotn, Sjur and Pedersen, Truls and Slavkovik, Marija},
title = {On the Distinction between Implicit and Explicit Ethical Agency},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278769},
doi = {10.1145/3278721.3278769},
abstract = {With recent advances in artificial intelligence and the rapidly increasing importance of autonomous intelligent systems in society, it is becoming clear that artificial agents will have to be designed to comply with complex ethical standards. As we work to develop moral machines, we also push the boundaries of existing legal categories. The most pressing question is what kind of ethical decision-making our machines are actually able to engage in. Both in law and in ethics, the concept of agency forms a basis for further legal and ethical categorisations, pertaining to decision-making ability. Hence, without a cross-disciplinary understanding of what we mean by ethical agency in machines, the question of responsibility and liability cannot be clearly addressed. Here we make first steps towards a comprehensive definition, by suggesting ways to distinguish between implicit and explicit forms of ethical agency.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {74–80},
numpages = {7},
keywords = {autonomy, artificial intelligence, ethics, agency, epistemology},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278736,
author = {Ehsan, Upol and Harrison, Brent and Chan, Larry and Riedl, Mark O.},
title = {Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278736},
doi = {10.1145/3278721.3278736},
abstract = {We introduce em AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had performed the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of an autonomous agent into natural language. We evaluate our technique in the Frogger game environment, training an autonomous game playing agent to rationalize its action choices using natural language. A natural language training corpus is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation and show the results of two experiments evaluating the effectiveness of rationalization. Results of these evaluations show that neural machine translation is able to accurately generate rationalizations that describe agent behavior, and that rationalizations are more satisfying to humans than other alternative methods of explanation.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {81–87},
numpages = {7},
keywords = {machine learning, transparency, user perception, ai rationalization, artificial intelligence, explainable ai, interpretability},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278760,
author = {Eicher, Bobbie and Polepeddi, Lalith and Goel, Ashok},
title = {Jill Watson Doesn't Care If You're Pregnant: Grounding AI Ethics in Empirical Studies},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278760},
doi = {10.1145/3278721.3278760},
abstract = {Jill Watson is our name for a virtual teaching assistant for a Georgia Tech course on artificial intelligence: Jill answers routine, frequently asked questions on the class discussion forum. In this paper, we outline some of the ethical issues that arose in the development and deployment of the virtual teaching assistant. We posit that experiments such as Jill Watson are critical for deeply understanding AI ethics.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {88–94},
numpages = {7},
keywords = {question-answering systems, virtual teaching assistant, online education, ethics of artificial intelligence, turing test, education access, theory of mind, metacognition},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278731,
author = {Erd\'{e}lyi, Olivia J. and Goldsmith, Judy},
title = {Regulating Artificial Intelligence: Proposal for a Global Solution},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278731},
doi = {10.1145/3278721.3278731},
abstract = {Given the ubiquity of artificial intelligence (AI) in modern societies, it is clear that individuals, corporations, and countries will be grappling with the legal and ethical issues of its use. As global problems require global solutions, we propose the establishment of an international AI regulatory agency that --- drawing on interdisciplinary expertise --- could create a unified framework for the regulation of AI technologies and inform the development of AI policies around the world. We urge that such an organization be developed with all deliberate haste, as issues such as cryptocurrencies, personalized political ad hacking, autonomous vehicles and autonomous weaponized agents are already a reality, affecting international trade, politics, and war.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {95–101},
numpages = {7},
keywords = {hard/soft law, transnational legal ordering, international organizations, international governance},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278730,
author = {Estrada, Daniel},
title = {Value Alignment, Fair Play, and the Rights of Service Robots},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278730},
doi = {10.1145/3278721.3278730},
abstract = {Ethics and safety research in artificial intelligence is increasingly framed in terms of "alignment'' with human values and interests. I argue that Turing's call for "fair play for machines'' is an early and often overlooked contribution to the alignment literature. Turing's appeal to fair play suggests a need to correct human behavior to accommodate our machines, a surprising inversion of how value alignment is treated today. Reflections on "fair play'' motivate a novel interpretation of Turing's notorious "imitation game'' as a condition not of intelligence but instead of value alignment : a machine demonstrates a minimal degree of alignment (with the norms of conversation, for instance) when it can go undetected when interrogated by a human. I carefully distinguish this interpretation from the Moral Turing Test, which is not motivated by a principle of fair play, but instead depends on imitation of human moral behavior. Finally, I consider how the framework of fair play can be used to situate the debate over robot rights within the alignment literature. I argue that extending rights to service robots operating in public spaces is "fair'' in precisely the sense that it encourages an alignment of interests between humans and machines.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {102–107},
numpages = {6},
keywords = {robot rights, fair play, value alignment, alan turing},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278733,
author = {Farnadi, Golnoosh and Babaki, Behrouz and Getoor, Lise},
title = {Fairness in Relational Domains},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278733},
doi = {10.1145/3278721.3278733},
abstract = {AI and machine learning tools are being used with increasing frequency for decision making in domains that affect peoples' lives such as employment, education, policing and loan approval. These uses raise concerns about biases of algorithmic discrimination and have motivated the development of fairness-aware machine learning. However, existing fairness approaches are based solely on attributes of individuals. In many cases, discrimination is much more complex, and taking into account the social, organizational, and other connections between individuals is important. We introduce new notions of fairness that are able to capture the relational structure in a domain. We use first-order logic to provide a flexible and expressive language for specifying complex relational patterns of discrimination. Furthermore, we extend an existing statistical relational learning framework, probabilistic soft logic (PSL), to incorporate our definition of relational fairness. We refer to this fairness-aware framework FairPSL. FairPSL makes use of the logical definitions of fairnesss but also supports a probabilistic interpretation. In particular, we show how to perform maximum a posteriori(MAP) inference by exploiting probabilistic dependencies within the domain while avoiding violation of fairness guarantees. Preliminary empirical evaluation shows that we are able to make both accurate and fair decisions.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {108–114},
numpages = {7},
keywords = {probabilistic soft logic, statistical relational learning, fairness},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278727,
author = {Freedman, Rachel and Schaich Borg, Jana and Sinnott-Armstrong, Walter and Dickerson, John P. and Conitzer, Vincent},
title = {Adapting a Kidney Exchange Algorithm to Align with Human Values},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278727},
doi = {10.1145/3278721.3278727},
abstract = {The efficient allocation of limited resources is a classical problem in economics and computer science. In kidney exchanges, a central market maker allocates living kidney donors to patients in need of an organ. Patients and donors in kidney exchanges are prioritized using ad-hoc weights decided on by committee and then fed into an allocation algorithm that determines who get what---and who does not. In this paper, we provide an end-to-end methodology for estimating weights of individual participant profiles in a kidney exchange. We first elicit from human subjects a list of patient attributes they consider acceptable for the purpose of prioritizing patients (e.g., medical characteristics, lifestyle choices, and so on). Then, we ask subjects comparison queries between patient profiles and estimate weights in a principled way from their responses. We show how to use these weights in kidney exchange market clearing algorithms. We then evaluate the impact of the weights in simulations and find that the precise numerical values of the weights we computed matter little, other than the ordering of profiles that they imply. However, compared to not prioritizing patients at all, there is a significant effect, with certain classes of patients being (de)prioritized based on the human-elicited value judgments.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {115},
numpages = {1},
keywords = {kidney exchange, ethical ai, value judgment alignment},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278722,
author = {Goel, Naman and Yaghini, Mohammad and Faltings, Boi},
title = {Non-Discriminatory Machine Learning through Convex Fairness Criteria},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278722},
doi = {10.1145/3278721.3278722},
abstract = {We introduce a novel technique to achieve non-discrimination in machine learning without sacrificing convexity and probabilistic interpretation. We also propose a new notion of fairness for machine learning called the weighted proportional fairness and show that our technique satisfies this subjective fairness criterion.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {116},
numpages = {1},
keywords = {non-discrimination, machine learning, proportional fairness},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278732,
author = {Grimm, Cindy M. and Smart, William D. and Hartzog, Woodrow},
title = {An Education Model of Reasonable and Good-Faith Effort for Autonomous Systems},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278732},
doi = {10.1145/3278721.3278732},
abstract = {In this paper we propose a framework for conceptualizing and demonstrating a good-faith effort when developing autonomous systems. The framework addresses two fundamental problems facing autonomous systems: (1) the disconnect between human-mental models and machine-based sensors and algorithms; and (2) unpredictability in complex systems. We address these problems using a mix of education - explicitly delineating the mapping between human concepts and their machine equivalents in a structured manner - and data sampling with expected ranges as a testing mechanism.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {117–121},
numpages = {5},
keywords = {product law, unit testing, ethics, mental models},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278746,
author = {Gruetzemacher, Ross},
title = {Rethinking AI Strategy and Policy as Entangled Super Wicked Problems},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278746},
doi = {10.1145/3278721.3278746},
abstract = {This paper attempts a preliminary analysis of the general approach to AI strategy/policy research through the lens of wicked problems literature. Wicked problems are a class of social policy problems for which traditional methods of resolution fail. Super wicked problems refer to even more complex social policy problems, e.g. climate change. We first propose a hierarchy of three classes of AI strategy/policy problems, all wicked or super wicked problems. We next identify three independent super wicked problems in AI strategy/policy and propose that the most significant of these challenges - the development of safe and beneficial artificial general intelligence - to be significantly more complex and nuanced, thus posing a new degree of 'wickedness.' We then explore analysis and techniques for addressing wicked problems and super wicked problems. This leads to a discussion of the implications of these ideas on the problems of AI strategy/policy.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {122},
numpages = {1},
keywords = {existential risk, wicked problems, ai strategy, ai policy, super wicked problems},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278777,
author = {Henderson, Peter and Sinha, Koustuv and Angelard-Gontier, Nicolas and Ke, Nan Rosemary and Fried, Genevieve and Lowe, Ryan and Pineau, Joelle},
title = {Ethical Challenges in Data-Driven Dialogue Systems},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278777},
doi = {10.1145/3278721.3278777},
abstract = {The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {123–129},
numpages = {7},
keywords = {machine learning, reproducibility, natural language processing, privacy, reinforcement learning, adversarial examples, security, dialogue systems, ethics and safety, bias, computers and society},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278753,
author = {Hooker, John N. and Kim, Tae Wan N.},
title = {Toward Non-Intuition-Based Machine and Artificial Intelligence Ethics: A Deontological Approach Based on Modal Logic},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278753},
doi = {10.1145/3278721.3278753},
abstract = {We propose a deontological approach to machine (or AI) ethics that avoids some weaknesses of an intuition-based system, such as that of Anderson and Anderson. In particular, it has no need to deal with conflicting intuitions, and it yields a more satisfactory account of when autonomy should be respected. We begin with a "dual standpoint'' theory of action that regards actions as grounded in reasons and therefore as having a conditional form that is suited to machine instructions. We then derive ethical principles based on formal properties that the reasons must exhibit to be coherent, and formulate the principles using quantified modal logic. We conclude that deontology not only provides a more satisfactory basis for machine ethics but endows the machine with an ability to explain its actions, thus contributing to transparency in AI.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {130–136},
numpages = {7},
keywords = {modal logic, machine ethics, accountable ai, deontology, autonomous machine ethics, kantian ai, artificial intelligence ethics, explainable ai},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278782,
author = {Hundman, Kyle and Gowda, Thamme and Kejriwal, Mayank and Boecking, Benedikt},
title = {Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278782},
doi = {10.1145/3278721.3278782},
abstract = {Web-based human trafficking activity has increased in recent years but it remains sparsely dispersed among escort advertisements and difficult to identify due to its often-latent nature. The use of intelligent systems to detect trafficking can thus have a direct impact on investigative resource allocation and decision-making, and, more broadly, help curb a widespread social problem. Trafficking detection involves assigning a normalized score to a set of escort advertisements crawled from the Web -- a higher score indicates a greater risk of trafficking-related (involuntary) activities. In this paper, we define and study the problem of trafficking detection and present a trafficking detection pipeline architecture developed over three years of research within the DARPA Memex program. Drawing on multi-institutional data, systems, and experiences collected during this time, we also conduct post hoc bias analyses and present a bias mitigation plan. Our findings show that, while automatic trafficking detection is an important application of AI for social good, it also provides cautionary lessons for deploying predictive machine learning algorithms without appropriate de-biasing. This ultimately led to integration of an interpretable solution into a search system that contains over 100 million advertisements and is used by over 200 law enforcement agencies to investigate leads.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {137–143},
numpages = {7},
keywords = {human trafficking, bias mitigation, web crawling, clustering, text classification},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278776,
author = {Iyer, Rahul and Li, Yuezhang and Li, Huao and Lewis, Michael and Sundar, Ramitha and Sycara, Katia},
title = {Transparency and Explanation in Deep Reinforcement Learning Neural Networks},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278776},
doi = {10.1145/3278721.3278776},
abstract = {Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of "object saliency maps", to provide visualization of internal states of DRLNs, thus enabling the formation of explanations and (c) can be incorporated in any existing deep reinforcement learning framework. We present computational results and human experiments to evaluate our approach.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {144–150},
numpages = {7},
keywords = {system transparency, human-ai interaction, explainable ai, human factors, deep reinforcement learning},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278772,
author = {Johnson, Collin and Kuipers, Benjamin},
title = {Socially-Aware Navigation Using Topological Maps and Social Norm Learning},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278772},
doi = {10.1145/3278721.3278772},
abstract = {We present socially-aware navigation for an intelligent robot wheelchair in an environment with many pedestrians. The robot learns social norms by observing the behaviors of human pedestrians, interpreting detected biases as social norms, and incorporating those norms into its motion planning. We compare our socially-aware motion planner with a baseline motion planner that produces safe, collision-free motion.The ability of our robot to learn generalizable social norms depends on our use of a topological map abstraction, so that a practical number of observations can allow learning of a social norm applicable in a wide variety of circumstances.We show that the robot can detect biases in observed human behavior that support learning the social norm of driving on the right. Furthermore, we show that when the robot follows these social norms, its behavior influences the behavior of pedestrians around it, increasing their adherence to the same norms. We conjecture that the legibility of the robot's normative behavior improves human pedestrians' ability to predict the robot's future behavior, making them more likely to follow the same norm.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {151–157},
numpages = {7},
keywords = {socially-aware navigation, social norms, robot motion planning, toplogical maps},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278764,
author = {Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Neel, Seth and Roth, Aaron},
title = {Meritocratic Fairness for Infinite and Contextual Bandits},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278764},
doi = {10.1145/3278721.3278764},
abstract = {We study fairness in linear bandit problems. Starting from the notion of meritocratic fairness introduced in~citeJKMR16, we carry out a more refined analysis of a more general problem, achieving better performance guarantees with fewer modelling assumptions on the number and structure of available choices as well as the number selected. We also analyze the previously-unstudied question of fairness in infinite linear bandit problems, obtaining instance-dependent regret upper bounds as well as lower bounds demonstrating that this instance-dependence is necessary. The result is a framework for meritocratic fairness in an online linear setting that is substantially more powerful, general, and realistic than the current state of the art.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {158–163},
numpages = {6},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278738,
author = {Kalyanakrishnan, Shivaram and Panicker, Rahul Alex and Natarajan, Sarayu and Rao, Shreya},
title = {Opportunities and Challenges for Artificial Intelligence in India},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278738},
doi = {10.1145/3278721.3278738},
abstract = {In the future of India lies the future of a sixth of the world's population. As the Artificial Intelligence (AI) revolution sweeps through societies and enters daily life, its role in shaping India's development and growth is bound to be substantial. For India, AI holds promise as a catalyst to accelerate progress, while providing mechanisms to leapfrog traditional hurdles such as poor infrastructure and bureaucracy. At the same time, an investment in AI is accompanied by risk factors with long-term implications on society: it is imperative that risks be vetted at this early stage. In this paper, we describe opportunities and challenges for AI in India. We detail opportunities that are cross-cutting (bridging India's linguistic divisions, mining public data), and also specific to one particular sector (healthcare). We list challenges that originate from existing social conditions (such as equations of caste and gender). Thereafter we distill out concrete steps and safeguards, which we believe are necessary for robust and inclusive development as India enters the AI era.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {164–170},
numpages = {7},
keywords = {artificial intelligence, india},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278781,
author = {Karbasian, Habib and Purohit, Hemant and Handa, Rajat and Malik, Aqdas and Johri, Aditya},
title = {Real-Time Inference of User Types to Assist with More Inclusive and Diverse Social Media Activism Campaigns},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278781},
doi = {10.1145/3278721.3278781},
abstract = {Social media provides a mechanism for people to engage with social causes across a range of issues. It also provides a strategic tool to those looking to advance a cause to exchange, promote or publicize their ideas. In such instances, AI can be either an asset if used appropriately or a barrier. One of the key issues for a workforce diversity campaign is to understand in real-time who is participating - specifically, whether the participants are individuals or organizations, and in case of individuals, whether they are male or female. In this paper, we present a study to demonstrate a case for AI for social good that develops a model to infer in real-time the different user types participating in a cause-driven hashtag campaign on Twitter, ILookLikeAnEngineer (ILLAE). A generic framework is devised to classify a Twitter user into three classes: organization, male and female in a real-time manner. The framework is tested against two datasets (ILLAE and a general dataset) and outperforms the baseline binary classifiers for categorizing organization/individual and male/female. The proposed model can be applied to future social cause-driven campaigns to get real-time insights on the macro-level social behavior of participants.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {171–177},
numpages = {7},
keywords = {multi-class classification, twitter, feature engineering, real-time user-type classification},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278775,
author = {Kasenberg, Daniel and Scheutz, Matthias},
title = {Inverse Norm Conflict Resolution},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278775},
doi = {10.1145/3278721.3278775},
abstract = {In previous work we provided a "norm conflict resolution" algorithm allowing agents in stochastic domains (represented by Markov Decision Processes) to "maximally satisfy" a set of moral or social norms, where such norms are represented by statements in linear temporal logic (LTL). This required the agent designer to provide weights specifying the relative importance of each norm. In this paper, we propose an "inverse norm conflict resolution'' algorithm for learning these weights from demonstration. This approach minimizes a cost function based on the relative entropy between a policy encoding the observed behavior and a policy representing optimal norm-following behavior. We demonstrate the effectiveness of the algorithm in a simple GridWorld domain.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {178–183},
numpages = {6},
keywords = {value alignment, norm conflict resolution, linear temporal logic, learning from demonstration},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278774,
author = {Kasenberg, Daniel and Arnold, Thomas and Scheutz, Matthias},
title = {Norms, Rewards, and the Intentional Stance: Comparing Machine Learning Approaches to Ethical Training},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278774},
doi = {10.1145/3278721.3278774},
abstract = {The challenge of training AI systems to perform responsibly and beneficially has inspired different approaches for teaching a system what people want and how it is acceptable to attain that in the world. In this paper we compare work in reinforcement learning, in particular inverse reinforcement learning, with our norm inference approach. We test those two systems and present results. Using the idea of the "intentional stance", we explain how a norm inference approach can work even when another agent is acting strictly according to reward functions. In this way norm inference presents itself as a promising, more explicitly accountable approach with which to design AI systems from the start.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {184–190},
numpages = {7},
keywords = {intentional stance, norm inference, value alignment},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278748,
author = {Kaul, Shiva},
title = {Margins and Opportunity},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278748},
doi = {10.1145/3278721.3278748},
abstract = {We use the statistical quantity of margin --- the distance between a decision boundary and a classified point, or the gap between two scores --- to formalize the principle of equal opportunity --- the chance to improve one's outcome, regardless of group status. This leads to a better definition of opportunity which recognizes, for example, that a strongly rejected individual was offered less recourse than a weakly rejected one, despite the shared outcome. It also leads to simpler algorithms, since real-valued margins are easier to analyze and optimize than discrete outcomes. We formalize two ways that a protected group may be guaranteed equal opportunity: (1) (social) mobility: acceptance should be within reach for the group (conversely, the general population shouldn't be cushioned from rejection), and (2) contrast: within the group, good candidates should get substantially higher scores than bad candidates, preventing the so-called 'token' effect. A simple linear classifier seems to offer roughly equal opportunity both experimentally and mathematically.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {191–196},
numpages = {6},
keywords = {fairness, classification, margins},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278770,
author = {Kim, Richard and Kleiman-Weiner, Max and Abeliuk, Andr\'{e}s and Awad, Edmond and Dsouza, Sohan and Tenenbaum, Joshua B. and Rahwan, Iyad},
title = {A Computational Model of Commonsense Moral Decision Making},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278770},
doi = {10.1145/3278721.3278770},
abstract = {We introduce a computational model for building moral autonomous vehicles by learning and generalizing from human moral judgments. We draw on a cognitively inspired model of how people and young children learn moral theories from sparse and noisy data and integrate observations made from different people in different groups. The problem of moral learning for autonomous vehicles is cast as learning how to weigh the different features of the dilemma using utility calculus, with the goal of making these trade-offs reflect how people make them in a wide variety of moral dilemma. By modeling the structures of individuals and groups in a hierarchical Bayesian model, we show that an individual's moral values -- as well as a group's shared values -- can be inferred from sparse and noisy data. We evaluate our approach with data from the Moral Machine, a web application that collects human judgments on moral dilemmas involving autonomous vehicles, and show that the model rapidly and accurately infers people's preferences and can predict the difficulty of moral dilemmas from limited data.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {197–203},
numpages = {7},
keywords = {machine ethics, artificial intelligence, moral learning, bayesian inference},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278752,
author = {Kramer, Max F. and Schaich Borg, Jana and Conitzer, Vincent and Sinnott-Armstrong, Walter},
title = {When Do People Want AI to Make Decisions?},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278752},
doi = {10.1145/3278721.3278752},
abstract = {AI systems are now or will soon be sophisticated enough to make consequential decisions. Although this technology has flourished, we also need public appraisals of AI systems playing these more important roles. This article reports surveys of preferences for and against AI systems making decisions in various domains as well as experiments that intervene on these preferences. We find that these preferences are contingent on subjects' previous exposure to computer systems making these kinds of decisions, and some interventions designed to mimic previous exposure successfully encourage subjects to be more hospitable to computer systems making these weighty decisions.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {204–209},
numpages = {6},
keywords = {survey, decision-making, kidney exchange, intervention, folk psychology, preferences},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278771,
author = {LaRosa, Emily and Danks, David},
title = {Impacts on Trust of Healthcare AI},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278771},
doi = {10.1145/3278721.3278771},
abstract = {Artificial Intelligence and robotics are rapidly moving into healthcare, playing key roles in specific medical functions, including diagnosis and clinical treatment. Much of the focus in the technology development has been on human-machine interactions, leading to a host of related technology-centric questions. In this paper, we focus instead on the impact of these technologies on human-human interactions and relationships within the healthcare domain. In particular, we argue that trust plays a central role for relationships in the healthcare domain, and the introduction of healthcare AI can potentially have significant impacts on those relations of trust. We contend that healthcare AI systems ought to be treated as assistive technologies that go beyond the usual functions of medical devices. As a result, we need to rethink regulation of healthcare AI systems to ensure they advance relevant values. We propose three distinct guidelines that can be universalized across federal regulatory boards to ensure that patient-doctor trust is not detrimentally affected by the deployment and widespread adoption of healthcare AI technologies.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {210–215},
numpages = {6},
keywords = {trust, regulatory framework, ethics, healthcare ai},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278763,
author = {London, Alex John and Danks, David},
title = {Regulating Autonomous Vehicles: A Policy Proposal},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278763},
doi = {10.1145/3278721.3278763},
abstract = {The widespread deployment and testing of autonomous vehicles in real-world environments raises key questions about how such systems should be regulated. Much of the current debate presupposes that the regulatory system we currently use for regular vehicles is also appropriate for semi- and fully-autonomous ones. In opposition, we first argue that there are serious challenges to regulating autonomous vehicles using current approaches, due to the nature of both autonomous capabilities (and their connections to operational domains), and also the systems' tasks and surrounding uncertainties. Instead, we argue that vehicles with autonomous capabilities are similar in key respects to drugs and other medical inter-ventions. Thus, we propose (on a "first principles" basis) a dynamic regulatory system with staged approvals and monitoring, analogous to the system used by the U.S. Food &amp; Drug Administration. We provide details about the operation of such a potential system, and conclude by characterizing its benefits, costs, and plausibility.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {216–221},
numpages = {6},
keywords = {context, regulation, autonomous systems, ethics},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278723,
author = {Loreggia, Andrea and Mattei, Nicholas and Rossi, Francesca and Venable, K. Brent},
title = {Preferences and Ethical Principles in Decision Making},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278723},
doi = {10.1145/3278721.3278723},
abstract = {If we want people to trust AI systems, we need to provide the systems we create with the ability to discriminate between what humans would consider good and bad decisions. The quality of a decision should not be based only on the preferences or optimization criteria of the decision makers, but also on other properties related to the impact of the decision, such as whether it is ethical, or if it complies to constraints and priorities given by feasibility constraints or safety regulations. The CP-net formalism [2] is a convenient and expressive way to model preferences, providing an effective compact way to qualitatively model preferences over outcomes, i.e., decisions, with a combinatorial structure [3, 7]. If we wish to incorporate ethical, moral, or norms based constraints to a decision context, it means that the subjective preferences of the decision makers are not the only source of information we should consider [1, 8]. Indeed, depending on the context, we may have to consider specific ethical principles derived from an appropriate ethical theory or various laws and norms. While preferences are important, when preferences and ethical principles are in conflict, the principles should override the subjective preferences of the decision maker. Therefore, it is essential to have well founded techniques to evaluate whether preferences are compatible with a set of ethical principles, and to measure how much these preferences deviate from the ethical principles.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {222},
numpages = {1},
keywords = {distance function, preferences, ethical principles, decision making},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278766,
author = {Maas, Matthijs M.},
title = {Regulating for 'Normal AI Accidents': Operational Lessons for the Responsible Governance of Artificial Intelligence Deployment},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278766},
doi = {10.1145/3278721.3278766},
abstract = {New technologies, particularly those which are deployed rapidly across sectors, or which have to operate in competitive conditions, can disrupt previously stable technology governance regimes. This leads to a precarious need to balance caution against performance while exploring the resulting 'safe operating space'. This paper will argue that Artificial Intelligence is one such critical technology, the responsible deployment of which is likely to prove especially complex, because even narrow AI applications often involve networked (tightly coupled, opaque) systems operating in complex or competitive environments. This ensures such systems are prone to 'normal accident'-type failures which can cascade rapidly, and are hard to contain or even detect in time. Legal and governance approaches to the deployment of AI will have to reckon with the specific causes and features of such 'normal accidents'. While this suggests that large-scale, cascading errors in AI systems are inevitable, an examination of the operational features that lead technologies to exhibit 'normal accidents' enables us to derive both tentative principles for precautionary policymaking, and practical recommendations for the safe(r) deployment of AI systems. This may help enhance the safety and security of these systems in the public sphere, both in the short- and in the long term.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {223–228},
numpages = {6},
keywords = {trust and explanations in ai systems, normal accident theory, ethical design and development of ai systems, ai and law},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278773,
author = {Manikonda, Lydia and Deotale, Aditya and Kambhampati, Subbarao},
title = {What's up with Privacy? User Preferences and Privacy Concerns in Intelligent Personal Assistants},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278773},
doi = {10.1145/3278721.3278773},
abstract = {The recent breakthroughs in Artificial Intelligence (AI) have allowed individuals to rely on automated systems for a variety of reasons. Some of these systems are the currently popular voice-enabled systems like Echo by Amazon and Home by Google that are also called as Intelligent Personal Assistants (IPAs). Though there are rising concerns about privacy and ethical implications, users of these IPAs seem to continue using these systems. We aim to investigate to what extent users are concerned about privacy and how they are handling these concerns while using the IPAs. By utilizing the reviews posted online along with the responses to a survey, this paper provides a set of insights about the detected markers related to user interests and privacy challenges. The insights suggest that users of these systems irrespective of their concerns about privacy, are generally positive in terms of utilizing IPAs in their everyday lives. However, there is a significant percentage of users who are concerned about privacy and take further actions to address related concerns. Some percentage of users expressed that they do not have any privacy concerns but when they learned about the "always listening" feature of these devices, their concern about privacy increased.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {229–235},
numpages = {7},
keywords = {user studies, privacy, user reviews, intelligent personal assistants, automated systems},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278749,
author = {Mattei, Nicholas and Saffidine, Abdallah and Walsh, Toby},
title = {Fairness in Deceased Organ Matching},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278749},
doi = {10.1145/3278721.3278749},
abstract = {As algorithms are given responsibility to make decisions that impact our lives, there is increasing awareness of the need to ensure the fairness of these decisions. One of the first challenges then is to decide what fairness means in a particular context. We consider here fairness in deciding how to match organs donated by deceased donors to patients. Due to the increasing age of patients on the waiting list, and of organs being donated, the current "first come, first served'' mechanism used in Australia is under review to take account of age of patients and of organs. We consider how to revise the mechanism to take account of age fairly. We identify a number of different types of fairness, such as to patients, to regions and to blood types and consider how they can be achieved.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {236–242},
numpages = {7},
keywords = {ethical principles, decision making, preferences, kidney allocation},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278742,
author = {Raff, Edward and Sylvester, Jared and Mills, Steven},
title = {Fair Forests: Regularized Tree Induction to Minimize Model Bias},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278742},
doi = {10.1145/3278721.3278742},
abstract = {The potential lack of fairness in the outputs of machine learning algorithms has recently gained attention both within the research community as well as in society more broadly. Surprisingly, there is no prior work developing tree-induction algorithms for building fair decision trees or fair random forests. These methods have widespread popularity as they are one of the few to be simultaneously interpretable, non-linear, and easy-to-use. In this paper we develop, to our knowledge, the first technique for the induction of fair decision trees.We show that our "Fair Forest" retains the benefits of the tree-based approach, while providing both greater accuracy and fairness than other alternatives, for both "group fairness'' and "individual fairness.'' We also introduce new measures for fairness which are able to handle multinomial and continues attributes as well as regression problems, as opposed to binary attributes and labels only. Finally, we demonstrate a new, more robust evaluation procedure for algorithms that considers the dataset in its entirety rather than only a specific protected attribute.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {243–250},
numpages = {8},
keywords = {feature importance, random forest, fairness},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278743,
author = {Scheessele, Michael R.},
title = {A Framework for Grounding the Moral Status of Intelligent Machines},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278743},
doi = {10.1145/3278721.3278743},
abstract = {I propose a framework, derived from moral theory, for assessing the moral status of intelligent machines. Using this framework, I claim that some current and foreseeable intelligent machines have approximately as much moral status as plants, trees, and other environmental entities. This claim raises the question: what obligations could a moral agent (e.g., a normal adult human) have toward an intelligent machine? I propose that the threshold for any moral obligation should be the "functional morality" of Wallach and Allen [20], while the upper limit of our obligations should not exceed the upper limit of our obligations toward plants, trees, and other environmental entities.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {251–256},
numpages = {6},
keywords = {Machine Ethics},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278758,
author = {Seo, Sungyong and Chan, Hau and Brantingham, P. Jeffrey and Leap, Jorja and Vayanos, Phebe and Tambe, Milind and Liu, Yan},
title = {Partially Generative Neural Networks for Gang Crime Classification with Partial Information},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278758},
doi = {10.1145/3278721.3278758},
abstract = {More than 1 million homicides, robberies, and aggravated assaults occur in the United States each year. These crimes are often further classified into different types based on the circumstances surrounding the crime (e.g., domestic violence, gang-related). Despite recent technological advances in AI and machine learning, these additional classification tasks are still done manually by specially trained police officers. In this paper, we provide the first attempt to develop a more automatic system for classifying crimes. In particular, we study the question of classifying whether a given violent crime is gang-related. We introduce a novel Partially Generative Neural Networks (PGNN) that is able to accurately classify gang-related crimes both when full information is available and when there is only partial information. Our PGNN is the first generative-classification model that enables to work when some features of the test examples are missing. Using a crime event dataset from Los Angeles covering 2014-2016, we experimentally show that our PGNN outperforms all other typically used classifiers for the problem of classifying gang-related violent crimes.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {257–263},
numpages = {7},
keywords = {generative model, gang crime classification},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278735,
author = {Serramia, Marc and Lopez-Sanchez, Maite and Rodriguez-Aguilar, Juan A. and Morales, Javier and Wooldridge, Michael and Ansotegui, Carlos},
title = {Exploiting Moral Values to Choose the Right Norms},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278735},
doi = {10.1145/3278721.3278735},
abstract = {Norms constitute regulative mechanisms extensively enacted in groups, organisations, and societies. However, 'choosing the right norms to establish' constitutes an open problem that requires the consideration of a number of constraints (such as norm relations) and preference criteria (e.g over involved moral values). This paper advances the state of the art in the Normative Multiagent Systems literature by formally defining this problem and by proposing its encoding as a linear program so that it can be automatically solved.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {264–270},
numpages = {7},
keywords = {value alignment, optimisation, norms, moral values, ethics},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278728,
author = {Shaw, Nolan P. and St\"{o}ckel, Andreas and Orr, Ryan W. and Lidbetter, Thomas F. and Cohen, Robin},
title = {Towards Provably Moral AI Agents in Bottom-up Learning Frameworks},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278728},
doi = {10.1145/3278721.3278728},
abstract = {We examine moral machine decision making as inspired by a central question posed by Rossi with respect to moral preferences: can AI systems based on statistical machine learning (which do not provide a natural way to explain or justify their decisions) be used for embedding morality into a machine in a way that allows us to prove that nothing morally wrong will happen? We argue for an evaluation which is held to the same standards as a human agent, removing the demand that ethical behaviour is always achieved. We introduce four key meta-qualities desired for our moral standards, and then proceed to clarify how we can prove that an agent will correctly learn to perform moral actions given a set of samples within certain error bounds. Our group-dynamic approach enables us to demonstrate that the learned models converge to a common function to achieve stability. We further explain a valuable intrinsic consistency check made possible through the derivation of logical statements from the machine learning model. In all, this work proposes an approach for building ethical AI systems, coming from the perspective of artificial intelligence research, and sheds important light on understanding how much learning is required in order for an intelligent agent to behave morally with negligible error.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {271–277},
numpages = {7},
keywords = {multi-agent systems, provably moral ai, statistical machine learning},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278754,
author = {Somaya, Deepak and Varshney, Lav R.},
title = {Embodiment, Anthropomorphism, and Intellectual Property Rights for AI Creations},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278754},
doi = {10.1145/3278721.3278754},
abstract = {Computational creativity is an emerging branch of artificial intelligence (AI) concerned with algorithms that can create novel and high-quality ideas or artifacts, either autonomously or semi-autonomously in collaboration with people. Quite simply, such algorithms may be described as artificial innovation engines. These technologies raise questions of authorship/inventorship and of agency, which become further muddled by the social context induced by AI that may be physically-embodied or anthropomorphized. These questions are fundamentally intertwined with the provision of appropriate incentives for conducting and commercializing computational creativity research through intellectual property regimes. This paper reviews current understanding of intellectual property rights for AI, and explores possible framings for intellectual property policy in social context.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {278–283},
numpages = {6},
keywords = {human-computer interaction, intellectual property rights, computational creativity, artificial intelligence},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278744,
author = {Srivastava, Biplav and Rossi, Francesca},
title = {Towards Composable Bias Rating of AI Services},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278744},
doi = {10.1145/3278721.3278744},
abstract = {A new wave of decision-support systems are being built today using AI services that draw insights from data (like text and video) and incorporate them in human-in-the-loop assistance. However, just as we expect humans to be ethical, the same expectation needs to be met by automated systems that increasingly get delegated to act on their behalf. A very important aspect of an ethical behavior is to avoid (intended, perceived, or accidental) bias. Bias occurs when the data distribution is not representative enough of the natural phenomenon one wants to model and reason about. The possibly biased behavior of a service is hard to detect and handle if the AI service is merely being used and not developed from scratch, since the training data set is not available. In this situation, we envisage a 3rd party rating agency that is independent of the API producer or consumer and has its own set of biased and unbiased data, with customizable distributions. We propose a 2-step rating approach that generates bias ratings signifying whether the AI service is unbiased compensating, data-sensitive biased, or biased. The approach also works on composite services. We implement it in the context of text translation and report interesting results.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {284–289},
numpages = {6},
keywords = {composite services, rating, ai systems, bias},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278734,
author = {Su\'{a}rez-Serrato, Pablo and Vel\'{a}zquez Richards, Eduardo Iv\'{a}n and Yazdani, Mehrdad},
title = {Socialbots Supporting Human Rights},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278734},
doi = {10.1145/3278721.3278734},
abstract = {Socialbots, or non-human/algorithmic social media users, have recently been documented as competing for information dissemination and disruption on online social networks. Here we investigate the influence of socialbots in Mexican Twitter in regards to the "Tanhuato" human rights abuse report. We analyze the applicability of the BotOrNot API to generalize from English to Spanish tweets and propose adaptations for Spanish-speaking bot detection. We then use text and sentiment analysis to compare the differences between bot and human tweets. Our analysis shows that bots actually aided in information proliferation among human users. This suggests that taxonomies classifying bots should include non-adversarial roles as well. Our study contributes to the understanding of different behaviors and intentions of automated accounts observed in empirical online social network data. Since this type of analysis is seldom performed in languages different from English, the proposed techniques we employ here are also useful for other non-English corpora.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {290–296},
numpages = {7},
keywords = {social network analysis, mexico, socialbots, human rights, spanish},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278759,
author = {Sun, Fan-Yun and Chang, Yen-Yu and Wu, Yueh-Hua and Lin, Shou-De},
title = {Designing Non-Greedy Reinforcement Learning Agents with Diminishing Reward Shaping},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278759},
doi = {10.1145/3278721.3278759},
abstract = {This paper intends to address an issue in RL that when agents possessing varying capabilities, most resources may be acquired by stronger agents, leaving the weaker ones "starving". We introduce a simple method to train non-greedy agents in multi-agent reinforcement learning scenarios with nearly no extra cost. Our model can achieve the following goals in designing the non-greedy agent:non-homogeneous equality, only need local information, cost-effective, generalizable and configurable. We propose the idea of diminishing reward that makes the agent feel less satisfied for consecutive rewards obtained. This idea allows the agents to behave less greedy with-out the need to explicitly coding any ethical pattern nor monitor other agents' status. Given our framework, resources distributed more equally without running the risk of reaching homogeneous equality. We designed two games, Gathering Game and Hunter Prey to evaluate the quality of the model.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {297–302},
numpages = {6},
keywords = {reward shaping, multi-agent reinforcement learning, non-greedy},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278725,
author = {Tan, Sarah and Caruana, Rich and Hooker, Giles and Lou, Yin},
title = {Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278725},
doi = {10.1145/3278721.3278725},
abstract = {Black-box risk scoring models permeate our lives, yet are typically proprietary or opaque. We propose Distill-and-Compare, an approach to audit such models without probing the black-box model API or pre-defining features to audit. To gain insight into black-box models, we treat them as teachers, training transparent student models to mimic the risk scores assigned by the black-box models. We compare the mimic model trained with distillation to a second, un-distilled transparent model trained on ground truth outcomes, and use differences between the two models to gain insight into the black-box model. We demonstrate the approach on four data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club. We also propose a statistical test to determine if a data set is missing key features used to train the black-box model. Our test finds that the ProPublica data is likely missing key feature(s) used in COMPAS.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {303–310},
numpages = {8},
keywords = {black-box models, fairness, distillation, interpretability},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278737,
author = {Tobey, Daniel L.},
title = {Software Malpractice in the Age of AI: A Guide for the Wary Tech Company},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278737},
doi = {10.1145/3278721.3278737},
abstract = {Professional malpractice - the concept of heightened duties for those entrusted with special knowledge and crucial tasks - is rooted in history. And yet, since the dawn of the computer age, courts in the United States have almost universally rejected a theory of software malpractice, declining to hold software engineers to the same professional standards as doctors, lawyers, and engineers. What is changing, however, is the speed at which software based on artificial intelligence technologies is replacing the very professionals already subject to professional liability. Society has already decided (in some cases, millennia ago) that those tasks warrant special accountability; new to the analysis is which human is closest in line to the adverse event. As AI expands, the pressure for courts to go one level up the causal chain in search of human agency and professional accountability will mount. This essay analyzes the case law rejecting software malpractice for clues about where the doctrine might go in the age of AI, then discusses what technology companies can learn from the safety enhancements of doctors, lawyers, and other historic professionals who have adapted to such heightened legal scrutiny for years.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {311–316},
numpages = {6},
keywords = {liability, malpractice, software-malpractice, professional-responsibility, human-in-the-loop},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278726,
author = {Vanderelst, Dieter and Winfield, Alan},
title = {The Dark Side of Ethical Robots},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278726},
doi = {10.1145/3278721.3278726},
abstract = {Concerns over the risks associated with advances in Artificial Intelligence have prompted calls for greater efforts toward robust and beneficial AI, including machine ethics. Recently, roboticists have responded by initiating the development of so-called ethical robots. These robots would, ideally, evaluate the consequences of their actions and morally justify their choices. This emerging field promises to develop extensively over the next few years. However, in this paper, we point out an inherent limitation of the emerging field of ethical robots. We show that building ethical robots also inevitably enables the construction of unethical robots. In three experiments, we show that it is remarkably easy to modify an ethical robot so that it behaves competitively, or even aggressively. The reason for this is that the cognitive machinery required to make an ethical robot can always be corrupted to make unethical robots. We discuss the implications of this finding to the governance of ethical robots. We conclude that the risks that unscrupulous actors might compromise a robot's ethics are so great as to raise serious doubts over the wisdom of embedding ethical decision making in real-world safety-critical robots, such as driverless cars.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {317–322},
numpages = {6},
keywords = {cybersecurity, ethical robots, ethical governance, malicious use, machine ethics},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278751,
author = {Vasconcelos, Marisa and Cardonha, Carlos and Gon\c{c}alves, Bernardo},
title = {Modeling Epistemological Principles for Bias Mitigation in AI Systems: An Illustration in Hiring Decisions},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278751},
doi = {10.1145/3278721.3278751},
abstract = {Artificial Intelligence (AI) has been used extensively in automatic decision making in a broad variety of scenarios, ranging from credit ratings for loans to recommendations of movies. Traditional design guidelines for AI models focus essentially on accuracy maximization, but recent work has shown that economically irrational and socially unacceptable scenarios of discrimination and unfairness are likely to arise unless these issues are explicitly addressed. This undesirable behavior has several possible sources, such as biased datasets used for training that may not be detected in black-box models. After pointing out connections between such bias of AI and the problem of induction, we focus on Popper's contributions after Hume's, which offer a logical theory of preferences. An AI model can be preferred over others on purely rational grounds after one or more attempts at refutation based on accuracy and fairness. Inspired by such epistemological principles, this paper proposes a structured approach to mitigate discrimination and unfairness caused by bias in AI systems. In the proposed computational framework, models are selected and enhanced after attempts at refutation. To illustrate our discussion, we focus on hiring decision scenarios where an AI system filters in which job applicants should go to the interview phase.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {323–329},
numpages = {7},
keywords = {semi-automatic decision making, constrained models, bias of ai, problem of induction, hiring algorithms, attempts at refutation},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278768,
author = {Wagner, Alan R.},
title = {An Autonomous Architecture That Protects the Right to Privacy},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278768},
doi = {10.1145/3278721.3278768},
abstract = {The advent and widespread adoption of wearable cameras and autonomous robots raises important issues related to privacy. The mobile cameras on these systems record and may re-transmit enormous amounts of video data that can then be used to identify, track, and characterize the behavior of the general populous. This paper presents a preliminary computational architecture designed to preserve specific types of privacy over a video stream by identifying categories of individuals, places, and things that require higher than normal privacy protection. This paper describes the architecture as a whole as well as preliminary results testing aspects of the system. Our intention is to implement and test the system on ground robots and small UAVs and demonstrate that the system can provide selective low-level masking or deletion of data requiring higher privacy protection.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {330–334},
numpages = {5},
keywords = {privacy, perception, architecture},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278779,
author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
title = {Mitigating Unwanted Biases with Adversarial Learning},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278779},
doi = {10.1145/3278721.3278779},
abstract = {Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {335–340},
numpages = {6},
keywords = {debiasing, adversarial learning, multi-task learning, unbiasing},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278778,
author = {Zhao, Jianxin and Mortier, Richard and Crowcroft, Jon and Wang, Liang},
title = {Privacy-Preserving Machine Learning Based Data Analytics on Edge Devices},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278778},
doi = {10.1145/3278721.3278778},
abstract = {Emerging Machine Learning (ML) techniques, such as Deep Neural Network, are widely used in today's applications and services. However, with social awareness of privacy and personal data rapidly rising, it becomes a pressing and challenging societal issue to both keep personal data private and benefit from the data analytics power of ML techniques at the same time. In this paper, we argue that to avoid those costs, reduce latency in data processing, and minimise the raw data revealed to service providers, many future AI and ML services could be deployed on users' devices at the Internet edge rather than putting everything on the cloud. Moving ML-based data analytics from cloud to edge devices brings a series of challenges. We make three contributions in this paper. First, besides the widely discussed resource limitation on edge devices, we further identify two other challenges that are not yet recognised in existing literature: lack of suitable models for users, and difficulties in deploying services for users. Second, we present preliminary work of the first systematic solution, i.e. Zoo, to fully support the construction, composing, and deployment of ML models on edge and local devices. Third, in the deployment example, ML service are proved to be easy to compose and deploy with Zoo. Evaluation shows its superior performance compared with state-of-art deep learning platforms and Google ML services.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {341–346},
numpages = {6},
keywords = {machine learning, edge computing, privacy},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278755,
author = {Zhao, Yuanshuo and Baldini, Ioana and Sattigeri, Prasanna and Padhi, Inkit and Lee, Yoong Keok and Smith, Ethan},
title = {Data Driven Techniques for Organizing Scientific Articles Relevant to Biomimicry},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278755},
doi = {10.1145/3278721.3278755},
abstract = {Life on earth presents elegant solutions to many of the challenges innovators and entrepreneurs across disciplines face every day. To facilitate innovations inspired by nature, there is an emerging need for systems that bring relevant biological information to this application-oriented market. In this paper, we discuss our approach to assembling a system that uses machine learning techniques to assess a scientific article's potential usefulness to innovators, and classifies these articles in a way that helps innovators find information relevant to the challenges they are attempting to solve.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {347–353},
numpages = {7},
keywords = {biomimicry, machine learning},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278794,
author = {Abebe, Rediet},
title = {Computational Perspectives on Social Good and Access to Opportunity},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278794},
doi = {10.1145/3278721.3278794},
abstract = {Computational techniques show immense promise to both deepen our understanding of socioeconomic inequality and inform interventions to alleviate it. With the increased prevalence of collaborations across disciplines and the availability of large data-sets, there is a wealth of areas in which nuanced questions and novel techniques can reveal powerful observations and point towards innovative solutions. In this piece, we highlight ways for using algorithmic, computational, and network-based techniques, in conjunction with insights from the social sciences, to improve access to opportunity for historically disadvantaged and under-served communities. We underline opportunities for work at the interface of these disciplines using examples from health, housing, and economic inequality.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {354–355},
numpages = {2},
keywords = {social networks, mechanism design for social good, algorithms, computational social science, artificial intelligence},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278795,
author = {Bansal, Gagan},
title = {Explanatory Dialogs: Towards Actionable, Interactive Explanations},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278795},
doi = {10.1145/3278721.3278795},
abstract = {Adoption of AI systems in high-stakes domains (e.g., transportation, law, and healthcare) demands that human users trust these systems. A desiderata for establishing trust is that the users understand the system's decision process. However, a high-performing system may use a complex decision process, which may not be interpretable by itself. We argue that existing solutions for generating interpretable explanations have limitations and as a solution, propose developing new explanation systems that enable interactive and actionable dialogs between the user and the system.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {356–357},
numpages = {2},
keywords = {Explainable AI},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278793,
author = {Blass, Joseph A.},
title = {Legal, Ethical, Customizable Artificial Intelligence},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278793},
doi = {10.1145/3278721.3278793},
abstract = {To be effective, useful, safe, and legal, AI must obey the laws of its users' societies and (where legal) its users' ethical intuitions. But laws and ethics can be difficult for people to express. My research involves ethical and legal instruction by example: synthesizing cases, applying synthesized principles, and explaining those applications.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {358–359},
numpages = {2},
keywords = {legal reasoning, analogical reasoning, cognitive modeling},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278800,
author = {Carley, Cassandra},
title = {Balancing Privacy and Utility with Pattern Based Activity Detection: Extended Abstract},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278800},
doi = {10.1145/3278721.3278800},
abstract = {The diffusion of surveillance cameras often leads to conflicts between utility, that is, the benefits of preserving the information the camera records, and privacy, that is, the ability for the people being observed to conceal information they want to protect. For example, a camera monitoring an office kitchen may be useful in identifying a food thief, but might unintentionally reveal the PIN someone enters on a mobile phone. We design a video processing system that detects private activities in surveillance video and filters them out of the recording with minimal disruption of video quality. At the core of our system is the light-weight computation of a fixed-size feature that describes the spatio-temporal aspects of human activities that extend over variable amounts of time and space. Converting events of variable length and extent to a fixed-size descriptor makes it possible to use off-the-shelf classifiers to recognize and localize activities to be protected from recording. Comparisons of our descriptor with several alternatives show improved performance with less computation. We contribute two new video datasets recorded with a kitchen security camera, and we carry out a pilot user study to show that PIN theft is a valid concern.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {360–361},
numpages = {2},
keywords = {activity detection, video analysis, hand tracking},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278790,
author = {Chen, Gong},
title = {Nurturing the Companion ChatBot},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278790},
doi = {10.1145/3278721.3278790},
abstract = {Although recent technical progress of artificial intelligence is impressive, the affective effect of intelligent agents has not been investigated sufficiently. This research focuses on the affective effect of ChatBot. We observe an interesting phenomenon that not only the affective response from ChatBot influences user's experience, but also the manner that user interacts with ChatBot affects the development of the ChatBot's intelligence. So this work proposes an ethical issue that it is necessary to regularize the user's behavior. We validate this argument by designing a novel paradigm, which enables the users to nurture companion ChatBots via developmental artificial intelligence techniques. With only twenty days nurturing, the users build affective bonding with the ChatBots and the ChatBots show significant progress in communication skills.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {362–363},
numpages = {2},
keywords = {affective bonding, nurturing, companion chatbot},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278797,
author = {Eicher, Bobbie},
title = {Giving AI a Theory of Mind},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278797},
doi = {10.1145/3278721.3278797},
abstract = {Effective collaboration between humans and artificially intelligent agents will require that the two are equipped to build a sense of mutual understanding with each other. When humans have an intuitive understanding of the motives and intentions of other humans, it is known as Theory of Mind. My work revolves around designing artificial intelligence to leverage this capacity to improve human collaborations with artificial agents.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {364–365},
numpages = {2},
keywords = {education access, ethics of artificial intelligence, theory of mind, question-answering systems, online education, metacognition, virtual teaching assistant, turing test},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278801,
author = {Garvey, Colin},
title = {AI Risk Mitigation Through Democratic Governance: Introducing the 7-Dimensional AI Risk Horizon},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278801},
doi = {10.1145/3278721.3278801},
abstract = {My dissertation asks two fundamental questions: What are the risks of AI? And what should be done about them? My research goes beyond existential threats to humanity to consider seven dimensions of AI risk: military, political, economic, social, environmental, psychophysiological, and spiritual. I examine extant AI risk mitigation strategies and, finding them insufficient, use a democratic governance framework to propose alternatives. This paper outlines the project and introduces the risk dimensions.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {366–367},
numpages = {2},
keywords = {governance, decision making, social impact, existential risk, risk mitigation, ai risk, democracy},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278798,
author = {Hu, Lily},
title = {Justice Beyond Utility in Artificial Intelligence},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278798},
doi = {10.1145/3278721.3278798},
abstract = {The entry of Artificial Intelligence into prominent social and economic environments has brought to the fore concerns about the ethical nature of such agents and tools. Though it is well-known that methods based in utilitarian calculus often fail to account for moral considerations, few alternatives have been adopted in the field of AI. My work advocates for a new approach toward the interaction between AI methods and the social that centers principles of distributive justice. As AI increasingly drives consequential social decision-making, we must consider not only what can be done but what ought to be done. Grappling with the inherently normative nature of these problems requires an orientation towards AI that is able to conceptualize justice beyond utility.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {368–369},
numpages = {2},
keywords = {economics, machine learning, ethics in ai},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278791,
author = {Kasenberg, Daniel},
title = {Learning and Obeying Conflicting Norms in Stochastic Domains},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278791},
doi = {10.1145/3278721.3278791},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {370–371},
numpages = {2},
keywords = {norm conflict resolution, linear temporal logic, norm inference},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278796,
author = {Kaul, Shiva},
title = {Speed And Accuracy Are Not Enough! Trustworthy Machine Learning},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278796},
doi = {10.1145/3278721.3278796},
abstract = {Classical linear/shallow learning is relatively easy to analyze and understand, but the power of deep learning is often desirable. I am developing a hybrid approach in order to obtain learning algorithms that are both trustworthy and accurate. My research has mostly focused on learning from corrupted or inconsistent training data (`agnostic learning'). Recently, I, as well as independent researchers, have found these same techniques could help make algorithms more fair.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {372–373},
numpages = {2},
keywords = {agnostic learning, fairness, classification, machine learning},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278787,
author = {Lee, Barton E.},
title = {A Win for Society! Conquering Barriers to Fair Elections},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278787},
doi = {10.1145/3278721.3278787},
abstract = {Social choice is a general framework used in the aggregation of agent preferences to make a collective decision, political elections whereby agents vote is a common example. It is often the case that society demands electoral systems which ensure, or election outcomes which satisfy, socially desirable outcomes such as representing large minorities and avoiding the 'tyranny of the majority'. Unfortunately, there are many natural barriers which may prevent desirable outcomes from being achieved. These barriers include the non-existence or computational intractability of achieving desirable outcomes, especially when combined with additional feasibility constraints, and the effect of strategic or manipulative agents. This thesis aims to improve our understanding of the scale of these barriers and if, or how, they can be overcome to provide socially desirable outcomes.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {374–375},
numpages = {2},
keywords = {preference aggregation, voting, strategic agents, game theory, computational social choice},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278789,
author = {Levy, Priel},
title = {Optimal Contest Design for Multi-Agent Systems},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278789},
doi = {10.1145/3278721.3278789},
abstract = {Contests have become a highly popular crowdsourcing mechanism aiming to solicit effort of the crowd in solving well defined problems, and as such are extensively studied within the framework of contest design. In this paper, I describe preliminary work on a type of contest that has recently gained momentum, and give an overview of my PhD research proposal, carried out under the supervision of Prof. David Sarne.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {376–377},
numpages = {2},
keywords = {game theory, economics, contest design},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278792,
author = {Serramia, Marc},
title = {Ethics in Norm Decision Making},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278792},
doi = {10.1145/3278721.3278792},
abstract = {Norms are an instrument to coordinate societies, but deciding which norms to enact is a difficult task. Not only norms might have incom- patibilities between themselves, such as norms contradicting other norms, but also the cost of implementation can be an important aspect to consider. Furthermore, due to the growing social inter- est in ethics and the ethical impact norms can have, this ethical implications should also be examined during the decision making process.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {378–379},
numpages = {2},
keywords = {ethics, optimisation, moral values, value alignment, norms},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278788,
author = {Strobel, Martin},
title = {An Axiomatic Approach to Explain Computer Generated Decisions: Extended Abstract},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278788},
doi = {10.1145/3278721.3278788},
abstract = {Recent years have seen the widespread implementation of data-driven algorithms making decisions in increasingly highstakes domains, such as finance, healthcare, transportation and public safety. Using novel ML techniques, these algorithms are able to process massive amounts of data and make highly accurate predictions; however, their inherent complexity makes it increasingly difficult for humans to understand why certain decisions were made. Indeed, these algorithms are black-box decision makers: their underlying decision processes are either hidden from human scrutiny by proprietary law, or (as is often the case) their inner workings are so complicated that even their own designers will be hard-pressed to explain the underlying reasoning behind their decision making processes. By obfuscating their function, data-driven classifiers run the risk of exposing human stakeholders to risks. These may include incorrect decisions (e.g. a loan application that was wrongly rejected due to system error), information leaks (e.g. an algorithm inadvertently uses information it should not have used), or discrimination (e.g. biased decisions against certain ethnic or gender groups). Government bodies and regulatory authorities have recently begun calling for algorithmic transparency: providing human-interpretable explanations of the underlying reasoning behind large-scale decision making algorithms. My thesis research will be concerned with an axiomatic analysis of automatically generated explanations of such classifiers. Especially, I'm interested in how to decide which explanation of a decision to trust given that there are many, potentially conflicting, possible explanations for any given decision.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {380–381},
numpages = {2},
keywords = {explainable machine learning, axiomatic approach},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278802,
author = {Tan, Sarah},
title = {Interpretable Approaches to Detect Bias in Black-Box Models},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278802},
doi = {10.1145/3278721.3278802},
abstract = {My dissertation research is grounded in the field of interpretability. I aim to develop methods to explain and interpret predictions from black-box machine learning models to help creators, as well as users, of machine learning models increase their trust and understanding of the models. In this doctoral consortium paper, I summarize my previous and current research projects in interpretability, and describe my future plans for research in this area.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {382–383},
numpages = {2},
keywords = {model distillation, algorithmic fairness, black-box models, interpretability, transparency},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278783,
author = {Tedeschi, Stefano},
title = {Accountable Agents and Where to Find Them},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278783},
doi = {10.1145/3278721.3278783},
abstract = {The aim of my PhD is to investigate the notion of computational accountability relying on approaches from the research on multi-agent systems. The main contribution will be to provide a notion of when an organization supports accountability, by exploring the process of construction of the organization itself, and to guarantee accountability as a design property.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {384–385},
numpages = {2},
keywords = {social commitments, multi-agent systems, computational accountability},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278784,
author = {Thelisson, Eva},
title = {Towards a Computational Sustainability for AI/ML to Foster Responsibility},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278784},
doi = {10.1145/3278721.3278784},
abstract = {This paper proposes to develop a new field of research designated as computational sustainability. It takes into account legal and ethical considerations of Artificial Intelligence (AI) and Machine Learning (ML) Technologies. As AI and ML will deeply impact the society within the next decade, this paper raises the awareness that technology is not value neutral and that technologists shall take responsibility for the ethical and social impact of their work. In particular, this paper aims at considering the last AI and ML developments and its convergence with associated technologies like Nano-technology, Biotechnology, Information Technology, Cognitive Science (NBIC). The challenge is to reflect on the finalities of AI / ML technologies, while referring to the Philosophy, Ethical Theory, Ethical Principles and Soft Law Mechanisms. Those Mechanisms refer to rules that are not strictly binding in nature (like guidelines or codes of conduct which set standards of conduct). National competent authorities may encourage their development, rewarding their implementation or making them enforceable. AI Codes of Conducts and Quality Labels may play a key role in developing computational sustainability for AI / ML Technologies, in parallel to the development of Hard Law Mechanisms based for example on an International Convention on Civil Liability for Algorithmic Damages or a Digital Geneva Convention.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {386–387},
numpages = {2},
keywords = {code of ethics, artificial intelligence, responsible ai, law and ethics, machine learning, computational sustainability},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278785,
author = {Verdiesen, Ilse},
title = {The Design of Human Oversight in Autonomous Weapon Systems},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278785},
doi = {10.1145/3278721.3278785},
abstract = {As the reach and capabilities of Artificial Intelligence (AI) systems increases, there is also a growing awareness of the ethical, legal and societal impact of the potential actions and decisions of these systems. Many are calling for guidelines and regulations that can ensure the responsible design, development, implementation, and policy of AI. In scientific literature, AI is characterized by the concepts of Adaptability, Interactivity and Autonomy (Floridi &amp; Sanders, 2004). According to Floridi and Sanders (2004), Adaptability means that the system can change based on its interaction and can learn from its experience. Machine learning techniques are an example of this. Interactivity occurs when the system and its environment act upon each other and Autonomy implies that the system itself can change its state.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {388–389},
numpages = {2},
keywords = {autonomous weapons systems, ethical decision-making, human oversight, moral judgement},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3278721.3278786,
author = {Xu, Jin},
title = {Overtrust of Robots in High-Risk Scenarios},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278786},
doi = {10.1145/3278721.3278786},
abstract = {From personal robot assistant to self-driving vehicles, artificial intelligence (AI) is the backbone underlying millions of future advanced applications. As robots become increasingly pervasive in daily life, it is expected that robots will augment human laborers in many domains in the near future. When robots are deployed in the real world, the underlying assumption is that they are capable of accomplishing their given tasks. However, researchers have shown that robots made mistakes, and in several cases, humans tend to overtrust robotic systems (Abney 2017; Borenstein et al. 2017; Robinette, Howard, and Wagner 2017). Overtrust of a robot happens in scenarios where (1) a person accepts risk because that person believes the robot can perform a function that it cannot or (2) the person accepts too much risk because the expectation is that the system will mitigate the risk." (Abney 2017). In particular, we are interested in two emerging domains where an appropriate amount of trust is a minimal requirement and overtrust could cause harm: 1) healthcare scenarios and 2) self-driving car (i.e. autonomous driving) scenarios. Both healthcare and autonomous driving scenarios often involve high risks, and the negative outcomes could be detrimental to the user. The objective of our research focuses on 1) investigating the causes that contribute to human overtrust of these robots systems 2) developing a behavior-based computational model to predict overtrust, and 3) developing techniques to mitigate outcomes caused by the overtrust.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {390–391},
numpages = {2},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

