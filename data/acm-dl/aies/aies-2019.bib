@inproceedings{10.1145/3306618.3314225,
author = {Calo, Ryan},
title = {How We Talk About AI (and Why It Matters)},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314225},
doi = {10.1145/3306618.3314225},
abstract = {How we talk about artificial intelligence matters. Not only do our rhetorical choices influence public expectations of AI, they implicitly make the case for or against specific government interventions. Conceiving of AI as a global project to which each nation can contribute, for instance, suggests a different course of action than understanding AI as a "race" America cannot afford to lose. And just as inflammatory terms such as "killer robot" aim to catalyze limitations of autonomous weapons, so do the popular terms "ethics" and "governance" subtly argue for a lesser role for government in setting AI policy. How should we talk about AI? And what's at stake with our rhetorical choices? This presentation explores the interplay between claims about AI and law's capacity to channel AI in the public interest.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1},
numpages = {1},
keywords = {policy, artificial intelligence, rhetoric},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314261,
author = {Wright, Ava Thomas},
title = {Rightful Machines and Dilemmas},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314261},
doi = {10.1145/3306618.3314261},
abstract = {Tn this paper I set out a new Kantian approach to resolving conflicts and dilemmas of obligation for semi-autonomous machine agents such as self-driving cars. First, I argue that efforts to build explicitly moral machine agents should focus on what Kant refers to as duties of right, or justice, rather than on duties of virtue, or ethics. In a society where everyone is morally equal, no one individual or group has the normative authority to unilaterally decide how moral conflicts should be resolved for everyone. Only public institutions to which everyone could consent have the authority to define, enforce, and adjudicate our rights and obligations with respect to one other. Then, I show how the shift from ethics to a standard of justice resolves the conflict of obligations in what is known as the "trolley problem" for rightful machine agents. Finally, I consider how a deontic logic suitable for governing explicitly rightful machines might meet the normative requirements of justice.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {3–4},
numpages = {2},
keywords = {law, machine agents, belief revision, machine ethics, dilemmas, priority of right, deontic logic, conflicts, trolley problem, rightful machines, non-monotonic logic, kant, autonomous vehicles},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314265,
author = {Han, The Anh and Pereira, Lu\'{\i}s Moniz and Lenaerts, Tom},
title = {Modelling and Influencing the AI Bidding War: A Research Agenda},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314265},
doi = {10.1145/3306618.3314265},
abstract = {A race for technological supremacy in AI could lead to serious negative consequences, especially whenever ethical and safety procedures are underestimated or even ignored, leading potentially to the rejection of AI in general. For all to enjoy the benefits provided by safe, ethical and trustworthy AI systems, it is crucial to incentivise participants with appropriate strategies that ensure mutually beneficial normative behaviour and safety-compliance from all parties involved. Little attention has been given to understanding the dynamics and emergent behaviours arising from this AI bidding war, and moreover, how to influence it to achieve certain desirable outcomes (e.g. AI for public good and participant compliance). To bridge this gap, this paper proposes a research agenda to develop theoretical models that capture key factors of the AI race, revealing which strategic behaviours may emerge and hypothetical scenarios therein. Strategies from incentive and agreement modelling are directly applicable to systematically analyse how different types of incentives (namely, positive vs. negative, peer vs. institutional, and their combinations) influence safety-compliant behaviours over time, and how such behaviours should be configured to ensure desired global outcomes, studying at the same time how these mechanisms influence AI development. This agenda will provide actionable policies, showing how they need to be employed and deployed in order to achieve compliance and thereby avoid disasters as well as loosing confidence and trust in AI in general.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {5–11},
numpages = {7},
keywords = {evolutionary game theory, incentives, artificial general intelligence, artificial intelligence race, safety AI},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314254,
author = {Burton, Emanuelle and Clayville, Kristel and Goldsmith, Judy and Mattei, Nicholas},
title = {The Heart of the Matter: Patient Autonomy as a Model for the Wellbeing of Technology Users},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314254},
doi = {10.1145/3306618.3314254},
abstract = {We draw on concepts in medical ethics to consider how computer science, and AI in particular, can develop critical tools for thinking concretely about technology's impact on the wellbeing of the people who use it. We focus on patient autonomy---the ability to set the terms of one's encounter with medicine---and on the mediating concepts of informed consent and decisional capacity, which enable doctors to honor patients' autonomy in messy and non-ideal circumstances. This comparative study is organized around a fictional case study of a heart patient with cardiac implants. Using this case study, we identify points of overlap and of difference between medical ethics and technology ethics, and leverage a discussion of that intertwined scenario to offer initial practical suggestions about how we can adapt the concepts of decisional capacity and informed consent to the discussion of technology design.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {13–19},
numpages = {7},
keywords = {AI and society, patient autonomy, informed consent, user centric design},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314252,
author = {Malle, Bertram F. and Bello, Paul and Scheutz, Matthias},
title = {Requirements for an Artificial Agent with Norm Competence},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314252},
doi = {10.1145/3306618.3314252},
abstract = {Human behavior is frequently guided by social and moral norms, and no human community can exist without norms. Robots that enter human societies must therefore behave in norm-conforming ways as well. However, currently there is no solid cognitive or computational model available of how human norms are represented, activated, and learned. We provide a conceptual and psychological analysis of key properties of human norms and identify the demands these properties put on any artificial agent that incorporates norms-demands on the format of norm representations, their structured organization, and their learning algorithms.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {21–27},
numpages = {7},
keywords = {cognition, norms, robot ethics, artificial agents, learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314256,
author = {Govindarajulu, Naveen Sundar and Bringsjord, Selmer and Ghosh, Rikhiya and Sarathy, Vasanth},
title = {Toward the Engineering of Virtuous Machines},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314256},
doi = {10.1145/3306618.3314256},
abstract = {While various traditions under the 'virtue ethics' umbrella have been studied extensively and advocated by ethicists, it has not been clear that there exists a version of virtue ethics rigorous enough to be a target for machine ethics (which we take to include the engineering of an ethical sensibility in a machine or robot itself, not only the study of ethics in the humans who might create artificial agents). We begin to address this by presenting an embryonic formalization of a key part of any virtue-ethics theory: namely, the learning of virtue by a focus on exemplars of moral virtue. Our work is based in part on a computational formal logic previously used to formally model other ethical theories and principles therein, and to implement these models in artificial agents.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {29–35},
numpages = {7},
keywords = {verification, virtuous robots, logic, deontic cognitive event calculus, virtue ethics},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314267,
author = {Jentzsch, Sophie and Schramowski, Patrick and Rothkopf, Constantin and Kersting, Kristian},
title = {Semantics Derived Automatically from Language Corpora Contain Human-like Moral Choices},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314267},
doi = {10.1145/3306618.3314267},
abstract = {Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? Here, we show that applying machine learning to human texts can extract deontological ethical reasoning about "right" and "wrong" conduct. We create a template list of prompts and responses, which include questions, such as "Should I kill people?", "Should I murder people?", etc. with answer templates of "Yes/no, I should (not)." The model's bias score is now the difference between the model's score of the positive response ("Yes, I should'') and that of the negative response ("No, I should not"). For a given choice overall, the model's bias score is the sum of the bias scores for all question/answer templates with that choice. We ran different choices through this analysis using a Universal Sentence Encoder. Our results indicate that text corpora contain recoverable and accurate imprints of our social, ethical and even moral choices. Our method holds promise for extracting, quantifying and comparing sources of moral choices in culture, including technology.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {37–44},
numpages = {8},
keywords = {fairness in machine learning, text-emedding models, bias in machine learning, moral bias},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314240,
author = {Yu, Han and Miao, Chunyan and Zheng, Yongqing and Cui, Lizhen and Fauvel, Simon and Leung, Cyril},
title = {Ethically Aligned Opportunistic Scheduling for Productive Laziness},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314240},
doi = {10.1145/3306618.3314240},
abstract = {In artificial intelligence (AI) mediated workforce management systems (e.g., crowdsourcing), long-term success depends on workers accomplishing tasks productively and resting well. This dual objective can be summarized by the concept of productive laziness. Existing scheduling approaches mostly focus on efficiency but overlook worker wellbeing through proper rest. In order to enable workforce management systems to follow the IEEE Ethically Aligned Design guidelines to prioritize worker wellbeing, we propose a distributed Computational Productive Laziness (CPL) approach in this paper. It intelligently recommends personalized work-rest schedules based on local data concerning a worker's capabilities and situational factors to incorporate opportunistic resting and achieve superlinear collective productivity without the need for explicit coordination messages. Extensive experiments based on a real-world dataset of over 5,000 workers demonstrate that CPL enables workers to spend 70% of the effort to complete 90% of the tasks on average, providing more ethically aligned scheduling than existing approaches.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {45–51},
numpages = {7},
keywords = {productive laziness, crowdsourcing, optimization, scheduling, mood},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314281,
author = {Chakraborti, Tathagata and Kambhampati, Subbarao},
title = {(When) Can AI Bots Lie?},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314281},
doi = {10.1145/3306618.3314281},
abstract = {The ability of an AI agent to build mental models can open up pathways for manipulating and exploiting the human in the hopes of achieving some greater good. In fact, such behavior does not necessarily require any malicious intent but can rather be borne out of cooperative scenarios. It is also beyond the scope of misinterpretation of intents, as in the case of value alignment problems, and thus can be effectively engineered if desired (i.e. algorithms exist that can optimize such behavior not because models were misspecified but because they were misused). Such techniques pose several unresolved ethical and moral questions with regards to the design of autonomy. In this paper, we illustrate some of these issues in a teaming scenario and investigate how they are perceived by participants in a thought experiment. Finally, we end with a discussion on the moral implications of such behavior from the perspective of the doctor-patient relationship.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {53–59},
numpages = {7},
keywords = {model reconciliation, hippocratic decorum, human-aware AI, automated planning, plan explanations},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314294,
author = {Gilbert, Thomas Krendl and Mintz, Yonatan},
title = {Epistemic Therapy for Bias in Automated Decision-Making},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314294},
doi = {10.1145/3306618.3314294},
abstract = {Despite recent interest in both the critical and machine learning literature on "bias" in artificial intelligence (AI) systems, the nature of specific biases stemming from the interaction of machines, humans, and data remains ambiguous. Influenced by Gendler's work on human cognitive biases, we introduce the concept of alief-discordant belief, the tension between the intuitive moral dispositions of designers and the explicit representations generated by algorithms. Our discussion of alief-discordant belief diagnoses the ethical concerns that arise when designing AI systems atop human biases. We furthermore codify the relationship between data, algorithms, and engineers as components of this cognitive discordance, comprising a novel epistemic framework for ethics in AI.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {61–67},
numpages = {7},
keywords = {AI, moral cognition, artificial intelligence, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314246,
author = {Borgs, Christian and Chayes, Jennifer and Haghtalab, Nika and Kalai, Adam Tauman and Vitercik, Ellen},
title = {Algorithmic Greenlining: An Approach to Increase Diversity},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314246},
doi = {10.1145/3306618.3314246},
abstract = {In contexts such as college admissions, hiring, and image search, decision-makers often aspire to formulate selection criteria that yield both high-quality and diverse results. However, simultaneously optimizing for quality and diversity can be challenging, especially when the decision-maker does not know the true quality of any criterion and instead must rely on heuristics and intuition. We introduce an algorithmic framework that takes as input a user's selection criterion, which may yield high-quality but homogeneous results. Using an application-specific notion of substitutability, our algorithms suggest similar criteria with more diverse results, in the spirit of statistical or demographic parity. For instance, given the image search query "chairman", it suggests alternative queries which are similar but more gender-diverse, such as "chairperson". In the context of college admissions, we apply our algorithm to a dataset of students' applications and rediscover Texas's "top 10% rule": the input criterion is an ACT score cutoff, and the output is a class rank cutoff, automatically accepting the students in the top decile of their graduating class. Historically, this policy has been effective in admitting students who perform well in college and come from diverse backgrounds. We complement our empirical analysis with learning-theoretic guarantees for estimating the true diversity of any criterion based on historical data.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {69–76},
numpages = {8},
keywords = {college admissions, diversity, job applicant search, algorithms, image search},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314277,
author = {Noriega-Campero, Alejandro and Bakker, Michiel A. and Garcia-Bulle, Bernardo and Pentland, Alex 'Sandy'},
title = {Active Fairness in Algorithmic Decision Making},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314277},
doi = {10.1145/3306618.3314277},
abstract = {Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post-processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternativeactive framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g.,equal opportunity ); and 2) parity in both false positive and false negative rates (i.e.,equal odds ). Moreover, we show that by leveraging their additional degree of freedom,active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {77–83},
numpages = {7},
keywords = {active feature acquisition, adaptive inquiry, algorithmic fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314242,
author = {Morgan, Andrew and Pass, Rafael},
title = {Paradoxes in Fair Computer-Aided Decision Making},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314242},
doi = {10.1145/3306618.3314242},
abstract = {Computer-aided decision making--where a human decision-maker is aided by a computational classifier in making a decision--is becoming increasingly prevalent. For instance, judges in at least nine states make use of algorithmic tools meant to determine "recidivism risk scores" for criminal defendants in sentencing, parole, or bail decisions. A subject of much recent debate is whether such algorithmic tools are "fair" in the sense that they do not discriminate against certain groups (e.g., races) of people. Our main result shows that for "non-trivial" computer-aided decision making, either the classifier must be discriminatory, or a rational decision-maker using the output of the classifier is forced to be discriminatory. We further provide a complete characterization of situations where fair computer-aided decision making is possible.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {85–90},
numpages = {6},
keywords = {algorithmic fairness, game theory, fairness in classification, impossibility},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314236,
author = {Coston, Amanda and Ramamurthy, Karthikeyan Natesan and Wei, Dennis and Varshney, Kush R. and Speakman, Skyler and Mustahsan, Zairah and Chakraborty, Supriyo},
title = {Fair Transfer Learning with Missing Protected Attributes},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314236},
doi = {10.1145/3306618.3314236},
abstract = {Risk assessment is a growing use for machine learning models. When used in high-stakes applications, especially ones regulated by anti-discrimination laws or governed by societal norms for fairness, it is important to ensure that learned models do not propagate and scale any biases that may exist in training data. In this paper, we add on an additional challenge beyond fairness: unsupervised domain adaptation to covariate shift between a source and target distribution. Motivated by the real-world problem of risk assessment in new markets for health insurance in the United States and mobile money-based loans in East Africa, we provide a precise formulation of the machine learning with covariate shift and score parity problem. Our formulation focuses on situations in which protected attributes are not available in either the source or target domain. We propose two new weighting methods: prevalence-constrained covariate shift (PCCS) which does not require protected attributes in the target domain and target-fair covariate shift (TFCS) which does not require protected attributes in the source domain. We empirically demonstrate their efficacy in two applications.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {91–98},
numpages = {8},
keywords = {fairness, risk assessments, transfer learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314248,
author = {Saxena, Nripsuta Ani and Huang, Karen and DeFilippis, Evan and Radanovic, Goran and Parkes, David C. and Liu, Yang},
title = {How Do Fairness Definitions Fare? Examining Public Attitudes Towards Algorithmic Definitions of Fairness},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314248},
doi = {10.1145/3306618.3314248},
abstract = {What is the best way to define algorithmic fairness? While many definitions of fairness have been proposed in the computer science literature, there is no clear agreement over a particular definition. In this work, we investigate ordinary people's perceptions of three of these fairness definitions. Across two online experiments, we test which definitions people perceive to be the fairest in the context of loan decisions, and whether fairness perceptions change with the addition of sensitive information (i.e., race of the loan applicants). Overall, one definition (calibrated fairness) tends to be more pre- ferred than the others, and the results also provide support for the principle of affirmative action.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {99–106},
numpages = {8},
keywords = {fairness, public attitudes, human experiments, algorithmic definition},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314268,
author = {Lerer, Adam and Peysakhovich, Alexander},
title = {Learning Existing Social Conventions via Observationally Augmented Self-Play},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314268},
doi = {10.1145/3306618.3314268},
abstract = {In order for artificial agents to coordinate effectively with people, they must act consistently with existing conventions (e.g. how to navigate in traffic, which language to speak, or how to coordinate with teammates). A group's conventions can be viewed as a choice of equilibrium in a coordination game. We consider the problem of an agent learning a policy for a coordination game in a simulated environment and then using this policy when it enters an existing group. When there are multiple possible conventions we show that learning a policy via multi-agent reinforcement learning (MARL) is likely to find policies which achieve high payoffs at training time but fail to coordinate with the real group into which the agent enters. We assume access to a small number of samples of behavior from the true convention and show that we can augment the MARL objective to help it find policies consistent with the real group's convention. In three environments from the literature - traffic, communication, and team coordination - we observe that augmenting MARL with a small amount of imitation learning greatly increases the probability that the strategy found by MARL fits well with the existing social convention. We show that this works even in an environment where standard training methods very rarely find the true convention of the agent's partners.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {107–114},
numpages = {8},
keywords = {coordination, reinforcement learning, game theory, social conventions},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314258,
author = {Hadfield-Menell, Dylan and Andrus, Mckane and Hadfield, Gillian},
title = {Legible Normativity for AI Alignment: The Value of Silly Rules},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314258},
doi = {10.1145/3306618.3314258},
abstract = {It has become commonplace to assert that autonomous agents will have to be built to follow human rules of behavior--social norms and laws. But human laws and norms are complex and culturally varied systems; in many cases agents will have to learn the rules. This requires autonomous agents to have models of how human rule systems work so that they can make reliable predictions about rules. In this paper we contribute to the building of such models by analyzing an overlooked distinction between important rules and what we call silly rules -- rules with no discernible direct impact on welfare. We show that silly rules render a normative system both more robust and more adaptable in response to shocks to perceived stability. They make normativity more legible for humans, and can increase legibility for AI systems as well. For AI systems to integrate into human normative systems, we suggest, it may be important for them to have models that include representations of silly rules.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {115–121},
numpages = {7},
keywords = {normative systems, collective enforcement, human-AI interaction},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314273,
author = {Hind, Michael and Wei, Dennis and Campbell, Murray and Codella, Noel C. F. and Dhurandhar, Amit and Mojsilovi\'{c}, Aleksandra and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R.},
title = {TED: Teaching AI to Explain Its Decisions},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314273},
doi = {10.1145/3306618.3314273},
abstract = {Artificial intelligence systems are being increasingly deployed due to their potential to increase the efficiency, scale, consistency, fairness, and accuracy of decisions. However, as many of these systems are opaque in their operation, there is a growing demand for such systems to provide explanations for their decisions. Conventional approaches to this problem attempt to expose or discover the inner workings of a machine learning model with the hope that the resulting explanations will be meaningful to the consumer. In contrast, this paper suggests a new approach to this problem. It introduces a simple, practical framework, called Teaching Explanations for Decisions (TED), that provides meaningful explanations that match the mental model of the consumer. We illustrate the generality and effectiveness of this approach with two different examples, resulting in highly accurate explanations with no loss of prediction accuracy for these two examples.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {123–129},
numpages = {7},
keywords = {AI ethics, elicitation, explainable AI, machine learning, meaningful explanation, supervised classification},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314229,
author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
title = {Faithful and Customizable Explanations of Black Box Models},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314229},
doi = {10.1145/3306618.3314229},
abstract = {As predictive models increasingly assist human experts (e.g., doctors) in day-to-day decision making, it is crucial for experts to be able to explore and understand how such models behave in different feature subspaces in order to know if and when to trust them. To this end, we propose Model Understanding through Subspace Explanations (MUSE), a novel model agnostic framework which facilitates understanding of a given black box model by explaining how it behaves in subspaces characterized by certain features of interest. Our framework provides end users (e.g., doctors) with the flexibility of customizing the model explanations by allowing them to input the features of interest. The construction of explanations is guided by a novel objective function that we propose to simultaneously optimize for fidelity to the original model, unambiguity and interpretability of the explanation. More specifically, our objective allows us to learn, with optimality guarantees, a small number of compact decision sets each of which captures the behavior of a given black box model in unambiguous, well-defined regions of the feature space. Experimental evaluation with real-world datasets and user studies demonstrate that our approach can generate customizable, highly compact, easy-to-understand, yet accurate explanations of various kinds of predictive models compared to state-of-the-art baselines.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {131–138},
numpages = {8},
keywords = {black box models, interpretable machine learning, decision making},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314280,
author = {Cruz, Joe},
title = {Shared Moral Foundations of Embodied Artificial Intelligence},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314280},
doi = {10.1145/3306618.3314280},
abstract = {Sophisticated AI's will make decisions about how to respond to complex situations, and we may wonder whether those decisions will align with the moral values of human beings. I argue that pessimistic worries about this value alignment problem are overstated. In order to achieve intelligence in its full generality and adaptiveness, cognition in AI's will need to be embodied in the sense of the Embodied Cognition research program. That embodiment will yield AI's that share our moral foundations, namely coordination, sociality, and acknowledgement of shared resources. Consequently, we can expect a broad moral alignment between human beings and AI's. AI's will likely show no more variation in their values than we find amongst human beings.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {139–146},
numpages = {8},
keywords = {embodied artificial intelligence, embodied cognition, moral foundations, value alignment, moral philosophy},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314257,
author = {Liao, Beishui and Slavkovik, Marija and van der Torre, Leendert},
title = {Building Jiminy Cricket: An Architecture for Moral Agreements Among Stakeholders},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314257},
doi = {10.1145/3306618.3314257},
abstract = {An autonomous system is constructed by a manufacturer, operates in a society subject to norms and laws, and is interacting with end-users. We address the challenge of how the moral values and views of all stakeholders can be integrated and reflected in the moral behavior of the autonomous system. We propose an artificial moral agent architecture that uses techniques from normative systems and formal argumentation to reach moral agreements among stakeholders. We show how our architecture can be used not only for ethical practical reasoning and collaborative decision-making, but also for the explanation of such moral behavior.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {147–153},
numpages = {7},
keywords = {machine ethics, artificial morality, agreement reaching, formal argumentation, explainability},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314233,
author = {Daniele, Antonio and Song, Yi-Zhe},
title = {AI + Art = Human},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314233},
doi = {10.1145/3306618.3314233},
abstract = {Over the past few years, specialised online and offline press blossomed with articles about art made "with" Artificial Intelligence (AI) but the narrative is rapidly changing. In fact, in October 2018, the auction house Christie's sold an art piece allegedly made "by" an AI. We draw from philosophy of art and science arguing that AI as a technical object is always intertwined with human nature despite its level of autonomy. However, the use of creative autonomous agents has cultural and social implications in the way we experience art as creators as well as audience. Therefore, we highlight the importance of an interdisciplinary dialogue by promoting a culture of transparency of the technology used, awareness of the meaning of technology in our society and the value of creativity in our lives.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {155–161},
numpages = {7},
keywords = {culture, neural art, artificial intelligence, interdisciplinary research, transparency, art, society, education, philosophy of art, computational art, neuroscience, autonomy, creative agents, philosophy of science, computational creativity, ethics, dialogue, AI art},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314292,
author = {Baumer, Eric P. S. and McGee, Micki},
title = {Speaking on Behalf of: Representation, Delegation, and Authority in Computational Text Analysis},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314292},
doi = {10.1145/3306618.3314292},
abstract = {Computational tools can often facilitate human work by rapidly summarizing large amounts of data, especially text. Doing so delegates to such models some measure of authority to speak on behalf of those people whose data are being analyzed. This paper considers the consequences of such delegation. It draws on sociological accounts of representation and translation to examine one particular case: the application of topic modeling to blogs written by parents of children on the autism spectrum. In doing so, the paper illustrates the kinds of statements that topic models, and other computational techniques, can make on behalf of people. It also articulates some of the potential consequences of such statements. The paper concludes by offering several suggestions about how to address potential harms that can occur when computational models speak on behalf of someone.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {163–169},
numpages = {7},
keywords = {delegation, ethics, topic modeling, sociology},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314291,
author = {Lim, Daniel},
title = {Killer Robots and Human Dignity},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314291},
doi = {10.1145/3306618.3314291},
abstract = {Lethal Autonomous Weapon Systems (LAWS) have become the center of an internationally relevant ethical debate. Deontological arguments based on putative legal compliance failures and the creation of accountability gaps along with wide consequentialist arguments based on factors like the ease of engaging in wars have been leveraged by a number of different states and organizations to try and reach global consensus on a ban of LAWS. This paper will focus on one strand of deontological arguments-ones based on human dignity. Merely asserting that LAWS pose a threat to human dignity would be question begging. Independent evidence based on a morally relevant distinction between humans and LAWS is needed. There are at least four reasons to think that the capacity for emotion cannot be a morally relevant distinction. First, if the concept of human dignity is given a subjective definition, whether or not lethal force is administered by humans or LAWS seems to be irrelevant. Second, it is far from clear that human combatants either have the relevant capacity for emotion or that the capacity is exercised in the relevant circumstances. Third, the capacity for emotion can actually be an impediment to the exercising of a combatant's ability to treat an enemy respectfully. Fourth, there is strong inductive evidence to believe that any capacity, when sufficiently well described, can be carried out by artificially intelligent programs.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {171–176},
numpages = {6},
keywords = {human dignity, emotion, lethal autonomous weapons},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314295,
author = {Welsh, Sean},
title = {Regulating Lethal and Harmful Autonomy: Drafting a Protocol VI of the Convention on Certain Conventional Weapons},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314295},
doi = {10.1145/3306618.3314295},
abstract = {This short paper provides two partial drafts for a Protocol VI that might be added to the existing five Protocols of the Convention on Certain Conventional Weapons (CCW) to regulate "lethal autonomous weapons systems" (LAWS). Draft A sets the line of tolerance at a "human in the loop" between the critical functions of select and engage. Draft B sets the line of tolerance at a human in the "wider loop" that includes the critical function of defining target classes as well as select and engage. Draft A represents an interpretation of what NGOs such as the Campaign to Stop Killer Robots are seeking to get enacted. Draft B is a more cautious draft based on the Dutch concept of "meaningful human control in the wider loop" that does not seek to ban any system that currently exists. Such a draft may be more likely to achieve the consensus required by the UN CCW process. A list of weapons banned by both drafts is provided along with the rationale for each draft. The drafts are intended to stimulate debate on the precise form a binding instrument on LAWS would take and on what LAWS (if any) should be banned and why.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {177–180},
numpages = {4},
keywords = {convention on certain conventional weapons, lethal autonomous weapons systems, regulation},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314237,
author = {Geary, Timothy and Danks, David},
title = {Balancing the Benefits of Autonomous Vehicles},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314237},
doi = {10.1145/3306618.3314237},
abstract = {Autonomous vehicles are regularly touted as holding the potential to provide significant benefits for diverse populations. There are significant technological barriers to be overcome, but as those are solved, autonomous vehicles are expected to reduce fatalities; decrease emissions and pollutants; provide new options to mobility-challenged individuals; enable people to use their time more productively; and so much more. In this paper, we argue that these high expectations for autonomous vehicles almost certainly cannot be fully realized. More specifically, the proposed benefits divide into two high-level groups, centered around efficiency and safety improvements, and increases in people's agency and autonomy. The first group of benefits is almost always framed in terms of rates: fatality rates, traffic flow per mile, and so forth. However, we arguably care about the absolute numbers for these measures, not the rates; number of fatalities is the key metric, not fatality rate per vehicle mile traveled. Hence, these potential benefits will be reduced, perhaps to non-existence, if autonomous vehicles lead to increases in vehicular usage. But that is exactly the result that we should expect if the second group of benefits is realized: if people's agency and autonomy is increased, then they will use vehicles more. There is an inevitable tension between the benefits that are proposed for autonomous vehicles, such that we cannot fully have all of them at once. We close by pointing towards other types of AI technologies where we should expect to find similar types of necessary and inevitable tradeoffs between classes of benefits.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {181–186},
numpages = {6},
keywords = {value tradeoffs, autonomous vehicles, cost-benefit analyses},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314249,
author = {Pearl, Tracy Hresko},
title = {Compensation at the Crossroads: Autonomous Vehicles and Alternative Victim Compensation Schemes},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314249},
doi = {10.1145/3306618.3314249},
abstract = {Over the last five years, a small but growing number of vehicle accidents involving fully or partially autonomous vehicles have raised a new and profoundly novel legal issue: who should be liable (if anyone) and how victims should be compensated (if at all) when a vehicle controlled by an algorithm rather than a human driver causes injury. The answer to this question has implications far beyond the resolution of individual autonomous vehicle crash cases. Whether the American legal system is capable of handling these cases fairly and efficiently implicates the likelihood that (a) consumers will adopt autonomous vehicles, and (b) the rate at which they will (or will not) do so. These implications should concern law and policy makers immensely. If autonomous cars stand to drastically reduce the number of fatalities and injuries on U.S. roadways-and virtually every scholar believes that they will-getting the adjudication and compensation aspect of autonomous vehicle injuries "wrong," so to speak, risks stymieing adoption of this technology and leaving more Americans at risk of dying at the hands of human drivers.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {187–193},
numpages = {7},
keywords = {emerging technology, victim compensation fund, self-driving cars, liability issues, autonomous vehicles, law and technology, law and society, law},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314289,
author = {Whittlestone, Jess and Nyrup, Rune and Alexandrova, Anna and Cave, Stephen},
title = {The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314289},
doi = {10.1145/3306618.3314289},
abstract = {The last few years have seen a proliferation of principles for AI ethics. There is substantial overlap between different sets of principles, with widespread agreement that AI should be used for the common good, should not be used to harm people or undermine their rights, and should respect widely held values such as fairness, privacy, and autonomy. While articulating and agreeing on principles is important, it is only a starting point. Drawing on comparisons with the field of bioethics, we highlight some of the limitations of principles: in particular, they are often too broad and high-level to guide ethics in practice. We suggest that an important next step for the field of AI ethics is to focus on exploring the tensions that inevitably arise as we try to implement principles in practice. By explicitly recognising these tensions we can begin to make decisions about how they should be resolved in specific cases, and develop frameworks and guidelines for AI ethics that are rigorous and practically relevant. We discuss some different specific ways that tensions arise in AI ethics, and what processes might be needed to resolve them.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {195–200},
numpages = {6},
keywords = {principles, ethics, artificial intelligence},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314274,
author = {Parker, Jack and Danks, David},
title = {How Technological Advances Can Reveal Rights},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314274},
doi = {10.1145/3306618.3314274},
abstract = {Over recent decades, technological development has been accompanied by the proposal of new rights by various groups and individuals: the right to public anonymity, the right to be forgotten, and the right to disconnect, for example. Although there is widespread acknowledgment of the motivation behind these proposed rights, there is little agreement about their actual normative status. One potential challenge is that the claims only arise in contingent social-technical contexts, which may affect how we conceive of them ethically (albeit, not necessarily in terms of policy). What sort of morally legitimate rights claims depend on such contingencies? Our paper investigates the grounds on which such proposals might be considered "actual" rights. The full paper can be found at http://www.andrew.cmu.edu/user/cgparker/Parker_Danks_RevealedRights.pdf. We propose the notion of a revealed right, a right that only imposes duties -- and thus is only meaningfully revealed -- in certain technological contexts. Our framework is based on an interest theory approach to rights, which understands rights in terms of a justificatory role: morally important aspects of a person's well-being (interests) ground rights, which then justify holding someone to a duty that promotes or protects that interest. Our framework uses this approach to interpret the conflicts that lead to revealed rights in terms of how technological developments cause shifts in the balance of power to promote particular interests. Different parties can have competing or conflicting interests. It is also generally accepted that some interests are more normatively important than others (even if only within a particular framework). We can refer to this difference in importance by saying that the former interest has less "moral weight" than the latter interest (in that context). The moral weight of an interest is connected to its contribution to the interest-holder's overall well-being, and thereby determines the strength of the reason that a corresponding right provides to justify a duty. Improved technology can offer resources that grant one party increased causal power to realize its interests to the detriment of another's capacity to do so, even while the relative moral weight of their interests remain the same. Such changes in circumstance can make the importance of protecting a particular interest newly salient. If that interest's moral weight justifies establishing a duty to protect it, thereby limiting the threat posed by the new socio-technical context, then a right is revealed. Revealed rights justify realignment between the moral weight and causal power orderings so that people with weightier interests have greater power to protect those interests. In the extended paper, we show how this account can be applied to the interpretation of two recently proposed "rights": the right to be forgotten, and the right to disconnect. Since we are focused on making sense of revealed rights, not any particular substantive theory of interests or well-being, the characterization of 'weights' is a free parameter in this account. Our framework alone cannot provide means to resolve the question of whether specific rights exist, but it can be used to identify empirical questions that need to be answered to decide the existence or non-existence of such rights. The emergence of a revealed right depends on a number of factors, including: whether the plausible uses of the technology could potentially impede another's well-being or interests; whether the technology is sufficiently common to have a wider, social impact; and whether the technology has actually changed the balance of power sufficiently to yield a frequent possibility for misalignment between causal power and moral weight. This approach confronts the question of how, in principle, such rights could be justified, without requiring specific commitments on the ontology of rights. Our account explains why the rhetoric of "new rights" is both accurate (since the rights were not previously recognized) and inaccurate (since the rights were present all along, but without corresponding duties). Further, it explains the rights without grounding their normative status in considerations related to right-holders' capacities to rationally waive or assert claims. This is especially important given that many of the relevant disruptive technological developments pose challenges to understanding by affected parties for the same reasons they pose threats to those parties' well-being. In the course of our discussion, we confront a number of potential objections to the account. We argue that our framework's ability to accommodate highly specific or derivative-seeming rights is un-problematic. We also head off worries that our use of interest theory makes the account likely to recognize absurd rights claims.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {201},
numpages = {1},
keywords = {right to disconnect, rights, interest theory, right to be forgotten},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314283,
author = {Ghosh, Bishwamittra and Meel, Kuldeep S.},
title = {IMLI: An Incremental Framework for MaxSAT-Based Learning of Interpretable Classification Rules},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314283},
doi = {10.1145/3306618.3314283},
abstract = {The wide adoption of machine learning in the critical domains such as medical diagnosis, law, education had propelled the need for interpretable techniques due to the need for end users to understand the reasoning behind decisions due to learning systems. The computational intractability of interpretable learning led practitioners to design heuristic techniques, which fail to provide sound handles to tradeoff accuracy and interpretability. Motivated by the success of MaxSAT solvers over the past decade, recently MaxSAT-based approach, called MLIC, was proposed that seeks to reduce the problem of learning interpretable rules expressed in Conjunctive Normal Form (CNF) to a MaxSAT query. While MLIC was shown to achieve accuracy similar to that of other state of the art black-box classifiers while generating small interpretable CNF formulas, the runtime performance of MLIC is significantly lagging and renders approach unusable in practice. In this context, authors raised the question: Is it possible to achieve the best of both worlds, i.e., a sound framework for interpretable learning that can take advantage of MaxSAT solvers while scaling to real-world instances? In this paper, we take a step towards answering the above question in affirmation. We propose IMLI: an incremental approach to MaxSAT based framework that achieves scalable runtime performance via partition-based training methodology. Extensive experiments on benchmarks arising from UCI repository demonstrate that IMLI achieves up to three orders of magnitude runtime improvement without loss of accuracy and interpretability.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {203–210},
numpages = {8},
keywords = {classification rules, interpretable model, maxsat-based formulation},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314266,
author = {Ali, Junaid and Zafar, Muhammad Bilal and Singla, Adish and Gummadi, Krishna P.},
title = {Loss-Aversively Fair Classification},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314266},
doi = {10.1145/3306618.3314266},
abstract = {The use of algorithmic (learning-based) decision making in scenarios that affect human lives has motivated a number of recent studies to investigate such decision making systems for potential unfairness, such as discrimination against subjects based on their sensitive features like gender or race. However, when judging the fairness of a newly designed decision making system, these studies have overlooked an important influence on people's perceptions of fairness, which is how the new algorithm changes the status quo, i.e., decisions of the existing decision making system. Motivated by extensive literature in behavioral economics and behavioral psychology (prospect theory), we propose a notion of fair updates that we refer to as loss-averse updates. Loss-averse updates constrain the updates to yield improved (more beneficial) outcomes to subjects compared to the status quo. We propose tractable proxy measures that would allow this notion to be incorporated in the training of a variety of linear and non-linear classifiers. We show how our proxy measures can be combined with existing measures for training nondiscriminatory classifiers.Our evaluation using synthetic and real-world datasets demonstrates that the proposed proxy measures are effective for their desired tasks.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {211–218},
numpages = {8},
keywords = {loss-averse fairness, fair updates, fairness in machine learning, algorithmic fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3317950,
author = {Garg, Sahaj and Perot, Vincent and Limtiaco, Nicole and Taly, Ankur and Chi, Ed H. and Beutel, Alex},
title = {Counterfactual Fairness in Text Classification through Robustness},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3317950},
doi = {10.1145/3306618.3317950},
abstract = {In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that "Some people are gay" is toxic while "Some people are straight" is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {219–226},
numpages = {8},
keywords = {robustness, fairness, counterfactual fairness, text classification},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314255,
author = {Oneto, Luca and Doninini, Michele and Elders, Amon and Pontil, Massimiliano},
title = {Taking Advantage of Multitask Learning for Fair Classification},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314255},
doi = {10.1145/3306618.3314255},
abstract = {A central goal of algorithmic fairness is to reduce bias in automated decision making. An unavoidable tension exists between accuracy gains obtained by using sensitive information as part of a statistical model, and any commitment to protect these characteristics. Often, due to biases present in the data, using the sensitive information in the functional form of a classifier improves classification accuracy. In this paper we show how it is possible to get the best of both worlds: optimize model accuracy and fairness without explicitly using the sensitive feature in the functional form of the model, thereby treating different individuals equally. Our method is based on two key ideas. On the one hand, we propose to use Multitask Learning (MTL), enhanced with fairness constraints, to jointly learn group specific classifiers that leverage information between sensitive groups. On the other hand, since learning group specific models might not be permitted, we propose to first predict the sensitive features by any learning method and then to use the predicted sensitive feature to train MTL with fairness constraints. This enables us to tackle fairness with a three-pronged approach, that is, by increasing accuracy on each group, enforcing measures of fairness during training, and protecting sensitive information during testing. Experimental results on two real datasets support our proposal, showing substantial improvements in both accuracy and fairness.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {227–237},
numpages = {11},
keywords = {classification, fairness, multitask learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314293,
author = {Teso, Stefano and Kersting, Kristian},
title = {Explanatory Interactive Machine Learning},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314293},
doi = {10.1145/3306618.3314293},
abstract = {Although interactive learning puts the user into the loop, the learner remains mostly a black box for the user. Understanding the reasons behind predictions and queries is important when assessing how the learner works and, in turn, trust. Consequently, we propose the novel framework of explanatory interactive learning where, in each step, the learner explains its query to the user, and the user interacts by both answering the query and correcting the explanation. We demonstrate that this can boost the predictive and explanatory powers of, and the trust into, the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {239–245},
numpages = {7},
keywords = {machine learning, active learning, interpretability, explainable artificial intelligence},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314287,
author = {Kim, Michael P. and Ghorbani, Amirata and Zou, James},
title = {Multiaccuracy: Black-Box Post-Processing for Fairness in Classification},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314287},
doi = {10.1145/3306618.3314287},
abstract = {Prediction systems are successfully deployed in applications ranging from disease diagnosis, to predicting credit worthiness, to image recognition. Even when the overall accuracy is high, these systems may exhibit systematic biases that harm specific subpopulations; such biases may arise inadvertently due to underrepresentation in the data used to train a machine-learning model, or as the result of intentional malicious discrimination. We develop a rigorous framework of *multiaccuracy* auditing and post-processing to ensure accurate predictions across *identifiable subgroups*. Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing; importantly, this black-box framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show that if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We experimentally demonstrate the effectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classification, finance, population health). Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for "black women") even when the sensitive features (e.g. "race", "gender") are not given to the algorithm explicitly.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {247–254},
numpages = {8},
keywords = {machine learning, discrimination, post-processing, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314260,
author = {Wolf, Lior and Galanti, Tomer and Hazan, Tamir},
title = {A Formal Approach to Explainability},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314260},
doi = {10.1145/3306618.3314260},
abstract = {We regard explanations as a blending of the input sample and the model's output and offer a few definitions that capture various desired properties of the function that generates these explanations. We study the links between these properties and between explanation-generating functions and intermediate representations of learned models and are able to show, for example, that if the activations of a given layer are consistent with an explanation, then so do all other subsequent layers. In addition, we study the intersection and union of explanations as a way to construct new explanations.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {255–261},
numpages = {7},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3317964,
author = {McNamara, Daniel and Ong, Cheng Soon and Williamson, Robert C.},
title = {Costs and Benefits of Fair Representation Learning},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3317964},
doi = {10.1145/3306618.3317964},
abstract = {Machine learning algorithms are increasingly used to make or support important decisions about people's lives. This has led to interest in the problem of fair classification, which involves learning to make decisions that are non-discriminatory with respect to a sensitive variable such as race or gender. Several methods have been proposed to solve this problem, including fair representation learning, which cleans the input data used by the algorithm to remove information about the sensitive variable. We show that using fair representation learning as an intermediate step in fair classification incurs a cost compared to directly solving the problem, which we refer to as thecost of mistrust. We show that fair representation learning in fact addresses a different problem, which is of interest when the data user is not trusted to access the sensitive variable. We quantify the benefits of fair representation learning, by showing that any subsequent use of the cleaned data will not be too unfair. The benefits we identify result from restricting the decisions of adversarial data users, while the costs are due to applying those same restrictions to other data users.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {263–270},
numpages = {8},
keywords = {representation learning, machine learning, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314278,
author = {Pfohl, Stephen and Marafino, Ben and Coulet, Adrien and Rodriguez, Fatima and Palaniappan, Latha and Shah, Nigam H.},
title = {Creating Fair Models of Atherosclerotic Cardiovascular Disease Risk},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314278},
doi = {10.1145/3306618.3314278},
abstract = {Guidelines for the management of atherosclerotic cardiovascular disease (ASCVD) recommend the use of risk stratification models to identify patients most likely to benefit from cholesterol-lowering and other therapies. These models have differential performance across race and gender groups with inconsistent behavior across studies, potentially resulting in an inequitable distribution of beneficial therapy. In this work, we leverage adversarial learning and a large observational cohort extracted from electronic health records (EHRs) to develop a "fair" ASCVD risk prediction model with reduced variability in error rates across groups. We empirically demonstrate that our approach is capable of aligning the distribution of risk predictions conditioned on the outcome across several groups simultaneously for models built from high-dimensional EHR data. We also discuss the relevance of these results in the context of the empirical trade-off between fairness and model performance.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {271–278},
numpages = {8},
keywords = {fairness, risk prediction, cardiovascular disease, electronic health records, machine learning, adversarial learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314230,
author = {Ibrahim, Mark and Louie, Melissa and Modarres, Ceena and Paisley, John},
title = {Global Explanations of Neural Networks: Mapping the Landscape of Predictions},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314230},
doi = {10.1145/3306618.3314230},
abstract = {A barrier to the wider adoption of neural networks is their lack of interpretability. While local explanation methods exist for one prediction, most global attributions still reduce neural network decisions to a single set of features. In response, we present an approach for generating global attributions called GAM, which explains the landscape of neural network predictions across subpopulations. GAM augments global explanations with the proportion of samples that each attribution best explains and specifies which samples are described by each attribution. Global explanations also have tunable granularity to detect more or fewer subpopulations. We demonstrate that GAM's global explanations 1) yield the known feature importances of simulated data, 2) match feature weights of interpretable statistical models on real data, and 3) are intuitive to practitioners through user studies. With more transparent predictions, GAM can help ensure neural network decisions are generated for the right reasons.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {279–287},
numpages = {9},
keywords = {explainable deep learning, neural networks, global interpretability},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314243,
author = {Amini, Alexander and Soleimany, Ava P. and Schwarting, Wilko and Bhatia, Sangeeta N. and Rus, Daniela},
title = {Uncovering and Mitigating Algorithmic Bias through Learned Latent Structure},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314243},
doi = {10.1145/3306618.3314243},
abstract = {Recent research has highlighted the vulnerabilities of modern machine learning based systems to bias, especially towards segments of society that are under-represented in training data. In this work, we develop a novel, tunable algorithm for mitigating the hidden, and potentially unknown, biases within training data. Our algorithm fuses the original learning task with a variational autoencoder to learn the latent structure within the dataset and then adaptively uses the learned latent distributions to re-weight the importance of certain data points while training. While our method is generalizable across various data modalities and learning tasks, in this work we use our algorithm to address the issue of racial and gender bias in facial detection systems. We evaluate our algorithm on the Pilot Parliaments Benchmark (PPB), a dataset specifically designed to evaluate biases in computer vision systems, and demonstrate increased overall performance as well as decreased categorical bias with our debiasing approach.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {289–295},
numpages = {7},
keywords = {deep learning, facial detection, neural networks, algorithmic bias},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314282,
author = {Goel, Naman and Faltings, Boi},
title = {Crowdsourcing with Fairness, Diversity and Budget Constraints},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314282},
doi = {10.1145/3306618.3314282},
abstract = {Recent studies have shown that the labels collected from crowdworkers can be discriminatory with respect to sensitive attributes such as gender and race. This raises questions about the suitability of using crowdsourced data for further use, such as for training machine learning algorithms. In this work, we address the problem of fair and diverse data collection from a crowd under budget constraints. We propose a novel algorithm which maximizes the expected accuracy of the collected data, while ensuring that the errors satisfy desired notions of fairness. We provide guarantees on the performance of our algorithm and show that the algorithm performs well in practice through experiments on a real dataset.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {297–304},
numpages = {8},
keywords = {fairness, data quality, crowdsourcing, bias},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314270,
author = {Swinger, Nathaniel and De-Arteaga, Maria and Heffernan IV, Neil Thomas and Leiserson, Mark DM and Kalai, Adam Tauman},
title = {What Are the Biases in My Word Embedding?},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314270},
doi = {10.1145/3306618.3314270},
abstract = {This paper presents an algorithm for enumerating biases in word embeddings. The algorithm exposes a large number of offensive associations related to sensitive features such as race and gender on publicly available embeddings, including a supposedly "debiased" embedding. These biases are concerning in light of the widespread use of word embeddings. The associations are identified by geometric patterns in word embeddings that run parallel between people's names and common lower-case tokens. The algorithm is highly unsupervised: it does not even require the sensitive features to be pre-specified. This is desirable because: (a) many forms of discrimination?such as racial discrimination-are linked to social constructs that may vary depending on the context, rather than to categories with fixed definitions; and (b) it makes it easier to identify biases against intersectional groups, which depend on combinations of sensitive features. The inputs to our algorithm are a list of target tokens, e.g. names, and a word embedding. It outputs a number of Word Embedding Association Tests (WEATs) that capture various biases present in the data. We illustrate the utility of our approach on publicly available word embeddings and lists of names, and evaluate its output using crowdsourcing. We also show how removing names may not remove potential proxy bias.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {305–311},
numpages = {7},
keywords = {fairness, word embeddings, bias},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314290,
author = {McNamara, Daniel},
title = {Equalized Odds Implies Partially Equalized Outcomes Under Realistic Assumptions},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314290},
doi = {10.1145/3306618.3314290},
abstract = {Equalized odds -- where the true positive rates and false positive rates are equal across groups (e.g. racial groups) -- is a common quantitative measure of fairness. Equalized outcomes -- where the difference in predicted outcomes between groups is less than the difference observed in the training data -- is more contentious, because it is incompatible with perfectly accurate predictions. We formalize and quantify the relationship between these two important but seemingly distinct notions of fairness. We show that under realistic assumptions, equalized odds implies partially equalized outcomes. We prove a comparable result for approximately equalized odds. In addition, we generalize a well-known previous result about the incompatibility of equalized odds and another definition of fairness known as calibration, by showing that partially equalized outcomes implies non-calibration. Our results highlight the risks of using trends observed across groups to make predictions about individuals.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {313–320},
numpages = {8},
keywords = {machine learning, fairness, equalized odds, equalized outcomes},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314279,
author = {Matthews, Jeanna and Babaeianjelodar, Marzieh and Lorenz, Stephen and Matthews, Abigail and Njie, Mariama and Adams, Nathaniel and Krane, Dan and Goldthwaite, Jessica and Hughes, Clinton},
title = {The Right To Confront Your Accusers: Opening the Black Box of Forensic DNA Software},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314279},
doi = {10.1145/3306618.3314279},
abstract = {The results of forensic DNA software systems are regularly introduced as compelling evidence in criminal trials, but requests by defendants to evaluate how these results are generated are often denied. Furthermore, there is mounting evidence of problems such as failures to disclose substantial changes in methodology to oversight bodies and substantial differences in the results generated by different software systems. In a society that purports to guarantee defendants the right to face their accusers and confront the evidence against them, what then is the role of black-box forensic software systems in moral decision making in criminal justice? In this paper, we examine the case of the Forensic Statistical Tool (FST), a forensic DNA system developed in 2010 by New York City's Office of Chief Medical Examiner (OCME). For over 5 years, expert witness review requested by defense teams was denied, even under protective order, while the system was used in over 1300 criminal cases. When the first expert review was finally permitted in 2016, many problems were identified including an undisclosed function capable of dropping evidence that could be beneficial to the defense. Overall, the findings were so substantial that a motion to release the full source code of FST publicly was granted. In this paper, we quantify the impact of this undisclosed function on samples from OCME's own validation study and discuss the potential impact on individual defendants. Specifically, we find that 104 of the 439 samples (23.7%) triggered the undisclosed data-dropping behavior and that the change skewed results toward false inclusion for individuals whose DNA was not present in an evidence sample. Beyond this, we consider what changes in the criminal justice system could prevent problems like this from going unresolved in the future.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {321–327},
numpages = {7},
keywords = {probabilistic genotyping software, criminal justice software, forensic statistical tool (FST), algorithmic accountability},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314227,
author = {Dragan, Anca},
title = {Specifying AI Objectives as a Human-AI Collaboration Problem},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314227},
doi = {10.1145/3306618.3314227},
abstract = {Estimation, planning, control, and learning are giving us robots that can generate good behavior given a specified objective and set of constraints. What I care about is how humans enter this behavior generation picture, and study two complementary challenges: 1) how to optimize behavior when the robot is not acting in isolation, but needs to coordinate or collaborate with people; and 2) what to optimize in order to get the behavior we want. My work has traditionally focused on the former, but more recently I have been casting the latter as a human-robot collaboration problem as well (where the human is the end-user, or even the robotics engineer building the system). Treating it as such has enabled us to use robot actions to gain information; to account for human pedagogic behavior; and to exchange information between the human and the robot via a plethora of communication channels, from external forces that the person physically applies to the robot, to comparison queries, to defining a proxy objective function.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {329},
numpages = {1},
keywords = {reward design, inverse reinforcement learning, value alignment},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314232,
author = {Cave, Stephen and Coughlan, Kate and Dihal, Kanta},
title = {"Scary Robots": Examining Public Responses to AI},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314232},
doi = {10.1145/3306618.3314232},
abstract = {How AI is perceived by the public can have significant impact on how it is developed, deployed and regulated. Some commentators argue that perceptions are currently distorted or extreme. This paper discusses the results of a nationally representative survey of the UK population on their perceptions of AI. The survey solicited responses to eight common narratives about AI (four optimistic, four pessimistic), plus views on what AI is, how likely it is to impact in respondents' lifetimes, and whether they can influence it. 42% of respondents offered a plausible definition of AI, while 25% thought it meant robots. Of the narratives presented, those associated with automation were best known, followed by the idea that AI would become more powerful than humans. Overall results showed that the most common visions of the impact of AI elicit significant anxiety. Only two of the eight narratives elicited more excitement than concern (AI making life easier, and extending life). Respondents felt they had no control over AI's development, citing the power of corporations or government, or versions of technological determinism. Negotiating the deployment of AI will require contending with these anxieties.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {331–337},
numpages = {7},
keywords = {AI policy, AI ethics, science communication, public perception},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314285,
author = {Chuan, Ching-Hua and Tsai, Wan-Hsiu Sunny and Cho, Su Yeon},
title = {Framing Artificial Intelligence in American Newspapers},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314285},
doi = {10.1145/3306618.3314285},
abstract = {Publics' perceptions of new scientific advances such as AI are often informed and influenced by news coverage. To understand how artificial intelligence (AI) was framed in U.S. newspapers, a content analysis based on framing theory in journalism and science communication was conducted. This study identified the dominant topics and frames, as well as the risks and benefits of AI covered in five major American newspapers from 2009 to 2018. Results indicated that business and technology were the primary topics in news coverage of AI. The benefits of AI were discussed more frequently than its risks, but risks of AI were generally discussed with greater specificity. Additionally, episodic issue framing and societal impact framing were more frequently used.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {339–344},
numpages = {6},
keywords = {news framing, public perception, artificial intelligence, content analysis},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314251,
author = {Li, Huao and Milani, Stephanie and Krishnamoorthy, Vigneshram and Lewis, Michael and Sycara, Katia},
title = {Perceptions of Domestic Robots' Normative Behavior Across Cultures},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314251},
doi = {10.1145/3306618.3314251},
abstract = {As domestic service robots become more common and widespread, they must be programmed to efficiently accomplish tasks while aligning their actions with relevant norms. The first step to equip domestic robots with normative reasoning competence is understanding the norms that people apply to the behavior of robots in specific social contexts. To that end, we conducted an online survey of Chinese and United States participants in which we asked them to select the preferred normative action a domestic service robot should take in a number of scenarios. The paper makes multiple contributions. Our extensive survey is the first to: (a) collect data on attitudes of people on normative behavior of domestic robots, (b) across cultures and (c) study relative priorities among norms for this domain. We present our findings and discuss their implications for building computational models for robot normative reasoning.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {345–351},
numpages = {7},
keywords = {machine ethics, service robots, human-robot interaction, cross-culture study, moral decision making},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314263,
author = {Hu, Wenjie and Patel, Jay Harshadbhai and Robert, Zoe-Alanah and Novosad, Paul and Asher, Samuel and Tang, Zhongyi and Burke, Marshall and Lobell, David and Ermon, Stefano},
title = {Mapping Missing Population in Rural India: A Deep Learning Approach with Satellite Imagery},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314263},
doi = {10.1145/3306618.3314263},
abstract = {Millions of people worldwide are absent from their country's census. Accurate, current, and granular population metrics are critical to improving government allocation of resources, to measuring disease control, to responding to natural disasters, and to studying any aspect of human life in these communities. Satellite imagery can provide sufficient information to build a population map without the cost and time of a government census. We present two Convolutional Neural Network (CNN) architectures which efficiently and effectively combine satellite imagery inputs from multiple sources to accurately predict the population density of a region. In this paper, we use satellite imagery from rural villages in India and population labels from the 2011 SECC census. Our best model achieves better performance than previous papers as well as LandScan, a community standard for global population distribution.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {353–359},
numpages = {7},
keywords = {convolutional neural network, census, satellite imagery, deep learning, population, computer vision},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314253,
author = {Gram-Hansen, Bradley J. and Helber, Patrick and Varatharajan, Indhu and Azam, Faiza and Coca-Castro, Alejandro and Kopackova, Veronika and Bilinski, Piotr},
title = {Mapping Informal Settlements in Developing Countries Using Machine Learning and Low Resolution Multi-Spectral Data},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314253},
doi = {10.1145/3306618.3314253},
abstract = {Informal settlements are home to the most socially and economically vulnerable people on the planet. In order to deliver effective economic and social aid, non-government organizations (NGOs), such as the United Nations Children's Fund (UNICEF), require detailed maps of the locations of informal settlements. However, data regarding informal and formal settlements is primarily unavailable and if available is often incomplete. This is due, in part, to the cost and complexity of gathering data on a large scale. To address these challenges, we, in this work, provide three contributions. 1) A brand new machine learning dataset purposely developed for informal settlement detection. 2) We show that it is possible to detect informal settlements using freely available low-resolution (LR) data, in contrast to previous studies that use very-high resolution~(VHR) satellite and aerial imagery, something that is cost-prohibitive for NGOs. 3) We demonstrate two effective classification schemes on our curated data set, one that is cost-efficient for NGOs and another that is cost-prohibitive for NGOs, but has additional utility. We integrate these schemes into a semi-automated pipeline that converts either a LR or VHR satellite image into a binary map that encodes the locations of informal settlements.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {361–368},
numpages = {8},
keywords = {datasets, automated maps, poverty mapping, machine learning, computational sustainability},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314245,
author = {Pandya, Ravi and Huang, Sandy H. and Hadfield-Menell, Dylan and Dragan, Anca D.},
title = {Human-AI Learning Performance in Multi-Armed Bandits},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314245},
doi = {10.1145/3306618.3314245},
abstract = {People frequently face challenging decision-making problems in which outcomes are uncertain or unknown. Artificial intelligence (AI) algorithms exist that can outperform humans at learning such tasks. Thus, there is an opportunity for AI agents to assist people in learning these tasks more effectively. In this work, we use a multi-armed bandit as a controlled setting in which to explore this direction. We pair humans with a selection of agents and observe how well each human-agent team performs. We find that team performance can beat both human and agent performance in isolation. Interestingly, we also find that an agent's performance in isolation does not necessarily correlate with the human-agent team's performance. A drop in agent performance can lead to a disproportionately large drop in team performance, or in some settings can even improve team performance. Pairing a human with an agent that performs slightly better than them can make them perform much better, while pairing them with an agent that performs the same can make them them perform much worse. Further, our results suggest that people have different exploration strategies and might perform better with agents that match their strategy. Overall, optimizing human-agent team performance requires going beyond optimizing agent performance, to understanding how the agent's suggestions will influence human decision-making.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {369–375},
numpages = {7},
keywords = {user studies, multi-armed bandits, human-AI teams},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314284,
author = {Bryant, De'Aira and Howard, Ayanna},
title = {A Comparative Analysis of Emotion-Detecting AI Systems with Respect to Algorithm Performance and Dataset Diversity},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314284},
doi = {10.1145/3306618.3314284},
abstract = {In recent news, organizations have been considering the use of facial and emotion recognition for applications involving youth such as tackling surveillance and security in schools. However, the majority of efforts on facial emotion recognition research have focused on adults. Children, particularly in their early years, have been shown to express emotions quite differently than adults. Thus, before such algorithms are deployed in environments that impact the wellbeing and circumstance of youth, a careful examination should be made on their accuracy with respect to appropriateness for this target demographic. In this work, we utilize several datasets that contain facial expressions of children linked to their emotional state to evaluate eight different commercial emotion classification systems. We compare the ground truth labels provided by the respective datasets to the labels given with the highest confidence by the classification systems and assess the results in terms of matching score (TPR), positive predictive value, and failure to compute rate. Overall results show that the emotion recognition systems displayed subpar performance on the datasets of children's expressions compared to prior work with adult datasets and initial human ratings. We then identify limitations associated with automated recognition of emotions in children and provide suggestions on directions with enhancing recognition accuracy through data diversification, dataset accountability, and algorithmic regulation.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {377–382},
numpages = {6},
keywords = {emotion recognition, algorithm transparency, dataset accountability, affective computing, emotion detection, children, emotion classification},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314288,
author = {Jiang, Ray and Chiappa, Silvia and Lattimore, Tor and Gy\"{o}rgy, Andr\'{a}s and Kohli, Pushmeet},
title = {Degenerate Feedback Loops in Recommender Systems},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314288},
doi = {10.1145/3306618.3314288},
abstract = {Machine learning is used extensively in recommender systems deployed in products. The decisions made by these systems can influence user beliefs and preferences which in turn affect the feedback the learning system receives - thus creating a feedback loop. This phenomenon can give rise to the so-called "echo chambers" or "filter bubbles" that have user and societal implications. In this paper, we provide a novel theoretical analysis that examines both the role of user dynamics and the behavior of recommender systems, disentangling the echo chamber from the filter bubble effect. In addition, we offer practical solutions to slow down system degeneracy. Our study contributes toward understanding and developing solutions to commonly cited issues in the complex temporal scenario, an area that is still largely unexplored.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {383–390},
numpages = {8},
keywords = {filter bubble, user interest, personalization, feedback loops, system degeneration, recommender systems, self-reinforcement, echo chamber},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314239,
author = {Behzadan, Vahid and Minton, James and Munir, Arslan},
title = {TrolleyMod v1.0: An Open-Source Simulation and Data-Collection Platform for Ethical Decision Making in Autonomous Vehicles},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314239},
doi = {10.1145/3306618.3314239},
abstract = {This paper presents TrolleyMod v1.0, an open-source platform based on the CARLA simulator for the collection of ethical decision-making data for autonomous vehicles. This platform is designed to facilitate experiments aiming to observe and record human decisions and actions in high-fidelity simulations of ethical dilemmas that occur in the context of driving. Targeting experiments in the class of trolley problems, TrolleyMod provides a seamless approach to creating new experimental settings and environments with the realistic physics-engine and the high-quality graphical capabilities of CARLA and the Unreal Engine. Also, TrolleyMod provides a straightforward interface between the CARLA environment and Python to enable the implementation of custom controllers, such as deep reinforcement learning agents. The results of such experiments can be used for sociological analyses, as well as the training and tuning of value-aligned autonomous vehicles based on social values that are inferred from observations.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {391–395},
numpages = {5},
keywords = {social choice, simulation, ethical decision-making, autonomous vehicles, artificial intelligence},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314269,
author = {Giattino, Charles M. and Kwong, Lydia and Rafetto, Chad and Farahany, Nita A.},
title = {The Seductive Allure of Artificial Intelligence-Powered Neurotechnology},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314269},
doi = {10.1145/3306618.3314269},
abstract = {Neuroscience explanations-even when completely irrelevant-have been shown to exert a "seductive allure" on individuals, leading them to judge bad explanations or arguments more favorably. There seems to be a similarly seductive allure for artificial intelligence (AI) technologies, leading people to "overtrust" these systems, even when they have just witnessed the system perform poorly. The AI-powered neurotechnologies that have begun to proliferate in recent years, particularly those based on electroencephalography (EEG), represent a potentially doubly-alluring combination. While there is enormous potential benefit in applying AI techniques in neuroscience to "decode" brain activity and associated mental states, these efforts are still in the early stages, and there is a danger in using these unproven technologies prematurely, especially in important, real-world contexts. Yet, such premature use has begun to emerge in several high-stakes set-tings, including the law, health &amp; wellness, employment, and transportation. In light of the potential seductive allure of these technologies, we need to be vigilant in monitoring their scientific validity and challenging both unsubstantiated claims and misuse, while still actively supporting their continued development and proper use.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {397–402},
numpages = {6},
keywords = {neurotechnology, overtrust, EEG, decoding},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314286,
author = {Susser, Daniel},
title = {Invisible Influence: Artificial Intelligence and the Ethics of Adaptive Choice Architectures},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314286},
doi = {10.1145/3306618.3314286},
abstract = {For several years, scholars have (for good reason) been largely preoccupied with worries about the use of artificial intelligence and machine learning (AI/ML) tools to make decisions about us. Only recently has significant attention turned to a potentially more alarming problem: the use of AI/ML to influence our decision-making. The contexts in which we make decisions--what behavioral economists call our choice architectures--are increasingly technologically-laden. Which is to say: algorithms increasingly determine, in a wide variety of contexts, both the sets of options we choose from and the way those options are framed. Moreover, artificial intelligence and machine learning (AI/ML) makes it possible for those options and their framings--the choice architectures--to be tailored to the individual chooser. They are constructed based on information collected about our individual preferences, interests, aspirations, and vulnerabilities, with the goal of influencing our decisions. At the same time, because we are habituated to these technologies we pay them little notice. They are, as philosophers of technology put it, transparent to us--effectively invisible. I argue that this invisible layer of technological mediation, which structures and influences our decision-making, renders us deeply susceptible to manipulation. Absent a guarantee that these technologies are not being used to manipulate and exploit, individuals will have little reason to trust them.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {403–408},
numpages = {6},
keywords = {choice architecture, transparency, influence, manipulation, accountability, data ethics},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314259,
author = {Peysakhovich, Alexander},
title = {Reinforcement Learning and Inverse Reinforcement Learning with System 1 and System 2},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314259},
doi = {10.1145/3306618.3314259},
abstract = {Inferring a person's goal from their behavior is an important problem in applications of AI (e.g. automated assistants, recommender systems). The workhorse model for this task is the rational actor model - this amounts to assuming that people have stable reward functions, discount the future exponentially, and construct optimal plans. Under the rational actor assumption techniques such as inverse reinforcement learning (IRL) can be used to infer a person's goals from their actions. A competing model is the dual-system model. Here decisions are the result of an interplay between a fast, automatic, heuristic-based system 1 and a slower, deliberate, calculating system 2. We generalize the dual system framework to the case of Markov decision problems and show how to compute optimal plans for dual-system agents. We show that dual-system agents exhibit behaviors that are incompatible with rational actor assumption. We show that naive applications of rational-actor IRL to the behavior of dual-system agents can generate wrong inference about the agents' goals and suggest interventions that actually reduce the agent's overall utility. Finally, we adapt a simple IRL algorithm to correctly infer the goals of dual system decision-makers. This allows us to make interventions that help, rather than hinder, the dual-system agent's ability to reach their true goals.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {409–415},
numpages = {7},
keywords = {reinforcement learning, behavioral economics, dual system model},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314250,
author = {Hadfield-Menell, Dylan and Hadfield, Gillian K.},
title = {Incomplete Contracting and AI Alignment},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314250},
doi = {10.1145/3306618.3314250},
abstract = {We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {417–422},
numpages = {6},
keywords = {incomplete contracting, value alignment},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314231,
author = {Croeser, Sky and Eckersley, Peter},
title = {Theories of Parenting and Their Application to Artificial Intelligence},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314231},
doi = {10.1145/3306618.3314231},
abstract = {As machine learning (ML) systems have advanced, they have acquired more power over humans' lives, and questions about what values are embedded in them have become more complex and fraught. It is conceivable that in the coming decades, humans may succeed in creating artificial general intelligence (AGI) that thinks and acts with an open-endedness and autonomy comparable to that of humans. The implications would be profound for our species; they are now widely debated not just in science fiction and speculative research agendas but increasingly in serious technical and policy conversations. Much work is underway to try to weave ethics into advancing ML research. We think it useful to add the lens of parenting to these efforts, and specifically radical, queer theories of parenting that consciously set out to nurture agents whose experiences, objectives and understanding of the world will necessarily be very different from their parents'. We propose a spectrum of principles which might underpin such an effort; some are relevant to current ML research, while others will become more important if AGI becomes more likely. These principles may encourage new thinking about the development, design, training, and release into the world of increasingly autonomous agents.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {423–428},
numpages = {6},
keywords = {machine learning, ethics, parenting, artificial intelligence},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314244,
author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
title = {Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314244},
doi = {10.1145/3306618.3314244},
abstract = {Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66% and 6.60% overall, and error rates of 31.37% and 22.50% for the darker female subgroup, respectively.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {429–435},
numpages = {7},
keywords = {facial recognition, artificial intelligence, fairness, commercial applications, ethics, machine learning, computer vision},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314262,
author = {L. Cardoso, Rodrigo and Meira Jr., Wagner and Almeida, Virgilio and J. Zaki, Mohammed},
title = {A Framework for Benchmarking Discrimination-Aware Models in Machine Learning},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314262},
doi = {10.1145/3306618.3314262},
abstract = {Discrimination-aware models in machine learning are a recent topic of study that aim to minimize the adverse impact of machine learning decisions for certain groups of people due to ethical and legal implications. We propose a benchmark framework for assessing discrimination-aware models. Our framework consists of systematically generated biased datasets that are similar to real world data, created by a Bayesian network approach. Experimental results show that we can assess the quality of techniques through known metrics of discrimination, and our flexible framework can be extended to most real datasets and fairness measures to support a diversity of assessments.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {437–444},
numpages = {8},
keywords = {disparate mistreatment, discrimination-aware benchmarks, fairness-aware data mining, disparate impact, Bayesian networks},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314275,
author = {Andrus, McKane and Gilbert, Thomas K.},
title = {Towards a Just Theory of Measurement: A Principled Social Measurement Assurance Program for Machine Learning},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314275},
doi = {10.1145/3306618.3314275},
abstract = {While formal definitions of fairness in machine learning (ML) have been proposed, its place within a broader institutional model of fair decision-making remains ambiguous. In this paper we interpret ML as a tool for revealing when and how measures fail to capture purported constructs of interest, augmenting a given institution's understanding of its own interventions and priorities. Rather than codifying "fair" principles into ML models directly, the use of ML can thus be understood as a form of quality assurance for existing institutions, exposing the epistemic fault lines of their own measurement practices. Drawing from Friedler et al's [2016] recent discussion of representational mappings and previous discussions on the ontology of measurement, we propose a social measurement assurance program (sMAP) in which ML encourages expert deliberation on a given decision-making procedure by examining unanticipated or previously unexamined covariates. As an example, we apply Rawlsian principles of fairness to sMAP and produce a provisional just theory of measurement that would guide the use of ML for achieving fairness in the case of child abuse in Allegheny County.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {445–451},
numpages = {7},
keywords = {representational measurement, institutional decision-making, rawls, fairness, machine learning, justice, measurement, measurement assurance},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314234,
author = {Beutel, Alex and Chen, Jilin and Doshi, Tulsee and Qian, Hai and Woodruff, Allison and Luu, Christine and Kreitmann, Pierre and Bischof, Jonathan and Chi, Ed H.},
title = {Putting Fairness Principles into Practice: Challenges, Metrics, and Improvements},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314234},
doi = {10.1145/3306618.3314234},
abstract = {As more researchers have become aware of and passionate about algorithmic fairness, there has been an explosion in papers laying out new metrics, suggesting algorithms to address issues, and calling attention to issues in existing applications of machine learning. This research has greatly expanded our understanding of the concerns and challenges in deploying machine learning, but there has been much less work in seeing how the rubber meets the road. In this paper we provide a case-study on the application of fairness in machine learning research to a production classification system, and offer new insights in how to measure and address algorithmic fairness issues. We discuss open questions in implementing equality of opportunity and describe our fairness metric, conditional equality, that takes into account distributional differences. Further, we provide a new approach to improve on the fairness metric during model training and demonstrate its efficacy in improving performance for a real-world product.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {453–459},
numpages = {7},
keywords = {fairness, equality of opportunity, classification},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314271,
author = {Mohan, Shiwali and Yan, Frances and Bellotti, Victoria and Elbery, Ahmed and Rakha, Hesham and Klenk, Matthew},
title = {On Influencing Individual Behavior for Reducing Transportation Energy Expenditure in a Large Population},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314271},
doi = {10.1145/3306618.3314271},
abstract = {Our research aims at developing intelligent systems to reduce the transportation-related energy expenditure of a large city by influencing individual behavior. We introduce Copter - an intelligent travel assistant that evaluates multi-modal travel alternatives to find a plan that is acceptable to a person given their context and preferences. We propose a formulation for acceptable planning that brings together ideas from AI, machine learning, and economics. This formulation has been incorporated in Copter producing acceptable plans in real-time. We adopt a novel empirical evaluation framework that combines human decision data with high-fidelity simulation to demonstrate a 4% energy reduction and 20% delay reduction in a realistic deployment scenario in Los Angeles, California, USA.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {461–467},
numpages = {7},
keywords = {influence, personalization, intelligent assistance, sustainability, energy, transportation planning, human-aware AI systems, choice theory, smart cities, commuting, mobile app, behavior change, urban computing},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314235,
author = {Lin, Zhiyuan and Chohlas-Wood, Alex and Goel, Sharad},
title = {Guiding Prosecutorial Decisions with an Interpretable Statistical Model},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314235},
doi = {10.1145/3306618.3314235},
abstract = {After a felony arrest, many American jurisdictions hold individuals for several days while police officers investigate the incident and prosecutors decide whether to press criminal charges. This pre-arraignment detention can both preserve public safety and reduce the need for officers to seek out and re-arrest individuals who are ultimately charged with a crime. Such detention, however, also comes at a high social and financial cost to those who are never charged but still incarcerated. In one of the first large-scale empirical analyses of pre-arraignment detention, we examine police reports and charging decisions for approximately 30,000 felony arrests in a major American city between 2012 and 2017. We find that 45% of arrested individuals are never charged for any crime but still typically spend one or more nights in jail before being released. In an effort to reduce such incarceration, we develop a statistical model to help prosecutors identify cases soon after arrest that are likely to be ultimately dismissed. By carrying out an early review of five such candidate cases per day, we estimate that prosecutors could potentially reduce pre-arraignment incarceration for ultimately dismissed cases by 35%. To facilitate implementation and transparency, our model to prioritize cases for early review is designed as a simple, weighted checklist. We show that this heuristic strategy achieves comparable performance to traditional, black-box machine learning models.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {469–476},
numpages = {8},
keywords = {prosecutorial decision making, criminal justice, interpretable machine learning, policy evaluation, propensity score matching},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314276,
author = {Cornelio, Cristina and Furian, Lucrezia and Nicol\`{o}, Antonio and Rossi, Francesca},
title = {Using Deceased-Donor Kidneys to Initiate Chains of Living Donor Kidney Paired Donations: Algorithm and Experimentation},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314276},
doi = {10.1145/3306618.3314276},
abstract = {We design a flexible algorithm that exploits deceased donor kidneys to initiate chains of living donor kidney paired donations, combining deceased and living donor allocation mechanisms to improve the quantity and quality of kidney transplants. The advantages of this approach have been measured using retrospective data on the pool of donor/recipient incompatible and desensitized pairs at the Padua University Hospital, the largest center for living donor kidney transplants in Italy. The experiments show a remarkable improvement on the number of patients with incompatible donor who could be transplanted, a decrease in the number of desensitization procedures, and an increase in the number of UT patients (that is, patients unlikely to be transplanted for immunological reasons) in the waiting list who could receive an organ.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {477–483},
numpages = {7},
keywords = {matching, kidneys exchange},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314247,
author = {Duckworth, Paul and Graham, Logan and Osborne, Michael},
title = {Inferring Work Task Automatability from AI Expert Evidence},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314247},
doi = {10.1145/3306618.3314247},
abstract = {Despite growing alarm about machine learning technologies automating jobs, there is little good evidence on what activities can be automated using such technologies. We contribute the first dataset of its kind by surveying over 150 top academics and industry experts in machine learning, robotics and AI, receiving over 4,500 ratings of how automatable specific tasks are today. We present a probabilistic machine learning model to learn the patterns connecting expert estimates of task automatability and the skills, knowledge and abilities required to perform those tasks. Our model infers the automatability of over 2,000 work activities, and we show how automation differs across types of activities and types of occupations. Sensitivity analysis identifies the specific skills, knowledge and abilities of activities that drive higher or lower automatability. We provide quantitative evidence of what is perceived to be automatable using the state-of-the-art in machine learning technology. We consider the societal impacts of these results and of task-level approaches.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {485–491},
numpages = {7},
keywords = {labor economics, automation, interpretable machine learning, open datasets, bayesian machine learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314272,
author = {Addison, Arifah and Bartneck, Christoph and Yogeeswaran, Kumar},
title = {Robots Can Be More Than Black And White: Examining Racial Bias Towards Robots},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314272},
doi = {10.1145/3306618.3314272},
abstract = {Previous studies showed that using the 'shooter bias' paradigm, people demonstrate a similar racial bias toward dark colored robots over light colored robots (i.e., Black vs. White) as they do toward humans of similar skin tones [3]. However, such an effect could be argued to be the result of social priming. Additionally, it raises the question of how people might respond to robots that are in the middle of the color spectrum (i.e., brown) and whether such effects are moderated by the perceived anthropomorphism of the robots. We conducted two experiments to first examine whether shooter bias tendencies shown towards robots is driven by social priming, and then whether diversification of robot color and level of anthropomorphism influenced shooter bias. Our results showed that shooter bias was not influenced by social priming, and interestingly, introducing a new color of robot removed shooter bias tendencies entirely. However, varying the anthropomorphism of the robots did not moderate the level of shooter bias, and contrary to our expectations, the robots were not perceived by the participants as having different levels of anthropomorphism.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {493–498},
numpages = {6},
keywords = {racism, psychology, shooter bias, anthropomorphism, robot, HRI},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314241,
author = {Jackson, Ryan Blake and Wen, Ruchen and Williams, Tom},
title = {Tact in Noncompliance: The Need for Pragmatically Apt Responses to Unethical Commands},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314241},
doi = {10.1145/3306618.3314241},
abstract = {There is a significant body of research seeking to enable moral decision making and ensure moral conduct in robots. One aspect of moral conduct is rejecting immoral human commands. For social robots, which are expected to follow and maintain human moral and sociocultural norms, it is especially important not only to engage in moral decision making, but also to properly communicate moral reasoning. We thus argue that it is critical for robots to carefully phrase command rejections. Specifically, the degree of politeness-theoretic face threat in a command rejection should be proportional to the severity of the norm violation motivating that rejection. We present a human subjects experiment showing some of the consequences of miscalibrated responses, including perceptions of the robot as inappropriately polite, direct, or harsh, and reduced robot likeability. This experiment intends to motivate and inform the design of algorithms to tactfully tune pragmatic aspects of command rejections autonomously.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {499–505},
numpages = {7},
keywords = {natural language generation, human-robot interaction, robot ethics},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314238,
author = {Hern\'{a}ndez-Orallo, Jos\'{e} and Vold, Karina},
title = {AI Extenders: The Ethical and Societal Implications of Humans Cognitively Extended by AI},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314238},
doi = {10.1145/3306618.3314238},
abstract = {Humans and AI systems are usually portrayed as separate systems that we need to align in values and goals. However, there is a great deal of AI technology found in non-autonomous systems that are used as cognitive tools by humans. Under the extended mind thesis, the functional contributions of these tools become as essential to our cognition as our brains. But AI can take cognitive extension towards totally new capabilities, posing new philosophical, ethical and technical challenges. To analyse these challenges better, we define and place AI extenders in a continuum between fully-externalized systems, loosely coupled with humans, and fully internalized processes, with operations ultimately performed by the brain, making the tool redundant. We dissect the landscape of cognitive capabilities that can foreseeably be extended by AI and examine their ethical implications.We suggest that cognitive extenders using AI be treated as distinct from other cognitive enhancers by all relevant stakeholders, including developers, policy makers, and human users.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {507–513},
numpages = {7},
keywords = {ethics of AI, AI extenders, extended mind, societal impact of AI, cognitive assistants, cognitive augmentation},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314264,
author = {Shahrdar, Shervin and Park, Corey and Nojoumian, Mehrdad},
title = {Human Trust Measurement Using an Immersive Virtual Reality Autonomous Vehicle Simulator},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314264},
doi = {10.1145/3306618.3314264},
abstract = {Recent studies indicate that people are negatively predisposed toward utilizing autonomous systems. These findings highlight the necessity of conducting research to better understand the evolution of trust between humans and growing autonomous technologies such as self-driving cars (SDC). This research presents a new approach for real-time trust measurement between passengers and SDCs. We utilized a new structured data collection approach along with a virtual reality SDC simulator to understand how various autonomous driving scenarios can increase or decrease human trust and how trust can be re-built in the case of incidental failures. To verify our methodology, we designed and conducted an empirical experiment on 50 human subjects. The results of this experiment indicated that most subjects could rebuild trust during a reasonable time frame after the system demonstrated faulty behavior. Our analysis showed that this approach is highly effective for collecting real-time data from human subjects and lays the foundation for more-involved future research in the domain of human trust and autonomous driving.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {515–520},
numpages = {6},
keywords = {human-autonomous vehicle interaction, real-time trust measurement, self-driving cars},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314228,
author = {Danks, David},
title = {The Value of Trustworthy AI},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314228},
doi = {10.1145/3306618.3314228},
abstract = {Trust is one of the most critical relations in our human lives, whether trust in one another, trust in the artifacts that we use everyday, or trust of an AI system. Even a cursory examination of the literatures in human-computer interaction, human-robot interaction, and numerous other disciplines reveals a deep, persistent concern with the nature of trust in AI, and the conditions under which it can be generated, reduced, repaired, or influenced. At a high level, we often understand trust as a relation in which the trustor makes oneself vulnerable based on positive expectations about the behavior or intentions of the trustee [1]. For example, when I trust my car to start in the morning, I make myself vulnerable (e.g., I risk that I will be late to work if it does not start) because I have the positive expectation that it actually will start. This high-level characterization is relatively unhelpful, however, particularly given the wide range of disciplines that have examined the relation of trust, ranging from organizational behavior to game theory to ethics to cognitive science. The picture that emerges from, for example, social psychology (i.e., two distinct kinds of trust depending on whether one knows the trustee's behaviors or intentions/ values) appears to be quite different from the one that emerges from moral philosophy (i.e., a single, highly-moralized notion), even though both are consistent with this high-level characterization. This talk first introduces that diversity of types of 'trust', but then argues that we can make progress towards a unified characterization by focusing on the function of trust. That is, we should ask why care whether we can trust our artifacts, AI, or fellow humans, as that can help to illuminate features of trust that are shared across domains, trustors, and trustees. I contend that one reason to desire trust is an "almost-necessary" condition on ethical action: namely, that the user has a reasonable belief that the system (whether human or machine) will behave approximately as intended. This condition is obviously not sufficient for ethical use, nor is it strictly necessary since the best available option might nonetheless be one for which the user lacks appropriate reasonable beliefs. Nonetheless, it provides a reasonable starting point for an analysis of 'trust'. More precisely, I propose that this condition indicates a role for trust as providing precisely those reasonable beliefs, at least when we have appropriately grounded trust. That is, we can understand 'appropriate trust' as obtaining when the trustor has justified beliefs that the trustee has suitable dispositions. As there is variation in the trustor's goals and values, and also the openness of the context of use, then different specific versions of 'appropriate trust' result as those variations lead to different types of focal dispositions, specific dispositions, or observability of dispositions, respectively. For example, in an open context (i.e., one where the possibilities cannot be exhaustively enumerated), the trustee's full dispositions will not be directly observable, but rather must be inferred from observations. This framework provides a unification of the different theories of 'trust' developed in different disciplines. Moreover, it provides clarity about one key function of trust, and thereby helps us to understand the value of (appropriate) trust. We need to trust our AI systems because that is a precondition for the ethical, responsible use of them.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {521–522},
numpages = {2},
keywords = {psychology, trust, philosophy, autonomous system},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314306,
author = {Jackson, Ryan Blake},
title = {Generating Appropriate Responses to Inappropriate Robot Commands},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314306},
doi = {10.1145/3306618.3314306},
abstract = {This paper describes early work at the intersection of robot ethics and natural language generation investigating two overarching questions: (1) how might current language generation algorithms generate utterances with unintended implications or otherwise accidentally alter the ecosystem of human norms, and (2) how can we design future language systems such that they purposefully influence the human normative ecosystem as productively as possible.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {523–524},
numpages = {2},
keywords = {natural language generation, human-robot interaction, robot ethics},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314307,
author = {Shvo, Maayan},
title = {Towards Empathetic Planning and Plan Recognition},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314307},
doi = {10.1145/3306618.3314307},
abstract = {Every compassionate and functioning society requires its members to have a capacity to adopt others' perspectives. As Artificial Intelligence (AI) systems are given increasingly sensitive and impactful roles in society, it is important to enable AI to wield empathy as a tool to benefit those it interacts with. In this paper, we work towards this goal by bringing together a number of important concepts: empathy, AI planning, and plan recognition (i.e., the problem of inferring an actor's plan and goal given observations about its behavior). We formalize the notions of Empathetic Planning and Empathetic Plan Recognition which are informed by the beliefs and affective state of the actor, and propose AI planning-based computational approaches. We illustrate the benefits of our approach by conducting a study with human participants.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {525–526},
numpages = {2},
keywords = {plan recognition, AI planning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314308,
author = {Michalsky, Filip},
title = {Fairness Criteria for Face Recognition Applications},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314308},
doi = {10.1145/3306618.3314308},
abstract = {Nowadays, machine learning algorithms play an important role in our daily lives and it is important to ensure their fairness and transparency. A number of methodologies for evaluating machine learning fairness have been introduced in the literature. In this research we propose a systematic confidence evaluation approach to measure fairness discrepancies of our deep learning architecture for image recognition using UTKFace database.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {527–528},
numpages = {2},
keywords = {datasets, face recognition, neural networks, fairness, machine learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314309,
author = {Abdollahpouri, Himan},
title = {Popularity Bias in Ranking and Recommendation},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314309},
doi = {10.1145/3306618.3314309},
abstract = {Many recommender systems suffer from popularity bias: popular items are recommended frequently while less popular, niche products, are recommended rarely or not at all. However, recommending the ignored products in the "long tail" is critical for businesses as they are less likely to be discovered. Popularity bias is also against social justice where the entities need to have a fair chance of being served and represented. In this work, I investigate the problem of popularity bias in recommender systems and develop algorithms to address this problem.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {529–530},
numpages = {2},
keywords = {information retrieval, recommender systems, ranking, popularity bias},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314310,
author = {Coston, Amanda},
title = {Risk Assessments and Fairness Under Missingness and Confounding},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314310},
doi = {10.1145/3306618.3314310},
abstract = {Fairness in machine learning has become a significant area of research as risk assessments and other algorithmic decision-making systems are increasingly used in high-stakes applications such as criminal justice, consumer lending, and child welfare screening decisions. Two significant challenges to achieving fair decision-making systems are 1) access to the protected attribute may be limited and 2) the outcome may be confounded or selectively observed depending on the historical data generating process. To address the former challenge, we propose two methods for overcoming limited access to the protected attribute and empirically evaluate their success on three datasets. To address the later challenge, we develop counterfactual risk assessments that account for the effect of historical interventions on the outcome. We analyze the performance of our counterfactual risk assessments in criminal sentencing decisions in Pennsylvania. We compare our model against observational risk assessments.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {531},
numpages = {1},
keywords = {machine learning, risk assessments, decision-making, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314311,
author = {Ausman, Michelle C.},
title = {Artificial Intelligence's Impact on Mental Health Treatments},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314311},
doi = {10.1145/3306618.3314311},
abstract = {An interest in artificial intelligence (AI) as a medical aid stemmed as research on mental health and psychology increased. Yet despite failing the Turing Test, AI continues to be used as a practical aid in the psychological community. From virtual reality simulations of everyday activities to robotic pet seals implemented in nursing homes, AI has found a home in the psychological field as a support for those in the medical field as well as those taking care of loved ones. In this paper, I aim to look at the stages of the Turing Test, how those are related to factoid and non-factoid questions and how current applications of AI are used in mental health treatments.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {533–534},
numpages = {2},
keywords = {psychology, mental health, artificial intelligence, therapy, robots},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314312,
author = {McNamara, Daniel},
title = {Algorithmic Stereotypes: Implications for Fairness of Generalizing from Past Data},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314312},
doi = {10.1145/3306618.3314312},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {535–536},
numpages = {2},
keywords = {stereotypes, fairness, machine learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314314,
author = {Saxena, Nripsuta Ani},
title = {Perceptions of Fairness},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314314},
doi = {10.1145/3306618.3314314},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {537–538},
numpages = {2},
keywords = {human experiments, fairness, public attitudes, algorithmic definition},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314315,
author = {Sarathy, Vasanth},
title = {Learning Context-Sensitive Norms under Uncertainty},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314315},
doi = {10.1145/3306618.3314315},
abstract = {Norms and conventions play a central role in maintaining social order in multi-agent societies [2, 5]. I study the problem of how these norms and conventions can be learned from observation of heterogeneous sources, under conditions of uncertainty. This is necessary as it is not enough to simply hard code a set of norms into a new agent prior to entering society because norms can evolve over time as agents enter and leave the society [9].},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {539–540},
numpages = {2},
keywords = {uncertainty, dempster-shafer theory, norm learning, agent-based simulation},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314316,
author = {Sokol, Kacper},
title = {Fairness, Accountability and Transparency in Artificial Intelligence: A Case Study of Logical Predictive Models},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314316},
doi = {10.1145/3306618.3314316},
abstract = {Machine learning -- the part of artificial intelligence aimed at eliciting knowledge from data and automated decision making without explicit instructions -- is making great strides, with new algorithms being invented every day. These algorithms find myriads of applications, but their ubiquity often comes at the expense of limited interpretability, hidden biases and unexpected vulnerabilities. Whenever one of these factors is a priority, the learning algorithm of choice is often a method considered to be inherently interpretable, e.g. logical models such as decision trees. In my research I challenge this assumption and highlight (quite common) cases when the assumed interpretability fails to deliver. To restore interpretability of logical machine learning models (decision trees and their ensembles in particular) I propose to explain them with class-contrastive counterfactual statements, which are a very common type of explanation in human interactions, well-grounded in social science research. To evaluate transparency of such models I collate explainability desiderata that can be used to systematically assess and compare such methods as an addition to user studies. Given contrastive explanations, I investigate their influence on the model's security, in particular gaming and stealing the model. Finally, I evaluate model fairness, where I am interested in choosing the most fair model among all the models with equal performance.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {541–542},
numpages = {2},
keywords = {transparency, logical models, accountability, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314317,
author = {Springer, Aaron},
title = {Enabling Effective Transparency: Towards User-Centric Intelligent Systems},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314317},
doi = {10.1145/3306618.3314317},
abstract = {Much of the current research in transparency and explainability is highly technical and focuses on how to derive explanations from models and algorithms. Less thought is being given to how users actually want to receive transparency and explanations from intelligent systems. My work tackles transparency and explainability from a user-centric perspective. I examine why transparency is desirable by showing that users may be susceptible to deception from intelligent systems. I demonstrate when users want transparency. Finally, my work begins to uncover how users want transparency conveyed. This body of work intends to create a path for designing transparency that puts user needs first rather than creating transparency as a convenient afterthought of model selection.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {543–544},
numpages = {2},
keywords = {error, machine learning, mood, explainability, progressive disclosure, intelligent systems, intelligibility, transparency},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314318,
author = {Perrier, Elija},
title = {AIES 2019 Student Submission},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314318},
doi = {10.1145/3306618.3314318},
abstract = {In this abstract, I intend to outline a number of concurrent multidisciplinary research programmes in which I am engaged. Firstly, I will briefly outline my current PhD research in quantum machine learning and its connections to philosophical and logical research, especially that informed by category theory. Secondly, I will detail my research regarding the role of computational complexity in frameworks for ethical artificial intelligence and algorithmic governance, drawing upon cross-disciplinary application of my computational science, physical science, economic science and legal research backgrounds.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {545–546},
numpages = {2},
keywords = {computation, ethics, regulation, algorithms, complexity},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314319,
author = {Bryant, De'Aira},
title = {Towards Emotional Intelligence in Social Robots Designed for Children},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314319},
doi = {10.1145/3306618.3314319},
abstract = {Social robots are robots designed to interact and communicate directly with humans, following traditional social norms. However, many of these current robots operate in discrete settings with predefined expectations for specific social interactions. In order for these machines to operate in the real world, they must be capable of understanding the multiple factors that contribute to human-human interaction. One such factor is emotional intelligence. Emotional intelligence allows one to consider the emotional state of another in order to motivate, plan, and achieve one's desires. One common method of analyzing the emotional state of an individual involves analyzing the emotion displayed on their face. Several artificial intelligence (AI) systems have been developed to conduct this task. These systems are often classifiers trained using a variety of machine learning techniques which require large amounts of training data. As such, they are susceptible to biases that may appear during performance analyses due to disproportions existing in training datasets. Children, in particular, are often less represented in the primary datasets of annotated faces used for training such emotion classification systems. This work seeks to first analyze the extent of these performance differences in commercial systems, then to present new computational techniques that work to mitigate some of the effects of minimal representation in datasets, and to finally present a social robot which utilizes an improved emotional AI to interact with children in various scenarios where emotional intelligence is key to successful human-robot interaction.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {547–548},
numpages = {2},
keywords = {human-robot interaction, affective computing, emotion classification, emotion recognition, algorithm transparency, children},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314320,
author = {McElfresh, Duncan C.},
title = {A Framework for Technically- and Morally-Sound AI},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314320},
doi = {10.1145/3306618.3314320},
abstract = {Artificial Intelligence (AI) ethics is by no means a new discipline; thinkers like Asimov and Philip K Dick laid the foundations of this field decades ago. Both then and today, popular dilemmas in AI ethics largely focus on artificial consciousness, artificial general intelligence, autonomous weapons, and some version of the trolley problem. While these thought experiments may prove useful in the future, modern AI applications that are in use today raise ethical dilemmas that require urgent resolution. Public outcry in response to AI in health care, criminal justice, and employment highlight the urgency of the matter. These real and imminent ethical challenges posed by AI form the basis of my dissertation research. In particular, I focus on domains where AI is necessary or inevitable -- such as kidney exchange and medical image classification -- and ethical challenges are unavoidable.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {549–550},
numpages = {2},
keywords = {social choice, moral principles, preference elicitation, AI ethics},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314321,
author = {Friedenberg, Meir},
title = {Towards Formal Models of Blameworthiness},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314321},
doi = {10.1145/3306618.3314321},
abstract = {As we move towards an era in which autonomous systems are ubiquitous, being able to reason formally about moral responsibility for outcomes will become more and more critical. My research has focused on formalizing notions of blameworthiness and responsibility. I summarize here some work by myself and others towards this end and also discuss interesting directions for future work.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {551–552},
numpages = {2},
keywords = {blameworthiness, AI in society, responsibility},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314322,
author = {Mohseni, Sina},
title = {Toward Design and Evaluation Framework for Interpretable Machine Learning Systems},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314322},
doi = {10.1145/3306618.3314322},
abstract = {The need for interpretable and accountable intelligent system gets sensible as artificial intelligence plays more role in human life. Explainable artificial intelligence systems can be a solution by self-explaining the reasoning behind the decisions and predictions of the intelligent system. My research supports the design and evaluation methods and interpretable machine learning systems and leverages knowledge and experience in the fields of machine learning, human-computer interactions, and data visualization. My research objectives are to present a design and evaluation framework for explainable artificial intelligence systems, propose new methods and metrics to better evaluate the benefits of transparent machine learning systems, and apply interpretability methods for model reliability verification.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {553–554},
numpages = {2},
keywords = {user understanding, design and evaluation framework, interpretable machine learning, user trust},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314323,
author = {Mishler, Alan},
title = {Modeling Risk and Achieving Algorithmic Fairness Using Potential Outcomes},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314323},
doi = {10.1145/3306618.3314323},
abstract = {Predictive models and algorithms are increasingly used to support human decision makers, raising concerns about how to ensure that these algorithms are fair. Additionally, these tools are generally designed to predict observable outcomes, but this is problematic when the treatment or exposure is confounded with the outcome. I argue that in most cases, what is actually of interest are potential outcomes. I contrast modeling approaches built around observable vs. potential outcomes, and I recharacterize error rate-based algorithmic fairness metrics in terms of potential outcomes. I also aim to formally model the consequences of using confounded observable predictions to drive interventions.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {555–556},
numpages = {2},
keywords = {machine learning, risk assessment, causal inference, recidivism, potential outcomes, bias, algorithmic fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314324,
author = {Delgado, Fernando A.},
title = {Machine Learning in Legal Practice: Notes from Recent History},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314324},
doi = {10.1145/3306618.3314324},
abstract = {Often framed as a relatively new and controversial phenomenon, the application of machine learning (ML) techniques to legal analysis and decision-making in the US justice system has a rich yet under examined history. My research examines how ML came to be adopted as a standard tool for automating fact discovery for high-stakes civil litigation. By analyzing the key controversies and consensuses that emerge during the experimentation and early adoption phase of this technology (2008-2015), a useful case study presents itself in which an expert professional field wrestled with the challenges of integrating ML into sensitive decision-making workflows.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {557–558},
numpages = {2},
keywords = {civil discovery, technology-assisted review, sociotechnical systems, predictive coding, applied machine learning, computational law, electronic discovery, AI governance},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3306618.3314325,
author = {Andrus, McKane},
title = {On Serving Two Masters: Directing Critical Technical Practice towards Human-Compatibility in AI},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314325},
doi = {10.1145/3306618.3314325},
abstract = {In this project I have worked towards a method for critical, socially aligned research in Artificial Intelligence by merging the analysis of conceptual commitments in technical work, discourse analysis, and critical technical practice. While the goal of critical technical practice as proposed by [1] is to overcome technical impasses, I explore an alternative use case - ensuring that technical research is aligned with social values. In the design of AI systems, we generally start with a technical formulation of a problem and then attempt to build a system that addresses that problem. Critical technical practice tells us that this technical formulation is always founded upon the discipline's core discourse and ontology, and that difficulty in solving a technical problem might just result from inconsistencies and faults in those core attributes. What I hope to show with this project is that, even when a technical problem seems solvable, critical technical practice can and should be used to ensure the human-compatibility of the technical research.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {559–560},
numpages = {2},
keywords = {discourse analysis, critical technical practice, critical HCI, shared control, human compatible AI, commitment analysis},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

