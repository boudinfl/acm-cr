@inproceedings{10.1145/3375627.3377142,
author = {Dabrock, Peter},
title = {How to Put the Data Subject's Sovereignty into Practice. Ethical Considerations and Governance Perspectives},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3377142},
doi = {10.1145/3375627.3377142},
abstract = {Ethical considerations and governance approaches of AI are at a crossroads. Either one tries to convey the impression that one can bring back a status quo ante of our given "onlife"-era [1,2], or one accepts to get responsibly involved in a digital world in which informational self-determination can no longer be safeguarded and fostered through the old fashioned data protection principles of informed consent, purpose limitation and data economy [3,4,6]. The main focus of the talk is on how under the given conditions of AI and machine learning, data sovereignty (interpreted as controllability [not control (!)] of the data subject over the use of her data throughout the entire data processing cycle [5]) can be strengthened without hindering innovation dynamics of digital economy and social cohesion of fully digitized societies. In order to put this approach into practice the talk combines a presentation of the concept of data sovereignty put forward by the German Ethics Council [3] with recent research trends in effectively applying the AI ethics principles of explainability and enforceability [4-9].},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1–2},
numpages = {2},
keywords = {controllability, ai ethics, data sovereignty, responsible governance, enforceability, explainability, german ethics council},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3377139,
author = {Gurumurthy, Anita},
title = {The AI-Development Connection - A View from the South},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3377139},
doi = {10.1145/3375627.3377139},
abstract = {The socialisation of Artificial Intelligence and the reality of an intelligence economy mark an epochal moment. The impacts of AI are now systemic - restructuring economic organisation and value chains, public sphere architectures and sociality. These shifts carry deep geo-political implications, reinforcing historical exclusions and power relations and disrupting the norms and rules that hold ideas of equality and justice together.At the centre of this rapid change is the intelligent corporation and its obsessive pursuit of data. Directly impinging on bodies and places, the de facto rules forged by the intelligent corporation are disenfranchising the already marginal subjects of development. Using trade deals to liberalise data flows, tighten trade secret rules and enclose AI-based innovation, Big Tech and their political masters have effectively taken away the economic and political autonomy of states in the global south. Big Tech's impunity extends to a brazen exploitation - enslaving labour through data over-reach and violating female bodies to universalise data markets. Thinking through the governance of AI needs new frameworks that can grapple with the fraught questions of data sovereignty, economic democracy, and institutional ethics in a global world with local aspirations. Any effort towards norm development in this domain will need to see the geo-economics of digital intelligence and the geo-politics of development ideologies as two sides of the same coin.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {3},
numpages = {1},
keywords = {digital intelligence, geo-politics of development, intelligent corporation},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3377140,
author = {McIlwain, Charlton D.},
title = {Computerize the Race Problem? Why We Must Plan for a Just AI Future},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3377140},
doi = {10.1145/3375627.3377140},
abstract = {1960s civil rights and racial justice activists tried to warn us about our technological ways, but we didn't hear them talk. The so-called wizards who stayed up late ignored or dismissed black voices, calling out from street corners to pulpits, union halls to the corridors of Congress. Instead, the men who took the first giant leaps towards conceiving and building our earliest "thinking" and "learning" machines aligned themselves with industry, government and their elite science and engineering institutions. Together, they conspired to make those fighting for racial justice the problem that their new computing machines would be designed to solve. And solve that problem they did, through color-coded, automated, and algorithmically-driven indignities and inumahities that thrive to this day. But what if yesterday's technological elite had listened to those Other voices? What if they had let them into their conversations, their classrooms, their labs, boardrooms and government task forces to help determine what new tools to build, how to build them and - most importantly - how to deploy them? What might our world look like today if the advocates for racial justice had been given the chance to frame the day's most preeminent technological question for the world and ask, "Computerize the Race Problem?" Better yet, what might our AI-driven future look like if we ask ourselves this question today?},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {4},
numpages = {1},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3377141,
author = {Neff, Gina},
title = {From Bad Users and Failed Uses to Responsible Technologies: A Call to Expand the AI Ethics Toolkit},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3377141},
doi = {10.1145/3375627.3377141},
abstract = {Recent advances in artificial intelligence applications have sparked scholarly and public attention to the challenges of the ethical design of technologies. These conversations about ethics have been targeted largely at technology designers and concerned with helping to inform building better and fairer AI tools and technologies. This approach, however, addresses only a small part of the problem of responsible use and will not be adequate for describing or redressing the problems that will arise as more types of AI technologies are more widely used.Many of the tools being developed today have potentially enormous and historic impacts on how people work, how society organises, stores and distributes information, where and how people interact with one another, and how people's work is valued and compensated. And yet, our ethical attention has looked at a fairly narrow range of questions about expanding the access to, fairness of, and accountability for existing tools. Instead, I argue that scholars should develop much broader questions of about the reconfiguration of societal power, for which AI technologies form a crucial component.This talk will argue that AI ethics needs to expand its theoretical and methodological toolkit in order to move away from prioritizing notions of good design that privilege the work of good and ethical technology designers. Instead, using approaches from feminist theory, organization studies, and science and technology, I argue for expanding how we evaluate uses of AI. This approach begins with the assumption of socially informed technological affordances, or "imagined affordances" [1] shaping how people understand and use technologies in practice. It also gives centrality to the power of social institutions for shaping technologies-in-practice.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {5–6},
numpages = {2},
keywords = {social sciences, sts, work and organizations, theory, data work, feminist theory, ai ethics},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375839,
author = {Pasquale, Frank},
title = {Machines Judging Humans: The Promise and Perils of Formalizing Evaluative Criteria},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375839},
doi = {10.1145/3375627.3375839},
abstract = {Over the past decade, algorithmic accountability has become an important concern for social scientists, computer scientists, journalists, and lawyers [1]. Expos\'{e}s have sparked vibrant debates about algorithmic sentencing. Researchers have exposed tech giants showing women ads for lower-paying jobs, discriminating against the aged, deploying deceptive dark patterns to trick consumers into buying things, and manipulating users toward rabbit holes of extremist content. Public-spirited regulators have begun to address algorithmic transparency and online fairness, building on the work of legal scholars who have called for technological due process, platform neutrality, and nondiscrimination principles.This policy work is just beginning, as experts translate academic research and activist demands into statutes and regulations. Lawmakers are proposing bills requiring basic standards of algorithmic transparency and auditing. We are starting down on a long road toward ensuring that AI-based hiring practices and financial underwriting are not used if they have a disparate impact on historically marginalized communities. And just as this "first wave" of algorithmic accountability research and activism has targeted existing systems, an emerging "second wave" of algorithmic accountability has begun to address more structural concerns. Both waves will be essential to ensure a fairer, and more genuinely emancipatory, political economy of technology. Second wave work is particularly important when it comes to illuminating the promise &amp; perils of formalizing evaluative criteria.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {7},
numpages = {1},
keywords = {accountability, emancipatory computing, fatml, bias, inequality, opportunity, ai, egalitarianism, anticipatory social research, ai for good},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375817,
author = {Avin, Shahar and Gruetzemacher, Ross and Fox, James},
title = {Exploring AI Futures Through Role Play},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375817},
doi = {10.1145/3375627.3375817},
abstract = {We present an innovative methodology for studying and teaching the impacts of AI through a role-play game. The game serves two primary purposes: 1) training AI developers and AI policy professionals to reflect on and prepare for future social and ethical challenges related to AI and 2) exploring possible futures involving AI technology development, deployment, social impacts, and governance. While the game currently focuses on the inter-relations between short-, mid- and long-term impacts of AI, it has potential to be adapted for a broad range of scenarios, exploring in greater depths issues of AI policy research and affording training within organizations. The game presented here has undergone two years of development and has been tested through over 30 events involving between 3 and 70 participants. The game is under active development, but preliminary findings suggest that role-play is a promising methodology for both exploring AI futures and training individuals and organizations in thinking about, and reflecting on, the impacts of AI and strategic mistakes that can be avoided today.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {8–14},
numpages = {7},
keywords = {ai strategy, education, forecasting, role-play, ai policy, ai futures, ai safety., ai governance},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375814,
author = {Belfield, Haydn},
title = {Activism by the AI Community: Analysing Recent Achievements and Future Prospects},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375814},
doi = {10.1145/3375627.3375814},
abstract = {The artificial intelligence (AI) community has recently engaged in activism in relation to their employers, other members of the community, and their governments in order to shape the societal and ethical implications of AI. It has achieved some notable successes, but prospects for further political organising and activism are uncertain. We survey activism by the AI community over the last six years; apply two analytical frameworks drawing upon the literature on epistemic communities, and worker organising and bargaining; and explore what they imply for the future prospects of the AI community. Success thus far has hinged on a coherent shared culture, and high bargaining power due to the high demand for a limited supply of AI 'talent'. Both are crucial to the future of AI activism and worthy of sustained attention.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {15–21},
numpages = {7},
keywords = {activism, lethal autonomous weapons, bargaining, organising, epistemic community, talent supply., ethics},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375823,
author = {Cai, William and Gaebler, Johann and Garg, Nikhil and Goel, Sharad},
title = {Fair Allocation through Selective Information Acquisition},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375823},
doi = {10.1145/3375627.3375823},
abstract = {Public and private institutions must often allocate scarce resources under uncertainty. Banks, for example, extend credit to loan applicants based in part on their estimated likelihood of repaying a loan. But when the quality of information differs across candidates (e.g., if some applicants lack traditional credit histories), common lending strategies can lead to disparities across groups. Here we consider a setting in which decision makers---before allocating resources---can choose to spend some of their limited budget further screening select individuals. We present a computationally efficient algorithm for deciding whom to screen that maximizes a standard measure of social welfare. Intuitively, decision makers should screen candidates on the margin, for whom the additional information could plausibly alter the allocation. We formalize this idea by showing the problem can be reduced to solving a series of linear programs. Both on synthetic and real-world datasets, this strategy improves utility, illustrating the value of targeted information acquisition in such decisions. Further, when there is social value for distributing resources to groups for whom we have a priori poor information---like those without credit scores---our approach can substantially improve the allocation of limited assets.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {22–28},
numpages = {7},
keywords = {algorithmic fairness, lending, information acquisition},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375813,
author = {Cave, Stephen},
title = {The Problem with Intelligence: Its Value-Laden History and the Future of AI},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375813},
doi = {10.1145/3375627.3375813},
abstract = {This paper argues that the concept of intelligence is highly value-laden in ways that impact on the field of AI and debates about its risks and opportunities. This value-ladenness stems from the historical use of the concept of intelligence in the legitimation of dominance hierarchies. The paper first provides a brief overview of the history of this usage, looking at the role of intelligence in patriarchy, the logic of colonialism and scientific racism. It then highlights five ways in which this ideological legacy might be interacting with debates about AI and its risks and opportunities: 1) how some aspects of the AI debate perpetuate the fetishization of intelligence; 2) how the fetishization of intelligence impacts on diversity in the technology industry; 3) how certain hopes for AI perpetuate notions of technology and the mastery of nature; 4) how the association of intelligence with the professional class misdirects concerns about AI; and 5) how the equation of intelligence and dominance fosters fears of superintelligence. This paper therefore takes a first step in bringing together the literature on intelligence testing, eugenics and colonialism from a range of disciplines with that on the ethics and societal impact of AI.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {29–35},
numpages = {7},
keywords = {iq, history of intelligence, eugenics, racism, intelligence, artificial intelligence, ethics of ai},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375826,
author = {Das, Subhro and Steffen, Sebastian and Clarke, Wyatt and Reddy, Prabhat and Brynjolfsson, Erik and Fleming, Martin},
title = {Learning Occupational Task-Shares Dynamics for the Future of Work},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375826},
doi = {10.1145/3375627.3375826},
abstract = {The recent wave of AI and automation has been argued to differ from previous General Purpose Technologies (GPTs), in that it may lead to rapid change in occupations' underlying task requirements and persistent technological unemployment. In this paper, we apply a novel methodology of dynamic task shares to a large dataset of online job postings to explore how exactly occupational task demands have changed over the past decade of AI innovation, especially across high, mid and low wage occupations. Notably, big data and AI have risen significantly among high wage occupations since 2012 and 2016, respectively. We built an ARIMA model to predict future occupational task demands and showcase several relevant examples in Healthcare, Administration, and IT. Such task demands predictions across occupations will play a pivotal role in retraining the workforce of the future.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {36–42},
numpages = {7},
keywords = {future of work, occupational task demands, ai, automation},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375829,
author = {Davoust, Alan and Rovatsos, Michael},
title = {Social Contracts for Non-Cooperative Games},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375829},
doi = {10.1145/3375627.3375829},
abstract = {In future agent societies, we might see AI systems engaging in selfish, calculated behavior, furthering their owners' interests instead of socially desirable outcomes. How can we promote morally sound behaviour in such settings, in order to obtain more desirable outcomes? A solution from moral philosophy is the concept of a social contract, a set of rules that people would voluntarily commit to in order to obtain better outcomes than those brought by anarchy. We adapt this concept to a game-theoretic setting, to systematically modify the payoffs of a non-cooperative game, so that agents will rationally pursue socially desirable outcomes. We show that for any game, a suitable social contract can be designed to produce an optimal outcome in terms of social welfare. We then investigate the limitations of applying this approach to alternative moral objectives, and establish that, for any alternative moral objective that is significantly different from social welfare, there are games for which no such social contract will be feasible that produces non-negligible social benefit compared to collective selfish behaviour.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {43–49},
numpages = {7},
keywords = {game theory, agents, moral philosophy, ethics},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375806,
author = {Erd\'{e}lyi, Olivia J. and Erd\'{e}lyi, G\'{a}bor},
title = {The AI Liability Puzzle and a Fund-Based Work-Around},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375806},
doi = {10.1145/3375627.3375806},
abstract = {Certainty around the regulatory environment is crucial to facilitate responsible AI innovation and its social acceptance. However, the existing legal liability system is inapt to assign responsibility where a potentially harmful conduct and/or the harm itself are unforeseeable, yet some instantiations of AI and/or the harms they may trigger are not foreseeable in the legal sense. The unpredictability of how courts would handle such cases makes the risks involved in the investment and use of AI incalculable, creating an environment that is not conducive to innovation and may deprive society of some benefits AI could provide. To tackle this problem, we propose to draw insights from financial regulatory best-practices and establish a system of AI guarantee schemes. We envisage the system to form part of the broader market-structuring regulatory framework, with the primary function to provide a readily available, clear, and transparent funding mechanism to compensate claims that are either extremely hard or impossible to realize via conventional litigation. We propose at least partial industry-funding, with funding arrangements depending on whether it would pursue other potential policy goals.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {50–56},
numpages = {7},
keywords = {guarantee schemes, ai liability},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375828,
author = {Fazelpour, Sina and Lipton, Zachary C.},
title = {Algorithmic Fairness from a Non-Ideal Perspective},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375828},
doi = {10.1145/3375627.3375828},
abstract = {Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In the hopes of mitigating these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might hope to observe in a fair world, offering a variety of algorithms that attempt to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to fair machine learning to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and ideal worlds. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of their actions, naive applications of ideal thinking can lead to misguided policies. In this paper, we demonstrate a connection between the recent literature on fair machine learning and the ideal approach in political philosophy, and show that some recently uncovered shortcomings in proposed algorithms reflect broader troubles faced by the ideal approach. We work this analysis through for different formulations of fairness and conclude with a critical discussion of real-world impacts and directions for new research.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {57–63},
numpages = {7},
keywords = {political philosophy, justice, algorithmic decision-making, fairness in machine learning},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375822,
author = {Jung, Jongbin and Shroff, Ravi and Feller, Avi and Goel, Sharad},
title = {Bayesian Sensitivity Analysis for Offline Policy Evaluation},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375822},
doi = {10.1145/3375627.3375822},
abstract = {On a variety of complex decision-making tasks, from doctors prescribing treatment to judges setting bail, machine learning algorithms have been shown to outperform expert human judgments. One complication, however, is that it is often difficult to anticipate the effects of algorithmic policies prior to deployment, as one generally cannot use historical data to directly observe what would have happened had the actions recommended by the algorithm been taken. A common strategy is to model potential outcomes for alternative decisions assuming that there are no unmeasured confounders (i.e., to assume ignorability). But if this ignorability assumption is violated, the predicted and actual effects of an algorithmic policy can diverge sharply. In this paper we present a flexible Bayesian approach to gauge the sensitivity of predicted policy outcomes to unmeasured confounders. In particular, and in contrast to past work, our modeling framework easily enables confounders to vary with the observed covariates. We demonstrate the efficacy of our method on a large dataset of judicial actions, in which one must decide whether defendants awaiting trial should be required to pay bail or can be released without payment.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {64–70},
numpages = {7},
keywords = {offline policy evaluation, pretrial risk assessment, sensitivity to unmeasured confounding},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375809,
author = {Kazimzade, Gunay and Miceli, Milagros},
title = {Biased Priorities, Biased Outcomes: Three Recommendations for Ethics-Oriented Data Annotation Practices},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375809},
doi = {10.1145/3375627.3375809},
abstract = {In this paper, we analyze the relation between data-related biases and practices of data annotation, by placing them in the context of market economy. We understand annotation as a praxis related to the sensemaking of data and investigate annotation practices for vision models by focusing on the values that are prioritized by industrial decision-makers and practitioners. The quality of data is critical for machine learning models as it holds the power to (mis-)represent the population it is intended to analyze. For autonomous systems to be able to make sense of the world, humans first need to make sense of the data these systems will be trained on. This paper addresses this issue, guided by the following research questions: Which goals are prioritized by decision-makers at the data annotation stage? How do these priorities correlate with data-related bias issues? Focusing on work practices and their context, our research goal aims at understanding the logics driving companies and their impact on the performed annotations. The study follows a qualitative design and is based on 24 interviews with relevant actors and extensive participatory observations, including several weeks of fieldwork at two companies dedicated to data annotation for vision models in Buenos Aires, Argentina and Sofia, Bulgaria. The prevalence of market-oriented values over socially responsible approaches is argued based on three corporate priorities that inform work practices in this field and directly shape the annotations performed: profit (short deadlines connected to the strive for profit are prioritized over alternative approaches that could prevent biased outcomes), standardization (the strive for standardized and, in many cases, reductive or biased annotations to make data fit the products and revenue plans of clients), and opacity (related to client's power to impose their criteria on the annotations that are performed. Criteria that most of the times remain opaque due to corporate confidentiality). Finally, we introduce three elements, aiming at developing ethics-oriented practices of data annotation, that could help prevent biased outcomes: transparency (regarding the documentation of data transformations, including information on responsibilities and criteria for decision-making.), education (training on the potential harms caused by AI and its ethical implications, that could help data annotators and related roles adopt a more critical approach towards the interpretation and labeling of data), and regulations (clear guidelines for ethical AI developed at the governmental level and applied both in private and public organizations).},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {71},
numpages = {1},
keywords = {annotation, bias, data, ethics, power, classification, priority, transparency, profit},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375835,
author = {Krafft, P. M. and Young, Meg and Katell, Michael and Huang, Karen and Bugingo, Ghislain},
title = {Defining AI in Policy versus Practice},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375835},
doi = {10.1145/3375627.3375835},
abstract = {Recent concern about harms of information technologies motivate consideration of regulatory action to forestall or constrain certain developments in the field of artificial intelligence (AI). However, definitional ambiguity hampers the possibility of conversation about this urgent topic of public concern. Legal and regulatory interventions require agreed-upon definitions, but consensus around a definition of AI has been elusive, especially in policy conversations. With an eye towards practical working definitions and a broader understanding of positions on these issues, we survey experts and review published policy documents to examine researcher and policy-maker conceptions of AI. We find that while AI researchers favor definitions of AI that emphasize technical functionality, policy-makers instead use definitions that compare systems to human thinking and behavior. We point out that definitions adhering closely to the functionality of AI systems are more inclusive of technologies in use today, whereas definitions that emphasize human-like capabilities are most applicable to hypothetical future technologies. As a result of this gap, ethical and regulatory efforts may overemphasize concern about future technologies at the expense of pressing issues with existing deployed technologies.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {72–78},
numpages = {7},
keywords = {policy, definitions, artificial intelligence, sociotechnical imaginaries},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375833,
author = {Lakkaraju, Himabindu and Bastani, Osbert},
title = {"How Do I Fool You?": Manipulating User Trust via Misleading Black Box Explanations},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375833},
doi = {10.1145/3375627.3375833},
abstract = {As machine learning black boxes are increasingly being deployed in critical domains such as healthcare and criminal justice, there has been a growing emphasis on developing techniques for explaining these black boxes in a human interpretable manner. There has been recent concern that a high-fidelity explanation of a black box ML model may not accurately reflect the biases in the black box. As a consequence, explanations have the potential to mislead human users into trusting a problematic black box. In this work, we rigorously explore the notion of misleading explanations and how they influence user trust in black box models. Specifically, we propose a novel theoretical framework for understanding and generating misleading explanations, and carry out a user study with domain experts to demonstrate how these explanations can be used to mislead users. Our work is the first to empirically establish how user trust in black box models can be manipulated via misleading explanations.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {79–85},
numpages = {7},
keywords = {user trust in machine learning, black box explanations, model interpretability, user studies},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375808,
author = {Leben, Derek},
title = {Normative Principles for Evaluating Fairness in Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375808},
doi = {10.1145/3375627.3375808},
abstract = {There are many incompatible ways to measure fair outcomes for machine learning algorithms. The goal of this paper is to characterize rates of success and error across protected groups (race, gender, sexual orientation) as a distribution problem, and describe the possible solutions to this problem according to different normative principles from moral and political philosophy. These normative principles are based on various competing attributes within a distribution problem: intentions, compensation, desert, consent, and consequences. Each principle will be applied to a sample risk-assessment classifier to demonstrate the philosophical arguments underlying different sets of fairness metrics.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {86–92},
numpages = {7},
keywords = {discrimination, fairness, algorithmic decision-making, machine learning, political philosophy},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375821,
author = {Lu, Joy and Lee, Dokyun (DK) and Kim, Tae Wan and Danks, David},
title = {Good Explanation for Algorithmic Transparency},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375821},
doi = {10.1145/3375627.3375821},
abstract = {Machine learning algorithms have gained widespread usage across a variety of domains, both in providing predictions to expert users and recommending decisions to everyday users. However, these AI systems are often black boxes, and end-users are rarely provided with an explanation. The critical need for explanation by AI systems has led to calls for algorithmic transparency, including the "right to explanation'' in the EU General Data Protection Regulation (GDPR). These initiatives presuppose that we know what constitutes a meaningful or good explanation, but there has actually been surprisingly little research on this question in the context of AI systems. In this paper, we (1) develop a generalizable framework grounded in philosophy, psychology, and interpretable machine learning to investigate and define characteristics of good explanation, and (2) conduct a large-scale lab experiment to measure the impact of different factors on people's perceptions of understanding, usage intention, and trust of AI systems. The framework and study together provide a concrete guide for managers on how to present algorithmic prediction rationales to end-users to foster trust and adoption, and elements of explanation and transparency to be considered by AI researchers and engineers in designing, developing, and deploying transparent or explainable algorithms.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {93},
numpages = {1},
keywords = {explainable ai, interpretable ai, lab experiments},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375831,
author = {Mart\'{\i}nez-Plumed, Fernando and Tolan, Song\"{u}l and Pesole, Annarosa and Hern\'{a}ndez-Orallo, Jos\'{e} and Fern\'{a}ndez-Mac\'{\i}as, Enrique and G\'{o}mez, Emilia},
title = {Does AI Qualify for the Job? A Bidirectional Model Mapping Labour and AI Intensities},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375831},
doi = {10.1145/3375627.3375831},
abstract = {In this paper we present a setting for examining the relation be-tween the distribution of research intensity in AI research and the relevance for a range of work tasks (and occupations) in current and simulated scenarios. We perform a mapping between labourand AI using a set of cognitive abilities as an intermediate layer. This setting favours a two-way interpretation to analyse (1) what impact current or simulated AI research activity has or would have on labour-related tasks and occupations, and (2) what areas of AI research activity would be responsible for a desired or undesired effect on specific labour tasks and occupations. Concretely, in our analysis we map 59 generic labour-related tasks from several worker surveys and databases to 14 cognitive abilities from the cognitive science literature, and these to a comprehensive list of 328 AI benchmarks used to evaluate progress in AI techniques. We provide this model and its implementation as a tool for simulations. We also show the effectiveness of our setting with some illustrative examples.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {94–100},
numpages = {7},
keywords = {ai intensity, tasks, ai benchmarks, labour market, simulation, ai impact},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375805,
author = {Martinho, Andreia and Kroesen, Maarten and Chorus, Caspar},
title = {An Empirical Approach to Capture Moral Uncertainty in AI},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375805},
doi = {10.1145/3375627.3375805},
abstract = {As AI Systems become increasingly autonomous they are expected to engage in complex moral decision-making processes. For the purpose of guidance of such processes theoretical and empirical solutions have been sought. In this research we integrate both theoretical and empirical lines of thought to address the matters of moral reasoning in AI Systems. We reconceptualize a metanormative framework for decision-making under moral uncertainty within the Discrete Choice Analysis domain and we operationalize it through a latent class choice model. The discrete choice analysis-based formulation of the metanormative framework is theory-rooted and practical as it captures moral uncertainty through a small set of latent classes. To illustrate our approach we conceptualize a society in which AI Systems are in charge of making policy choices. In the proof of concept two AI systems make policy choices on behalf of a society but while one of the systems uses a baseline moral certain model the other uses a moral uncertain model. It was observed that there are cases in which the AI Systems disagree about the policy to be chosen which we believe is an indication about the relevance of moral uncertainty.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {101},
numpages = {1},
keywords = {moral uncertainty, metanormative theory, artificial intelligence, morality},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375807,
author = {Matthews, Jeanna Neefe and Northup, Graham and Grasso, Isabella and Lorenz, Stephen and Babaeianjelodar, Marzieh and Bashaw, Hunter and Mondal, Sumona and Matthews, Abigail and Njie, Mariama and Goldthwaite, Jessica},
title = {When Trusted Black Boxes Don't Agree: Incentivizing Iterative Improvement and Accountability in Critical Software Systems},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375807},
doi = {10.1145/3375627.3375807},
abstract = {Software increasingly plays a key role in regulated areas like housing, hiring, and credit, as well as major public functions such as criminal justice and elections. It is easy for there to be unintended defects with a large impact on the lives of individuals and society as a whole. Preventing, finding, and fixing software defects is a key focus of both industrial software development efforts as well as academic research in software engineering. In this paper, we discuss flaws in the larger socio-technical decision-making processes in which critical black-box software systems are developed, deployed, and trusted. We use criminal justice software, specifically probabilistic genotyping (PG) software, as a concrete example. We describe how PG software systems, designed to do the same job, produce different results. We highlight the under-appreciated impact of changes in key parameters and the disparate impact that one such parameter can have on different racial/ethnic groups. We propose concrete changes to the socio-technical decision-making processes surrounding the use of PG software that could be used to incentivize iterative improvements in the accuracy, fairness, reliability, and accountability of these systems.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {102–108},
numpages = {7},
keywords = {algorithmic accountability, probabilistic genotyping, disparate impact, software verification, criminal justice software},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375824,
author = {McCradden, Melissa and Mazwi, Mjaye and Joshi, Shalmali and Anderson, James A.},
title = {When Your Only Tool Is A Hammer: Ethical Limitations of Algorithmic Fairness Solutions in Healthcare Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375824},
doi = {10.1145/3375627.3375824},
abstract = {It is no longer a hypothetical worry that artificial intelligence - more specifically, machine learning (ML) - can propagate the effects of pernicious bias in healthcare. To address these problems, some have proposed the development of 'algorithmic fairness' solutions. The primary goal of these solutions is to constrain the effect of pernicious bias with respect to a given outcome of interest as a function of one's protected identity (i.e., characteristics generally protected by civil or human rights legislation. The technical limitations of these solutions have been well-characterized. Ethically, the problematic implication - of developers, potentially, and end users - is that by virtue of algorithmic fairness solutions a model can be rendered 'objective' (i.e., free from the influence of pernicious bias). The ostensible neutrality of these solutions may unintentionally prompt new consequences for vulnerable groups by obscuring downstream problems due to the persistence of real-world bias.The main epistemic limitation of algorithmic fairness is that it assumes the relationship between the extent of bias's impact on a given health outcome and one's protected identity is mathematically quantifiable. The reality is that social and structural factors confluence in complex and unknown ways to produce health inequalities. Some of these are biologic in nature, and differences like these are directly relevant to predicting a health event and should be incorporated into the model's design. Others are reflective of prejudice, lack of access to healthcare, or implicit bias. Sometimes, there may be a combination. With respect to any specific task, it is difficult to untangle the complex relationships between potentially influential factors and which ones are 'fair' and which are not to inform their inclusion or mitigation in the model's design.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {109},
numpages = {1},
keywords = {healthcare, ethics, discrimination, sexism, racism, bias, machine learning, bioethics, medicine, algorithmic fairness},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375811,
author = {McKee, Heidi A. and Porter, James E.},
title = {Ethics for AI Writing: The Importance of Rhetorical Context},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375811},
doi = {10.1145/3375627.3375811},
abstract = {Implicit in any rhetorical interaction-between humans or between humans and machines-are ethical codes that shape the rhetorical context, the social situation in which communication happens and also the engine that drives communicative interaction. Such implicit codes are usually invisible to AI writing systems because the social factors shaping communication (the why and how of language, not the what) are not usually explicitly evident in databases the systems use to produce discourse. Can AI writing systems learn to learn rhetorical context, particularly the implicit codes for communication ethics? We see evidence that some systems do address issues of rhetorical context, at least in rudimentary ways. But we critique the information transfer communication model supporting many AI writing systems, arguing for a social context model that accounts for rhetorical context-what is, in a sense, "not there" in the data corpus but that is critical for the production of meaningful, significant, and ethical communication. We offer two ethical principles to guide design of AI writing systems: transparency about machine presence and critical data awareness, a methodological reflexivity about rhetorical context and omissions in the data that need to be provided by a human agent or accounted for in machine learning.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {110–116},
numpages = {7},
keywords = {text generation, language models, machine ethics, rhetoric, rhetorical context, ethics, critical date awareness, information transfer model, ai writing systems, transparency, communication ethics, social context model},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375832,
author = {Mitchell, Margaret and Baker, Dylan and Moorosi, Nyalleng and Denton, Emily and Hutchinson, Ben and Hanna, Alex and Gebru, Timnit and Morgenstern, Jamie},
title = {Diversity and Inclusion Metrics in Subset Selection},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375832},
doi = {10.1145/3375627.3375832},
abstract = {The ethical concept of fairness has recently been applied in machine learning (ML) settings to describe a wide range of constraints and objectives. When considering the relevance of ethical concepts to subset selection problems, the concepts of diversity and inclusion are additionally applicable in order to create outputs that account for social power and access differentials. We introduce metrics based on these concepts, which can be applied together, separately, and in tandem with additional fairness constraints. Results from human subject experiments lend support to the proposed criteria. Social choice methods can additionally be leveraged to aggregate and choose preferable sets, and we detail how these may be applied.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {117–123},
numpages = {7},
keywords = {subset selection, machine learning fairness, diversity and inclusion},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375825,
author = {Nahian, Md Sultan Al and Frazier, Spencer and Riedl, Mark and Harrison, Brent},
title = {Learning Norms from Stories: A Prior for Value Aligned Agents},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375825},
doi = {10.1145/3375627.3375825},
abstract = {Value alignment is a property of an intelligent agent indicating that it can only pursue goals and activities that are beneficial to humans. Traditional approaches to value alignment use imitation learning or preference learning to infer the values of humans by observing their behavior. We introduce a complementary technique in which a value-aligned prior is learned from naturally occurring stories which encode societal norms. Training data is sourced from the children's educational comic strip, Goofus &amp; Gallant. In this work, we train multiple machine learning models to classify natural language descriptions of situations found in the comic strip as normative or non-normative by identifying if they align with the main characters' behavior. We also report the models' performance when transferring to two unrelated tasks with little to no additional training on the new task.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {124–130},
numpages = {7},
keywords = {value alignment, natural language processing, learning from stories},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375818,
author = {Nanda, Vedant and Xu, Pan and Sankararaman, Karthik Abinav and Dickerson, John P. and Srinivasan, Aravind},
title = {Balancing the Tradeoff between Profit and Fairness in Rideshare Platforms during High-Demand Hours},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375818},
doi = {10.1145/3375627.3375818},
abstract = {Rideshare platforms, when assigning requests to drivers, tend to maximize profit for the system and/or minimize waiting time for riders. Such platforms can exacerbate biases that drivers may have over certain types of requests. We consider the case of peak hours when the demand for rides is more than the supply of drivers. Drivers are well aware of their advantage during the peak hours and can choose to be selective about which rides to accept. Moreover, if in such a scenario, the assignment of requests to drivers (by the platform) is made only to maximize profit and/or minimize wait time for riders, requests of a certain type (e.g., from a non-popular pickup location, or to a non-popular drop-off location) might never be assigned to a driver. Such a system can be highly unfair to riders. However, increasing fairness might come at a cost of the overall profit made by the rideshare platform. To balance these conflicting goals, we present a flexible, non-adaptive algorithm, NAdap, that allows the platform designer to control the profit and fairness of the system via parameters α and β respectively.We model the matching problem as an online bipartite matching where the set of drivers is offline and requests arrive online. Upon the arrival of a request, we use NAdap to assign it to a driver (the driver might then choose to accept or reject it) or reject the request. We formalize the measures of profit and fairness in our setting and show that by using NAdap, the competitive ratios for profit and fairness measures would be no worse than α/e and β/e respectively. Extensive experimental results on both real-world and synthetic datasets confirm the validity of our theoretical lower bounds. Additionally, they show that NAdap under some choice of (α, β) can beat two natural heuristics, Greedy and Uniform, on both fairness and profit. Code is available at: https://github.com/nvedant07/rideshare-fairness-peak/. Full paper can be found in the proceedings of AAAI 2020 and on ArXiv: http://arxiv.org/abs/1912.08388).},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {131},
numpages = {1},
keywords = {rideshare, fairness, optimization, matching},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375834,
author = {Osoba, Osonde A.},
title = {Technocultural Pluralism: A "Clash of Civilizations" in Technology?},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375834},
doi = {10.1145/3375627.3375834},
abstract = {At the end of the Cold War, the renowned political scientist, Samuel Huntington, argued that future conflicts were more likely to stem from cultural frictions -- ideologies, social norms, and political systems -- rather than political or economic frictions. Huntington focused his concern on the future of geopolitics in a rapidly shrinking world. This paper argues that a similar dynamic is at play in the interaction of technology cultures. We emphasize the role of culture in the evolution of technology and identify the particular role that culture (esp. privacy culture) plays in the development of AI/ML technologies. Then we examine some implications that this perspective brings to the fore.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {132–137},
numpages = {6},
keywords = {cultural evolution, government regulation, data privacy},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375803,
author = {Prunkl, Carina and Whittlestone, Jess},
title = {Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375803},
doi = {10.1145/3375627.3375803},
abstract = {One way of carving up the broad 'AI ethics and society' research space that has emerged in recent years is to distinguish between 'near-term' and 'long-term' research. While such ways of breaking down the research space can be useful, we put forward several concerns about the near/long-term distinction gaining too much prominence in how research questions and priorities are framed. We highlight some ambiguities and inconsistencies in how the distinction is used, and argue that while there are differing priorities within this broad research community, these differences are not well-captured by the near/long-term distinction. We unpack the near/long-term distinction into four different dimensions, and propose some ways that researchers can communicate more clearly about their work and priorities using these dimensions. We suggest that moving towards a more nuanced conversation about research priorities can help establish new opportunities for collaboration, aid the development of more consistent and coherent research agendas, and enable identification of previously neglected research areas.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {138–143},
numpages = {6},
keywords = {ai ethics and society, ai policy, artificial intelligence, ai ethics},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375816,
author = {Qadri, Rida},
title = {Algorithmized but Not Atomized? How Digital Platforms Engender New Forms of Worker Solidarity in Jakarta},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375816},
doi = {10.1145/3375627.3375816},
abstract = {Jakarta's roads are green, filled as they are with the fluorescent green jackets, bright green logos and fluttering green banners of basecamps created by the city's digitized, 'online' motorbike-taxi drivers (ojol). These spaces function as waiting posts, regulatory institutions, information networks and spaces of solidarity for the ojol working for mobility-app companies, Grab and GoJek. Their existence though, presents a puzzle. In the world of on-demand matching, literature either predicts an isolated, atomized, disempowered digital worker or expects workers to have only temporary, online, ephemeral networks of mutual aid. Yet, Jakarta's ojol then introduce us to a new form of labor action that relies on an interface of the physical world and digital realm, complete with permanent shelters, quirky names, emblems, social media accounts and even their own emergency response service. This paper explores the contours of these labor formations and asks why digital workers in Jakarta are able to create collective structures of solidarity, even as app-mediated work may force them towards an individualized labor regime? I argue that these digital labor collectives are not accidental but a product of interactions between histories of social organization structures in Jakarta and affordances created by technological-mediation. Through participant observation and semi-structured interviews I excavate the bi-directional conversation between globalizing digital platforms and social norms, civic culture and labor market conditions in Jakarta which has allowed for particular forms of digital worker resistances to emerge. I recover power for the digital worker, who provides us with a path to resisting algorithmization of work while still participating in it through agentic labor actions rooted in shared identities, enabled by technological fluency and borne out of a desire for community.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {144},
numpages = {1},
keywords = {labor solidarity, mobility platforms, digital labor, global south},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375820,
author = {Raji, Inioluwa Deborah and Gebru, Timnit and Mitchell, Margaret and Buolamwini, Joy and Lee, Joonseok and Denton, Emily},
title = {Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375820},
doi = {10.1145/3375627.3375820},
abstract = {Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of fiveethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {145–151},
numpages = {7},
keywords = {facial recognition, auditing, benchmarking, evaluation, algorithms, ethics},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375819,
author = {Saha, Debjani and Schumann, Candice and McElfresh, Duncan C. and Dickerson, John P. and Mazurek, Michelle L. and Tschantz, Michael Carl},
title = {Human Comprehension of Fairness in Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375819},
doi = {10.1145/3375627.3375819},
abstract = {Bias in machine learning has manifested injustice in several areas, with notable examples including gender bias in job-related ads [4], racial bias in evaluating names on resumes [3], and racial bias in predicting criminal recidivism [1]. In response, research into algorithmic fairness has grown in both importance and volume over the past few years. Different metrics and approaches to algorithmic fairness have been proposed, many of which are based on prior legal and philosophical concepts [2]. The rapid expansion of this field makes it difficult for professionals to keep up, let alone the general public. Furthermore, misinformation about notions of fairness can have significant legal implications.Computer scientists have largely focused on developing mathematical notions of fairness and incorporating them in fielded ML systems. A much smaller collection of studies has measured public perception of bias and (un)fairness in algorithmic decision-making. However, one major question underlying the study of ML fairness remains unanswered in the literature: Does the general public understand mathematical definitions of ML fairness and their behavior in ML applications? We take a first step towards answering this question by studying non-expert comprehension and perceptions of one popular definition of ML fairness, demographic parity [5]. Specifically, we developed an online survey to address the following: (1) Does a non-technical audience comprehend the definition and implications of demographic parity? (2) Do demographics play a role in comprehension? (3) How are comprehension and sentiment related? (4) Does the application scenario affect comprehension?},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {152},
numpages = {1},
keywords = {fair machine learning, empirical study, algorithmic bias, human-computer interaction},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375804,
author = {Schiff, Daniel and Biddle, Justin and Borenstein, Jason and Laas, Kelly},
title = {What's Next for AI Ethics, Policy, and Governance? A Global Overview},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375804},
doi = {10.1145/3375627.3375804},
abstract = {Since 2016, more than 80 AI ethics documents - including codes, principles, frameworks, and policy strategies - have been produced by corporations, governments, and NGOs. In this paper, we examine three topics of importance related to our ongoing empirical study of ethics and policy issues in these emerging documents. First, we review possible challenges associated with the relative homogeneity of the documents' creators. Second, we provide a novel typology of motivations to characterize both obvious and less obvious goals of the documents. Third, we discuss the varied impacts these documents may have on the AI governance landscape, including what factors are relevant to assessing whether a given document is likely to be successful in achieving its goals.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {153–158},
numpages = {6},
keywords = {ai policy, ai ethics, corporate social responsibility},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375802,
author = {Schutzman, Zachary},
title = {Trade-Offs in Fair Redistricting},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375802},
doi = {10.1145/3375627.3375802},
abstract = {What constitutes a 'fair' electoral districting plan is a discussion dating back to the founding of the United States and, in light of several recent court cases, mathematical developments, and the approaching 2020 U.S. Census, is still a fiercely debated topic today. In light of the growing desire and ability to use algorithmic tools in drawing these districts, we discuss two prototypical formulations of fairness in this domain: drawing the districts by a neutral procedure or drawing them to intentionally induce an equitable electoral outcome. We then generate a large sample of districting plans for North Carolina and Pennsylvania and consider empirically how compactness and partisan symmetry, as instantiations of these frameworks, trade off with each other -- prioritizing the value of one of these necessarily comes at a cost in the other.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {159–165},
numpages = {7},
keywords = {compactness, pareto frontier, fairness, pareto-optimal, markov chain monte carlo, redistricting, gerrymandering, partisan symmetry},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375812,
author = {Sharma, Shubham and Henderson, Jette and Ghosh, Joydeep},
title = {CERTIFAI: A Common Framework to Provide Explanations and Analyse the Fairness and Robustness of Black-Box Models},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375812},
doi = {10.1145/3375627.3375812},
abstract = {Concerns within the machine learning community and external pressures from regulators over the vulnerabilities of machine learning algorithms have spurred on the fields of explainability, robustness, and fairness. Often, issues in explainability, robustness, and fairness are confined to their specific sub-fields and few tools exist for model developers to use to simultaneously build their modeling pipelines in a transparent, accountable, and fair way. This can lead to a bottleneck on the model developer's side as they must juggle multiple methods to evaluate their algorithms. In this paper, we present a single framework for analyzing the robustness, fairness, and explainability of a classifier. The framework, which is based on the generation of counterfactual explanations through a custom genetic algorithm, is flexible, model-agnostic, and does not require access to model internals. The framework allows the user to calculate robustness and fairness scores for individual models and generate explanations for individual predictions which provide a means for actionable recourse (changes to an input to help get a desired outcome). This is the first time that a unified tool has been developed to address three key issues pertaining towards building a responsible artificial intelligence system.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {166–172},
numpages = {7},
keywords = {robust-ness, responsible artificial intelligence, explainability, machine learning, fairness},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375815,
author = {Shevlane, Toby and Dafoe, Allan},
title = {The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375815},
doi = {10.1145/3375627.3375815},
abstract = {There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {173–179},
numpages = {7},
keywords = {misuse of ai, ai governance, publication norms, disclosure of research},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375830,
author = {Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
title = {Fooling LIME and SHAP: Adversarial Attacks on Post Hoc Explanation Methods},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375830},
doi = {10.1145/3375627.3375830},
abstract = {As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {180–186},
numpages = {7},
keywords = {black box explanations, bias detection, adversarial attacks, model interpretability},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375827,
author = {Zhang, Baobao and Dafoe, Allan},
title = {U.S. Public Opinion on the Governance of Artificial Intelligence},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375827},
doi = {10.1145/3375627.3375827},
abstract = {Artificial intelligence (AI) has widespread societal implications, yet social scientists are only beginning to study public attitudes toward the technology. Existing studies find that the public's trust in institutions can play a major role in shaping the regulation of emerging technologies. Using a large-scale survey (N=2000), we examined Americans' perceptions of 13 AI governance challenges as well as their trust in governmental, corporate, and multistakeholder institutions to responsibly develop and manage AI. While Americans perceive all of the AI governance issues to be important for tech companies and governments to manage, they have only low to moderate trust in these institutions to manage AI applications.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {187–193},
numpages = {7},
keywords = {ai governance, public trust, public opinion},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375810,
author = {Zhou, Yishan and Danks, David},
title = {Different "Intelligibility" for Different Folks},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375810},
doi = {10.1145/3375627.3375810},
abstract = {Many arguments have concluded that our autonomous technologies must be intelligible, interpretable, or explainable, even if that property comes at a performance cost. In this paper, we consider the reasons why some property like these might be valuable, we conclude that there is not simply one kind of 'intelligibility', but rather different types for different individuals and uses. In particular, different interests and goals require different types of intelligibility (or explanations, or other related notion). We thus provide a typography of 'intelligibility' that distinguishes various notions, and draw methodological conclusions about how autonomous technologies should be designed and deployed in different ways, depending on whose intelligibility is required.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {194–199},
numpages = {6},
keywords = {prediction algorithms, explainability, intelligibility},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375871,
author = {Alvero, A.J. and Arthurs, Noah and antonio, anthony lising and Domingue, Benjamin W. and Gebre-Medhin, Ben and Giebel, Sonia and Stevens, Mitchell L.},
title = {AI and Holistic Review: Informing Human Reading in College Admissions},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375871},
doi = {10.1145/3375627.3375871},
abstract = {College admissions in the United States is carried out by a human-centered method of evaluation known as holistic review, which typically involves reading original narrative essays submitted by each applicant. The legitimacy and fairness of holistic review, which gives human readers significant discretion over determining each applicant's fitness for admission, has been repeatedly challenged in courtrooms and the public sphere. Using a unique corpus of 283,676 application essays submitted to a large, selective, state university system between 2015 and 2016, we assess the extent to which applicant demographic characteristics can be inferred from application essays. We find a relatively interpretable classifier (logistic regression) was able to predict gender and household income with high levels of accuracy. Findings suggest that data auditing might be useful in informing holistic review, and perhaps other evaluative systems, by checking potential bias in human or computational readings.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {200–206},
numpages = {7},
keywords = {holistic review, data auditing, college admissions, text analysis, natural language processing, bias, supervised learning, fairness},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375855,
author = {Birhane, Abeba and van Dijk, Jelle},
title = {Robot Rights? Let's Talk about Human Welfare Instead},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375855},
doi = {10.1145/3375627.3375855},
abstract = {The 'robot rights' debate, and its related question of 'robot responsibility', invokes some of the most polarized positions in AI ethics. While some advocate for granting robots rights on a par with human beings, others, in a stark opposition argue that robots are not deserving of rights but are objects that should be our slaves. Grounded in post-Cartesian philosophical foundations, we argue not just to deny robots 'rights', but to deny that robots, as artifacts emerging out of and mediating human being, are the kinds of things that could be granted rights in the first place. Once we see robots as mediators of human being, we can understand how the 'robots rights' debate is focused on first world problems, at the expense of urgent ethical concerns, such as machine bias, machine elicited human labour exploitation, and erosion of privacy all impacting society's least privileged individuals. We conclude that, if human being is our starting point and human welfare is the primary concern, the negative impacts emerging from machinic systems, as well as the lack of taking responsibility by people designing, selling and deploying such machines, remains the most pressing ethical discussion in AI.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {207–213},
numpages = {7},
keywords = {ai ethics, embodiment, human welfare, robot rights},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375870,
author = {Chan, Lok and Doyle, Kenzie and McElfresh, Duncan and Conitzer, Vincent and Dickerson, John P. and Schaich Borg, Jana and Sinnott-Armstrong, Walter},
title = {Artificial Artificial Intelligence: Measuring Influence of AI 'Assessments' on Moral Decision-Making},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375870},
doi = {10.1145/3375627.3375870},
abstract = {Given AI's growing role in modeling and improving decision-making, how and when to present users with feedback is an urgent topic to address. We empirically examined the effect of feedback from false AI on moral decision-making about donor kidney allocation. We found some evidence that judgments about whether a patient should receive a kidney can be influenced by feedback about participants' own decision-making perceived to be given by AI, even if the feedback is entirely random. We also discovered different effects between assessments presented as being from human experts and assessments presented as being from AI.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {214–220},
numpages = {7},
keywords = {computer ethics, ai ethics, decision making, preference elicitation, moral psychology},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375844,
author = {Chen, Violet (Xinying) and Hooker, J. N.},
title = {A Just Approach Balancing Rawlsian Leximax Fairness and Utilitarianism},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375844},
doi = {10.1145/3375627.3375844},
abstract = {Numerous AI-assisted resource allocation decisions need to balance the conflicting goals of fairness and efficiency. Our paper studies the challenging task of defining and modeling a proper fairness-efficiency trade off. We define fairness with Rawlsian leximax fairness, which views the lexicographic maximum among all feasible outcomes as the most equitable; and define efficiency with Utilitarianism, which seeks to maximize the sum of utilities received by entities regardless of individual differences. Motivated by a justice-driven trade off principle: prioritize fairness to benefit the less advantaged unless too much efficiency is sacrificed, we propose a sequential optimization procedure to balance leximax fairness and utilitarianism in decision-making. Each iteration of our approach maximizes a social welfare function, and we provide a practical mixed integer/linear programming (MILP) formulation for each maximization problem. We illustrate our method on a budget allocation example. Compared with existing approaches of balancing equity and efficiency, our method is more interpretable in terms of parameter selection, and incorporates a strong equity criterion with a thoroughly balanced perspective.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {221–227},
numpages = {7},
keywords = {distributive justice, trade off, utilitarianism, fairness},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375857,
author = {Cihon, Peter and Maas, Matthijs M. and Kemp, Luke},
title = {Should Artificial Intelligence Governance Be Centralised? Design Lessons from History},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375857},
doi = {10.1145/3375627.3375857},
abstract = {Can effective international governance for artificial intelligence remain fragmented, or is there a need for a centralised international organisation for AI? We draw on the history of other international regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak in favour of centralisation. Conversely, the risk of creating a slow and brittle institution speaks against it, as does the difficulty in securing participation while creating stringent rules. Other considerations depend on the specific design of a centralised institution. A well-designed body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial and a fragmented landscape of institutions can be self-organising. Centralisation entails trade-offs and the details matter. We conclude with two core recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, for now fragmentation will likely persist. This should be closely monitored to see if it is self-organising or simply inadequate.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {228–234},
numpages = {7},
keywords = {global governance, governance fragmentation, artificial intelligence policy, centralization, institutional design, regime complex},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375860,
author = {Cruz Cort\'{e}s, Efr\'{e}n and Ghosh, Debashis},
title = {An Invitation to System-Wide Algorithmic Fairness},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375860},
doi = {10.1145/3375627.3375860},
abstract = {We propose a framework for analyzing and evaluating system-wide algorithmic fairness. The core idea is to use simulation techniques in order to extend the scope of current fairness assessments by incorporating context and feedback to a phenomenon of interest. By doing so, we expect to better understand the interaction among the social behavior giving rise to discrimination, automated decision making tools, and fairness-inspired statistical constraints. In particular, we invite the community to use agent based models as an explanatory tool for causal mechanisms of population level properties. We also propose embedding these into a reinforcement learning algorithm to find optimal actions for meaningful change. As an incentive for taking a system-wide approach , we show through a simple model of predictive policing and trials that if we limit our attention to one portion of the system, we may determine some blatantly unfair practices as fair, and be blind to overall unfairness.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {235–241},
numpages = {7},
keywords = {agent based modeling, ethical ai, recidivism, fairness},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375861,
author = {Dobbe, Roel I.J. and Gilbert, Thomas Krendl and Mintz, Yonatan},
title = {Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375861},
doi = {10.1145/3375627.3375861},
abstract = {The implementation of AI systems has led to new forms of harm in various sensitive social domains. We analyze these as problems How to address these harms remains at the center of controversial debate. In this paper, we discuss the inherent normative uncertainty and political debates surrounding the safety of AI systems.of vagueness to illustrate the shortcomings of current technical approaches in the AI Safety literature, crystallized in three dilemmas that remain in the design, training and deployment of AI systems. We argue that resolving normative uncertainty to render a system 'safe' requires a sociotechnical orientation that combines quantitative and qualitative methods and that assigns design and decision power across affected stakeholders to navigate these dilemmas through distinct channels for dissent. We propose a set of sociotechnical commitments and related virtues to set a bar for declaring an AI system 'human-compatible', implicating broader interdisciplinary design approaches.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {242},
numpages = {1},
keywords = {sociotechnical approach, artificial intelligence, democracy, vagueness, normative uncertainty},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375853,
author = {Dubljevic, Veljko},
title = {Toward Implementing the Agent-Deed-Consequence Model of Moral Judgment in Autonomous Vehicles},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375853},
doi = {10.1145/3375627.3375853},
abstract = {Autonomous vehicles (AVs) and accidents they are involved in attest to the urgent need to consider the ethics of AI. The question dominating the discussion has been whether we want AVs to behave in a 'selfish' or utilitarian manner. Rather than considering modeling self-driving cars on a single moral system like utilitarianism, one possible way to approach programming for AI would be to reflect recent work in neuroethics. The Agent-Deed-Consequence (ADC) model [1-4] provides a promising account while also lending itself well to implementation in AI. The ADC model explains moral judgments by breaking them down into positive or negative intuitive evaluations of the Agent, Deed, and Consequence in any given situation. These intuitive evaluations combine to produce a judgment of moral acceptability. This explains the considerable flexibility and stability of human moral judgment that has yet to be replicated in AI. This&nbsp;paper examines the advantages and disadvantages of implementing the ADC model and how the model could inform future work on ethics of AI in general.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {243},
numpages = {1},
keywords = {neuroethics, artificial intelligence (ai), autonomous vehicles (avs), agent-deed-consequence (adc) model, artificial neural networks, artificial morality},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375875,
author = {Dulhanty, Chris and Wong, Alexander},
title = {Investigating the Impact of Inclusion in Face Recognition Training Data on Individual Face Identification},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375875},
doi = {10.1145/3375627.3375875},
abstract = {Modern face recognition systems leverage datasets containing images of hundreds of thousands of specific individuals' faces to train deep convolutional neural networks to learn an embedding space that maps an arbitrary individual's face to a vector representation of their identity. The performance of a face recognition system in face verification (1:1) and face identification (1:N) tasks is directly related to the ability of an embedding space to discriminate between identities. Recently, there has been significant public scrutiny into the source and privacy implications of large-scale face recognition training datasets such as MS-Celeb-1M and MegaFace, as many people are uncomfortable with their face being used to train dual-use technologies that can enable mass surveillance. However, the impact of an individual's inclusion in training data on a derived system's ability to recognize them has not previously been studied. In this work, we audit ArcFace, a state-of-the-art, open source face recognition system, in a large-scale face identification experiment with more than one million distractor images. We find a Rank-1 face identification accuracy of 79.71% for individuals present in the model's training data and an accuracy of 75.73% for those not present. This modest difference in accuracy demonstrates that face recognition systems using deep learning work better for individuals they are trained on, which has serious privacy implications when one considers all major open source face recognition training datasets do not obtain informed consent from individuals during their collection.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {244–250},
numpages = {7},
keywords = {privacy, neural networks, face recognition, informed consent},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375846,
author = {Ema, Arisa and Nagakura, Katsue and Fujita, Takanori},
title = {Proposal for Type Classification for Building Trust in Medical Artificial Intelligence Systems},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375846},
doi = {10.1145/3375627.3375846},
abstract = {This paper proposes the establishment of Medical Artificial Intelligence (AI) Types (MA Types)"that classify AI in medicine not only by technical system requirements but also implications to healthcare workers' roles and users/patients. MA Types can be useful to promote discussion regarding the purpose and application of the clinical site. Although MA Types are based on the current technologies and regulations in Japan, but that does not hinder the potential reform of the technologies and regulations. MA Types aims to facilitate discussions among physicians, healthcare workers, engineers, public/patients and policymakers on AI systems in medical practices.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {251–257},
numpages = {7},
keywords = {real-world data, trust, medical ai, doctor-patient relationships, patient-centered medicine},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375847,
author = {Fernandes, Pedro M. and Santos, Francisco C. and Lopes, Manuel},
title = {Adoption Dynamics and Societal Impact of AI Systems in Complex Networks},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375847},
doi = {10.1145/3375627.3375847},
abstract = {We propose a game-theoretical model to simulate the dynamics of AI adoption in adaptive networks. This formalism allows us to understand the impact of the adoption of AI systems for society as a whole, addressing some of the concerns on the need for regulation. Using this model we study the adoption of AI systems, the distribution of the different types of AI (from selfish to utilitarian), the appearance of clusters of specific AI types, and the impact on the fitness of each individual. We suggest that the entangled evolution of individual strategy and network structure constitutes a key mechanism for the sustainability of utilitarian and human-conscious AI. Differently, in the absence of rewiring, a minority of the population can easily foster the adoption of selfish AI and gains a benefit at the expense of the remaining majority.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {258–264},
numpages = {7},
keywords = {ai regulation, social simulation, ai ethics, game theoretical analysis},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375852,
author = {Galdon Clavell, Gemma and Mart\'{\i}n Zamorano, Mariano and Castillo, Carlos and Smith, Oliver and Matic, Aleksandar},
title = {Auditing Algorithms: On Lessons Learned and the Risks of Data Minimization},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375852},
doi = {10.1145/3375627.3375852},
abstract = {In this paper, we present the Algorithmic Audit (AA) of REM!X, a personalized well-being recommendation app developed by Telef\'{o}nica Innovaci\'{o}n Alpha. The main goal of the AA was to identify and mitigate algorithmic biases in the recommendation system that could lead to the discrimination of protected groups. The audit was conducted through a qualitative methodology that included five focus groups with developers and a digital ethnography relying on users comments reported in the Google Play Store. To minimize the collection of personal information, as required by best practice and the GDPR [1], the REM!X app did not collect gender, age, race, religion, or other protected attributes from its users. This limited the algorithmic assessment and the ability to control for different algorithmic biases. Indirect evidence was thus used as a partial mitigation for the lack of data on protected attributes, and allowed the AA to identify four domains where bias and discrimination were still possible, even without direct personal identifiers. Our analysis provides important insights into how general data ethics principles such as data minimization, fairness, non-discrimination and transparency can be operationalized via algorithmic auditing, their potential and limitations, and how the collaboration between developers and algorithmic auditors can lead to better technologies},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {265–271},
numpages = {7},
keywords = {ai, recommender systems, algorithms, bias, gdpr, data ethics},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375868,
author = {Garrett, Natalie and Beard, Nathan and Fiesler, Casey},
title = {More Than "If Time Allows": The Role of Ethics in AI Education},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375868},
doi = {10.1145/3375627.3375868},
abstract = {Even as public pressure mounts for technology companies to consider societal impacts of products, industries and governments in the AI race are demanding technical talent. To meet this demand, universities clamor to add technical artificial intelligence (AI) and machine learning (ML) courses into computing curriculum-but how are societal and ethical considerations part of this landscape? We explore two pathways for ethics content in AI education: (1) standalone AI ethics courses, and (2) integrating ethics into technical AI courses. For both pathways, we ask: What is being taught? As we train computer scientists who will build and deploy AI tools, how are we training them to consider the consequences of their work? In this exploratory work, we qualitatively analyzed 31 standalone AI ethics classes from 22 U.S. universities and 20 AI/ML technical courses from 12 U.S. universities to understand which ethics-related topics instructors include in courses. We identify and categorize topics in AI ethics education, share notable practices, and note omissions. Our analysis will help AI educators identify what topics should be taught and create scaffolding for developing future AI ethics education.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {272–278},
numpages = {7},
keywords = {curriculum, artificial intelligence, ethics education},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375864,
author = {He, Yuzi and Burghardt, Keith and Lerman, Kristina},
title = {A Geometric Solution to Fair Representations},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375864},
doi = {10.1145/3375627.3375864},
abstract = {To reduce human error and prejudice, many high-stakes decisions have been turned over to machine algorithms. However, recent research suggests that this does not remove discrimination, and can perpetuate harmful stereotypes. While algorithms have been developed to improve fairness, they typically face at least one of three shortcomings: they are not interpretable, their prediction quality deteriorates quickly compared to unbiased equivalents, and %the methodology cannot easily extend other algorithms they are not easily transferable across models% (e.g., methods to reduce bias in random forests cannot be extended to neural networks) . To address these shortcomings, we propose a geometric method that removes correlations between data and any number of protected variables. Further, we can control the strength of debiasing through an adjustable parameter to address the trade-off between prediction quality and fairness. The resulting features are interpretable and can be used with many popular models, such as linear regression, random forest, and multilayer perceptrons. The resulting predictions are found to be more accurate and fair compared to several state-of-the-art fair AI algorithms across a variety of benchmark datasets. Our work shows that debiasing data is a simple and effective solution toward improving fairness.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {279–285},
numpages = {7},
keywords = {sensitive information, fair ai, fair classification, debiased features, geometric method, orthogonal space, interpretable method, projection},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375854,
author = {Herington, Jonathan},
title = {Measuring Fairness in an Unfair World},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375854},
doi = {10.1145/3375627.3375854},
abstract = {Computer scientists have made great strides in characterizing different measures of algorithmic fairness, and showing that certain measures of fairness cannot be jointly satisfied. In this paper, I argue that the three most popular families of measures - unconditional independence, target-conditional independence and classification-conditional independence - make assumptions that are unsustainable in the context of an unjust world. I begin by introducing the measures and the implicit idealizations they make about the underlying causal structure of the contexts in which they are deployed. I then discuss how these idealizations fall apart in the context of historical injustice, ongoing unmodeled oppression, and the permissibility of using sensitive attributes to rectify injustice. In the final section, I suggest an alternative framework for measuring fairness in the context of existing injustice: distributive fairness.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {286–292},
numpages = {7},
keywords = {distributive justice, fairness, discrimination, algorithmic decision-making, causal inference},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375848,
author = {Huang, Lingxiao and Wei, Julia and Celis, Elisa},
title = {Towards Just, Fair and Interpretable Methods for Judicial Subset Selection},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375848},
doi = {10.1145/3375627.3375848},
abstract = {In many judicial systems -- including the United States courts of appeals, the European Court of Justice, the UK Supreme Court and the Supreme Court of Canada -- a subset of judges is selected from the entire judicial body for each case in order to hear the arguments and decide the judgment. Ideally, the subset selected is representative, i.e., the decision of the subset would match what the decision of the entire judicial body would have been had they all weighed in on the case. Further, the process should be fair in that all judges should have similar workloads, and the selection process should not allow for certain judge's opinions to be silenced or amplified via case assignments. Lastly, in order to be practical and trustworthy, the process should also be interpretable, easy to use, and (if algorithmic) computationally efficient. In this paper, we propose an algorithmic method for the judicial subset selection problem that satisfies all of the above criteria. The method satisfies fairness by design, and we prove that it has optimal representativeness asymptotically for a large range of parameters and under noisy information models about judge opinions -- something no existing methods can provably achieve. We then assess the benefits of our approach empirically by counterfactually comparing against the current practice and recent alternative algorithmic approaches using cases from the United States courts of appeals database.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {293–299},
numpages = {7},
keywords = {judicial subset selection, fair, interpretable, representative},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375873,
author = {Javadi, Seyyed Ahmad and Cloete, Richard and Cobbe, Jennifer and Lee, Michelle Seng Ah and Singh, Jatinder},
title = {Monitoring Misuse for Accountable 'Artificial Intelligence as a Service'},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375873},
doi = {10.1145/3375627.3375873},
abstract = {AI is increasingly being offered 'as a service' (AIaaS). This entails service providers offering customers access to pre-built AI models and services, for tasks such as object recognition, text translation, text-to-voice conversion, and facial recognition, to name a few. The offerings enable customers to easily integrate a range of powerful AI-driven capabilities into their applications. Customers access these models through the provider's APIs, sending particular data to which models are applied, the results of which returned.However, there are many situations in which the use of AI can be problematic. AIaaS services typically represent generic functionality, available 'at a click'. Providers may therefore, for reasons of reputation or responsibility, seek to ensure that the AIaaS services they offer are being used by customers for 'appropriate' purposes.This paper introduces and explores the concept whereby AIaaS providers uncover situations of possible service misuse by their customers. Illustrated through topical examples, we consider the technical usage patterns that could signal situations warranting scrutiny, and raise some of the legal and technical challenges of monitoring for misuse. In all, by introducing this concept, we indicate a potential area for further inquiry from a range of perspectives.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {300–306},
numpages = {7},
keywords = {law, audit, aiaas, compliance, misuse, accountability, machine learning, cloud computing, monitoring, mlaas, artificial intelligence},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375859,
author = {Kak, Amba},
title = {"The Global South is Everywhere, but Also Always Somewhere": National Policy Narratives and AI Justice},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375859},
doi = {10.1145/3375627.3375859},
abstract = {There is more attention than ever on the social implications of AI. In contrast to universalized paradigms of ethics and fairness, a growing body of critical work highlights bias and discrimination in AI within the frame of social justice and human rights ("AI justice"). However, the geographical location of much of this critique in the West could be engendering its own blind spots. The global supply chain of AI (data, computational power, natural resources, labor) today replicates historical colonial inequities, and the continued subordination of Global South countries. This paper draws attention to official narratives from the Indian government and the United Nations Conference on Trade and Development (UNCTAD) advocating for the role (and place) of these regions in the AI economy. Domestically, these policies are being contested for their top-down formulation, and reflect narrow industry interests. This underscores the need to approach the political economy of AI from varying altitudes - global, national, and from the perspective of communities whose lives and livelihoods are most directly impacted in this economy. Without a deliberate effort at centering this conversation it is inevitable that mainstream discourse on AI justice will grow parallel to (and potentially undercut) demands emanating from Global South governments and communities},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {307–312},
numpages = {6},
keywords = {data flows, global south, decolonial, ai accountability, political economy},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375874,
author = {Karpati, Daniel and Najjar, Amro and Ambrossio, Diego Agustin},
title = {Ethics of Food Recommender Applications},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375874},
doi = {10.1145/3375627.3375874},
abstract = {The recent unprecedented popularity of food recommender applications has raised several issues related to the ethical, societal and legal implications of relying on these applications. In this paper, in order to assess the relevant ethical issues, we rely on the emerging principles across the AI &amp; Ethics community and define them tailored context specifically. Considering the popular Food Recommender Systems (henceforth F-RS) in the European market cannot be regarded as personalised F-RS, we show how merely this lack of feature shifts the relevance of the focal ethical concerns. We identify the major challenges and propose a scheme for how explicit ethical agendas should be explained. We also argue how a multi-stakeholder approach is indispensable to ensure producing long-term benefits for all stakeholders. After proposing eight ethical desiderata points for F-RS, we present a case-study and assess it based on our proposed desiderata points.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {313–319},
numpages = {7},
keywords = {food-recommender systems, explainability, transparency, ethics},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375845,
author = {Maitra, Suvradip},
title = {Artificial Intelligence and Indigenous Perspectives: Protecting and Empowering Intelligent Human Beings},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375845},
doi = {10.1145/3375627.3375845},
abstract = {As 'control' is increasingly ceded to AI systems, potentially Artificial General Intelligence (AGI) humanity may be facing an identity crisis sooner rather than later, whereby the notion of 'intelligence' no longer remains solely our own. This paper characterizes the problem in terms of an impending loss of control and proposes a relational shift in our attitude towards AI. The shortcomings of value alignment as a solution to the problem are outlined which necessitate an extension of these principles. One such approach is considering strongly relational Indigenous epistemologies. The value of Indigenous perspectives has not been canvassed widely in the literature. Their utility becomes clear when considering the existence of well-developed epistemologies adept at accounting for the non-human, a task that defies Western anthropocentrism. Accommodating AI by considering it as part of our network is a step towards building a symbiotic relationship. Given that AGI questions our fundamental notions of what it means to have human rights, it is argued that in order to co-exist, we find assistance in Indigenous traditions such as the Hawaiian and Lakota ontologies. Lakota rituals provide comfort with the conception of non-human soul-bearer while Hawaiian stories provide possible relational schema to frame our relationship with AI.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {320–326},
numpages = {7},
keywords = {relational shift, human rights, value alignment, indigenous perspectives, ethics},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375842,
author = {O'Keefe, Cullen and Cihon, Peter and Garfinkel, Ben and Flynn, Carrick and Leung, Jade and Dafoe, Allan},
title = {The Windfall Clause: Distributing the Benefits of AI for the Common Good},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375842},
doi = {10.1145/3375627.3375842},
abstract = {As the transformative potential of AI has become increasingly salient as a matter of public and political interest, there has been growing discussion about the need to ensure that AI broadly benefits humanity. This in turn has spurred debate on the social responsibilities of large technology companies to serve the interests of society at large. In response, ethical principles and codes of conduct have been proposed to meet the escalating demand for this responsibility to be taken seriously. As yet, however, few institutional innovations have been suggested to translate this responsibility into legal commitments which apply to companies positioned to reap large financial gains from the development and use of AI. This paper offers one potentially attractive tool for addressing such issues: the Windfall Clause, which is an ex ante commitment by AI firms to donate a significant amount of any eventual extremely large profits. By this we mean an early commitment that profits that a firm could not earn without achieving fundamental, economically transformative breakthroughs in AI capabilities will be donated to benefit humanity broadly, with particular attention towards mitigating any downsides from deployment of windfall-generating AI.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {327–331},
numpages = {5},
keywords = {inequality, future of work, automation},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375872,
author = {Osoba, Osonde A. and Boudreaux, Benjamin and Yeung, Douglas},
title = {Steps Towards Value-Aligned Systems},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375872},
doi = {10.1145/3375627.3375872},
abstract = {Algorithmic (including AI/ML) decision-making artifacts are an established and growing part of our decision-making ecosystem. They are now indispensable tools that help us manage the flood of information we use to try to make effective decisions in a complex world. The current literature is full of examples of how individual artifacts violate societal norms and expectations (e.g. violations of fairness, privacy, or safety norms). Against this backdrop, this discussion highlights an under-emphasized perspective in the body of research focused on assessing value misalignment in AI-equipped sociotechnical systems. The research on value misalignment so far has a strong focus on the behavior of individual tech artifacts. This discussion argues for a more structured systems-level approach for assessing value-alignment in sociotechnical systems. We rely primarily on the research on fairness to make our arguments more concrete. And we use the opportunity to highlight how adopting a system perspective improves our ability to explain and address value misalignments better. Our discussion ends with an exploration of priority questions that demand attention if we are to assure the value alignment of whole systems, not just individual artifacts.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {332–336},
numpages = {5},
keywords = {ml fairness, sociotechnical systems, systems analysis, value alignment},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375841,
author = {Patton, Desmond U. and Frey, William R. and McGregor, Kyle A. and Lee, Fei-Tzin and McKeown, Kathleen and Moss, Emanuel},
title = {Contextual Analysis of Social Media: The Promise and Challenge of Eliciting Context in Social Media Posts with Natural Language Processing},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375841},
doi = {10.1145/3375627.3375841},
abstract = {While natural language processing affords researchers an opportunity to automatically scan millions of social media posts, there is growing concern that automated computational tools lack the ability to understand context and nuance in human communication and language. This article introduces a critical systematic approach for extracting culture, context and nuance in social media data. The Contextual Analysis of Social Media (CASM) ap-proach considers and critiques the gap between inadequacies in natural language processing tools and differences in geographic, cultural, and age-related variance of social media use and communication. CASM utilizes a team-based approach to analysis of social media data, explicitly informed by community expertise. We use of CASM to analyze Twitter posts from gang-involved youth in Chicago. We designed a set of experiments to evaluate the performance of a support vector machine us-ing CASM hand-labeled posts against a distant model. We found that the CASM-informed hand-labeled data outperforms the baseline distant labels, indicating that the CASM labels capture additional dimensions of information that content-only methods lack. We then question whether this is helpful or harmful for gun violence prevention.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {337–342},
numpages = {6},
keywords = {nlp, ethics, qualitative analysis, social science},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375869,
author = {Peng, Andi and Simard-Halm, Malina},
title = {The Perils of Objectivity: Towards a Normative Framework for Fair Judicial Decision-Making},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375869},
doi = {10.1145/3375627.3375869},
abstract = {Fair decision-making in criminal justice relies on the recognition and incorporation of infinite shades of grey. In this paper, we detail how algorithmic risk assessment tools are counteractive to fair legal proceedings in social institutions where desired states of the world are contested ethically and practically. We provide a normative framework for assessing fair judicial decision-making, one that does not seek the elimination of human bias from decision-making as algorithmic fairness efforts currently focus on, but instead centers on sophisticating the incorporation of individualized or discretionary bias--a process that is requisitely human. Through analysis of a case study on social disadvantage, we use this framework to provide an assessment of potential features of consideration, such as political disempowerment and demographic exclusion, that are irreconcilable by current algorithmic efforts and recommend their incorporation in future reform.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {343},
numpages = {1},
keywords = {fairness, risk assessment, bias, criminal justice, decision-making},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375850,
author = {Poyiadzi, Rafael and Sokol, Kacper and Santos-Rodriguez, Raul and De Bie, Tijl and Flach, Peter},
title = {FACE: Feasible and Actionable Counterfactual Explanations},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375850},
doi = {10.1145/3375627.3375850},
abstract = {Work in Counterfactual Explanations tends to focus on the principle of "the closest possible world" that identifies small changes leading to the desired outcome. In this paper we argue that while this approach might initially seem intuitively appealing it exhibits shortcomings not addressed in the current literature. First, a counterfactual example generated by the state-of-the-art systems is not necessarily representative of the underlying data distribution, and may therefore prescribe unachievable goals (e.g., an unsuccessful life insurance applicant with severe disability may be advised to do more sports). Secondly, the counterfactuals may not be based on a "feasible path" between the current state of the subject and the suggested one, making actionable recourse infeasible (e.g., low-skilled unsuccessful mortgage applicants may be told to double their salary, which may be hard without first increasing their skill level). These two shortcomings may render counterfactual explanations impractical and sometimes outright offensive. To address these two major flaws, first of all, we propose a new line of Counterfactual Explanations research aimed at providing actionable and feasible paths to transform a selected instance into one that meets a certain goal. Secondly, we propose FACE: an algorithmically sound way of uncovering these "feasible paths" based on the shortest path distances defined via density-weighted metrics. Our approach generates counterfactuals that are coherent with the underlying data distribution and supported by the "feasible paths" of change, which are achievable and can be tailored to the problem at hand.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {344–350},
numpages = {7},
keywords = {black-box models, counterfactuals, explainability, interpretability},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375843,
author = {Saisubramanian, Sandhya and Galhotra, Sainyam and Zilberstein, Shlomo},
title = {Balancing the Tradeoff Between Clustering Value and Interpretability},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375843},
doi = {10.1145/3375627.3375843},
abstract = {Graph clustering groups entities -- the vertices of a graph -- based on their similarity, typically using a complex distance function over a large number of features. Successful integration of clustering approaches in automated decision-support systems hinges on the interpretability of the resulting clusters. This paper addresses the problem of generating interpretable clusters, given features of interest that signify interpretability to an end-user, by optimizing interpretability in addition to common clustering objectives. We propose a β-interpretable clustering algorithm that ensures that at least β fraction of nodes in each cluster share the same feature value. The tunable parameter β is user-specified. We also present a more efficient algorithm for scenarios with β!=!1$ and analyze the theoretical guarantees of the two algorithms. Finally, we empirically demonstrate the benefits of our approaches in generating interpretable clusters using four real-world datasets. The interpretability of the clusters is complemented by generating simple explanations denoting the feature values of the nodes in the clusters, using frequent pattern mining.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {351–357},
numpages = {7},
keywords = {interpretability, centroid-based clustering},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375865,
author = {Sharma, Shubham and Zhang, Yunfeng and R\'{\i}os Aliaga, Jes\'{u}s M. and Bouneffouf, Djallel and Muthusamy, Vinod and Varshney, Kush R.},
title = {Data Augmentation for Discrimination Prevention and Bias Disambiguation},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375865},
doi = {10.1145/3375627.3375865},
abstract = {Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an "ideal world'' dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers who want to use unbiased datasets to train machine learning models for their applications to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {358–364},
numpages = {7},
keywords = {responsible artificial intelligence, discrimination prevention, fairness in machine learning},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375876,
author = {Shulman, Eyal and Wolf, Lior},
title = {Meta Decision Trees for Explainable Recommendation Systems},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375876},
doi = {10.1145/3375627.3375876},
abstract = {We tackle the problem of building explainable recommendation systems that are based on a per-user decision tree, with decision rules that are based on single attribute values. We build the trees by applying learned regression functions to obtain the decision rules as well as the values at the leaf nodes. The regression functions receive as input the embedding of the user's training set, as well as the embedding of the samples that arrive at the current node. The embedding and the regressors are learned end-to-end with a loss that encourages the decision rules to be sparse. By applying our method, we obtain a collaborative filtering solution that provides a direct explanation to every rating it provides. With regards to accuracy, it is competitive with other algorithms. However, as expected, explainability comes at a cost and the accuracy is typically slightly lower than the state of the art result reported in the literature. Our code is available at urlhttps://github.com/shulmaneyal/metatrees.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {365–371},
numpages = {7},
keywords = {meta learning, recommendation systems, decision trees, explainability},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375866,
author = {Smart, Andrew and James, Larry and Hutchinson, Ben and Wu, Simone and Vallor, Shannon},
title = {Why Reliabilism Is Not Enough: Epistemic and Moral Justification in Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375866},
doi = {10.1145/3375627.3375866},
abstract = {In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of em justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed wide adoption of machine learning? We argue that, in general, people implicitly adoptreliabilism regarding machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method citegoldman2012reliabilism. We argue that, in cases where model deployments require em moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral "wrapper'' around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justification---moral justification. Finally, we offer cautions relevant to the (implicit or explicit) adoption of the reliabilist interpretation of machine learning.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {372–377},
numpages = {6},
keywords = {explainability, neural networks, moral justification, interpretability, machine learning, epistemology},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375863,
author = {Tucker, Aaron D. and Anderljung, Markus and Dafoe, Allan},
title = {Social and Governance Implications of Improved Data Efficiency},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375863},
doi = {10.1145/3375627.3375863},
abstract = {Many researchers work on improving the data efficiency of machine learning. What would happen if they succeed? This paper explores the social-economic impact of increased data efficiency. Specifically, we examine the intuition that data efficiency will erode the barriers to entry protecting incumbent data-rich AI firms, exposing them to more competition from data-poor firms. We find that this intuition is only partially correct: data efficiency makes it easier to create ML applications, but large AI firms may have more to gain from higher performing AI systems. Further, we find that the effect on privacy, data markets, robustness, and misuse are complex. For example, while it seems intuitive that misuse risk would increase along with data efficiency -- as more actors gain access to any level of capability -- the net effect crucially depends on how much defensive measures are improved. More investigation into data efficiency, as well as research into the "AI production function", will be key to understanding the development of the AI industry and its societal impacts.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {378–384},
numpages = {7},
keywords = {active learning, data markets, transfer learning, data efficiency, production function, competitive advantage},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375851,
author = {Turner, Alexander Matt and Hadfield-Menell, Dylan and Tadepalli, Prasad},
title = {Conservative Agency via Attainable Utility Preservation},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375851},
doi = {10.1145/3375627.3375851},
abstract = {Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {385–391},
numpages = {7},
keywords = {ai alignment, reinforcement learning, reward specification, side effects},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375867,
author = {Wright, Ava Thomas},
title = {A Deontic Logic for Programming Rightful Machines},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375867},
doi = {10.1145/3375627.3375867},
abstract = {A "rightful machine" is an explicitly moral, autonomous machine agent whose behavior conforms to principles of justice and the positive public law of a legitimate state. In this paper, I set out some basic elements of a deontic logic appropriate for capturing conflicting legal obligations for purposes of programming rightful machines. Justice demands that the prescriptive system of enforceable public laws be consistent, yet statutes or case holdings may often describe legal obligations that contradict; moreover, even fundamental constitutional rights may come into conflict. I argue that a deontic logic of the law should not try to work around such conflicts but, instead, identify and expose them so that the rights and duties that generate inconsistencies in public law can be explicitly qualified and the conflicts resolved. I then argue that a credulous, non-monotonic deontic logic can describe inconsistent legal obligations while meeting the normative demand for consistency in the prescriptive system of public law. I propose an implementation of this logic via a modified form of "answer set programming," which I demonstrate with some simple examples.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {392},
numpages = {1},
keywords = {answer set programming, conflicts, law, logic programming, deontic logic, justice, machine ethics, rightful machines},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375840,
author = {Yu, Han and Liu, Zelei and Liu, Yang and Chen, Tianjian and Cong, Mingshu and Weng, Xi and Niyato, Dusit and Yang, Qiang},
title = {A Fairness-Aware Incentive Scheme for Federated Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375840},
doi = {10.1145/3375627.3375840},
abstract = {In federated learning (FL), data owners "share" their local data in a privacy preserving manner in order to build a federated model, which in turn, can be used to generate revenues for the participants. However, in FL involving business participants, they might incur significant costs if several competitors join the same federation. Furthermore, the training and commercialization of the models will take time, resulting in delays before the federation accumulates enough budget to pay back the participants. The issues of costs and temporary mismatch between contributions and rewards have not been addressed by existing payoff-sharing schemes. In this paper, we propose the Federated Learning Incentivizer (FLI) payoff-sharing scheme. The scheme dynamically divides a given budget in a context-aware manner among data owners in a federation by jointly maximizing the collective utility while minimizing the inequality among the data owners, in terms of the payoff gained by them and the waiting time for receiving payoff. Extensive experimental comparisons with five state-of-the-art payoff-sharing schemes show that FLI is the most attractive to high quality data owners and achieves the highest expected revenue for a data federation.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {393–399},
numpages = {7},
keywords = {incentive mechanism design, federated learning},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375862,
author = {Zhang, Yunfeng and Bellamy, Rachel and Varshney, Kush},
title = {Joint Optimization of AI Fairness and Utility: A Human-Centered Approach},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375862},
doi = {10.1145/3375627.3375862},
abstract = {Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {400–406},
numpages = {7},
keywords = {policy elicitation, multi-criteria decision making, algorithmic fairness},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375856,
author = {Zhou, Tongyu and Sheng, Haoyu and Howley, Iris},
title = {Assessing Post-Hoc Explainability of the BKT Algorithm},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375856},
doi = {10.1145/3375627.3375856},
abstract = {As machine intelligence is increasingly incorporated into educational technologies, it becomes imperative for instructors and students to understand the potential flaws of the algorithms on which their systems rely. This paper describes the design and implementation of an interactive post-hoc explanation of the Bayesian Knowledge Tracing algorithm which is implemented in learning analytics systems used across the United States. After a user-centered design process to smooth out interaction design difficulties, we ran a controlled experiment to evaluate whether the interactive or static version of the explainable led to increased learning. Our results reveal that learning about an algorithm through an explainable depends on users' educational background. For other contexts, designers of post-hoc explainables must consider their users' educational background to best determine how to empower more informed decision-making with AI-enhanced systems.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {407–413},
numpages = {7},
keywords = {post-hoc explanations, explainable ai, evaluation of xai systems, communicating algorithmic systems, interpretability of algorithms},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375849,
author = {Zhu, Bingquan and Fang, Hao and Sui, Yanan and Li, Luming},
title = {Deepfakes for Medical Video De-Identification: Privacy Protection and Diagnostic Information Preservation},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375849},
doi = {10.1145/3375627.3375849},
abstract = {Data sharing for medical research has been difficult as open-sourcing clinical data may violate patient privacy. Traditional methods for face de-identification wipe out facial information entirely, making it impossible to analyze facial behavior. Recent advancements on whole-body keypoints detection also rely on facial input to estimate body keypoints. Both facial and body keypoints are critical in some medical diagnoses, and keypoints invariability after de-identification is of great importance. Here, we propose a solution using deepfake technology, the face swapping technique. While this swapping method has been criticized for invading privacy and portraiture right, it could conversely protect privacy in medical video: patients' faces could be swapped to a proper target face and become unrecognizable. However, it remained an open question that to what extent the swapping de-identification method could affect the automatic detection of body keypoints. In this study, we apply deepfake technology to Parkinson's disease examination videos to de-identify subjects, and quantitatively show that: face-swapping as a de-identification approach is reliable, and it keeps the keypoints almost invariant, significantly better than traditional methods. This study proposes a pipeline for video de-identification and keypoint preservation, clearing up some ethical restrictions for medical data sharing. This work could make open-source high quality medical video datasets more feasible and promote future medical research that benefits our society.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {414–420},
numpages = {7},
keywords = {medical data sharing, de-identification, deepfakes, privacy protection, keypoint detection},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{10.1145/3375627.3375858,
author = {Zucker, Julian and d'Leeuwen, Myraeka},
title = {Arbiter: A Domain-Specific Language for Ethical Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375858},
doi = {10.1145/3375627.3375858},
abstract = {The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {421–425},
numpages = {5},
keywords = {domain-specific languages, ethical machine learning},
location = {New York, NY, USA},
series = {AIES '20}
}

