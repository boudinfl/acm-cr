@inproceedings{10.1145/3256685,
author = {Shasha, Dennis},
title = {Session Details: Research Session 1: Tracking Data in Space},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256685},
doi = {10.1145/3256685},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376621,
author = {U, Leong Hou and Yiu, Man Lung and Mouratidis, Kyriakos and Mamoulis, Nikos},
title = {Capacity Constrained Assignment in Spatial Databases},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376621},
doi = {10.1145/1376616.1376621},
abstract = {Given a point set P of customers (e.g., WiFi receivers) and a point set Q of service providers (e.g., wireless access points), where each q ∈ Q has a capacity q.k, the capacity constrained assignment (CCA) is a matching M ⊆ Q \texttimes{} P such that (i) each point q ∈ Q (p ∈ P) appears at most k times (at most once) in M, (ii) the size of M is maximized (i.e., it comprises min{|P|, ∑q∈Qq.k} pairs), and (iii) the total assignment cost (i.e., the sum of Euclidean distances within all pairs) is minimized. Thus, the CCA problem is to identify the assignment with the optimal overall quality; intuitively, the quality of q's service to p in a given (q, p) pair is anti-proportional to their distance. Although max-flow algorithms are applicable to this problem, they require the complete distance-based bipartite graph between Q and P. For large spatial datasets, this graph is expensive to compute and it may be too large to fit in main memory. Motivated by this fact, we propose efficient algorithms for optimal assignment that employ novel edge-pruning strategies, based on the spatial properties of the problem. Additionally, we develop approximate (i.e., suboptimal) CCA solutions that provide a trade-off between result accuracy and computation cost, abiding by theoretical quality guarantees. A thorough experimental evaluation demonstrates the efficiency and practicality of the proposed techniques.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {15–28},
numpages = {14},
keywords = {spatial databases, optimal assignment},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376622,
author = {Chen, Su and Ooi, Beng Chin and Tan, Kian-Lee and Nascimento, Mario A.},
title = {ST<sup>2</sup>B-Tree: A Self-Tunable Spatio-Temporal b<sup>+</sup>-Tree Index for Moving Objects},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376622},
doi = {10.1145/1376616.1376622},
abstract = {In a moving objects database (MOD) the dataset and the workload change frequently. As the locations of objects change in space and time, the data distribution also changes and the answer for a same query over the same region may vary widely over time. As a result, traditional static indexes are not able to perform well and it is critical to develop self-tuning indexes that can be reconfigured automatically based on the state of the system. Towards this goal we propose the ST2B-tree, a Self-Tunable Spatio-Temporal B+-Tree index for MODs, which is amenable to tuning. Frequent updates to its subtrees allows rebuilding (tuning) a subtree using a different set of reference points and different grid size without significant overhead. We also present an online tuning framework for the ST2B-tree, where the tuning is conducted online and automatically without human intervention, also not interfering with regular functions of the MOD. Our extensive experiments show that the self-tuning process minimizes the effectiveness degradation of the index caused by workload changes at the cost of virtually no overhead.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {29–42},
numpages = {14},
keywords = {data distribution, index tuning, moving object indexing, location-based services, self-tuning},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376623,
author = {Samet, Hanan and Sankaranarayanan, Jagan and Alborzi, Houman},
title = {Scalable Network Distance Browsing in Spatial Databases},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376623},
doi = {10.1145/1376616.1376623},
abstract = {An algorithm is presented for finding the k nearest neighbors in a spatial network in a best-first manner using network distance. The algorithm is based on precomputing the shortest paths between all possible vertices in the network and then making use of an encoding that takes advantage of the fact that the shortest paths from vertex u to all of the remaining vertices can be decomposed into subsets based on the first edges on the shortest paths to them from u. Thus, in the worst case, the amount of work depends on the number of objects that are examined and the number of links on the shortest paths to them from q, rather than depending on the number of vertices in the network. The amount of storage required to keep track of the subsets is reduced by taking advantage of their spatial coherence which is captured by the aid of a shortest path quadtree. In particular, experiments on a number of large road networks as well as a theoretical analysis have shown that the storage has been reduced from O(N3) to O(N1.5) (i.e., by an order of magnitude equal to the square root). The precomputation of the shortest paths along the network essentially decouples the process of computing shortest paths along the network from that of finding the neighbors, and thereby also decouples the domain S of the query objects and that of the objects from which the neighbors are drawn from the domain V of the vertices of the spatial network. This means that as long as the spatial network is unchanged, the algorithm and underlying representation of the shortest paths in the spatial network can be used with different sets of objects.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {43–54},
numpages = {12},
keywords = {shortest path quadtree, spatial networks, decoupling, nearest neighbor, scalability},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256686,
author = {Amer-Yahia, Sihem},
title = {Session Details: Research Session 2: Ranking},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256686},
doi = {10.1145/3256686},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376625,
author = {Feng, Jianlin and Fang, Qiong and Ng, Wilfred},
title = {Discovering Bucket Orders from Full Rankings},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376625},
doi = {10.1145/1376616.1376625},
abstract = {Discovering a bucket order B from a collection of possibly noisy full rankings is a fundamental problem that relates to various applications involving rankings. Informally, a bucket order is a total order that allows "ties" between items in a bucket. A bucket order B can be viewed as a "representative" that summarizes a given set of full rankings {T1, T2, ..., Tm}, or conversely B can be an "approximation" of some "ground truth" G where the rankings {T1, T2, ..., Tm} are simply the "linear extensions" of G.Current work of finding bucket orders such as the dynamic programming algorithm is mainly developed from the "representative" perspective, which maximizes items' intra-bucket similarity when forming a bucket. The underlying idea of maximizing intra-bucket similarity is realized via minimizing the sum of the deviations of median ranks within a bucket. In contrast, from the "approximation" perspective, since each observed full ranking Ti is simply a linear extension of the given "ground truth" bucket order G, items in a big bucket b in G are forced to have different median ranks, and as a result b will have a big sum of deviations. Thus, minimizing the sum of deviations may result in an undesirable scenario that big buckets are mostly decomposed into small ones.In this paper, we propose a novel heuristic called Abnormal Rank Gap to capture the inter-bucket dissimilarity for better bucket forming. In addition, we propose to use the "closeness" on multiple quantile ranks to determine if two items should be put into the same bucket. We develop a novel bucket order discovering method termed the Bucket Gap algorithm. Our extensive experiments demonstrate that the Bucket Gap algorithm significantly outperforms the major related work, i.e., the Bucket Pivot algorithm. In particular, the error distance of the generated bucket order can be reduced by about 30% on a real paleontological dataset and the noise tolerance can be increased from 30% to 50% in the synthetic dataset.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {55–66},
numpages = {12},
keywords = {full ranking, bucket order, quantile rank, ranking aggregation, order discovering, rank gap, ground truth},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376626,
author = {Bansal, Nilesh and Guha, Sudipto and Koudas, Nick},
title = {Ad-Hoc Aggregations of Ranked Lists in the Presence of Hierarchies},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376626},
doi = {10.1145/1376616.1376626},
abstract = {A variety of web sites and web based services produce textual lists at varying time granularities ranked according to several criteria. For example, Google Trends produces lists of popular query keywords which can be visualized according to several criteria. At Flickr, lists of popular tags used to tag the images uploaded can be visualized as a cloud based on their popularity. Identification of the k most popular terms can be easily conducted by utilizing well known rank aggregation algorithms.In this paper we take a different approach to information discovery from such ranked lists. We maintain the same rank aggregation framework but we elevate terms at a higher level by making use of popular term hierarchies commonly available. Under such a transformation we show that typical early stopping certificates available for rank aggregation algorithms are no longer applicable. Based on this observation, in this paper, we present a probabilistic framework for early stopping in this setting. We introduce a relaxed version of the rank aggregation problem involving a deterministic stopping condition with user specified precision. We introduce an algorithm pH -- RA for the solution of this problem. In addition we introduce techniques to improve the performance of pH -- RAeven further via precomputation utilizing a sparse set system. Through a detailed experimental evaluation using synthetic and real datasets we demonstrate the efficiency of our framework.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {67–78},
numpages = {12},
keywords = {list aggregation, top-k},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376627,
author = {Wu, Tianyi and Xin, Dong and Han, Jiawei},
title = {ARCube: Supporting Ranking Aggregate Queries in Partially Materialized Data Cubes},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376627},
doi = {10.1145/1376616.1376627},
abstract = {Supporting ranking queries in database systems has been a popular research topic recently. However, there is a lack of study on supporting ranking queries in data warehouses where ranking is on multidimensional aggregates instead of on measures of base facts. To address this problem, we propose a query execution model to answer different types of ranking aggregate queries based on a unified, partial cube structure, ARCube. The query execution model follows a candidate generation and verification framework, where the most promising candidate cells are generated using a set of high-level guiding cells. We also identify a bounding principle for effective pruning: once a guiding cell is pruned, all of its children candidate cells can be pruned. We further address the problem of efficient online candidate aggregation and verification by developing a chunk-based execution model to verify a bulk of candidates within a bounded memory buffer. Our extensive performance study shows that the new framework not only leads to an order of magnitude performance improvements over the state-of-the-art method, but also is much more flexible in terms of the types of ranking aggregate queries supported.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {79–92},
numpages = {14},
keywords = {data cube, partial materialization, ranking aggregate queries},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256687,
author = {Tung, Anthony K. H.},
title = {Session Details: Research Session 3: Privacy &amp; Anonymization},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256687},
doi = {10.1145/3256687},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376629,
author = {Liu, Kun and Terzi, Evimaria},
title = {Towards Identity Anonymization on Graphs},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376629},
doi = {10.1145/1376616.1376629},
abstract = {The proliferation of network data in various application domains has raised privacy concerns for the individuals involved. Recent studies show that simply removing the identities of the nodes before publishing the graph/social network data does not guarantee privacy. The structure of the graph itself, and in its basic form the degree of the nodes, can be revealing the identities of individuals. To address this issue, we study a specific graph-anonymization problem. We call a graph k-degree anonymous if for every node v, there exist at least k-1 other nodes in the graph with the same degree as v. This definition of anonymity prevents the re-identification of individuals by adversaries with a priori knowledge of the degree of certain nodes. We formally define the graph-anonymization problem that, given a graph G, asks for the k-degree anonymous graph that stems from G with the minimum number of graph-modification operations. We devise simple and efficient algorithms for solving this problem. Our algorithms are based on principles related to the realizability of degree sequences. We apply our methods to a large spectrum of synthetic and real datasets and demonstrate their efficiency and practical utility.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {93–106},
numpages = {14},
keywords = {degree sequence, anonymity, dynamic programming},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376630,
author = {Xiao, Xiaokui and Tao, Yufei},
title = {Dynamic Anonymization: Accurate Statistical Analysis with Privacy Preservation},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376630},
doi = {10.1145/1376616.1376630},
abstract = {A statistical database (StatDB) retrieves only aggregate results, as opposed to individual tuples. This paper investigates the construction of a privacy preserving StatDB that can (i) accurately answer an infinite number of counting queries, and (ii) effectively protect privacy against an adversary that may have acquired all the previous query results. The core of our solutions is a novel technique called dynamic anonymization. Specifically, given a query, we on the fly compute a tailor-made anonymized version of the microdata, which maximizes the precision of the query result. Privacy preservation is achieved by ensuring that the combination of all the versions deployed to process the past queries does not allow accurate inference of sensitive information. Extensive experiments with real data confirm that our technique enables highly effective data analysis, while offering strong privacy guarantees.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {107–120},
numpages = {14},
keywords = {privacy, m-invariance, statistical database, dynamic anonymization},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376631,
author = {Ghinita, Gabriel and Kalnis, Panos and Khoshgozaran, Ali and Shahabi, Cyrus and Tan, Kian-Lee},
title = {Private Queries in Location Based Services: Anonymizers Are Not Necessary},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376631},
doi = {10.1145/1376616.1376631},
abstract = {Mobile devices equipped with positioning capabilities (e.g., GPS) can ask location-dependent queries to Location Based Services (LBS). To protect privacy, the user location must not be disclosed. Existing solutions utilize a trusted anonymizer between the users and the LBS. This approach has several drawbacks: (i) All users must trust the third party anonymizer, which is a single point of attack. (ii) A large number of cooperating, trustworthy users is needed. (iii) Privacy is guaranteed only for a single snapshot of user locations; users are not protected against correlation attacks (e.g., history of user movement).We propose a novel framework to support private location-dependent queries, based on the theoretical work on Private Information Retrieval (PIR). Our framework does not require a trusted third party, since privacy is achieved via cryptographic techniques. Compared to existing work, our approach achieves stronger privacy for snapshots of user locations; moreover, it is the first to provide provable privacy guarantees against correlation attacks. We use our framework to implement approximate and exact algorithms for nearest-neighbor search. We optimize query execution by employing data mining techniques, which identify redundant computations. Contrary to common belief, the experimental results suggest that PIR approaches incur reasonable overhead and are applicable in practice.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {121–132},
numpages = {12},
keywords = {private information retrieval, location anonymity, query privacy},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256688,
author = {Rundensteiner, Elke A.},
title = {Session Details: Research Session 4: Streaming Filters},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256688},
doi = {10.1145/3256688},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376633,
author = {Liu, Zhen and Parthasarathy, Srinivasan and Ranganathan, Anand and Yang, Hao},
title = {Near-Optimal Algorithms for Shared Filter Evaluation in Data Stream Systems},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376633},
doi = {10.1145/1376616.1376633},
abstract = {We consider the problem of evaluating multiple overlapping queries defined on data streams, where each query is a conjunction of multiple filters and each filter may be shared across multiple queries. Efficient support for overlapping queries is a critical issue in the emerging data stream systems, and this is particularly the case when filters are expensive in terms of their computational complexity and processing time. This problem generalizes other well-known problems such as pipelined filter ordering and set cover, and is not only NP-Hard but also hard to approximate within a factor of o(log n) from the optimum, where n is the number of queries. In this paper, we present two near-optimal approximation lgorithms with provably-good performance guarantees for the evaluation of overlapping queries. We present an edge-coverage based Greedy algorithm which achieves an approximation ratio of (1 + log(n) + log(α)), where n is the number of queries and α is the average number of filters in a query. We also present a randomized, fast and easily parallelizable Harmonic algorithm which achieves an approximation ratio of 2β, where β is the maximum number of filters in a query. We have implemented these algorithms in a prototype system, and evaluated their performance using extensive experiments in the context of multimedia stream analysis. The results show that our Greedy algorithm consistently outperforms other known algorithms under various settings and scales well as the numbers of queries and filters increase.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {133–146},
numpages = {14},
keywords = {query optimization, shared filter ordering, randomized algorithm, greedy algorithm},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376634,
author = {Agrawal, Jagrati and Diao, Yanlei and Gyllstrom, Daniel and Immerman, Neil},
title = {Efficient Pattern Matching over Event Streams},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376634},
doi = {10.1145/1376616.1376634},
abstract = {Pattern matching over event streams is increasingly being employed in many areas including financial services, RFIDbased inventory management, click stream analysis, and electronic health systems. While regular expression matching is well studied, pattern matching over streams presents two new challenges: Languages for pattern matching over streams are significantly richer than languages for regular expression matching. Furthermore, efficient evaluation of these pattern queries over streams requires new algorithms and optimizations: the conventional wisdom for stream query processing (i.e., using selection-join-aggregation) is inadequate.In this paper, we present a formal evaluation model that offers precise semantics for this new class of queries and a query evaluation framework permitting optimizations in a principled way. We further analyze the runtime complexity of query evaluation using this model and develop a suite of techniques that improve runtime efficiency by exploiting sharing in storage and processing. Our experimental results provide insights into the various factors on runtime performance and demonstrate the significant performance gains of our sharing techniques.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {147–160},
numpages = {14},
keywords = {query optimization, event streams, pattern matching},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376635,
author = {Majumder, Anirban and Rastogi, Rajeev and Vanama, Sriram},
title = {Scalable Regular Expression Matching on Data Streams},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376635},
doi = {10.1145/1376616.1376635},
abstract = {Regular Expression (RE) matching has important applications in the areas of XML content distribution and network security. In this paper, we present the end-to-end design of a high performance RE matching system. Our system combines the processing efficiency of Deterministic Finite Automata (DFA) with the space efficiency of Non-deterministic Finite Automata (NFA) to scale to hundreds of REs. In experiments with real-life RE data on data streams, we found that a bulk of the DFA transitions are concentrated around a few DFA states. We exploit this fact to cache only the frequent core of each DFA in memory as opposed to the entire DFA (which may be exponential in size). Further, we cluster REs such that REs whose interactions cause an exponential increase in the number of states are assigned to separate groups -- this helps to improve cache hits by controlling the overall DFA size.To the best of our knowledge, ours is the first end-to-end system capable of matching REs at high speeds and in their full generality. Through a clever combination of RE grouping, and static and dynamic caching, it is able to perform RE matching at high speeds, even in the presence of limited memory. Through experiments with real-life data sets, we show that our RE matching system convincingly outperforms a state-of-the-art Network Intrusion Detection tool with support for efficient RE matching.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {161–172},
numpages = {12},
keywords = {regular expression matching, data streams, deep packet inspection},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256689,
author = {Han, Jiawei},
title = {Session Details: Research Session 5: Clustering in High Dimensions},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256689},
doi = {10.1145/3256689},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376637,
author = {Pan, Feng and Zhang, Xiang and Wang, Wei},
title = {CRD: Fast Co-Clustering on Large Datasets Utilizing Sampling-Based Matrix Decomposition},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376637},
doi = {10.1145/1376616.1376637},
abstract = {The problem of simultaneously clustering columns and rows (co-clustering) arises in important applications, such as text data mining, microarray analysis, and recommendation system analysis. Compared with the classical clustering algorithms, co-clustering algorithms have been shown to be more effective in discovering hidden clustering structures in the data matrix. The complexity of previous co-clustering algorithms is usually O(m X n), where m and n are the numbers of rows and columns in the data matrix respectively. This limits their applicability to data matrices involving a large number of columns and rows. Moreover, some huge datasets can not be entirely held in main memory during co-clustering which violates the assumption made by the previous algorithms. In this paper, we propose a general framework for fast co-clustering large datasets, CRD. By utilizing recently developed sampling-based matrix decomposition methods, CRD achieves an execution time linear in m and n. Also, CRD does not require the whole data matrix be in the main memory. We conducted extensive experiments on both real and synthetic data. Compared with previous co-clustering algorithms, CRD achieves competitive accuracy but with much less computational cost.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {173–184},
numpages = {12},
keywords = {co-clustering, matrix decomposition},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376638,
author = {B\"{o}hm, Christian and Faloutsos, Christos and Plant, Claudia},
title = {Outlier-Robust Clustering Using Independent Components},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376638},
doi = {10.1145/1376616.1376638},
abstract = {How can we efficiently find a clustering, i.e. a concise description of the cluster structure, of a given data set which contains an unknown number of clusters of different shape and distribution and is contaminated by noise? Most existing clustering methods are restricted to the Gaussian cluster model and are very sensitive to noise. If the cluster content follows a non-Gaussian distribution and/or the data set contains a few outliers belonging to no cluster, then the computed data distribution does not match well the true data distribution, or an unnaturally high number of clusters is required to represent the true data distribution of the data set. In this paper we propose OCI (Outlier-robust Clustering using Independent Components), a clustering method which overcomes these problems by (1) applying the exponential power distribution (EPD) as cluster model which is a generalization of Gaussian, uniform, Laplacian and many other distribution functions, (2) applying the Independent Component Analysis (ICA) for both determining the main directions inside a cluster as well as finding split planes in a top-down clustering approach, and (3) defining an efficient and effective filter for outliers, based on EPD and ICA. Our method is parameter-free and as a top-down clustering approach very efficient. An extensive experimental evaluation shows both the accuracy of the obtained clustering result as well as the efficiency of our method.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {185–198},
numpages = {14},
keywords = {ica, epd, outlier-robust clustering},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376639,
author = {Wichterich, Marc and Assent, Ira and Kranen, Philipp and Seidl, Thomas},
title = {Efficient EMD-Based Similarity Search in Multimedia Databases via Flexible Dimensionality Reduction},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376639},
doi = {10.1145/1376616.1376639},
abstract = {The Earth Mover's Distance (EMD) was developed in computer vision as a flexible similarity model that utilizes similarities in feature space to define a high quality similarity measure in feature representation space. It has been successfully adopted in a multitude of applications with low to medium dimensionality. However, multimedia applications commonly exhibit high-dimensional feature representations for which the computational complexity of the EMD hinders its adoption. An efficient query processing approach that mitigates and overcomes this effect is crucial. We propose novel dimensionality reduction techniques for the EMD in a filter-and-refine architecture for efficient lossless retrieval. Thorough experimental evaluation on real world data sets demonstrates a substantial reduction of the number of expensive high-dimensional EMD computations and thus remarkably faster response times. Our techniques are fully flexible in the number of reduced dimensions, which is a novel feature in approximation techniques for the EMD.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {199–212},
numpages = {14},
keywords = {multimedia databases, dimensionality reduction, earth mover's distance, filter distance, lower bound},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256690,
author = {Tao, Yufei},
title = {Session Details: Research Session 6: Skylines},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256690},
doi = {10.1145/3256690},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376641,
author = {Lian, Xiang and Chen, Lei},
title = {Monochromatic and Bichromatic Reverse Skyline Search over Uncertain Databases},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376641},
doi = {10.1145/1376616.1376641},
abstract = {Reverse skyline queries over uncertain databases have many important applications such as sensor data monitoring and business planning. Due to the existence of uncertainty in many real-world data, answering reverse skyline queries accurately and efficiently over uncertain data has become increasingly important. In this paper, we model the probabilistic reverse skyline query on uncertain data, in both monochromatic and bichromatic cases, and propose effective pruning methods to reduce the search space of query processing. Moreover, efficient query procedures have been presented seamlessly integrating the proposed pruning methods. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed approach with various experimental settings.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {213–226},
numpages = {14},
keywords = {monochromatic reverse skyline, uncertain database, bichromatic reverse skyline},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376642,
author = {Vlachou, Akrivi and Doulkeridis, Christos and Kotidis, Yannis},
title = {Angle-Based Space Partitioning for Efficient Parallel Skyline Computation},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376642},
doi = {10.1145/1376616.1376642},
abstract = {Recently, skyline queries have attracted much attention in the database research community. Space partitioning techniques, such as recursive division of the data space, have been used for skyline query processing in centralized, parallel and distributed settings. Unfortunately, such grid-based partitioning is not suitable in the case of a parallel skyline query, where allpartitions are examined at the same time, since many data partitions do not contribute to the overall skyline set, resulting in a lot of redundant processing.In this paper we propose a novel angle-based space partitioning scheme using the hyperspherical coordinates of the data points. We demonstrate both formally as well as through an exhaustive set of experiments that this new scheme is very suitable for skyline query processing in a parallel share-nothing architecture. The intuition of our partitioning technique is that the skyline points are equally spread to all partitions. We also show that partitioning the data according to the hyperspherical coordinates manages to increase the average pruning power of points within a partition. Our novel partitioning scheme alleviates most of the problems of traditional grid partitioning techniques, thus managing to reduce the response time and share the computational workload more fairly. As demonstrated by our experimental study, our technique outperforms grid partitioning in all cases, thus becoming an efficient and scalable solution for skyline query processing in parallel environments.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {227–238},
numpages = {12},
keywords = {skyline operator, space partitioning, parallel computation},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376643,
author = {Sarkas, Nikos and Das, Gautam and Koudas, Nick and Tung, Anthony K. H.},
title = {Categorical Skylines for Streaming Data},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376643},
doi = {10.1145/1376616.1376643},
abstract = {The problem of skyline computation has attracted considerable research attention. In the categorical domain the problem becomes more complicated, primarily due to the partially-ordered nature of the attributes of tuples.In this paper, we initiate a study of streaming categorical skylines. We identify the limitations of existing work for offline categorical skyline computation and realize novel techniques for the problem of maintaining the skyline of categorical data in a streaming environment. In particular, we develop a lightweight data structure for indexing the tuples in the streaming buffer, that can gracefully adapt to tuples with many attributes and partially ordered domains of any size and complexity. Additionally, our study of the dominance relation in the dual space allows us to utilize geometric arrangements in order to index the categorical skyline and efficiently evaluate dominance queries. Lastly, a thorough experimental study evaluates the efficiency of the proposed techniques.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {239–250},
numpages = {12},
keywords = {categorical, data stream, skyline, partial order},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256691,
author = {Alonso, Gustavo},
title = {Session Details: Research Session 7: Special Platforms},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256691},
doi = {10.1145/3256691},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376645,
author = {Brantner, Matthias and Florescu, Daniela and Graf, David and Kossmann, Donald and Kraska, Tim},
title = {Building a Database on S3},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376645},
doi = {10.1145/1376616.1376645},
abstract = {There has been a great deal of hype about Amazon's simple storage service (S3). S3 provides infinite scalability and high availability at low cost. Currently, S3 is used mostly to store multi-media documents (videos, photos, audio) which are shared by a community of people and rarely updated. The purpose of this paper is to demonstrate the opportunities and limitations of using S3 as a storage system for general-purpose database applications which involve small objects and frequent updates. Read, write, and commit protocols are presented. Furthermore, the cost ($), performance, and consistency properties of such a storage system are studied.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {251–264},
numpages = {14},
keywords = {cost trade-off, concurrency, performance, cloud computing, eventual consistency, aws, ec2, storage system, database, sqs, s3, simpledb},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376646,
author = {Lupu, Mihai and Ooi, Beng Chin and Tay, Y. C.},
title = {Paths to Stardom: Calibrating the Potential of a Peer-Based Data Management System},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376646},
doi = {10.1145/1376616.1376646},
abstract = {As peer-to-peer (P2P) networks become more familiar to the database community, intense interest has built up in using their scalability and resilience properties to scale database applications. Indexing methods are adapted on top of P2P networks and querying methods are developed to handle the data distribution on different nodes. These procedures largely depend on how nodes are connected to each other. So far, limited attempts have been made to compare all these systems in a generalized framework. This is because the systems are quite different from each other, and there are so many of them that brute force comparison is practically impossible. Fortunately, it has recently been observed that a large subset of the most important P2P networks share a common algebraic and combinatorial base, in the form of Cayley graphs.The specific requirements of Peer-based Data Management Systems (PDMS), such as query completeness, range queries, load balancing, communication overhead, and scalability are strongly related to the properties of the underlying graphs, and naturally, some graphs are better than others. We conduct a comprehensive graph-theoretic analysis from the point of view of PDMS and identify the necessary conditions for a graph to be considered a potential network structure for a PDMS. In so doing, we provide a basis for the future development of such networks. We complement our analytical study with extensive experimental results and identify three measures that provide significant information about the potential of a [Cayley] graph to support the requirements of a PDMS.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {265–278},
numpages = {14},
keywords = {peer to peer, query completeness, cayley graph},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376647,
author = {Wu, Sai and Li, Jianzhong and Ooi, Beng Chin and Tan, Kian-Lee},
title = {Just-in-Time Query Retrieval over Partially Indexed Data on Structured P2P Overlays},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376647},
doi = {10.1145/1376616.1376647},
abstract = {Structured peer-to-peer (P2P) overlays have been successfully employed in many applications to locate content. However, they have been less effective in handling massive amounts of data because of the high overhead of maintaining indexes. In this paper, we propose PISCES, a Peer-based system that Indexes Selected Content for Efficient Search. Unlike traditional approaches that index all data, PISCES identifies a subset of tuples to index based on some criteria (such as query frequency, update frequency, index cost, etc.). In addition, a coarse-grained range index is built to facilitate the processing of queries that cannot be fully answered by the tuple-level index. More importantly, PISCES can adaptively self-tune to optimize the subset of tuples to be indexed. That is, the (partial) index in PISCES is built in a Just-In-Time (JIT) manner. Beneficial tuples for current users are pulled for indexing while indexed tuples with infrequent access and high maintenance cost are discarded. We also introduce a light-weight monitoring scheme for structured networks to collect the necessary statistics. We have conducted an extensive experimental study on PlanetLab to illustrate the feasibility, practicality and efficiency of PISCES. The results show that PISCES incurs lower maintenance cost and offers better search and query efficiency compared to existing methods.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {279–290},
numpages = {12},
keywords = {partial indexing, just-in-time, peer-to-peer, sampling, baton, can, self-tuning},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376648,
author = {Lee, Chun-Hee and Chung, Chin-Wan},
title = {Efficient Storage Scheme and Query Processing for Supply Chain Management Using RFID},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376648},
doi = {10.1145/1376616.1376648},
abstract = {As the size of an RFID tag becomes smaller and the price of the tag gets lower, RFID technology has been applied to a wide range of areas. Recently, RFID has been adopted in the business area such as supply chain management. Since companies can get movement information for products easily using the RFID technology, it is expected to revolutionize supply chain management. However, the amount of RFID data in supply chain management is huge. Therefore, it requires much time to extract valuable information from RFID data for supply chain management.In this paper, we define query templates for tracking queries and path oriented queries to analyze the supply chain. We then propose an effective path encoding scheme to encode the flow information for products. To retrieve the time information for products efficiently, we utilize a numbering scheme used in the XML area. Based on the path encoding scheme and the numbering scheme, we devise a storage scheme to process tracking queries and path oriented queries efficiently. Finally, we propose a method which translates the queries to SQL queries. Experimental results show that our approach can process the queries efficiently. On the average, our approach is about 680 times better than a recent technique in terms of query performance.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {291–302},
numpages = {12},
keywords = {rfid, supply chain management, prime number, region numbering scheme, path encoding scheme},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256692,
author = {Paredaens, Jan},
title = {Session Details: Research Session 8: XML Query Processing},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256692},
doi = {10.1145/3256692},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376650,
author = {Saito, Taro L. and Morishita, Shinichi},
title = {Relational-Style XML Query},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376650},
doi = {10.1145/1376616.1376650},
abstract = {We study the problem of querying relational data embedded in XML. Relational data can be represented by various tree structures in XML. However, current XML query methods, such as XPath and XQuery, demand explicit path expressions, and thus it is quite difficult for users to produce correct XML queries in the presence of structural variations.To solve this problem, we introduce a novel query method that automatically discovers various XML structures derived from relational data. A challenge in implementing our method is to reduce the cost of enumerating all possible tree structures that match the query. We show that the notion of functional dependencies has an important role in generating efficient query schedules that avoid irrelevant tree structures.Our proposed method, the relational-style XML query, has several advantages over traditional XML data management. These include removing the burden of designing strict tree-pattern schemas, enhancing the descriptions of relational data with XML's rich semantics, and taking advantage of schema evolution capability of XML. In addition, the independence of query statements from the underlying XML structure is advantageous for integrating XML data from several sources. We present extensive experimental results that confirm the scalability and tolerance of our query method for various sizes of XML data containing structural variations.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {303–314},
numpages = {12},
keywords = {amoeba join, xml, relational query, functional dependency},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376651,
author = {Huang, Yu and Liu, Ziyang and Chen, Yi},
title = {Query Biased Snippet Generation in XML Search},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376651},
doi = {10.1145/1376616.1376651},
abstract = {Snippets are used by almost every text search engine to complement ranking scheme in order to effectively handle user searches, which are inherently ambiguous and whose relevance semantics are difficult to assess. Despite the fact that XML is a standard representation format of web data, research on generating result snippets for XML search remains untouched.In this paper we present a system, eXtract, which addresses this important yet open problem. We identify that a good XML result snippet should be a self-contained meaningful information unit of a small size that effectively summarizes this query result and differentiates it from others, according to which users can quickly assess the relevance of the query result. We have designed and implemented a novel algorithm to satisfy these requirements and verified its efficiency and effectiveness through experiments.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {315–326},
numpages = {12},
keywords = {keyword search, xml, snippets},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376652,
author = {Lillis, Kostas and Pitoura, Evaggelia},
title = {Cooperative XPath Caching},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376652},
doi = {10.1145/1376616.1376652},
abstract = {Motivated by the fact that XML is increasingly being used in distributed applications, we propose building a cooperative caching scheme for XML documents. Our scheme allows sharing cache content among a number of peers. To facilitate sharing, a distributed prefix-based index is built based on the queries whose results are cached. In the loosely-coupled sharing approach, each peer stores in its local cache results of its own queries and just publishes the associated queries to the index. In the tightly-coupled approach, each peer is assigned a specific part of the query space and stores in its local cache the results of the corresponding queries. Both approaches result in a dynamic organization of content that evolves over time based on the query load, the number of peers and the overall storage available. We present a number of associated design choices such as using a DHT for distributing the prefix-based index and a proactive cache replacement policy. We also report on a number of experiments that show the benefits of cooperative caching and highlight the pros and cons of loosely and tightly coupled cache sharing.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {327–338},
numpages = {12},
keywords = {peer-to-peer systems, cache, xml},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376653,
author = {Ghelli, Giorgio and Onose, Nicola and Rose, Kristoffer and Simeon, Jerome},
title = {XML Query Optimization in the Presence of Side Effects},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376653},
doi = {10.1145/1376616.1376653},
abstract = {The emergence of database languages with side effects, notably for XML, raises significant challenges for database compilers and optimizers. In this paper, we extend an algebra for the W3C XML query language with operations that allow data to be immediately updated. We study the impact of that extension on logical optimization, join detection, and pipelining. The main result of this work is to show that, with proper care, a number of important optimizations based on nested relational algebras remain applicable in the presence of side effects. Our approach relies on an analysis of the conditions that must be checked in order for algebraic rewritings to hold. An implementation and experimental results demonstrate the effectiveness of the approach.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {339–352},
numpages = {14},
keywords = {xquery, xml, algebra, updates, side-effects, optimization},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256693,
author = {Meng, Xiaofeng},
title = {Session Details: Research Session 9: Strings and Time},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256693},
doi = {10.1145/3256693},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376655,
author = {Yang, Xiaochun and Wang, Bin and Li, Chen},
title = {Cost-Based Variable-Length-Gram Selection for String Collections to Support Approximate Queries Efficiently},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376655},
doi = {10.1145/1376616.1376655},
abstract = {Approximate queries on a collection of strings are important in many applications such as record linkage, spell checking, and Web search, where inconsistencies and errors exist in data as well as queries. Several existing algorithms use the concept of "grams," which are substrings of strings used as signatures for the strings to build index structures. A recently proposed technique, called VGRAM, improves the performance of these algorithms by using a carefully chosen dictionary of variable-length grams based on their requencies in the string collection. Since an index structure using fixed-length grams can be viewed as a special case of VGRAM, a fundamental problem arises naturally: what is the relationship between the gram dictionary and the performance of queries? We study this problem in this paper. We propose a dynamic programming algorithm for computing a tight lower bound on the number of common grams shared by two similar strings in order to improve query performance. We analyze how a gram dictionary affects the index structure of the string collection and ultimately the performance of queries. We also propose an algorithm for automatically computing a dictionary of high-quality grams for a workload of queries. Our experiments on real data sets show the improvement on query performance achieved by these techniques. To our best knowledge, this study is the first cost-based quantitative approach to deciding good grams for approximate string queries.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {353–364},
numpages = {12},
keywords = {vgram, gram selection, approximate string query},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376656,
author = {Athitsos, Vassilis and Papapetrou, Panagiotis and Potamias, Michalis and Kollios, George and Gunopulos, Dimitrios},
title = {Approximate Embedding-Based Subsequence Matching of Time Series},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376656},
doi = {10.1145/1376616.1376656},
abstract = {A method for approximate subsequence matching is introduced, that significantly improves the efficiency of subsequence matching in large time series data sets under the dynamic time warping (DTW) distance measure. Our method is called EBSM, shorthand for Embedding-Based Subsequence Matching. The key idea is to convert subsequence matching to vector matching using an embedding. This embedding maps each database time series into a sequence of vectors, so that every step of every time series in the database is mapped to a vector. The embedding is computed by applying full dynamic time warping between reference objects and each database time series. At runtime, given a query object, an embedding of that object is computed in the same manner, by running dynamic time warping between the reference objects and the query. Comparing the embedding of the query with the database vectors is used to efficiently identify relatively few areas of interest in the database sequences. Those areas of interest are then fully explored using the exact DTW-based subsequence matching algorithm. Experiments on a large, public time series data set produce speedups of over one order of magnitude compared to brute-force search, with very small losses (&lt; 1%) in retrieval accuracy.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {365–378},
numpages = {14},
keywords = {subsequence matching, embeddings, filter-and-refine retrieval, time series, similarity indexing, dynamic time warping},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376657,
author = {Gemulla, Rainer and Lehner, Wolfgang},
title = {Sampling Time-Based Sliding Windows in Bounded Space},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376657},
doi = {10.1145/1376616.1376657},
abstract = {Random sampling is an appealing approach to build synopses of large data streams because random samples can be used for a broad spectrum of analytical tasks. Users are often interested in analyzing only the most recent fraction of the data stream in order to avoid outdated results. In this paper, we focus on sampling schemes that sample from a sliding window over a recent time interval; such windows are a popular and highly comprehensible method to model recency. In this setting, the main challenge is to guarantee an upper bound on the space consumption of the sample while using the allotted space efficiently at the same time. The difficulty arises from the fact that the number of items in the window is unknown in advance and may vary significantly over time, so that the sampling fraction has to be adjusted dynamically. We consider uniform sampling schemes, which produce each sample of the same size with equal probability, and stratified sampling schemes, in which the window is divided into smaller strata and a uniform sample is maintained per stratum. For uniform sampling, we prove that it is impossible to guarantee a minimum sample size in bounded space. We then introduce a novel sampling scheme called bounded priority sampling (BPS), which requires only bounded space. We derive a lower bound on the expected sample size and show that BPS quickly adapts to changing data rates. For stratified sampling, we propose a merge-based stratification scheme (MBS), which maintains strata of approximately equal size. Compared to naive stratification, MBS has the advantage that the sample is evenly distributed across the window, so that no part of the window is over- or underrepresented. We conclude the paper with a feasibility study of our algorithms on large real-world datasets.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {379–392},
numpages = {14},
keywords = {stratified sampling, data stream sampling, uniform sampling, sliding window sampling},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376658,
author = {Patel, Dhaval and Hsu, Wynne and Lee, Mong Li},
title = {Mining Relationships among Interval-Based Events for Classification},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376658},
doi = {10.1145/1376616.1376658},
abstract = {Existing temporal pattern mining assumes that events do not have any duration. However, events in many real world applications have durations, and the relationships among these events are often complex. These relationships are modeled using a hierarchical representation that extends Allen's interval algebra. However, this representation is lossy as the exact relationships among the events cannot be fully recovered. In this paper, we augment the hierarchical representation with additional information to achieve a lossless representation. An efficient algorithm called IEMiner is designed to discover frequent temporal patterns from interval-based events. The algorithm employs two optimization techniques to reduce the search space and remove non-promising candidates. From the discovered temporal patterns, we build an interval-based classifier called IEClassifier to differentiate closely related classes. Experiments on both synthetic and real world datasets indicate the efficiency and scalability of the proposed approach, as well as the improved accuracy of IEClassifier.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {393–404},
numpages = {12},
keywords = {classifier for interval data, interval-based event mining, temporal relation},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256694,
author = {Agrawal, Divyakant},
title = {Session Details: Research Session 10: Graphs I},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256694},
doi = {10.1145/3256694},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376660,
author = {He, Huahai and Singh, Ambuj K.},
title = {Graphs-at-a-Time: Query Language and Access Methods for Graph Databases},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376660},
doi = {10.1145/1376616.1376660},
abstract = {With the prevalence of graph data in a variety of domains, there is an increasing need for a language to query and manipulate graphs with heterogeneous attributes and structures. We propose a query language for graph databases that supports arbitrary attributes on nodes, edges, and graphs. In this language, graphs are the basic unit of information and each query manipulates one or more collections of graphs. To allow for flexible compositions of graph structures, we extend the notion of formal languages from strings to the graph domain. We present a graph algebra extended from the relational algebra in which the selection operator is generalized to graph pattern matching and a composition operator is introduced for rewriting matched graphs. Then, we investigate access methods of the selection operator. Pattern matching over large graphs is challenging due to the NP-completeness of subgraph isomorphism. We address this by a combination of techniques: use of neighborhood subgraphs and profiles, joint reduction of the search space, and optimization of the search order. Experimental results on real and synthetic large graphs demonstrate that our graph specific optimizations outperform an SQL-based implementation by orders of magnitude.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {405–418},
numpages = {14},
keywords = {graph algebra, graph query language, query optimization},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376661,
author = {Navlakha, Saket and Rastogi, Rajeev and Shrivastava, Nisheeth},
title = {Graph Summarization with Bounded Error},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376661},
doi = {10.1145/1376616.1376661},
abstract = {We propose a highly compact two-part representation of a given graph G consisting of a graph summary and a set of corrections. The graph summary is an aggregate graph in which each node corresponds to a set of nodes in G, and each edge represents the edges between all pair of nodes in the two sets. On the other hand, the corrections portion specifies the list of edge-corrections that should be applied to the summary to recreate G. Our representations allow for both lossless and lossy graph compression with bounds on the introduced error. Further, in combination with the MDL principle, they yield highly intuitive coarse-level summaries of the input graph G. We develop algorithms to construct highly compressed graph representations with small sizes and guaranteed accuracy, and validate our approach through an extensive set of experiments with multiple real-life graph data sets.To the best of our knowledge, this is the first work to compute graph summaries using the MDL principle, and use the summaries (along with corrections) to compress graphs with bounded error.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {419–432},
numpages = {14},
keywords = {approximation, minimum description length, graph compression},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376662,
author = {Yan, Xifeng and Cheng, Hong and Han, Jiawei and Yu, Philip S.},
title = {Mining Significant Graph Patterns by Leap Search},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376662},
doi = {10.1145/1376616.1376662},
abstract = {With ever-increasing amounts of graph data from disparate sources, there has been a strong need for exploiting significant graph patterns with user-specified objective functions. Most objective functions are not antimonotonic, which could fail all of frequency-centric graph mining algorithms. In this paper, we give the first comprehensive study on general mining method aiming to find most significant patterns directly. Our new mining framework, called LEAP (Descending Leap Mine), is developed to exploit the correlation between structural similarity and significance similarity in a way that the most significant pattern could be identified quickly by searching dissimilar graph patterns. Two novel concepts, structural leap search and frequency descending mining, are proposed to support leap search in graph pattern space. Our new mining method revealed that the widely adopted branch-and-bound search in data mining literature is indeed not the best, thus sketching a new picture on scalable graph pattern discovery. Empirical results show that LEAP achieves orders of magnitude speedup in comparison with the state-of-the-art method. Furthermore, graph classifiers built on mined patterns outperform the up-to-date graph kernel method in terms of efficiency and accuracy, demonstrating the high promise of such patterns.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {433–444},
numpages = {12},
keywords = {classification, graph, pattern, optimality},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376663,
author = {Wang, Nan and Parthasarathy, Srinivasan and Tan, Kian-Lee and Tung, Anthony K. H.},
title = {CSV: Visualizing and Mining Cohesive Subgraphs},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376663},
doi = {10.1145/1376616.1376663},
abstract = {Extracting dense sub-components from graphs efficiently is an important objective in a wide range of application domains ranging from social network analysis to biological network analysis, from the World Wide Web to stock market analysis. Motivated by this need recently we have seen several new algorithms to tackle this problem based on the (frequent) pattern mining paradigm. A limitation of most of these methods is that they are highly sensitive to parameter settings, rely on exhaustive enumeration with exponential time complexity, and often fail to help the users understand the underlying distribution of components embedded within the host graph.In this article we propose an approximate algorithm, to mine and visualize cohesive subgraphs (dense sub components) within a large graph. The approach, refereed to as Cohesive Subgraph Visualization (CSV) relies on a novel mapping strategy that maps edges and nodes to a multi-dimensional space wherein dense areas in the mapped space correspond to cohesive subgraphs. The algorithm then walks through the dense regions in the mapped space to output a visual plot that effectively captures the overall dense sub-component distribution of the graph. Unlike extant algorithms with exponential complexity, CSV has a complexity of O(V2logV) when fixing the parameter mapping dimension, where V corresponds to the number of vertices in the graph, although for many real datasets the performance is typically sub-quadratic.We demonstrate the utility of CSV as a stand-alone tool for visual graph exploration and as a pre-filtering step to significantly scale up exact subgraph mining algorithms such as CLAN.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {445–458},
numpages = {14},
keywords = {visualization, graph density, graph mining, data mining, clique},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256695,
author = {Freytag, Johann Christoph},
title = {Session Details: Research Session 11: Privacy and Testing},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256695},
doi = {10.1145/3256695},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376665,
author = {Du, Wenliang and Teng, Zhouxuan and Zhu, Zutao},
title = {Privacy-MaxEnt: Integrating Background Knowledge in Privacy Quantification},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376665},
doi = {10.1145/1376616.1376665},
abstract = {Privacy-Preserving Data Publishing (PPDP) deals with the publication of microdata while preserving people' private information in the data. To measure how much private information can be preserved, privacy metrics is needed. An essential element for privacy metrics is the measure of how much adversaries can know about an individual's sensitive attributes (SA) if they know the individual's quasi-identifiers (QI), i.e., we need to measure P(SA|QI). Such a measure is hard to derive when adversaries' background knowledge has to be considered.We propose a systematic approach, Privacy-MaxEnt, to integrate background knowledge in privacy quantification. Our approach is based on the maximum entropy principle. We treat all the conditional probabilities P(SA|QI) as unknown variables; we treat the background knowledge as the constraints of these variables; in addition, we also formulate constraints from the published data. Our goal becomes finding a solution to those variables (the probabilities) that satisfy all these constraints. Although many solutions may exist, the most unbiased estimate of P(SA|QI) is the one that achieves the maximum entropy.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {459–472},
numpages = {14},
keywords = {privacy quantification, data publishing},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376666,
author = {Li, Jiexing and Tao, Yufei and Xiao, Xiaokui},
title = {Preservation of Proximity Privacy in Publishing Numerical Sensitive Data},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376666},
doi = {10.1145/1376616.1376666},
abstract = {We identify proximity breach as a privacy threat specific to numerical sensitive attributes in anonymized data publication. Such breach occurs when an adversary concludes with high confidence that the sensitive value of a victim individual must fall in a short interval --- even though the adversary may have low confidence about the victim's actual value.None of the existing anonymization principles (e.g., k-anonymity, l-diversity, etc.) can effectively prevent proximity breach. We remedy the problem by introducing a novel principle called (ε, m)-anonymity. Intuitively, the principle demands that, given a QI-group G, for every sensitive value x in G, at most 1/m of the tuples in G can have sensitive values "similar" to x, where the similarity is controlled by ε. We provide a careful analytical study of the theoretical characteristics of (ε, m)-anonymity, and the corresponding generalization algorithm. Our findings are verified by experiments with real data.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {473–486},
numpages = {14},
keywords = {(ε, numeric, anonymization, privacy, m)-anonymity},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376667,
author = {Benedikt, Michael and Jeffrey, Alan and Ley-Wild, Ruy},
title = {Stream Firewalling of Xml Constraints},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376667},
doi = {10.1145/1376616.1376667},
abstract = {As XML-based messages have become common in many client-server protocols, there is a need to protect application servers from invalid or dangerous messages. This leads to the XML stream firewalling problem; that of applying integrity constraints against a large number of simultaneous streams. We conduct the first investigation of a constraint engine optimized for the generation of XML stream firewalls. We isolate a class of DTDs and XPath constraints which support the generation of low-space filters, and provide algorithms for generating firewalls with low per-input-character time and per-stream space. We give experimental results which show that we have achieved these goals in practice.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {487–498},
numpages = {12},
keywords = {query processing, xml, streams, xpath},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376668,
author = {Mishra, Chaitanya and Koudas, Nick and Zuzarte, Calisto},
title = {Generating Targeted Queries for Database Testing},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376668},
doi = {10.1145/1376616.1376668},
abstract = {Tools for generating test queries for databases do not explicitly take into account the actual data in the database. As a consequence, such tools cannot guarantee suitable coverage of test cases commonly required for database testing. In this paper, we investigate the problem of generating queries that satisfy cardinality constraints on intermediate subexpressions when executed on a given test database. Such queries are required to test the performance of a database system under different operating conditions.We formally analyze this problem, quantify its difficulty and follow up this analysis with a description of a practical algorithm which utilizes sampling and space pruning techniques to quickly generate test queries that have desired properties. We present the results of an experimental evaluation of our approach as implemented in an open source data manager, demonstrating the utility of our proposal.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {499–510},
numpages = {12},
keywords = {query generation, database testing},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256696,
author = {Kosugi, Naoko},
title = {Session Details: Research Session 12: Query Optimization},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256696},
doi = {10.1145/3256696},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376670,
author = {He, Bingsheng and Yang, Ke and Fang, Rui and Lu, Mian and Govindaraju, Naga and Luo, Qiong and Sander, Pedro},
title = {Relational Joins on Graphics Processors},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376670},
doi = {10.1145/1376616.1376670},
abstract = {We present a novel design and implementation of relational join algorithms for new-generation graphics processing units (GPUs). The most recent GPU features include support for writing to random memory locations, efficient inter-processor communication, and a programming model for general-purpose computing. Taking advantage of these new features, we design a set of data-parallel primitives such as split and sort, and use these primitives to implement indexed or non-indexed nested-loop, sort-merge and hash joins. Our algorithms utilize the high parallelism as well as the high memory bandwidth of the GPU, and use parallel computation and memory optimizations to effectively reduce memory stalls. We have implemented our algorithms on a PC with an NVIDIA G80 GPU and an Intel quad-core CPU. Our GPU-based join algorithms are able to achieve a performance improvement of 2-7X over their optimized CPU-based counterparts.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {511–524},
numpages = {14},
keywords = {join, primitive, parallel processing, relational database, sort, graphics processors},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376671,
author = {Cao, Yu and Das, Gopal C. and Chan, Chee-Yong and Tan, Kian-Lee},
title = {Optimizing Complex Queries with Multiple Relation Instances},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376671},
doi = {10.1145/1376616.1376671},
abstract = {Today's query processing engines do not take advantage of the multiple occurrences of a relation in a query to improve performance. Instead, each instance is treated as a distinct relation and has its own independent table access method. In this paper, we present MAPLE, a Multi-instance-Aware PLan Evaluation engine that enables multiple instances of a relation to share one physical scan (called SharedScan) with limited buffer space. During execution, as SharedScan pulls a tuple for any instance, that tuple is also pushed to the buffers of other instances with matching predicates. To avoid buffer overflow, a novel interleaved execution strategy is proposed: whenever an instance's buffer becomes full, the execution is temporarily switched to a drainer (an ancestor blocking operator of the instance) to consume all the tuples in the buffer. Thus, the execution is interleaved between normal processing and drainers. We also propose a cost-based approach to generate a plan to maximize the shared scan benefit as well as to avoid interleaved execution deadlocks. MAPLE is light-weight and can be easily integrated into existing RDBMS executors. We have implemented MAPLE in PostgreSQL, and our experimental study on the TPC-DS benchmark shows significant reduction in execution time.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {525–538},
numpages = {14},
keywords = {shared scan, query optimization, query processing, interleaved execution},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376672,
author = {Moerkotte, Guido and Neumann, Thomas},
title = {Dynamic Programming Strikes Back},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376672},
doi = {10.1145/1376616.1376672},
abstract = {Two highly efficient algorithms are known for optimally ordering joins while avoiding cross products: DPccp, which is based on dynamic programming, and Top-Down Partition Search, based on memoization. Both have two severe limitations: They handle only (1) simple (binary) join predicates and (2) inner joins. However, real queries may contain complex join predicates, involving more than two relations, and outer joins as well as other non-inner joins.Taking the most efficient known join-ordering algorithm, DPccp, as a starting point, we first develop a new algorithm, DPhyp, which is capable to handle complex join predicates efficiently. We do so by modeling the query graph as a (variant of a) hypergraph and then reason about its connected subgraphs. Then, we present a technique to exploit this capability to efficiently handle the widest class of non-inner joins dealt with so far. Our experimental results show that this reformulation of non-inner joins as complex predicates can improve optimization time by orders of magnitude, compared to known algorithms dealing with complex join predicates and non-inner joins. Once again, this gives dynamic programming a distinct advantage over current memoization techniques.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {539–552},
numpages = {14},
keywords = {complex joins, query optimization, hypergraphs},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376673,
author = {Sereni, Damien and Avgustinov, Pavel and de Moor, Oege},
title = {Adding Magic to an Optimising Datalog Compiler},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376673},
doi = {10.1145/1376616.1376673},
abstract = {The magic-sets transformation is a useful technique for dramatically improving the performance of complex queries, but it has been observed that this transformation can also drastically reduce the performance of some queries. Successful implementations of magic in previous work require integration with the database optimiser to make appropriate decisions to guide the transformation (the sideways information passing strategy, or SIPS).This paper reports on the addition of the magic-sets transformation to a fully automatic optimising compiler from Datalog to SQL with no support from the database optimiser. We present an algorithm for making a good choice of SIPS using heuristics based on the sizes of relations. To achieve this, we define an abstract interpretation of Datalog programs to estimate the sizes of relations in the program.The effectiveness of our technique is evaluated over a substantial set of over a hundred queries, and in the context of the other optimisations performed by our compiler. It is shown that using the SIPS chosen by our algorithm, query performance is often significantly improved, as expected, but more importantly performance is never significantly degraded on queries that cannot benefit from magic.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {553–566},
numpages = {14},
keywords = {query optimisation, magic sets, datalog},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256697,
author = {Rastogi, Rajeev},
title = {Session Details: Research Session 13: Graphs II},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256697},
doi = {10.1145/3256697},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376675,
author = {Tian, Yuanyuan and Hankins, Richard A. and Patel, Jignesh M.},
title = {Efficient Aggregation for Graph Summarization},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376675},
doi = {10.1145/1376616.1376675},
abstract = {Graphs are widely used to model real world objects and their relationships, and large graph datasets are common in many application domains. To understand the underlying characteristics of large graphs, graph summarization techniques are critical. However, existing graph summarization methods are mostly statistical (studying statistics such as degree distributions, hop-plots and clustering coefficients). These statistical methods are very useful, but the resolutions of the summaries are hard to control.In this paper, we introduce two database-style operations to summarize graphs. Like the OLAP-style aggregation methods that allow users to drill-down or roll-up to control the resolution of summarization, our methods provide an analogous functionality for large graph datasets. The first operation, called SNAP, produces a summary graph by grouping nodes based on user-selected node attributes and relationships. The second operation, called k-SNAP, further allows users to control the resolutions of summaries and provides the "drill-down" and "roll-up" abilities to navigate through summaries with different resolutions. We propose an efficient algorithm to evaluate the SNAP operation. In addition, we prove that the k-SNAP computation is NP-complete. We propose two heuristic methods to approximate the k-SNAP results. Through extensive experiments on a variety of real and synthetic datasets, we demonstrate the effectiveness and efficiency of the proposed methods.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {567–580},
numpages = {14},
keywords = {graphs, aggregation, summarization, social networks},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376676,
author = {Gou, Gang and Chirkova, Rada},
title = {Efficient Algorithms for Exact Ranked Twig-Pattern Matching over Graphs},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376676},
doi = {10.1145/1376616.1376676},
abstract = {Querying large-scale graph-structured data with twig patterns is attracting growing interest. Generally, a twig pattern could have an extremely large, potentially exponential, number of matches in a graph. Retrieving and returning to the user this many answers may both incur high computational overhead and overwhelm the user.In this paper we propose two efficient algorithms, DP-B and DP-P, for retrieving top-ranked twig-pattern matches from large graphs. Our first algorithm, DP-B, is able to retrieve exact top-ranked answer matches from potentially exponentially many matches in time and space linear in the size of our data inputs even in the worst case. Further, beyond the linear-cost result of DP-B, our second algorithm, DP-P, could take far less than linear time and space cost in practice. To the best of our knowledge, our algorithms are the first to have these performance properties. Our experimental results demonstrate the high performance of both algorithms on large datasets. We also analyze and compare the performance trade-off between DP-B and DP-P from the theoretical and practical viewpoints.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {581–594},
numpages = {14},
keywords = {xml, graph, twig pattern matching, top-k},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376677,
author = {Jin, Ruoming and Xiang, Yang and Ruan, Ning and Wang, Haixun},
title = {Efficiently Answering Reachability Queries on Very Large Directed Graphs},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376677},
doi = {10.1145/1376616.1376677},
abstract = {Efficiently processing queries against very large graphs is an important research topic largely driven by emerging real world applications, as diverse as XML databases, GIS, web mining, social network analysis, ontologies, and bioinformatics. In particular, graph reachability has attracted a lot of research attention as reachability queries are not only common on graph databases, but they also serve as fundamental operations for many other graph queries. The main idea behind answering reachability queries in graphs is to build indices based on reachability labels. Essentially, each vertex in the graph is assigned with certain labels such that the reachability between any two vertices can be determined by their labels. Several approaches have been proposed for building these reachability labels; among them are interval labeling (tree cover) and 2-hop labeling. However, due to the large number of vertices in many real world graphs (some graphs can easily contain millions of vertices), the computational cost and (index) size of the labels using existing methods would prove too expensive to be practical. In this paper, we introduce a novel graph structure, referred to as path-tree, to help labeling very large graphs. The path-tree cover is a spanning subgraph of G in a tree shape. We demonstrate both analytically and empirically the effectiveness of our new approaches.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {595–608},
numpages = {14},
keywords = {reachability queries, maximal directed spanning tree, graph indexing, transitive closure, path-tree cover},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376678,
author = {Chen, Ding and Chan, Chee-Yong},
title = {Minimization of Tree Pattern Queries with Constraints},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376678},
doi = {10.1145/1376616.1376678},
abstract = {Tree pattern queries (TPQs) provide a natural and easy formalism to query tree-structured XML data, and the efficient processing of such queries has attracted a lot of attention. Since the size of a TPQ is a key determinant of its evaluation cost, recent research has focused on the problem of query minimization using integrity constraints to eliminate redundant query nodes; specifically, TPQ minimization has been studied for the class of forward and subtype constraints (FT-constraints). In this paper, we explore the TPQ minimization problem further for a richer class of FBST-constraints that includes not only FT-constraints but also backward and sibling constraints. By exploiting the properties of minimal queries under FBST-constraints, we propose efficient algorithms to both compute a single minimal query as well as enumerate all minimal queries. In addition, we also develop more efficient minimization algorithms for the previously studied class of FT-constraints. Our experimental study demonstrates the effectiveness and efficiency of query minimization using FBST-constraints.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {609–622},
numpages = {14},
keywords = {tree pattern queries, xpath, simulation, integrity constraints, chase, query minimization, xml},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256698,
author = {Shim, Kyuseok},
title = {Session Details: Research Session 14: Ordered Data},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256698},
doi = {10.1145/3256698},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376680,
author = {Mitra, Soumyadeb and Winslett, Marianne and Hsu, Windsor W.},
title = {Query-Based Partitioning of Documents and Indexes for Information Lifecycle Management},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376680},
doi = {10.1145/1376616.1376680},
abstract = {Regulations require businesses to archive many electronic documents for extended periods of time. Given the sheer volume of documents and the response time requirements, documents that are unlikely to ever be accessed should be stored on an inexpensive device (such as tape), while documents that are likely to be accessed should be placed on a more expensive, higher-performance device. Unfortunately, traditional data partitioning techniques either require substantial manual involvement, or are not suitable for read-rarely workloads. In this paper, we present a novel technique to address this problem. We estimate the future access likelihood for a document based on past workloads of keyword queries and the click-through behavior for top-K query answers, then use this information to drive partitioning decisions. Our overall best scheme, the document-split inverted index, does not require any parameter tuning and yet performs close to the optimal partitioning strategy. Experiments show that document-split partitioning improves performance on a large intranet query workload by a factor of 4 when we add a fast storage server that holds 20% of the data.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {623–636},
numpages = {14},
keywords = {inverted index partitioning, tiered storage, keyword query, hierarchical storage management, lifecycle management, document partitioning},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376681,
author = {Shaull, Ross and Shrira, Liuba and Xu, Hao},
title = {Skippy: A New Snapshot Indexing Method for Time Travel in the Storage Manager},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376681},
doi = {10.1145/1376616.1376681},
abstract = {The storage manager of a general-purpose database system can retain consistent disk page level snapshots and run application programs "back-in-time" against long-lived past states, virtualized to look like the current state. This opens the possibility that functions, such as on-line trend analysis and audit, formerly available in specialized temporal databases, can become available to general applications in general-purpose databases.Up to now, in-place updating database systems had no satisfactory way to run programs on-line over long-lived, disk page level, copy-on-write snapshots, because there was no efficient indexing method for such snapshots. We describe Skippy, a new indexing approach that solves this problem. Using Skippy, database application code can run against an arbitrarily old snapshot, and iterate over snapshot ranges, as efficiently it can access recent snapshots, for all update workloads. Performance evaluation of Skippy, based on theoretical analysis and experimental measurements, indicates that the new approach provides efficient access to snapshots at low cost.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {637–648},
numpages = {12},
keywords = {versions, database, snapshots, time travel},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376682,
author = {Lo, Eric and Kao, Ben and Ho, Wai-Shing and Lee, Sau Dan and Chui, Chun Kit and Cheung, David W.},
title = {OLAP on Sequence Data},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376682},
doi = {10.1145/1376616.1376682},
abstract = {Many kinds of real-life data exhibit logical ordering among their data items and are thus sequential in nature. However, traditional online analytical processing (OLAP) systems and techniques were not designed for sequence data and they are incapable of supporting sequence data analysis. In this paper, we propose the concept of Sequence OLAP, or S-OLAP for short. The biggest distinction of S-OLAP from traditional OLAP is that a sequence can be characterized not only by the attributes' values of its constituting items, but also by the subsequence/substring patterns it possesses. This paper studies many aspects related to Sequence OLAP. The concepts of sequence cuboid and sequence data cube are introduced. A prototype S-OLAP system is built in order to validate the proposed concepts. The prototype is able to support "pattern-based" grouping and aggregation, which is currently not supported by any OLAP system. The implementation details of the prototype system as well as experimental results are presented.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {649–660},
numpages = {12},
keywords = {sequence data, olap},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376683,
author = {Sinha, Ranjan and Puglisi, Simon and Moffat, Alistair and Turpin, Andrew},
title = {Improving Suffix Array Locality for Fast Pattern Matching on Disk},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376683},
doi = {10.1145/1376616.1376683},
abstract = {The suffix tree (or equivalently, the enhanced suffix array) provides efficient solutions to many problems involving pattern matching and pattern discovery in large strings, such as those arising in computational biology. Here we address the problem of arranging a suffix array on disk so that querying is fast in practice. We show that the combination of a small trie and a suffix array-like blocked data structure allows queries to be answered as much as three times faster than the best alternative disk-based suffix array arrangement. Construction of our data structure requires only modest processing time on top of that required to build the suffix tree, and requires negligible extra memory.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {661–672},
numpages = {12},
keywords = {suffix array, suffix tree, disk-based algorithm, secondary storage, pattern matching},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inbook{10.1145/3256699,
author = {Tay, Y. C.},
title = {Session Details: Research Session 15: Probabilistic I},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256699},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1}
}

@inproceedings{10.1145/1376616.1376685,
author = {Hua, Ming and Pei, Jian and Zhang, Wenjie and Lin, Xuemin},
title = {Ranking Queries on Uncertain Data: A Probabilistic Threshold Approach},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376685},
doi = {10.1145/1376616.1376685},
abstract = {Uncertain data is inherent in a few important applications such as environmental surveillance and mobile object tracking. Top-k queries (also known as ranking queries) are often natural and useful in analyzing uncertain data in those applications. In this paper, we study the problem of answering probabilistic threshold top-k queries on uncertain data, which computes uncertain records taking a probability of at least p to be in the top-k list where p is a user specified probability threshold. We present an efficient exact algorithm, a fast sampling algorithm, and a Poisson approximation based algorithm. An empirical study using real and synthetic data sets verifies the effectiveness of probabilistic threshold top-k queries and the efficiency of our methods.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {673–686},
numpages = {14},
keywords = {query processing, uncertain data, probabilistic threshold top-k queries},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376686,
author = {Jampani, Ravi and Xu, Fei and Wu, Mingxi and Perez, Luis Leopoldo and Jermaine, Christopher and Haas, Peter J.},
title = {MCDB: A Monte Carlo Approach to Managing Uncertain Data},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376686},
doi = {10.1145/1376616.1376686},
abstract = {To deal with data uncertainty, existing probabilistic database systems augment tuples with attribute-level or tuple-level probability values, which are loaded into the database along with the data itself. This approach can severely limit the system's ability to gracefully handle complex or unforeseen types of uncertainty, and does not permit the uncertainty model to be dynamically parameterized according to the current state of the database. We introduce MCDB, a system for managing uncertain data that is based on a Monte Carlo approach. MCDB represents uncertainty via "VG functions," which are used to pseudorandomly generate realized values for uncertain attributes. VG functions can be parameterized on the results of SQL queries over "parameter tables" that are stored in the database, facilitating what-if analyses. By storing parameters, and not probabilities, and by estimating, rather than exactly computing, the probability distribution over possible query answers, MCDB avoids many of the limitations of prior systems. For example, MCDB can easily handle arbitrary joint probability distributions over discrete or continuous attributes, arbitrarily complex SQL queries, and arbitrary functionals of the query-result distribution such as means, variances, and quantiles. To achieve good performance, MCDB uses novel query processing techniques, executing a query plan exactly once, but over "tuple bundles" instead of ordinary tuples. Experiments indicate that our enhanced functionality can be obtained with acceptable overheads relative to traditional systems.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {687–700},
numpages = {14},
keywords = {mcdb, monte carlo, uncertainty},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376687,
author = {Kimelfeld, Benny and Kosharovsky, Yuri and Sagiv, Yehoshua},
title = {Query Efficiency in Probabilistic XML Models},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376687},
doi = {10.1145/1376616.1376687},
abstract = {Various known models of probabilistic XML can be represented as instantiations of abstract p-documents. Such documents have, in addition to ordinary nodes, distributional nodes that specify the probabilistic process of generating a random document. Within this abstraction, families of pdocuments, which are natural extensions and combinations of previous models, are considered. The focus is on efficiency of applying twig queries (with projection) to p-documents. A closely related issue is the ability to (efficiently) translate a given document of one family into another family. Furthermore, both of these tasks have two variants that correspond to the value-based and object-based semantics.The translation relationships among different families of p-documents are studied. An efficient algorithm for evaluating twig queries over one specific family is given. This algorithm generalizes a known algorithm and significantly improves its running time, both analytically and experimentally. It is shown that this family is the maximal, among the ones considered, for which query evaluation is tractable. For the rest, efficient approximate algorithms for query evaluation are presented.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {701–714},
numpages = {14},
keywords = {probabilistic databases, query optimization, probabilistic xml, approximate query evaluation, query processing},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376688,
author = {R\'{e}, Christopher and Letchner, Julie and Balazinksa, Magdalena and Suciu, Dan},
title = {Event Queries on Correlated Probabilistic Streams},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376688},
doi = {10.1145/1376616.1376688},
abstract = {A major problem in detecting events in streams of data is that the data can be imprecise (e.g. RFID data). However, current state-ofthe-art event detection systems such as Cayuga [14], SASE [46] or SnoopIB[1], assume the data is precise. Noise in the data can be captured using techniques such as hidden Markov models. Inference on these models creates streams of probabilistic events which cannot be directly queried by existing systems. To address this challenge we propose Lahar1, an event processing system for probabilistic event streams. By exploiting the probabilistic nature of the data, Lahar yields a much higher recall and precision than deterministic techniques operating over only the most probable tuples. By using a novel static analysis and novel algorithms, Lahar processes data orders of magnitude more efficiently than a na\"{\i}ve approach based on sampling. In this paper, we present Lahar's static analysis and core algorithms. We demonstrate the quality and performance of our approach through experiments with our prototype implementation and comparisons with alternate methods.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {715–728},
numpages = {14},
keywords = {probabilistic databases, hidden markov models, streams, query processing},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256700,
author = {Chrysanthis, Panos K.},
title = {Session Details: Research Session 16: Transactions and Distribution},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256700},
doi = {10.1145/3256700},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376690,
author = {Cahill, Michael J. and R\"{o}hm, Uwe and Fekete, Alan D.},
title = {Serializable Isolation for Snapshot Databases},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376690},
doi = {10.1145/1376616.1376690},
abstract = {Many popular database management systems offer snapshot isolation rather than full serializability. There are well-known anomalies permitted by snapshot isolation that can lead to violations of data consistency by interleaving transactions that individually maintain consistency. Until now, the only way to prevent these anomalies was to modify the applications by introducing artificial locking or update conflicts, following careful analysis of conflicts between all pairs of transactions.This paper describes a modification to the concurrency control algorithm of a database management system that automatically detects and prevents snapshot isolation anomalies at runtime for arbitrary applications, thus providing serializable isolation. The new algorithm preserves the properties that make snapshot isolation attractive, including that readers do not block writers and vice versa. An implementation and performance study of the algorithm are described, showing that the throughput approaches that of snapshot isolation in most cases.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {729–738},
numpages = {10},
keywords = {multiversion concurrency control, serializability theory, snapshot isolation},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376691,
author = {Cecchet, Emmanuel and Candea, George and Ailamaki, Anastasia},
title = {Middleware-Based Database Replication: The Gaps between Theory and Practice},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376691},
doi = {10.1145/1376616.1376691},
abstract = {The need for high availability and performance in data management systems has been fueling a long running interest in database replication from both academia and industry. However, academic groups often attack replication problems in isolation, overlooking the need for completeness in their solutions, while commercial teams take a holistic approach that often misses opportunities for fundamental innovation. This has created over time a gap between academic research and industrial practice.This paper aims to characterize the gap along three axes: performance, availability, and administration. We build on our own experience developing and deploying replication systems in commercial and academic settings, as well as on a large body of prior related work. We sift through representative examples from the last decade of open-source, academic, and commercial database replication systems and combine this material with case studies from real systems deployed at Fortune 500 customers. We propose two agendas, one for academic research and one for industrial R&amp;D, which we believe can bridge the gap within 5-10 years. This way, we hope to both motivate and help researchers in making the theory and practice of middleware-based database replication more relevant to each other.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {739–752},
numpages = {14},
keywords = {database replication, middleware, practice and experience},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376692,
author = {Vlachou, Akrivi and Doulkeridis, Christos and N\o{}rv\r{a}g, Kjetil and Vazirgiannis, Michalis},
title = {On Efficient Top-k Query Processing in Highly Distributed Environments},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376692},
doi = {10.1145/1376616.1376692},
abstract = {Lately the advances in centralized database management systems show a trend towards supporting rank-aware query operators, like top-k, that enable users to retrieve only the most interesting data objects. A challenging problem is to support rank-aware queries in highly distributed environments. In this paper, we present a novel approach, called SPEERTO, for top-k query processing in large-scale peer-to-peer networks, where the dataset is horizontally distributed over the peers. Towards this goal, we explore the applicability of the skyline operator for efficiently routing top-k queries in a large super-peer network. Relying on a thresholding scheme, SPEERTO returns the exact results progressively to the user, while the number of queried super-peers and transferred data is minimized. Finally, we propose different variations of SPEERTO that allow balancing between transferred data volume and response time. Through simulations we demonstrate the feasibility of our approach.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {753–764},
numpages = {12},
keywords = {skyline operator, top-k queries, peer-to-peer systems},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376693,
author = {Silberstein, Adam and Cooper, Brian F. and Srivastava, Utkarsh and Vee, Erik and Yerneni, Ramana and Ramakrishnan, Raghu},
title = {Efficient Bulk Insertion into a Distributed Ordered Table},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376693},
doi = {10.1145/1376616.1376693},
abstract = {We study the problem of bulk-inserting records into tables in a system that horizontally range-partitions data over a large cluster of shared-nothing machines. Each table partition contains a contiguous portion of the table's key range, and must accept all records inserted into that range. Examples of such systems include BigTable[8] at Google, and PNUTS [15] at Yahoo! During bulk inserts into an existing table, if most of the inserted records end up going into a small number of data partitions, the obtained throughput may be very poor due to ineffective use of cluster parallelism. We propose a novel approach in which a planning phase is invoked before the actual insertions. By creating new partitions and intelligently distributing partitions across machines, the planning phase ensures that the insertion load will be well-balanced. Since there is a tradeoff between the cost of moving partitions and the resulting throughput gain, the planning phase must minimize the sum of partition movement time and insertion time. We show that this problem is a variation of NP-hard bin-packing, reduce it to a problem of packing vectors, and then give a solution with provable approximation guarantees. We evaluate our approach on a prototype system deployed on a cluster of 50 machines, and show that it yields significant improvements over more na\"{\i}ve techniques.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {765–778},
numpages = {14},
keywords = {bulk loading, distributed and parallel databases, ordered tables},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256701,
author = {Yang, Jun},
title = {Session Details: Research Session 17: Probabilistic II},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256701},
doi = {10.1145/3256701},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376695,
author = {Li, Xiaolei and Han, Jiawei and Yin, Zhijun and Lee, Jae-Gil and Sun, Yizhou},
title = {Sampling Cube: A Framework for Statistical Olap over Sampling Data},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376695},
doi = {10.1145/1376616.1376695},
abstract = {Sampling is a popular method of data collection when it is impossible or too costly to reach the entire population. For example, television show ratings in the United States are gathered from a sample of roughly 5,000 households. To use the results effectively, the samples are further partitioned in a multidimensional space based on multiple attribute values. This naturally leads to the desirability of OLAP (Online Analytical Processing) over sampling data. However, unlike traditional data, sampling data is inherently uncertain, i.e., not representing the full data in the population. Thus, it is desirable to return not only query results but also the confidence intervals indicating the reliability of the results. Moreover, a certain segment in a multidimensional space may contain none or too few samples. This requires some additional analysis to return trustable results.In this paper we propose a Sampling Cube framework, which efficiently calculates confidence intervals for any multidimensional query and uses the OLAP structure to group similar segments to increase sampling size when needed. Further, to handle high dimensional data, a Sampling Cube Shell method is proposed to effectively reduce the storage requirement while still preserving query result quality.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {779–790},
numpages = {12},
keywords = {olap sampling},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376696,
author = {Thiagarajan, Arvind and Madden, Samuel},
title = {Querying Continuous Functions in a Database System},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376696},
doi = {10.1145/1376616.1376696},
abstract = {Many scientific, financial, data mining and sensor network applications need to work with continuous, rather than discrete data e.g., temperature as a function of location, or stock prices or vehicle trajectories as a function of time. Querying raw or discrete data is unsatisfactory for these applications -- e.g., in a sensor network, it is necessary to interpolate sensor readings to predict values at locations where sensors are not deployed. In other situations, raw data can be inaccurate owing to measurement errors, and it is useful to fit continuous functions to raw data and query the functions, rather than raw data itself -- e.g., fitting a smooth curve to noisy sensor readings, or a smooth trajectory to GPS data containing gaps or outliers. Existing databases do not support storing or querying continuous functions, short of brute-force discretization of functions into a collection of tuples. We present FunctionDB, a novel database system that treats mathematical functions as first-class citizens that can be queried like traditional relations. The key contribution of FunctionDB is an efficient and accurate algebraic query processor - for the broad class of multi-variable polynomial functions, FunctionDB executes queries directly on the algebraic representation of functions without materializing them into discrete points, using symbolic operations: zero finding, variable substitution, and integration. Even when closed form solutions are intractable, FunctionDB leverages symbolic approximation operations to improve performance. We evaluate FunctionDB on real data sets from a temperature sensor network, and on traffic traces from Boston roads. We show that operating in the functional domain has substantial advantages in terms of accuracy (15-30%) and up to order of magnitude (10x-100x) performance wins over existing approaches that represent models as discrete collections of points.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {791–804},
numpages = {14},
keywords = {regression, model based views, imprecise data, functions, uncertain data, erroneous data, symbolic query processing, continuous data},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376697,
author = {Chakrabarti, Kaushik and Chaudhuri, Surajit and Ganti, Venkatesh and Xin, Dong},
title = {An Efficient Filter for Approximate Membership Checking},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376697},
doi = {10.1145/1376616.1376697},
abstract = {We consider the problem of identifying sub-strings of input text strings that approximately match with some member of a potentially large dictionary. This problem arises in several important applications such as extracting named entities from text documents and identifying biological concepts from biomedical literature. In this paper, we develop a filter-verification framework, and propose a novel in-memory filter structure. That is, we first quickly filter out sub-strings that cannot match with any dictionary member, and then verify the remaining sub-strings against the dictionary. Our method does not produce false negatives. We demonstrate the efficiency and effectiveness of our filter over real datasets, and show that it significantly outperforms the previous best-known methods in terms of both filtering power and computation time.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {805–818},
numpages = {14},
keywords = {string match, approximate membership checking, filtering},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376698,
author = {Zhang, Qin and Li, Feifei and Yi, Ke},
title = {Finding Frequent Items in Probabilistic Data},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376698},
doi = {10.1145/1376616.1376698},
abstract = {Computing statistical information on probabilistic data has attracted a lot of attention recently, as the data generated from a wide range of data sources are inherently fuzzy or uncertain. In this paper, we study an important statistical query on probabilistic data: finding the frequent items. One straightforward approach to identify the frequent items in a probabilistic data set is to simply compute the expected frequency of an item and decide if it exceeds a certain fraction of the expected size of the whole data set. However, this simple definition misses important information about the internal structure of the probabilistic data and the interplay among all the uncertain entities. Thus, we propose a new definition based on the possible world semantics that has been widely adopted for many query types in uncertain data management, trying to find all the items that are likely to be frequent in a randomly generated possible world. Our approach naturally leads to the study of ranking frequent items based on confidence as well.Finding likely frequent items in probabilistic data turns out to be much more difficult. We first propose exact algorithms for offline data with either quadratic or cubic time. Next, we design novel sampling-based algorithms for streaming data to find all approximately likely frequent items with theoretically guaranteed high probability and accuracy. Our sampling schemes consume sublinear memory and exhibit excellent scalability. Finally, we verify the effectiveness and efficiency of our algorithms using both real and synthetic data sets with extensive experimental evaluations.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {819–832},
numpages = {14},
keywords = {uncertain databases, probabilistic data, heavy hitters, x-relation model},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256702,
author = {Lohman, Guy M.},
title = {Session Details: Research Session 18: Database Integration As You Go},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256702},
doi = {10.1145/3256702},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376700,
author = {Chiticariu, Laura and Kolaitis, Phokion G. and Popa, Lucian},
title = {Interactive Generation of Integrated Schemas},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376700},
doi = {10.1145/1376616.1376700},
abstract = {Schema integration is the problem of creating a unified target schema based on a set of existing source schemas that relate to each other via specified correspondences. The unified schema gives a standard representation of the data, thus offering a way to deal with the heterogeneity in the sources. In this paper, we develop a method and a design tool that provide: 1) adaptive enumeration of multiple interesting integrated schemas, and 2) easy-to-use capabilities for refining the enumerated schemas via user interaction. Our method is a departure from previous approaches to schema integration, which do not offer a systematic exploration of the possible integrated schemas.The method operates at a logical level, where we recast each source schema into a graph of concepts with Has-A relationships. We then identify matching concepts in different graphs by taking into account the correspondences between their attributes. For every pair of matching concepts, we have two choices: merge them into one integrated concept or keep them as separate concepts. We develop an algorithm that can systematically output, without duplication, all possible integrated schemas resulting from the previous choices. For each integrated schema, the algorithm also generates a mapping from the source schemas to the integrated schema that has precise information-preserving properties. Furthermore, we avoid a full enumeration, by allowing users to specify constraints on the merging process, based on the schemas produced so far. These constraints are then incorporated in the enumeration of the subsequent schemas. The result is an adaptive and interactive enumeration method that significantly reduces the space of alternative schemas, and facilitates the selection of the final integrated schema.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {833–846},
numpages = {14},
keywords = {model management, schema integration, data integration, interactive generation, concept graph, schema mapping},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376701,
author = {Jeffery, Shawn R. and Franklin, Michael J. and Halevy, Alon Y.},
title = {Pay-as-You-Go User Feedback for Dataspace Systems},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376701},
doi = {10.1145/1376616.1376701},
abstract = {A primary challenge to large-scale data integration is creating semantic equivalences between elements from different data sources that correspond to the same real-world entity or concept. Dataspaces propose a pay-as-you-go approach: automated mechanisms such as schema matching and reference reconciliation provide initial correspondences, termed candidate matches, and then user feedback is used to incrementally confirm these matches. The key to this approach is to determine in what order to solicit user feedback for confirming candidate matches.In this paper, we develop a decision-theoretic framework for ordering candidate matches for user confirmation using the concept of the value of perfect information (VPI). At the core of this concept is a utility function that quantifies the desirability of a given state; thus, we devise a utility function for dataspaces based on query result quality. We show in practice how to efficiently apply VPI in concert with this utility function to order user confirmations. A detailed experimental evaluation on both real and synthetic datasets shows that the ordering of user feedback produced by this VPI-based approach yields a dataspace with a significantly higher utility than a wide range of other ordering strategies. Finally, we outline the design of Roomba, a system that utilizes this decision-theoretic framework to guide a dataspace in soliciting user feedback in a pay-as-you-go manner.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {847–860},
numpages = {14},
keywords = {decision theory, dataspace, data integration, user feedback},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376702,
author = {Das Sarma, Anish and Dong, Xin and Halevy, Alon},
title = {Bootstrapping Pay-as-You-Go Data Integration Systems},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376702},
doi = {10.1145/1376616.1376702},
abstract = {Data integration systems offer a uniform interface to a set of data sources. Despite recent progress, setting up and maintaining a data integration application still requires significant upfront effort of creating a mediated schema and semantic mappings from the data sources to the mediated schema. Many application contexts involving multiple data sources (e.g., the web, personal information management, enterprise intranets) do not require full integration in order to provide useful services, motivating a pay-as-you-go approach to integration. With that approach, a system starts with very few (or inaccurate) semantic mappings and these mappings are improved over time as deemed necessary.This paper describes the first completely self-configuring data integration system. The goal of our work is to investigate how advanced of a starting point we can provide a pay-as-you-go system. Our system is based on the new concept of a probabilistic mediated schema that is automatically created from the data sources. We automatically create probabilistic schema mappings between the sources and the mediated schema. We describe experiments in multiple domains, including 50-800 data sources, and show that our system is able to produce high-quality answers with no human intervention.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {861–874},
numpages = {14},
keywords = {pay-as-you-go, mediated schema, data integration, schema mapping},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376703,
author = {Qi, Yan and Candan, K. Sel\c{c}uk and Tatemura, Junichi and Chen, Songting and Liao, Fenglin},
title = {Supporting OLAP Operations over Imperfectly Integrated Taxonomies},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376703},
doi = {10.1145/1376616.1376703},
abstract = {OLAP is an important tool in decision support. With the help of domain knowledge, such as hierarchies of attribute values, OLAP helps the user observe the effects of various decisions. One assumption of most OLAP operations is that the available domain knowledge is precise. In particular, they assume that the hierarchy of values over which the user can navigate forms a taxonomy. In this paper, we first note that when multiple heterogeneous data sources are involved in the gathering of the data and the associated domain knowledge, the integrated knowledge-base, constructed by combining locally available taxonomies based on the concept matchings, may not be a taxonomy itself. Specifically, existence of intersections among concepts from different sources compromises the tree-structure of the integrated taxonomy and prevents effective use of hierarchical navigation techniques, such as drill-down and roll-up. To cope with this, we introduce concept un-classification, where a select few of the concepts are eliminated to ensure that the remaining structure is a navigable taxonomy, without concept intersections. Since un-classifying an originally classified data is not desirable, we consider ways to minimize un-classification in the process. We introduce a cost model which captures the imprecision caused by the un-classification process and we formulate the problem of finding an un-classification strategy which eliminates intersections and which adds minimal imprecision to the resulting structure. We show that, when performed naively, this task can be very costly and thus we propose a bottom-up preprocessing strategy which supports basic navigational analytics operations, such as drill-down and roll-up. Experiments over synthetic and real-life data verified the effectiveness and efficiency of our approach.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {875–888},
numpages = {14},
keywords = {taxonomy correction, imperfect integration, imprecise data, olap},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256703,
author = {Reinwald, Berthold},
title = {Session Details: Research Session 19: Keywords on Structure},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256703},
doi = {10.1145/3256703},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376705,
author = {Tata, Sandeep and Lohman, Guy M.},
title = {SQAK: Doing More with Keywords},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376705},
doi = {10.1145/1376616.1376705},
abstract = {Today's enterprise databases are large and complex, often relating hundreds of entities. Enabling ordinary users to query such databases and derive value from them has been of great interest in database research. Today, keyword search over relational databases allows users to find pieces of information without having to write complicated SQL queries. However, in order to compute even simple aggregates, a user is required to write a SQL statement and can no longer use simple keywords. This not only requires the ordinary user to learn SQL, but also to learn the schema of the complex database in detail in order to correctly construct the required query. This greatly limits the options of the user who wishes to examine a database in more depth.As a solution to this problem, we propose a framework called SQAK1 (SQL Aggregates using Keywords) that enables users to pose aggregate queries using simple keywords with little or no knowledge of the schema. SQAK provides a novel and exciting way to trade-off some of the expressive power of SQL in exchange for the ability to express a large class of aggregate queries using simple keywords. SQAK accomplishes this by taking advantage of the data in the database and the schema (tables, attributes, keys, and referential constraints). SQAK does not require any changes to the database engine and can be used with any existing database. We demonstrate using several experiments that SQAK is effective and can be an enormously powerful tool for ordinary users.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {889–902},
numpages = {14},
keywords = {aggregates, query tools, keyword queries, sql, relational database},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376706,
author = {Li, Guoliang and Ooi, Beng Chin and Feng, Jianhua and Wang, Jianyong and Zhou, Lizhu},
title = {EASE: An Effective 3-in-1 Keyword Search Method for Unstructured, Semi-Structured and Structured Data},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376706},
doi = {10.1145/1376616.1376706},
abstract = {Conventional keyword search engines are restricted to a given data model and cannot easily adapt to unstructured, semi-structured or structured data. In this paper, we propose an efficient and adaptive keyword search method, called EASE, for indexing and querying large collections of heterogenous data. To achieve high efficiency in processing keyword queries, we first model unstructured, semi-structured and structured data as graphs, and then summarize the graphs and construct graph indices instead of using traditional inverted indices. We propose an extended inverted index to facilitate keyword-based search, and present a novel ranking mechanism for enhancing search effectiveness. We have conducted an extensive experimental study using real datasets, and the results show that EASE achieves both high search efficiency and high accuracy, and outperforms the existing approaches significantly.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {903–914},
numpages = {12},
keywords = {graph index, indexing, keyword search, ranking},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376707,
author = {Vu, Quang Hieu and Ooi, Beng Chin and Papadias, Dimitris and Tung, Anthony K. H.},
title = {A Graph Method for Keyword-Based Selection of the Top-K Databases},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376707},
doi = {10.1145/1376616.1376707},
abstract = {While database management systems offer a comprehensive solution to data storage, they require deep knowledge of the schema, as well as the data manipulation language, in order to perform effective retrieval. Since these requirements pose a problem to lay or occasional users, several methods incorporate keyword search (KS) into relational databases. However, most of the existing techniques focus on querying a single DBMS. On the other hand, the proliferation of distributed databases in several conventional and emerging applications necessitates the support for keyword-based data sharing and querying over multiple DMBSs. In order to avoid the high cost of searching in numerous, potentially irrelevant, databases in such systems, we propose G-KS, a novel method for selecting the top-K candidates based on their potential to contain results for a given query. G-KSsummarizes each database by a keyword relationship graph, where nodes represent terms and edges describe relationships between them. Keyword relationship graphs are utilized for computing the similarity between each database and a KS query, so that, during query processing, only the most promising databases are searched. An extensive experimental evaluation demonstrates that G-KS outperforms the current state-of-the-art technique on all aspects, including precision, recall, efficiency, space overhead and flexibility of accommodating different semantics.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {915–926},
numpages = {12},
keywords = {graph, information retrieval, database summary, distributed databases, keyword search, relational databases},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376708,
author = {Golenberg, Konstantin and Kimelfeld, Benny and Sagiv, Yehoshua},
title = {Keyword Proximity Search in Complex Data Graphs},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376708},
doi = {10.1145/1376616.1376708},
abstract = {In keyword search over data graphs, an answer is a nonredundant subtree that includes the given keywords. An algorithm for enumerating answers is presented within an architecture that has two main components: an engine that generates a set of candidate answers and a ranker that evaluates their score. To be effective, the engine must have three fundamental properties. It should not miss relevant answers, has to be efficient and must generate the answers in an order that is highly correlated with the desired ranking. It is shown that none of the existing systems has implemented an engine that has all of these properties. In contrast, this paper presents an engine that generates all the answers with provable guarantees. Experiments show that the engine performs well in practice. It is also shown how to adapt this engine to queries under the OR semantics. In addition, this paper presents a novel approach for implementing rankers destined for eliminating redundancy. Essentially, an answer is ranked according to its individual properties (relevancy) and its intersection with the answers that have already been presented to the user. Within this approach, experiments with specific rankers are described.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {927–940},
numpages = {14},
keywords = {approximate top-k answers, subtree enumeration by height, redundancy elimination, keyword proximity search, information retrieval on graphs},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256704,
author = {Ross, Kenneth A.},
title = {Session Details: Research Session 20: Tuning and Probing},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256704},
doi = {10.1145/3256704},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376710,
author = {Bruno, Nicolas and Nehme, Rimma V.},
title = {Configuration-Parametric Query Optimization for Physical Design Tuning},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376710},
doi = {10.1145/1376616.1376710},
abstract = {Automated physical design tuning for database systems has recently become an active area of research and development. Existing tuning tools explore the space of feasible solutions by repeatedly optimizing queries in the input workload for several candidate configurations. This general approach, while scalable, often results in tuning sessions waiting for results from the query optimizer over 90% of the time. In this paper we introduce a novel approach, called Configuration-Parametric Query Optimization, that drastically improves the performance of current tuning tools. By issuing a single optimization call per query, we are able to generate a compact representation of the optimization space that can then produce very efficiently execution plans for the input query under arbitrary configurations. Our experiments show that our technique speeds-up query optimization by 30x to over 450x with virtually no loss in quality, and effectively eliminates the optimization bottleneck in existing tuning tools. Our techniques open the door for new, more sophisticated optimization strategies by eliminating the main bottleneck of current tuning tools.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {941–952},
numpages = {12},
keywords = {physical design tuning, parametric optimization},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376711,
author = {Soror, Ahmed A. and Minhas, Umar Farooq and Aboulnaga, Ashraf and Salem, Kenneth and Kokosielis, Peter and Kamath, Sunil},
title = {Automatic Virtual Machine Configuration for Database Workloads},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376711},
doi = {10.1145/1376616.1376711},
abstract = {Virtual machine monitors are becoming popular tools for the deployment of database management systems and other enterprise software applications. In this paper, we consider a common resource consolidation scenario, in which several database management system instances, each running in a virtual machine, are sharing a common pool of physical computing resources. We address the problem of optimizing the performance of these database management systems by controlling the configurations of the virtual machines in which they run. These virtual machine configurations determine how the shared physical resources will be allocated to the different database instances. We introduce a virtualization design advisor that uses information about the anticipated workloads of each of the database systems to recommend workload-specific configurations offine. Furthermore, runtime information collected after the deployment of the recommended configurations can be used to refine the recommendation. To estimate the effect of a particular resource allocation on workload performance, we use the query optimizer in a new what-if mode. We have implemented our approach using both PostgreSQL and DB2, and we have experimentally evaluated its effectiveness using DSS and OLTP workloads.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {953–966},
numpages = {14},
keywords = {virtual machine configuration, virtualization, resource consolidation},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376712,
author = {Abadi, Daniel J. and Madden, Samuel R. and Hachem, Nabil},
title = {Column-Stores vs. Row-Stores: How Different Are They Really?},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376712},
doi = {10.1145/1376616.1376712},
abstract = {There has been a significant amount of excitement and recent work on column-oriented database systems ("column-stores"). These database systems have been shown to perform more than an order of magnitude better than traditional row-oriented database systems ("row-stores") on analytical workloads such as those found in data warehouses, decision support, and business intelligence applications. The elevator pitch behind this performance difference is straightforward: column-stores are more I/O efficient for read-only queries since they only have to read from disk (or from memory) those attributes accessed by a query.This simplistic view leads to the assumption that one can obtain the performance benefits of a column-store using a row-store: either by vertically partitioning the schema, or by indexing every column so that columns can be accessed independently. In this paper, we demonstrate that this assumption is false. We compare the performance of a commercial row-store under a variety of different configurations with a column-store and show that the row-store performance is significantly slower on a recently proposed data warehouse benchmark. We then analyze the performance difference and show that there are some important differences between the two systems at the query executor level (in addition to the obvious differences at the storage layer level). Using the column-store, we then tease apart these differences, demonstrating the impact on performance of a variety of column-oriented query execution techniques, including vectorized query processing, compression, and a new join algorithm we introduce in this paper. We conclude that while it is not impossible for a row-store to achieve some of the performance advantages of a column-store, changes must be made to both the storage layer and the query executor to fully obtain the benefits of a column-oriented approach.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {967–980},
numpages = {14},
keywords = {column-oriented dbms, invisible join, compression, tuple reconstruction, c-store, column-store, tuple materialization},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376713,
author = {Harizopoulos, Stavros and Abadi, Daniel J. and Madden, Samuel and Stonebraker, Michael},
title = {OLTP through the Looking Glass, and What We Found There},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376713},
doi = {10.1145/1376616.1376713},
abstract = {Online Transaction Processing (OLTP) databases include a suite of features - disk-resident B-trees and heap files, locking-based concurrency control, support for multi-threading - that were optimized for computer technology of the late 1970's. Advances in modern processors, memories, and networks mean that today's computers are vastly different from those of 30 years ago, such that many OLTP databases will now fit in main memory, and most OLTP transactions can be processed in milliseconds or less. Yet database architecture has changed little.Based on this observation, we look at some interesting variants of conventional database systems that one might build that exploit recent hardware trends, and speculate on their performance through a detailed instruction-level breakdown of the major components involved in a transaction processing database system (Shore) running a subset of TPC-C. Rather than simply profiling Shore, we progressively modified it so that after every feature removal or optimization, we had a (faster) working system that fully ran our workload. Overall, we identify overheads and optimizations that explain a total difference of about a factor of 20x in raw performance. We also show that there is no single "high pole in the tent" in modern (memory resident) database systems, but that substantial time is spent in logging, latching, locking, B-tree, and buffer management operations.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {981–992},
numpages = {12},
keywords = {oltp, dbms architecture, online transaction processing, main memory transaction processing},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256705,
author = {Koch, Christoph},
title = {Session Details: Research Session 21: Provenance, Integration and Extraction},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256705},
doi = {10.1145/3256705},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376715,
author = {Chapman, Adriane P. and Jagadish, H. V. and Ramanan, Prakash},
title = {Efficient Provenance Storage},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376715},
doi = {10.1145/1376616.1376715},
abstract = {As the world is increasingly networked and digitized, the data we store has more and more frequently been chopped, baked, diced and stewed. In consequence, there is an increasing need to store and manage provenance for each data item stored in a database, describing exactly where it came from, and what manipulations have been applied to it. Storage of the complete provenance of each data item can become prohibitively expensive. In this paper, we identify important properties of provenance that can be used to considerably reduce the amount of storage required.We identify three different techniques: a family of factorization processes and two methods based on inheritance, to decrease the amount of storage required for provenance. We have used the techniques described in this work to significantly reduce the provenance storage costs associated with constructing MiMI [22], a warehouse of data regarding protein interactions, as well as two provenance stores, Karma [31] and PReServ [20], produced through workflow execution. In these real provenance sets, we were able to reduce the size of the provenance by up to a factor of 20. Additionally, we show that this reduced store can be queried efficiently and further that incremental changes can be made inexpensively.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {993–1006},
numpages = {14},
keywords = {provenance compression, provenance storage},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376716,
author = {Heinis, Thomas and Alonso, Gustavo},
title = {Efficient Lineage Tracking for Scientific Workflows},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376716},
doi = {10.1145/1376616.1376716},
abstract = {Data lineage and data provenance are key to the management of scientific data. Not knowing the exact provenance and processing pipeline used to produce a derived data set often renders the data set useless from a scientific point of view. On the positive side, capturing provenance information is facilitated by the widespread use of workflow tools for processing scientific data. The workflow process describes all the steps involved in producing a given data set and, hence, captures its lineage. On the negative side, efficiently storing and querying workflow based data lineage is not trivial. All existing solutions use recursive queries and even recursive tables to represent the workflows. Such solutions do not scale and are rather inefficient. In this paper we propose an alternative approach to storing lineage information captured as a workflow process. We use a space and query efficient interval representation for dependency graphs and show how to transform arbitrary workflow processes into graphs that can be stored using such representation. We also characterize the problem in terms of its overall complexity and provide a comprehensive performance evaluation of the approach.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1007–1018},
numpages = {12},
keywords = {data provenance, data lineage, scientific workflows},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376717,
author = {Wu, Wensheng and Reinwald, Berthold and Sismanis, Yannis and Manjrekar, Rajesh},
title = {Discovering Topical Structures of Databases},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376717},
doi = {10.1145/1376616.1376717},
abstract = {The increasing complexity of enterprise databases and the prevalent lack of documentation incur significant cost in both understanding and integrating the databases. Existing solutions addressed mining for keys and foreign keys, but paid little attention to more high-level structures of databases. In this paper, we consider the problem of discovering topical structures of databases to support semantic browsing and large-scale data integration. We describe iDisc, a novel discovery system based on a multi-strategy learning framework. iDisc exploits varied evidence in database schema and instance values to construct multiple kinds of database representations. It employs a set of base clusterers to discover preliminary topical clusters of tables from database representations, and then aggregate them into final clusters via meta-clustering. To further improve the accuracy, we extend iDisc with novel multiple-level aggregation and clusterer boosting techniques. We introduce a new measure on table importance and propose an approach to discovering cluster representatives to facilitate semantic browsing. An important feature of our framework is that it is highly extensible, where additional database representations and base clusterers may be easily incorporated into the framework. We have extensively evaluated iDisc using large real-world databases and results show that it discovers topical structures with a high degree of accuracy.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1019–1030},
numpages = {12},
keywords = {discovery, database structure, topical structure, clustering},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376718,
author = {Shen, Warren and DeRose, Pedro and McCann, Robert and Doan, AnHai and Ramakrishnan, Raghu},
title = {Toward Best-Effort Information Extraction},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376718},
doi = {10.1145/1376616.1376718},
abstract = {Current approaches to develop information extraction (IE) programs have largely focused on producing precise IE results. As such, they suffer from three major limitations. First, it is often difficult to execute partially specified IE programs and obtain meaningful results, thereby producing a long "debug loop". Second, it often takes a long time before we can obtain the first meaningful result (by finishing and running a precise IE program), thereby rendering these approaches impractical for time-sensitive IE applications. Finally, by trying to write precise IE programs we may also waste a significant amount of effort, because an approximate result -- one that can be produced quickly -- may already be satisfactory in many IE settings.To address these limitations, we propose iFlex, an IE approach that relaxes the precise IE requirement to enable best-effort IE. In iFlex, a developer U uses a declarative language to quickly write an initial approximate IE program P with a possible-worlds semantics. Then iFlex evaluates P using an approximate query processor to quickly extract an approximate result. Next, U examines the result, and further refines P if necessary, to obtain increasingly more precise results. To refine P, U can enlist a next-effort assistant, which suggests refinements based on the data and the current version of P. Extensive experiments on real-world domains demonstrate the utility of the iFlex approach.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1031–1042},
numpages = {12},
keywords = {information extraction, best-effort, approximate},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inbook{10.1145/3256706,
author = {Blakeley, Jose},
title = {Session Details: Industrial Session 1: Query Optimization and Performance},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256706},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1}
}

@inproceedings{10.1145/1376616.1376720,
author = {Xu, Yu and Kostamaa, Pekka and Zhou, Xin and Chen, Liang},
title = {Handling Data Skew in Parallel Joins in Shared-Nothing Systems},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376720},
doi = {10.1145/1376616.1376720},
abstract = {Parallel processing continues to be important in large data warehouses. The processing requirements continue to expand in multiple dimensions. These include greater volumes, increasing number of concurrent users, more complex queries, and more applications which define complex logical, semantic, and physical data models. Shared nothing parallel database management systems [16] can scale up "horizontally" by adding more nodes. Most parallel algorithms, however, do not take into account data skew. Data skew occurs naturally in many applications. A query processing skewed data not only slows down its response time, but generates hot nodes, which become a bottleneck throttling the overall system performance. Motivated by real business problems, we propose a new join geography called PRPD (Partial Redistribution &amp; Partial Duplication) to improve the performance and scalability of parallel joins in the presence of data skew in a shared-nothing system. Our experimental results show that PRPD significantly speeds up query elapsed time in the presence of data skew. Our experience shows that eliminating system bottlenecks caused by data skew improves the throughput of the whole system which is important in parallel data warehouses that often run high concurrency workloads.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1043–1052},
numpages = {10},
keywords = {parallel joins, shared nothing, data skew},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376721,
author = {Chakkappen, Sunil and Cruanes, Thierry and Dageville, Benoit and Jiang, Linan and Shaft, Uri and Su, Hong and Zait, Mohamed},
title = {Efficient and Scalable Statistics Gathering for Large Databases in Oracle 11g},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376721},
doi = {10.1145/1376616.1376721},
abstract = {Large tables are often decomposed into smaller pieces called partitions in order to improve query performance and ease the data management. Query optimizers rely on both the statistics of the entire table and the statistics of the individual partitions to select a good execution plan for a SQL statement. In Oracle 10g, we scan the entire table twice, one pass for gathering the table level statistics and the other pass for gathering the partition level statistics. A consequence of this gathering method is that, when the data in some partitions change, not only do we need to scan the changed partitions to gather the partition level statistics, but also we have to scan the entire table again to gather the table level statistics. Oracle 11g adopts a one-pass distinct sampling based method which can accurately derive the table level statistics from the partition level statistics. When data change, Oracle only re-gathers the statistics for the changed partitions and then derives the table level statistics without touching the unchanged partitions. To the best of our knowledge, although the one-pass distinct sampling has been researched in academia for some years, Oracle is the first commercial database that implements the technique. We have performed extensive experiments on both benchmark data and real customer data. Our experiments illustrate the this new method is highly accurate and has significantly better performance than the old method used in Oracle 10g.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1053–1064},
numpages = {12},
keywords = {large databases, statistics gathering, sampling, synopses, partitioned tables},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376722,
author = {Balmin, Andrey and \"{O}zcan, Fatma and Singh, Ashutosh and Ting, Edison},
title = {Grouping and Optimization of XPath Expressions in DB2<sup>®</sup> PureXML},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376722},
doi = {10.1145/1376616.1376722},
abstract = {Several XML DBMSs support XQuery and/or SQL/XML languages, which are based on navigational primitives in the form of XPath expressions. Typically, these systems either model each XPath step as a separate query plan operator, or employ holistic approaches that can evaluate multiple steps of a single XPath expression. There have also been proposals to execute as many XPath expressions as possible within a single FLWOR block simultaneously in a data streaming context.We observe that blindly combining all possible XPath expressions for concurrent execution can result in significant performance degradation in a database system. We identify two main problems with this strategy. First, the simple strategy of grouping all XPath expressions on a single document does not always work if the query involves more than one data source or has nested query blocks. Second, merging XPath expressions may result in unnecessary execution of branches that can be filtered by predicates in other branches or elsewhere in the query. To rectify these problems, IBM® DB2® pureXML" adopts a combination of heuristic-based rewrite transformations, to decide which XPath expressions should be grouped for concurrent evaluation, and cost-based optimization to globally order the groups within the query execution plan, and locally order the branches within individual groups. Experimental evaluation confirms that selectively grouping multiple XPath expressions allows for better query evaluation performance and reduces the query optimization complexity. These optimization techniques have been implemented as part of IBM DB2 9.5 (pureXML).},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1065–1074},
numpages = {10},
keywords = {xpath and xquery processing, xml query optimization},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376723,
author = {Lee, Sang-Won and Moon, Bongki and Park, Chanik and Kim, Jae-Myung and Kim, Sang-Woo},
title = {A Case for Flash Memory Ssd in Enterprise Database Applications},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376723},
doi = {10.1145/1376616.1376723},
abstract = {Due to its superiority such as low access latency, low energy consumption, light weight, and shock resistance, the success of flash memory as a storage alternative for mobile computing devices has been steadily expanded into personal computer and enterprise server markets with ever increasing capacity of its storage. However, since flash memory exhibits poor performance for small-to-moderate sized writes requested in a random order, existing database systems may not be able to take full advantage of flash memory without elaborate flash-aware data structures and algorithms. The objective of this work is to understand the applicability and potential impact that flash memory SSD (Solid State Drive) has for certain type of storage spaces of a database server where sequential writes and random reads are prevalent. We show empirically that up to more than an order of magnitude improvement can be achieved in transaction processing by replacing magnetic disk with flash memory SSD for transaction log, rollback segments, and temporary table spaces.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1075–1086},
numpages = {12},
keywords = {flash-memory database server, flash-memory ssd},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256707,
author = {Hsiao, Hui-I},
title = {Session Details: Industrial Session 2: Database Programming and Performance},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256707},
doi = {10.1145/3256707},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376725,
author = {Blakeley, Jos\'{e} A. and Rao, Vineet and Kunen, Isaac and Prout, Adam and Henaire, Mat and Kleinerman, Christian},
title = {..NET Database Programmability and Extensibility in Microsoft SQL Server},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376725},
doi = {10.1145/1376616.1376725},
abstract = {The integration of the .NET Common Language Runtime (CLR) into the SQL Server DBMS enables rich business logic written in modern .NET languages to run close to the data. Database application developers can write business logic as functions, stored procedures, and triggers. They can also extend the native capabilities of the DBMS by adding new scalar data types, and aggregates. A previous paper [2] described the architecture and design principles of the integration of the CLR inside SQL Server. Here we present new aspects of this work. First, we describe the extensibility contracts for user-defined types and aggregates in detail. Second, we present the advances to the CLR integration in SQL Server 2008 which significantly enhances the breath of applications supported by SQL Server. In particular, we describe the support for large (greater than 8000 byte) user-defined types and aggregates, multiple-input user-defined aggregates, and order-aware table valued functions. Third, we show how we leveraged scalar type extensibility to provide a hierarchical identifier data type that enables encoding of keys describing hierarchies as well as built-in support for spatial applications. This support includes both flat- and round-earth spatial types, as well as a spatial index. Fourth, we present how we use Language Integrated Query (LINQ) enhancements in .NET languages to improve developer productivity when creating routines that require data access. Finally, we present preliminary performance results showing the efficiency of streaming TVFs and aggregates relative to equivalent native features.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1087–1098},
numpages = {12},
keywords = {extensibility, abstract data types, server programming},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376726,
author = {Olston, Christopher and Reed, Benjamin and Srivastava, Utkarsh and Kumar, Ravi and Tomkins, Andrew},
title = {Pig Latin: A Not-so-Foreign Language for Data Processing},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376726},
doi = {10.1145/1376616.1376726},
abstract = {There is a growing need for ad-hoc analysis of extremely large data sets, especially at internet companies where innovation critically depends on being able to analyze terabytes of data collected every day. Parallel database products, e.g., Teradata, offer a solution, but are usually prohibitively expensive at this scale. Besides, many of the people who analyze this data are entrenched procedural programmers, who find the declarative, SQL style to be unnatural. The success of the more procedural map-reduce programming model, and its associated scalable implementations on commodity hardware, is evidence of the above. However, the map-reduce paradigm is too low-level and rigid, and leads to a great deal of custom user code that is hard to maintain, and reuse.We describe a new language called Pig Latin that we have designed to fit in a sweet spot between the declarative style of SQL, and the low-level, procedural style of map-reduce. The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. We give a few examples of how engineers at Yahoo! are using Pig to dramatically reduce the time required for the development and execution of their data analysis tasks, compared to using Hadoop directly. We also report on a novel debugging environment that comes integrated with Pig, that can lead to even higher productivity gains. Pig is an open-source, Apache-incubator project, and available for general use.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1099–1110},
numpages = {12},
keywords = {pig latin, dataflow language},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376727,
author = {Eadon, George and Chong, Eugene Inseok and Shankar, Shrikanth and Raghavan, Ananth and Srinivasan, Jagannathan and Das, Souripriya},
title = {Supporting Table Partitioning by Reference in Oracle},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376727},
doi = {10.1145/1376616.1376727},
abstract = {Partitioning is typically employed on large-scale data to improve manageability, availability, and performance. However, for tables connected by a referential constraint (capturing a parent-child relationship), the current approaches require individually partitioning each table thereby burdening the user with the task of maintaining the tables equi-partitioned, which not only is cumbersome but also error prone. This paper proposes a new partitioning method (partition by reference) that allows tables with a parent-child relationship to be logically equi-partitioned by inheriting the partition key from the parent table without duplicating the key columns. The partitioning key is resolved through an existing parent-child relationship, enforced by an active referential constraint. This logical dependency is used to automatically i) cascade partition maintenance operations performed on parent table to child tables, and ii) handle migration of child rows when partition key or parent key in parent table is updated, as a single atomic operation. This method has been introduced in Oracle Database 11gR1 with support for tables with both single level and composite partitioning methods. The paper describes the key concepts of table partitioning by reference method, discusses the design and implementation challenges, and presents an experimental study covering a usage scenario common in Information Life Cycle Management (ILM) applications.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1111–1122},
numpages = {12},
keywords = {vertical partitioning, partition by reference, partitioning, partition-wise join, partition maintenance, partition pruning, information lifecycle management},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256708,
author = {Stanoi, Ioana},
title = {Session Details: Industrial Session 3: Streams, Conversations and Verification:},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256708},
doi = {10.1145/3256708},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376729,
author = {Gedik, Bugra and Andrade, Henrique and Wu, Kun-Lung and Yu, Philip S. and Doo, Myungcheol},
title = {SPADE: The System s Declarative Stream Processing Engine},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376729},
doi = {10.1145/1376616.1376729},
abstract = {In this paper, we present Spade - the System S declarative stream processing engine. System S is a large-scale, distributed data stream processing middleware under development at IBM T. J. Watson Research Center. As a front-end for rapid application development for System S, Spade provides (1) an intermediate language for flexible composition of parallel and distributed data-flow graphs, (2) a toolkit of type-generic, built-in stream processing operators, that support scalar as well as vectorized processing and can seamlessly inter-operate with user-defined operators, and (3) a rich set of stream adapters to ingest/publish data from/to outside sources. More importantly, Spade automatically brings performance optimization and scalability to System S applications. To that end, Spade employs a code generation framework to create highly-optimized applications that run natively on the Stream Processing Core (SPC), the execution and communication substrate of System S, and take full advantage of other System S services. Spade allows developers to construct their applications with fine granular stream operators without worrying about the performance implications that might exist, even in a distributed system. Spade's optimizing compiler automatically maps applications into appropriately sized execution units in order to minimize communication overhead, while at the same time exploiting available parallelism. By virtue of the scalability of the System S runtime and Spade's effective code generation and optimization, we can scale applications to a large number of nodes. Currently, we can run Spade jobs on ≈ 500 processors within more than 100 physical nodes in a tightly connected cluster environment. Spade has been in use at IBM Research to create real-world streaming applications, ranging from monitoring financial market feeds to radio telescopes to semiconductor fabrication lines.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1123–1134},
numpages = {12},
keywords = {distributed data stream processing},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376730,
author = {Johnson, Theodore and Muthukrishnan, Muthu S. and Shkapenyuk, Vladislav and Spatscheck, Oliver},
title = {Query-Aware Partitioning for Monitoring Massive Network Data Streams},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376730},
doi = {10.1145/1376616.1376730},
abstract = {Data Stream Management Systems (DSMS) are gaining acceptance for applications that need to process very large volumes of data in real time. The load generated by such applications frequently exceeds by far the computation capabilities of a single centralized server. In particular, a single-server instance of our DSMS, Gigascope, cannot keep up with the processing demands of the new OC-786 networks, which can generate more than 100 million packets per second. In this paper, we explore a mechanism for the distributed processing of very high speed data streams.Existing distributed DSMSs employ two mechanisms for distributing the load across the participating machines: partitioning of the query execution plans and partitioning of the input data stream in a query-independent fashion. However, for a large class of queries, both approaches fail to reduce the load as compared to centralized system, and can even lead to an increase in the load. In this paper we present an alternative approach - query-aware data stream partitioning that allows for more efficient scaling. We present methods for analyzing any given query set and choose the optimal partitioning scheme, and show how to reconcile potentially conflicting requirements that different queries might place on partitioning. We conclude with experiments on a small cluster of processing nodes on high-rate network traffic feed that demonstrates with different query sets that our methods effectively distribute the load across all processing nodes and facilitate efficient scaling whenever more processing nodes becomes available.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1135–1146},
numpages = {12},
keywords = {data streams, partitioning, query optimization},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376731,
author = {Nambiar, Ullas and Gupta, Himanshu and Balakrishnan, Raju and Mohania, Mukesh},
title = {Helping Satisfy Multiple Objectives during a Service Desk Conversation},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376731},
doi = {10.1145/1376616.1376731},
abstract = {Agents manning a service desk have the unenviable task of satisfying multiple conflicting objectives. Specifically, businesses require the agents to meet pre-specified customer satisfaction levels while keeping the cost of operations low or meeting sales targets, objectives that end up being complementary. Additional complexity is introduced by the fact that the objectives are often inter-dependent and have to be met in real-time. Moreover, business might change the objectives from time to time e.g. from reducing cost of operation to increasing sales of slow moving product. In this paper, we describe CallAssist - a speech enabled real-time dialog management system that dynamically helps agents in building a conversation that meets the various business objectives while satisfying customer requirements. An added benefit of our solution is the ability to adapt to changing business needs without incurring agent re-training costs. We provide evaluation results displaying the efficiency and effectiveness of our system.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1147–1158},
numpages = {12},
keywords = {dialog systems, preference elicitation, real-time entity analytics},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376732,
author = {Galanis, Leonidas and Buranawatanachoke, Supiti and Colle, Romain and Dageville, Beno\^{\i}t and Dias, Karl and Klein, Jonathan and Papadomanolakis, Stratos and Tan, Leng Leng and Venkataramani, Venkateshwaran and Wang, Yujun and Wood, Graham},
title = {Oracle Database Replay},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376732},
doi = {10.1145/1376616.1376732},
abstract = {This paper presents Oracle Database Replay, a novel approach to testing changes to the relational database management system component of an information system (software upgrades, hardware changes etc). Database Replay makes it possible to subject a test system to a real production system workload, which helps identify all potential problems before implementing the planned changes on the production system. Any interesting workload period of a production database system can be captured with minimal overhead. The captured workload can be used to drive a test system while maintaining the concurrency and load characteristics of the real production workload. Therefore, the test results using database replay can provide very high assurance in determining the impact of changes to a production system before applying these changes. This paper presents the architecture of Database Replay as well as experimental results that demonstrate its usefulness as testing methodology.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1159–1170},
numpages = {12},
keywords = {capture, testing, record, replay, database},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/3256709,
author = {O'Neil, Patrick},
title = {Session Details: Industrial Session 4: Data and Application Integration, Spatial Data},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256709},
doi = {10.1145/3256709},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376734,
author = {Simmen, David E. and Altinel, Mehmet and Markl, Volker and Padmanabhan, Sriram and Singh, Ashutosh},
title = {Damia: Data Mashups for Intranet Applications},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376734},
doi = {10.1145/1376616.1376734},
abstract = {Increasingly large numbers of situational applications are being created by enterprise business users as a by-product of solving day-to-day problems. In efforts to address the demand for such applications, corporate IT is moving toward Web 2.0 architectures. In particular, the corporate intranet is evolving into a platform of readily accessible data and services where communities of business users can assemble and deploy situational applications. Damia is a web style data integration platform being developed to address the data problem presented by such applications, which often access and combine data from a variety of sources. Damia allows business users to quickly and easily create data mashups that combine data from desktop, web, and traditional IT sources into feeds that can be consumed by AJAX, and other types of web applications. This paper describes the key features and design of Damia's data integration engine, which has been packaged with Mashup Hub, an enterprise feed server currently available for download on IBM alphaWorks. Mashup Hub exposes Damia's data integration capabilities in the form of a service that allows users to create hosted data mashups.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1171–1182},
numpages = {12},
keywords = {xml, information integration, data feeds},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376735,
author = {Ma, Li and Wang, Chen and Lu, Jing and Cao, Feng and Pan, Yue and Yu, Yong},
title = {Effective and Efficient Semantic Web Data Management over DB2},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376735},
doi = {10.1145/1376616.1376735},
abstract = {With the fast growth of Semantic Web, more and more RDF data and ontologies are created and widely used in Web applications and enterprise information systems. It is reported that the W3C Linking Open Data community project consists of over two billion RDF triples, which are interlinked by about three million RDF links. Recently, efficient RDF data management on top of relational databases gains particular attentions from both Semantic Web community and database community. In this paper, we present effective and efficient Semantic Web data management over DB2, including efficient schema and indexes design for storage, practical ontology reasoning support, and an effective SPARQL-to-SQL translation method for RDF query. Moreover, we show the performance and scalability of our system by an evaluation among well-known RDF stores and discuss future work.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1183–1194},
numpages = {12},
keywords = {ontology, triple store, rdf, semantic web, sparql},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376736,
author = {Aulbach, Stefan and Grust, Torsten and Jacobs, Dean and Kemper, Alfons and Rittinger, Jan},
title = {Multi-Tenant Databases for Software as a Service: Schema-Mapping Techniques},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376736},
doi = {10.1145/1376616.1376736},
abstract = {In the implementation of hosted business services, multiple tenants are often consolidated into the same database to reduce total cost of ownership. Common practice is to map multiple single-tenant logical schemas in the application to one multi-tenant physical schema in the database. Such mappings are challenging to create because enterprise applications allow tenants to extend the base schema, e.g., for vertical industries or geographic regions. Assuming the workload stays within bounds, the fundamental limitation on scalability for this approach is the number of tables the database can handle. To get good consolidation, certain tables must be shared among tenants and certain tables must be mapped into fixed generic structures such as Universal and Pivot Tables, which can degrade performance.This paper describes a new schema-mapping technique for multi-tenancy called Chunk Folding, where the logical tables are vertically partitioned into chunks that are folded together into different physical multi-tenant tables and joined as needed. The database's "meta-data budget" is divided between application-specific conventional tables and a large fixed set of generic structures called Chunk Tables. Good performance is obtained by mapping the most heavily-utilized parts of the logical schemas into the conventional tables and the remaining parts into Chunk Tables that match their structure as closely as possible. We present the re sults of several experiments designed to measure the efficacy of Chunk Folding and describe the multi-tenant database testbed in which these experiments were performed.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1195–1206},
numpages = {12},
keywords = {chunk folding, software as a service, multi-tenancy},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376737,
author = {Fang, Yi and Friedman, Marc and Nair, Giri and Rys, Michael and Schmid, Ana-Elisa},
title = {Spatial Indexing in Microsoft SQL Server 2008},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376737},
doi = {10.1145/1376616.1376737},
abstract = {Microsoft SQL Server 2008 adds built-in support for 2-dimensional spatial data types for both planar and geodetic geometries to address the increasing demands for managing location-aware data. SQL Server 2008 also adds indexing capabilities that, together with the necessary plan selections done by the query optimizer, provide efficient processing of spatial queries. This paper will present an overview of the spatial indexing implementation in SQL Server 2008 and outline how the indexing is implemented and how the cost-based query optimizer chooses among the different plans.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1207–1216},
numpages = {10},
keywords = {spatial databases, spatial query processing, spatial indexing},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376739,
author = {Albright, Robert and Demers, Alan and Gehrke, Johannes and Gupta, Nitin and Lee, Hooyeon and Keilty, Rick and Sadowski, Gregory and Sowell, Ben and White, Walker},
title = {SGL: A Scalable Language for Data-Driven Games},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376739},
doi = {10.1145/1376616.1376739},
abstract = {We propose to demonstrate SGL, a language and system for writing computer games using data management techniques. We will demonstrate a complete game built using the system, and show how complex game behavior can be expressed in a declarative scripting language. The demo will also illustrate the workflow necessary to modify a game and include a visualization of the relational operations that are executed as the game runs.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1217–1222},
numpages = {6},
keywords = {indexing, games, scripting, aggregates},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376740,
author = {Rusu, Florin and Xu, Fei and Perez, Luis Leopoldo and Wu, Mingxi and Jampani, Ravi and Jermaine, Chris and Dobra, Alin},
title = {The DBO Database System},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376740},
doi = {10.1145/1376616.1376740},
abstract = {We demonstrate our prototype of the DBO database system. DBO is designed to facilitate scalable analytic processing over large data archives. DBO's analytic processing performance is competitive with other database systems; however, unlike any other existing research or industrial system, DBO maintains a statistically meaningful guess to the final answer to a query from start to finish during query processing. This guess may be quite accurate after only a few seconds or minutes, while answering a query exactly may take hours. This can result in significant savings in both user and computer time, since a user can abort a query as soon as he or she is happy with the guess' accuracy.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1223–1226},
numpages = {4},
keywords = {sampling, online aggregation, dbo},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376741,
author = {Mishra, Chaitanya and Koudas, Nick},
title = {Stretch 'n' Shrink: Resizing Queries to User Preferences},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376741},
doi = {10.1145/1376616.1376741},
abstract = {We present Stretch 'n' Shrink, a query design framework that explicitly takes into account user preferences about the desired answer size, and subsequently modifies the query with user feedback to meet this target. Our system has been prototyped inside an open source data manager, and requires minimal modifications to the database engine.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1227–1230},
numpages = {4},
keywords = {query relaxation/contraction, many/few answers problem},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376742,
author = {Arasu, Arvind and Chaudhuri, Surajit and Ganjam, Kris and Kaushik, Raghav},
title = {Incorporating String Transformations in Record Matching},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376742},
doi = {10.1145/1376616.1376742},
abstract = {Today's record matching infrastructure does not allow a flexible way to account for synonyms such as "Robert" and "Bob" which refer to the same name, and more general forms of string transformations such as abbreviations. We expand the problem of record matching to take such user-defined string transformations as input. These transformations coupled with an underlying similarity function are used to define the similarity between two strings. We demonstrate the effectiveness of this approach via a fuzzy match operation that is used to lookup an input record against a table of records, where we have an additional table of transformations as input. We demonstrate an improvement in record matching quality and efficient retrieval based on our index structure that is cognizant of transformations.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1231–1234},
numpages = {4},
keywords = {transformation rules, record matching, data cleaning},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376743,
author = {Gupta, Nitin and Demers, Alan J. and Gehrke, Johannes E.},
title = {SEMMO: A Scalable Engine for Massively Multiplayer Online Games},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376743},
doi = {10.1145/1376616.1376743},
abstract = {We propose to demonstrate SEMMO, a consistency server for MMOs. The key features of SEMMO are its novel distributed consistency protocol and system architecture. The distributed nature of the engine allows the clients to perform all computations locally; the only computation that the central server performs is to determine the serialization order of game actions.We will demo SEMMO through a game called Manhattan Pals, and show how we can exploit game semantics in order to support large-scale MMOs. In the demo, avatars of the audience will be able to play Manhattan Pals and thus experience various scalability and consistency effects of an MMO.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1235–1238},
numpages = {4},
keywords = {games, virtual environments, scalability},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376744,
author = {Singh, Sarvjeet and Mayfield, Chris and Mittal, Sagar and Prabhakar, Sunil and Hambrusch, Susanne and Shah, Rahul},
title = {Orion 2.0: Native Support for Uncertain Data},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376744},
doi = {10.1145/1376616.1376744},
abstract = {Orion is a state-of-the-art uncertain database management system with built-in support for probabilistic data as first class data types. In contrast to other uncertain databases, Orion supports both attribute and tuple uncertainty with arbitrary correlations. This enables the database engine to handle both discrete and continuous pdfs in a natural and accurate manner. The underlying model is closed under the basic relational operators and is consistent with Possible Worlds Semantics. We demonstrate how Orion simplifies the design and enhances the capabilities of two example applications: managing sensor data (continuous uncertainty) and inferring missing values (discrete uncertainty).},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1239–1242},
numpages = {4},
keywords = {postgresql, probabilistic database, uncertainty, database},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376745,
author = {Sengar, Vibhuti and Joshi, Tanuja and Joy, Joseph M. and Prakash, Samarth},
title = {Building a Global Location Search Service},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376745},
doi = {10.1145/1376616.1376745},
abstract = {We present a crosslingual location search service that works across multiple countries and deals effectively with ambiguous and ill-formed queries. The system returns a ranked list of spatial regions (points, lines and polygons) that best match users' text queries, which can range from postal addresses to unstructured queries that list a few col-located map features. The system's robustness comes from a novel approach that exploits spatial coherence to identify viable interpretations of input text. Unlike existing state of the art geocoding systems, our system requires no region-specific rules, training or customization, and thus may be built to cover any region for which detailed map data is available, making it possible, for the first time, to rapidly build location search services for new regions, and more generally, approximate text search over arbitrary spatial repositories. Our system has been shown to outperform commercial geocoding systems, especially when spelling or format variations are introduced. We demonstrate our sys-tem's capabilities by showing results for a variety of text queries over a large dataset from several countries with widely differing address formats.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1243–1246},
numpages = {4},
keywords = {crosslingual search, local search, geocoding, spatial search},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376746,
author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
title = {Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376746},
doi = {10.1145/1376616.1376746},
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1247–1250},
numpages = {4},
keywords = {tuple store, semantic network, collaborative systems},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376747,
author = {Scheidegger, Carlos E. and Vo, Huy T. and Koop, David and Freire, Juliana and Silva, Claudio T.},
title = {Querying and Re-Using Workflows with VsTrails},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376747},
doi = {10.1145/1376616.1376747},
abstract = {We show how work flow systems can be augmented to leverage provenance information to enhance usability. In particular, we will demonstrate new mechanisms and intuitive user interfaces designed to allow users to query work flows by example and to refine work flows by analogies. These techniques are implemented in VisTrails, an open-source provenance-enabled scientific work flow system that can be combined with a wide range of tools, libraries, and visualization systems. We will show di erent scenarios where these techniques can be used to simplify the notoriously hard tasks of creating and refining work flows.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1251–1254},
numpages = {4},
keywords = {visualization, scientific workflows, query-by-example, provenance, analogy},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376748,
author = {Pelekis, Nikos and Frentzos, Elias and Giatrakos, Nikos and Theodoridis, Yannis},
title = {HERMES: Aggregative LBS via a Trajectory DB Engine},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376748},
doi = {10.1145/1376616.1376748},
abstract = {We present HERMES, a prototype system based on a powerful query language for trajectory databases, which enables the support of aggregative Location-Based Services (LBS). The key observation that motivates HERMES is that the more the knowledge in hand about the trajectory of a mobile user, the better the exploitation of the advances in spatio-temporal query processing for providing intelligent LBS. HERMES is fully incorporated into a state-of-the-art Object-Relational DBMS, and its demonstration illustrates its flexibility and usefulness for delivering custom-defined LBS.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1255–1258},
numpages = {4},
keywords = {moving object databases, trajectories, location-based services},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376750,
author = {Bex, Geert Jan and Neven, Frank and Vansummeren, Stijn},
title = {SchemaScope: A System for Inferring and Cleaning XML Schemas},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376750},
doi = {10.1145/1376616.1376750},
abstract = {We present SchemaScope, a system to derive Document Type Definitions and XML schema from a corpus of sample XML documents. Tools are provided to visualize, clean and refine existing or inferred schemas. A number of use cases illustrate the versatility of the system, as well as various types of applications.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1259–1262},
numpages = {4},
keywords = {xml, regular expressions, schema inference},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376751,
author = {Hua, Ming and Pei, Jian},
title = {DiMaC: A System for Cleaning Disguised Missing Data},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376751},
doi = {10.1145/1376616.1376751},
abstract = {In some applications such as filling in a customer information form on the web, some missing values may not be explicitly represented as such, but instead appear as potentially valid data values. Such missing values are known as disguised missing data, which may impair the quality of data analysis severely. The very limited previous studies on cleaning disguised missing data highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers.Recently, we have studied the problem of cleaning disguised missing data systematically, and proposed an effective heuristic approach [2]. In this paper, we describe a demonstration of DiMaC, a <u>Di</u>sguised <u>M</u>issing D<u>a</u>ta <u>C</u>leaning system which can find the frequently used disguise values in data sets without requiring any domain background knowledge. In this demo, we will show (1) the critical techniques of finding suspicious disguise values; (2) the architecture and user interface of DiMaC system; (3) an empirical case study on both real and synthetic data sets, which verifies the effectiveness and the efficiency of the techniques; (4) some challenges arising from real applications and several direction for future work.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1263–1266},
numpages = {4},
keywords = {data cleaning, disguised missing data, data quality},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376752,
author = {Elghandour, Iman and Aboulnaga, Ashraf and Zilio, Daniel C. and Chiang, Fei and Balmin, Andrey and Beyer, Kevin and Zuzarte, Calisto},
title = {An Xml Index Advisor for DB2},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376752},
doi = {10.1145/1376616.1376752},
abstract = {XML database systems are expected to handle increasingly complex queries over increasingly large and highly structured XML databases. An important problem that needs to be solved for these systems is how to choose the best set of indexes for a given workload. We have developed an XML Index Advisor that solves this XML index recommendation problem and is tightly coupled with the query optimizer of the database system. We have implemented our XML Index Advisor for DB2. In this demonstration we showcase the new query optimizer modes that we added to DB2, the index recommendation process, and the effectiveness of the recommended indexes.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1267–1270},
numpages = {4},
keywords = {xml databases, automatic physical database design, index advisor},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376753,
author = {Raffio, Alessandro and Braga, Daniele and Ceri, Stefano and Papotti, Paolo and Hern\'{a}ndez, Mauricio A.},
title = {Clip: A Tool for Mapping Hierarchical Schemas},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376753},
doi = {10.1145/1376616.1376753},
abstract = {Many data integration solutions in the market today include visual tools for schema mapping. Users connect schema elements with lines that are interpreted as high-level logical expressions capturing the relationship between source and target data-sets. These expressions are compiled into queries or programs that convert source-side data instances into target-side instances. In this demo we showcase Clip, an XML Schema mapping tool. Clip is distinguished from existing tools in that mappings explicitly specify structural transformations in addition to value correspondences. We show how Clip's users enter mappings by drawing lines and how these lines are translated into XQuery.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1271–1274},
numpages = {4},
keywords = {mapping languages, tgd, xquery, xml schema},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376754,
author = {Tatemura, Junichi and Chen, Songting and Liao, Fenglin and Po, Oliver and Candan, K. Selcuk and Agrawal, Divyakant},
title = {UQBE: Uncertain Query by Example for Web Service Mashup},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376754},
doi = {10.1145/1376616.1376754},
abstract = {The UQBE is a mashup tool for non-programmers that supports query-by-example (QBE) over a schema made up by the user without knowing the schema of the original sources. Based on automated schema matching with uncertainty, the UQBE system returns the best confident results. The system lets the user refine them interactively. A tuple in the query result is associated with lineage that is a boolean formula over schema matching decisions representing underlying conditions on which the corresponding tuple is included in the result. Given binary feedbacks on tuples by the user, which are possibly imprecise, the system solves it as an optimization problem to refine confidence values of matching decisions. The demo features graphical user interaction on the UQBE system, including querying and refinement.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1275–1280},
numpages = {6},
keywords = {uncertain query, lineage, query by example, web service, data integration},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376755,
author = {Alexe, Bogdan and Chiticariu, Laura and Miller, Ren\'{e}e J. and Pepper, Daniel and Tan, Wang-Chiew},
title = {Muse: A System for Understanding and Designing Mappings},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376755},
doi = {10.1145/1376616.1376755},
abstract = {Schema mappings are logical assertions that specify the relationships between a source and a target schema in a declarative way. The specification of such mappings is a fundamental problem in information integration. Mappings can be generated by existing mapping systems (semi-)automatically from a visual specification between two schemas. In general, the well-known 80-20 rule applies for mapping generation tools. They can automate 80% of the work, covering common cases and creating a mapping that is close to correct. However, ensuring complete correctness can still require intricate manual work to perfect portions of the mapping.Previous research on mapping understanding and refinement and anecdotal evidence from mapping designers suggest that the mapping design process can be perfected by using data examples to explain the mapping and alternative mappings. We demonstrate Muse, a data example driven mapping design tool currently implemented on top of the Clio schema mapping system. Muse leverages data examples that are familiar to a designer to illustrate nuances of how a small change to a mapping specification changes its semantics. We demonstrate how Muse can differentiate between alternative mapping specifications and infer the desired mapping semantics based on the designer's actions on a short sequence of simple data examples.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1281–1284},
numpages = {4},
keywords = {data translation, data examples, refinement, schema mappings, design, data exchange},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376756,
author = {Kasneci, Gjergji and Suchanek, Fabian M. and Ifrim, Georgiana and Elbassuoni, Shady and Ramanath, Maya and Weikum, Gerhard},
title = {NAGA: Harvesting, Searching and Ranking Knowledge},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376756},
doi = {10.1145/1376616.1376756},
abstract = {The presence of encyclopedic Web sources, such as Wikipedia, the Internet Movie Database (IMDB), World Factbook, etc. calls for new querying techniques that are simple and yet more expressive than those provided by standard keyword-based search engines. Searching for explicit knowledge needs to consider inherent semantic structures involving entities and relationships.In this demonstration proposal, we describe a semantic search system named NAGA. NAGA operates on a knowledge graph, which contains millions of entities and relationships derived from various encyclopedic Web sources, such as the ones above. NAGA's graph-based query language is geared towards expressing queries with additional semantic information. Its scoring model is based on the principles of generative language models, and formalizes several desiderata such as confidence, informativeness and compactness of answers.We propose a demonstration of NAGA which will allow users to browse the knowledge base through a user interface, enter queries in NAGA's query language and tune the ranking parameters to test various ranking aspects.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1285–1288},
numpages = {4},
keywords = {user interface, ranking, entities, relationships, semantic search},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376757,
author = {Bonifati, Angela and Mecca, Giansalvatore and Pappalardo, Alessandro and Raunich, Salvatore and Summa, Gianvito},
title = {The Spicy System: Towards a Notion of Mapping Quality},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376757},
doi = {10.1145/1376616.1376757},
abstract = {We introduce the Spicy system, a novel approach to the problem of automatically selecting the best mappings among two data sources. Known schema mapping algorithms rely on value correspondences -- i.e. correspondences among semantically related attributes -- to produce complex transformations among data sources. Spicy brings together schema matching and mapping generation tools to further automate this process. A key observation, here, is that the quality of the mappings is strongly influenced by the quality of the input correspondences. To address this problem, Spicy adopts a three-layer architecture, in which a schema matching module is used to provide input to a mapping generation module. Then, a third module, the mapping verification module, is used to check candidate mappings and choose the ones that represent better transformations of the source into the target. At the core of the system stands a new technique for comparing the structure and actual content of trees, called structural analysis. Experimental results show that our mapping discovery algorithm achieves both good scalability and high precision in mapping selection.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1289–1294},
numpages = {6},
keywords = {mapping verification, schema matching, mappings},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376758,
author = {M\"{u}ller, Heiko and Buneman, Peter and Koltsidas, Ioannis},
title = {XArch: Archiving Scientific and Reference Data},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376758},
doi = {10.1145/1376616.1376758},
abstract = {Database archiving is important for the retrieval of old versions of a database and for temporal queries over the history of data. We demonstrate XArch, a management system for maintaining, populating, and querying archives of hierarchical data. XArch is based on a nested merge approach that efficiently stores multiple versions of hierarchical data in a compact archive. By merging elements into one data structure, any specific version is retrievable from the archive in a single pass over the data and efficient tracking of object history is possible. XArch implements this approach and extends it in two important ways. First, in order to merge large hierarchical data sets, elements need to be sorted according to their key values. We developed an efficient algorithm for sorting hierarchical data in secondary storage and modified the nested merge algorithm accordingly. Second, we designed and implemented a declarative query language that enables one both to view data from particular versions and to track the history of objects. We demonstrate this using both molecular biology and demographic reference data as examples.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1295–1298},
numpages = {4},
keywords = {archiving, hierarchical data, temporal queries},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376759,
author = {Fisher, Kathleen and Walker, David and Zhu, Kenny Q.},
title = {LearnPADS: Automatic Tool Generation from Ad Hoc Data},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376759},
doi = {10.1145/1376616.1376759},
abstract = {In this demonstration, we will present LEARNPADS, a fully automatic system for generating ad hoc data processing tools. When presented with a collection of ad hoc data, the system (1) analyzes the data, (2) infers a PADS [4, 5] description, (3) generates parser, printer, validation and traversal libraries and (4) links these libraries with format-independent tool suites to form stand-alone applications. These applications provide statistical analysis, XML conversion, CSV conversion, the ability to query with the Galax XQuery engine [3], and the ability to graph selected data elements, all directly from ASCII ad hoc data without human intervention. SIGMOD attendees will see both the user experience with LEARNPADS and the internals of the multi-phase inference algorithm which lies at the heart of the system.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1299–1302},
numpages = {4},
keywords = {data description language, grammar induction, tools generation, ad hoc data},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376761,
author = {Hwang, Jeong-Hyon and Cha, Sanghoon and Cetintemel, Uundefinedur and Zdonik, Stan},
title = {Borealis-R: A Replication-Transparent Stream Processing System for Wide-Area Monitoring Applications},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376761},
doi = {10.1145/1376616.1376761},
abstract = {Borealis-R is a replication-based system for both fast andhighly-available processing of data streams over wide-area networks. In Borealis-R, multiple operator replicas send outputs to downstream replicas, allowing each replica to use whichever data arrives first. To further reduce latency, replicas run without coordination, possibly processing data in different orders. Despite this flexibility, Borealis-R guarantees that applications always receive the same results as in the non-replicated, failure-free case. In addition, Borealis-R deploys replicas at select network locations to effectively improve performance as well as availability.We demonstrate the strengths of Borealis-R using a live wide-area monitoring application. We show that Borealis-R outperforms previous solutions in terms of latency and that it uses system resources efficiently by carefully deploying and discarding replicas.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1303–1306},
numpages = {4},
keywords = {wide-area networks, replication, reliability, availability, fault tolerance, stream processing},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376762,
author = {Chow, Chi-Yin and Mokbel, Mohamed F. and He, Tian},
title = {Tinycasper: A Privacy-Preserving Aggregate Location Monitoring System in Wireless Sensor Networks},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376762},
doi = {10.1145/1376616.1376762},
abstract = {This demo presents a privacy-preserving aggregate location monitoring system, namely, TinyCasper, in which we can monitor moving objects in wireless sensor networks while preserving their location privacy. TinyCasper consists of two main modules, in-network location anonymization and aggregate query processing over anonymized locations. In the first module, trusted wireless sensor nodes collaborate with each other to anonymize users' exact locations by a cloaked spatial region that satisfies a prespecified privacy requirement. On the other side, the aggregate query processing module collects and analyzes the cloaked spatial regions reported from the wireless sensor nodes to support aggregate and alarm queries over anonymized locations. The prototype of TinyCasper is implemented on a physical test-bed on the TinyOS/Mote platform with 39 MICAz motes.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1307–1310},
numpages = {4},
keywords = {aggregate query processing, location anonymization, location privacy, aggregate location monitoring systems, wireless sensor networks},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376763,
author = {B\"{o}hm, Alexander and Marth, Erich and Kanne, Carl-Christian},
title = {The Demaq System: Declarative Development of Distributed Applications},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376763},
doi = {10.1145/1376616.1376763},
abstract = {The goal of the Demaq project is to investigate a novel way of thinking about distributed applications that are based on the asynchronous exchange of XML messages. Unlike today's solutions that rely on imperative programming languages and multi-tiered application servers, Demaq uses a declarative language for implementing the application logic as a set of rules. A rule compiler transforms the application specifications into execution plans against the message history. The plans are evaluated using our optimized runtime engine. This allows us to leverage existing knowledge about declarative query processing for optimizing distributed applications.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1311–1314},
numpages = {4},
keywords = {xml, declarative, demaq, language, messaging, queues},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376764,
author = {Chandramouli, Badrish and Yang, Jun and Agarwal, Pankaj K. and Yu, Albert and Zheng, Ying},
title = {ProSem: Scalable Wide-Area Publish/Subscribe},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376764},
doi = {10.1145/1376616.1376764},
abstract = {We demonstrate ProSem, a scalable wide-area publish/subscribe system that supports complex, stateful subscriptions as well as simple ones. One unique feature of ProSem is its cost-based joint optimization of both subscription processing and notification dissemination. ProSem uses novel reformulation techniques to expose new alternatives for processing and disseminating data using standard stateless content-driven network components.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1315–1318},
numpages = {4},
keywords = {demonstration, publish, query, subscribe, continuous, database},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376765,
author = {Khoussainova, Nodira and Welbourne, Evan and Balazinska, Magdalena and Borriello, Gaetano and Cole, Garrett and Letchner, Julie and Li, Yang and R\'{e}, Christopher and Suciu, Dan and Walke, Jordan},
title = {A Demonstration of Cascadia through a Digital Diary Application},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376765},
doi = {10.1145/1376616.1376765},
abstract = {The Cascadia system provides RFID-based pervasive computing applications with an infrastructure for specifying, extracting and managing meaningful high-level events from raw RFID data. Cascadia allows application developers and even users to specify events of interest using either a declarative query language or a graphical interface with an intuitive visual language. Cascadia then effectively extracts these events from data in spite of the unreliability of RFID technology and the inherent ambiguity in event extraction.We demonstrate Cascadia's technique through a digital diary application in the form of a calendar. Cascadia automatically populates the calendar with meaningful events for the user. We use data collected in a building-wide RFID deployment.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1319–1322},
numpages = {4},
keywords = {rfid, probabilistic event detection, user interfaces},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376766,
author = {Amer-Yahia, Sihem and Galland, Alban and Stoyanovich, Julia and Yu, Cong},
title = {From Del.Icio.Us to x.Qui.Site: Recommendations in Social Tagging Sites},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376766},
doi = {10.1145/1376616.1376766},
abstract = {We present X.QUI.SITE, a scalable system for managing recommendations for social tagging sites like del.icio.us. seamlessly incorporates various user behaviors into the recommendations and aims to recommend not only items of interest, but also other relevant information like interesting people and/or topics. Explanations are also provided so that users can obtain a better understanding of the recommendations and decide which recommendations to pursue further. We discuss the technical challenges involved in characterizing different user behaviors and in efficiently computing recommendation explanations.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1323–1326},
numpages = {4},
keywords = {recommendation explanation, collaborative tagging sites, recommender systems},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376767,
author = {Boim, Rubi and Milo, Tova},
title = {Enriching Topic-Based Publish-Subscribe Systems with Related Content},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376767},
doi = {10.1145/1376616.1376767},
abstract = {This demonstration presents RMFinder (Related Messages Finder), a system that retains the simplicity and efficiency of topic-based P2P pub-sub, while providing a richer service where users can automatically receive all messages related to those in the topics to which they are subscribed. RMFinder is based on a novel, dynamic, distributed clustering algorithm, that takes advantage of similarities between topic messages to group topics together, into topic-clusters. The clusters adjust automatically to shifts in the focus of the messages published by the topics, as well as to changes in the users interest, and allow for an effective delivery of related messages with minimal overhead.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1327–1330},
numpages = {4},
keywords = {dynamic clustering, related content, publish-subscribe, p2p},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376768,
author = {Zhang, Ying and Boncz, Peter},
title = {XRPC: Distributed XQuery and Update Processing with Heterogeneous XQuery Engines},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376768},
doi = {10.1145/1376616.1376768},
abstract = {We demonstrate XRPC, a minimal XQuery extension that enables distributed querying between heterogeneous XQuery engines. The XRPC language extension enhances the existing concept of XQuery functions with the Remote Procedure Call (RPC) paradigm. XRPC is orthogonal to all XQuery features, including the XQuery Update Facility (XQUF). Note that executing xquf updating functions over XRPC leads to the phenomenon of distributed transactions. XRPC achieves heterogeneity by an open SOAP-based network protocol, that can be implemented by any engine, and an XRPC Wrapper that allows even XRPC-oblivious XQuery engines to handle XRPC requests efficiently. XRPC is fully implemented in the open-source MonetDB/XQuery engine, and is demonstrated here to co-operate with Saxon, Galax and X-Hive through the XRPC wrapper.This demonstration will focus on the following features of XRPC: (i) glue-less interaction between AJAX style webbased applications with XQuery databases thanks to the SOAP-based nature of the XRPC network protocol, (ii) the efficiency of XRPC communication also for voluminous interserver communication thanks to the Bulk RPC feature that optimizes network communication and exposes set-at-a-time opportunities to the underlying XQuery engines, (iii) the interoperability between different XQuery engines that can handle both distributed transactions (both read-only requests and updates) (iv) support and performance trade-offs of two different isolation levels for distributed transactions among different XQuery engines.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1331–1336},
numpages = {6},
keywords = {distributed transaction, xml, xquery, distributed querying},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376769,
author = {Fourny, Ghislain and Kossmann, Donald and Kraska, Tim and Pilman, Markus and Florescu, Daniela},
title = {XQuery in the Browser},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376769},
doi = {10.1145/1376616.1376769},
abstract = {Over the years, the browser has become a complete runtime environment for client-side programs. The main scripting language used towards this purpose is JavaScript, which was designed so as to program the browser. A lot of extensions and new layers have been built on top of it to allow e.g. DOM navigation and manipulation. However, JavaScript has become a victim of its own success and is used way beyond its possibilities, leading to increased code complexity. We suggest to reduce programming complexity by proposing XQuery as a client-side programming language. We wrote an extension for Microsoft Internet Explorer, based on the Zorba XQuery engine, which allows execution of XQuery scripts in the browser. An extension for Firefox is on the way as well. This paper demonstrates how client-side applications in XQuery look like and what they can do within a very small amount of code.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1337–1340},
numpages = {4},
keywords = {browser, dom, update, html, event handling, xml, javascript, xquery, programming, xqueryp, script},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376770,
author = {Sun, Yizhou and Wu, Tianyi and Yin, Zhijun and Cheng, Hong and Han, Jiawei and Yin, Xiaoxin and Zhao, Peixiang},
title = {BibNetMiner: Mining Bibliographic Information Networks},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376770},
doi = {10.1145/1376616.1376770},
abstract = {Online bibliographic databases, such as DBLP in computer science and PubMed in medical sciences, contain abundant information about research publications in different fields. Each such database forms a gigantic information network (hence called BibNet), connecting in complex ways research papers, authors, conferences/journals, and possibly citation information as well, and provides a fertile land for information network analysis. Our BibNetMiner is designed for sophisticated information network mining on such bibliographic databases. In this demo, we will take the DBLP database as an example, demonstrate several attractive functions of BibNetMiner, including clustering, ranking and profiling of conferences and authors based on the research subfields. A user-friendly, visualization-enhanced interface will be provided to facilitate interactive exploration of a bibliographic database. This project will serve as an example to demonstrate the power of links in information network mining. Since the dataset is large and the network is heterogeneous, such a study will benefit the research on the analysis of massive heterogeneous information networks.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1341–1344},
numpages = {4},
keywords = {bibliographic information networks, ranking, clustering, link analysis},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376772,
author = {Davidson, Susan B. and Freire, Juliana},
title = {Provenance and Scientific Workflows: Challenges and Opportunities},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376772},
doi = {10.1145/1376616.1376772},
abstract = {Provenance in the context of workflows, both for the data they derive and for their specification, is an essential component to allow for result reproducibility, sharing, and knowledge re-use in the scientific community. Several workshops have been held on the topic, and it has been the focus of many research projects and prototype systems. This tutorial provides an overview of research issues in provenance for scientific workflows, with a focus on recent literature and technology in this area. It is aimed at a general database research audience and at people who work with scientific data and workflows. We will (1) provide a general overview of scientific workflows, (2) describe research on provenance for scientific workflows and show in detail how provenance is supported in existing systems; (3) discuss emerging applications that are enabled by provenance; and (4) outline open problems and new directions for database-related research.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1345–1350},
numpages = {6},
keywords = {provenance, scientific workflows},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376773,
author = {O'Neil, Elizabeth J.},
title = {Object/Relational Mapping 2008: Hibernate and the Entity Data Model (Edm)},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376773},
doi = {10.1145/1376616.1376773},
abstract = {Object/Relational Mapping (ORM) provides a methodology and mechanism for object-oriented systems to hold their long-term data safely in a database, with transactional control over it, yet have it expressed when needed in program objects. Instead of bundles of special code for this, ORM encourages models and use of constraints for the application, which then runs in a context set up by the ORM. Today's web applications are particularly well-suited to this approach, as they are necessarily multithreaded and thus are prone to race conditions unless the interaction with the database is very carefully implemented. The ORM approach was first realized in Hibernate, an open source project for Java systems started in 2002, and this year is joined by Microsoft's Entity Data Model for .NET systems. Both are described here.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1351–1356},
numpages = {6},
keywords = {impedance mismatch, hibernate, entity data model, data model, schema mapping, persistence, object-relational mapping},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376774,
author = {Pei, Jian and Hua, Ming and Tao, Yufei and Lin, Xuemin},
title = {Query Answering Techniques on Uncertain and Probabilistic Data: Tutorial Summary},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376774},
doi = {10.1145/1376616.1376774},
abstract = {Uncertain data are inherent in some important applications, such as environmental surveillance, market analysis, and quantitative economics research. Due to the importance of those applications and the rapidly increasing amount of uncertain data collected and accumulated, analyzing large collections of uncertain data has become an important task and has attracted more and more interest from the database community. Recently, uncertain data management has become an emerging hot area in database research and development. In this tutorial, we systematically review some representative studies on answering various queries on uncertain and probabilistic data.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1357–1364},
numpages = {8},
keywords = {query processing, uncertain data, probabilistic data},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376775,
author = {Nakamura, Eduardo F. and Loureiro, Antonio A. F.},
title = {Information Fusion in Wireless Sensor Networks},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376775},
doi = {10.1145/1376616.1376775},
abstract = {Wireless sensor networks (WSNs) are commonly treated as a distributed database system that is accessed by means of a query language. However, the computation of such queries are usually performed by information fusion techniques. Information fusion has been used by applications to detect/classify events, track targets, and filter noisy measurements. In this tutorial, we discuss some of the information fusion techniques that are currently used in WSNs.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1365–1372},
numpages = {8},
keywords = {data fusion, data aggregation, architectures and models, wireless sensor networks, information fusion},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376776,
author = {Konstan, Joseph A.},
title = {Introduction to Recommender Systems},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376776},
doi = {10.1145/1376616.1376776},
abstract = {Recommender systems help users find the information, products, and other people they most want to find. This tutorial provides participants with a hands-on learning experience about using recommender system technologies. After completing this tutorial, participants will understand the range of technologies being used for recommender systems, including collaborative filtering, rules-based systems, and information filtering.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1373–1374},
numpages = {2},
keywords = {collaborative filtering, recommender systems},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1376778,
author = {Andoni, Alexandr and Fagin, Ronald and Kumar, Ravi and Patrascu, Mihai and Sivakumar, D.},
title = {Corrigendum to "Efficient Similarity Search and Classification via Rank Aggregation" by Ronald Fagin, Ravi Kumar and D. Sivakumar (Proc. SIGMOD'03)},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376778},
doi = {10.1145/1376616.1376778},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1375–1376},
numpages = {2},
keywords = {median, nearest neighbor, rank aggregation, score aggregation},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1452517,
author = {Vardi, Moshe},
title = {PODS Alberto O. Mendelzon Test-of-Time Award 2008},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1452517},
doi = {10.1145/1376616.1452517},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
articleno = {1},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1452518,
author = {Cohen, William},
title = {SIGMOD 10-Year Test-of-Time Award: “Integration of Heterogeneous Databases without Common Domains Using Queries Based on Textual Simularity”},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1452518},
doi = {10.1145/1376616.1452518},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
articleno = {2},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1452519,
author = {Fuxman, Ariel},
title = {SIGMOD Best Dissertation Award: “Efficient Query Processing over Inconsistent Databases”},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1452519},
doi = {10.1145/1376616.1452519},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
articleno = {3},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{10.1145/1376616.1452516,
author = {Schneider, Carl and Polyzotis, Neoklis},
title = {Best Newcomer Award: Evaluating Rank Joins with Optimal Costs},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1452516},
doi = {10.1145/1376616.1452516},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
articleno = {4},
numpages = {1},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

