@inproceedings{10.1145/1247480.1379334,
author = {Hellerstein, Joseph M. and Haas, Peter J. and Wang, Helen J.},
title = {2007 Test-of-Time Award “Online Aggregation”},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1379334},
doi = {10.1145/1247480.1379334},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
articleno = {1},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1379335,
author = {Hellerstein, Joseph M. and Stoica, Ion and Loo, Boon Thau},
title = {2007 Dissertation Award “Declarative Networking”},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1379335},
doi = {10.1145/1247480.1379335},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
articleno = {2},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1379336,
author = {Widom, Jennifer},
title = {2007 SIGMOD Edger F. Codd Innovations Award: “Research Principles Revealed”},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1379336},
doi = {10.1145/1247480.1379336},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
articleno = {3},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247482,
author = {Bernstein, Philip A. and Melnik, Sergey},
title = {Model Management 2.0: Manipulating Richer Mappings},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247482},
doi = {10.1145/1247480.1247482},
abstract = {Model management is a generic approach to solving problems of data programmability where precisely engineered mappings are required. Applications include data warehousing, e-commerce, object-to-relational wrappers, enterprise information integration, database portals, and report generators. The goal is to develop a model management engine that can support tools for all of these applications. The engine supports operations to match schemas, compose mappings, diff schemas, merge schemas, translate schemas into different data models, and generate data transformations from mappings.Much has been learned about model management since it was proposed seven years ago. This leads us to a revised vision that differs from the original in two main respects: the operations must handle more expressive mappings, and the runtime that executes mappings should be added as an important model management component. We review what has been learned from recent experience, explain the revised model management vision based on that experience, and identify the research problems that the revised vision opens up.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1–12},
numpages = {12},
keywords = {engineered mapping, model management, data translation, schema matching, data exchange, schema evolution, data integration, schema mapping},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245745,
author = {Halevy, Alon},
title = {Session Details: Keynote Talks},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245745},
doi = {10.1145/3245745},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247483,
author = {Jagadish, H. V. and Chapman, Adriane and Elkiss, Aaron and Jayapandian, Magesh and Li, Yunyao and Nandi, Arnab and Yu, Cong},
title = {Making Database Systems Usable},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247483},
doi = {10.1145/1247480.1247483},
abstract = {Database researchers have striven to improve the capability of a database in terms of both performance and functionality. We assert that the usability of a database is as important as its capability. In this paper, we study why database systems today are so difficult to use. We identify a set of five pain points and propose a research agenda to address these. In particular, we introduce a presentation data model and recommend direct data manipulation with a schema later approach. We also stress the importance of provenance and of consistency across presentation models.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {13–24},
numpages = {12},
keywords = {user interface, database, usability},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247484,
author = {Weikum, Gerhard},
title = {DB&amp;IR: Both Sides Now},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247484},
doi = {10.1145/1247480.1247484},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {25–30},
numpages = {6},
keywords = {semantic search, XML, DB&amp;IR integration, information retrieval, database systems, web knowledge},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245746,
author = {Kossman, Donald},
title = {Session Details: Database Technology for Novel Applications},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245746},
doi = {10.1145/3245746},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247486,
author = {White, Walker and Demers, Alan and Koch, Christoph and Gehrke, Johannes and Rajagopalan, Rajmohan},
title = {Scaling Games to Epic Proportions},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247486},
doi = {10.1145/1247480.1247486},
abstract = {We introduce scalability for computer games as the next frontier for techniques from data management. A very important aspect of computer games is the artificial intelligence (AI) of non-player characters. To create interesting AI in games today, developers or players have to create complex, dynamic behavior for a very small number of characters, but neither the game engines nor the style of AI programming enables intelligent behavior that scales to a very large number of non-player characters.In this paper we make a first step towards truly scalable AI in computer games by modeling game AI as a data management problem. We present a highly expressive scripting language SGL that provides game designers and players with a data-driven AI scheme for customizing behavior for individual non-player characters. We use sophisticated query processing and indexing techniques to efficiently execute large numbers of SGL scripts, thus providing a framework for games with a truly epic number of non-player characters. Experiments show the efficacy of our solutions.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {31–42},
numpages = {12},
keywords = {aggregates, games, scripting, indexing},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247487,
author = {Dong, Xin and Halevy, Alon},
title = {Indexing Dataspaces},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247487},
doi = {10.1145/1247480.1247487},
abstract = {Dataspaces are collections of heterogeneous and partially unstructured data. Unlike data-integration systems that also offer uniform access to heterogeneous data sources, dataspaces do not assume that all the semantic relationships between sources are known and specified. Much of the user interaction with dataspaces involves exploring the data, and users do not have a single schema to which they can pose queries. Consequently, it is important that queries are allowed to specify varying degrees of structure, spanning keyword queries to more structure-aware queries.This paper considers indexing support for queries that combine keywords and structure. We describe several extensions to inverted lists to capture structure when it is present. In particular, our extensions incorporate attribute labels, relationships between data items, hierarchies of schema elements, and synonyms among schema elements. We describe experiments showing that our indexing techniques improve query efficiency by an order of magnitude compared with alternative approaches, and scale well with the size of the data.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {43–54},
numpages = {12},
keywords = {indexing, dataspace, heterogeneity},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247488,
author = {Lee, Sang-Won and Moon, Bongki},
title = {Design of Flash-Based DBMS: An in-Page Logging Approach},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247488},
doi = {10.1145/1247480.1247488},
abstract = {The popularity of high-density flash memory as data storage media has increased steadily for a wide spectrum of computing devices such as PDA's, MP3 players, mobile phones and digital cameras. More recently, computer manufacturers started launching new lines of mobile or portable computers that did away with magnetic disk drives altogether, replacing them with tens of gigabytes of NAND flash memory. Like EEPROM and magnetic disk drives, flash memory is non-volatile and retains its contents even when the power is turned off. As its capacity increases and price drops, flash memory will compete more successfully with lower-end, lower-capacity disk drives. It is thus not inconceivable to consider running a full database system on the flash-only computing platforms or running an embedded database system on the lightweight computing devices. In this paper, we present a new design called in-page logging (IPL) for flash memory based database servers. This new design overcomes the limitations of flash memory such as high write latency, and exploits unique characteristics of flash memory to achieve the best attainable performance for flash-based database servers. We show empirically that the IPL approach can yield considerable performance benefit over traditional design for disk-based database servers. We also show that the basic design of IPL can be elegantly extended to support transactional database recovery.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {55–66},
numpages = {12},
keywords = {flash-memory database server, in-page logging},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245747,
author = {Bertino, Elisa},
title = {Session Details: Database Privacy and Security},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245747},
doi = {10.1145/3245747},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247490,
author = {Park, Hyoungmin and Shim, Kyuseok},
title = {Approximate Algorithms for K-Anonymity},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247490},
doi = {10.1145/1247480.1247490},
abstract = {When a table containing individual data is published, disclosure of sensitive information should be prohibitive. A naive approach for the problem is to remove identifiers such as name and social security number. However, linking attacks which joins the published table with other tables on some attributes, called quasi-identifier, may reveal the sensitive information. To protect privacy against linking attack, the notion of k-anonymity which makes each record in the table be indistinguishable with k-1 other records has been proposed previously. It is shown to be NP-Hard to k-anonymize a table minimizing the number of suppressed cells. To alleviate this, O(k log k)-approximation and O(k)-approximation algorithms were proposed in previous works.In this paper, we propose several approximation algorithms that guarantee O(log k)-approximation ratio and perform significantly better than the traditional algorithms. We also provide O(\ss{} log k)-approximate algorithms which gracefully adjust their running time according to the tolerance \'{e} (≥ 1) of the approximation ratios. Experimental results confirm that our approximation algorithms perform significantly better than traditional approximation algorithms.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {67–78},
numpages = {12},
keywords = {data publishing, local recoding, privacy preservation, anonymity, data mining},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247491,
author = {Agrawal, Rakesh and Evfimievski, Alexandre and Kiernan, Jerry and Velu, Raja},
title = {Auditing Disclosure by Relevance Ranking},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247491},
doi = {10.1145/1247480.1247491},
abstract = {Numerous widely publicized cases of theft and misuse of private information underscore the need for audit technology to identify the sources of unauthorized disclosure. We present an auditing methodology that ranks potential disclosure sources according to their proximity to the leaked records. Given a sensitive table that contains the disclosed data, our methodology prioritizes by relevance the past queries to the database that could have potentially been used to produce the sensitive table. We provide three conceptually different measures of proximity between the sensitive table and a query result. One measure is inspired by information retrieval in text processing, another is based on statistical record linkage, and the third computes the derivation probability of the sensitive table in a tree-based generative model. We also analyze the characteristics of the three measures and the corresponding ranking algorithms.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {79–90},
numpages = {12},
keywords = {information retrieval, hippocratic database, derivation probability, record linkage, privacy},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247492,
author = {Stahlberg, Patrick and Miklau, Gerome and Levine, Brian Neil},
title = {Threats to Privacy in the Forensic Analysis of Database Systems},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247492},
doi = {10.1145/1247480.1247492},
abstract = {The use of any modern computer system leaves unintended traces of expired data and remnants of users' past activities. In this paper, we investigate the unintended persistence of data stored in database systems. This data can be recovered by forensic analysis, and it poses a threat to privacy.First, we show how data remnants are preserved in database table storage, the transaction log, indexes, and other system components. Our evaluation of several real database systems reveals that deleted data is not securely removed from database storage and that users have little control over the persistence of deleted data.Second, we address the problem of unintended data retention by proposing a set of system transparency criteria: data retention should be avoided when possible, evident to users when it cannot be avoided, and bounded in time.Third, we propose specific techniques for secure record deletion and log expunction that increase the transparency of database systems, making them more resistant to forensic analysis.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {91–102},
numpages = {12},
keywords = {transparency, forensics, privacy},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245748,
author = {Koudas, Nick},
title = {Session Details: Top-k Queries and Ranking},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245748},
doi = {10.1145/3245748},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247494,
author = {Xin, Dong and Han, Jiawei and Chang, Kevin C.},
title = {Progressive and Selective Merge: Computing Top-k with Ad-Hoc Ranking Functions},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247494},
doi = {10.1145/1247480.1247494},
abstract = {The family of threshold algorithm (ie, TA) has been widely studied for efficiently computing top-k queries. TA uses a sort-merge framework that assumes data lists are pre-sorted, and the ranking functions are monotone. However, in many database applications, attribute values are indexed by tree-structured indices (eg, B-tree, R-tree), and the ranking functions are not necessarily monotone. To answer top-k queries with ad-hoc ranking functions, this paper studies anindex-merge paradigm that performs progressive search over the space of joint states composed by multiple index nodes.We address two challenges for efficient query processing. First, to minimize the search complexity, we present a double-heap algorithm which supports not only progressive state search but also progressive state generation. Second, to avoid unnecessary disk access, we characterize a type of "empty-state" that does not contribute to the final results, and propose a new materialization model, join-signature, to prune empty-states. Our performance study shows that the proposed method achieves one order of magnitude speed-up over baseline solutions.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {103–114},
numpages = {12},
keywords = {top-k query, selective merge, progressive merge},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247495,
author = {Luo, Yi and Lin, Xuemin and Wang, Wei and Zhou, Xiaofang},
title = {Spark: Top-k Keyword Query in Relational Databases},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247495},
doi = {10.1145/1247480.1247495},
abstract = {With the increasing amount of text data stored in relational databases, there is a demand for RDBMS to support keyword queries over text data. As a search result is often assembled from multiple relational tables, traditional IR-style ranking and query evaluation methods cannot be applied directly.In this paper, we study the effectiveness and the efficiency issues of answering top-k keyword query in relational database systems. We propose a new ranking formula by adapting existing IR techniques based on a natural notion of virtual document. Compared with previous approaches, our new ranking method is simple yet effective, and agrees with human perceptions. We also study efficient query processing methods for the new ranking method, and propose algorithms that have minimal accesses to the database. We have conducted extensive experiments on large-scale real databases using two popular RDBMSs. The experimental results demonstrate significant improvement to the alternative approaches in terms of retrieval effectiveness and efficiency.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {115–126},
numpages = {12},
keywords = {top-k, information retrieval, relational database, keyword search},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247496,
author = {Li, Chengkai and Wang, Min and Lim, Lipyeow and Wang, Haixun and Chang, Kevin Chen-Chuan},
title = {Supporting Ranking and Clustering as Generalized Order-by and Group-By},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247496},
doi = {10.1145/1247480.1247496},
abstract = {The Boolean semantics of SQL queries cannot adequately capture the "fuzzy" preferences and "soft" criteria required in non-traditional data retrieval applications. One way to solve this problem is to add a flavor of "information retrieval" into database queries by allowing fuzzy query conditions and flexibly supporting grouping and ranking of the query results within the DBMS engine. While ranking is already supported by all major commercial DBMSs natively, support of flexibly grouping is still very limited (i.e., group-by).In this paper, we propose to generalize group-by to enable flexible grouping (clustering specifically) of the query results. Different from clustering in data mining applications, our focus is on supporting efficient clustering of Boolean results generated at query time. Moreover, we propose to integrate ranking and clustering with Boolean conditions, forming a new type of ClusterRank query to allow structured data retrieval. Such an integration is nontrivial in terms of both semantics and query processing. We investigate various semantics of this type of queries. To process such queries, a straightforward approach is to simply glue the techniques developed for ranking-only and clustering-only together. This approach is costly since both ranking and clustering are treated as blocking post-processing tasks upon Boolean query results by existing techniques. We propose a summary-based evaluation method that utilizes bitmap index to seamlessly integrate Boolean conditions, clustering, and ranking. Experimental study shows that our approach significantly outperforms the straightforward one and maintains high clustering quality.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {127–138},
numpages = {12},
keywords = {data exploration, ranking, retrieval, grouping, query processing, top-k, clustering},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245749,
author = {Fan, Wenfei},
title = {Session Details: Data Source Selection and Integration},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245749},
doi = {10.1145/3245749},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247498,
author = {Yu, Bei and Li, Guoliang and Sollins, Karen and Tung, Anthony K. H.},
title = {Effective Keyword-Based Selection of Relational Databases},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247498},
doi = {10.1145/1247480.1247498},
abstract = {The wide popularity of free-and-easy keyword based searches over World Wide Web has fueled the demand for incorporating keyword-based search over structured databases. However, most of the current research work focuses on keyword-based searching over a single structured data source. With the growing interest in distributed databases and service oriented architecture over the Internet, it is important to extend such a capability over multiple structured data sources. One of the most important problems for enabling such a query facility is to be able to select the most useful data sources relevant to the keyword query. Traditional database summary techniques used for selecting unstructured datasources developed in IR literature are inadequate for our problem, as they do not capture the structure of the data sources. In this paper, we study the database selection problem for relational data sources, and propose a method that effectively summarizes the relationships between keywords in a relational database based on its structure. We develop effective ranking methods based on the keyword relationship summaries in order to select the most useful databases for a given keyword query. We have implemented our system on PlanetLab. In that environment we use extensive experiments with real datasets to demonstrate the effectiveness of our proposed summarization method.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {139–150},
numpages = {12},
keywords = {summarization, database selection, keyword query},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247499,
author = {Qi, Yan and Candan, K. Sel\c{c}uk and Sapino, Maria Luisa},
title = {FICSR: <i>F</i>Eedback-Based <i>i</i>n<i>c</i>on<i>s</i>Istency <i>r</i>Esolution and Query Processing on Misaligned Data Sources},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247499},
doi = {10.1145/1247480.1247499},
abstract = {A critical reality in data integration is that knowledge from different sources may often be conflicting with each other. Conflict resolutioncan be costly and, if done without proper context, can be ineffective. In this paper, we propose a novel query-driven and feedback-based approach (FICSR1) to conflict resolution when integrating data sources. In particular, instead of relying on traditional model based definition of consistency, we introduce a ranked interpretation. This not only enables FICSR to deal with the complexity of the conflict resolution process, but also helps achieve a more direct match between the users' (subjective) interpretation of the data and the system's (objective) treatment of the available alternatives. Consequently, the ranked interpretation leads to new opportunities for bi-directional (data informsover ↔ user) feedback cycle for conflict resolution: given a query, (a) a preliminary ranking of candidate results on data can inform the user regarding constraints critical to the query, while (b) user feedback regarding the ranks can be exploited to inform the system about user's relevant domain knowledge. To enable this feedback process, we develop data structures and algorithms for efficient off-line conflict/agreement analysis of the integrated data as well as for on-line query processing, candidate result enumeration, and validity analysis. The results are brought together and evaluated in the FICSR system.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {151–162},
numpages = {12},
keywords = {relevance feedback, reasoning with misaligned data, query processing, taxonomy, conflicts},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247500,
author = {Huang, Jiansheng and Naughton, Jeffrey F.},
title = {K-Relevance: A Spectrum of Relevance for Data Sources Impacting a Query},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247500},
doi = {10.1145/1247480.1247500},
abstract = {Applications ranging from grid management to sensor nets to web-based information integration and extraction can be viewed as receiving data from some number of autonomous remote data sources and then answering queries over this collected data. In such environments it is helpful to inform users which data sources are "relevant" to their query results. It is not immediately obvious what "relevant" should mean in this context, as different users will have different requirements. In this paper, rather than proposing a single definition of relevance, we propose a spectrum of definitions, which we term "k-relevance", for k ≥ 0. We give algorithms for identifying k-relevant data sources for relational queries and explore their efficiency both analytically and experimentally. Finally, we explore the impact of integrity constraints (including dependencies) and materialized views on the problem of computing and maintaining relevant data sources.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {163–174},
numpages = {12},
keywords = {relevant sources, update relevance, lineage relevance, materialized view, k-relevance, k-relevant},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245750,
author = {Garofalakis, Minos N.},
title = {Session Details: Approximate Query Processing},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245750},
doi = {10.1145/3245750},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247502,
author = {Larson, Per-Ake and Lehner, Wolfgang and Zhou, Jingren and Zabback, Peter},
title = {Cardinality Estimation Using Sample Views with Quality Assurance},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247502},
doi = {10.1145/1247480.1247502},
abstract = {Accurate cardinality estimation is critically important to high-quality query optimization. It is well known that conventional cardinality estimation based on histograms or similar statistics may produce extremely poor estimates in a variety of situations, for example, queries with complex predicates, correlation among columns, or predicates containing user-defined functions. In this paper, we propose a new, general cardinality estimation technique that combines random sampling and materialized view technology to produce accurate estimates even in these situations. As a major innovation, we exploit feedback information from query execution and process control techniques to assure that estimates remain statistically valid when the underlying data changes. Experimental results based on a prototype implementation in Microsoft SQL Server demonstrate the practicality of the approach and illustrate the dramatic effects improved cardinality estimates may have.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {175–186},
numpages = {12},
keywords = {statistical quality control, query optimization, sequential sampling, sample views, cardinality estimation},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247503,
author = {Rusu, Florin and Dobra, Alin},
title = {Statistical Analysis of Sketch Estimators},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247503},
doi = {10.1145/1247480.1247503},
abstract = {Sketching techniques can provide approximate answers to aggregate queries either for data-streaming or distributed computation. Small space summaries that have linearity properties are required for both types of applications. The prevalent method for analyzing sketches uses moment analysis and distribution independent bounds based on moments. This method produces clean, easy to interpret, theoretical bounds that are especially useful for deriving asymptotic results. However, the theoretical bounds obscure fine details of the behavior of various sketches and they are mostly not indicative of which type of sketches should be used in practice. Moreover, no significant empirical comparison between various sketching techniques has been published, which makes the choice even harder. In this paper, we take a close look at the sketching techniques proposed in the literature from a statistical point of view with the goal of determining properties that indicate the actual behavior and producing tighter confidence bounds. Interestingly, the statistical analysis reveals that two of the techniques, Fast-AGMS and Count-Min, provide results that are in some cases orders of magnitude better than the corresponding theoretical predictions. We conduct an extensive empirical study that compares the different sketching techniques in order to corroborate the statistical analysis with the conclusions we draw from it. The study indicates the expected performance of various sketches, which is crucial if the techniques are to be used by practitioners. The overall conclusion of the study is that Fast-AGMS sketches are, for the full spectrum of problems, either the best, or close to the best, sketching technique. This makes Fast-AGMS sketches the preferred choice irrespective of the situation.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {187–198},
numpages = {12},
keywords = {fast-count sketches, size of join estimation, AGMS sketches, count-min sketches, fast-AGMS sketches},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247504,
author = {Beyer, Kevin and Haas, Peter J. and Reinwald, Berthold and Sismanis, Yannis and Gemulla, Rainer},
title = {On Synopses for Distinct-Value Estimation under Multiset Operations},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247504},
doi = {10.1145/1247480.1247504},
abstract = {The task of estimating the number of distinct values (DVs) in a large dataset arises in a wide variety of settings in computer science and elsewhere. We provide DV estimation techniques that are designed for use within a flexible and scalable "synopsis warehouse" architecture. In this setting, incoming data is split into partitions and a synopsis is created for each partition; each synopsis can then be used to quickly estimate the number of DVs in its corresponding partition. By combining and extending a number of results in the literature, we obtain both appropriate synopses and novel DV estimators to use in conjunction with these synopses. Our synopses can be created in parallel, and can then be easily combined to yield synopses and DV estimates for arbitrary unions, intersections or differences of partitions. Our synopses can also handle deletions of individual partition elements. We use the theory of order statistics to show that our DV estimators are unbiased, and to establish moment formulas and sharp error bounds. Based on a novel limit theorem, we can exploit results due to Cohen in order to select synopsis sizes when initially designing the warehouse. Experiments and theory indicate that our synopses and estimators lead to lower computational costs and more accurate DV estimates than previous approaches.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {199–210},
numpages = {12},
keywords = {synopsis warehouse, distinct-value estimation},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245751,
author = {Zhou, Aoying},
title = {Session Details: P2P Based Data Management},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245751},
doi = {10.1145/3245751},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247506,
author = {Akbarinia, Reza and Pacitti, Esther and Valduriez, Patrick},
title = {Data Currency in Replicated DHTs},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247506},
doi = {10.1145/1247480.1247506},
abstract = {Distributed Hash Tables (DHTs) provide a scalable solution for data sharing in P2P systems. To ensure high data availability, DHTs typically rely on data replication, yet without data currency guarantees. Supporting data currency in replicated DHTs is difficult as it requires the ability to return a current replica despite peers leaving the network or concurrent updates. In this paper, we give a complete solution to this problem. We propose an Update Management Service (UMS) to deal with data availability and efficient retrieval of current replicas based on timestamping. For generating timestamps, we propose a Key-based Timestamping Service (KTS) which performs distributed timestamp generation using local counters. Through probabilistic analysis, we compute the expected number of replicas which UMS must retrieve for finding a current replica. Except for the cases where the availability of current replicas is very low, the expected number of retrieved replicas is typically small, e.g. if at least 35% of available replicas are current then the expected number of retrieved replicas is less than 3. We validated our solution through implementation and experimentation over a 64-node cluster and evaluated its scalability through simulation up to 10,000 peers using SimJava. The results show the effectiveness of our solution. They also show that our algorithm used in UMS achieves major performance gains, in terms of response time and communication cost, compared with a baseline algorithm.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {211–222},
numpages = {12},
keywords = {peer-to-peer, data currency, data dvailability, distributed hash table (DHT), data replication},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247507,
author = {Crainiceanu, Adina and Linga, Prakash and Machanavajjhala, Ashwin and Gehrke, Johannes and Shanmugasundaram, Jayavel},
title = {P-Ring: An Efficient and Robust P2P Range Index Structure},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247507},
doi = {10.1145/1247480.1247507},
abstract = {Peer-to-peer systems have emerged as a robust, scalable and decentralized way to share and publish data. In this paper, we propose P-Ring, a new P2P index structure that supports both equality and range queries. P-Ring is fault-tolerant, provides logarithmic search performance even for highly skewed data distributions and efficiently supports large sets of data items per peer. We experimentally evaluate P-Ring using both simulations and a real distributed deployment on PlanetLab, and we compare its performance with Skip Graphs, Online Balancing and Chord.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {223–234},
numpages = {12},
keywords = {peer-to-peer systems, load balancing, range queries},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247508,
author = {Geambasu, Roxana and Balazinska, Magdalena and Gribble, Steven D. and Levy, Henry M.},
title = {Homeviews: Peer-to-Peer Middleware for Personal Data Sharing Applications},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247508},
doi = {10.1145/1247480.1247508},
abstract = {This paper presents HomeViews, a peer-to-peer middleware system for building personal data management applications. HomeViews provides abstractions and services for data organization and distributed data sharing. The key innovation in HomeViews is the integration of three concepts: views and queries from databases, a capability-based protection model from operating systems, and a peer-to-peer distributed architecture. Using HomeViews, applications can (1)create views to organize files into dynamic collections, (2) share these views in a protected way across the Internet through simple exchange of capabilities, and (3) transparently integrate remote views and data into a user's local organizational structures. HomeViews operates in a purely peer-to-peer fashion, without the need for account administration or centralized data and protection management inherent in typical data-sharing systems.We have prototyped HomeViews, deployed it on a small network of Linux machines, and used it to develop two distributed data-sharing applications: a peer-to-peer version of the Gallery photo-sharing application and a simple read-only shared file system. Using measurements, we demonstrate the practicality and performance of our approach.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {235–246},
numpages = {12},
keywords = {search, capabilities, access control, peer-to-peer, personal information management},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245752,
author = {Nascimento, Mario},
title = {Session Details: Data Stream Management},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245752},
doi = {10.1145/3245752},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247510,
author = {Bandi, Nagender and Metwally, Ahmed and Agrawal, Divyakant and El Abbadi, Amr},
title = {Fast Data Stream Algorithms Using Associative Memories},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247510},
doi = {10.1145/1247480.1247510},
abstract = {The primary goal of data stream research is to develop space and time efficient solutions for answering continuous on-line summarization queries. Research efforts over the last decade have resulted in a number of efficient algorithms with varying degrees of space and time complexities. While these techniques are developed in a standard CPU setting, many of their applications such as click-fraud detection and network-traffic summarization typically execute on special networking architectures called Network Processing Units (NPUs). These NPUs interface with special associative memories known as Ternary Content Addressable Memories (TCAMs) to provide gigabit rate forwarding at network routers. In this paper, we describe how the integrated architecture of NPU and TCAMs can be exploited towards achieving the goal of developing high-speed stream summarization solutions. We propose two TCAM-conscious solutions for the frequent elements problem in data streams and present a comprehensive evaluation of these techniques on a state-of-the-art networking platform.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {247–256},
numpages = {10},
keywords = {hardware, TCAMS, data streams},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247511,
author = {Tang, Lv-an and Cui, Bin and Li, Hongyan and Miao, Gaoshan and Yang, Dongqing and Zhou, Xinbiao},
title = {Effective Variation Management for Pseudo Periodical Streams},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247511},
doi = {10.1145/1247480.1247511},
abstract = {Many database applications require the analysis and processing of data streams. In such systems, huge amounts of data arrive rapidly and their values change over time. The variations on streams typically imply some fundamental changes of the underlying objects and possess significant domain meanings. In some data streams, successive events seem to recur in a certain time interval, but the data indeed evolves with tiny differences as time elapses. This feature is called pseudo periodicity, which poses a non-trivial challenge to variation management in data streams. This paper presents our research effort in online variation management over such streams, and the idea can be applied to the problem domain of medical applications, such as patient vital signal monitoring. We propose a new method named Pattern Growth Graph (PGG) to detect and manage variations over pseudo periodical streams. PGG adopts the wave-pattern to capture the major information of data evolution and represent them compactly. With the help of wave-pattern matching algorithm, PGG detects the stream variations in a single pass over the stream data. PGG only stores the different segments of the pattern for incoming stream, and hence it can substantially compress the data without losing important information. The statistical information of PGG helps to distinguish meaningful data changes from noise and to reconstruct the stream with acceptable accuracy. Extensive experiments on real datasets containing millions of data items demonstrate the feasibility and effectiveness of the proposed scheme.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {257–268},
numpages = {12},
keywords = {date stream, pattern growth, pseudo periodicity, variation management},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247512,
author = {Gou, Gang and Chirkova, Rada},
title = {Efficient Algorithms for Evaluating Xpath over Streams},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247512},
doi = {10.1145/1247480.1247512},
abstract = {In this paper we address the problem of evaluating XPath queries over streaming XML data. We consider a practical XPath fragment called Univariate XPath, which includes the commonly used '/' and '//' axes and allows *-node tests and arbitrarily nested predicates. It is well known that this XPath fragment can be efficiently evaluated in O(|D||Q|) time in the non-streaming environment, where |D| is the document size and |Q| is the query size. However, this is not necessarily true in the streaming environment, since streaming algorithms have to satisfy stricter requirement than non-streaming algorithms, in that all data must be read sequentially in one pass. Therefore, it is not surprising that state-of-the-art stream-querying algorithms have higher time complexity than O(|D||Q|).In this paper we revisit the XPath stream-querying problem, and show that Univariate XPath can be efficiently evaluated in O|D||Q|) time in the streaming environment. Specifically, we propose two O(|D||Q|)-time stream-querying algorithms, LQ and EQ, which are based on the lazy strategy and on the eager strategy, respectively. To the best of our knowledge, LQ and EQ are the first XPath stream-querying algorithms that achieve O(|D||Q|) time performance. Further, our algorithms achieve O(|D||Q|) time performance without trading off space performance. Instead, they have better buffering-space performance than state-of-the-art stream-querying algorithms. In particular, EQ achieves optimal buffering-space performance. Our experimental results show that our algorithms have not only good theoretical complexity but also considerable practical performance advantages over existing algorithms.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {269–280},
numpages = {12},
keywords = {streams, XPath, query processing, XML},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247513,
author = {Cormode, Graham and Garofalakis, Minos},
title = {Sketching Probabilistic Data Streams},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247513},
doi = {10.1145/1247480.1247513},
abstract = {The management of uncertain, probabilistic data has recently emerged as a useful paradigm for dealing with the inherent unreliabilities of several real-world application domains, including data cleaning, information integration, and pervasive, multi-sensor computing. Unlike conventional data sets, a set of probabilistic tuples defines a probability distribution over an exponential number of possible worlds (i.e., "grounded", deterministic databases). This "possibleworlds" interpretation allows for clean query semantics but also raises hard computational problems for probabilistic database query processors. To further complicate matters, in many scenarios (e.g., large-scale process and environmental monitoring using multiple sensor modalities), probabilistic data tuples arrive and need to be processed in a streaming fashion; that is, using limited memory and CPU resources and without the benefit of multiple passes over a static probabilistic database. Such probabilistic data streams raise a host of new research challenges for stream-processing engines that, to date, remain largely unaddressed.In this paper, we propose the first space- and time-efficient algorithms for approximating complex aggregate queries (including, the number of distinct values and join/self-join sizes) over probabilistic data streams. Following the possible-worlds semantics, such aggregates essentially define probability distributions over the space of possible aggregation results, and our goal is to characterize such distributions through efficient approximations of their key moments (such as expectation and variance). Our algorithms offer strong randomized estimation guarantees while using only sublinear space in the size of the stream(s), and rely on novel, concise streaming sketch synopses that extend conventional sketching ideas to the probabilistic streams setting. Our experimental results verify the effectiveness of our approach.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {281–292},
numpages = {12},
keywords = {uncertain data, data streams},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245753,
author = {Lin, Xuemin},
title = {Session Details: Query Processing of Semi-Structured Data},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245753},
doi = {10.1145/3245753},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247515,
author = {Fan, Wenfei and Cong, Gao and Bohannon, Philip},
title = {Querying Xml with Update Syntax},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247515},
doi = {10.1145/1247480.1247515},
abstract = {This paper investigates a class of transform queries proposed by XQuery Update [6]. A transform query is defined in terms of XML update syntax. When posed on an XML tree T, it returns another XML tree that would be produced by executing its embedded update on T, without destructive impact on T. Transform queries support a variety of applications including XML hypothetical queries, the simulation of updates on virtual views, and the enforcement of XML access control. In light of the wide-range of applications for transform queries, we develop automaton-based techniques for efficiently evaluating transform queries and for computing their compositions with user queries in standard XQuery. We provide (a)three algorithms to implement transform queries without change to existing XQuery processors,(b) a linear-time algorithm, based on a seamless integration of automaton execution and SAX parsing, to evaluate transform queries on large XML documents that are difficult to handle by existing XQuery engines, and (c) an algorithm to rewrite the composition of user queries and transform queries into a single efficient query in standard XQuery. We also present experimental results comparing the efficiency of our evaluation and composition algorithms for transform queries.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {293–304},
numpages = {12},
keywords = {updates, transform queries, XML, query composition},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247516,
author = {He, Hao and Wang, Haixun and Yang, Jun and Yu, Philip S.},
title = {BLINKS: Ranked Keyword Searches on Graphs},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247516},
doi = {10.1145/1247480.1247516},
abstract = {Query processing over graph-structured data is enjoying a growing number of applications. A top-k keyword search query on a graph finds the top k answers according to some ranking criteria, where each answer is a substructure of the graph containing all query keywords. Current techniques for supporting such queries on general graphs suffer from several drawbacks, e.g., poor worst-case performance, not taking full advantage of indexes, and high memory requirements. To address these problems, we propose BLINKS, a bi-level indexing and query processing scheme for top-k keyword search on graphs. BLINKS follows a search strategy with provable performance bounds, while additionally exploiting a bi-level index for pruning and accelerating the search. To reduce the index space, BLINKS partitions a data graph into blocks: The bi-level index stores summary information at the block level to initiate and guide search among blocks, and more detailed information for each block to accelerate search within blocks. Our experiments show that BLINKS offers orders-of-magnitude performance improvement over existing approaches.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {305–316},
numpages = {12},
keywords = {graphs, keyword search, ranking, indexing},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247517,
author = {Georgiadis, Haris and Vassalos, Vasilis},
title = {Xpath on Steroids: Exploiting Relational Engines for Xpath Performance},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247517},
doi = {10.1145/1247480.1247517},
abstract = {A lot of research has been conducted by the database community on methods and techniques for efficient XPath processing, with great success. Despite the progress made, significant opportunities for optimization of XPath still exist. One key to further improvements is to utilize more effectively existing facilities of relational RDBSes for the processing of XPath queries. After taking a comprehensive look at such facilities, we present techniques for XPath processing that work by identifying the best relational join algorithms, indices and file organization strategies for XPath queries. Our techniques both reduce the latency of the resulting SQL translations and guarantee their pipelined execution. We also propose a new technique for XML reconstruction from relations-mapped XML that "splits the difference" between schema-aware and schema-oblivious XML-to-relational mapping for a significant performance improvement. An extensive experimental study confirms the performance benefits of our optimization techniques and shows that a system implementing these techniques on top of a commercial RDBMS is competitive with respect to query performance with other native and relational-based state-of-the-art XPath processing systems, commercial as well as research prototypes.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {317–328},
numpages = {12},
keywords = {indices, relational databases, XML, XPath, structural joins, dewey encoding, XML reconstruction, schema mapping},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247518,
author = {Liu, Ziyang and Chen, Yi},
title = {Identifying Meaningful Return Information for XML Keyword Search},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247518},
doi = {10.1145/1247480.1247518},
abstract = {Keyword search enables web users to easily access XML data without the need to learn a structured query language and to study possibly complex data schemas. Existing work has addressed the problem of selecting qualified data nodes that match keywords and connecting them in a meaningful way, in the spirit of inferring a where clause in XQuery. However, how to infer the return clause for keyword search is an open problem.To address this challenge, we present an XML keyword search engine, XSeek, to infer the semantics of the search and identify return nodes effectively. XSeek recognizes possible entities and attributes inherently represented in the data. It also distinguishes between search predicates and return specifications in the keywords. Then based on the analysis of both XML data structures and keyword match patterns, XSeek generates return nodes. Extensive experimental studies show the effectiveness of XSeek.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {329–340},
numpages = {12},
keywords = {XML, keyword search},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245754,
author = {Shasha, Dennis},
title = {Session Details: Benchmarking and Performance Evaluation},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245754},
doi = {10.1145/3245754},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247520,
author = {Binnig, Carsten and Kossmann, Donald and Lo, Eric and \"{O}zsu, M. Tamer},
title = {QAGen: Generating Query-Aware Test Databases},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247520},
doi = {10.1145/1247480.1247520},
abstract = {Today, a common methodology for testing a database management system (DBMS) is to generate a set of test databases and then execute queries on top of them. However, for DBMS testing, it would be a big advantage if we can control the input and/or the output (e.g., the cardinality) of each individual operator of a test query for a particular test case. Unfortunately, current database generators generate databases independent of queries. As a result, it is hard to guarantee that executing the test query on the generated test databases can obtain the desired (intermediate) query results that match the test case. In this paper, we propose a novel way for DBMS testing. Instead of first generating a test database and then seeing how well it matches a particular test case (or otherwise use a trial-and-error approach to generate another test database), we propose to generate a query-aware database for each test case. To that end, we designed a query-aware test database generator called QAGen. In addition to the database schema and the set of basic constraints defined on the base tables, QAGen takes the query and the set of constraints defined on the query as input, and generates a query-aware test database as output. The generated database guarantees that the test query can get the desired (intermediate) query results as defined in the test case. This approach of testing facilitates a wide range of DBMS testing tasks such as testing of memory managers and testing the cardinality estimation components of query optimizers.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {341–352},
numpages = {12},
keywords = {testing, symbolic query processing, symbolic execution, symbolic database, query processing, database},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247521,
author = {Chandel, Amit and Hassanzadeh, Oktie and Koudas, Nick and Sadoghi, Mohammad and Srivastava, Divesh},
title = {Benchmarking Declarative Approximate Selection Predicates},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247521},
doi = {10.1145/1247480.1247521},
abstract = {Declarative data quality has been an active research topic. The fundamental principle behind a declarative approach to data quality is the use of declarative statements to realize data quality primitives on top of any relational data source. A primary advantage of such an approach is the ease of use and integration with existing applications. Over the last few years several similarity predicates have been proposed for common quality primitives (approximate selections, joins, etc) and have been fully expressed using declarative SQL statements. In this paper we propose new similarity predicates along with their declarative realization, based on notions of probabilistic information retrieval. In particular we show how language models and hidden Markov models can be utilized as similarity predicates for data quality and present their full declarative instantiation. We also show how other scoring methods from information retrieval, can be utilized in a similar setting. We then present full declarative specifications of previously proposed similarity predicates in the literature, grouping them into classes according to their primary characteristics. Finally, we present a thorough performance and accuracy study comparing a large number of similarity predicates for data cleaning operations. We quantify both their runtime performance as well as their accuracy for several types of common quality problems encountered in operational databases.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {353–364},
numpages = {12},
keywords = {declarative data quality, performance, SQL, accuracy, data cleaning},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247522,
author = {Rivoire, Suzanne and Shah, Mehul A. and Ranganathan, Parthasarathy and Kozyrakis, Christos},
title = {JouleSort: A Balanced Energy-Efficiency Benchmark},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247522},
doi = {10.1145/1247480.1247522},
abstract = {The energy efficiency of computer systems is an important concern in a variety of contexts. In data centers, reducing energy use improves operating cost, scalability, reliability, and other factors. For mobile devices, energy consumption directly affects functionality and usability. We propose and motivate JouleSort, an external sort benchmark, for evaluating the energy efficiency of a wide range of computer systems from clusters to handhelds. We list the criteria, challenges, and pitfalls from our experience in creating a fair energy-efficiency benchmark. Using a commercial sort, we demonstrate a JouleSort system that is over 3.5x as energy-efficient as last year's estimated winner. This system is quite different from those currently used in data centers. It consists of a commodity mobile CPU and 13 laptop drives connected by server-style I/O interfaces.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {365–376},
numpages = {12},
keywords = {benchmark, energy-efficiency, power, sort, servers},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247523,
author = {Ozmen, Oguzhan and Salem, Kenneth and Uysal, Mustafa and Attar, M. Hossein Sheikh},
title = {Storage Workload Estimation for Database Management Systems},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247523},
doi = {10.1145/1247480.1247523},
abstract = {Modern storage systems are sophisticated. Simple direct-attached storage devices are giving way to storage systems that are shared, flexible, virtualized and network-attached. Today, storage systems have their own administrators, who use specialized tools and expertise to configure and manage storage resources. Although the separation of storage management and database management has many advantages, it also introduces problems. Database physical design and storage configuration are closely related tasks, and the separation makes it more difficult to achieve a good end-to-end design. In this paper, we attempt to close this gap by addressing the problem of predicting the storage workload that will be generated by a database management system. Specifically, we show how to translate a database workload description, together with a database physical design, into a characterization of the storage workload that will result. Such a characterization can be used by a storage administrator to guide storage configuration. The ultimate goal of this work is to enable effective end-to-end design and configuration spanning both the database and storage system tiers. We present an empirical assessment of the cost of workload prediction as well as the accuracy of the result.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {377–388},
numpages = {12},
keywords = {storage management, storage configuration, database management systems, workload characterization},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245755,
author = {Larson, Paul},
title = {Session Details: Storage Engine and Access Methods},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245755},
doi = {10.1145/3245755},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247525,
author = {Holloway, Allison L. and Raman, Vijayshankar and Swart, Garret and DeWitt, David J.},
title = {How to Barter Bits for Chronons: Compression and Bandwidth Trade Offs for Database Scans},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247525},
doi = {10.1145/1247480.1247525},
abstract = {Two trends are converging to make the CPU cost of a table scan a more important component of database performance. First, table scans are becoming a larger fraction of the query processing workload, and second, large memories and compression are making table scans CPU, rather than disk bandwidth, bound. Data warehouse systems have found that they can avoid the unpredictability of joins and indexing and achieve good performance by using massive parallel processing to perform scans over compressed vertical partitions of a denormalized schema.In this paper we present a study of how to make such scans faster by the use of a scan code generator that produces code tuned to the database schema, the compression dictionaries, the queries being evaluated and the target CPU architecture. We investigate a variety of compression formats and propose two novel optimizations: tuple length quantization and a field length lookup table, for efficiently processing variable length fields and tuples. We present a detailed experimental study of the performance of generated scans against these compression formats, and use this to explore the trade off between compression quality and scan speed. We also introduce new strategies for removing instruction-level dependencies and increasing instruction-level parallelism, allowing for greater exploitation of multi-issue processors.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {389–400},
numpages = {12},
keywords = {Huffman coding, bandwidth trade offs, difference coding, compression},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247526,
author = {Srivastava, Divesh and Velegrakis, Yannis},
title = {Intensional Associations between Data and Metadata},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247526},
doi = {10.1145/1247480.1247526},
abstract = {There is a growing need to associate a variety of metadata with the underlying data, but a simple, elegant approach to uniformly model and query both the data and the metadata has been elusive. In this paper, we argue that (1) the relational model augmented with queries as data values is a natural way to uniformly model data, arbitrary metadata and their associations, and (2) relational queries with a join mechanism augmented to permit matching of query result relations, instead of only atomic values, is an elegant way to uniformly query across data and metadata. We describe the architecture of a system we have prototyped for this purpose, demonstrate the generality of our approach and evaluate the performance of the system, in comparison with previous proposals for metadata management.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {401–412},
numpages = {12},
keywords = {annotations, queries as data, metadata management, intensional associations},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247527,
author = {Idreos, Stratos and Kersten, Martin L. and Manegold, Stefan},
title = {Updating a Cracked Database},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247527},
doi = {10.1145/1247480.1247527},
abstract = {A cracked database is a datastore continuously reorganized based on operations being executed. For each query, the data of interest is physically reclustered to speed-up future access to the same, overlapping or even disjoint data. This way, a cracking DBMS self-organizes and adapts itself to the workload.So far, cracking has been considered for static databases only. In this paper, we introduce several novel algorithms for high-volume insertions, deletions and updates against a cracked database. We show that the nice performance properties of a cracked database can be maintained in a dynamic environment where updates interleave with queries. Our algorithms comply with the cracking philosophy, i.e., a table is informed on pending insertions and deletions, but only when the relevant data is needed for query processing just enough pending update actions are applied.We discuss details of our implementation in the context of an open-source DBMS and we show through a detailed experimental evaluation that our algorithms always manage to keep the cost of querying a cracked datastore with pending updates lower than the non-cracked case.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {413–424},
numpages = {12},
keywords = {self-organization, updates, database cracking},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247528,
author = {Wang, Rui and Salzberg, Betty and Lomet, David},
title = {Log-Based Recovery for Middleware Servers},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247528},
doi = {10.1145/1247480.1247528},
abstract = {We have developed new methods for log-based recovery for middleware servers which involve thread pooling, private in-memory states for clients, shared in-memory state and message interactions among middleware servers. Due to the observed rareness of crashes, relatively small size of shared state and infrequency of shared state read/write accesses, we are able to reduce the overhead of message logging and shared state logging while maintaining recovery independence. Checkpointing has a very small impact on ongoing activities while still reducing recovery time. Our recovery mechanism enables client private states to be recovered in parallel after a crash. On a commercial middleware server platform, we have implemented a recovery infrastructure prototype, which demonstrates the manageability of system complexity and shows promising performance results.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {425–436},
numpages = {12},
keywords = {distributed systems, exactly-once execution, recovery, application fault tolerance, optimistic logging},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245756,
author = {Tung, Anthony},
title = {Session Details: Data Cleaning and Integration},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245756},
doi = {10.1145/3245756},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247530,
author = {Chaudhuri, Surajit and Das Sarma, Anish and Ganti, Venkatesh and Kaushik, Raghav},
title = {Leveraging Aggregate Constraints for Deduplication},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247530},
doi = {10.1145/1247480.1247530},
abstract = {We show that aggregate constraints (as opposed to pairwise constraints) that often arise when integrating multiple sources of data, can be leveraged to enhance the quality of deduplication. However, despite its appeal, we show that the problem is challenging, both semantically and computationally. We define a restricted search space for deduplication that is intuitive in our context and we solve the problem optimally for the restricted space. Our experiments on real data show that incorporating aggregate constraints significantly enhances the accuracy of deduplication.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {437–448},
numpages = {12},
keywords = {entity resolution, deduplication, constraint satisfaction},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247531,
author = {Udrea, Octavian and Getoor, Lise and Miller, Ren\'{e}e J.},
title = {Leveraging Data and Structure in Ontology Integration},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247531},
doi = {10.1145/1247480.1247531},
abstract = {There is a great deal of research on ontology integration which makes use of rich logical constraints to reason about the structural and logical alignment of ontologies. There is also considerable work on matching data instances from heterogeneous schema or ontologies. However, little work exploits the fact that ontologies include both data and structure. We aim to close this gap by presenting a new algorithm (ILIADS) that tightly integrates both data matching and logical reasoning to achieve better matching of ontologies. We evaluate our algorithm on a set of 30 pairs of OWL Lite ontologies with the schema and data matchings found by human reviewers. We compare against two systems - the ontology matching tool FCA-merge [28] and the schema matching tool COMA++ [1]. ILIADS shows an average improvement of 25% in quality over FCA-merge and a 11% improvement in recall over COMA++.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {449–460},
numpages = {12},
keywords = {statistical inference, data integration, logical inference, ontology alignment, schema mapping},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247532,
author = {Melnik, Sergey and Adya, Atul and Bernstein, Philip A.},
title = {Compiling Mappings to Bridge Applications and Databases},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247532},
doi = {10.1145/1247480.1247532},
abstract = {Translating data and data access operations between applications and databases is a longstanding data management problem. We present a novel approach to this problem, in which the relationship between the application data and the persistent storage is specified using a declarative mapping, which is compiled into bidirectional views that drive the data transformation engine. Expressing the application model as a view on the database is used to answer queries, while viewing the database in terms of the application model allows us to leverage view maintenance algorithms for update translation. This approach has been implemented in a commercial product. It enables developers to interact with a relational database via a conceptual schema and an object oriented programming surface. We outline the implemented system and focus on the challenges of mapping compilation, which include rewriting queries under constraints and supporting non-relational constructs.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {461–472},
numpages = {12},
keywords = {query rewriting, updateable views, mapping},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247533,
author = {Wyss, Catharine M. and Wyss, Felix I.},
title = {Extending Relational Query Optimization to Dynamic Schemas for Information Integration in Multidatabases},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247533},
doi = {10.1145/1247480.1247533},
abstract = {This paper extends relational processing and optimization to the FISQL/FIRA languages for dynamic schema queries over multidatabases. Dynamic schema queries involve the creation and restructuring of metadata at runtime. We present a full implementation of a FISQL/FIRA engine, which includes subqueries and all transformational capabilities of FISQL/FIRA on distributed, multidatabase platforms. An important application of the system is to enhance traditional information architectures by enabling the creation and maintenance of dynamic wrappers and mapping queries at source databases within GAV, LAV, GLAV, peer-to-peer, or other integration frameworks. In addition to fully supporting FISQL/FIRA on multidatabases, our implementation introduces a bi-level optimization paradigm where purely relational sub-fragments of queries are pushed into source engines. This paradigm shares features of canonical distributed database processing, but has a new dimension through the extension of the relational model to dynamic schemas. We present empirical results showing the feasibility of optimization in this context, and discuss tradeoffs involved. Our system is the first to extend relational databases with these capabilities on this scale.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {473–484},
numpages = {12},
keywords = {federated data model, metadata integration, metadata query processing, FIRA, federated databases, FISQL, federated interoperable SQL, federated interoperable RA, data integration, dynamic schema query processing},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245757,
author = {Naughton, Jeffrey},
title = {Session Details: Distributed Data Management},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245757},
doi = {10.1145/3245757},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247535,
author = {Huebsch, Ryan and Garofalakis, Minos and Hellerstein, Joseph M. and Stoica, Ion},
title = {Sharing Aggregate Computation for Distributed Queries},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247535},
doi = {10.1145/1247480.1247535},
abstract = {An emerging challenge in modern distributed querying is to efficiently process multiple continuous aggregation queries simultaneously. Processing each query independently may be infeasible, so multi-query optimizations are critical for sharing work across queries. The challenge is to identify overlapping computations that may not be obvious in the queries themselves.In this paper, we reveal new opportunities for sharing work in the context of distributed aggregation queries that vary in their selection predicates. We identify settings in which a large set of q such queries can be answered by executing k &lt;&lt; q different queries. The k queries are revealed by analyzing a boolean matrix capturing the connection between data and the queries that they satisfy, in a manner akin to familiar techniques like Gaussian elimination. Indeed, we identify a class of linear aggregate functions (including SUM, COUNT and AVERAGE), and show that the sharing potential for such queries can be optimally recovered using standard matrix decompositions from computational linear algebra. For some other typical aggregation functions (including MIN and MAX) we find that optimal sharing maps to the NP-hard set basis problem. However, for those scenarios, we present a family of heuristic algorithms and demonstrate that they perform well for moderate-sized matrices. We also present a dynamic distributed system architecture to exploit sharing opportunities, and experimentally evaluate the benefits of our techniques via a novel, flexible random workload generator we develop for this setting.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {485–496},
numpages = {12},
keywords = {linear algebra, multi-query optimization, aggregation, duplicate insensitive},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247536,
author = {Luo, Gang and Tang, Chunqiang and Yu, Philip S.},
title = {Resource-Adaptive Real-Time New Event Detection},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247536},
doi = {10.1145/1247480.1247536},
abstract = {In a document streaming environment, online detection of the first documents that mention previously unseen events is an open challenge. For this online new event detection (ONED) task, existing studies usually assume that enough resources are always available and focus entirely on detection accuracy without considering efficiency. Moreover, none of the existing work addresses the issue of providing an effective and friendly user interface. As a result, there is a significant gap between the existing systems and a system that can be used in practice. In this paper, we propose an ONED framework with the following prominent features. First, a combination of indexing and compression methods is used to improve the document processing rate by orders of magnitude without sacrificing much detection accuracy. Second, when resources are tight, a resource-adaptive computation method is used to maximize the benefit that can be gained from the limited resources. Third, when the new event arrival rate is beyond the processing capability of the consumer of the ONED system, new events are further filtered and prioritized before they are presented to the consumer. Fourth, implicit citation relationships are created among all the documents and used to compute the importance of document sources. This importance information can guide the selection of document sources. We implemented a prototype of our framework on top of IBM's Stream Processing Core middleware. We also evaluated the effectiveness of our techniques on the standard TDT5 benchmark. To the best of our knowledge, this is the first implementation of a real application in a large-scale stream processing system.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {497–508},
numpages = {12},
keywords = {document streaming, online new event detection},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247537,
author = {Cong, Gao and Fan, Wenfei and Kementsietsidis, Anastasios},
title = {Distributed Query Evaluation with Performance Guarantees},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247537},
doi = {10.1145/1247480.1247537},
abstract = {Partial evaluation has recently proven an effective technique for evaluating Boolean XPath queries over a fragmented tree that is distributed over a number of sites. What left open is whether or not the technique is applicable to generic data-selecting XPath queries. In contrast to Boolean queries that return a single truth value, a generic XPath query returns a set of elements, and its evaluation introduces difficulties to avoiding excessive data shipping. This paper settles this question in positive by providing evaluation algorithms and optimizations for generic XPath queries in the same distributed and fragmented setting. These algorithms explore parallelism and retain the performance guarantees of their counterpart for Boolean queries, regardless of how the tree is fragmented and distributed. First, each site is visited at most three times, and down to at most twice when optimizations are in place. Second, the network traffic is determined by the final answer of the query, rather than the size of the tree, without incurring unnecessary data shipping. Third, the total computation is comparable to that of centralized algorithms on the tree stored in a single site. We show both analytically and experimentally that our algorithms and optimizations are scalable and efficient on large trees and complex XPath queries.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {509–520},
numpages = {12},
keywords = {parallel query processing, distributed XML documents, Xpath queries},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247538,
author = {Yang, Xiaoyan and Lim, Hock Beng and \"{O}zsu, Tamer M. and Tan, Kian Lee},
title = {In-Network Execution of Monitoring Queries in Sensor Networks},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247538},
doi = {10.1145/1247480.1247538},
abstract = {Sensor networks are widely used in many applications for collecting information from the physical environment. In these applications, it is usually necessary to track the relationships between sensor data readings within a time window to detect events of interest. However, it is difficult to detect such events by using the common aggregate or selection queries. We address the problem of processing window self-join in order to detect events of interest. Self-joins are useful in tracking correlations between different sensor readings, which can indicate an event of interest. We propose the Two-Phase Self-Join (TPSJ) scheme to efficiently evaluate self-join queries for event detection in sensor networks. Our TPSJ scheme takes advantage of the properties of the events and carries out data filtering during in-network processing. We discuss TPSJ execution with one window and we extend it for continuous event monitoring. Our experimental evaluation results indicate that the TPSJ scheme is effective in reducing the amount of radio transmissions during event detection.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {521–532},
numpages = {12},
keywords = {sensor networks, self-join queries},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245758,
author = {Tan, Kian-Lee},
title = {Session Details: Query Processing},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245758},
doi = {10.1145/3245758},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247540,
author = {Zhou, Jingren and Larson, Per-Ake and Freytag, Johann-Christoph and Lehner, Wolfgang},
title = {Efficient Exploitation of Similar Subexpressions for Query Processing},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247540},
doi = {10.1145/1247480.1247540},
abstract = {Complex queries often contain common or similar subexpressions, either within a single query or among multiple queries submitted as a batch. If so, query execution time can be improved by evaluating a common subexpression once and reusing the result in multiple places. However, current query optimizers do not recognize and exploit similar subexpressions, even within the same query.We present an efficient, scalable, and principled solution to this long-standing optimization problem. We introduce a light-weight and effective mechanism to detect potential sharing opportunities among expressions. Candidate covering subexpressions are constructed and optimization is resumed to determine which, if any, such subexpressions to include in the final query plan. The chosen subexpression(s) are computed only once and the results are reused to answer other parts of queries. Our solution automatically applies to optimization of query batches, nested queries, and maintenance of multiple materialized views. It is the first comprehensive solution covering all aspects of the problem: detection, construction, and cost-based optimization. Experiments on Microsoft SQL Server show significant performance improvements with minimal overhead.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {533–544},
numpages = {12},
keywords = {query processing, similar subexpressions, query optimization},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247541,
author = {Zhou, Xuan and Gaugaz, Julien and Balke, Wolf-Tilo and Nejdl, Wolfgang},
title = {Query Relaxation Using Malleable Schemas},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247541},
doi = {10.1145/1247480.1247541},
abstract = {In contrast to classical databases and IR systems, real-world information systems have to deal increasingly with very vague and diverse structures for information management and storage that cannot be adequately handled yet. While current object-relational database systems require clear and unified data schemas, IR systems usually ignore the structured information completely. Malleable schemas, as recently introduced, provide a novel way to deal with vagueness, ambiguity and diversity by incorporating imprecise and overlapping definitions of data structures. In this paper, we propose a novel query relaxation scheme that enables users to find best matching information by exploiting malleable schemas to effectively query vaguely structured information. Our scheme utilizes duplicates in differently described data sets to discover the correlations within a malleable schema, and then uses these correlations to appropriately relax the users' queries. In addition, it ranks results of the relaxed query according to their respective probability of satisfying the original query's intent. We have implemented the scheme and conducted extensive experiments with real-world data to confirm its performance and practicality.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {545–556},
numpages = {12},
keywords = {query relaxation, malleable schema},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247542,
author = {Chandramouli, Badrish and Bond, Christopher N. and Babu, Shivnath and Yang, Jun},
title = {Query Suspend and Resume},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247542},
doi = {10.1145/1247480.1247542},
abstract = {Suppose a long-running analytical query is executing on a database server and has been allocated a large amount of physical memory. A high-priority task comes in and we need to run it immediately with all available resources. We have several choices. We could swap out the old query to disk, but writing out a large execution state may take too much time. Another option is to terminate the old query and restart it after the new task completes, but we would waste all the work already performed by the old query. Yet another alternative is to periodically checkpoint the query during execution, but traditional synchronous checkpointing carries high overhead. In this paper, we advocate a database-centric approach to implementing query suspension and resumption, with negligible execution overhead, bounded suspension cost, and efficient resumption. The basic idea is to let each physical query operator perform lightweight checkpointing according to its own semantics, and coordinate asynchronous checkpoints among operators through a novel contracting mechanism. At the time of suspension, we find an optimized suspend plan for the query, which may involve a combination of dumping current state to disk and going back to previous checkpoints. The plan seeks to minimize the suspend/resume overhead while observing the constraint on suspension time. Our approach requires only small changes to the iterator interface, which we have implemented in the PREDATOR database system. Experiments with our implementation demonstrate significant advantages of our approach over traditional alternatives.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {557–568},
numpages = {12},
keywords = {query, suspend, processing, resume, optimization},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245759,
author = {\'{E}zsu, M. Tamer},
title = {Session Details: Spatio-Temporal Data Management},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245759},
doi = {10.1145/3245759},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247544,
author = {Morse, Michael D. and Patel, Jignesh M.},
title = {An Efficient and Accurate Method for Evaluating Time Series Similarity},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247544},
doi = {10.1145/1247480.1247544},
abstract = {A variety of techniques currently exist for measuring the similarity between time series datasets. Of these techniques, the methods whose matching criteria is bounded by a specified ε threshold value, such as the LCSS and the EDR techniques, have been shown to be robust in the presence of noise, time shifts, and data scaling. Our work proposes a new algorithm, called the Fast Time Series Evaluation (FTSE) method, which can be used to evaluate such threshold value techniques, including LCSS and EDR. Using FTSE, we show that these techniques can be evaluated faster than using either traditional dynamic programming or even warp-restricting methods such as the Sakoe-Chiba band and the Itakura Parallelogram.We also show that FTSE can be used in a framework that can evaluate a richer range of ε threshold-based scoring techniques, of which EDR and LCSS are just two examples. This framework, called Swale, extends the ε threshold-based scoring techniques to include arbitrary match rewards and gap penalties. Through extensive empirical evaluation, we show that Swale can obtain greater accuracy than existing methods.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {569–580},
numpages = {12},
keywords = {clustering, trajectory similarity, time series},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247545,
author = {Xu, Zhengdao and Jacobsen, Arno},
title = {Adaptive Location Constraint Processing},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247545},
doi = {10.1145/1247480.1247545},
abstract = {An important problem for many location-based applications is the continuous evaluation of proximity relations among moving objects. These relations express whether a given set of objects is in a spatial constellation or in a spatial constellation relative to a given point of demarcation in the environment. We represent proximity relations as location constraints, which resemble standing queries over continuously changing location position information. The challenge lies in the continuous processing of large numbers of location constraints as the location of objects and the constraint load change. In this paper, we propose an adaptive location constraint indexing approach which adapts as the constraint load and movement pattern of the objects change. The approach takes correlations between constraints into account to further reduce processing time. We also introduce a new location update policy that detects constraint matches with fewer location update requests. Our approach stabilizes system performance, avoids oscillation, reduces constraint matching time by 70% for in-memory processing, and reduces secondary storage accesses by 80% for I/O-incurring environments.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {581–592},
numpages = {12},
keywords = {standing query, moving object indexing, location query, location-based services, constraint matching, location constraint processing, adaptive indexing, location update policy, continuous location query},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247546,
author = {Lee, Jae-Gil and Han, Jiawei and Whang, Kyu-Young},
title = {Trajectory Clustering: A Partition-and-Group Framework},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247546},
doi = {10.1145/1247480.1247546},
abstract = {Existing trajectory clustering algorithms group similar trajectories as a whole, thus discovering common trajectories. Our key observation is that clustering trajectories as a whole could miss common sub-trajectories. Discovering common sub-trajectories is very useful in many applications, especially if we have regions of special interest for analysis. In this paper, we propose a new partition-and-group framework for clustering trajectories, which partitions a trajectory into a set of line segments, and then, groups similar line segments together into a cluster. The primary advantage of this framework is to discover common sub-trajectories from a trajectory database. Based on this partition-and-group framework, we develop a trajectory clustering algorithm TRACLUS. Our algorithm consists of two phases: partitioning and grouping. For the first phase, we present a formal trajectory partitioning algorithm using the minimum description length(MDL) principle. For the second phase, we present a density-based line-segment clustering algorithm. Experimental results demonstrate that TRACLUS correctly discovers common sub-trajectories from real trajectory data.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {593–604},
numpages = {12},
keywords = {MDL principle, trajectory clustering, density-based clustering, partition-and-group framework},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245760,
author = {Sellis, Timos},
title = {Session Details: Search},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245760},
doi = {10.1145/3245760},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247548,
author = {Markowetz, Alexander and Yang, Yin and Papadias, Dimitris},
title = {Keyword Search on Relational Data Streams},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247548},
doi = {10.1145/1247480.1247548},
abstract = {Increasing monitoring of transactions, environmental parameters, homeland security, RFID chips and interactions of online users rapidly establishes new data sources and application scenarios. In this paper, we propose keyword search on relational data streams (S-KWS) as an effective way for querying in such intricate and dynamic environments. Compared to conventional query methods, S-KWS has several benefits. First, it allows search for combinations of interesting terms without a-priori knowledge of the data streams in which they appear. Second, it hides the schema from the user and allows it to change, without the need for query re-writing. Finally, keyword queries are easy to express.Our contributions are summarized as follows. (i) We provide formal semantics for S-KWS, addressing the temporal validity and order of results. (ii) We propose an efficient algorithm for generating operator trees, applicable to arbitrary schemas. (iii) We integrate these trees into an operator mesh that shares common expressions. (iv) We develop techniques that utilize the operator mesh for efficient query processing. The techniques adapt dynamically to changes in the schema and input characteristics. Finally, (v) we present methods for purging expired tuples, minimizing either CPU, or memory requirements.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {605–616},
numpages = {12},
keywords = {keyword search, data streams},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247549,
author = {Wu, Ping and Sismanis, Yannis and Reinwald, Berthold},
title = {Towards Keyword-Driven Analytical Processing},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247549},
doi = {10.1145/1247480.1247549},
abstract = {Gaining business insights from data has recently been the focus of research and product development. On Line-Analytical Processing (OLAP) tools provide elaborate query languages that allow users to group and aggregate data in various ways, and explore interesting trends and patterns in the data. However, the dynamic nature of today's data along with the overwhelming detail at which data is provided, make it nearly impossible to organize the data in a way that a business analyst needs for thinking about the data. In this paper, we introduce "Keyword-Driven Analytical Processing" (KDAP), which combines intuitive keyword-based search with the power of aggregation in OLAP without having to spend considerable effort in organizing the data in terms that the business analyst understands. Our design point is around a user mentality that we frequently encounter: "users don't know how to specify what they want, but they know it when they see it". We present our complete solution framework, which implements various phases from disambiguating the keyword terms to organizing and ranking the results in dynamic facets, that allow the user to explore efficiently the aggregation space. We address specific issues that analysts encounter, like joins, groupings and aggregations, and we provide efficient and scalable solutions. We show, how KDAP can handle both categorical and numerical data equally well and, finally, we demonstrate the generality and applicability of KDAP to two different aspects of OLAP, namely, finding exceptions or surprises in the data and finding bellwether regions where local aggregates are highly correlated with global aggregates, using various experiments on real data.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {617–628},
numpages = {12},
keywords = {aggregation, ranking, keyword search, data warehouse, OLAP},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247550,
author = {Dasgupta, Arjun and Das, Gautam and Mannila, Heikki},
title = {A Random Walk Approach to Sampling Hidden Databases},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247550},
doi = {10.1145/1247480.1247550},
abstract = {A large part of the data on the World Wide Web is hidden behind form-like interfaces. These interfaces interact with a hidden back-end database to provide answers to user queries. Generating a uniform random sample of this hidden database by using only the publicly available interface gives us access to the underlying data distribution. In this paper, we propose a random walk scheme over the query space provided by the interface to sample such databases. We discuss variants where the query space is visualized as a fixed and random ordering of attributes. We also propose techniques to further improve the sample quality by using a probabilistic rejection based approach. We conduct extensive experiments to illustrate the accuracy and efficiency of our techniques.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {629–640},
numpages = {12},
keywords = {hidden databases, random walk, sampling, top-k interfaces},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247551,
author = {Chen, Zhiyuan and Li, Tao},
title = {Addressing Diverse User Preferences in SQL-Query-Result Navigation},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247551},
doi = {10.1145/1247480.1247551},
abstract = {Database queries are often exploratory and users often find their queries return too many answers, many of them irrelevant. Existing work either categorizes or ranks the results to help users locate interesting results. The success of both approaches depends on the utilization of user preferences. However, most existing work assumes that all users have the same user preferences, but in real life different users often have different preferences. This paper proposes a two-step solution to address the diversity issue of user preferences for the categorization approach. The proposed solution does not require explicit user involvement. The first step analyzes query history of all users in the system offline and generates a set of clusters over the data, each corresponding to one type of user preferences. When user asks a query, the second step presents to the user a navigational tree over clusters generated in the first step such that the user can easily select the subset of clusters matching his needs. The user then can browse, rank, or categorize the results in selected clusters. The navigational tree is automatically constructed using a cost-based algorithm which considers the cost of visiting both intermediate nodes and leaf nodes in the tree. An empirical study demonstrates the benefits of our approach.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {641–652},
numpages = {12},
keywords = {data exploration, user preferences},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245761,
author = {Freytag, Johann-Christoph},
title = {Session Details: Database Sharing and Privacy},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245761},
doi = {10.1145/3245761},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247553,
author = {Scannapieco, Monica and Figotin, Ilya and Bertino, Elisa and Elmagarmid, Ahmed K.},
title = {Privacy Preserving Schema and Data Matching},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247553},
doi = {10.1145/1247480.1247553},
abstract = {In many business scenarios, record matching is performed across different data sources with the aim of identifying common information shared among these sources. However such need is often in contrast with privacy requirements concerning the data stored by the sources. In this paper, we propose a protocol for record matching that preserves privacy both at the data level and at the schema level. Specifically, if two sources need to identify their common data, by running the protocol they can compute the matching of their datasets without sharing their data in clear and only sharing the result of the matching. The protocol uses a third party, and maps records into a vector space in order to preserve their privacy. Experimental results show the efficiency of the matching protocol in terms of precision and recall as well as the good computational performance.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {653–664},
numpages = {12},
keywords = {record matching, privacy},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247554,
author = {Nergiz, Mehmet Ercan and Atzori, Maurizio and Clifton, Chris},
title = {Hiding the Presence of Individuals from Shared Databases},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247554},
doi = {10.1145/1247480.1247554},
abstract = {Advances in information technology, and its use in research, are increasing both the need for anonymized data and the risks of poor anonymization. We present a metric, δ-presence, that clearly links the quality of anonymization to the risk posed by inadequate anonymization. We show that existing anonymization techniques are inappropriate for situations where δ-presence is a good metric (specifically, where knowing an individual is in the database poses a privacy risk), and present algorithms for effectively anonymizing to meet δ-presence. The algorithms are evaluated in the context of a real-world scenario, demonstrating practical applicability of the approach.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {665–676},
numpages = {12},
keywords = {medical databases, delta presence, privacy, k-anonymity},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247555,
author = {Anciaux, Nicolas and Benzine, Mehdi and Bouganim, Luc and Pucheral, Philippe and Shasha, Dennis},
title = {GhostDB: Querying Visible and Hidden Data without Leaks},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247555},
doi = {10.1145/1247480.1247555},
abstract = {Imagine that you have been entrusted with private data, such as corporate product information, sensitive government information, or symptom and treatment information about hospital patients. You may want to issue queries whose result will combine private and public data, but private data must not be revealed. GhostDB is an architecture and system to achieve this. You carry private data in a smart USB key (a large Flash persistent store combined with a tamper and snoop-resistant CPU and small RAM). When the key is plugged in, you can issue queries that link private and public data and be sure that the only information revealed to a potential spy is which queries you pose. Queries linking public and private data entail novel distributed processing techniques on extremely unequal devices (standard computer and smart USB key). This paper presents the basic framework to make this all work intuitively and efficiently.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {677–688},
numpages = {12},
keywords = {privacy, secure device, storage model},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247556,
author = {Xiao, Xiaokui and Tao, Yufei},
title = {M-Invariance: Towards Privacy Preserving Re-Publication of Dynamic Datasets},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247556},
doi = {10.1145/1247480.1247556},
abstract = {The previous literature of privacy preserving data publication has focused on performing "one-time" releases. Specifically, none of the existing solutions supports re-publication of the microdata, after it has been updated with insertions <u>and</u> deletions. This is a serious drawback, because currently a publisher cannot provide researchers with the most recent dataset continuously.This paper remedies the drawback. First, we reveal the characteristics of the re-publication problem that invalidate the conventional approaches leveraging k-anonymity and l-diversity. Based on rigorous theoretical analysis, we develop a new generalization principle m-invariance that effectively limits the risk of privacy disclosure in re-publication. We accompany the principle with an algorithm, which computes privacy-guarded relations that permit retrieval of accurate aggregate information about the original microdata. Our theoretical results are confirmed by extensive experiments with real data.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {689–700},
numpages = {12},
keywords = {privacy, m-invariance, generalization},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245762,
author = {Haas, Peter},
title = {Session Details: Approximate and Probabilistic Processing},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245762},
doi = {10.1145/3245762},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247558,
author = {Bravo, H\'{e}ctor Corrada and Ramakrishnan, Raghu},
title = {Optimizing Mpf Queries: Decision Support and Probabilistic Inference},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247558},
doi = {10.1145/1247480.1247558},
abstract = {Managing uncertain data using probabilistic frameworks has attracted much interest lately in the database literature, and a central computational challenge is probabilistic inference. This paper presents a broad class of aggregate queries, called MPF queries, inspired by the literature on probabilistic inference in statistics and machine learning. An MPF (Marginalize a Product Function) query is an aggregate query over a stylized join of several relations. In probabilistic inference, this join corresponds to taking the product of several probability distributions, while the aggregate operation corresponds to marginalization. Probabilistic inference can be expressed directly as MPF queries in a relational setting, and therefore, by optimizing evaluation of MPF queries, we provide scalable support for probabilistic inference in database systems. To optimize MPF queries, we build on ideas from database query optimization as well as traditional algorithms such as Variable Elimination and Belief Propagation from the probabilistic inference literature.Although our main motivation for introducing MPF queries is to support easy expression and efficient evaluation of probabilistic inference in a DBMS, we observe that this class of queries is very useful for a range of decision support tasks. We present and optimize MPF queries in a general form where arbitrary functions (i.e., other than probability distributions) are handled, and demonstrate their value for decision support applications through a number of illustrative and natural examples.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {701–712},
numpages = {12},
keywords = {aggregate queries, probabilistic inference},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247559,
author = {Antova, Lyublena and Koch, Christoph and Olteanu, Dan},
title = {From Complete to Incomplete Information and Back},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247559},
doi = {10.1145/1247480.1247559},
abstract = {Incomplete information arises naturally in numerous data management applications. Recently, several researchers have studied query processing in the context of incomplete information. Most work has combined the syntax of a traditional query language like relational algebra with a nonstandard semantics such as certain or ranked possible answers. There are now also languages with special features to deal with uncertainty. However, to the standards of the data management community, to date no language proposal has been made that can be considered a natural analog to SQL or relational algebra for the case of incomplete information.In this paper we propose such a language, World-set Algebra, which satisfies the robustness criteria and analogies to relational algebra that we expect. The language supports the contemplation on alternatives and can thus map from a complete database to an incomplete one comprising several possible worlds. We show that World-set Algebra is conservative over relational algebra in the sense that any query that maps from a complete database to a complete database (a complete-to-complete query) is equivalent to a relational algebra query. Moreover, we give an efficient algorithm for effecting this translation.We then study algebraic query optimization of such queries. We argue that query languages with explicit constructs for handling uncertainty allow for the more natural and simple expression of many real-world decision support queries. The results of this paper not only suggest a language for specifying queries in this way, but also allow for their efficient evaluation in any relational database management system.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {713–724},
numpages = {12},
keywords = {query rewriting, hypothetical queries, world-set algebra, use cases, incomplete information},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247560,
author = {Jermaine, Christopher and Arumugam, Subramanian and Pol, Abhijit and Dobra, Alin},
title = {Scalable Approximate Query Processing with the DBO Engine},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247560},
doi = {10.1145/1247480.1247560},
abstract = {This paper describes query processing in the DBO database system. Like other database systems designed for ad-hoc, analytic processing, DBO is able to compute the exact answer to queries over a large relational database in a scalable fashion. Unlike any other system designed for analytic processing, DBO can constantly maintain a guess as to the final answer to an aggregate query throughout execution, along with statistically meaningful bounds for the guess's accuracy. As DBO gathers more and more information, the guess gets more and more accurate, until it is 100% accurate as the query is completed. This allows users to stop the execution at any time that they are happy with the query accuracy, and encourages exploratory data analysis.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {725–736},
numpages = {12},
keywords = {sampling, DBO, online aggregation, randomized algorithms},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245763,
author = {Zhou, Xiaofang},
title = {Session Details: Publish-Subscribe Systems},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245763},
doi = {10.1145/3245763},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247562,
author = {Chan, Chee Yong and Ni, Yuan},
title = {Efficient Xml Data Dissemination with Piggybacking},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247562},
doi = {10.1145/1247480.1247562},
abstract = {Content-based dissemination of XML data using the publish-subscribe paradigm is an effective means to deliver relevant data to interested data consumers. To meet the performance challenges of content-based filtering and routing, two key optimizations have been developed: the use of efficient indexes to speed up subscription filtering, and the use of effective aggregation algorithms to reduce the number of subscriptions. The effectiveness of both these techniques are, however, limited to locally improving the performance of individual routers. In this paper, we propose a novel and holistic optimization approach that allows a downstream router to leverage the subscription matchings done by upstream routers to reduce its own filtering work. This is achieved by piggybacking useful annotations to the XML document being forwarded. We explore several design options and tradeoffs of this novel optimization approach. Our experimental results demonstrate that our piggyback optimization achieves significant performance improvement under various conditions.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {737–748},
numpages = {12},
keywords = {XPath, XML, pub/sub system, data dissemination, annotation, piggybacking},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247563,
author = {Milo, Tova and Zur, Tal and Verbin, Elad},
title = {Boosting Topic-Based Publish-Subscribe Systems with Dynamic Clustering},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247563},
doi = {10.1145/1247480.1247563},
abstract = {We consider in this paper a class of Publish-Subscribe (pub-sub) systems called topic-based systems, where users subscribe to topics and are notified on events that belong to those subscribed topics. With the recent flourishing of RSS news syndication, these systems are regaining popularity and are raising new challenging problems.In most of the modern topics-based systems, the events in each topic are delivered to the subscribers via a supporting, distributed, data structure (typically a multicast tree). Since peers in the network may come and go frequently, this supporting structure must be continuously maintained so that "holes" do not disrupt the events delivery. The dissemination of events in each topic thus incurs two main costs: (1) the actual transmission cost for the topic events,and (2) the maintenance cost for its supporting structure. This maintenance overhead becomes particularly dominating when a pub-sub system supports a large number of topics with moderate event frequency; a typical scenario in nowadays news syndication scene.The goal of this paper is to devise a method for reducing this maintenance overhead to the minimum. Our aim is not to invent yet another topic-based pub-sub system, but rather to develop a generic technique for better utilization of existing platforms. Our solution is based on a novel distributed clustering algorithm that utilizes correlations between user subscriptions to dynamically group topics together, into virtual topics (called topic-clusters), andt hereby unifies their supporting structures and reduces costs. Our technique continuously adapts the topic-clusters and the user subscriptions to the system state, and incurs only very minimal overhead. We have implemented our solution in the Tamara pub-sub system. Our experimental study shows this approach to be extremely effective, improving the performance by an order of magnitude.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {749–760},
numpages = {12},
keywords = {dynamic clustering, peer-to-peer, publish-subscribe},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247564,
author = {Hong, Mingsheng and Demers, Alan J. and Gehrke, Johannes E. and Koch, Christoph and Riedewald, Mirek and White, Walker M.},
title = {Massively Multi-Query Join Processing in Publish/Subscribe Systems},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247564},
doi = {10.1145/1247480.1247564},
abstract = {There has been much recent interest in XML publish/subscribe systems. Some systems scale to thousands of concurrent queries, but support a limited query language (usually a fragment of XPath 1.0). Other systems support more expressive languages, but do not scale well with the number of concurrent queries. In this paper, we propose a set of novel query processing techniques, referred to as Massively Multi-Query Join Processing techniques, for processing a large number of XML stream queries involving value joins over multiple XML streams and documents. These techniques enable the sharing of representations of inputs to multiple joins, and the sharing of join computation. Our techniques are also applicable to relational event processing systems and publish/subscribe systems that support join queries. We present experimental results to demonstrate the effectiveness of our techniques. We are able to process thousands of XML messages with hundreds of thousands of join queries on real RSS feed streams. Our techniques gain more than two orders of magnitude speedup compared to the naive approach of evaluating such join queries.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {761–772},
numpages = {12},
keywords = {XML join, multi-query optimization, stream query processing, publish/subscribe},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245764,
author = {Faloutsos, Christos},
title = {Session Details: Optimization},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245764},
doi = {10.1145/3245764},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247566,
author = {Raman, Vijayshankar and Qiao, Lin and Han, Wei and Narang, Inderpal and Chen, Ying-Lin and Yang, Kou-Horng and Ling, Fen-Ling},
title = {Lazy, Adaptive Rid-List Intersection, and Its Application to Index Anding},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247566},
doi = {10.1145/1247480.1247566},
abstract = {RID-List (row id list) intersection is a common strategy in query processing, used in star joins, column stores, and even search engines. To apply a conjunction of predicates on a table, a query process ordoes index lookups to form sorted RID-lists (or bitmap) of the rows matching each predicate, then intersects the RID-lists via an AND-tree, and finally fetches the corresponding rows to apply any residual predicates and aggregates.This process can be expensive when the RID-lists are large. Furthermore, the performance is sensitive to the order in which RID lists are intersected together, and to treating the right predicates as residuals. If the optimizer chooses a wrong order or a wrong residual, due to a poor cardinality estimate, the resulting plan can run orders of magnitude slower than expected.We present a new algorithm for RID-list intersection that is both more efficient and more robust than this standard algorithm. First, we avoid forming the RID-lists up front, and instead form this lazily as part of the intersection. This reduces the associated IO and sort cost significantly, especially when the data distribution is skewed. It also ameliorates the problem of wrong residual table selection. Second, we do not intersect the RID-lists via an AND-tree, because this is vulnerable to cardinality mis-estimations. Instead, we use an adaptive set intersection algorithm that performs well even when the cardinality estimates are wrong.We present detailed experiments of this algorithm on data with varying distributions to validate its efficiency and predictability.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {773–784},
numpages = {12},
keywords = {lazy, intersection, sorting, AND tree, star join},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247567,
author = {DeHaan, David and Tompa, Frank Wm.},
title = {Optimal Top-down Join Enumeration},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247567},
doi = {10.1145/1247480.1247567},
abstract = {Most contemporary database systems perform cost-based join enumeration using some variant of System-R's bottom-up dynamic programming method. The notable exceptions are systems based on the top-down transformational search of Volcano/Cascades. As recent work has demonstrated, bottom-up dynamic programming can attain optimality with respect to the shape of the join graph; no comparable results have been published for transformational search. However, transformational systems leverage benefits of top-down search not available to bottom-up methods.In this paper we describe a top-down join enumeration algorithm that is optimal with respect to the join graph. We present performance results demonstrating that a combination of optimal enumeration with search strategies such as branch-and-bound yields an algorithm significantly faster than those previously described in the literature. Although our algorithm enumerates the search space top-down, it does not rely on transformations and thus retains much of the architecture of traditional dynamic programming. As such, this work provides a migration path for existing bottom-up optimizers to exploit top-down search without drastically changing to the transformational paradigm.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {785–796},
numpages = {12},
keywords = {branch-and-bound, dynamic programming, top-down, join enumeration, memoization, query optimization},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247568,
author = {Lazaridis, Iosif and Mehrotra, Sharad},
title = {Optimization of Multi-Version Expensive Predicates},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247568},
doi = {10.1145/1247480.1247568},
abstract = {Modern query optimizers need to take into account the performance of expensive user-defined predicates. Existing research has shown how to incorporate such predicates in a traditional cost-based query optimizer. In this paper we deal with the optimization of the expensive predicates themselves, showing how their cost can be reduced by utilizing cheaper, but less accurate, versions of the predicates to pre-filter tuples. We discuss the generalized tuple handling mechanism, which processes tuples along a fixed sequence of versions, as well as adaptive approaches that either split tuple streams into groups, or make routing decisions at the individual tuple level. We identify the lower bound to the problem of evaluating a multi-version selection predicate by an ideal individualized plan (IIP), and develop an optimal generalized plan (OGP). We then show how realistic individualized or grouped schemes can produce an intermediate cost between OGP and IIP, if tuples substantially deviate from the average stream behavior. Our algorithms are tested experimentally, identifying many of the issues that arise whenever multi-version predicates are used.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {797–808},
numpages = {12},
keywords = {adaptive query processing, data streams, user-defined predicates, query optimization, expensive methods, multi-version predicates, multimedia sensor networks},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247569,
author = {Han, Wook-Shin and Ng, Jack and Markl, Volker and Kache, Holger and Kandil, Mokhtar},
title = {Progressive Optimization in a Shared-Nothing Parallel Database},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247569},
doi = {10.1145/1247480.1247569},
abstract = {Commercial enterprise data warehouses are typically implemented on parallel databases due to the inherent scalability and performance limitation of a serial architecture. Queries used in such large data warehouses can contain complex predicates as well as multiple joins, and the resulting query execution plans generated by the optimizer may be sub-optimal due to mis-estimates of row cardinalities. Progressive optimization (POP) is an approach to detect cardinality estimation errors by monitoring actual cardinalities at run-time and to recover by triggering re-optimization with the actual cardinalities measured. However, the original serial POP solution is based on a serial processing architecture, and the core ideas cannot be readily applied to a parallel shared-nothing environment. Extending the serial POP to a parallel environment is a challenging problem since we need to determine when and how we can trigger re-optimization based on cardinalities collected from multiple independent nodes. In this paper, we present a comprehensive and practical solution to this problem, including several novel voting schemes whether to trigger re-optimization, a mechanism to reuse local intermediate results across nodes as a partitioned materialized view, several flavors of parallel checkpoint operators, and parallel checkpoint processing methods using efficient communication protocols. This solution has been prototyped in a leading commercial parallel DBMS. We have performed extensive experiments using the TPC-H benchmark and a real-world database. Experimental results show that our solution has negligible runtime overhead and accelerates the performance of complex OLAP queries by up to a factor of 22.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {809–820},
numpages = {12},
keywords = {OLAP, autonomous computing, query optimization, parallel databases},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245765,
author = {Cui, Bin},
title = {Session Details: Indexing},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245765},
doi = {10.1145/3245765},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247571,
author = {Chu, Eric and Beckmann, Jennifer and Naughton, Jeffrey},
title = {The Case for a Wide-Table Approach to Manage Sparse Relational Data Sets},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247571},
doi = {10.1145/1247480.1247571},
abstract = {A "sparse" data set typically has hundreds or even thousands of attributes, but most objects have non-null values for only a small number of these attributes. A popular view about sparse data is that it arises merely as the result of poor schema design. In this paper, we argue that rather than being the result of inept schema design,storing a sparse data set in a single table is the right way to proceed. However, for this to be the case, RDBMSs must provide sparse data management facilities that go beyond the previously studied requirement of storing such data sets efficiently. In particular, an RDBMS must 1) enable users to effectively build ad hoc queries over a very large number of attributes, and 2) support efficient evaluation of these queries over a wide, sparse table. We propose techniques that provide these capabilities, and argue that the single-table approach is a necessary component of self-managing database systems because it frees users from a tedious and potentially ineffective schema-design phase when managing sparse data sets.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {821–832},
numpages = {12},
keywords = {relational, wide table, sparse data},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247572,
author = {Phoophakdee, Benjarath and Zaki, Mohammed J.},
title = {Genome-Scale Disk-Based Suffix Tree Indexing},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247572},
doi = {10.1145/1247480.1247572},
abstract = {With the exponential growth of biological sequence databases, it has become critical to develop effective techniques for storing, querying, and analyzing these massive data. Suffix trees are widely used to solve many sequence-based problems, and they can be built in linear time and space, provided the resulting tree fits in main-memory. To index larger sequences, several external suffix tree algorithms have been proposed in recent years. However, they suffer from several problems such as susceptibility to data skew, non-scalability to genome-scale sequences, and non-existence of suffix links, which are crucial in various suffix tree based algorithms. In this paper, we target DNA sequences and propose a novel disk-based suffix tree algorithm called TRELLIS, which effectively scales up to genome-scale sequences. Specifically, it can index the entire human genome using 2GB of memory, in about 4 hours and can recover all its suffix links within 2 hours. TRELLIS was compared to various state-of-the-art persistent disk-based suffix tree construction algorithms, and was shown to outperform the best previous methods, both in terms of indexing time and querying time.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {833–844},
numpages = {12},
keywords = {sequence indexing, external memory, genome-scale, disk-based, suffix tree},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247573,
author = {Tri\ss{}l, Silke and Leser, Ulf},
title = {Fast and Practical Indexing and Querying of Very Large Graphs},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247573},
doi = {10.1145/1247480.1247573},
abstract = {Many applications work with graph-structured data. As graphs grow in size, indexing becomes essential to ensure sufficient query performance. We present the GRIPP index structure (GRaph Indexing based on Pre- and Postorder numbering) for answering reachability queries in graphs.GRIPP requires only linear time and space. Using GRIPP, we can answer reachability queries on graphs with 5 million nodes on average in less than 5 milliseconds, which is unrivaled by previous methods. We evaluate the performance and scalability of our approach on real and synthetic random and scale-free graphs and compare our approach to existing indexing schemes. GRIPP is implemented as stored procedure inside a relational database management system and can therefore very easily be integrated into existing graph-oriented applications.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {845–856},
numpages = {12},
keywords = {reachability queries, databases, graph indexing},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247574,
author = {Cheng, James and Ke, Yiping and Ng, Wilfred and Lu, An},
title = {Fg-Index: Towards Verification-Free Query Processing on Graph Databases},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247574},
doi = {10.1145/1247480.1247574},
abstract = {Graphs are prevalently used to model the relationships between objects in various domains. With the increasing usage of graph databases, it has become more and more demanding to efficiently process graph queries. Querying graph databases is costly since it involves subgraph isomorphism testing, which is an NP-complete problem. In recent years, some effective graph indexes have been proposed to first obtain a candidate answer set by filtering part of the false results and then perform verification on each candidate by checking subgraph isomorphism. Query performance is improved since the number of subgraph isomorphism tests is reduced. However, candidate verification is still inevitable, which can be expensive when the size of the candidate answer set is large.In this paper, we propose a novel indexing technique that constructs a nested inverted-index, called FG-index, based on the set of Frequent subGraphs (FGs). Given a graph query that is an FG in the database, FG-index returns the exact set of query answers without performing candidate verification. When the query is an infrequent graph, FG-index produces a candidate answer set which is close to the exact answer set. Since an infrequent graph means the graph occurs in only a small number of graphs in the database, the number of subgraph isomorphism tests is small. To ensure that the index fits into the main memory, we propose a new notion of δ-Tolerance Closed Frequent Graphs (δ-TCFGs), which allows us to flexibly tune the size of the index in a parameterized way. Our extensive experiments verify that query processing using FG-index is orders of magnitude more efficient than using the state-of-the-art graph index.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {857–872},
numpages = {16},
keywords = {graph querying, frequent subgraphs, graph databases, graph indexing},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245766,
author = {Freire, Juliana},
title = {Session Details: Large-Scale Analytics},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245766},
doi = {10.1145/3245766},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247576,
author = {Chang, William I.},
title = {Turning Data into Knowledge: Challenges and Opportunities at Baidu.Com},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247576},
doi = {10.1145/1247480.1247576},
abstract = {The Chinese language search engine Baidu.com is the fourth most trafficked web site in the world, in what will likely soon become the world's largest user-base, China. We will outline three key systems: search engine, iknow.baidu.com, and advertising platform, and describe open problems encountered in the continuing process to improve Baidu.com's technology and services.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {873},
numpages = {1},
keywords = {VLDB, NLP, search engine, question-answering, china, online advertising, WWW, asian languages},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247577,
author = {Ramakrishnan, Raghu},
title = {Databases on the Web},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247577},
doi = {10.1145/1247480.1247577},
abstract = {What role will database management play in the next generation of the web? I believe that a number of trends signal a growing and central role for the ideas and techniques that have emerged over the past three decades of database research. However, we will have to re-examine some basic issues and build a new generation of data management infrastructures in order to address many of the new demands made by web data management.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {874},
numpages = {1},
keywords = {web search, web data management},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247578,
author = {Wen, Ji-Rong and Ma, Wei-Ying},
title = {Webstudio: Building Infrastructure for Web Data Management},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247578},
doi = {10.1145/1247480.1247578},
abstract = {To explore various ideas and algorithms for improving relevance of a search engine, we found it necessary to build an infrastructure to provide large-scale data management and data processing capabilities. WebStudio is an infrastructure we have constructed to provide an integrated development environment (IDE) for researchers and developers to use in quickly building prototypes and conducting experiments at Web-scale. It is also a Web data management system to allow users to easily store, access, and manipulate Web data.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {875–876},
numpages = {2},
keywords = {web search, web data management},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245767,
author = {Rys, Michael},
title = {Session Details: Data Persistence and Binding},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245767},
doi = {10.1145/3245767},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247580,
author = {Adya, Atul and Blakeley, Jos\'{e} A. and Melnik, Sergey and Muralidhar, S.},
title = {Anatomy of the ADO.NET Entity Framework},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247580},
doi = {10.1145/1247480.1247580},
abstract = {Traditional client-server applications relegate query and persistence operations on their data to database systems. The database system operates on data in the form of rows and tables, while the application operates on data in terms of higher-level programming language constructs (classes, structures etc.). The impedance mismatch in the data manipulation services between the application and the database tier was problematic even in traditional systems. With the advent of service-oriented architectures (SOA), application servers and multi-tier applications, the need for data access and manipulation services that are well-integrated with programming environments and can operate in any tier has increased tremendously.Microsoft's ADO.NET Entity Framework is a platform for programming against data that raises the level of abstraction from the relational level to the conceptual (entity) level, and thereby significantly reduces the impedance mismatch for applications and data-centric services. This paper describes the key aspects of the Entity Framework, the overall system architecture, and the underlying technologies.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {877–888},
numpages = {12},
keywords = {data programming, ADO.NET, conceptual modeling},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247581,
author = {Linskey, Patrick Connor and Prud'hommeaux, Marc},
title = {An In-Depth Look at the Architecture of an Object/Relational Mapper},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247581},
doi = {10.1145/1247480.1247581},
abstract = {Object/relational mapping (ORM) tools can deliver a number of benefits to object-oriented, data-centric applications. These benefits are usually artifacts of the implementation of the tool used for the mapping, and not of the ORM paradigm itself. We will examine some of the significant features that the Kodo[1] ORM implementation delivers, by looking at the architecture of Kodo to see the source of these features.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {889–894},
numpages = {6},
keywords = {Java persistence API, JPA, JDO, openJPA, BEA weblogic server, bject/relational mapping, ORM, Kodo},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247582,
author = {Resende, Luciano},
title = {Handling Heterogeneous Data Sources in a SOA Environment with Service Data Objects (SDO)},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247582},
doi = {10.1145/1247480.1247582},
abstract = {Service Oriented Architecture (SOA) is gaining momentum in the development community based on its model for creating richer, reusable solutions at lower cost that can adjust to the agile business environments. The solutions built on this principle are composed of services that offer business functionality as a unit or as a whole. These services will access data from different data source types and potentially need to aggregate data from different data source types with different data formats. In this type of environment, handling of different forms of data poses a big challenge for application developers, and the challenge increases in size when you consider the broad diversity of programming models in use. Service Data Objects (SDO) is a specification for a programming model that unifies data programming across data source types and provides robust support for common application patterns in a disconnected way. Although Service Data Objects offer many other capabilities that are very useful for supporting integration with tools and other frameworks, this paper will focus on describing SDOs and how it can be used to handle heterogeneous data sources in SOA environments.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {895–897},
numpages = {3},
keywords = {data services, service data objects, xml, service oriented architectures},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245768,
author = {Haas, Laura},
title = {Session Details: Information Management Technology in Asia},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245768},
doi = {10.1145/3245768},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247584,
author = {Wang, Yun},
title = {Recent Database Challenges in China on Data Consolidation and Integration},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247584},
doi = {10.1145/1247480.1247584},
abstract = {During the first phase of IT exploitation in China, a lot of isolated systems were developed and deployed over all the sites where the business took place. Each system typically dealt with one simple local business application. Since the mid 90's, data consolidation and data integration became critical to address the business need for cheaper cost, shorter time to the market as well as better business decision and control. Without consolidated and integrated data, there is no way to support new and complex applications for rapid and large scale business growth in China across lots of industries such as financial, telecom, government, manufacture and distribution.When database is used for companies and government agencies in China, the large Chinese population makes the database volume huge. A nation-wide bank or an insurance company has hundreds of millions accounts easily. A province-level telecom service provider has tens of millions subscribers. In certain domains, such as banking and telecom, many applications are executed as on-line transactions which ask for large transaction throughput with good response time. A province-level telecom service provider expects to handle up to 100 billion calls in a month. In this case, most of the phone calls will be pre-paid type and these calls need to be handled in near-real time.In order to satisfy the performance requirement, some applications implement application cache on application servers. This approach improves the application performance in getting data but it also introduces the complexity and longer delay when a failure in application occurs. Database management systems can provide client cache for powerful application servers in these days. As more and more application servers are added to respond large number of concurrent users and high volume transaction throughput, database management system must be highly scalable. Both logical and physical contentions should be avoided or greatly reduced. Continuous availability and high reliability are more critical for a consolidated large database because many users and applications rely on it. Both planned outage and unplanned outage should be avoided or minimized through appropriate duplication and redundancy. Smooth transition through cooperative versions of data helps reduce planned outages.Swift switch over to hot stand-by database system helps minimize the unplanned outages. Database management systems are expected to be autonomic and remain healthy in resource utilization with minimal administrative involvement. As a summary, data consolidation in China asks for low-cost, high performance database systems with high scalability, availability and reliability on very large database volume.Data integration in China often deals with both a horizontal layer of multiple lines of business and a vertical organizational structure. For government agencies, there are at least four levels across nation, province, district and county. There are a few industry specific data models and solutions to address the requirements of data integration. However, a common data integration platform has been constantly requested by many IT projects of large scale. In many cases, business control processes need to integrate business operation processes to support near-real time decision and control. There is a high demand for a practical and well-proven methodology with tool sets which can support data integration life cycle of architect, design, development, deployment and change management.Rapid economic and business growth in combination of the join to WTO created big demand in IT evolution across China business, industry and government. There are many demands and opportunities of data consolidation and integration with large data volume and system complexity. The demand of low-cost solutions makes it even more challenging.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {898},
numpages = {1},
keywords = {data center consolidation, information integration},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247585,
author = {Li, Jianzhong and Gao, Hong and Luo, Jizhou and Shi, Shengfei and Zhang, Wei},
title = {InfiniteDB: A Pc-Cluster Based Parallel Massive Database Management System},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247585},
doi = {10.1145/1247480.1247585},
abstract = {This paper describes a PC-cluster based parallel DBMS, InfiniteDB, developed by the authors. InfiniteDB aims at efficiently storing and processing of massive databases in response to the rapidly growing in database size and the need of high performance analyzing of massive databases. It supports the parallelisms of intra-query, inter-query, intra-operation, inter-operation and pipelining. It provides effective strategies for processing massive databases including the multiple data declustering methods, the declustering-aware algorithms for the execution of relational operations and other database operations, and the adaptive query optimization method. It also provides the functions of parallel data warehousing and data mining, the coordinator-wrapper mechanism to support the integration of heterogeneous information resources on the Internet, and the fault tolerant and resilient infrastructures. It has been used in many applications and has proved quite effective for storing and processing massive databases in practice.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {899–909},
numpages = {11},
keywords = {parallel algorithm, parallel query processing, data declustering, parallel database},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247586,
author = {Reddy, P. Krishna and Ramaraju, G. V. and Reddy, G. S.},
title = {ESagu™: A Data Warehouse Enabled Personalized Agricultural Advisory System},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247586},
doi = {10.1145/1247480.1247586},
abstract = {In this paper, we explain a personalized agricultural advisory system called eSagu, which has been developed to improve the performance and utilization of agriculture technology and help Indian farmers. In eSagu, rather than visiting the crop in person, the agricultural expert delivers the expert advice at regular intervals (once in one or two weeks) to each farm by getting the crop status in the form of digital photographs and other information. During 2004-06, through eSagu, agricultural expert advices delivered for about 6000 farms covering six crops. The results show that the expert advices helped the farmers to achieve savings in capital investment and improved the crop yield. Mainly, the data warehouse of farm histories has been developed which is providing the crop related information to the agricultural expert in an integrated manner for generating a quality agricultural expert advice. In this paper, after explaining eSagu and its advantages, we discuss how data warehouse of farm histories is enabling agricultural expert to deliver a quality expert advice. We also discuss some research issues to improve the performance of eSagu.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {910–914},
numpages = {5},
keywords = {last-mile problem, computers in agriculture, personalization, digital divide, query-less systems, e-farming, eSagu, scalable systems, IT for development, information and communication technologies for development (ICT4D), IT for rural development, information dissemination, agricultural extension, IT in agriculture},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247587,
author = {Bhide, Manish A. and Gupta, Ajay and Gupta, Rahul and Roy, Prasan and Mohania, Mukesh K. and Ichhaporia, Zenita},
title = {LIPTUS: Associating Structured and Unstructured Information in a Banking Environment},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247587},
doi = {10.1145/1247480.1247587},
abstract = {Growing competition has made today's banks understand the value of knowing their customers better. In this paper, we describe a tool, LIPTUS, that associates the customer interactions (emails and transcribed phone calls) with customer and account profiles stored in an existing data warehouse. The associations discovered by LIPTUS enable analytics spanning the customer and account profiles on one hand and the meta-data associated or derived from the interaction (using text mining techniques) on the other. We illustrate the value derived from this consolidated analysis through specific customer intelligence applications. LIPTUS is today being extensively used in a large bank in India. A highlight of this paper is a discussion of the technical challenges encountered while building LIPTUS and deploying it on real-life customer data.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {915–924},
numpages = {10},
keywords = {customer intelligence, customer support, information integration},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245769,
author = {Lehner, Wolfgang},
title = {Session Details: XML},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245769},
doi = {10.1145/3245769},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247589,
author = {Liu, Zhen Hua and Krishnaprasad, Muralidhar and Warner, James W. and Angrish, Rohan and Arora, Vikas},
title = {Effective and Efficient Update of Xml in RDBMS},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247589},
doi = {10.1145/1247480.1247589},
abstract = {Querying XML effectively and efficiently using declarative languages such as XQuery and XPath has been widely studied in both academic and industrial settings. Most RDBMS vendors now support XML as a native data type with SQL/XML and XQuery support over it. However, the problem of updating XML is still the subject of ongoing effort. Several SQL/XML update extensions have been implemented and an XQuery Update Facility is in the proposal phase to add an update capability to XQuery. There are a lot of challenges involved in updating XML, particularly identifying and updating partial fragments of XML while maintaining concurrency, transactional semantics and validity of the document. In this paper, we illustrate the XML update functionality provided by Oracle XML DB within the context of SQL/XML. This functionality has been developed and optimized based on actual customer use cases of querying and updating XML. We discuss our design philosophy, optimization details for providing capability of updating XML and compare it with the current XQuery Update Facility proposal with the goal of providing insight into incorporating the XQuery Update Facility in the SQL/XML standard in the future.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {925–936},
numpages = {12},
keywords = {XML, XQuery, update},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247590,
author = {Nicola, Matthias and Kogan, Irina and Schiefer, Berni},
title = {An XML Transaction Processing Benchmark},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247590},
doi = {10.1145/1247480.1247590},
abstract = {XML database functionality has been emerging in "XML-only" databases as well as in the major relational database products. Yet, there is no industry standard XML database benchmark to evaluate alternative implementations. The research community has proposed several benchmarks which are all useful in their respective scope, such as evaluating XQuery processors. However, they do not aim to evaluate a database system in its entirety and do not represent all relevant characteristics of a real-world XML application. Often they only define read-only single-user tests on a single XML document. We have developed an application-oriented and domain-specific benchmark called "Transaction Processing over XML" (TPoX). It exercises all aspects of XML databases, including storage, indexing, logging, transaction processing, and concurrency control. Based on our analysis of real XML applications, TPoX simulates a financial multi-user workload with XML data conforming to the FIXML standard. In this paper we describe TPoX and present early performance results. We also make its implementation publicly available.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {937–948},
numpages = {12},
keywords = {benchmark, XQuery, database, XML, TPoX, SQL/XML},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247591,
author = {Grust, Torsten and Rittinger, Jan and Teubner, Jens},
title = {Why Off-the-Shelf RDBMSs Are Better at XPath than You Might Expect},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247591},
doi = {10.1145/1247480.1247591},
abstract = {To compensate for the inherent impedance mismatch between the relational data model (tables of tuples) and XML (ordered, unranked trees), tree join algorithms have become the prevalent means to process XML data in relational databases, most notably the TwigStack[6], structural join[1], and staircase join[13] algorithms. However, the addition of these algorithms to existing systems depends on a significant invasion of the underlying database kernel, an option intolerable for most database vendors.Here, we demonstrate that we can achieve comparable XPath performance without touching the heart of the system. We carefully exploit existing database functionality and accelerate XPath navigation by purely relational means: partitioned B-trees bring access costs to secondary storage to a minimum, while aggregation functions avoid an expensive computation and removal of duplicate result nodes to comply with the XPath semantics. Experiments carried out on IBM DB2 confirm that our approach can turn off-the-shelf database systems into efficient XPath processors.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {949–958},
numpages = {10},
keywords = {relational databases, SQL, partitioned B-tree, XPath},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247592,
author = {Moro, Mirella M. and Lim, Lipyeow and Chang, Yuan-Chi},
title = {Schema Advisor for Hybrid Relational-XML DBMS},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247592},
doi = {10.1145/1247480.1247592},
abstract = {In response to the widespread use of the XML format for document representation and message exchange, major database vendors support XML in terms of persistence, querying and indexing. Specifically, the recently released IBM DB2 9 (for Linux, Unix and Windows) is a hybrid data server with optimized management of both XML and relational data. With the new option of storing and querying XML in a relational DBMS, data architects face the the decision of what portion of their data to persist as XML and what portion as relational data. This problem has not been addressed yet and represents a serious need in the industry. Hence, this paper describes ReXSA, a schema advisor tool that is being prototyped for IBM DB2 9. ReXSA proposes candidate database schemas given an information model of the enterprise data. It has the advantage of considering qualitative properties of the information model such as reuse, evolution and performance profiles for deciding how to persist the data. Finally, we show the viability and practicality of ReXSA by applying it to custom and real usecases.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {959–970},
numpages = {12},
keywords = {hybrid relational-XML database design},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245770,
author = {Lefler, Mike},
title = {Session Details: Data Streams Go Mainstream},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245770},
doi = {10.1145/3245770},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247594,
author = {Lefler, Robert Michael},
title = {Data Streams Go Mainstream},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247594},
doi = {10.1145/1247480.1247594},
abstract = {Data Stream Processing (DSP) has moved rapidly from being the focus of an invited talk at PODS 2002 to start-up companies in 2005 and 2006. Today the giant RDBMS vendors such as IBM, Microsoft, and Oracle have added, or are seeking to add, DSP capabilities to their respective product lines. This panel will discuss the likely trajectory of Data Stream Processing as it moves forward as a component of the market-leading RDBMS product lines.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {971–972},
numpages = {2},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245771,
author = {Mohania, Mukesh},
title = {Session Details: DB Systems Topics},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245771},
doi = {10.1145/3245771},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247596,
author = {Murthy, Ravi and Sedlar, Eric},
title = {Flexible and Efficient Access Control in Oracle},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247596},
doi = {10.1145/1247480.1247596},
abstract = {A single model for access control across the database and application server tiers is crucial to ensure consistent secure access to data in all the tiers. In this paper, we present the common model for access control within Oracle database and application tiers which is based on the standard WebDAV ACLs (Access Control Lists). Further, we discuss the flexible mechanisms for defining ACLs and associating them with data and various optimization techniques for efficiently evaluating ACLs in large scale enterprise applications.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {973–980},
numpages = {8},
keywords = {access control, ACL, enterprise applications, Oracle security model},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247597,
author = {Qiao, Lin and Soetarman, Basuki and Fuh, Gene and Pannu, Adarsh and Cui, Baoqiu and Beavin, Thomas and Kyu, William},
title = {A Framework for Enforcing Application Policies in Database Systems},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247597},
doi = {10.1145/1247480.1247597},
abstract = {As database systems have grown in terms of scale and complexity, administration tasks have become increasingly difficult and time consuming. A scarcity of skilled database professionals has meant that human costs have begun to dominate the total cost of ownership (TCO) of a database system. Database vendors are under immense pressure to provide solutions that make their products easy to administer in areas such as problem diagnostics, monitoring, query tuning, access control and system configuration.To address this issue, we have built a framework that allows control over many administration operations via the use of policies. Users can uniformly define, manage and enforce policies to affect disparate aspects of the system.In our framework, policies are declarative constructs that are comprised of type, scope, condition and action. Policy groups cover query monitoring and tuning, query prioritization, system configuration, access control, report generation, etc. Policy scope defines the domain over which policies apply. Policy actions are performed if certain conditions are true. This framework has been fully integrated into DB2 for z/OS V9. Using detailed system performance evaluations, we report that enforcement of policies is largely a function of data-collection granularity. Under the setting for normal monitoring with minimal report, the overhead on system performance is very low (0.1%).},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {981–992},
numpages = {12},
keywords = {self-managing, database system, policy},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247598,
author = {Elhemali, Mostafa and Galindo-Legaria, C\'{e}sar A. and Grabs, Torsten and Joshi, Milind M.},
title = {Execution Strategies for SQL Subqueries},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247598},
doi = {10.1145/1247480.1247598},
abstract = {Optimizing SQL subqueries has been an active area in database research and the database industry throughout the last decades. Previous work has already identified some approaches to efficiently execute relational subqueries. For satisfactory performance, proper choice of subquery execution strategies becomes even more essential today with the increase in decision support systems and automatically generated SQL, e.g., with ad-hoc reporting tools. This goes hand in hand with increasing query complexity and growing data volumes, which all pose challenges for an industrial-strength query optimizer.This current paper explores the basic building blocks that Microsoft SQL Server utilizes to optimize and execute relational subqueries. We start with indispensable prerequisites such as detection and removal of correlations for subqueries. We identify a full spectrum of fundamental subquery execution strategies such as forward and reverse lookup as well as set-based approaches, explain the different execution strategies for subqueries implemented in SQL Server, and relate them to the current state of the art. To the best of our knowledge, several strategies discussed in this paper have not been published before.An experimental evaluation complements the paper. It quantifies the performance characteristics of the different approaches and shows that indeed alternative execution strategies are needed in different circumstances, which make a cost-based query optimizer indispensable for adequate query performance.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {993–1004},
numpages = {12},
keywords = {relational database systems, microsoft SQL server, query optimization, subqueries},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247599,
author = {Ordonez, Carlos},
title = {Building Statistical Models and Scoring with UDFs},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247599},
doi = {10.1145/1247480.1247599},
abstract = {Multidimensional statistical models are generally computed outside a relational DBMS, exporting data sets. This article explains how fundamental multidimensional statistical models are computed inside the DBMS in a single table scan exploiting SQL and User-Defined Functions (UDFs). The techniques described herein are used in a commercial data mining tool, called Teradata Warehouse Miner. Specifically, we explain how correlation, linear regression, PCA and clustering, are integrated into the Teradata DBMS. Two major database processing tasks are discussed: building a model and scoring a data set based on a model. To build a model two summary matrices are shown to be common and essential for all linear models: the linear sum of points and the quadratic sum of cross-products of points. Since such matrices are generally significantly smaller than the data set, we explain how the remaining matrix operations to build the model can be quickly performed outside the DBMS. We first explain how to efficiently compute summary matrices with plain SQL queries. Then we present two sets of UDFs that work in a single table scan: an aggregate UDF to compute summary matrices and a set of scalar UDFs to score data sets. Experiments compare UDFs and SQL queries (running inside the DBMS) with C++ (running outside on exported files). In general, UDFs are faster than SQL queries and UDFs are more efficient than C++, due to long export times. Statistical models based on the summary matrices can be built outside the DBMS in just a few seconds. Aggregate and scalar UDFs scale linearly and require only one table scan, making them ideal to process large data sets.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1005–1016},
numpages = {12},
keywords = {DBMS, SQL, UDF, statistical model},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/3245772,
author = {Shim, Kyuseok},
title = {Session Details: Data Processing in the Large},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245772},
doi = {10.1145/3245772},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247601,
author = {Liu, King-Lup and Meng, Weiyi and Qiu, Jing and Yu, Clement and Raghavan, Vijay and Wu, Zonghuan and Lu, Yiyao and He, Hai and Zhao, Hongkun},
title = {AllInOneNews: Development and Evaluation of a Large-Scale News Metasearch Engine},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247601},
doi = {10.1145/1247480.1247601},
abstract = {AllInOneNews is the largest news metasearch engine in the world, connecting to over 1,000 news sites over 150 countries. Implementing a large-scale metasearch engine like AllInOneNews needs to overcome unique challenges not faced by building small metasearch engines such as developing highly scalable search engine selection techniques. In this paper, we discuss these unique challenges and our solutions to these challenges. We also discuss some novel features of AllInOneNews such as highly automated solution and semantic query match. This paper also reports the results of a comparative evaluation of three commercial news search systems, one search engine - Google News and two metasearch engines - Mamma News and AllInOneNews. Several measures such as effectiveness, diversity and time-sensitivity are used to perform the comparison. Another contribution of this paper is that we introduce a novel scheme to compare multiple news search systems in a combined measure that takes both relevance and time-sensitivity of retrieved information into consideration.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1017–1028},
numpages = {12},
keywords = {news search, time-sensitive ranking, metasearch engine, search engine},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247602,
author = {Yang, Hung-chih and Dasdan, Ali and Hsiao, Ruey-Lung and Parker, D. Stott},
title = {Map-Reduce-Merge: Simplified Relational Data Processing on Large Clusters},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247602},
doi = {10.1145/1247480.1247602},
abstract = {Map-Reduce is a programming model that enables easy development of scalable parallel applications to process a vast amount of data on large clusters of commodity machines. Through a simple interface with two functions, map and reduce, this model facilitates parallel implementation of many real-world tasks such as data processing jobs for search engines and machine learning.However,this model does not directly support processing multiple related heterogeneous datasets. While processing relational data is a common need, this limitation causes difficulties and/or inefficiency when Map-Reduce is applied on relational operations like joins.We improve Map-Reduce into a new model called Map-Reduce-Merge. It adds to Map-Reduce a Merge phase that can efficiently merge data already partitioned and sorted (or hashed) by map and reduce modules. We also demonstrate that this new model can express relational algebra operators as well as implement several join algorithms.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1029–1040},
numpages = {12},
keywords = {search engine, join, map-reduce, parallel, data processing, cluster, relational, map-reduce-merge, distributed},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247603,
author = {He, Bin and Wang, Rui and Chen, Ying and Lelescu, Ana and Rhodes, James},
title = {BIwTL: A Business Information Warehouse Toolkit and Language for Warehousing Simplification and Automation},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247603},
doi = {10.1145/1247480.1247603},
abstract = {Rapidly leveraging information analytics technologies to mine the mounting information in structured and unstructured forms, derive business insights and improve decision making is becoming increasingly critical to today's business successes. One of the key enablers of the analytics technologies is an Information Warehouse Management System (IWMS) that processes different types and forms of information, builds, and maintains the information warehouse (IW) effectively. Although traditional multi-dimensional data warehousing techniques, coupled with the well-known ETL processes (Extract, Transform, Load) may meet some of the requirements in an IWMS, in general, they fall short on several major aspects: 1. They often lack comprehensive support for both structured and unstructured data processing; 2. they are database-centric and require detailed database and data warehouse knowledge to perform IWMS tasks, and hence they are tedious and time-consuming to operate and learn; 3. they are often inflexible and insufficient in coping with a wide variety of on-going IW maintenance tasks, such as adding new dimensions and handling regular and lengthy data updates with potential failures and errors.To cope with such issues, this paper describes an IWMS, called BIwTL (Business Information Warehouse Toolkit and Language), that automates and simplifies IWMS tasks by devising a high-level declarative information warehousing language, GIWL, and building the runtime system components for such a language. BIwTL hides system details, e.g., databases, full text indexers, and data warehouse models, from users by automatically generating appropriate runtime scripts and executing them based on the GIWL language specification. Moreover, BIwTL supports structured and unstructured information processing by embedding flexible data extraction and transformation capabilities, while ensuring high performance processing for large datasets. In addition, this paper systematically studied the core tasks around information warehousing and identified five key areas. In particular, we describe our technologies in three areas, i.e., constructing an IW, data loading, and maintaining an IW. We have implemented such technologies in BIwTL 1.0 and validated it in real world environments with a number of customers. Our experience suggests that BIwTL is light-weight, simple, efficient, and flexible.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1041–1052},
numpages = {12},
keywords = {warehousing language, information warehouse, data mining},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247604,
author = {Campbell, David and Nori, Anil},
title = {The Microsoft Data Platform},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247604},
doi = {10.1145/1247480.1247604},
abstract = {Advances in hardware, storage, devices, connectivity, and web technology are changing the way applications are designed, deployed, and managed. Applications are increasingly becoming data-centric and data is everywhere, and in all tiers (from the client to the cloud). Data across multiple tiers requires data access and management capabilities across these tiers.The Microsoft Data Platform presents a vision for an end-to-end data platform that offers data services across all tiers.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1053–1060},
numpages = {8},
keywords = {tiers, model-centric data and services, synchronization, cloud, client, data platform, device},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247606,
author = {Fang, Rui and He, Bingsheng and Lu, Mian and Yang, Ke and Govindaraju, Naga K. and Luo, Qiong and Sander, Pedro V.},
title = {GPUQP: Query Co-Processing Using Graphics Processors},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247606},
doi = {10.1145/1247480.1247606},
abstract = {We present GPUQP, a relational query engine that employs both CPUs and GPUs (Graphics Processing Units) for in-memory query co-processing. GPUs are commodity processors traditionally designed for graphics applications. Recent research has shown that they can accelerate some database operations orders of magnitude over CPUs. So far, there has been little work on how GPUs can be programmed for heavy-duty database constructs, such as tree indexes and joins, and how well a full-fledged GPU query co-processor performs in comparison with their CPU counterparts. In this work, we explore the design decisions in using GPUs for query co-processing using both a graphics API and a general purpose programming model. We then demonstrate the processing flows as well as the performance results of our methods.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1061–1063},
numpages = {3},
keywords = {query processing, graphics processing units},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247607,
author = {He, Bingsheng and Li, Yinan and Luo, Qiong and Yang, Dongqing},
title = {EaseDB: A Cache-Oblivious in-Memory Query Processor},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247607},
doi = {10.1145/1247480.1247607},
abstract = {We propose to demonstrate EaseDB, the first cache-oblivious queryprocessor for in-memory relational query processing. The cache-oblivious notion from the theory community refers to the property that no parameters in an algorithm or a data structure need to be tuned for a specific memory hierarchy for optimality. As a result, EaseDB automatically optimizes the cache performance as well as the overall performance of query processing on any memory hierarchy. We have developed a visualization interface to show the detailed performance of EaseDB in comparison with its cache-conscious counterpart, with both the parameters in the cache-conscious algorithms and the hardware platforms varied.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1064–1066},
numpages = {3},
keywords = {cache-oblivious, in-memory query processing},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247608,
author = {Bruno, Nicolas and Chaudhuri, Surajit},
title = {Online Autoadmin: (Physical Design Tuning)},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247608},
doi = {10.1145/1247480.1247608},
abstract = {Existing solutions for the automated physical design problem require explicit invocations of tuning tools and critically depend on DBAs gathering representative workloads manually. In this demonstration, we show an alternative approach to the physical design problem. Specifically, we demonstrate a novel monitoring/tuning DBMS component that we prototyped in Microsoft SQL Server 2005 as a server-side extension. This component is always-on and continuously modifies the current physical design reacting to varying workload or data characteristics. Our solution imposes low overhead and takes into account storage constraints, update statements, and the cost to create physical structures.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1067–1069},
numpages = {3},
keywords = {online algorithms, physical design, continuous tuning},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247609,
author = {Castro, Pablo and Melnik, Sergey and Adya, Atul},
title = {ADO.NET Entity Framework: Raising the Level of Abstraction in Data Programming},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247609},
doi = {10.1145/1247480.1247609},
abstract = {The ADO.NET Entity Framework provides a persistence layer for .NET applications that allows developers to work at a higher level of abstraction when interacting with data and data-access interfaces. Developers can model and access their data using a conceptual schema that is mapped to a relational database via a flexible mapping. Interaction with the data can take place using a SQL-based data manipulation language and iterator APIs, or through an object-based domain model in the spirit of object-to-relational mappers.We demonstrate how the Entity Framework simplifies application development using sample scenarios. We illustrate how the data is modeled, queried and presented to the developer. We also show how the provided data programming infrastructure can result in easier-to-understand code by making its intent more explicit, as well as how it can help with maintenance by adding a level of indirection between the logical database schema and the conceptual model that applications operate on.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1070–1072},
numpages = {3},
keywords = {conceptual modeling, ADO.NET, data programming},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247610,
author = {Larson, Per-Ake and Lehner, Wolfgang and Zhou, Jingren and Zabback, Peter},
title = {Exploiting Self-Monitoring Sample Views for Cardinality Estimation},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247610},
doi = {10.1145/1247480.1247610},
abstract = {Good cardinality estimates are critical for generating good execution plans during query optimization. Complex predicates, correlations between columns, and user-defined functions are extremely hard to handle when using the traditional histogram approach. This demo illustrates the use of sample views for cardinality estimations as prototyped in Microsoft SQL Server. We show the creation of sample views, discuss how they are exploited during query optimization, and explain their potential effect on query plans. In addition, we also show our implementation of maintenance policies using statistical quality control techniques based on query feedback.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1073–1075},
numpages = {3},
keywords = {statistical quality control, sample views, cardinality estimation, sequential sampling, query optimization},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247611,
author = {Mishra, Chaitanya and Volkovs, Maksims},
title = {ConEx: A System for Monitoring Queries},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247611},
doi = {10.1145/1247480.1247611},
abstract = {We present a system, ConEx, for monitoring query execution in a relational database management system. ConEx offers a unified view of query execution, providing continuous visual feedback on the progress of the query, and the status of operators in the query evaluation plan. It incorporates novel techniques to dynamically estimate important parameters affecting query progress efficiently. We describe the design and features of ConEx, and discuss its technology.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1076–1078},
numpages = {3},
keywords = {database adminstration, query monitoring, progress estimation},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247612,
author = {Shivam, Piyush and Demberel, Azbayar and Gunda, Pradeep and Irwin, David and Grit, Laura and Yumerefendi, Aydan and Babu, Shivnath and Chase, Jeff},
title = {Automated and On-Demand Provisioning of Virtual Machines for Database Applications},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247612},
doi = {10.1145/1247480.1247612},
abstract = {Utility computing delivers compute and storage resources to applications as an 'on-demand utility', much like electricity, from a distributed collection of computing resources. There is great interest in running database applications on utility resources (e.g., Oracle's Grid initiative) due to reduced infrastructure and management costs, higher resource utilization, and the ability to handle sudden load surges. Virtual Machine (VM) technology offers powerful mechanisms to manage a utility resource infrastructure. However, provisioning VMs for applications to meet system performance goals, e.g., to meet service level agreements (SLAs), is an open problem. We are building two systems at Duke - Shirako and NIMO - that collectively address this problem.Shirako is a toolkit for leasing VMs to an application from a utility resource infrastructure. NIMO learns application performance models using novel techniques based on active learning, and uses these models to guide VM provisioning in Shirako. We will demonstrate: (a) how NIMO learns performance models in an online and automatic fashion using active learning; and (b) how NIMO uses these models to do automated and on-demand provisioning of VMs in Shirako for two classes of database applications - multi-tier web services and computational science workflows.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1079–1081},
numpages = {3},
keywords = {active learning, modeling, virtual machines},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247613,
author = {Soliman, Mohamed A. and Ilyas, Ihab F. and Chang, Kevin Chen-Chuan},
title = {URank: Formulation and Efficient Evaluation of Top-k Queries in Uncertain Databases},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247613},
doi = {10.1145/1247480.1247613},
abstract = {Top-k processing in uncertain databases is semantically and computationally different from traditional top-k processing. The interplay between query scores and data uncertainty makes traditional techniques inapplicable. We introduce URank, a system that processes new probabilistic formulations of top-k queries inuncertain databases. The new formulations are based on marriage of traditional top-k semantics with possible worlds semantics. URank encapsulates a new processing framework that leverages existing query processing capabilities, and implements efficient search strategies that integrate ranking on scores with ranking on probabilities, to obtain meaningful answers for top-k queries.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1082–1084},
numpages = {3},
keywords = {ranking, probabilistic data, query processing, topk, uncertain data},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247615,
author = {Mueller, Rene and Rellermeyer, Jan S. and Duller, Michael and Alonso, Gustavo and Kossmann, Donald},
title = {A Dynamic and Flexible Sensor Network Platform},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247615},
doi = {10.1145/1247480.1247615},
abstract = {SwissQM is a novel sensor network platform for acquiring data from the real world. Instead of statically hand-crafted programs, SwissQM is a virtual machine capable of executing bytecode programs on the sensor nodes. By using a central and intelligent gateway, it is possible to either push aggregation and other operations into the network, or to execute them on the gateway. Since the gateway is built in an entirely modular style, it can be dynamically extended with new functionality such as user interfaces, user defined functions, or additional query optimizations. The goal of this demonstration is to show the flexibility and the unique features of SwissQM.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1085–1087},
numpages = {3},
keywords = {SwissQM, query machine, sensor networks, R-OSGi, OSGi},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247616,
author = {Duller, Michael and Tamosevicius, Rokas and Alonso, Gustavo and Kossmann, Donald},
title = {XTream: Personal Data Streams},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247616},
doi = {10.1145/1247480.1247616},
abstract = {The real usability of data stream systems depends on the practical aspect of building applications on data streams. In this demo we show two possible applications on data streams implemented on our prototype platform XTream. One application integrates VoIP and E-Mail, the other one incorporates streams in a Smart Home setting. Using these applications we try to identify and discuss the functionality that data stream management systems should provide. Those attending the demo will be able to compose their own applications.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1088–1090},
numpages = {3},
keywords = {data streams, personalization, XTream, information system, distribution, R-OSGi, OSGi},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247617,
author = {Tufte, Kristin and Li, Jin and Maier, David and Papadimos, Vassilis and Bertini, Robert L. and Rucker, James},
title = {Travel Time Estimation Using NiagaraST and Latte},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247617},
doi = {10.1145/1247480.1247617},
abstract = {To address increasing traffic congestion and its associated consequences, traffic managers are turning to intelligent transportation management. The latte project is extending data stream technology to handle queries that combine live streams with large data archives, motivated by needs in the Intelligent Transportation Systems (ITS) domain. In particular, we focus on queries that combine live data streams with large data archives. We demonstrate such stream-archive queries via the travel-time estimation problem. The demonstration uses the new latte system which has been developed using the NiagaraST stream processing system and the PORTAL transportation data archive.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1091–1093},
numpages = {3},
keywords = {data stream management systems, stream-archive queries, hybrid queries},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247618,
author = {Zhou, Aoying and Qian, Weining and Gong, Xueqing and Zhou, Minqi},
title = {Sonnet: An Efficient Distributed Content-Based Dissemination Broker},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247618},
doi = {10.1145/1247480.1247618},
abstract = {In this demonstration, we present a prototype content-based dissemination broker, called Sonnet, which is built upon structured overlay network. It combines approximate filtering of XML packets with routing in the overlay network. Deliberate optimization technologies are implemented. The running and tracing of the system in a real-life application are to be demonstrated.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1094–1096},
numpages = {3},
keywords = {distributed publish/subscribe, XML data dissemination, approximate filtering, path digest},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247619,
author = {Brettlecker, Gert and Schuldt, Heiko},
title = {The OSIRIS-SE (Stream-Enabled) Infrastructure for Reliable Data Stream Management on Mobile Devices},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247619},
doi = {10.1145/1247480.1247619},
abstract = {The proliferation of software and hardware sensors which continuously create large amounts of data has significantly facilitated novel types of applications such as healthcare telemonitoring or roadside traffic management. All these applications demand new mechanisms for online processing and analysis of relevant data coming from multiple data streams. Especially telemonitoring applications in healthcare require a high degree of reliability and must be able to be deployed in a distributed environment. We present OSIRIS-SE, an infrastructure for reliable data stream management in a failure-prone distributed setting including resource-limited mobile devices. OSIRIS-SE supports the combination of different data stream operators into stream processes and offers efficient coordinated operator check pointing for the execution of these stream processes. In order to support mobile devices, OSIRIS-SE is able to deal with multiple failures, offers fine-grained reliability at operator level, and supports decentralized stream process orchestration in a peer-to-peer fashion. Moreover, OSIRIS-SE is fully implemented in Java and thus can be run on different platforms. The demo shows the reliable execution of stream processes in a health monitoring application including a wearable ECG sensor, a Bluetooth enabled blood pressure sensor, and a web cam as data sources. Operators are hosted at mobile devices (PDAs, smart phones) of a patient and at a laptop computer which also acts as base station. An important feature of the demo is to show that sensor data can losslessly be processed by seamlessly migrating stream processing to other devices in the network even in case of multiple failures.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1097–1099},
numpages = {3},
keywords = {checkpointing, healthcare, information management infrastructure, telemonitoring, data stream management},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247620,
author = {Brenna, Lars and Demers, Alan and Gehrke, Johannes and Hong, Mingsheng and Ossher, Joel and Panda, Biswanath and Riedewald, Mirek and Thatte, Mohit and White, Walker},
title = {Cayuga: A High-Performance Event Processing Engine},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247620},
doi = {10.1145/1247480.1247620},
abstract = {We propose a demonstration of Cayuga, a complex event monitoring system for high speed data streams. Our demonstration will show Cayuga applied to monitoring Web feeds; the demo will illustrate the expressiveness of the Cayuga query language, the scalability of its query processing engine to high stream rates, and a visualization of the internals of the query processing engine.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1100–1102},
numpages = {3},
keywords = {continuous query processing, publish/subscribe, complex event processing},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247621,
author = {Varde, Aparna S. and Rundensteiner, Elke A. and Sisson, Richard D.},
title = {AutoDomainMine: A Graphical Data Mining System for Process Optimization},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247621},
doi = {10.1145/1247480.1247621},
abstract = {This paper describes a graphical data mining system called AutoDomainMine. It is based on our proposed approach of integrating clustering and classification to mine scientific data stored in a database. The data consists of input conditions of scientific experiments and graphs plotted as their results. This system mines the stored data in order to submit exact or approximate ranked responses to user queries intended to optimize the scientific processes.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1103–1105},
numpages = {3},
keywords = {search heuristics, ranking, approximate query processing},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247622,
author = {Qu, Huiming and Xu, Jie and Labrinidis, Alexandros},
title = {Quality is in the Eye of the Beholder: Towards User-Centric Web-Databases},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247622},
doi = {10.1145/1247480.1247622},
abstract = {The proliferation of database-driven web sites (or web-databases) has brought upon a plethora of applications where both Quality of Service (QoS) and Quality of Data (QoD) are of paramount importance to the end users. In our previous work, we have proposed Quality Contracts, a comprehensive framework for specifying multiple dimensions of QoS/QoD; we have also developed user-centric admission control and scheduling algorithms in web databases, whose goal is to maximize overall system performance. In this work, we turn our attention to the user side of the equation. Specifically, we propose to demonstrate how the adaptation of Quality Contracts (QCs) by the users can lead to vastly different performance results, both from the user point of view (i.e., user satisfaction) and also from the system point of view. Towards this, we propose to structure our demo in the form of an interactive game, where participants will be playing the role of users continuously adapting their QCs over time, while "playing" against system-generated users, who follow predetermined QC adaptation policies. Finally, we also propose to illustrate the effect of different admission control and scheduling policies.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1106–1108},
numpages = {3},
keywords = {quality contracts, query processing, web-databases, quality of service, user-centric, quality of data, transaction processing},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247623,
author = {Rundensteiner, Elke A. and Ward, Matthew O. and Xie, Zaixian and Cui, Qingguang and Wad, Charudatta V. and Yang, Di and Huang, Shiping},
title = {Xmdvtool<sup><i>Q</i></sup>: Quality-Aware Interactive Data Exploration},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247623},
doi = {10.1145/1247480.1247623},
abstract = {In this work, we describe our approach for making the interactive data exploration system, called XmdvTool, quality-aware to assure informed decision-making. XmdvToolQ, makes quality or lack thereof explicit for all stages of the data exploration process from raw data, to abstracted data, to the final visual displays, allowing users to query and navigate through data-, structure- and quality-spaces.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1109–1112},
numpages = {4},
keywords = {data quality, abstraction quality, display quality},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247625,
author = {Lu, Yiyao and Wu, Zonghuan and Zhao, Hongkun and Meng, Weiyi and Liu, King-Lup and Raghavan, Vijay and Yu, Clement},
title = {MySearchView: A Customized Metasearch Engine Generator},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247625},
doi = {10.1145/1247480.1247625},
abstract = {In this paper, we describe MySearchView, a system for assembling search engines into metasearch engines. With this system, any user can create a metasearch engine by simply letting the system know the URLs of the search engines the user wants to be included and the metasearch engine will be built fully automatically. In this paper, the main steps of building metasearch engines will be sketched. We will also outline our plan to demonstrate all the features of MySearchView.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1113–1115},
numpages = {3},
keywords = {result merging, metasearch engine, customization, wrapper generation},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247626,
author = {Ennals, Robert J. and Garofalakis, Minos N.},
title = {MashMaker: Mashups for the Masses},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247626},
doi = {10.1145/1247480.1247626},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1116–1118},
numpages = {3},
keywords = {web, end-users, mashup},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247627,
author = {Petropoulos, Michalis and Deutsch, Alin and Papakonstantinou, Yannis},
title = {CLIDE: Interactive Query Formulation for Service Oriented Architectures},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247627},
doi = {10.1145/1247480.1247627},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1119–1121},
numpages = {3},
keywords = {web services, query rewriting, interactive query formulation, query capabilities},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247628,
author = {Beeri, Catriel and Eyal, Anat and Milo, Tova and Pilberg, Alon},
title = {Query-Based Monitoring of BPEL Business Processes},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247628},
doi = {10.1145/1247480.1247628},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1122–1124},
numpages = {3},
keywords = {query language, business processes, monitoring},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247629,
author = {Gupta, Nitin and Yang, Fan and Demers, Alan J. and Gehrke, Johannes and Shanmugasundaram, Jayavel},
title = {User-Centric Personalized Extensibility for Data-Driven Web Applications},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247629},
doi = {10.1145/1247480.1247629},
abstract = {We describe a novel programming model for building, extending, and personalizing web-based data-driven applications.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1125–1127},
numpages = {3},
keywords = {declarative language, extensibility, database application},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247630,
author = {Tatemura, Junichi and Sawires, Arsany and Po, Oliver and Chen, Songting and Candan, K. Selcuk and Agrawal, Diviyakant and Goveas, Maria},
title = {Mashup Feeds: Continuous Queries over Web Services},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247630},
doi = {10.1145/1247480.1247630},
abstract = {Mashup Feeds is a system that supports integrated web service feeds as continuous queries. We introduce collection-based stream processing semantics to enable information extraction by monitoring source evolution over time.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1128–1130},
numpages = {3},
keywords = {web service, integration, continuous query},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247631,
author = {Green, Todd J. and Karvounarakis, Grigoris and Taylor, Nicholas E. and Biton, Olivier and Ives, Zachary G. and Tannen, Val},
title = {ORCHESTRA: Facilitating Collaborative Data Sharing},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247631},
doi = {10.1145/1247480.1247631},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1131–1133},
numpages = {3},
keywords = {data exchange, reconciliation, data integration, schema mappings, data sharing},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247632,
author = {Atzeni, Paolo and Cappellari, Paolo and Gianforme, Giorgio},
title = {MIDST: Model Independent Schema and Data Translation},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247632},
doi = {10.1145/1247480.1247632},
abstract = {MIDST is a tool for the translation of schemas and databases from a model to another, in a framework that is flexible and extensible with respect to the family of models. The major novelties with respect to existing proposals consist in the generation of data-level translations and on the customizability of translations (at both schema and data level).},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1134–1136},
numpages = {3},
keywords = {data translation, model management, schema translation},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247633,
author = {Leonardi, Erwin and Bhowmick, Sourav S},
title = {XANADUE: A System for Detecting Changes to XML Data in Tree-Unaware Relational Databases},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247633},
doi = {10.1145/1247480.1247633},
abstract = {Recently, a number of main memory algorithms for detecting the changes to XML data have been proposed. These approaches are not suitable for detecting changes to large XML document as it requires a lot of memory to keep the two versions of XML documents in the memory. We have developed a novel XML change detection system, called XANADUE that uses traditional relational database engines for detecting changes to large XML data. In this approach, we store the XML documents in the relational database and issue SQL queries (whenever appropriate) to detect the changes. This demonstration will showcase the functionality of our system and the effectiveness of XML change detection in relational environment.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1137–1140},
numpages = {4},
keywords = {XML, relational databases, change detection},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247635,
author = {Theobald, Martin and Schenkel, Ralf and Weikum, Gerhard},
title = {The TopX DB&amp;IR Engine},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247635},
doi = {10.1145/1247480.1247635},
abstract = {This paper proposes a demo of the TopX search engine, an extensive framework for unified indexing, querying, and ranking of large collections of unstructured, semistructured, and structured data. TopX integrates efficient algorithms for top-k-style ranked retrieval with powerful scoring models for text and XML documents, as well as dynamic and self-tuning query expansion based on background ontologies.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1141–1143},
numpages = {3},
keywords = {XML, text, information retrieval, top-k query processing},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247636,
author = {Cheng, Tao and Yan, Xifeng and Chang, Kevin Chen-Chuan},
title = {Supporting Entity Search: A Large-Scale Prototype Search Engine},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247636},
doi = {10.1145/1247480.1247636},
abstract = {As the Web has evolved into a data-rich repository, with the standard page view," current search engines are increasingly inadequate. While we often search for various data "entities" (e.g. phone number, paper PDF, date), today's engines only take us indirectly to pages. Therefore, we propose the concept of entity search, a significant departure from traditional document retrieval. Towards our goal of supporting entity search, in the WISDM project at UIUC we build and evaluate our prototype search engine over a 2TB Web corpus. Our demonstration shows the feasibility and promise of a large-scale system architecture to support entity search.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1144–1146},
numpages = {3},
keywords = {web search, large-scale, entity search, association mining},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247637,
author = {Hwang, Heasoo and Balmin, Andrey and Pirahesh, Hamid and Reinwald, Berthold},
title = {Information Discovery in Loosely Integrated Data},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247637},
doi = {10.1145/1247480.1247637},
abstract = {We model heterogeneous data sources with cross references, such as those crawled on the (enterprise) web, as a labeled graph with data objects as typed nodes and references or links as edges. Given the labeled data graph, we introduce flexible and efficient querying capabilities that go beyond existing capabilities by additionally discovering meaningful relationships between objects that satisfy keyword and/or structured query filters. We introduce the relationship search operator that exploits the link structure between data objects to rank objects related to the result of a filter. We implement the search operator using the ObjectRank [1] algorithm that uses the random surfer model. We study several alternatives for constructing summary graphs for query results that consist of individual and aggregate nodes that are somehow linked to qualifying result nodes. Some of the summary graphs are useful for presenting query results to the user, while others could be used to evaluate subsequent queries efficiently without considering all the nodes and links in the original data graph.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1147–1149},
numpages = {3},
keywords = {information discovery, XML, search},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247638,
author = {Missier, Paolo and Embury, Suzanne M. and Greenwood, Mark and Preece, Alun and Jin, Binling},
title = {Managing Information Quality in E-Science: The Qurator Workbench},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247638},
doi = {10.1145/1247480.1247638},
abstract = {Data-intensive e-science applications often rely on third-party data found in public repositories, whose quality is largely unknown. Although scientists are aware that this uncertainty may lead to incorrect scientific conclusions, in the absence of a quantitative characterization of data quality properties they find it difficult to formulate precise data acceptability criteria. We present an Information Quality management workbench, called Qurator, that supports data experts in the specification of personal quality models, and lets them derive effective criteria for data acceptability. The demo of our working prototype will illustrate our approach on a real e-science workflow for a bioinformatics application.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1150–1152},
numpages = {3},
keywords = {semantic modelling of information quality, information quality management},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247639,
author = {Qi, Yan and Candan, K. Sel\c{c}uk and Sapino, Maria Luisa and Kintigh, Keith W.},
title = {Integrating and Querying Taxonomies with Quest in the Presence of Conflicts},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247639},
doi = {10.1145/1247480.1247639},
abstract = {We present the QUery-driven Exploration of Semistructured dataand meta-data with conflicTs and partial knowledge (QUEST) system for supporting the integration of scientific data and taxonomies in the presence of misalignments and conflicts. QUEST relies on a novel constraint-based data model that captures both value and structural conflicts and enables researchers to observe and resolve such misalignments in the integrated data by considering the context provided by the data requirements of given research questions.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1153–1155},
numpages = {3},
keywords = {reasoning with misaligned data, taxonomy, query processing, relevance feedback, conflicts},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247640,
author = {Nandi, Arnab and Jagadish, H. V.},
title = {Assisted Querying Using Instant-Response Interfaces},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247640},
doi = {10.1145/1247480.1247640},
abstract = {We demonstrate a novel query interface that enables users to construct a rich search query without any prior knowledge of the underlying schema or data. The interface, which is in the form of a single text input box, interacts in real-time with the users as they type, guiding them through the query construction. We discuss the issues of schema and data complexity, result size estimation, and query validity; and provide novel approaches to solving these problems. We demonstrate our query interface on two popular applications; an enterprise-wide personnel search, and a biological information database.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1156–1158},
numpages = {3},
keywords = {autocompletion, query, interface, keyword},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247641,
author = {Fern\`{a}ndez, Mary F. and Jim, Trevor and Morton, Kristi and Onose, Nicola and Sim\'{e}on, J\'{e}r\^{o}me},
title = {Highly Distributed XQuery with DXQ},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247641},
doi = {10.1145/1247480.1247641},
abstract = {Many modern applications, from Grid computing to RSS handling, need to support data processing in a distributed environment. Currently, most such applications are implemented using a general purpose programming language, which can be expensive to maintain, hard to configure and modify, and require hand optimization of the distributed data processing operations. We present Distributed XQuery (DXQ), a simple, yet powerful, extension of XQuery to support distributed applications. This extension includes the ability to deploy networks of XQuery servers, to remotely invoke XQuery programs on those servers, and to ship code between servers. Our demonstration presents two applications implemented in DXQ: the resolution algorithm of DNS, the Domain Name System, and the Narada overlay-network protocol. We show that our system can flexibly accommodate different patterns of distributed computation and present some simple but essential distributed optimizations.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1159–1161},
numpages = {3},
keywords = {distribution, XML, protocols, XQuery},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247642,
author = {Grust, Torsten and Mayr, Manuel and Rittinger, Jan and Sakr, Sherif and Teubner, Jens},
title = {A SQL: 1999 Code Generator for the Pathfinder Xquery Compiler},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247642},
doi = {10.1145/1247480.1247642},
abstract = {The Pathfinder XQuery compiler has been enhanced by a new code generator that can target any SQL:1999-compliant relational database system(RDBMS). This code generator marks an important next step towards truly relational XQuery processing, a branch of database technology that aims to turn RDBMSs into highly efficient XML and XQuery processors without the need to invade the relational database kernel. Pathfinder, a retargetable front-end compiler, translates input XQuery expressions into DAG-shaped relational algebra plans. The code generator then turns these plans into sequences of either SQL:1999 statements or view definitions which jointly implement the (sometimes intricate) XQuery semantics. In a sense, this demonstration thus lets relational algebra and SQL swap their traditional roles in database query processing. The result is a code generator that (1) supports an almost complete dialect of XQuery, (2) can target any RDBMS with a SQL:1999 language interface, and (3) exhibits quite promising performance characteristics when run against high-volume XML data as well as complex XQuery expressions.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1162–1164},
numpages = {3},
keywords = {relational databases, XQuery, relational algebra, SQL},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247643,
author = {Li, Yunyao and Chaudhuri, Ishan and Yang, Huahai and Singh, Satinder and Jagadish, H. V.},
title = {DaNaLIX: A Domain-Adaptive Natural Language Interface for Querying XML},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247643},
doi = {10.1145/1247480.1247643},
abstract = {We present DaNaLIX, a prototype domain-adaptive natural language interface for querying XML. Our system is an extension of NaLIX, a generic natural language interface for querying XML. While retaining the portability of a purely generic system like NaLIX, DaNaLIX can exploit domain knowledge, whenever available, to its advantage for query translation. More importantly, in DaNaLIX such domain knowledge does not have to be pre-defined; instead it can be automatically obtained from the interactions between a user and the system. In this demonstration, we describe the overall architecture of DaNaLIX. We also demonstrate how a generic system like DaNaLIX can take advantage of domain knowledge to improve its usability and query translation accuracy. In addition, we show DaNaLIX still possesses the portability of a generic system by using data collections from three different domains. Finally, we present how domain knowledge can be obtained through user interactions in an automatic fashion.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1165–1168},
numpages = {4},
keywords = {XML, domain adaptiveness, domain awareness, natural language interface for databases},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247645,
author = {Chandy, K. Mani and Gawlick, Dieter},
title = {Event Processing Using Database Technology},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247645},
doi = {10.1145/1247480.1247645},
abstract = {This tutorial deals with applications that help systems and individuals respond to critical conditions in their environments. The identification of critical conditions requires correlating vast amounts of data within and outside an enterprise. Conditions that signal opportunities or threats are defined by complex patterns of data over time, space and other attributes. Systems and individuals have models (expectations) of behaviors of their environments, and applications notify them when reality - as determined by measurements and estimates - deviate from their expectations. Components of event systems are also sent information to validate their current models and when specific responses are required. Valuable information is that which supports or contradicts current expectations or that which requires an action on the part of the receiver. A major problem today is information overload; this problem can be solved by identifying what information is critical, complementing existing pull technology with sophisticated push technology, and filtering out non-critical data.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1169–1170},
numpages = {2},
keywords = {streams, events, false negatives, distributed systems, errors, false positives, statistics},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247646,
author = {Buneman, Peter and Tan, Wang-Chiew},
title = {Provenance in Databases},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247646},
doi = {10.1145/1247480.1247646},
abstract = {The provenance of data has recently been recognized as central tothe trust one places in data. It is also important to annotation, todata integration and to probabilistic databases. Three workshops havebeen held on the topic, and it has been the focus of several researchprojects and prototype systems. This tutorial will attempt to providean overview of research in provenance in databases with a focus onrecent database research and technology in this area. This tutorialis aimed at a general database research audience and at people whowork with scientific data.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1171–1173},
numpages = {3},
keywords = {lineage, data provenance},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247647,
author = {Faloutsos, Christos and Kolda, Tamara G. and Sun, Jimeng},
title = {Mining Large Graphs and Streams Using Matrix and Tensor Tools},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247647},
doi = {10.1145/1247480.1247647},
abstract = {Coevolving streams of numerical measurements, as well astime evolving graphs, can well be represented as tensors. Here we review the fundamental matrix and tensors tools forthe analysis and mining of large scale streams and graphs.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1174},
numpages = {1},
keywords = {data mining, streams, tensors},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247648,
author = {Nori, Anil},
title = {Mobile and Embedded Databases},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247648},
doi = {10.1145/1247480.1247648},
abstract = {Recent advances in device technology and connectivity have paved the way for next generation applications that are data-driven, whose data can reside anywhere, can be accessed at any time, from any client. Also, advances in memory technology are driving the capacities of RAM and Flash higher, and their costs down. These trends lead to applications that are mobile, embedded, and data-centric. This tutorial presents an overview of the mobile and embedded database systems and their applications.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1175–1177},
numpages = {3},
keywords = {mobile, streams, in-memory database, embedded, extensible, devices, synchronization},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247649,
author = {Cormode, Graham and Garofalakis, Minos},
title = {Streaming in a Connected World: Querying and Tracking Distributed Data Streams},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247649},
doi = {10.1145/1247480.1247649},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1178–1181},
numpages = {4},
keywords = {distributed data, data streams, sensor networks},
location = {Beijing, China},
series = {SIGMOD '07}
}

@inproceedings{10.1145/1247480.1247650,
author = {Luo, Qiong and Wu, Hejun},
title = {System Design Issues in Sensor Databases},
year = {2007},
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247480.1247650},
doi = {10.1145/1247480.1247650},
abstract = {In-network sensor query processing systems (ISQPs), or sensor databases, have been developed to acquire, process and aggregate data from wireless sensor networks (WSNs). Because WSNs are resource-limited and involve multiple layers of embedded software, the system design issues have a significant impact on the performance of sensor databases. Therefore, we propose this tutorial to study the state of the art on these issues with a focus on their interaction with query processing techniques. Our goal is to present the challenges and efforts in developing holistic, efficient ISQPs. Specifically, we will cover architectural design, scheduling, data-centric routing, and wireless medium access control. This tutorial is intended for database researchers who are interested in sensor networks.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data},
pages = {1182–1185},
numpages = {4},
keywords = {scheduling, data-centric routing, sensor networks, wireless medium access control, in-network query processing},
location = {Beijing, China},
series = {SIGMOD '07}
}

