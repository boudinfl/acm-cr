@inproceedings{10.1145/872757.872759,
author = {Ullman, Jeffrey D.},
title = {Improving the Efficiency of Database-System Teaching},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872759},
doi = {10.1145/872757.872759},
abstract = {The education industry has a very poor record of productivity gains. In this brief article, I outline some of the ways the teaching of a college course in database systems could be made more efficient, and staff time used more productively. These ideas carry over to other programming-oriented courses, and many of them apply to any academic subject whatsoever. After proposing a number of things that could be done, I concentrate here on a system under development, called OTC (On-line Testing Center), and on its methodology of "root questions." These questions encourage students to do homework of the long-answer type, yet we can have their work checked and graded automatically by a simple multiple-choice-question grader. OTC also offers some improvement in the way we handle SQL homework, and could be used with other languages as well.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {1–3},
numpages = {3},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872761,
author = {Al-Khalifa, Shurug and Yu, Cong and Jagadish, H. V.},
title = {Querying Structured Text in an XML Database},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872761},
doi = {10.1145/872757.872761},
abstract = {XML databases often contain documents comprising structured text. Therefore, it is important to integrate "information retrieval style" query evaluation, which is well-suited for natural language text, with standard "database style" query evaluation, which handles structured queries efficiently. Relevance scoring is central to information retrieval. In the case of XML, this operation becomes more complex because the data required for scoring could reside not directly in an element itself but also in its descendant elements.In this paper, we propose a bulk-algebra, TIX, and describe how it can be used as a basis for integrating information retrieval techniques into a standard pipelined database query evaluation engine. We develop new evaluation strategies essential to obtaining good performance, including a stack-based TermJoin algorithm for efficiently scoring composite elements. We report results from an extensive experimental evaluation, which show, among other things, that the new TermJoin access method outperforms a direct implementation of the same functionality using standard operators by a large factor.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {4–15},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872762,
author = {Guo, Lin and Shao, Feng and Botev, Chavdar and Shanmugasundaram, Jayavel},
title = {XRANK: Ranked Keyword Search over XML Documents},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872762},
doi = {10.1145/872757.872762},
abstract = {We consider the problem of efficiently producing ranked results for keyword search queries over hyperlinked XML documents. Evaluating keyword search queries over hierarchical XML documents, as opposed to (conceptually) flat HTML documents, introduces many new challenges. First, XML keyword search queries do not always return entire documents, but can return deeply nested XML elements that contain the desired keywords. Second, the nested structure of XML implies that the notion of ranking is no longer at the granularity of a document, but at the granularity of an XML element. Finally, the notion of keyword proximity is more complex in the hierarchical XML data model. In this paper, we present the XRANK system that is designed to handle these novel features of XML keyword search. Our experimental results show that XRANK offers both space and performance benefits when compared with existing approaches. An interesting feature of XRANK is that it naturally generalizes a hyperlink based HTML search engine such as Google. XRANK can thus be used to query a mix of HTML and XML documents.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {16–27},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872764,
author = {Babcock, Brian and Olston, Chris},
title = {Distributed Top-k Monitoring},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872764},
doi = {10.1145/872757.872764},
abstract = {The querying and analysis of data streams has been a topic of much recent interest, motivated by applications from the fields of networking, web usage analysis, sensor instrumentation, telecommunications, and others. Many of these applications involve monitoring answers to continuous queries over data streams produced at physically distributed locations, and most previous approaches require streams to be transmitted to a single location for centralized processing. Unfortunately, the continual transmission of a large number of rapid data streams to a central location can be impractical or expensive. We study a useful class of queries that continuously report the k largest values obtained from distributed data streams ("top-k monitoring queries"), which are of particular interest because they can be used to reduce the overhead incurred while running other types of monitoring queries. We show that transmitting entire data streams is unnecessary to support these queries and present an alternative approach that reduces communication significantly. In our approach, arithmetic constraints are maintained at remote stream sources to ensure that the most recently provided top-k answer remains valid to within a user-specified error tolerance. Distributed communication is only necessary on occasion, when constraints are violated, and we show empirically through extensive simulation on real-world data that our approach reduces overall communication cost by an order of magnitude compared with alternatives that o er the same error guarantees.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {28–39},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872765,
author = {Das, Abhinandan and Gehrke, Johannes and Riedewald, Mirek},
title = {Approximate Join Processing over Data Streams},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872765},
doi = {10.1145/872757.872765},
abstract = {We consider the problem of approximating sliding window joins over data streams in a data stream processing system with limited resources. In our model, we deal with resource constraints by shedding load in the form of dropping tuples from the data streams. We first discuss alternate architectural models for data stream join processing, and we survey suitable measures for the quality of an approximation of a set-valued query result. We then consider the number of generated result tuples as the quality measure, and we give optimal offline and fast online algorithms for it. In a thorough experimental study with synthetic and real data we show the efficacy of our solutions. For applications with demand for exact results we introduce a new Archive-metric which captures the amount of work needed to complete the join in case the streams are archived for later processing.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {40–51},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872767,
author = {Witkowski, Andrew and Bellamkonda, Srikanth and Bozkaya, Tolga and Dorman, Gregory and Folkert, Nathan and Gupta, Abhinav and Shen, Lei and Subramanian, Sankar},
title = {Spreadsheets in RDBMS for OLAP},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872767},
doi = {10.1145/872757.872767},
abstract = {One of the critical deficiencies of SQL is lack of support for n-dimensional array-based computations which are frequent in OLAP environments. Relational OLAP (ROLAP) applications have to emulate them using joins, recently introduced SQL Window Functions [18] and complex and inefficient CASE expressions. The designated place in SQL for specifying calculations is the SELECT clause, which is extremely limiting and forces the user to generate queries using nested views, subqueries and complex joins. Furthermore, SQL-query optimizer is pre-occupied with determining efficient join orders and choosing optimal access methods and largely disregards optimization of complex numerical formulas. Execution methods concentrated on efficient computation of a cube [11], [16] rather than on random access structures for inter-row calculations. This has created a gap that has been filled by spreadsheets and specialized MOLAP engines, which are good at formulas for mathematical modeling but lack the formalism of the relational model, are difficult to manage, and exhibit scalability problems. This paper presents SQL extensions involving array based calculations for complex modeling. In addition, we present optimizations, access structures and execution models for processing them efficiently.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {52–63},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872768,
author = {Lakshmanan, Laks V. S. and Pei, Jian and Zhao, Yan},
title = {QC-Trees: An Efficient Summary Structure for Semantic OLAP},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872768},
doi = {10.1145/872757.872768},
abstract = {Recently, a technique called quotient cube was proposed as a summary structure for a data cube that preserves its semantics, with applications for online exploration and visualization. The authors showed that a quotient cube can be constructed very efficiently and it leads to a significant reduction in the cube size. While it is an interesting proposal, that paper leaves many issues unaddressed. Firstly, a direct representation of a quotient cube is not as compact as possible and thus still wastes space. Secondly, while a quotient cube can in principle be used for answering queries, no specific algorithms were given in the paper. Thirdly, maintaining any summary structure incrementally against updates is an important task, a topic not addressed there. In this paper, we propose an efficient data structure called QC-tree and an efficient algorithm for directly constructing it from a base table, solving the first problem. We give efficient algorithms that address the remaining questions. We report results from an extensive performance study that illustrate the space and time savings achieved by our algorithms over previous ones (wherever they exist).},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {64–75},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872770,
author = {Schleimer, Saul and Wilkerson, Daniel S. and Aiken, Alex},
title = {Winnowing: Local Algorithms for Document Fingerprinting},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872770},
doi = {10.1145/872757.872770},
abstract = {Digital content is for copying: quotation, revision, plagiarism, and file sharing all create copies. Document fingerprinting is concerned with accurately identifying copying, including small partial copies, within large sets of documents.We introduce the class of local document fingerprinting algorithms, which seems to capture an essential property of any finger-printing technique guaranteed to detect copies. We prove a novel lower bound on the performance of any local algorithm. We also develop winnowing, an efficient local fingerprinting algorithm, and show that winnowing's performance is within 33% of the lower bound. Finally, we also give experimental results on Web data, and report experience with MOSS, a widely-used plagiarism detection service.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {76–85},
numpages = {10},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872771,
author = {Agrawal, Rakesh and Evfimievski, Alexandre and Srikant, Ramakrishnan},
title = {Information Sharing across Private Databases},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872771},
doi = {10.1145/872757.872771},
abstract = {Literature on information integration across databases tacitly assumes that the data in each database can be revealed to the other databases. However, there is an increasing need for sharing information across autonomous entities in such a way that no information apart from the answer to the query is revealed. We formalize the notion of minimal information sharing across private databases, and develop protocols for intersection, equijoin, intersection size, and equijoin size. We also show how new applications can be built using the proposed protocols.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {86–97},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872772,
author = {Sion, Radu and Atallah, Mikhail and Prabhakar, Sunil},
title = {Rights Protection for Relational Data},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872772},
doi = {10.1145/872757.872772},
abstract = {Protecting rights over relational data is of ever increasing interest, especially considering areas where sensitive, valuable content is to be outsourced. A good example is a data mining application, where data is sold in pieces to parties specialized in mining it.Different avenues for rights protection are available, each with its own advantages and drawbacks. Enforcement by legal means is usually ineffective in preventing theft of copyrighted works, unless augmented by a digital counter-part, for example watermarking.Recent research of the authors introduces the issue of digital watermarking for generic number sets. In the present paper we expand on this foundation and introduce a solution for relational database content rights protection through watermarking.Our solution addresses important attacks, such as data re-sorting, subset selection, linear data changes (applying a linear transformation on arbitrary subsets of the data). Our watermark also survives up to 50% and above data loss.Finally we present wmdb.*, a proof-of-concept implementation of our algorithm and its application to real life data, namely in watermarking the outsourced Wal-Mart sales data that we have available at our institute.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {98–109},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872774,
author = {Wang, Haixun and Park, Sanghyun and Fan, Wei and Yu, Philip S.},
title = {ViST: A Dynamic Index Method for Querying XML Data by Tree Structures},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872774},
doi = {10.1145/872757.872774},
abstract = {With the growing importance of XML in data exchange, much research has been done in providing flexible query facilities to extract data from structured XML documents. In this paper, we propose ViST, a novel index structure for searching XML documents. By representing both XML documents and XML queries in structure-encoded sequences, we show that querying XML data is equivalent to finding subsequence matches. Unlike index methods that disassemble a query into multiple sub-queries, and then join the results of these sub-queries to provide the final answers, ViST uses tree structures as the basic unit of query to avoid expensive join operations. Furthermore, ViST provides a unified index on both content and structure of the XML documents, hence it has a performance advantage over methods indexing either just content or structure. ViST supports dynamic index update, and it relies solely on B+ Trees without using any specialized data structures that are not well supported by DBMSs. Our experiments show that ViST is effective, scalable, and efficient in supporting structural queries.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {110–121},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872775,
author = {Min, Jun-Ki and Park, Myung-Jae and Chung, Chin-Wan},
title = {XPRESS: A Queriable Compression for XML Data},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872775},
doi = {10.1145/872757.872775},
abstract = {Like HTML, many XML documents are resident on native file systems. Since XML data is irregular and verbose, the disk space and the network bandwidth are wasted. To overcome the verbosity problem, the research on compressors for XML data has been conducted. However, some XML compressors do not support querying compressed data, while other XML compressors which support querying compressed data blindly encode tags and data values using predefined encoding methods. Thus, the query performance on compressed XML data is degraded.In this paper, we propose XPRESS, an XML compressor which supports direct and efficient evaluations of queries on compressed XML data. XPRESS adopts a novel encoding method, called reverse arithmetic encoding, which is intended for encoding label paths of XML data, and applies diverse encoding methods depending on the types of data values. Experimental results with real life data sets show that XPRESS achieves significant improvements on query performance for compressed XML data and reasonable compression ratios. On the average, the query performance of XPRESS is 2.83 times better than that of an existing XML compressor and the compression ratio of XPRESS is 73%.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {122–133},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872776,
author = {Chen, Qun and Lim, Andrew and Ong, Kian Win},
title = {D(k)-Index: An Adaptive Structural Summary for Graph-Structured Data},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872776},
doi = {10.1145/872757.872776},
abstract = {To facilitate queries over semi-structured data, various structural summaries have been proposed. Structural summaries are derived directly from the data and serve as indices for evaluating path expressions on semi-structured or XML data. We introduce the D(k) index, an adaptive structural summary for general graph structured documents. Building on previous work, 1-index and A(k) index, the D(k)-index is also based on the concept of bisimilarity. However, as a generalization of the 1-index and A(k)-index, the D(k) index possesses the adaptive ability to adjust its structure according to the current query load. This dynamism also facilitates efficient update algorithms, which are crucial to practical applications of structural indices, but have not been adequately addressed in previous index proposals. Our experiments show that the D(k) index is a more effective structural summary than previous static ones, as a result of its query load sensitivity. In addition, update operations on the D(k) index can be performed more efficiently than on its predecessors.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {134–144},
numpages = {11},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872777,
author = {Wang, Wei and Jiang, Haifeng and Lu, Hongjun and Yu, Jeffrey Xu},
title = {Containment Join Size Estimation: Models and Methods},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872777},
doi = {10.1145/872757.872777},
abstract = {Recent years witnessed an increasing interest in researches in XML, partly due to the fact that XML has now become the de facto standard for data interchange over the internet. A large amount of work has been reported on XML storage models and query processing techniques. However, few works have addressed issues of XML query optimization. In this paper, we report our study on one of the challenges in XML query optimization: containment join size estimation. Containment join is well accepted as an important operation in XML query processing. Estimating the size of its results is no doubt essential to generate efficient XML query processing plans. We propose two models, the interval model and the position model, and a set of estimation methods based on these two models. Comprehensive performance studies were conducted. The results not only demonstrate the advantages of our new algorithms over existing algorithms, but also provide valuable insights into the tradeoff among various parameters.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {145–156},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872778,
author = {Mamoulis, Nikos},
title = {Efficient Processing of Joins on Set-Valued Attributes},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872778},
doi = {10.1145/872757.872778},
abstract = {Object-oriented and object-relational DBMS support set valued attributes, which are a natural and concise way to model complex information. However, there has been limited research to-date on the evaluation of query operators that apply on sets. In this paper we study the join of two relations on their set-valued attributes. Various join types are considered, namely the set containment, set equality, and set overlap joins. We show that the inverted file, a powerful index for selection queries, can also facilitate the efficient evaluation of most join predicates. We propose join algorithms that utilize inverted files and compare them with signature-based methods for several set-comparison predicates.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {157–168},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872779,
author = {Dyreson, Curtis E.},
title = {Temporal Coalescing with <i>Now</i> Granularity, and Incomplete Information},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872779},
doi = {10.1145/872757.872779},
abstract = {This paper presents a novel strategy for temporal coalescing. Temporal coalescing merges the temporal extents of value-equivalent tuples. A temporal extent is usually coalesced offline and stored since coalescing is an expensive operation. But the temporal extent of a tuple with now, times at different granularities, or incomplete times cannot be determined until query evaluation. This paper presents a strategy to partially coalesce temporal extents by identifying regions that are potentially covered. The covered regions can be used to evaluate temporal predicates and constructors on the coalesced extent. Our strategy uses standard relational database technology. We quantify the cost using the Oracle DBMS.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {169–180},
numpages = {12},
keywords = {now, incomplete information, granularity, coalescing, temporal databases},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872780,
author = {Zhu, Yunyue and Shasha, Dennis},
title = {Warping Indexes with Envelope Transforms for Query by Humming},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872780},
doi = {10.1145/872757.872780},
abstract = {A Query by Humming system allows the user to find a song by humming part of the tune. No musical training is needed. Previous query by humming systems have not provided satisfactory results for various reasons. Some systems have low retrieval precision because they rely on melodic contour information from the hum tune, which in turn relies on the error-prone note segmentation process. Some systems yield better precision when matching the melody directly from audio, but they are slow because of their extensive use of Dynamic Time Warping (DTW). Our approach improves both the retrieval precision and speed compared to previous approaches. We treat music as a time series and exploit and improve well-developed techniques from time series databases to index the music for fast similarity queries. We improve on existing DTW indexes technique by introducing the concept of envelope transforms, which gives a general guideline for extending existing dimensionality reduction methods to DTW indexes. The net result is high scalability. We confirm our claims through extensive experiments.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {181–192},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872782,
author = {Melnik, Sergey and Rahm, Erhard and Bernstein, Philip A.},
title = {Rondo: A Programming Platform for Generic Model Management},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872782},
doi = {10.1145/872757.872782},
abstract = {Model management aims at reducing the amount of programming needed for the development of metadata-intensive applications. We present a first complete prototype of a generic model management system, in which high-level operators are used to manipulate models and mappings between models. We define the key conceptual structures: models, morphisms, and selectors, and describe their use and implementation. We specify the semantics of the known model-management operators applied to these structures, suggest new ones, and develop new algorithms for implementing the individual operators. We examine the solutions for two model-management tasks that involve manipulations of relational schemas, XML schemas, and SQL views.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {193–204},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872783,
author = {Kang, Jaewoo and Naughton, Jeffrey F.},
title = {On Schema Matching with Opaque Column Names and Data Values},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872783},
doi = {10.1145/872757.872783},
abstract = {Most previous solutions to the schema matching problem rely in some fashion upon identifying "similar" column names in the schemas to be matched, or by recognizing common domains in the data stored in the schemas. While each of these approaches is valuable in many cases, they are not infallible, and there exist instances of the schema matching problem for which they do not even apply. Such problem instances typically arise when the column names in the schemas and the data in the columns are "opaque" or very difficult to interpret. In this paper we propose a two-step technique that works even in the presence of opaque column names and data values. In the first step, we measure the pair-wise attribute correlations in the tables to be matched and construct a dependency graph using mutual information as a measure of the dependency between attributes. In the second stage, we find matching node pairs in the dependency graphs by running a graph matching algorithm. We validate our approach with an experimental study, the results of which suggest that such an approach can be a useful addition to a set of (semi) automatic schema matching techniques.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {205–216},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872784,
author = {He, Bin and Chang, Kevin Chen-Chuan},
title = {Statistical Schema Matching across Web Query Interfaces},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872784},
doi = {10.1145/872757.872784},
abstract = {Schema matching is a critical problem for integrating heterogeneous information sources. Traditionally, the problem of matching multiple schemas has essentially relied on finding pairwise-attribute correspondence. This paper proposes a different approach, motivated by integrating large numbers of data sources on the Internet. On this "deep Web," we observe two distinguishing characteristics that offer a new view for considering schema matching: First, as the Web scales, there are ample sources that provide structured information in the same domains (e.g., books and automobiles). Second, while sources proliferate, their aggregate schema vocabulary tends to converge at a relatively small size. Motivated by these observations, we propose a new paradigm, statistical schema matching: Unlike traditional approaches using pairwise-attribute correspondence, we take a holistic approach to match all input schemas by finding an underlying generative schema model. We propose a general statistical framework MGS for such hidden model discovery, which consists of hypothesis modeling, generation, and selection. Further, we specialize the general framework to develop Algorithm MGSsd, targeting at synonym discovery, a canonical problem of schema matching, by designing and discovering a model that specifically captures synonym attributes. We demonstrate our approach over hundreds of real Web sources in four domains and the results show good accuracy.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {217–228},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872786,
author = {Deligiannakis, Antonios and Roussopoulos, Nick},
title = {Extended Wavelets for Multiple Measures},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872786},
doi = {10.1145/872757.872786},
abstract = {While work in recent years has demonstrated that wavelets can be efficiently used to compress large quantities of data and provide fast and fairly accurate answers to queries, little emphasis has been placed on using wavelets in approximating datasets containing multiple measures. Existing decomposition approaches will either operate on each measure individually, or treat all measures as a vector of values and process them simultaneously. We show in this paper that the resulting individual or combined storage approaches for the wavelet coefficients of different measures that stem from these existing algorithms may lead to suboptimal storage utilization, which results to reduced accuracy to queries. To alleviate this problem, we introduce in this work the notion of an extended wavelet coefficient as a flexible storage method for the wavelet coefficients, and propose novel algorithms for selecting which extended wavelet coefficients to retain under a given storage constraint. Experimental results with both real and synthetic datasets demonstrate that our approach achieves improved accuracy to queries when compared to existing techniques.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {229–240},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872787,
author = {Cohen, Saar and Matias, Yossi},
title = {Spectral Bloom Filters},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872787},
doi = {10.1145/872757.872787},
abstract = {A Bloom Filter is a space-efficient randomized data structure allowing membership queries over sets with certain allowable errors. It is widely used in many applications which take advantage of its ability to compactly represent a set, and filter out effectively any element that does not belong to the set, with small error probability. This paper introduces the Spectral Bloom Filter (SBF), an extension of the original Bloom Filter to multi-sets, allowing the filtering of elements whose multiplicities are below a threshold given at query time. Using memory only slightly larger than that of the original Bloom Filter, the SBF supports queries on the multiplicities of individual keys with a guaranteed, small error probability. The SBF also supports insertions and deletions over the data set. We present novel methods for reducing the probability and magnitude of errors. We also present an efficient data structure and algorithms to build it incrementally and maintain it over streaming data, as well as over materialized data with arbitrary insertions and deletions. The SBF does not assume any a priori filtering threshold and effectively and efficiently maintains information over the entire data-set, allowing for ad-hoc queries with arbitrary parameters and enabling a range of new applications.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {241–252},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872789,
author = {Babcock, Brian and Babu, Shivnath and Motwani, Rajeev and Datar, Mayur},
title = {Chain: Operator Scheduling for Memory Minimization in Data Stream Systems},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872789},
doi = {10.1145/872757.872789},
abstract = {In many applications involving continuous data streams, data arrival is bursty and data rate fluctuates over time. Systems that seek to give rapid or real-time query responses in such an environment must be prepared to deal gracefully with bursts in data arrival without compromising system performance. We discuss one strategy for processing bursty streams --- adaptive, load-aware scheduling of query operators to minimize resource consumption during times of peak load. We show that the choice of an operator scheduling strategy can have significant impact on the run-time system memory usage. We then present Chain scheduling, an operator scheduling strategy for data stream systems that is near-optimal in minimizing run-time memory usage for any collection of single-stream queries involving selections, projections, and foreign-key joins with stored relations. Chain scheduling also performs well for queries with sliding-window joins over multiple streams, and multiple queries of the above types. A thorough experimental evaluation is provided where we demonstrate the potential benefits of Chain scheduling, compare it with competing scheduling strategies, and validate our analytical conclusions.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {253–264},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872790,
author = {Ganguly, Sumit and Garofalakis, Minos and Rastogi, Rajeev},
title = {Processing Set Expressions over Continuous Update Streams},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872790},
doi = {10.1145/872757.872790},
abstract = {There is growing interest in algorithms for processing and querying continuous data streams (i.e., data that is seen only once in a fixed order) with limited memory resources. In its most general form, a data stream is actually an update stream, i.e., comprising data-item deletions as well as insertions. Such massive update streams arise naturally in several application domains (e.g., monitoring of large IP network installations, or processing of retail-chain transactions).Estimating the cardinality of set expressions defined over several (perhaps, distributed) update streams is perhaps one of the most fundamental query classes of interest; as an example, such a query may ask "what is the number of distinct IP source addresses seen in passing packets from both router R1 and R2 but not router R3?". Earlier work has only addressed very restricted forms of this problem, focusing solely on the special case of insert-only streams and specific operators (e.g., union). In this paper, we propose the first space-efficient algorithmic solution for estimating the cardinality of full-fledged set expressions over general update streams. Our estimation algorithms are probabilistic in nature and rely on a novel, hash-based synopsis data structure, termed "2-level hash sketch". We demonstrate how our 2-level hash sketch synopses can be used to provide low-error, high-confidence estimates for the cardinality of set expressions (including operators such as set union, intersection, and difference) over continuous update streams, using only small space and small processing time per update. Furthermore, our estimators never require rescanning or resampling of past stream items, regardless of the number of deletions in the stream. We also present lower bounds for the problem, demonstrating that the space usage of our estimation algorithms is within small factors of the optimal. Preliminary experimental results verify the effectiveness of our approach.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {265–276},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872792,
author = {Benedikt, Michael and Chan, Chee-Yong and Fan, Wenfei and Freire, Juliana and Rastogi, Rajeev},
title = {Capturing Both Types and Constraints in Data Integration},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872792},
doi = {10.1145/872757.872792},
abstract = {We propose a framework for integrating data from multiple relational sources into an XML document that both conforms to a given DTD and satisfies predefined XML constraints. The framework is based on a specification language, AIG, that extends a DTD by (1) associating element types with semantic attributes (inherited and synthesized, inspired by the corresponding notions from Attribute Grammars), (2) computing these attributes via parameterized SQL queries over multiple data sources, and (3) incorporating XML keys and inclusion constraints. The novelty of AIG consists in semantic attributes and their dependency relations for controlling context-dependent, DTD-directed construction of XML documents, as well as for checking XML constraints in parallel with document-generation. We also present cost-based optimization techniques for efficiently evaluating AIGs, including algorithms for merging queries and for scheduling queries on multiple data sources. This provides a new grammar-based approach for data integration under both syntactic and semantic constraints.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {277–288},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872793,
author = {Milo, Tova and Abiteboul, Serge and Amann, Bernd and Benjelloun, Omar and Ngoc, Fred Dang},
title = {Exchanging Intensional XML Data},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872793},
doi = {10.1145/872757.872793},
abstract = {XML is becoming the universal format for data exchange between applications. Recently, the emergence of Web services as standard means of publishing and accessing data on the Web introduced a new class of XML documents, which we call intensional documents. These are XML documents where some of the data is given explicitly while other parts are defined only intensionally by means of embedded calls to Web services.When such documents are exchanged between applications, one has the choice to materialize the intensional data (i.e. to invoke the embedded calls) or not, before the document is sent. This choice may be influenced by various parameters, such as performance and security considerations. This paper addresses the problem of guiding this materialization process.We argue that, just like for regular XML data, schemas (ala DTD and XML Schema) may be used to control the exchange of intensional data and, in particular, to determine which data should be materialized before sending a document, and which should not. We formalize the problem and provide algorithms to solve it. We also present an implementation that complies with real life standards for XML data, schemas, and Web services, and is used in the Active XML system [3, 1].},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {289–300},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872795,
author = {Fagin, Ronald and Kumar, Ravi and Sivakumar, D.},
title = {Efficient Similarity Search and Classification via Rank Aggregation},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872795},
doi = {10.1145/872757.872795},
abstract = {We propose a novel approach to performing efficient similarity search and classification in high dimensional data. In this framework, the database elements are vectors in a Euclidean space. Given a query vector in the same space, the goal is to find elements of the database that are similar to the query. In our approach, a small number of independent "voters" rank the database elements based on similarity to the query. These rankings are then combined by a highly efficient aggregation algorithm. Our methodology leads both to techniques for computing approximate nearest neighbors and to a conceptually rich alternative to nearest neighbors.One instantiation of our methodology is as follows. Each voter projects all the vectors (database elements and the query) on a random line (different for each voter), and ranks the database elements based on the proximity of the projections to the projection of the query. The aggregation rule picks the database element that has the best median rank. This combination has several appealing features. On the theoretical side, we prove that with high probability, it produces a result that is a (1 + ε) factor approximation to the Euclidean nearest neighbor. On the practical side, it turns out to be extremely efficient, often exploring no more than 5% of the data to obtain very high-quality results. This method is also database-friendly, in that it accesses data primarily in a pre-defined order without random accesses, and, unlike other methods for approximate nearest neighbors, requires almost no extra storage. Also, we extend our approach to deal with the k nearest neighbors.We conduct two sets of experiments to evaluate the efficacy of our methods. Our experiments include two scenarios where nearest neighbors are typically employed---similarity search and classification problems. In both cases, we study the performance of our methods with respect to several evaluation criteria, and conclude that they are uniformly excellent, both in terms of quality of results and in terms of efficiency.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {301–312},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872796,
author = {Chaudhuri, Surajit and Ganjam, Kris and Ganti, Venkatesh and Motwani, Rajeev},
title = {Robust and Efficient Fuzzy Match for Online Data Cleaning},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872796},
doi = {10.1145/872757.872796},
abstract = {To ensure high data quality, data warehouses must validate and cleanse incoming data tuples from external sources. In many situations, clean tuples must match acceptable tuples in reference tables. For example, product name and description fields in a sales record from a distributor must match the pre-recorded name and description fields in a product reference relation.A significant challenge in such a scenario is to implement an efficient and accurate fuzzy match operation that can effectively clean an incoming tuple if it fails to match exactly with any tuple in the reference relation. In this paper, we propose a new similarity function which overcomes limitations of commonly used similarity functions, and develop an efficient fuzzy match algorithm. We demonstrate the effectiveness of our techniques by evaluating them on real datasets.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {313–324},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872798,
author = {Kementsietsidis, Anastasios and Arenas, Marcelo and Miller, Ren\'{e}e J.},
title = {Mapping Data in Peer-to-Peer Systems: Semantics and Algorithmic Issues},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872798},
doi = {10.1145/872757.872798},
abstract = {We consider the problem of mapping data in peer-to-peer data-sharing systems. Such systems often rely on the use of mapping tables listing pairs of corresponding values to search for data residing in different peers. In this paper, we address semantic and algorithmic issues related to the use of mapping tables. We begin by arguing why mapping tables are appropriate for data mapping in a peer-to-peer environment. We discuss alternative semantics for these tables and we present a language that allows the user to specify mapping tables under different semantics. Then, we show that by treating mapping tables as constraints (called mapping constraints) on the exchange of information between peers it is possible to reason about them. We motivate why reasoning capabilities are needed to manage mapping tables and show the importance of inferring new mapping tables from existing ones. We study the complexity of this problem and we propose an efficient algorithm for its solution. Finally, we present an implementation along with experimental results that show that mapping tables may be managed efficiently in practice.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {325–336},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872799,
author = {Arasu, Arvind and Garcia-Molina, Hector},
title = {Extracting Structured Data from Web Pages},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872799},
doi = {10.1145/872757.872799},
abstract = {Many web sites contain large sets of pages generated using a common template or layout. For example, Amazon lays out the author, title, comments, etc. in the same way in all its book pages. The values used to generate the pages (e.g., the author, title,...) typically come from a database. In this paper, we study the problem of automatically extracting the database values from such template-generated web pages without any learning examples or other similar human input. We formally define a template, and propose a model that describes how values are encoded into pages using a template. We present an algorithm that takes, as input, a set of template-generated pages, deduces the unknown template used to generate the pages, and extracts, as output, the values encoded in the pages. Experimental evaluation on a large number of real input page collections indicates that our algorithm correctly extracts data in most cases.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {337–348},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872800,
author = {Stolte, Etzard and von Praun, Christoph and Alonso, Gustavo and Gross, Thomas},
title = {Scientific Data Repositories: Designing for a Moving Target},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872800},
doi = {10.1145/872757.872800},
abstract = {Managing scientific data warehouses requires constant adaptations to cope with changes in processing algorithms, computing environments, database schemas, and usage patterns. We have faced this challenge in the RHESSI Experimental Data Center (HEDC), a datacenter for the RHESSI NASA spacecraft. In this paper we describe our experience in developing HEDC and discuss in detail the design choices made. To successfully accommodate typical adaptations encountered in scientific data management systems, HEDC (i) clearly separates generic from domain specific code in all tiers, (ii) uses a file system for the actual data in combination with a DBMS to manage the corresponding meta data, and (iii) revolves around a middle tier designed to scale if more browsing or processing power is required. These design choices are valuable contributions as they address common concerns in a wide range of scientific data management systems.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {349–360},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872802,
author = {Chaudhuri, Surajit and Ganesan, Prasanna and Sarawagi, Sunita},
title = {Factorizing Complex Predicates in Queries to Exploit Indexes},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872802},
doi = {10.1145/872757.872802},
abstract = {Decision-support applications generate queries with complex predicates. We show how the factorization of complex query expressions exposes significant opportunities for exploiting available indexes. We also present a novel idea of relaxing predicates in a complex condition to create possibilities for factoring. Our algorithms are designed for easy integration with existing query optimizers and support multiple optimization levels, providing different trade-offs between plan complexity and optimization time.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {361–372},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872803,
author = {Ilyas, Ihab F. and Rao, Jun and Lohman, Guy and Gao, Dengfeng and Lin, Eileen},
title = {Estimating Compilation Time of a Query Optimizer},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872803},
doi = {10.1145/872757.872803},
abstract = {A query optimizer compares alternative plans in its search space to find the best plan for a given query. Depending on the search space and the enumeration algorithm, optimizers vary in their compilation time and the quality of the execution plan they can generate. This paper describes a compilation time estimator that provides a quantified estimate of the optimizer compilation time for a given query. Such an estimator is useful for automatically choosing the right level of optimization in commercial database systems. In addition, compilation time estimates can be quite helpful for mid-query reoptimization, for monitoring the progress of workload analysis tools where a large number queries need to be compiled (but not executed), and for judicious design and tuning of an optimizer.Previous attempts to estimate optimizer compilation complexity used the number of possible binary joins as the metric and overlooked the fact that each join often translates into a different number of join plans because of the presence of "physical" properties. We use the number of plans (instead of joins) to estimate query compilation time, and employ two novel ideas: (1) reusing an optimizer's join enumerator to obtain actual number of joins, but bypassing plan generation to save estimation overhead; (2) maintaining a small number of "interesting" properties to facilitate plan counting. We prototyped our approach in a commercial database system and our experimental results show that we can achieve good compilation time estimates (less than 30% error, on average) for complex real queries, using a small fraction (within 3%) of the actual compilation time.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {373–384},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872804,
author = {Reiss, Frederick R. and Kanungo, Tapas},
title = {A Characterization of the Sensitivity of Query Optimization to Storage Access Cost Parameters},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872804},
doi = {10.1145/872757.872804},
abstract = {Most relational query optimizers make use of information about the costs of accessing tuples and data structures on various storage devices. This information can at times be off by several orders of magnitude due to human error in configuration setup, sudden changes in load, or hardware failure. In this paper, we attempt to answer the following questions:• Are inaccurate access cost estimates likely to cause a typical query optimizer to choose a suboptimal query plan?• If an optimizer chooses a suboptimal plan as a result of inaccurate access cost estimates, how far from optimal is this plan likely to be?To address these issues, we provide a theoretical, vector-based framework for analyzing the costs of query plans under various storage parameter costs. We then use this geometric framework to characterize experimentally a commercial query optimizer. We develop algorithms for extracting detailed information about query plans through narrow optimizer interfaces, and we perform the characterization using database statistics from a published run of the TPC-H benchmark and a wide range of storage parameters.We show that, when data structures such as tables, indexes, and sorted runs reside on different storage devices, the optimizer can derive significant benefits from having accurate and timely information regarding the cost of accessing storage devices.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {385–396},
numpages = {12},
keywords = {parametric query optimization, databases, computational geometry, autonomic computing, storage systems},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872806,
author = {Lomet, David and Tuttle, Mark},
title = {A Theory of Redo Recovery},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872806},
doi = {10.1145/872757.872806},
abstract = {Our goal is to understand redo recovery. We define an installation graph of operations in an execution, an ordering significantly weaker than conflict ordering from concurrency control. The installation graph explains recoverable system state in terms of which operations are considered installed. This explanation and the set of operations replayed during recovery form an invariant that is the contract between normal operation and recovery. It prescribes how to coordinate changes to system components such as the state, the log, and the cache. We also describe how widely used recovery techniques are modeled in our theory, and why they succeed in providing redo recovery.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {397–406},
numpages = {10},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872807,
author = {Bierman, G. M.},
title = {Formal Semantics and Analysis of Object Queries},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872807},
doi = {10.1145/872757.872807},
abstract = {Modern database systems provide not only powerful data models but also complex query languages supporting powerful features such as the ability to create new database objects and invocation of arbitrary methods (possibly written in a third-party programming language).In this sense query languages have evolved into powerful programming languages. Surprisingly little work exists utilizing techniques from programming language research to specify and analyse these query languages. This paper provides a formal, high-level operational semantics for a complex-value OQL-like query language that can create fresh database objects, and invoke external methods. We define a type system for our query language and prove an important soundness property.We define a simple effect typing discipline to delimit the computational effects within our queries. We prove that this effect system is correct and show how it can be used to detect cases of non-determinism and to define correct query optimizations.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {407–418},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872809,
author = {Gupta, Ashish Kumar and Suciu, Dan},
title = {Stream Processing of XPath Queries with Predicates},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872809},
doi = {10.1145/872757.872809},
abstract = {We consider the problem of evaluating large numbers of XPath filters, each with many predicates, on a stream of XML documents. The solution we propose is to lazily construct a single deterministic pushdown automata, called the XPush Machine from the given XPath fllters. We describe a number of optimization techniques to make the lazy XPush machine more efficient, both in terms of space and time. The combination of these optimizations results in high, sustained throughput. For example, if the total number of atomic predicates in the filters is up to 200000, then the throughput is at least 0.5 MB/sec: it increases to 4.5 MB/sec when each fllter contains a single predicate.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {419–430},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872810,
author = {Peng, Feng and Chawathe, Sudarshan S.},
title = {XPath Queries on Streaming Data},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872810},
doi = {10.1145/872757.872810},
abstract = {We present the design and implementation of the XSQ system for querying streaming XML data using XPath 1.0. Using a clean design based on a hierarchical arrangement of pushdown transducers augmented with buffers, XSQ supports features such as multiple predicates, closures, and aggregation. XSQ not only provides high throughput, but is also memory efficient: It buffers only data that must be buffered by any streaming XPath processor. We also present an empirical study of the performance characteristics of XPath features, as embodied by XSQ and several other systems.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {431–442},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872812,
author = {Zhang, Jun and Zhu, Manli and Papadias, Dimitris and Tao, Yufei and Lee, Dik Lun},
title = {Location-Based Spatial Queries},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872812},
doi = {10.1145/872757.872812},
abstract = {In this paper we propose an approach that enables mobile clients to determine the validity of previous queries based on their current locations. In order to make this possible, the server returns in addition to the query result, a validity region around the client's location within which the result remains the same. We focus on two of the most common spatial query types, namely nearest neighbor and window queries, define the validity region in each case and propose the corresponding query processing algorithms. In addition, we provide analytical models for estimating the expected size of the validity region. Our techniques can significantly reduce the number of queries issued to the server, while introducing minimal computational and network overhead compared to traditional spatial queries.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {443–454},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872813,
author = {Sun, Chengyu and Agrawal, Divyakant and El Abbadi, Amr},
title = {Hardware Acceleration for Spatial Selections and Joins},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872813},
doi = {10.1145/872757.872813},
abstract = {Spatial database operations are typically performed in two steps. In the filtering step, indexes and the minimum bounding rectangles (MBRs) of the objects are used to quickly determine a set of candidate objects, and in the refinement step, the actual geometries of the objects are retrieved and compared to the query geometry or each other. Because of the complexity of the computational geometry algorithms involved, the CPU cost of the refinement step is usually the dominant cost of the operation for complex geometries such as polygons. In this paper, we propose a novel approach to address this problem using efficient rendering and searching capabilities of modern graphics hardware. This approach does not require expensive pre-processing of the data or changes to existing storage and index structures, and it applies to both intersection and distance predicates. Our experiments with real world datasets show that by combining hardware and software methods, the overall computational cost can be reduced substantially for both spatial selections and joins.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {455–466},
numpages = {12},
keywords = {spatial selection, spatial join, hardware acceleration},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872814,
author = {Papadias, Dimitris and Tao, Yufei and Fu, Greg and Seeger, Bernhard},
title = {An Optimal and Progressive Algorithm for Skyline Queries},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872814},
doi = {10.1145/872757.872814},
abstract = {The skyline of a set of d-dimensional points contains the points that are not dominated by any other point on all dimensions. Skyline computation has recently received considerable attention in the database community, especially for progressive (or online) algorithms that can quickly return the first skyline points without having to read the entire data file. Currently, the most efficient algorithm is NN (<u>n</u>earest <u>n</u>eighbors), which applies the divide -and-conquer framework on datasets indexed by R-trees. Although NN has some desirable features (such as high speed for returning the initial skyline points, applicability to arbitrary data distributions and dimensions), it also presents several inherent disadvantages (need for duplicate elimination if d&gt;2, multiple accesses of the same node, large space overhead). In this paper we develop BBS (<u>b</u>ranch-and-<u>b</u>ound <u>s</u>kyline), a progressive algorithm also based on nearest neighbor search, which is IO optimal, i.e., it performs a single access only to those R-tree nodes that may contain skyline points. Furthermore, it does not retrieve duplicates and its space overhead is significantly smaller than that of NN. Finally, BBS is simple to implement and can be efficiently applied to a variety of alternative skyline queries. An analytical and experimental comparison shows that BBS outperforms NN (usually by orders of magnitude) under all problem instances.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {467–478},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872815,
author = {Cui, Bin and Ooi, Beng Chin and Su, Jianwen and Tan, Kian-Lee},
title = {Contorting High Dimensional Data for Efficient Main Memory KNN Processing},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872815},
doi = {10.1145/872757.872815},
abstract = {In this paper, we present a novel index structure, called Δ-tree, to speed up processing of high-dimensional K-nearest neighbor (KNN) queries in main memory environment. The Δ-tree is a multi-level structure where each level represents the data space at different dimensionalities: the number of dimensions increases towards the leaf level which contains the data at their full dimensions. The remaining dimensions are obtained using Principal Component Analysis, which has the desirable property that the first few dimensions capture most of the information in the dataset. Each level of the tree serves to prune the search space more efficiently as the reduced dimensions can better exploit the small cache line size. Moreover, the distance computation on lower dimensionality is less expensive. We also propose an extension, called Δ+-tree, that globally clusters the data space and then further partitions clusters into small regions to reduce the search space. We conducted extensive experiments to evaluate the proposed structures against existing techniques on different kinds of datasets. Our results show that the Δ+-tree is superior in most cases.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {479–490},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872817,
author = {Madden, Samuel and Franklin, Michael J. and Hellerstein, Joseph M. and Hong, Wei},
title = {The Design of an Acquisitional Query Processor for Sensor Networks},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872817},
doi = {10.1145/872757.872817},
abstract = {We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {491–502},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872818,
author = {Deshpande, Amol and Nath, Suman and Gibbons, Phillip B. and Seshan, Srinivasan},
title = {Cache-and-Query for Wide Area Sensor Databases},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872818},
doi = {10.1145/872757.872818},
abstract = {Webcams, microphones, pressure gauges and other sensors provide exciting new opportunities for querying and monitoring the physical world. In this paper we focus on querying wide area sensor databases, containing (XML) data derived from sensors spread over tens to thousands of miles. We present the first scalable system for executing XPATH queries on such databases. The system maintains the logical view of the data as a single XML document, while physically the data is fragmented across any number of host nodes. For scalability, sensor data is stored close to the sensors, but can be cached elsewhere as dictated by the queries. Our design enables self starting distributed queries that jump directly to the lowest common ancestor of the query result, dramatically reducing query response times. We present a novel query-evaluate gather technique (using XSLT) for detecting (1) which data in a local database fragment is part of the query result, and (2) how to gather the missing parts. We define partitioning and cache invariants that ensure that even partial matches on cached data are exploited and that correct answers are returned, despite our dynamic query-driven caching. Experimental results demonstrate that our techniques dramatically increase query throughputs and decrease query response times in wide area sensor databases.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {503–514},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872820,
author = {Li, Chengkai and Bohannon, Philip and Narayan, P. P. S.},
title = {Composing XSL Transformations with XML Publishing Views},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872820},
doi = {10.1145/872757.872820},
abstract = {While the XML Stylesheet Language for Transformations (XSLT) was not designed as a query language, it is well-suited for many query-like operations on XML documents including selecting and restructuring data. Further, it actively fulfills the role of an XML query language in modern applications and is widely supported by application platform software. However, the use of database techniques to optimize and execute XSLT has only recently received attention in the research community. In this paper, we focus on the case where XSL transformations are to be run on XML documents defined as views of relational databases. For a subset of XSLT, we present an algorithm to compose a transformation with an XML view, eliminating the need for the XSLT execution. We then describe how to extend this algorithm to handle several additional features of XSLT, including a proposed approach for handling recursion.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {515–526},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872821,
author = {Abiteboul, Serge and Bonifati, Angela and Cob\'{e}na, Gr\'{e}gory and Manolescu, Ioana and Milo, Tova},
title = {Dynamic XML Documents with Distribution and Replication},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872821},
doi = {10.1145/872757.872821},
abstract = {The advent of XML as a universal exchange format, and of Web services as a basis for distributed computing, has fostered the apparition of a new class of documents: dynamic XML documents. These are XML documents where some data is given explicitly while other parts are given only intensionally by means of embedded calls to web services that can be called to generate the required information. By the sole presence of Web services, dynamic documents already include inherently some form of distributed computation. A higher level of distribution that also allows (fragments of) dynamic documents to be distributed and/or replicated over several sites is highly desirable in today's Web architecture, and in fact is also relevant for regular (non dynamic) documents.The goal of this paper is to study new issues raised by the distribution and replication of dynamic XML data. Our study has originated in the context of the Active XML system [1, 3, 22] but the results are applicable to many other systems supporting dynamic XML data. Starting from a data model and a query language, we describe a complete framework for distributed and replicated dynamic XML documents. We provide a comprehensive cost model for query evaluation and show how it applies to user queries and service calls. Finally, we describe an algorithm that, for a given peer, chooses data and services that the peer should replicate to improve the efficiency of maintaining and querying its dynamic data.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {527–538},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872822,
author = {Babcock, Brian and Chaudhuri, Surajit and Das, Gautam},
title = {Dynamic Sample Selection for Approximate Query Processing},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872822},
doi = {10.1145/872757.872822},
abstract = {In decision support applications, the ability to provide fast approximate answers to aggregation queries is desirable. One commonly-used technique for approximate query answering is sampling. For many aggregation queries, appropriately constructed biased (non-uniform) samples can provide more accurate approximations than a uniform sample. The optimal type of bias, however, varies from query to query. In this paper, we describe an approximate query processing technique that dynamically constructs an appropriately biased sample for each query by combining samples selected from a family of non-uniform samples that are constructed during a pre-processing phase. We show that dynamic selection of appropriate portions of previously constructed samples can provide more accurate approximate answers than static, non-adaptive usage of uniform or non-uniform samples.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {539–550},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872823,
author = {Cheng, Reynold and Kalashnikov, Dmitri V. and Prabhakar, Sunil},
title = {Evaluating Probabilistic Queries over Imprecise Data},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872823},
doi = {10.1145/872757.872823},
abstract = {Many applications employ sensors for monitoring entities such as temperature and wind speed. A centralized database tracks these entities to enable query processing. Due to continuous changes in these values and limited resources (e.g., network bandwidth and battery power), it is often infeasible to store the exact values at all times. A similar situation exists for moving object environments that track the constantly changing locations of objects. In this environment, it is possible for database queries to produce incorrect or invalid results based upon old data. However, if the degree of error (or uncertainty) between the actual value and the database value is controlled, one can place more confidence in the answers to queries. More generally, query answers can be augmented with probabilistic estimates of the validity of the answers. In this paper we study probabilistic query evaluation based upon uncertain data. A classification of queries is made based upon the nature of the result set. For each class, we develop algorithms for computing probabilistic answers. We address the important issue of measuring the quality of the answers to these queries, and provide algorithms for efficiently pulling data from relevant sensors or moving objects in order to improve the quality of the executing queries. Extensive experiments are performed to examine the effectiveness of several data update policies.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {551–562},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872825,
author = {Olston, Chris and Jiang, Jing and Widom, Jennifer},
title = {Adaptive Filters for Continuous Queries over Distributed Data Streams},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872825},
doi = {10.1145/872757.872825},
abstract = {We consider an environment where distributed data sources continuously stream updates to a centralized processor that monitors continuous queries over the distributed data. Significant communication overhead is incurred in the presence of rapid update streams, and we propose a new technique for reducing the overhead. Users register continuous queries with precision requirements at the central stream processor, which installs filters at remote data sources. The filters adapt to changing conditions to minimize stream rates while guaranteeing that all continuous queries still receive the updates necessary to provide answers of adequate precision at all times. Our approach enables applications to trade precision for communication overhead at a fine granularity by individually adjusting the precision constraints of continuous queries over streams in a multi-query workload. Through experiments performed on synthetic data simulations and a real network monitoring implementation, we demonstrate the effectiveness of our approach in achieving low communication overhead compared with alternate approaches.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {563–574},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872826,
author = {Aggarwal, Charu C.},
title = {A Framework for Diagnosing Changes in Evolving Data Streams},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872826},
doi = {10.1145/872757.872826},
abstract = {In recent years, the progress in hardware technology has made it possible for organizations to store and record large streams of transactional data. This results in databases which grow without limit at a rapid rate. This data can often show important changes in trends over time. In such cases, it is useful to understand, visualize and diagnose the evolution of these trends. When the data streams are fast and continuous, it becomes important to analyze and predict the trends quickly in online fashion. In this paper, we discuss the concept of velocity density estimation, a technique used to understand, visualize and determine trends in the evolution of fast data streams. We show how to use velocity density estimation in order to create both temporal velocity profiles and spatial velocity profiles at periodic instants in time. These profiles are then used in order to predict three kinds of data evolution: dissolution, coagulation and shift. Methods are proposed to visualize the changing data trends in a single online scan of the data stream, and a computational requirement which is linear in the number of data points. In addition, batch processing techniques are proposed in order to identify combinations of dimensions which show the greatest amount of global evolution. The techniques discussed in this paper can be easily extended to spatio-temporal data, changes in data snapshots at fixed instances in time, or any other data which has a temporal component during its evolution.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {575–586},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872828,
author = {Kriegel, Hans-Peter and Brecheisen, Stefan and Kr\"{o}ger, Peer and Pfeifle, Martin and Schubert, Matthias},
title = {Using Sets of Feature Vectors for Similarity Search on Voxelized CAD Objects},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872828},
doi = {10.1145/872757.872828},
abstract = {In modern application domains such as multimedia, molecular biology and medical imaging, similarity search in database systems is becoming an increasingly important task. Especially for CAD applications, suitable similarity models can help to reduce the cost of developing and producing new parts by maximizing the reuse of existing parts. Most of the existing similarity models are based on feature vectors. In this paper, we shortly review three models which pursue this paradigm. Based on the most promising of these three models, we explain how sets of feature vectors can be used for more effective and still efficient similarity search. We first introduce an intuitive distance measure on sets of feature vectors together with an algorithm for its efficient computation. Furthermore, we present a method for accelerating the processing of similarity queries on vector set data. The experimental evaluation is based on two real world test data sets and points out that our new similarity approach yields more meaningful results in comparatively short time.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {587–598},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872829,
author = {Kim, Deok-Hwan and Chung, Chin-Wan},
title = {QCluster: Relevance Feedback Using Adaptive Clustering for Content-Based Image Retrieval},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872829},
doi = {10.1145/872757.872829},
abstract = {The learning-enhanced relevance feedback has been one of the most active research areas in content-based image retrieval in recent years. However, few methods using the relevance feedback are currently available to process relatively complex queries on large image databases. In the case of complex image queries, the feature space and the distance function of the user's perception are usually different from those of the system. This difference leads to the representation of a query with multiple clusters (i.e., regions) in the feature space. Therefore, it is necessary to handle disjunctive queries in the feature space.In this paper, we propose a new content-based image retrieval method using adaptive classification and cluster-merging to find multiple clusters of a complex image query. When the measures of a retrieval method are invariant under linear transformations, the method can achieve the same retrieval quality regardless of the shapes of clusters of a query. Our method achieves the same high retrieval quality regardless of the shapes of clusters of a query since it uses such measures. Extensive experiments show that the result of our method converges to the user's true information need fast, and the retrieval quality of our method is about 22% in recall and 20% in precision better than that of the query expansion approach, and about 34% in recall and about 33% in precision better than that of the query point movement approach, in MARS.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {599–610},
numpages = {12},
keywords = {classification, image database, content-based image retrieval, cluster-merging, relevance feedback},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872831,
author = {Chaudhuri, Surajit and Kaushik, Raghav and Naughton, Jeffrey F.},
title = {On Relational Support for XML Publishing: Beyond Sorting and Tagging},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872831},
doi = {10.1145/872757.872831},
abstract = {In this paper, we study whether the need for efficient XML publishing brings any new requirements for relational query engines, or if sorting query results in the relational engine and tagging them in middleware is sufficient. We observe that the mismatch between the XML data model and the relational model requires relational engines to be enhanced for efficiency. Specifically, they need to support relation valued variables. We discuss how such support can be provided through the addition of an operator, GApply, with minimal extensions to existing relational engines. We discuss how the operator may be exposed in SQL syntax and provide a comprehensive study of optimization rules that govern this operator. We report the results of a preliminary performance evaluation showing the speedup obtained through our approach and the effectiveness of our optimization rules.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {611–622},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872832,
author = {DeHaan, David and Toman, David and Consens, Mariano P. and \"{O}zsu, M. Tamer},
title = {A Comprehensive XQuery to SQL Translation Using Dynamic Interval Encoding},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872832},
doi = {10.1145/872757.872832},
abstract = {The W3C XQuery language recommendation, based on a hierarchical and ordered document model, supports a wide variety of constructs and use cases. There is a diversity of approaches and strategies for evaluating XQuery expressions, in many cases only dealing with limited subsets of the language. In this paper we describe an implementation approach that handles XQuery with arbitrarily-nested FLWR expressions, element constructors and built-in functions (including structural comparisons). Our proposal maps an XQuery expression to a single equivalent SQL query using a novel dynamic interval encoding of a collection of XML documents as relations, augmented with information tied to the query evaluation environment. The dynamic interval technique enables (suitably enhanced) relational engines to produce predictably good query plans that do not preclude the use of sort-merge join query operators. The benefits are realized despite the challenges presented by intermediate results that create arbitrary documents and the need to preserve document order as prescribed by semantics of XQuery. Finally, our experimental results demonstrate that (native or relational) XML systems can benefit from the above technique to avoid a quadratic scale up penalty that effectively prevents the evaluation of nested FLWR expressions for large documents.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {623–634},
numpages = {12},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872833,
author = {Ives, Zachary},
title = {Abstracts of Invited Industrial Track Presentations},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872833},
doi = {10.1145/872757.872833},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {635–636},
numpages = {2},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872835,
author = {Padmanabhan, Sriram and Bhattacharjee, Bishwaranjan and Malkemus, Tim and Cranston, Leslie and Huras, Matthew},
title = {Multi-Dimensional Clustering: A New Data Layout Scheme in DB2},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872835},
doi = {10.1145/872757.872835},
abstract = {We describe the design and implementation of a new data layout scheme, called multi-dimensional clustering, in DB2 Universal Database Version 8. Many applications, e.g., OLAP and data warehousing, process a table or tables in a database using a multi-dimensional access paradigm. Currently, most database systems can only support organization of a table using a primary clustering index. Secondary indexes are created to access the tables when the primary key index is not applicable. Unfortunately, secondary indexes perform many random I/O accesses against the table for a simple operation such as a range query. Our work in multi-dimensional clustering addresses this important deficiency in database systems. Multi-Dimensional Clustering is based on the definition of one or more orthogonal clustering attributes (or expressions) of a table. The table is organized physically by associating records with similar values for the dimension attributes in a cluster. We describe novel techniques for maintaining this physical layout efficiently and methods of processing database operations that provide significant performance improvements. We show results from experiments using a star-schema database to validate our claims of performance with minimal overhead.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {637–641},
numpages = {5},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872837,
author = {Goto, Koichi and Kambayashi, Yahiko},
title = {Integration of Electronic Tickets and Personal Guide System for Public Transport Using Mobile Terminals},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872837},
doi = {10.1145/872757.872837},
abstract = {We have been developing a mobile passenger guide system for public transports. Passengers can make their travel plans and purchase necessary electronic tickets using mobile terminals via Internet. During the travel, the mobile terminal, which also works as an electronic ticket, compares the stored travel plan with the passenger's actual activities and offers appropriate guide messages. To execute this task, the mobile terminal collects various kinds of information about the travel fields (routes, fares, area maps, station maps, operation schedule, timetables, facilities of stations and vehicles etc.) using multi-channel data communications. The mobile terminal contains a personal database for the passenger by selecting and integrating necessary data according to the user's situation and characteristics.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {642–646},
numpages = {5},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872838,
author = {Cranor, Chuck and Johnson, Theodore and Spataschek, Oliver and Shkapenyuk, Vladislav},
title = {Gigascope: A Stream Database for Network Applications},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872838},
doi = {10.1145/872757.872838},
abstract = {We have developed Gigascope, a stream database for network applications including traffic analysis, intrusion detection, router configuration analysis, network research, network monitoring, and performance monitoring and debugging. Gigascope is undergoing installation at many sites within the AT&amp;T network, including at OC48 routers, for detailed monitoring. In this paper we describe our motivation for and constraints in developing Gigascope, the Gigascope architecture and query language, and performance issues. We conclude with a discussion of stream database research problems we have found in our application.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {647–651},
numpages = {5},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872840,
author = {Zuzarte, Calisto and Pirahesh, Hamid and Ma, Wenbin and Cheng, Qi and Liu, Linqi and Wong, Kwai},
title = {WinMagic: Subquery Elimination Using Window Aggregation},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872840},
doi = {10.1145/872757.872840},
abstract = {Database queries often take the form of correlated SQL queries. Correlation refers to the use of values from the outer query block to compute the inner subquery. This is a convenient paradigm for SQL programmers and closely mimics a function invocation paradigm in a typical computer programming language. Queries with correlated subqueries are also often created by SQL generators that translate queries from application domain-specific languages into SQL. Another significant class of queries that use this correlated subquery form is that involving temporal databases using SQL. Performance of these queries is an important consideration particularly in large databases. Several proposals to improve the performance of SQL queries containing correlated subqueries can be found in database literature. One of the main ideas in many of these proposals is to suitably decorrelate the subquery internally to avoid a tuple-at-a-time invocation of the subquery. Magic decorrelation is one method that has been successfully used. Another proposal is to cache the portion of the subquery that is invariant with the changing values of the outer query block. What we propose here is a new technique to handle some typical correlated queries. We go a step further than to simply decorrelate the subquery. By making use of extended window aggregation capabilities, we eliminate redundant access to common tables referenced in the outer query block and the subquery. This technique can be exploited even for non-correlated subqueries. It is possible to get a huge boost in performance for queries that can exploit this technique, which we call WinMagic. This technique was implemented in IBM® DB2® Universal Database" Version 7 and Version 8. In addition to improving DB2 customer queries that contain aggregation subqueries, it has provided significant improvements in a number of TPCH benchmarks that IBM has published since late in 2001.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {652–656},
numpages = {5},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872842,
author = {Shen, Jialie and Ngu, Anne H. H. and Shepherd, John and Huynh, Du Q. and Sheng, Quan Z.},
title = {CMVF: A Novel Dimension Reduction Scheme for Efficient Indexing in a Large Image Database},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872842},
doi = {10.1145/872757.872842},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {657},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872843,
author = {Lakshmanan, Laks V. S. and Pei, Jian and Zhao, Yan},
title = {SOCQET: Semantic OLAP with Compressed Cube and Summarization},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872843},
doi = {10.1145/872757.872843},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {658},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872845,
author = {Ooi, Beng Chin and Tan, Kian-Lee and Zhou, Aoying and Goh, Chin Hong and Li, Yingguang and Liau, Chu Yee and Ling, Bo and Ng, Wee Siong and Shu, Yanfeng and Wang, Xiaoyu and Zhang, Ming},
title = {PeerDB: Peering into Personal Databases},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872845},
doi = {10.1145/872757.872845},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {659},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872846,
author = {Liu, David T. and Franklin, Michael J. and Parekh, Devesh},
title = {GridDB: A Relational Interface for the Grid},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872846},
doi = {10.1145/872757.872846},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {660},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872848,
author = {Larson, Per-\r{A}ke and Goldstein, Jonathan and Zhou, Jingren},
title = {Transparent Mid-Tier Database Caching in SQL Server},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872848},
doi = {10.1145/872757.872848},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {661},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872849,
author = {Bornh\"{o}vd, Christof and Altinel, Mehmet and Krishnamurthy, Sailesh and Mohan, C. and Pirahesh, Hamid and Reinwald, Berthold},
title = {DBCache: Middle-Tier Database Caching for Highly Scalable e-Business Architectures},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872849},
doi = {10.1145/872757.872849},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {662},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872851,
author = {Agichtein, Eugene and Gravano, Luis},
title = {QXtract: A Building Block for Efficient Information Extraction from Text Databases},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872851},
doi = {10.1145/872757.872851},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {663},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872852,
author = {Amer-Yahia, Sihem and Fern\'{a}ndez, Mary and Srivastava, Divesh and Xu, Yu},
title = {PIX: Exact and Approximate Phrase Matching in XML},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872852},
doi = {10.1145/872757.872852},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {664},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872854,
author = {Arasu, Arvind and Babcock, Brian and Babu, Shivnath and Datar, Mayur and Ito, Keith and Nishizawa, Itaru and Rosenstein, Justin and Widom, Jennifer},
title = {STREAM: The Stanford Stream Data Manager (Demonstration Description)},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872854},
doi = {10.1145/872757.872854},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {665},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872855,
author = {Abadi, D. and Carney, D. and \c{C}etintemel, U. and Cherniack, M. and Convey, C. and Erwin, C. and Galvez, E. and Hatoun, M. and Maskey, A. and Rasin, A. and Singer, A. and Stonebraker, M. and Tatbul, N. and Xing, Y. and Yan, R. and Zdonik, S.},
title = {Aurora: A Data Stream Management System},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872855},
doi = {10.1145/872757.872855},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {666},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872856,
author = {Deshpande, Amol and Nath, Suman and Gibbons, Phillip B. and Seshan, Srinivasan},
title = {IrisNet: Internet-Scale Resource-Intensive Sensor Services},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872856},
doi = {10.1145/872757.872856},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {667},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872857,
author = {Chandrasekaran, Sirish and Cooper, Owen and Deshpande, Amol and Franklin, Michael J. and Hellerstein, Joseph M. and Hong, Wei and Krishnamurthy, Sailesh and Madden, Samuel R. and Reiss, Fred and Shah, Mehul A.},
title = {TelegraphCQ: Continuous Dataflow Processing},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872857},
doi = {10.1145/872757.872857},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {668},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872859,
author = {Cho, SungRan and Amer-Yahia, Sihem and Lakshmanan, Laks V. S. and Srivastava, Divesh},
title = {LockX: A System for Efficiently Querying Secure XML},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872859},
doi = {10.1145/872757.872859},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {669},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872860,
author = {Zhou, Aoying and Wang, Qing and Guo, Zhimao and Gong, Xueqing and Zheng, Shihui and Wu, Hongwei and Xiao, Jianchang and Yue, Kun and Fan, Wenfei},
title = {TREX: DTD-Conforming XML to XML Transformations},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872860},
doi = {10.1145/872757.872860},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {670},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872861,
author = {Zhang, Xin and Dimitrova, Katica and Wang, Ling and El Sayed, Maged and Murphy, Brian and Pielech, Bradford and Mulchandani, Mukesh and Ding, Luping and Rundensteiner, Elke A.},
title = {Rainbow: Multi-XQuery Optimization Using Materialized XML Views},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872861},
doi = {10.1145/872757.872861},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {671},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872862,
author = {Paparizos, Stelios and Al-Khalifa, Shurug and Chapman, Adriane and Jagadish, H. V. and Lakshmanan, Laks V. S. and Nierman, Andrew and Patel, Jignesh M. and Srivastava, Divesh and Wiwatwattana, Nuwee and Wu, Yuqing and Yu, Cong},
title = {TIMBER: A Native System for Querying XML},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872862},
doi = {10.1145/872757.872862},
abstract = {XML has become ubiquitous, and XML data has to be managed in databases. The current industry standard is to map XML data into relational tables and store this information in a relational database. Such mappings create both expressive power problems and performance problems.In the TIMBER [7] project we are exploring the issues involved in storing XML in native format. We believe that the key intellectual contribution of this system is a comprehensive set-at-a-time query processing ability in a native XML store, with all the standard components of relational query processing, including algebraic rewriting and a cost-based optimizer.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {672},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872863,
author = {Bohannon, Philip and Dong, Xin (Luna) and Ganguly, Sumit and Korth, Henry F. and Li, Chengkai and Narayan, P. P. S. and Shenoy, Pradeep},
title = {ROLEX: Relational on-Line Exchange with XML},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872863},
doi = {10.1145/872757.872863},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {673},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872865,
author = {Agrawal, Rakesh and Haas, Peter J. and Kiernan, Jerry},
title = {A System for Watermarking Relational Databases},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872865},
doi = {10.1145/872757.872865},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {674},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872866,
author = {Zhu, Yunyue and Shasha, Dennis and Zhao, Xiaojian},
title = {Query by Humming: In Action with Its Technology Revealed},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872866},
doi = {10.1145/872757.872866},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {675},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872867,
author = {Sengar, Vibhuti S. and Haritsa, Jayant R.},
title = {PLASTIC: Reducing Query Optimization Overheads through Plan Recycling},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872867},
doi = {10.1145/872757.872867},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {676},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872868,
author = {Korn, Flip and Muthukrishnan, S. and Zhu, Yunyue},
title = {IPSOFACTO: A Visual Correlation Tool for Aggregate Network Traffic Data},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872868},
doi = {10.1145/872757.872868},
abstract = {IP network operators collect aggregate traffic statistics on network interfaces via the Simple Network Management Protocol (SNMP). This is part of routine network operations for most ISPs; it involves a large infrastructure with multiple network management stations polling information from all the network elements and collating a real time data feed. This demo will present a tool that manages the live SNMP data feed on a fully operational large ISP at industry scale. The tool primarily serves to study correlations in the network traffic, by providing a rich mix of ad-hoc querying based on a user-friendly correlation interface and as well as canned queries, based on the expertise of the network operators with field experience. The tool is called IPSOFACTO for IP Stream-Oriented FAst Correlation TOol.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {677},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872869,
author = {Gupta, Amarnath and Lud\"{a}scher, Bertram and Martone, Maryann E.},
title = {BIRN-M: A Semantic Mediator for Solving Real-World Neuroscience Problems},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872869},
doi = {10.1145/872757.872869},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {678},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872871,
author = {Koudas, Nick and Srivastava, Divesh},
title = {Panel: Querying Networked Databases},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872871},
doi = {10.1145/872757.872871},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {679},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872873,
author = {Gray, Jim and Schek, Hans and Stonebraker, Michael and Ullman, Jeff},
title = {The Lowell Report},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872873},
doi = {10.1145/872757.872873},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {680},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872875,
author = {Johnson, Theodore and Dasu, Tamraparni},
title = {Data Quality and Data Cleaning: An Overview},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872875},
doi = {10.1145/872757.872875},
abstract = {Data quality is a serious concern in any data-driven enterprise, often creating misleading findings during data mining, and causing process disruptions in operational databases. The manifestations of data quality problems can be very expensive- "losing" customers, "misplacing" billions of dollars worth of equipment, misallocated resources due to glitched forecasts, and so on. Solving data quality problems typically requires a very large investment of time and energy -- often 80% to 90% of a data analysis project is spent in making the data reliable enough that the results can be trusted.In this tutorial, we present a multi disciplinary approach to data quality problems. We start by discussing the meaning of data quality and the sources of data quality problems. We show how these problems can be addressed by a multidisciplinary approach, combining techniques from management science, statistics, database research, and metadata management. Next, we present an updated definition of data quality metrics, and illustrate their application with a case study. We conclude with a survey of recent database research that is relevant to data quality problems, and suggest directions for future research.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {681},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872877,
author = {Chamberlin, Don},
title = {XQuery: A Query Language for XML},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872877},
doi = {10.1145/872757.872877},
abstract = {XQuery is the XML query language currently under development in the World Wide Web Consortium (W3C). XQuery specifications have been published in a series of W3C working drafts, and several reference implementations of the language are already available on the Web. If successful, XQuery has the potential to be one of the most important new computer languages to be introduced in several years. This tutorial will provide an overview of the syntax and semantics of XQuery, as well as insight into the principles that guided the design of the language.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {682},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/872757.872878,
author = {Jagatheesan, Arun and Rajasekar, Arcot},
title = {Data Grid Management Systems},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872878},
doi = {10.1145/872757.872878},
abstract = {Data Grids are being built across the world as the next generation data handling systems to manage peta-bytes of inter organizational data and storage space. A data grid (datagrid) is a logical name space consisting of storage resources and digital entities that is created by the cooperation of autonomous organizations and its users based on the coordination of local and global policies. Data Grid Management Systems (DGMSs) provide services for the confluence of organizations and management of inter-organizational data and resources in the datagrid.The objective of the tutorial is to provide an introduction to the opportunities and challenges of this emerging technology. Novices and experts would benefit from this tutorial. The tutorial would cover introduction, use cases, design philosophies, architecture, research issues, existing technologies and demonstrations. Hands on sessions for the participants to use and feel the existing technologies could be provided based on the availability of internet connections.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {683},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

