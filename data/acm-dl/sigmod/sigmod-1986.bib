@inproceedings{10.1145/16894.16857,
author = {Roussopoulos, Nick},
title = {Engineering Information Systems (Panel Session): Builders and Designers Perspective},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16857},
doi = {10.1145/16894.16857},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {1–3},
numpages = {3},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16857,
author = {Roussopoulos, Nick},
title = {Engineering Information Systems (Panel Session): Builders and Designers Perspective},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16857},
doi = {10.1145/16856.16857},
journal = {SIGMOD Rec.},
month = jun,
pages = {1–3},
numpages = {3}
}

@inproceedings{10.1145/16894.16858,
author = {Buneman, Peter and Atkinson, Malcolm},
title = {Inheritance and Persistence in Database Programming Languages},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16858},
doi = {10.1145/16894.16858},
abstract = {In order to represent inheritance, several recent designs for database programming languages have made use of class construct, which can be thought of as a restricted data type with an associated set of instances. Moreover, these classes are persistent they survive from one program invocation to another. This paper examines whether it is necessary to the together type, extent and persistence in order to model inheritance and suggests that they may be separated to provide more general database programming languages. In particular we shall see that it is possible to assign a generic data type to a function that extracts all the objects of a given type in the database so that the class hierarchy can be derived from the type hierarchy. We shall also examine object-level inheritance and its relationship to data types for relational databases. A final section examines how the various forms of persistence interact with inheritance at both object and type level.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {4–15},
numpages = {12},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16858,
author = {Buneman, Peter and Atkinson, Malcolm},
title = {Inheritance and Persistence in Database Programming Languages},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16858},
doi = {10.1145/16856.16858},
abstract = {In order to represent inheritance, several recent designs for database programming languages have made use of class construct, which can be thought of as a restricted data type with an associated set of instances. Moreover, these classes are persistent they survive from one program invocation to another. This paper examines whether it is necessary to the together type, extent and persistence in order to model inheritance and suggests that they may be separated to provide more general database programming languages. In particular we shall see that it is possible to assign a generic data type to a function that extracts all the objects of a given type in the database so that the class hierarchy can be derived from the type hierarchy. We shall also examine object-level inheritance and its relationship to data types for relational databases. A final section examines how the various forms of persistence interact with inheritance at both object and type level.},
journal = {SIGMOD Rec.},
month = jun,
pages = {4–15},
numpages = {12}
}

@inproceedings{10.1145/16894.16859,
author = {Bancilhon, Francois and Ramakrishnan, Raghu},
title = {An Amateur's Introduction to Recursive Query Processing Strategies},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16859},
doi = {10.1145/16894.16859},
abstract = {This paper surveys and compares various strategies for processing logic queries in relational databases. The survey and comparison is limited to the case of Horn Clauses with evaluable predicates but without function symbols. The paper is organized in three parts. In the first part, we introduce the main concepts and definitions. In the second, we describe the various strategies. For each strategy, we give its main characteristics, its application range and a detailed description. We also give an example of a query evaluation. The third part of the paper compares the strategies on performance grounds. We first present a set of sample rules and queries which are used for the performance comparisons, and then we characterize the data. Finally, we give an analytical solution for each query/rule system. Cost curves are plotted for specific configurations of the data.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {16–52},
numpages = {37},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16859,
author = {Bancilhon, Francois and Ramakrishnan, Raghu},
title = {An Amateur's Introduction to Recursive Query Processing Strategies},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16859},
doi = {10.1145/16856.16859},
abstract = {This paper surveys and compares various strategies for processing logic queries in relational databases. The survey and comparison is limited to the case of Horn Clauses with evaluable predicates but without function symbols. The paper is organized in three parts. In the first part, we introduce the main concepts and definitions. In the second, we describe the various strategies. For each strategy, we give its main characteristics, its application range and a detailed description. We also give an example of a query evaluation. The third part of the paper compares the strategies on performance grounds. We first present a set of sample rules and queries which are used for the performance comparisons, and then we characterize the data. Finally, we give an analytical solution for each query/rule system. Cost curves are plotted for specific configurations of the data.},
journal = {SIGMOD Rec.},
month = jun,
pages = {16–52},
numpages = {37}
}

@inproceedings{10.1145/16894.16860,
author = {Lindsay, Bruce and Haas, Laura and Mohan, C. and Pirahesh, Hamid and Wilms, Paul},
title = {A Snapshot Differential Refresh Algorithm},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16860},
doi = {10.1145/16894.16860},
abstract = {This article presents an algorithm to refresh the contents of database snapshots. A database snapshot is a read-only table whose contents are extracted from other tables in the database. The snapshot contents can be periodically refreshed to reflect the current state of the database. Snapshots are useful in many applications as a cost effective substitute for replicated data in a distributed database system.When the snapshot contents are a simple restriction and projection of a single base table, differential refresh techniques can reduce the message and update costs of the snapshot refresh operation. The algorithm presented annotates the base table to detect the changes which must be applied to the snapshot table during snapshot refresh. The cost of maintaining the base table annotations is minimal and the amount of data transmitted during snapshot refresh is close to optimal in most circumstances.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {53–60},
numpages = {8},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16860,
author = {Lindsay, Bruce and Haas, Laura and Mohan, C. and Pirahesh, Hamid and Wilms, Paul},
title = {A Snapshot Differential Refresh Algorithm},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16860},
doi = {10.1145/16856.16860},
abstract = {This article presents an algorithm to refresh the contents of database snapshots. A database snapshot is a read-only table whose contents are extracted from other tables in the database. The snapshot contents can be periodically refreshed to reflect the current state of the database. Snapshots are useful in many applications as a cost effective substitute for replicated data in a distributed database system.When the snapshot contents are a simple restriction and projection of a single base table, differential refresh techniques can reduce the message and update costs of the snapshot refresh operation. The algorithm presented annotates the base table to detect the changes which must be applied to the snapshot table during snapshot refresh. The cost of maintaining the base table annotations is minimal and the amount of data transmitted during snapshot refresh is close to optimal in most circumstances.},
journal = {SIGMOD Rec.},
month = jun,
pages = {53–60},
numpages = {8}
}

@inproceedings{10.1145/16894.16861,
author = {Blakeley, Jose A. and Larson, Per-Ake and Tompa, Frank Wm},
title = {Efficiently Updating Materialized Views},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16861},
doi = {10.1145/16894.16861},
abstract = {Query processing can be sped up by keeping frequently accessed users' views materialized. However, the need to access base relations in response to queries can be avoided only if the materialized view is adequately maintained. We propose a method in which all database updates to base relations are first filtered to remove from consideration those that cannot possibly affect the view. The conditions given for the detection of updates of this type, called irrelevant updates, are necessary and sufficient and are independent of the database state. For the remaining database updates, a differential algorithm can be applied to re-evaluate the view expression. The algorithm proposed exploits the knowledge provided by both the view definition expression and the database update operations.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {61–71},
numpages = {11},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16861,
author = {Blakeley, Jose A. and Larson, Per-Ake and Tompa, Frank Wm},
title = {Efficiently Updating Materialized Views},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16861},
doi = {10.1145/16856.16861},
abstract = {Query processing can be sped up by keeping frequently accessed users' views materialized. However, the need to access base relations in response to queries can be avoided only if the materialized view is adequately maintained. We propose a method in which all database updates to base relations are first filtered to remove from consideration those that cannot possibly affect the view. The conditions given for the detection of updates of this type, called irrelevant updates, are necessary and sufficient and are independent of the database state. For the remaining database updates, a differential algorithm can be applied to re-evaluate the view expression. The algorithm proposed exploits the knowledge provided by both the view definition expression and the database update operations.},
journal = {SIGMOD Rec.},
month = jun,
pages = {61–71},
numpages = {11}
}

@inproceedings{10.1145/16894.16862,
author = {Moss, J. Eliot B and Griffeth, Nancy D. and Graham, Marc H.},
title = {Abstraction in Recovery Management},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16862},
doi = {10.1145/16894.16862},
abstract = {There are many examples of actions on abstract data types which can be correctly implemented with nonserializable and nonrecoverable schedules of reads and writes. We examine a model of multiple layers of abstraction that explains this phenomenon and suggests an approach to building layered systems with transaction oriented synchronization and roll back. Our model may make it easier to provide the high data integrity of reliable database transaction processing in a broader class of information systems. We concentrate on the recovery aspects here, a technical report [Moss et al 85] has a more complete discussion of concurrency control.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {72–83},
numpages = {12},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16862,
author = {Moss, J. Eliot B and Griffeth, Nancy D. and Graham, Marc H.},
title = {Abstraction in Recovery Management},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16862},
doi = {10.1145/16856.16862},
abstract = {There are many examples of actions on abstract data types which can be correctly implemented with nonserializable and nonrecoverable schedules of reads and writes. We examine a model of multiple layers of abstraction that explains this phenomenon and suggests an approach to building layered systems with transaction oriented synchronization and roll back. Our model may make it easier to provide the high data integrity of reliable database transaction processing in a broader class of information systems. We concentrate on the recovery aspects here, a technical report [Moss et al 85] has a more complete discussion of concurrency control.},
journal = {SIGMOD Rec.},
month = jun,
pages = {72–83},
numpages = {12}
}

@inproceedings{10.1145/16894.16863,
author = {Mackert, Lothar F. and Lohman, Guy M.},
title = {R* Optimizer Validation and Performance Evaluation for Local Queries},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16863},
doi = {10.1145/16894.16863},
abstract = {Few database query optimizer models have been validated against actual performance. This paper presents the methodology and results of a thorough validation of the optimizer and evaluation of the performance of the experimental distributed relational database management system R*, which inherited and extended to a distributed environment the optimization algorithms of System R. Optimizer estimated costs and actual R* resources consumed were written to database tables using new SQL commands, permitting automated control from SQL application programs of test data collection and reduction. A number of tests were run over a wide variety of dynamically-created test databases, SQL queries, and system parameters. The results for single-table access, sorting, and local 2-table joins are reported here. The tests confirmed the accuracy of the majority of the I/O cost model, the significant contribution of CPU cost to total cost, and the need to model CPU cost in more detail than was done in System R. The R* optimizer now retains cost components separately and estimates the number of CPU instructions, including those for applying different kinds of predicates. The sensitivity of I/O cost to buffer space motivated the development of more detailed models of buffer utilization unclustered index scans and nested-loop joins often benefit from pages remaining in the buffers, whereas concurrent scans of the data pages and the index pages for multiple tables during joins compete for buffer share. Without an index on the join column of the inner table, the optimizer correctly avoids the nested-loop join, confirming the need for merge-scan joins. When the join column of the inner is indexed, the optimizer overestimates the cost of the nested-loop join, whose actual performance is very sensitive to three parameters that are extremely difficult to estimate (1) the join (result) cardinality, (2) the outer table's cardinality, and (3) the number of buffer pages available to store the inner table. Suggestions are given for improved database statistics, prefetch and page replacement strategies for the buffer manager, and the use of temporary indexes and Bloom filters (hashed semijoins) to reduce access of unneeded data.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {84–95},
numpages = {12},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16863,
author = {Mackert, Lothar F. and Lohman, Guy M.},
title = {R* Optimizer Validation and Performance Evaluation for Local Queries},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16863},
doi = {10.1145/16856.16863},
abstract = {Few database query optimizer models have been validated against actual performance. This paper presents the methodology and results of a thorough validation of the optimizer and evaluation of the performance of the experimental distributed relational database management system R*, which inherited and extended to a distributed environment the optimization algorithms of System R. Optimizer estimated costs and actual R* resources consumed were written to database tables using new SQL commands, permitting automated control from SQL application programs of test data collection and reduction. A number of tests were run over a wide variety of dynamically-created test databases, SQL queries, and system parameters. The results for single-table access, sorting, and local 2-table joins are reported here. The tests confirmed the accuracy of the majority of the I/O cost model, the significant contribution of CPU cost to total cost, and the need to model CPU cost in more detail than was done in System R. The R* optimizer now retains cost components separately and estimates the number of CPU instructions, including those for applying different kinds of predicates. The sensitivity of I/O cost to buffer space motivated the development of more detailed models of buffer utilization unclustered index scans and nested-loop joins often benefit from pages remaining in the buffers, whereas concurrent scans of the data pages and the index pages for multiple tables during joins compete for buffer share. Without an index on the join column of the inner table, the optimizer correctly avoids the nested-loop join, confirming the need for merge-scan joins. When the join column of the inner is indexed, the optimizer overestimates the cost of the nested-loop join, whose actual performance is very sensitive to three parameters that are extremely difficult to estimate (1) the join (result) cardinality, (2) the outer table's cardinality, and (3) the number of buffer pages available to store the inner table. Suggestions are given for improved database statistics, prefetch and page replacement strategies for the buffer manager, and the use of temporary indexes and Bloom filters (hashed semijoins) to reduce access of unneeded data.},
journal = {SIGMOD Rec.},
month = jun,
pages = {84–95},
numpages = {12}
}

@inproceedings{10.1145/16894.16864,
author = {Ahn, Ilsoo and Snodgrass, Richard},
title = {Performance Evaluation of a Temporal Database Management System},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16864},
doi = {10.1145/16894.16864},
abstract = {A prototype of a temporal database management system was built by extending Ingres. It supports the temporal query language TQuel, a superset of Quel, handling four types of database static, rollback, historical and temporal. A benchmark set of queries was run to study the performance of the prototype on the four types of databases. We analyze the results of the benchmark, and identify major factors that have the greatest impact on the performance of the system. We also discuss several mechanisms to address the performance bottlenecks we encountered.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {96–107},
numpages = {12},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16864,
author = {Ahn, Ilsoo and Snodgrass, Richard},
title = {Performance Evaluation of a Temporal Database Management System},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16864},
doi = {10.1145/16856.16864},
abstract = {A prototype of a temporal database management system was built by extending Ingres. It supports the temporal query language TQuel, a superset of Quel, handling four types of database static, rollback, historical and temporal. A benchmark set of queries was run to study the performance of the prototype on the four types of databases. We analyze the results of the benchmark, and identify major factors that have the greatest impact on the performance of the system. We also discuss several mechanisms to address the performance bottlenecks we encountered.},
journal = {SIGMOD Rec.},
month = jun,
pages = {96–107},
numpages = {12}
}

@inproceedings{10.1145/16894.16865,
author = {Carey, Michael J. and Lu, Hongjun},
title = {Load Balancing in a Locally Distributed DB System},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16865},
doi = {10.1145/16894.16865},
abstract = {Most previous work on query optimization in distributed database systems has focused on finding optimal or near-optimal processing plans based solely on static system characteristics, and few researchers have addressed the problem of copy selection when data is replicated. This paper describes a new approach to query processing for locally distributed database systems. Our approach uses load information to select the processing site(s) for a query, dynamically choosing from among those sites that have copies of relations referenced by the query. Query compilation is used to produce a statically-optimized logical plan for the query, and then a dynamic optimization phase converts this logical plan into an executable physical plan at runtime. This paper motivates the separation of static and dynamic optimization, presents algorithms for the various phases of the optimization process, and describes a simulation study that was undertaken to investigate the performance of this approach. Our simulation results indicate that load-balanced query processing can provide improvements in both query response times and overall system throughput as compared to schemes where execution sites are either statistically or randomly selected.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {108–119},
numpages = {12},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16865,
author = {Carey, Michael J. and Lu, Hongjun},
title = {Load Balancing in a Locally Distributed DB System},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16865},
doi = {10.1145/16856.16865},
abstract = {Most previous work on query optimization in distributed database systems has focused on finding optimal or near-optimal processing plans based solely on static system characteristics, and few researchers have addressed the problem of copy selection when data is replicated. This paper describes a new approach to query processing for locally distributed database systems. Our approach uses load information to select the processing site(s) for a query, dynamically choosing from among those sites that have copies of relations referenced by the query. Query compilation is used to produce a statically-optimized logical plan for the query, and then a dynamic optimization phase converts this logical plan into an executable physical plan at runtime. This paper motivates the separation of static and dynamic optimization, presents algorithms for the various phases of the optimization process, and describes a simulation study that was undertaken to investigate the performance of this approach. Our simulation results indicate that load-balanced query processing can provide improvements in both query response times and overall system throughput as compared to schemes where execution sites are either statistically or randomly selected.},
journal = {SIGMOD Rec.},
month = jun,
pages = {108–119},
numpages = {12}
}

@inproceedings{10.1145/16894.16866,
author = {Motro, Amihai},
title = {Constructing Queries from Tokens},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16866},
doi = {10.1145/16894.16866},
abstract = {A database token is a value of either the data or the metadata. Usually, such tokens are combined with formal language constructs to form queries. In this paper we show how a given set of tokens may be completed to a proper query. This process provides a useful means of communication between naive users and databases, allowing them to express simple requests by listing several tokens. As the inferred query is always shown to the user, this process has a side effect of instructing the user in the proper use of the query language. The method is described and demonstrated with relational databases, but its principles may be implemented with other databases as well.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {120–131},
numpages = {12},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16866,
author = {Motro, Amihai},
title = {Constructing Queries from Tokens},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16866},
doi = {10.1145/16856.16866},
abstract = {A database token is a value of either the data or the metadata. Usually, such tokens are combined with formal language constructs to form queries. In this paper we show how a given set of tokens may be completed to a proper query. This process provides a useful means of communication between naive users and databases, allowing them to express simple requests by listing several tokens. As the inferred query is always shown to the user, this process has a side effect of instructing the user in the proper use of the query language. The method is described and demonstrated with relational databases, but its principles may be implemented with other databases as well.},
journal = {SIGMOD Rec.},
month = jun,
pages = {120–131},
numpages = {12}
}

@inproceedings{10.1145/16894.16867,
author = {Delisle, Norman and Schwartz, Mayer},
title = {Neptune: A Hypertext System for CAD Applications},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16867},
doi = {10.1145/16894.16867},
abstract = {Even though many of the essential notions of hypertext were first contained in the description of a “memex,” written by Vannevar Bush in 1945 [Bus45], there are today only a few scattered implementations of hypertext, let alone any serious use of it in a CAD environment. In what follows, we describe what hypertext is all about. We describe a prototype hypertext system, named Neptune, that we have built. We show how it is useful, especially its broad applicability to CAD.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {132–143},
numpages = {12},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16867,
author = {Delisle, Norman and Schwartz, Mayer},
title = {Neptune: A Hypertext System for CAD Applications},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16867},
doi = {10.1145/16856.16867},
abstract = {Even though many of the essential notions of hypertext were first contained in the description of a “memex,” written by Vannevar Bush in 1945 [Bus45], there are today only a few scattered implementations of hypertext, let alone any serious use of it in a CAD environment. In what follows, we describe what hypertext is all about. We describe a prototype hypertext system, named Neptune, that we have built. We show how it is useful, especially its broad applicability to CAD.},
journal = {SIGMOD Rec.},
month = jun,
pages = {132–143},
numpages = {12}
}

@inproceedings{10.1145/16894.16868,
author = {Frasson, C. and Er-radi, M.},
title = {Principles of an Icons-Based Language},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16868},
doi = {10.1145/16894.16868},
abstract = {Improvements both in technology and in user-oriented software have shown the feasibility of new kinds of non-procedural languages. However, interaction between end-user and data should rely more and more on graphical languages and, particularly, on 'iconic” languages. In the following we review and analyze the forces which are at the origin of changes in the user environment. We give the main specifications of an iconic interface and a command language based on icons. Examples are given in a medical environment.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {144–152},
numpages = {9},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16868,
author = {Frasson, C. and Er-radi, M.},
title = {Principles of an Icons-Based Language},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16868},
doi = {10.1145/16856.16868},
abstract = {Improvements both in technology and in user-oriented software have shown the feasibility of new kinds of non-procedural languages. However, interaction between end-user and data should rely more and more on graphical languages and, particularly, on 'iconic” languages. In the following we review and analyze the forces which are at the origin of changes in the user environment. We give the main specifications of an iconic interface and a command language based on icons. Examples are given in a medical environment.},
journal = {SIGMOD Rec.},
month = jun,
pages = {144–152},
numpages = {9}
}

@inproceedings{10.1145/16894.16869,
author = {Boral, Hanan},
title = {Panel: Database System Performance Management},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16869},
doi = {10.1145/16894.16869},
abstract = {In the past few years we have seen in the literature a number of proposals for benchmarks to be used in measuring the performance of database management and transaction processing systems. The TP1 benchmark [Anon et al 1985] and the Wisconsin benchmark [Bitton et al 1983], [Boral and DeWitt 1984], and [Bitton and Turbyfill 1985] have been used to benchmark several systems. Other benchmarks have also been proposed.The TP1 benchmark actually consists of three different benchmarks Debit-Credit, Scan, and Sort. It is oriented towards transaction processing systems. Each of the benchmarks consists of a single transaction type and operates on a large database — around 10 GBytes. The database consists of artificial data but is modeled around data maintained by a large bank.The Debit-Credit benchmark consists of a transaction that reads and updates a small number (about 4) of random records. It imposes stringent response time and throughput requirements on the system.The Scan benchmark consists of a COBOL program that exercises the system by executing 1,000 scan transactions each of which accesses and updates 1,000 records in a sequentially organized file.Finally, the Sort benchmark sorts 1M records of 100 bytes each.Each of the benchmarks stresses different aspects of the system. Each requires different amount of CPU, communication, and I/O cycles. In addition to the diversity of system resource requirements the benchmark methodology described in [Anon et al 1985] also requires that the cost of the system be calculated. Thus, the final measure one obtains from running the TP1 benchmark is $K/TPS.Whereas TP1 is oriented towards transaction processing the Wisconsin benchmark was conceived for the purpose of measuring the performance of relational database systems. It consists of two parts a single user benchmark in which a suite of approximately 30 different queries are used to obtain response time measures in standalone mode (described in [Bitton et al 1983], and a multi-user benchmark in which several queries of varying complexity are used to determine the response time and throughput behavior under a variety of conditions (one version of the multi-user benchmark is described in [Boral and DeWitt 1984] and a second version in [Bitton and Turbyfill 1985]).The test database consists of a number of relations of varying sizes. The relations are generated according to statistical distributions and do not model any real world data. Users of the benchmark can modify the database generator routines to adapt the database characteristics so that they are more representative of their application.It appears as though both the TP1 and the Wisconsin benchmark have the potential of becoming de facto standard benchmarks, in their respective areas, to be used in a variety of ways. For example, a vendor could use the benchmarks to stress test a system under development. Another use for a vendor is in establishing a particular rating for a system (equivalent MIPS Whetstones, etc. for mainframes). Finally, a user can use a benchmark to compare several systems before purchasing one.The purpose of this panel is to discuss the use of benchmarking for measuring the performance of transaction processing systems and database management systems in general and the use of the TP1 and Wisconsin benchmarks in particular.The panelists have been chosen so that we have a representation of experts in the particular benchmarks (Gawlick) and DeWitt), a benchmark “consumer” (Hawthorn), and a “performance expert” — someone who understands benchmarking as a science/art (Brice).The panelists will address the following issues (as well as others raised by the audience)
What are the strengths and weaknesses of the TP1 and Wisconsin benchmarks?Is benchmarking a good technique for measuring the performance of data management and transaction processing systems?What can these benchmarks tell us about a system and what can they not tell us about it?},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {153–154},
numpages = {2},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16869,
author = {Boral, Hanan},
title = {Panel: Database System Performance Management},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16869},
doi = {10.1145/16856.16869},
abstract = {In the past few years we have seen in the literature a number of proposals for benchmarks to be used in measuring the performance of database management and transaction processing systems. The TP1 benchmark [Anon et al 1985] and the Wisconsin benchmark [Bitton et al 1983], [Boral and DeWitt 1984], and [Bitton and Turbyfill 1985] have been used to benchmark several systems. Other benchmarks have also been proposed.The TP1 benchmark actually consists of three different benchmarks Debit-Credit, Scan, and Sort. It is oriented towards transaction processing systems. Each of the benchmarks consists of a single transaction type and operates on a large database — around 10 GBytes. The database consists of artificial data but is modeled around data maintained by a large bank.The Debit-Credit benchmark consists of a transaction that reads and updates a small number (about 4) of random records. It imposes stringent response time and throughput requirements on the system.The Scan benchmark consists of a COBOL program that exercises the system by executing 1,000 scan transactions each of which accesses and updates 1,000 records in a sequentially organized file.Finally, the Sort benchmark sorts 1M records of 100 bytes each.Each of the benchmarks stresses different aspects of the system. Each requires different amount of CPU, communication, and I/O cycles. In addition to the diversity of system resource requirements the benchmark methodology described in [Anon et al 1985] also requires that the cost of the system be calculated. Thus, the final measure one obtains from running the TP1 benchmark is $K/TPS.Whereas TP1 is oriented towards transaction processing the Wisconsin benchmark was conceived for the purpose of measuring the performance of relational database systems. It consists of two parts a single user benchmark in which a suite of approximately 30 different queries are used to obtain response time measures in standalone mode (described in [Bitton et al 1983], and a multi-user benchmark in which several queries of varying complexity are used to determine the response time and throughput behavior under a variety of conditions (one version of the multi-user benchmark is described in [Boral and DeWitt 1984] and a second version in [Bitton and Turbyfill 1985]).The test database consists of a number of relations of varying sizes. The relations are generated according to statistical distributions and do not model any real world data. Users of the benchmark can modify the database generator routines to adapt the database characteristics so that they are more representative of their application.It appears as though both the TP1 and the Wisconsin benchmark have the potential of becoming de facto standard benchmarks, in their respective areas, to be used in a variety of ways. For example, a vendor could use the benchmarks to stress test a system under development. Another use for a vendor is in establishing a particular rating for a system (equivalent MIPS Whetstones, etc. for mainframes). Finally, a user can use a benchmark to compare several systems before purchasing one.The purpose of this panel is to discuss the use of benchmarking for measuring the performance of transaction processing systems and database management systems in general and the use of the TP1 and Wisconsin benchmarks in particular.The panelists have been chosen so that we have a representation of experts in the particular benchmarks (Gawlick) and DeWitt), a benchmark “consumer” (Hawthorn), and a “performance expert” — someone who understands benchmarking as a science/art (Brice).The panelists will address the following issues (as well as others raised by the audience)
What are the strengths and weaknesses of the TP1 and Wisconsin benchmarks?Is benchmarking a good technique for measuring the performance of data management and transaction processing systems?What can these benchmarks tell us about a system and what can they not tell us about it?},
journal = {SIGMOD Rec.},
month = jun,
pages = {153–154},
numpages = {2}
}

@inproceedings{10.1145/16894.16870,
author = {Van Gelder, Allen},
title = {A Message Passing Framework for Logical Query Evaluation},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16870},
doi = {10.1145/16894.16870},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {155–165},
numpages = {11},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16870,
author = {Van Gelder, Allen},
title = {A Message Passing Framework for Logical Query Evaluation},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16870},
doi = {10.1145/16856.16870},
journal = {SIGMOD Rec.},
month = jun,
pages = {155–165},
numpages = {11}
}

@inproceedings{10.1145/16894.16871,
author = {Rosenthal, Arnon and Heiler, Sandra and Dayal, Umeshwar and Manola, Frank},
title = {Traversal Recursion: A Practical Approach to Supporting Recursive Applications},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16871},
doi = {10.1145/16894.16871},
abstract = {Many capabilities that are needed for recursive applications in engineering and project management are not well supported by the usual formulations of recursion. We identify a class of recursions called “traversal recursions” (which model traversals of a directed graph) that have two important properties they can supply the necessary capabilities and efficient processing algorithms have been defined for them. First we present a taxonomy of traversal recursions based on properties of the recursion on graph structure and on unusual types of metadata. This taxonomy is exploited to identify solvable recursions and to select an execution algorithm. We show how graph traversal can sometimes outperform the more general iteration algorithm. Finally we show how a conventional query optimizer architecture can be extended to handle recursive queries and views.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {166–176},
numpages = {11},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16871,
author = {Rosenthal, Arnon and Heiler, Sandra and Dayal, Umeshwar and Manola, Frank},
title = {Traversal Recursion: A Practical Approach to Supporting Recursive Applications},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16871},
doi = {10.1145/16856.16871},
abstract = {Many capabilities that are needed for recursive applications in engineering and project management are not well supported by the usual formulations of recursion. We identify a class of recursions called “traversal recursions” (which model traversals of a directed graph) that have two important properties they can supply the necessary capabilities and efficient processing algorithms have been defined for them. First we present a taxonomy of traversal recursions based on properties of the recursion on graph structure and on unusual types of metadata. This taxonomy is exploited to identify solvable recursions and to select an execution algorithm. We show how graph traversal can sometimes outperform the more general iteration algorithm. Finally we show how a conventional query optimizer architecture can be extended to handle recursive queries and views.},
journal = {SIGMOD Rec.},
month = jun,
pages = {166–176},
numpages = {11}
}

@inproceedings{10.1145/16894.16872,
author = {Gardarin, Georges and de Maindreville, Christophe},
title = {Evaluation of Database Recursive Logic Programs as Recurrent Function Series},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16872},
doi = {10.1145/16894.16872},
abstract = {The authors introduce a new method to compile queries referencing recursively defined predicates. This method is based on an interpretation of the query and the relations as functions which map one column of a relation to another column. It is shown that a large class of queries with associated recursive rules, including mutually recursive rules, can be computed as the limit of a series of functions. Typical cases of series of functions are given and solved. The solutions lend themselves towards either extended relational algebra or SQL optimized programs to compute the recursive query answers. Examples of applications are given.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {177–186},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16872,
author = {Gardarin, Georges and de Maindreville, Christophe},
title = {Evaluation of Database Recursive Logic Programs as Recurrent Function Series},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16872},
doi = {10.1145/16856.16872},
abstract = {The authors introduce a new method to compile queries referencing recursively defined predicates. This method is based on an interpretation of the query and the relations as functions which map one column of a relation to another column. It is shown that a large class of queries with associated recursive rules, including mutually recursive rules, can be computed as the limit of a series of functions. Typical cases of series of functions are given and solved. The solutions lend themselves towards either extended relational algebra or SQL optimized programs to compute the recursive query answers. Examples of applications are given.},
journal = {SIGMOD Rec.},
month = jun,
pages = {177–186},
numpages = {10}
}

@inproceedings{10.1145/16894.16873,
author = {Batory, D. S. and Mannino, M.},
title = {Panel: Extensible Database Systems},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16873},
doi = {10.1145/16894.16873},
abstract = {New implementation techniques and new capabilities for database systems are being developed and proposed at a rapid rate. Novel file structures and improved algorithms for query optimization, buffer and recovery management, and transaction management have the potential of realizing significant gains in DBMS performance. The proposed integration of design objects, voice, text, rules, vector graphics, and images into databases promises exciting new capabilities for DBMSs. To accommodate advances in database technology and to support new classes of database applications, DBMSs must be extensible (i.e., customizable).To achieve extensibility forces a fundamental rethinking about how DBMSs are built, and how special-purpose features can be integrated into a DBMS with little effort and expense. Customizing DBMSs implies the availability of extensible data models, to allow for the introduction of new object types and operations, and extensible storage structures, to take advantage of special properties of stored data or operations to enhance performance.Although research on extensible DBMSs is still in its infancy, a fundamental concept underlying their construction is now evident. This is the standardization of interfaces and the plug-compatibility of modules. An extensible DBMS will be a 'software bus' whereby new modules (and hence new DBMS capabilities) can be added, exchanged, or removed by plugging or unplugging modules. Extensible DBMSs will thus rely on extensive software libraries, where new modules can be added as needed. Furthermore, changes to DBMSs can be made in months rather than years, and the reinvention of established technology is kept to a minimum because of the reusability of modules.The perception of DBMSs as monolithic entities that are difficult to modify will change as extensible DBMS technology becomes better understood. The use of database systems will not change, the ANSI/SPARC roles of database users, who write and execute transactions, and the database administrator (DBA), who designs and writes database schemas, will remain. Extensible DBMSs will require the introduction of an additional party, the database architecture administrator (DDA), who is responsible for the construction and customization of a DBMS.A growing number of researchers are developing extensible DBMSs. The purpose of this panel is to explain and discuss some of the approaches that are now being taken (and those that can be taken), and to survey the problems that confront extensible database technology. Descriptions of the systems and research represented at this panel are given in the following sections.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {187–190},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16873,
author = {Batory, D. S. and Mannino, M.},
title = {Panel: Extensible Database Systems},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16873},
doi = {10.1145/16856.16873},
abstract = {New implementation techniques and new capabilities for database systems are being developed and proposed at a rapid rate. Novel file structures and improved algorithms for query optimization, buffer and recovery management, and transaction management have the potential of realizing significant gains in DBMS performance. The proposed integration of design objects, voice, text, rules, vector graphics, and images into databases promises exciting new capabilities for DBMSs. To accommodate advances in database technology and to support new classes of database applications, DBMSs must be extensible (i.e., customizable).To achieve extensibility forces a fundamental rethinking about how DBMSs are built, and how special-purpose features can be integrated into a DBMS with little effort and expense. Customizing DBMSs implies the availability of extensible data models, to allow for the introduction of new object types and operations, and extensible storage structures, to take advantage of special properties of stored data or operations to enhance performance.Although research on extensible DBMSs is still in its infancy, a fundamental concept underlying their construction is now evident. This is the standardization of interfaces and the plug-compatibility of modules. An extensible DBMS will be a 'software bus' whereby new modules (and hence new DBMS capabilities) can be added, exchanged, or removed by plugging or unplugging modules. Extensible DBMSs will thus rely on extensive software libraries, where new modules can be added as needed. Furthermore, changes to DBMSs can be made in months rather than years, and the reinvention of established technology is kept to a minimum because of the reusability of modules.The perception of DBMSs as monolithic entities that are difficult to modify will change as extensible DBMS technology becomes better understood. The use of database systems will not change, the ANSI/SPARC roles of database users, who write and execute transactions, and the database administrator (DBA), who designs and writes database schemas, will remain. Extensible DBMSs will require the introduction of an additional party, the database architecture administrator (DDA), who is responsible for the construction and customization of a DBMS.A growing number of researchers are developing extensible DBMSs. The purpose of this panel is to explain and discuss some of the approaches that are now being taken (and those that can be taken), and to survey the problems that confront extensible database technology. Descriptions of the systems and research represented at this panel are given in the following sections.},
journal = {SIGMOD Rec.},
month = jun,
pages = {187–190},
numpages = {4}
}

@inproceedings{10.1145/16894.16874,
author = {Sellis, Timos K.},
title = {Global Query Optimization},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16874},
doi = {10.1145/16894.16874},
abstract = {In some recently proposed extensions to relational database systems as well as in deductive databases, a database system is presented with a collection of queries to process instead of just one. It is an interesting problem then, to come up with algorithms that process these queries together instead of one query at a time. We examine the problem of multiple (global) query optimization in this paper. A hierarchy of algorithms that can be used for global query optimization is exhibited and analyzed. These algorithms range from an arbitrary serial execution without any sharing of common results among the queries to an exhaustive search of all possible ways to process all queries.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {191–205},
numpages = {15},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16874,
author = {Sellis, Timos K.},
title = {Global Query Optimization},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16874},
doi = {10.1145/16856.16874},
abstract = {In some recently proposed extensions to relational database systems as well as in deductive databases, a database system is presented with a collection of queries to process instead of just one. It is an interesting problem then, to come up with algorithms that process these queries together instead of one query at a time. We examine the problem of multiple (global) query optimization in this paper. A hierarchy of algorithms that can be used for global query optimization is exhibited and analyzed. These algorithms range from an arbitrary serial execution without any sharing of common results among the queries to an exhaustive search of all possible ways to process all queries.},
journal = {SIGMOD Rec.},
month = jun,
pages = {191–205},
numpages = {15}
}

@inproceedings{10.1145/16894.16875,
author = {Freytag, Johann Christoph and Goodman, Nathan},
title = {Rule-Based Transformation of Relational Queries into Iterative Programs},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16875},
doi = {10.1145/16894.16875},
abstract = {Over the last decade many techniques for optimizing relational queries have been developed. However, the problem of translating these set-oriented query specifications into other forms for efficient execution has received little attention.This paper presents an algorithm that translates algebra-based query specifications into iterative programs for an efficient execution. While the source level operates on sets of tuples, the generated programs manipulate tuples as their basic objects. The algorithm incorporates techniques which have been developed in the areas of functional programming and program transformation.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {206–214},
numpages = {9},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16875,
author = {Freytag, Johann Christoph and Goodman, Nathan},
title = {Rule-Based Transformation of Relational Queries into Iterative Programs},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16875},
doi = {10.1145/16856.16875},
abstract = {Over the last decade many techniques for optimizing relational queries have been developed. However, the problem of translating these set-oriented query specifications into other forms for efficient execution has received little attention.This paper presents an algorithm that translates algebra-based query specifications into iterative programs for an efficient execution. While the source level operates on sets of tuples, the generated programs manipulate tuples as their basic objects. The algorithm incorporates techniques which have been developed in the areas of functional programming and program transformation.},
journal = {SIGMOD Rec.},
month = jun,
pages = {206–214},
numpages = {9}
}

@inproceedings{10.1145/16894.16876,
author = {Wedekind, H. and Zoerntlein, George},
title = {Prefetching in Realtime Database Applications},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16876},
doi = {10.1145/16894.16876},
abstract = {In this paper a method is proposed how to achieve response times of main memory database systems without keeping the whole database in main memory. The method was originally developed for real-time systems in manufacturing automation, but it is applicable in environments where canned transactions interact with databases rather than people performing free transactions. The main idea is to preanalyse canned transactions in order to extract knowledge about their local access behaviour. This knowledge is used by the runtime system of the database when the transaction is started. Concepts for modules doing the preanalysis and the runtime tasks are described in detail. Furthermore a database architecture is developed incorporating these new components.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {215–226},
numpages = {12},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16876,
author = {Wedekind, H. and Zoerntlein, George},
title = {Prefetching in Realtime Database Applications},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16876},
doi = {10.1145/16856.16876},
abstract = {In this paper a method is proposed how to achieve response times of main memory database systems without keeping the whole database in main memory. The method was originally developed for real-time systems in manufacturing automation, but it is applicable in environments where canned transactions interact with databases rather than people performing free transactions. The main idea is to preanalyse canned transactions in order to extract knowledge about their local access behaviour. This knowledge is used by the runtime system of the database when the transaction is started. Concepts for modules doing the preanalysis and the runtime tasks are described in detail. Furthermore a database architecture is developed incorporating these new components.},
journal = {SIGMOD Rec.},
month = jun,
pages = {215–226},
numpages = {12}
}

@inproceedings{10.1145/16894.16877,
author = {Faloutsos, Christos},
title = {Multiattribute Hashing Using Gray Codes},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16877},
doi = {10.1145/16894.16877},
abstract = {Multiattribute hashing and its variations have been proposed for partial match and range queries in the past. The main idea is that each record yields a bitstring @@@@ (“record signature”), according to the values of its attributes. The binary value (@@@@)2 of this string decides the bucket that the record is stored. In this paper we propose to use Gray codes instead of binary codes, in order to map record signatures to buckets. In Gray codes, successive codewords differ in the value of exactly one bit position, thus, successive buckets hold records with similar record signatures. The proposed method achieves better clustering of similar records and avoids some of the (expensive) random disk accesses, replacing them with sequential ones. We develop a mathematical model, derive formulas giving the average performance of both methods and show that the proposed method achieves 0% - 50% relative savings over the binary codes. We also discuss how Gray codes could be applied to some retrieval methods designed for range queries, such as the grid file [Nievergelt84a] and the approach based on the so-called z-ordering [Orenstein84a].},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {227–238},
numpages = {12},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16877,
author = {Faloutsos, Christos},
title = {Multiattribute Hashing Using Gray Codes},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16877},
doi = {10.1145/16856.16877},
abstract = {Multiattribute hashing and its variations have been proposed for partial match and range queries in the past. The main idea is that each record yields a bitstring @@@@ (“record signature”), according to the values of its attributes. The binary value (@@@@)2 of this string decides the bucket that the record is stored. In this paper we propose to use Gray codes instead of binary codes, in order to map record signatures to buckets. In Gray codes, successive codewords differ in the value of exactly one bit position, thus, successive buckets hold records with similar record signatures. The proposed method achieves better clustering of similar records and avoids some of the (expensive) random disk accesses, replacing them with sequential ones. We develop a mathematical model, derive formulas giving the average performance of both methods and show that the proposed method achieves 0% - 50% relative savings over the binary codes. We also discuss how Gray codes could be applied to some retrieval methods designed for range queries, such as the grid file [Nievergelt84a] and the approach based on the so-called z-ordering [Orenstein84a].},
journal = {SIGMOD Rec.},
month = jun,
pages = {227–238},
numpages = {12}
}

@inproceedings{10.1145/16894.16878,
author = {Lehman, Tobin J. and Carey, Michael J.},
title = {Query Processing in Main Memory Database Management Systems},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16878},
doi = {10.1145/16894.16878},
abstract = {Most previous work in the area of main memory database systems has focused on the problem of developing query processing techniques that work well with a very large buffer pool. In this paper, we address query processing issues for memory resident relational databases, an environment with a very different set of costs and priorities. We present an architecture for a main memory DBMS, discussing the ways in which a memory resident database differs from a disk-based database. We then address the problem of processing relational queries in this architecture, considering alternative algorithms for selection, projection, and join operations and studying their performance. We show that a new index structure, the T Tree, works well for selection and join processing in memory resident databases. We also show that hashing methods work well for processing projections and joins, and that an old join method, sort-merge, still has a place in main memory.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {239–250},
numpages = {12},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16878,
author = {Lehman, Tobin J. and Carey, Michael J.},
title = {Query Processing in Main Memory Database Management Systems},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16878},
doi = {10.1145/16856.16878},
abstract = {Most previous work in the area of main memory database systems has focused on the problem of developing query processing techniques that work well with a very large buffer pool. In this paper, we address query processing issues for memory resident relational databases, an environment with a very different set of costs and priorities. We present an architecture for a main memory DBMS, discussing the ways in which a memory resident database differs from a disk-based database. We then address the problem of processing relational queries in this architecture, considering alternative algorithms for selection, projection, and join operations and studying their performance. We show that a new index structure, the T Tree, works well for selection and join processing in memory resident databases. We also show that hashing methods work well for processing projections and joins, and that an old join method, sort-merge, still has a place in main memory.},
journal = {SIGMOD Rec.},
month = jun,
pages = {239–250},
numpages = {12}
}

@inproceedings{10.1145/16894.16879,
author = {Willard, Dan E.},
title = {Good Worst-Case Algorithms for Inserting and Deleting Records in Dense Sequential Files},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16879},
doi = {10.1145/16894.16879},
abstract = {Consider a file which arranges records in sequential order, and stores them with possible empty spaces in M consecutive pages of memory. We develop an insertion-deletion algorithm which runs in a worst-case time approximately proportional to log2M divided by the page-size when the set of manipulated records has cardinality O(M).},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {251–260},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16879,
author = {Willard, Dan E.},
title = {Good Worst-Case Algorithms for Inserting and Deleting Records in Dense Sequential Files},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16879},
doi = {10.1145/16856.16879},
abstract = {Consider a file which arranges records in sequential order, and stores them with possible empty spaces in M consecutive pages of memory. We develop an insertion-deletion algorithm which runs in a worst-case time approximately proportional to log2M divided by the page-size when the set of manipulated records has cardinality O(M).},
journal = {SIGMOD Rec.},
month = jun,
pages = {251–260},
numpages = {10}
}

@inproceedings{10.1145/16894.16880,
author = {Harandi, Mehdi T. and Schang, Thierry and Cohen, Seth},
title = {Rule Base Management Using Meta Knowledge},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16880},
doi = {10.1145/16894.16880},
abstract = {This paper describes the rule base management strategy of an expert system environment. The environment includes a set of integrated tools which facilitate acquisition, manipulation and maintenance of knowledge. The rule base management component of the system, called RBM, assists these tasks by organizing global semantic information within the rule base. RBM extracts this semantic information from the texts included in “rule structures” and builds a semantic network of the concepts found in the rule base. The rule base is then divided into rulesets which are clusters of rules that refer to the same atomic concept. Construction of this meta knowledge is achieved through a keyword matching mechanism. The paper includes a brief description of the RBM system, the dictionary it uses for building meta-level knowledge, and its keyword matching technique.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {261–267},
numpages = {7},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16880,
author = {Harandi, Mehdi T. and Schang, Thierry and Cohen, Seth},
title = {Rule Base Management Using Meta Knowledge},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16880},
doi = {10.1145/16856.16880},
abstract = {This paper describes the rule base management strategy of an expert system environment. The environment includes a set of integrated tools which facilitate acquisition, manipulation and maintenance of knowledge. The rule base management component of the system, called RBM, assists these tasks by organizing global semantic information within the rule base. RBM extracts this semantic information from the texts included in “rule structures” and builds a semantic network of the concepts found in the rule base. The rule base is then divided into rulesets which are clusters of rules that refer to the same atomic concept. Construction of this meta knowledge is achieved through a keyword matching mechanism. The paper includes a brief description of the RBM system, the dictionary it uses for building meta-level knowledge, and its keyword matching technique.},
journal = {SIGMOD Rec.},
month = jun,
pages = {261–267},
numpages = {7}
}

@inproceedings{10.1145/16894.16881,
author = {Imielinski, Tomasz},
title = {Query Processing in Deductive Databases with Incomplete Information},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16881},
doi = {10.1145/16894.16881},
abstract = {We study here automated deduction in databases in the presence of various types of inference rules of the form of Horn Clauses with Skolem functions. These inference rules are typical for databases with incomplete information. We demonstrate a number of results related to processing of conjunctive queries for different types of database intensions. In particular, we show that when a database intension is built from possibly cyclic inclusion dependencies and view definitions any conjunctive query can be translated to the an equivalent form which can be evaluated directly over the database extension (disregarding inference rules). We also demonstrate that the complexity of query processing significantly grows when we mix incomplete information with recursive rules. In particular, we demonstrate here that even the power of least fixpoint extension of first order logic may be not sufficient to process queries in the presence of incomplete data and recursive rules. The same is demonstrated in case disjunctive information is allowed in the database.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {268–280},
numpages = {13},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16881,
author = {Imielinski, Tomasz},
title = {Query Processing in Deductive Databases with Incomplete Information},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16881},
doi = {10.1145/16856.16881},
abstract = {We study here automated deduction in databases in the presence of various types of inference rules of the form of Horn Clauses with Skolem functions. These inference rules are typical for databases with incomplete information. We demonstrate a number of results related to processing of conjunctive queries for different types of database intensions. In particular, we show that when a database intension is built from possibly cyclic inclusion dependencies and view definitions any conjunctive query can be translated to the an equivalent form which can be evaluated directly over the database extension (disregarding inference rules). We also demonstrate that the complexity of query processing significantly grows when we mix incomplete information with recursive rules. In particular, we demonstrate here that even the power of least fixpoint extension of first order logic may be not sufficient to process queries in the presence of incomplete data and recursive rules. The same is demonstrated in case disjunctive information is allowed in the database.},
journal = {SIGMOD Rec.},
month = jun,
pages = {268–280},
numpages = {13}
}

@inproceedings{10.1145/16894.16882,
author = {Chen, Qiming},
title = {A Rule-Based Object/Task Modelling Approach},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16882},
doi = {10.1145/16894.16882},
abstract = {A rule-based object/task modelling approach is proposed which is characterized by specifying object behaviors and domain rules in terms of object-oriented logic programming, and specifying tasks and meta-rules in terms of network-oriented formalism. In addition the concepts of associations, virtual objects, multiple level integrity control and net expressions are introduced. The object-oriented logic programming system is extended for supporting the semantic modelling, and an explicit control knowledge representation mechanism is developed. This approach may be viewed as a step to the integration of object-oriented programming, logic programming, semantic modelling and event modelling, and to the combination of forward chaining and backward chaining techniques. Therefore, it can provide complementary benefits in deductive query support, integrity control, explicit control knowledge representation and intelligent user interface, and enhance the flexibility and extendibility of knowledge based systems to accommodate applications in multiple domains, towards a generalized, rule-based management of data, action and operational schemes. This approach is being designed and partially implemented on top of System C [Chen 85b] on a VAX computer.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {281–292},
numpages = {12},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16882,
author = {Chen, Qiming},
title = {A Rule-Based Object/Task Modelling Approach},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16882},
doi = {10.1145/16856.16882},
abstract = {A rule-based object/task modelling approach is proposed which is characterized by specifying object behaviors and domain rules in terms of object-oriented logic programming, and specifying tasks and meta-rules in terms of network-oriented formalism. In addition the concepts of associations, virtual objects, multiple level integrity control and net expressions are introduced. The object-oriented logic programming system is extended for supporting the semantic modelling, and an explicit control knowledge representation mechanism is developed. This approach may be viewed as a step to the integration of object-oriented programming, logic programming, semantic modelling and event modelling, and to the combination of forward chaining and backward chaining techniques. Therefore, it can provide complementary benefits in deductive query support, integrity control, explicit control knowledge representation and intelligent user interface, and enhance the flexibility and extendibility of knowledge based systems to accommodate applications in multiple domains, towards a generalized, rule-based management of data, action and operational schemes. This approach is being designed and partially implemented on top of System C [Chen 85b] on a VAX computer.},
journal = {SIGMOD Rec.},
month = jun,
pages = {281–292},
numpages = {12}
}

@inproceedings{10.1145/16894.16883,
author = {Anderson, T. Lougenia and Ariav, Gad},
title = {Panel: User Interfaces and Database Management Systems},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16883},
doi = {10.1145/16894.16883},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {293–294},
numpages = {2},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16883,
author = {Anderson, T. Lougenia and Ariav, Gad},
title = {Panel: User Interfaces and Database Management Systems},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16883},
doi = {10.1145/16856.16883},
journal = {SIGMOD Rec.},
month = jun,
pages = {293–294},
numpages = {2}
}

@inproceedings{10.1145/16894.16884,
author = {Christodoulakis, S. and Ho, F. and Theodoridou, M.},
title = {The Multimedia Object Presentation Manager of MINOS: A Symmetric Approach},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16884},
doi = {10.1145/16894.16884},
abstract = {Large multimedia data bases become feasible due to recent advances in hardware technology. A very important component of multimedia data base management systems will be the presentation manager which will be responsible for effective multimedia presentation and browsing on the screen of workstations.In this paper we present the functions provided for multimedia presentation and browsing in MINOS, a multimedia information system. The presentation and browsing capabilities provided make effective use of the capabilities of a modern workstation to increase the man-machine communication bandwidth. We regard voice as an important means of communication. Symmetric capabilities for text and voice browsing are provided.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {295–310},
numpages = {16},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16884,
author = {Christodoulakis, S. and Ho, F. and Theodoridou, M.},
title = {The Multimedia Object Presentation Manager of MINOS: A Symmetric Approach},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16884},
doi = {10.1145/16856.16884},
abstract = {Large multimedia data bases become feasible due to recent advances in hardware technology. A very important component of multimedia data base management systems will be the presentation manager which will be responsible for effective multimedia presentation and browsing on the screen of workstations.In this paper we present the functions provided for multimedia presentation and browsing in MINOS, a multimedia information system. The presentation and browsing capabilities provided make effective use of the capabilities of a modern workstation to increase the man-machine communication bandwidth. We regard voice as an important means of communication. Symmetric capabilities for text and voice browsing are provided.},
journal = {SIGMOD Rec.},
month = jun,
pages = {295–310},
numpages = {16}
}

@inproceedings{10.1145/16894.16885,
author = {Woelk, Darrell and Kim, Won and Luther, Willis},
title = {An Object-Oriented Approach to Multimedia Databases},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16885},
doi = {10.1145/16894.16885},
abstract = {This paper identifies data modelling and data access and sharing requirements which multimedia applications impose on a database system. It shows the capabilities of an object-based data model and indicates extensions which are needed to meet the data modelling aspects of these requirements. A logical implementation of the operations on the model is described. The model generalizes the notions of instantiation and generalization in the standard object-oriented paradigm, and augments it with the notions of aggregation and relationships which are specialized for a multimedia application environment. Objects may exist in aggregation hierarchies which provide the capability to integrate diverse types of multimedia information such as text, sound, bit-mapped images, and complex graphics drawings. Objects may also be linked through other user-defined relationships to capture such application functions as voice annotation and referencing of one document by another. Using this model, the semantics of aggregation and relationships in a multimedia application environment can be understood and efficiently supported by a database system.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {311–325},
numpages = {15},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16885,
author = {Woelk, Darrell and Kim, Won and Luther, Willis},
title = {An Object-Oriented Approach to Multimedia Databases},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16885},
doi = {10.1145/16856.16885},
abstract = {This paper identifies data modelling and data access and sharing requirements which multimedia applications impose on a database system. It shows the capabilities of an object-based data model and indicates extensions which are needed to meet the data modelling aspects of these requirements. A logical implementation of the operations on the model is described. The model generalizes the notions of instantiation and generalization in the standard object-oriented paradigm, and augments it with the notions of aggregation and relationships which are specialized for a multimedia application environment. Objects may exist in aggregation hierarchies which provide the capability to integrate diverse types of multimedia information such as text, sound, bit-mapped images, and complex graphics drawings. Objects may also be linked through other user-defined relationships to capture such application functions as voice annotation and referencing of one document by another. Using this model, the semantics of aggregation and relationships in a multimedia application environment can be understood and efficiently supported by a database system.},
journal = {SIGMOD Rec.},
month = jun,
pages = {311–325},
numpages = {15}
}

@inproceedings{10.1145/16894.16886,
author = {Orenstein, Jack A.},
title = {Spatial Query Processing in an Object-Oriented Database System},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16886},
doi = {10.1145/16894.16886},
abstract = {DBMSs must offer spatial query processing capabilities to meet the needs of applications such as cartography, geographic information processing and CAD. Many data structures and algorithms that process grid representations of spatial data have appeared in the literature. We unify much of this work by identifying common principles and distilling them into a small set of constructs. (Published data structures and algorithms can be derived as special cases.) We show how these constructs can be supported with only minor modifications to current DBMS implementations. The ideas are demonstrated in the context of the range query problem. Analytical and experimental evidence indicates that performance of the derived solution is very good (e.g., comparable to performance of the kd tree.)},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {326–336},
numpages = {11},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16886,
author = {Orenstein, Jack A.},
title = {Spatial Query Processing in an Object-Oriented Database System},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16886},
doi = {10.1145/16856.16886},
abstract = {DBMSs must offer spatial query processing capabilities to meet the needs of applications such as cartography, geographic information processing and CAD. Many data structures and algorithms that process grid representations of spatial data have appeared in the literature. We unify much of this work by identifying common principles and distilling them into a small set of constructs. (Published data structures and algorithms can be derived as special cases.) We show how these constructs can be supported with only minor modifications to current DBMS implementations. The ideas are demonstrated in the context of the range query problem. Analytical and experimental evidence indicates that performance of the derived solution is very good (e.g., comparable to performance of the kd tree.)},
journal = {SIGMOD Rec.},
month = jun,
pages = {326–336},
numpages = {11}
}

@inproceedings{10.1145/16894.16887,
author = {Bitton, Dina},
title = {Panel: The Effect of Large Main Memory on Database Systems},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16887},
doi = {10.1145/16894.16887},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {337–339},
numpages = {3},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16887,
author = {Bitton, Dina},
title = {Panel: The Effect of Large Main Memory on Database Systems},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16887},
doi = {10.1145/16856.16887},
journal = {SIGMOD Rec.},
month = jun,
pages = {337–339},
numpages = {3}
}

@inproceedings{10.1145/16894.16888,
author = {Stonebraker, Michael and Rowe, Lawrence A.},
title = {The Design of POSTGRES},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16888},
doi = {10.1145/16894.16888},
abstract = {This paper presents the preliminary design of a new database management system, called POSTGRES, that is the successor to the INGRES relational database system. The main design goals of the new system are to
provide better support for complex objects,provide user extendibility for data types, operators and access methods,provide facilities for active databases (i.e., alerters and triggers) and inferencing including forward- and backward-chaining,simplify the DBMS code for crash recovery,produce a design that can take advantage of optical disks, workstations composed of multiple tightly-coupled processors, and custom designed VLSI chips, andmake as few changes as possible (preferably none) to the relational model.The paper describes the query language, programming language interface, system architecture, query processing strategy, and storage system for the new system.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {340–355},
numpages = {16},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16888,
author = {Stonebraker, Michael and Rowe, Lawrence A.},
title = {The Design of POSTGRES},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16888},
doi = {10.1145/16856.16888},
abstract = {This paper presents the preliminary design of a new database management system, called POSTGRES, that is the successor to the INGRES relational database system. The main design goals of the new system are to
provide better support for complex objects,provide user extendibility for data types, operators and access methods,provide facilities for active databases (i.e., alerters and triggers) and inferencing including forward- and backward-chaining,simplify the DBMS code for crash recovery,produce a design that can take advantage of optical disks, workstations composed of multiple tightly-coupled processors, and custom designed VLSI chips, andmake as few changes as possible (preferably none) to the relational model.The paper describes the query language, programming language interface, system architecture, query processing strategy, and storage system for the new system.},
journal = {SIGMOD Rec.},
month = jun,
pages = {340–355},
numpages = {16}
}

@inproceedings{10.1145/16894.16889,
author = {Dadam, P. and Kuespert, K. and Andersen, F. and Blanken, H. and Erbe, R.},
title = {A DBMS Prototype to Support Extended NF2 Relations: An Integrated View on Flat Tables and Hierarchies},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16889},
doi = {10.1145/16894.16889},
abstract = {Recently, extensions for relational database management systems (DBMS) have been proposed to support also hierarchical structures (complex objects). These extensions have been mainly implemented on top of an existing DBMS. Such an approach leads to many disadvantages not only from the conceptual point of view but also from performance aspects. Thus paper reports on a 3-year effort to design and prototype a DBMS to support a generalized relational data model, called extended NF2 (Non First Normal Form) data model which treats flat relations, lists, and hierarchical structures in a uniform way. The logical data model, a language for this model, and alternatives for storage structures to implement generalized relations are presented and discussed.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {356–367},
numpages = {12},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16889,
author = {Dadam, P. and Kuespert, K. and Andersen, F. and Blanken, H. and Erbe, R.},
title = {A DBMS Prototype to Support Extended NF2 Relations: An Integrated View on Flat Tables and Hierarchies},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16889},
doi = {10.1145/16856.16889},
abstract = {Recently, extensions for relational database management systems (DBMS) have been proposed to support also hierarchical structures (complex objects). These extensions have been mainly implemented on top of an existing DBMS. Such an approach leads to many disadvantages not only from the conceptual point of view but also from performance aspects. Thus paper reports on a 3-year effort to design and prototype a DBMS to support a generalized relational data model, called extended NF2 (Non First Normal Form) data model which treats flat relations, lists, and hierarchical structures in a uniform way. The logical data model, a language for this model, and alternatives for storage structures to implement generalized relations are presented and discussed.},
journal = {SIGMOD Rec.},
month = jun,
pages = {356–367},
numpages = {12}
}

@inproceedings{10.1145/16894.16890,
author = {Bocca, Jorge},
title = {On the Evaluation Strategy of EDUCE},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16890},
doi = {10.1145/16894.16890},
abstract = {Educe is a logic programming system for handling large knowledge bases. It was constructed by fully integrating the logic programming language Prolog and the relational data base management system Ingres. Educe uses a hybrid strategy for the evaluation of queries. This strategy is based on two contrasting strategies. The strategy known as sets retrieval, transforms recursive and non-recursive queries into a form suitable for evaluation by a relational data base management system. The other strategy, known as one-tuple-at-a time, evaluates queries by imitating the evaluation strategy of the programming language Prolog. In earlier versions of Educe, users selected the strategy by using two different query languages. In order to remove this responsibility from the user, algorithms to map expressions from either of the languages into the other were implemented and added to Educe. This paper briefly reviews the implementation of both evaluators and the mappings compares the basic strategies of evaluation, and then proceeds to explain Educe's own strategy.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {368–378},
numpages = {11},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16890,
author = {Bocca, Jorge},
title = {On the Evaluation Strategy of EDUCE},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16890},
doi = {10.1145/16856.16890},
abstract = {Educe is a logic programming system for handling large knowledge bases. It was constructed by fully integrating the logic programming language Prolog and the relational data base management system Ingres. Educe uses a hybrid strategy for the evaluation of queries. This strategy is based on two contrasting strategies. The strategy known as sets retrieval, transforms recursive and non-recursive queries into a form suitable for evaluation by a relational data base management system. The other strategy, known as one-tuple-at-a time, evaluates queries by imitating the evaluation strategy of the programming language Prolog. In earlier versions of Educe, users selected the strategy by using two different query languages. In order to remove this responsibility from the user, algorithms to map expressions from either of the languages into the other were implemented and added to Educe. This paper briefly reviews the implementation of both evaluators and the mappings compares the basic strategies of evaluation, and then proceeds to explain Educe's own strategy.},
journal = {SIGMOD Rec.},
month = jun,
pages = {368–378},
numpages = {11}
}

@inproceedings{10.1145/16894.16891,
author = {Katz, Randy H. and Chang, Ellis and Bhateja, Rajiv},
title = {Version Modeling Concepts for Computer-Aided Design Databases},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16891},
doi = {10.1145/16894.16891},
abstract = {We describe a semantic object-oriented data model for representing how a complex design database evolves over time. Structural relationships, introduced by the data management system, are imposed on the objects created by existing CAD tools. The relationships supported by the model are (1) version histories, (2) time-varying configurations, and (3) equivalences among objects of different types. We describe mechanisms for (1) identifying current versions, (2) supporting dynamic configuration binding, and (3) verifying equivalence relationships. The data model is being implemented in a Version Server, under development at the University of California, Berkeley.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {379–386},
numpages = {8},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16891,
author = {Katz, Randy H. and Chang, Ellis and Bhateja, Rajiv},
title = {Version Modeling Concepts for Computer-Aided Design Databases},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16891},
doi = {10.1145/16856.16891},
abstract = {We describe a semantic object-oriented data model for representing how a complex design database evolves over time. Structural relationships, introduced by the data management system, are imposed on the objects created by existing CAD tools. The relationships supported by the model are (1) version histories, (2) time-varying configurations, and (3) equivalences among objects of different types. We describe mechanisms for (1) identifying current versions, (2) supporting dynamic configuration binding, and (3) verifying equivalence relationships. The data model is being implemented in a Version Server, under development at the University of California, Berkeley.},
journal = {SIGMOD Rec.},
month = jun,
pages = {379–386},
numpages = {8}
}

@inproceedings{10.1145/16894.16892,
author = {Lipeck, Udo W.},
title = {Stepwise Specification of Dynamic Database Behaviour},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16892},
doi = {10.1145/16894.16892},
abstract = {This paper presents a methodology for the stepwise specification of dynamic database behaviour. A conceptual schema is described in three levels: data, objects and transactions. To determine which sequences of database states are “admissible”, integrity constraints on objects are given in temporal logic. Transactions are specified by pre/postconditions to produce “executable” state sequences. In order to guarantee that executable state sequences already become admissible, integrity constraints are completely transformed into additional pre/postconditions. We introduce general rules for these transformations. Thus, schema specifications can be refined and simplified systematically.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {387–397},
numpages = {11},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16892,
author = {Lipeck, Udo W.},
title = {Stepwise Specification of Dynamic Database Behaviour},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16892},
doi = {10.1145/16856.16892},
abstract = {This paper presents a methodology for the stepwise specification of dynamic database behaviour. A conceptual schema is described in three levels: data, objects and transactions. To determine which sequences of database states are “admissible”, integrity constraints on objects are given in temporal logic. Transactions are specified by pre/postconditions to produce “executable” state sequences. In order to guarantee that executable state sequences already become admissible, integrity constraints are completely transformed into additional pre/postconditions. We introduce general rules for these transformations. Thus, schema specifications can be refined and simplified systematically.},
journal = {SIGMOD Rec.},
month = jun,
pages = {387–397},
numpages = {11}
}

@inproceedings{10.1145/16894.16893,
author = {Biskup, Joachim and Convent, Bernhard},
title = {A Formal View Integration Method},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16893},
doi = {10.1145/16894.16893},
abstract = {The design of an appropriate conceptual database scheme is one of the most difficult tasks in usual database applications. Especially, the design of a common global database scheme for many different user groups requires a great amount of effort and skill, because the desired scheme should fit a great variety of requirements and expectations. Here, view integration is a natural method that should help to manage the complexity of such a design problem. For each user group the requirements and expectations are separately collected and specified as views, that are subsequently integrated into a global scheme supporting all those different views.In this paper, we carefully develop a formal model, clarifying many notions and concepts, related to the view integration method. This formal model serves as a theoretical basis of our integration approach that uses equivalence preserving, local scheme transformations as the main integration operations.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {398–407},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16893,
author = {Biskup, Joachim and Convent, Bernhard},
title = {A Formal View Integration Method},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16893},
doi = {10.1145/16856.16893},
abstract = {The design of an appropriate conceptual database scheme is one of the most difficult tasks in usual database applications. Especially, the design of a common global database scheme for many different user groups requires a great amount of effort and skill, because the desired scheme should fit a great variety of requirements and expectations. Here, view integration is a natural method that should help to manage the complexity of such a design problem. For each user group the requirements and expectations are separately collected and specified as views, that are subsequently integrated into a global scheme supporting all those different views.In this paper, we carefully develop a formal model, clarifying many notions and concepts, related to the view integration method. This formal model serves as a theoretical basis of our integration approach that uses equivalence preserving, local scheme transformations as the main integration operations.},
journal = {SIGMOD Rec.},
month = jun,
pages = {398–407},
numpages = {10}
}

