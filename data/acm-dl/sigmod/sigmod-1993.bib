@inproceedings{10.1145/170035.170038,
author = {Stonebraker, Michael and Frew, Jim and Gardels, Kenn and Meredith, Jeff},
title = {The SEQUOIA 2000 Storage Benchmark},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170038},
doi = {10.1145/170035.170038},
abstract = {This paper presents a benchmark that concisely captures the data base requirements of a collection of Earth Scientists working in the SEQUOIA 2000 project on various aspects of global change research. This benchmark has the novel characteristic that it uses real data sets and real queries that are representative of Earth Science tasks. Because it appears that Earth Science problems are typical of the problems of engineering and scientific DBMS users, we claim that this benchmark represents the needs of this more general community. Also included in the paper are benchmark results for three example DBMSs: GRASS, IPW and POSTGRES.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {2–11},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170038,
author = {Stonebraker, Michael and Frew, Jim and Gardels, Kenn and Meredith, Jeff},
title = {The SEQUOIA 2000 Storage Benchmark},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170038},
doi = {10.1145/170036.170038},
abstract = {This paper presents a benchmark that concisely captures the data base requirements of a collection of Earth Scientists working in the SEQUOIA 2000 project on various aspects of global change research. This benchmark has the novel characteristic that it uses real data sets and real queries that are representative of Earth Science tasks. Because it appears that Earth Science problems are typical of the problems of engineering and scientific DBMS users, we claim that this benchmark represents the needs of this more general community. Also included in the paper are benchmark results for three example DBMSs: GRASS, IPW and POSTGRES.},
journal = {SIGMOD Rec.},
month = jun,
pages = {2–11},
numpages = {10}
}

@inproceedings{10.1145/170035.170041,
author = {Carey, Michael J. and DeWitt, David J. and Naughton, Jeffrey F.},
title = {The 007 Benchmark},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170041},
doi = {10.1145/170035.170041},
abstract = {The OO7 Benchmark represents a comprehensive test of OODBMS performance. In this paper we describe the benchmark and present performance results from its implementation in three OODBMS systems. It is our hope that the OO7 Benchmark will provide useful insight for end-users evaluating the performance of OODBMS systems; we also hope that the research community will find that OO7 provides a database schema, instance, and workload that is useful for evaluating new techniques and algorithms for OODBMS implementation.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {12–21},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170041,
author = {Carey, Michael J. and DeWitt, David J. and Naughton, Jeffrey F.},
title = {The 007 Benchmark},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170041},
doi = {10.1145/170036.170041},
abstract = {The OO7 Benchmark represents a comprehensive test of OODBMS performance. In this paper we describe the benchmark and present performance results from its implementation in three OODBMS systems. It is our hope that the OO7 Benchmark will provide useful insight for end-users evaluating the performance of OODBMS systems; we also hope that the research community will find that OO7 provides a database schema, instance, and workload that is useful for evaluating new techniques and algorithms for OODBMS implementation.},
journal = {SIGMOD Rec.},
month = jun,
pages = {12–21},
numpages = {10}
}

@inproceedings{10.1145/170035.170042,
author = {Leutenegger, Scott T. and Dias, Daniel},
title = {A Modeling Study of the TPC-C Benchmark},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170042},
doi = {10.1145/170035.170042},
abstract = {The TPC-C benchmark is a new benchmark approved by the TPC council intended for comparing database platforms running a medium complexity transaction processing workload. Some key aspects in which this new benchmark differs from the TPC-A benchmark are in having several transaction types, some of which are more complex than that in TPC-A, and in having data access skew. In this paper we present results from a modelling study of the TPC-C benchmark for both single node and distributed database management systems. We simulate the TPC-C workload to determine expected buffer miss rates assuming an LRU buffer management policy. These miss rates are then used as inputs to a throughput model. From these models we show the following: (i) We quantify the data access skew as specified in the benchmark and show what fraction of the accesses go to what fraction of the data. (ii) We quantify the resulting buffer hit ratios for each relation as a function of buffer size. (iii) We show that close to linear scale-up (about 3% from the ideal) can be achieved in a distributed system, assuming replication of a read-only table. (iv) We examine the effect of packing hot tuples into pages and show that significant price/performance benefit can be thus achieved. (v) Finally, by coupling the buffer simulations with the throughput model, we examine typical disk/memory configurations that maximize the overall price/performance.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {22–31},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170042,
author = {Leutenegger, Scott T. and Dias, Daniel},
title = {A Modeling Study of the TPC-C Benchmark},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170042},
doi = {10.1145/170036.170042},
abstract = {The TPC-C benchmark is a new benchmark approved by the TPC council intended for comparing database platforms running a medium complexity transaction processing workload. Some key aspects in which this new benchmark differs from the TPC-A benchmark are in having several transaction types, some of which are more complex than that in TPC-A, and in having data access skew. In this paper we present results from a modelling study of the TPC-C benchmark for both single node and distributed database management systems. We simulate the TPC-C workload to determine expected buffer miss rates assuming an LRU buffer management policy. These miss rates are then used as inputs to a throughput model. From these models we show the following: (i) We quantify the data access skew as specified in the benchmark and show what fraction of the accesses go to what fraction of the data. (ii) We quantify the resulting buffer hit ratios for each relation as a function of buffer size. (iii) We show that close to linear scale-up (about 3% from the ideal) can be achieved in a distributed system, assuming replication of a read-only table. (iv) We examine the effect of packing hot tuples into pages and show that significant price/performance benefit can be thus achieved. (v) Finally, by coupling the buffer simulations with the throughput model, we examine typical disk/memory configurations that maximize the overall price/performance.},
journal = {SIGMOD Rec.},
month = jun,
pages = {22–31},
numpages = {10}
}

@inproceedings{10.1145/170035.170044,
author = {Abiteboul, Serge and Lausen, Georg and Uphoff, Heinz and Waller, Emmanuel},
title = {Methods and Rules},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170044},
doi = {10.1145/170035.170044},
abstract = {We show how classical datalog semantics can be used directly and very simply to provide semantics to a syntactic extension of datalog with methods, classes, inheritance, overloading and late binding. Several approaches to resolution are considered, implemented in the model, and formally compared. They range from resolution in C++ style to original kinds of resolution suggested by the declarative nature of the language. We show connections to view specification and a further extension allowing runtime derivation of the class hierarchy.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {32–41},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170044,
author = {Abiteboul, Serge and Lausen, Georg and Uphoff, Heinz and Waller, Emmanuel},
title = {Methods and Rules},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170044},
doi = {10.1145/170036.170044},
abstract = {We show how classical datalog semantics can be used directly and very simply to provide semantics to a syntactic extension of datalog with methods, classes, inheritance, overloading and late binding. Several approaches to resolution are considered, implemented in the model, and formally compared. They range from resolution in C++ style to original kinds of resolution suggested by the declarative nature of the language. We show connections to view specification and a further extension allowing runtime derivation of the class hierarchy.},
journal = {SIGMOD Rec.},
month = jun,
pages = {32–41},
numpages = {10}
}

@inproceedings{10.1145/170035.170047,
author = {Brant, David A. and Miranker, Daniel P.},
title = {Index Support for Rule Activation},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170047},
doi = {10.1145/170035.170047},
abstract = {Integrated rule and database systems are quickly moving from the research laboratory into commercial systems. However, the current generation of prototypes are designed to work with small rule sets involving limited inferencing. The problem of supporting large complex rule programs within database management systems still presents significant challenges. The basis for many of these challenges is providing support for rule activation. Rule activation is defined as the process of determining which rules are satisfied and what data satisfies them. In this paper we present performance results for the DATEX database rule system and its novel indexing technique for supporting rule activation. Our approach assumes that both the rule program and the database must be optimized synergistically. However, as an experimental result we have determined that DATEX requires very few changes to a standard DBMS environment, and we argue that these changes are reasonable for the problems being solved. Based on the performance of DATEX we believe we have demonstrated a satisfactory solution to the rule activation problem for complex rule programs operating within a database system.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {42–48},
numpages = {7},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170047,
author = {Brant, David A. and Miranker, Daniel P.},
title = {Index Support for Rule Activation},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170047},
doi = {10.1145/170036.170047},
abstract = {Integrated rule and database systems are quickly moving from the research laboratory into commercial systems. However, the current generation of prototypes are designed to work with small rule sets involving limited inferencing. The problem of supporting large complex rule programs within database management systems still presents significant challenges. The basis for many of these challenges is providing support for rule activation. Rule activation is defined as the process of determining which rules are satisfied and what data satisfies them. In this paper we present performance results for the DATEX database rule system and its novel indexing technique for supporting rule activation. Our approach assumes that both the rule program and the database must be optimized synergistically. However, as an experimental result we have determined that DATEX requires very few changes to a standard DBMS environment, and we argue that these changes are reasonable for the problems being solved. Based on the performance of DATEX we believe we have demonstrated a satisfactory solution to the rule activation problem for complex rule programs operating within a database system.},
journal = {SIGMOD Rec.},
month = jun,
pages = {42–48},
numpages = {7}
}

@inproceedings{10.1145/170035.170048,
author = {Gupta, Ashish and Widom, Jennifer},
title = {Local Verification of Global Integrity Constraints in Distributed Databases},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170048},
doi = {10.1145/170035.170048},
abstract = {We present an optimization for integrity constraint verification in distributed databases. The optimization allows a global constraint, i.e. a constraint spanning multiple databases, to be verified by accessing data at a single database, eliminating the cost of accessing remote data. The optimization is based on an algorithm that takes as input a global constraint and data to be inserted into a local database. The algorithm produces a local condition such that if the local data satisfies this condition then, based on the previous satisfaction of the global constraint, the global constraint is still satisfied. If the local data does not satisfy the condition, then a conventional global verification procedure is required.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {49–58},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170048,
author = {Gupta, Ashish and Widom, Jennifer},
title = {Local Verification of Global Integrity Constraints in Distributed Databases},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170048},
doi = {10.1145/170036.170048},
abstract = {We present an optimization for integrity constraint verification in distributed databases. The optimization allows a global constraint, i.e. a constraint spanning multiple databases, to be verified by accessing data at a single database, eliminating the cost of accessing remote data. The optimization is based on an algorithm that takes as input a global constraint and data to be inserted into a local database. The algorithm produces a local condition such that if the local data satisfies this condition then, based on the previous satisfaction of the global constraint, the global constraint is still satisfied. If the local data does not satisfy the condition, then a conventional global verification procedure is required.},
journal = {SIGMOD Rec.},
month = jun,
pages = {49–58},
numpages = {10}
}

@inproceedings{10.1145/170035.170051,
author = {Pang, Hwee Hwa and Carey, Michael J. and Livny, Miron},
title = {Partially Preemptible Hash Joins},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170051},
doi = {10.1145/170035.170051},
abstract = {With the advent of real-time and goal-oriented database systems, priority scheduling is likely to be an important feature in future database management systems. A consequence of priority scheduling is that a transaction may lose its buffers to higher-priority transactions, and may be given additional memory when transactions leave the system. Due to their heavy reliance on main memory, hash joins are especially vulnerable to fluctuations in memory availability. Previous studies have proposed modifications to the hash join algorithm to cope with these fluctuations, but the proposed algorithms have not been extensively evaluated or compared with each other. This paper contains a performance study of these algorithms. In addition, we introduce a family of memory-adaptive hash join algorithms that turns out to offer even better solutions to the memory fluctuation problem that hash joins experience.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {59–68},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170051,
author = {Pang, Hwee Hwa and Carey, Michael J. and Livny, Miron},
title = {Partially Preemptible Hash Joins},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170051},
doi = {10.1145/170036.170051},
abstract = {With the advent of real-time and goal-oriented database systems, priority scheduling is likely to be an important feature in future database management systems. A consequence of priority scheduling is that a transaction may lose its buffers to higher-priority transactions, and may be given additional memory when transactions leave the system. Due to their heavy reliance on main memory, hash joins are especially vulnerable to fluctuations in memory availability. Previous studies have proposed modifications to the hash join algorithm to cope with these fluctuations, but the proposed algorithms have not been extensively evaluated or compared with each other. This paper contains a performance study of these algorithms. In addition, we introduce a family of memory-adaptive hash join algorithms that turns out to offer even better solutions to the memory fluctuation problem that hash joins experience.},
journal = {SIGMOD Rec.},
month = jun,
pages = {59–68},
numpages = {10}
}

@inproceedings{10.1145/170035.170053,
author = {Lo, Ming-Ling and Chen, Ming-Syan Syan and Ravishankar, C. V. and Yu, Philip S.},
title = {On Optimal Processor Allocation to Support Pipelined Hash Joins},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170053},
doi = {10.1145/170035.170053},
abstract = {In this paper, we develop algorithms to achieve optimal processor allocation for pipelined hash joins in a multiprocessor-based database system. A pipeline of hash joins is composed of several stages, each of which is associated with one join operation. The whole pipeline is executed in two phases: (1) the table-building phase, and (2) the tuple-probing phase. We focus on the problem of allocating processors to the stages of a pipeline to minimize the query execution time. We formulate the processor allocation problem as a two-phase mini-max optimization problem, and develop three optimal allocation schemes under three different constraints. The effectiveness of our problem formulation and solution is verified through a detailed tuple-by-tuple simulation of pipelined hash joins. Our solution scheme is general and applicable to any optimal resource allocation problem formulated as a two-phase mini-max problem.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {69–78},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170053,
author = {Lo, Ming-Ling and Chen, Ming-Syan Syan and Ravishankar, C. V. and Yu, Philip S.},
title = {On Optimal Processor Allocation to Support Pipelined Hash Joins},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170053},
doi = {10.1145/170036.170053},
abstract = {In this paper, we develop algorithms to achieve optimal processor allocation for pipelined hash joins in a multiprocessor-based database system. A pipeline of hash joins is composed of several stages, each of which is associated with one join operation. The whole pipeline is executed in two phases: (1) the table-building phase, and (2) the tuple-probing phase. We focus on the problem of allocating processors to the stages of a pipeline to minimize the query execution time. We formulate the processor allocation problem as a two-phase mini-max optimization problem, and develop three optimal allocation schemes under three different constraints. The effectiveness of our problem formulation and solution is verified through a detailed tuple-by-tuple simulation of pipelined hash joins. Our solution scheme is general and applicable to any optimal resource allocation problem formulated as a two-phase mini-max problem.},
journal = {SIGMOD Rec.},
month = jun,
pages = {69–78},
numpages = {10}
}

@inproceedings{10.1145/170035.170055,
author = {Sun, Wei and Ling, Yibei and Rishe, Naphtali and Deng, Yi},
title = {An Instant and Accurate Size Estimation Method for Joins and Selections in a Retrieval-Intensive Environment},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170055},
doi = {10.1145/170035.170055},
abstract = {This paper proposes a novel strategy for estimating the size of the resulting relation after an equi-join and selection using a regression model. An approximating series representing the underlying data distribution and dependency is derived from the actual data. The proposed method provides an instant and accurate size estimation by performing an evaluation of the series, with no run-time overheads in page faults and space, and with negligible CPU overhead. In contrast, the popular sampling methods incur run-time overheads in page faults (for sampling), CPU time and space. These overheads of sampling methods increase the response time of processing a query. The results of a comprehensive experimental study are also reported, which demonstrate that the estimation accuracy by the proposed method is comparable with that of the sampling methods which are believed to provide the most accurate estimation. The proposed method seems ideal for retrieval-intensive database and information systems. Since the overheads involved in deriving the approximating series are fairly moderate, we believe that this method is also an extremely competent method when moderate or periodical updates are present.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {79–88},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170055,
author = {Sun, Wei and Ling, Yibei and Rishe, Naphtali and Deng, Yi},
title = {An Instant and Accurate Size Estimation Method for Joins and Selections in a Retrieval-Intensive Environment},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170055},
doi = {10.1145/170036.170055},
abstract = {This paper proposes a novel strategy for estimating the size of the resulting relation after an equi-join and selection using a regression model. An approximating series representing the underlying data distribution and dependency is derived from the actual data. The proposed method provides an instant and accurate size estimation by performing an evaluation of the series, with no run-time overheads in page faults and space, and with negligible CPU overhead. In contrast, the popular sampling methods incur run-time overheads in page faults (for sampling), CPU time and space. These overheads of sampling methods increase the response time of processing a query. The results of a comprehensive experimental study are also reported, which demonstrate that the estimation accuracy by the proposed method is comparable with that of the sampling methods which are believed to provide the most accurate estimation. The proposed method seems ideal for retrieval-intensive database and information systems. Since the overheads involved in deriving the approximating series are fairly moderate, we believe that this method is also an extremely competent method when moderate or periodical updates are present.},
journal = {SIGMOD Rec.},
month = jun,
pages = {79–88},
numpages = {10}
}

@inproceedings{10.1145/170035.170057,
author = {Meseguer, Jos\'{e} and Qian, Xiaolei},
title = {A Logical Semantics for Object-Oriented Databases},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170057},
doi = {10.1145/170035.170057},
abstract = {Although the mathematical foundations of relational databases are very well established, the state of affairs for object-oriented databases is much less satisfactory. We propose a semantic foundation for object-oriented databases based on a simple logic of change called rewriting logic, and a language called MaudeLog that is based on that logic. Some key advantages of our approach include its logical nature, its simplicity without any need for higher-order features, the fact that dynamic aspects are directly addressed, the rigorous integration of user-definable algebraic data types within the framework, the existence of initial models, and the integration of query, update, and programming aspects within a single declarative language.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {89–98},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170057,
author = {Meseguer, Jos\'{e} and Qian, Xiaolei},
title = {A Logical Semantics for Object-Oriented Databases},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170057},
doi = {10.1145/170036.170057},
abstract = {Although the mathematical foundations of relational databases are very well established, the state of affairs for object-oriented databases is much less satisfactory. We propose a semantic foundation for object-oriented databases based on a simple logic of change called rewriting logic, and a language called MaudeLog that is based on that logic. Some key advantages of our approach include its logical nature, its simplicity without any need for higher-order features, the fact that dynamic aspects are directly addressed, the rigorous integration of user-definable algebraic data types within the framework, the existence of initial models, and the integration of query, update, and programming aspects within a single declarative language.},
journal = {SIGMOD Rec.},
month = jun,
pages = {89–98},
numpages = {10}
}

@inproceedings{10.1145/170035.170059,
author = {Anwar, E. and Maugis, L. and Chakravarthy, S.},
title = {A New Perspective on Rule Support for Object-Oriented Databases},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170059},
doi = {10.1145/170035.170059},
abstract = {This paper proposes a new approach for supporting reactive capability in an object-oriented database. We introduce an event interface, which extends the conventional object semantics to include the role of an event generator. This interface provides a basis for the specification of events spanning sets of objects, possibly from different classes, and detection of primitive and complex events. This approach clearly separated event detection from rules. New rules can be added and use existing objects, enabling objects to react to their own changes as well as to the changes of other objects.We use a runtime subscription mechanism, between rules and objects to selectively monitor particular objects dynamically. This elegantly supports class level as well as instance level rules. Both events and rules are treated as first class objects.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {99–108},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170059,
author = {Anwar, E. and Maugis, L. and Chakravarthy, S.},
title = {A New Perspective on Rule Support for Object-Oriented Databases},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170059},
doi = {10.1145/170036.170059},
abstract = {This paper proposes a new approach for supporting reactive capability in an object-oriented database. We introduce an event interface, which extends the conventional object semantics to include the role of an event generator. This interface provides a basis for the specification of events spanning sets of objects, possibly from different classes, and detection of primitive and complex events. This approach clearly separated event detection from rules. New rules can be added and use existing objects, enabling objects to react to their own changes as well as to the changes of other objects.We use a runtime subscription mechanism, between rules and objects to selectively monitor particular objects dynamically. This elegantly supports class level as well as instance level rules. Both events and rules are treated as first class objects.},
journal = {SIGMOD Rec.},
month = jun,
pages = {99–108},
numpages = {10}
}

@inproceedings{10.1145/170035.170061,
author = {Ananthanarayanan, R. and Gottemukkala, V. and Kaefer, W. and Lehman, T. J. and Pirahesh, H.},
title = {Using the Co-Existence Approach to Achieve Combined Functionality of Object-Oriented and Relational Systems},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170061},
doi = {10.1145/170035.170061},
abstract = {Once considered a novelty, object oriented systems have now entered the mainstream. Their impressive performance and rich type systems have created a demand for object oriented features in other areas, such as relational database systems. We believe the current efforts to combine object oriented and relational features into a single hybrid system will fall short of the mark, whereas our approach, the co-existence approach, has the distinction of requiring far less work, but at the same time promising both the desired functionality and performance. We describe the attributes of our co-existing systems, an object oriented system (C++) and a relational system (Starburst), and show how this combination supports the desired features of both object-oriented and relational systems.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {109–118},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170061,
author = {Ananthanarayanan, R. and Gottemukkala, V. and Kaefer, W. and Lehman, T. J. and Pirahesh, H.},
title = {Using the Co-Existence Approach to Achieve Combined Functionality of Object-Oriented and Relational Systems},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170061},
doi = {10.1145/170036.170061},
abstract = {Once considered a novelty, object oriented systems have now entered the mainstream. Their impressive performance and rich type systems have created a demand for object oriented features in other areas, such as relational database systems. We believe the current efforts to combine object oriented and relational features into a single hybrid system will fall short of the mark, whereas our approach, the co-existence approach, has the distinction of requiring far less work, but at the same time promising both the desired functionality and performance. We describe the attributes of our co-existing systems, an object oriented system (C++) and a relational system (Starburst), and show how this combination supports the desired features of both object-oriented and relational systems.},
journal = {SIGMOD Rec.},
month = jun,
pages = {109–118},
numpages = {10}
}

@inproceedings{10.1145/170035.170062,
author = {Shatdal, Ambuj and Naughton, Jeffrey F.},
title = {Using Shared Virtual Memory for Parallel Join Processing},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170062},
doi = {10.1145/170035.170062},
abstract = {In this paper, we show that shared virtual memory, in a shared-nothing multiprocessor, facilitates the design and implementation of parallel join processing algorithms that perform significantly better in the presence of skew than previously proposed parallel join processing algorithms. We propose two variants of an algorithm for parallel join processing using shared virtual memory, and perform a detailed simulation to investigate their performance. The algorithm is unique in that it employs both the shared virtual memory paradigm and the message-passing paradigm used by current shared-nothing parallel database systems. The implementation of the algorithm requires few modifications to existing shared-nothing parallel database systems.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {119–128},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170062,
author = {Shatdal, Ambuj and Naughton, Jeffrey F.},
title = {Using Shared Virtual Memory for Parallel Join Processing},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170062},
doi = {10.1145/170036.170062},
abstract = {In this paper, we show that shared virtual memory, in a shared-nothing multiprocessor, facilitates the design and implementation of parallel join processing algorithms that perform significantly better in the presence of skew than previously proposed parallel join processing algorithms. We propose two variants of an algorithm for parallel join processing using shared virtual memory, and perform a detailed simulation to investigate their performance. The algorithm is unique in that it employs both the shared virtual memory paradigm and the message-passing paradigm used by current shared-nothing parallel database systems. The implementation of the algorithm requires few modifications to existing shared-nothing parallel database systems.},
journal = {SIGMOD Rec.},
month = jun,
pages = {119–128},
numpages = {10}
}

@inproceedings{10.1145/170035.170063,
author = {Tomasic, Anthony and Garcia-Molina, Hector},
title = {Caching and Database Scaling in Distributed Shared-Nothing Information Retrieval Systems},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170063},
doi = {10.1145/170035.170063},
abstract = {A common class of existing information retrieval system provides access to abstracts. For example Stanford University, through its FOLIO system, provides access to the INSPECT database of abstracts of the literature on physics, computer science, electrical engineering, etc. In this paper this database is studied by using a trace-driven simulation. We focus on physical index design, inverted index caching, and database scaling in a distributed shared-nothing system. All three issues are shown to have a strong effect on response time and throughput. Database scaling is explored in two ways. One way assumes an “optimal” configuration for a single host and then linearly scales the database by duplicating the host architecture as needed. The second way determines the optimal number of hosts given a fixed database size.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {129–138},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170063,
author = {Tomasic, Anthony and Garcia-Molina, Hector},
title = {Caching and Database Scaling in Distributed Shared-Nothing Information Retrieval Systems},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170063},
doi = {10.1145/170036.170063},
abstract = {A common class of existing information retrieval system provides access to abstracts. For example Stanford University, through its FOLIO system, provides access to the INSPECT database of abstracts of the literature on physics, computer science, electrical engineering, etc. In this paper this database is studied by using a trace-driven simulation. We focus on physical index design, inverted index caching, and database scaling in a distributed shared-nothing system. All three issues are shown to have a strong effect on response time and throughput. Database scaling is explored in two ways. One way assumes an “optimal” configuration for a single host and then linearly scales the database by duplicating the host architecture as needed. The second way determines the optimal number of hosts given a fixed database size.},
journal = {SIGMOD Rec.},
month = jun,
pages = {129–138},
numpages = {10}
}

@inproceedings{10.1145/170035.170064,
author = {Mohan, C. and Narang, Inderpal},
title = {An Efficient and Flexible Method for Archiving a Data Base},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170064},
doi = {10.1145/170035.170064},
abstract = {We describe an efficient method for supporting incremental and full archiving of data bases (e.g., individual files). Customers archive their data bases quite frequently to minimize the duration of data outage. Because of the growing sizes of data bases and the ever increasing need for high availability of data, the efficiency of the archive copy utility is very important. The method presented here minimizes interferences with concurrent transactions by not acquiring any locks on the data being copied. It significantly reduces disk I/Os by not keeping on data pages any extra tracking information in connection with archiving. These features make the archive copy operation be more efficient in terms of resource consumption compared to other methods. The method is also flexible in that it optionally supports direct copying of data from disks, bypassing the DBMS's buffer pool. This reduces buffer pool pollution and processing overheads, and allows the utility to take advantage of device geometries for efficiently retrieving data. We also describe extensions to the method to accommodate the multisystem shared disks transaction environment. The method tolerates gracefully system failures during the archive copy operation.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {139–146},
numpages = {8},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170064,
author = {Mohan, C. and Narang, Inderpal},
title = {An Efficient and Flexible Method for Archiving a Data Base},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170064},
doi = {10.1145/170036.170064},
abstract = {We describe an efficient method for supporting incremental and full archiving of data bases (e.g., individual files). Customers archive their data bases quite frequently to minimize the duration of data outage. Because of the growing sizes of data bases and the ever increasing need for high availability of data, the efficiency of the archive copy utility is very important. The method presented here minimizes interferences with concurrent transactions by not acquiring any locks on the data being copied. It significantly reduces disk I/Os by not keeping on data pages any extra tracking information in connection with archiving. These features make the archive copy operation be more efficient in terms of resource consumption compared to other methods. The method is also flexible in that it optionally supports direct copying of data from disks, bypassing the DBMS's buffer pool. This reduces buffer pool pollution and processing overheads, and allows the utility to take advantage of device geometries for efficiently retrieving data. We also describe extensions to the method to accommodate the multisystem shared disks transaction environment. The method tolerates gracefully system failures during the archive copy operation.},
journal = {SIGMOD Rec.},
month = jun,
pages = {139–146},
numpages = {8}
}

@inproceedings{10.1145/170035.170065,
author = {Derr, Marcia A. and Morishita, Shinichi and Phipps, Geoffrey},
title = {Design and Implementation of the Glue-Nail Database System},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170065},
doi = {10.1145/170035.170065},
abstract = {We describe the design and implementation of the Glue-Nail database system. The Nail language is a purely declarative query language; Glue is a procedural language used for non-query activities. The two languages combined are sufficient to write a complete application. Nail and Glue code both compile into the target language IGlue. The Nail compiler uses variants of the magic sets algorithm, and supports well-founded models. Static optimization is performed by the Glue compiler using techniques that include peephole methods and data flow analysis. The IGlue code is executed by the IGlue interpreter, which features a run-time adaptive optimizer. The three optimizers each deal with separate optimization domains, and experiments indicate that an effective synergism is achieved. The Glue-Nail system is largely complete and has been tested using a suite of representative applications.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {147–156},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170065,
author = {Derr, Marcia A. and Morishita, Shinichi and Phipps, Geoffrey},
title = {Design and Implementation of the Glue-Nail Database System},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170065},
doi = {10.1145/170036.170065},
abstract = {We describe the design and implementation of the Glue-Nail database system. The Nail language is a purely declarative query language; Glue is a procedural language used for non-query activities. The two languages combined are sufficient to write a complete application. Nail and Glue code both compile into the target language IGlue. The Nail compiler uses variants of the magic sets algorithm, and supports well-founded models. Static optimization is performed by the Glue compiler using techniques that include peephole methods and data flow analysis. The IGlue code is executed by the IGlue interpreter, which features a run-time adaptive optimizer. The three optimizers each deal with separate optimization domains, and experiments indicate that an effective synergism is achieved. The Glue-Nail system is largely complete and has been tested using a suite of representative applications.},
journal = {SIGMOD Rec.},
month = jun,
pages = {147–156},
numpages = {10}
}

@inproceedings{10.1145/170035.170066,
author = {Gupta, Ashish and Mumick, Inderpal Singh and Subrahmanian, V. S.},
title = {Maintaining Views Incrementally},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170066},
doi = {10.1145/170035.170066},
abstract = {We present incremental evaluation algorithms to compute changes to materialized views in relational and deductive database systems, in response to changes (insertions, deletions, and updates) to the relations. The view definitions can be in SQL or Datalog, and may use UNION, negation, aggregation (e.g. SUM, MIN), linear recursion, and general recursion.We first present a counting algorithm that tracks the number of alternative derivations (counts) for each derived tuple in a view. The algorithm works with both set and duplicate semantics. We present the algorithm for nonrecursive views (with negation and aggregation), and show that the count for a tuple can be computed at little or no cost above the cost of deriving the tuple. The algorithm is optimal in that it computes exactly those view tuples that are inserted or deleted. Note that we store only the number of derivations, not the derivations themselves.We then present the Delete and Rederive algorithm, DRed, for incremental maintenance of recursive views (negation and aggregation are permitted). The algorithm works by first deleting a superset of the tuples that need to be deleted, and then rederiving some of them. The algorithm can also be used when the view definition is itself altered.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {157–166},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170066,
author = {Gupta, Ashish and Mumick, Inderpal Singh and Subrahmanian, V. S.},
title = {Maintaining Views Incrementally},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170066},
doi = {10.1145/170036.170066},
abstract = {We present incremental evaluation algorithms to compute changes to materialized views in relational and deductive database systems, in response to changes (insertions, deletions, and updates) to the relations. The view definitions can be in SQL or Datalog, and may use UNION, negation, aggregation (e.g. SUM, MIN), linear recursion, and general recursion.We first present a counting algorithm that tracks the number of alternative derivations (counts) for each derived tuple in a view. The algorithm works with both set and duplicate semantics. We present the algorithm for nonrecursive views (with negation and aggregation), and show that the count for a tuple can be computed at little or no cost above the cost of deriving the tuple. The algorithm is optimal in that it computes exactly those view tuples that are inserted or deleted. Note that we store only the number of derivations, not the derivations themselves.We then present the Delete and Rederive algorithm, DRed, for incremental maintenance of recursive views (negation and aggregation are permitted). The algorithm works by first deleting a superset of the tuples that need to be deleted, and then rederiving some of them. The algorithm can also be used when the view definition is itself altered.},
journal = {SIGMOD Rec.},
month = jun,
pages = {157–166},
numpages = {10}
}

@inproceedings{10.1145/170035.170067,
author = {Ramakrishnan, Raghu and Srivastava, Divesh and Sudarshan, S. and Seshadri, Praveen},
title = {Implementation of the CORAL Deductive Database System},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170067},
doi = {10.1145/170035.170067},
abstract = {CORAL is a deductive database system that supports a rich declarative language, provides a wide range of evaluation methods, and allows a combination of declarative and imperative programming. The data can be persistent on disk or can reside in main-memory. We describe the architecture and implementation of CORAL.There were two important goals in the design of the CORAL architecture: (1) to integrate the different evaluation strategies in a reasonable fashion, and (2) to allow users to influence the optimization techniques used so as to exploit the full power of the CORAL implementation. A CORAL declarative program can be organized as a collection of interacting modules and this modular structure is the key to satisfying both these goals. The high level module interface allows modules with different evaluation techniques to interact in a transparent fashion. Further, users can optionally tailor the execution of a program by selecting from among a wide range of control choices at the level of each module.CORAL also has an interface with C++, and users can program in a combination of declarative CORAL, and C++ extended with CORAL primitives. A high degree of extensibility is provided by allowing C++ programmers to use the class structure of C++ to enhance the CORAL implementation.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {167–176},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170067,
author = {Ramakrishnan, Raghu and Srivastava, Divesh and Sudarshan, S. and Seshadri, Praveen},
title = {Implementation of the CORAL Deductive Database System},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170067},
doi = {10.1145/170036.170067},
abstract = {CORAL is a deductive database system that supports a rich declarative language, provides a wide range of evaluation methods, and allows a combination of declarative and imperative programming. The data can be persistent on disk or can reside in main-memory. We describe the architecture and implementation of CORAL.There were two important goals in the design of the CORAL architecture: (1) to integrate the different evaluation strategies in a reasonable fashion, and (2) to allow users to influence the optimization techniques used so as to exploit the full power of the CORAL implementation. A CORAL declarative program can be organized as a collection of interacting modules and this modular structure is the key to satisfying both these goals. The high level module interface allows modules with different evaluation techniques to interact in a transparent fashion. Further, users can optionally tailor the execution of a program by selecting from among a wide range of control choices at the level of each module.CORAL also has an interface with C++, and users can program in a combination of declarative CORAL, and C++ extended with CORAL primitives. A high degree of extensibility is provided by allowing C++ programmers to use the class structure of C++ to enhance the CORAL implementation.},
journal = {SIGMOD Rec.},
month = jun,
pages = {167–176},
numpages = {10}
}

@inproceedings{10.1145/170035.170068,
author = {Kolodner, Elliot K. and Weihl, William E.},
title = {Atomic Incremental Garbage Collection and Recovery for a Large Stable Heap},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170068},
doi = {10.1145/170035.170068},
abstract = {A stable heap is storage that is managed automatically using garbage collection, manipulated using atomic transactions, and accessed using a uniform storage model. These features enhance reliability and simplify programming by preventing errors due to explicit deallocation, by masking failures and concurrency using transactions, and by eliminating the distinction between accessing temporary storage and permanent storage. Stable heap management is useful for programming languages for reliable distributed computing, programming languages with persistent storage, and object-oriented database systems.Many applications that could benefit from a stable heap (e.g., computer-aided design, computer-aided software engineering, and office information systems) require large amounts of storage, timely responses for transactions, and high availability. We present garbage collection and recovery algorithms for a stable heap implementation that meet these goals and are appropriate for stock hardware. The collector is incremental: it does not attempt to collect the whole heap at once. The collector is also atomic: it is coordinated with the recovery system to prevent problems when it moves and modifies objects. The time for recovery is independent of heap size, even if a failure occurs during garbage collection.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {177–186},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170068,
author = {Kolodner, Elliot K. and Weihl, William E.},
title = {Atomic Incremental Garbage Collection and Recovery for a Large Stable Heap},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170068},
doi = {10.1145/170036.170068},
abstract = {A stable heap is storage that is managed automatically using garbage collection, manipulated using atomic transactions, and accessed using a uniform storage model. These features enhance reliability and simplify programming by preventing errors due to explicit deallocation, by masking failures and concurrency using transactions, and by eliminating the distinction between accessing temporary storage and permanent storage. Stable heap management is useful for programming languages for reliable distributed computing, programming languages with persistent storage, and object-oriented database systems.Many applications that could benefit from a stable heap (e.g., computer-aided design, computer-aided software engineering, and office information systems) require large amounts of storage, timely responses for transactions, and high availability. We present garbage collection and recovery algorithms for a stable heap implementation that meet these goals and are appropriate for stock hardware. The collector is incremental: it does not attempt to collect the whole heap at once. The collector is also atomic: it is coordinated with the recovery system to prevent problems when it moves and modifies objects. The time for recovery is independent of heap size, even if a failure occurs during garbage collection.},
journal = {SIGMOD Rec.},
month = jun,
pages = {177–186},
numpages = {10}
}

@inproceedings{10.1145/170035.170070,
author = {Keen, John S. and Dally, William J.},
title = {Performance Evaluation of Ephemeral Logging},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170070},
doi = {10.1145/170035.170070},
abstract = {Ephemeral logging (EL) is a new technique for managing a log of database activity on disk. It does not require periodic checkpoints and does not abort lengthy transactions as frequently as traditional firewall logging for the same amount of disk space. Therefore, it is well suited for highly concurrent databases and applications which have a wide distribution of transaction lifetimes.This paper briefly explains EL and then analyzes its performance. Simulation studies indicate that it can offer significant savings in disk space, at the expense of slightly higher bandwidth for logging and more main memory. The reduced size of the log implies much faster recovery after a crash as well as cost savings.EL is the method of choice in some but not all situations. We assess the limitations of our current knowledge about EL and suggest promising directions for further research.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {187–196},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170070,
author = {Keen, John S. and Dally, William J.},
title = {Performance Evaluation of Ephemeral Logging},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170070},
doi = {10.1145/170036.170070},
abstract = {Ephemeral logging (EL) is a new technique for managing a log of database activity on disk. It does not require periodic checkpoints and does not abort lengthy transactions as frequently as traditional firewall logging for the same amount of disk space. Therefore, it is well suited for highly concurrent databases and applications which have a wide distribution of transaction lifetimes.This paper briefly explains EL and then analyzes its performance. Simulation studies indicate that it can offer significant savings in disk space, at the expense of slightly higher bandwidth for logging and more main memory. The reduced size of the log implies much faster recovery after a crash as well as cost savings.EL is the method of choice in some but not all situations. We assess the limitations of our current knowledge about EL and suggest promising directions for further research.},
journal = {SIGMOD Rec.},
month = jun,
pages = {187–196},
numpages = {10}
}

@inproceedings{10.1145/170035.170071,
author = {Hong, D. and Johnson, T. and Chakravarthy, S.},
title = {Real-Time Transaction Scheduling: A Cost Conscious Approach},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170071},
doi = {10.1145/170035.170071},
abstract = {Real-time databases are an important component of embedded real-time systems. In a real-time database context, transactions must not only maintain the consistency constraints of the database but must also satisfy the timing constraints specified for each transaction. Although several approaches have been proposed to integrate real-time scheduling and database concurrency control methods, none of them take into account the dynamic cost of scheduling a transaction. In this paper, we propose a new cost conscious real-time transaction scheduling algorithm which considers dynamic costs associated with a transaction. Our dynamic priority assignment algorithm adapts to changes in the system load without causing excessive numbers of transaction restarts. Our simulations show its superiority over EDF-HP algorithm.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {197–206},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170071,
author = {Hong, D. and Johnson, T. and Chakravarthy, S.},
title = {Real-Time Transaction Scheduling: A Cost Conscious Approach},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170071},
doi = {10.1145/170036.170071},
abstract = {Real-time databases are an important component of embedded real-time systems. In a real-time database context, transactions must not only maintain the consistency constraints of the database but must also satisfy the timing constraints specified for each transaction. Although several approaches have been proposed to integrate real-time scheduling and database concurrency control methods, none of them take into account the dynamic cost of scheduling a transaction. In this paper, we propose a new cost conscious real-time transaction scheduling algorithm which considers dynamic costs associated with a transaction. Our dynamic priority assignment algorithm adapts to changes in the system load without causing excessive numbers of transaction restarts. Our simulations show its superiority over EDF-HP algorithm.},
journal = {SIGMOD Rec.},
month = jun,
pages = {197–206},
numpages = {10}
}

@inproceedings{10.1145/170035.170072,
author = {Agrawal, Rakesh and Imieli\'{n}ski, Tomasz and Swami, Arun},
title = {Mining Association Rules between Sets of Items in Large Databases},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170072},
doi = {10.1145/170035.170072},
abstract = {We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {207–216},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170072,
author = {Agrawal, Rakesh and Imieli\'{n}ski, Tomasz and Swami, Arun},
title = {Mining Association Rules between Sets of Items in Large Databases},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170072},
doi = {10.1145/170036.170072},
abstract = {We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm.},
journal = {SIGMOD Rec.},
month = jun,
pages = {207–216},
numpages = {10}
}

@inproceedings{10.1145/170035.170073,
author = {Borgida, Alex and Brachman, Ronald J.},
title = {Loading Data into Description Reasoners},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170073},
doi = {10.1145/170035.170073},
abstract = {Knowledge-base management systems (KBMS) based on description logics are being used in a variety of situations where access is needed to large amounts of data stored in existing relational databases. We present the architecture and algorithms of a system that converts most of the inferences made by the KBMS into a collection of SQL queries, thereby relying on the optimization facilities of existing DBMS to gain efficiency, while maintaining an object-centered view of the world with a substantive semantics and significantly different reasoning facilities than those provided by Relational DBMS and their deductive extensions. We address a number of optimization issues that arise in the translation process due to the fact that SQL queries with different syntax (but identical semantics) are not treated uniformly by current database management systems.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {217–226},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170073,
author = {Borgida, Alex and Brachman, Ronald J.},
title = {Loading Data into Description Reasoners},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170073},
doi = {10.1145/170036.170073},
abstract = {Knowledge-base management systems (KBMS) based on description logics are being used in a variety of situations where access is needed to large amounts of data stored in existing relational databases. We present the architecture and algorithms of a system that converts most of the inferences made by the KBMS into a collection of SQL queries, thereby relying on the optimization facilities of existing DBMS to gain efficiency, while maintaining an object-centered view of the world with a substantive semantics and significantly different reasoning facilities than those provided by Relational DBMS and their deductive extensions. We address a number of optimization issues that arise in the translation process due to the fact that SQL queries with different syntax (but identical semantics) are not treated uniformly by current database management systems.},
journal = {SIGMOD Rec.},
month = jun,
pages = {217–226},
numpages = {10}
}

@inproceedings{10.1145/170035.170074,
author = {Wang, X. Sean and Jajodia, Sushil and Subrahmanian, V. S.},
title = {Temporal Modules: An Approach toward Federated Temporal Databases},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170074},
doi = {10.1145/170035.170074},
abstract = {In a federated database environment, different constituents of the federation may use different temporal models or physical representations for temporal information. This paper introduces a new concept, called a temporal module, to resolve these differences, or mismatches, among the constituents. Intuitively, a temporal module hides the implementation details of a temporal relation by exposing its information only through two windowing functions: The first function associates each time point with a set of tuples and the second function links each tuple to a set of time points. A calculus-style language is given to form queries on temporal modules.Temporal modules are then extended to resolve another type of mismatch among the constituents of a federation, namely, the mismatch involving different time units (e.g., month, week and day) used to record temporal information. Our solution relies on “information conversions” provided by each constituent. Specifically, each temporal module is extended to provide several “windows” to its information, each in terms of a different time unit. The first step to process a query addressed to the federation is to select suitable windows to the underlying temporal modules. In order to facilitate such a process, time units are formally defined and studied. A federated temporal database model and its query language are proposed. The query language is an extension of the above calculus-style language.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {227–236},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170074,
author = {Wang, X. Sean and Jajodia, Sushil and Subrahmanian, V. S.},
title = {Temporal Modules: An Approach toward Federated Temporal Databases},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170074},
doi = {10.1145/170036.170074},
abstract = {In a federated database environment, different constituents of the federation may use different temporal models or physical representations for temporal information. This paper introduces a new concept, called a temporal module, to resolve these differences, or mismatches, among the constituents. Intuitively, a temporal module hides the implementation details of a temporal relation by exposing its information only through two windowing functions: The first function associates each time point with a set of tuples and the second function links each tuple to a set of time points. A calculus-style language is given to form queries on temporal modules.Temporal modules are then extended to resolve another type of mismatch among the constituents of a federation, namely, the mismatch involving different time units (e.g., month, week and day) used to record temporal information. Our solution relies on “information conversions” provided by each constituent. Specifically, each temporal module is extended to provide several “windows” to its information, each in terms of a different time unit. The first step to process a query addressed to the federation is to select suitable windows to the underlying temporal modules. In order to facilitate such a process, time units are formally defined and studied. A federated temporal database model and its query language are proposed. The query language is an extension of the above calculus-style language.},
journal = {SIGMOD Rec.},
month = jun,
pages = {227–236},
numpages = {10}
}

@inproceedings{10.1145/170035.170075,
author = {Brinkhoff, Thomas and Kriegel, Hans-Peter and Seeger, Bernhard},
title = {Efficient Processing of Spatial Joins Using R-Trees},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170075},
doi = {10.1145/170035.170075},
abstract = {Spatial joins are one of the most important operations for combining spatial objects of several relations. The efficient processing of a spatial join is extremely important since its execution time is superlinear in the number of spatial objects of the participating relations, and this number of objects may be very high. In this paper, we present a first detailed study of spatial join processing using R-trees, particularly R*-trees. R-trees are very suitable for supporting spatial queries and the R*-tree is one of the most efficient members of the R-tree family. Starting from a straightforward approach, we present several techniques for improving its execution time with respect to both, CPU- and I/O-time. Eventually, we end up with an algorithm whose total execution time is improved over the first approach by an order of magnitude. Using a buffer of reasonable size, I/O-time is almost optimal, i.e. it almost corresponds to the time for reading each required page of the relations exactly once. The performance of the various approaches is investigated in an experimental performance comparison where several large data sets from real applications are used.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {237–246},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170075,
author = {Brinkhoff, Thomas and Kriegel, Hans-Peter and Seeger, Bernhard},
title = {Efficient Processing of Spatial Joins Using R-Trees},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170075},
doi = {10.1145/170036.170075},
abstract = {Spatial joins are one of the most important operations for combining spatial objects of several relations. The efficient processing of a spatial join is extremely important since its execution time is superlinear in the number of spatial objects of the participating relations, and this number of objects may be very high. In this paper, we present a first detailed study of spatial join processing using R-trees, particularly R*-trees. R-trees are very suitable for supporting spatial queries and the R*-tree is one of the most efficient members of the R-tree family. Starting from a straightforward approach, we present several techniques for improving its execution time with respect to both, CPU- and I/O-time. Eventually, we end up with an algorithm whose total execution time is improved over the first approach by an order of magnitude. Using a buffer of reasonable size, I/O-time is almost optimal, i.e. it almost corresponds to the time for reading each required page of the relations exactly once. The performance of the various approaches is investigated in an experimental performance comparison where several large data sets from real applications are used.},
journal = {SIGMOD Rec.},
month = jun,
pages = {237–246},
numpages = {10}
}

@inproceedings{10.1145/170035.170076,
author = {Ishikawa, Yoshiharu and Kitagawa, Hiroyuki and Ohbo, Nobuo},
title = {Evaluation of Signature Files as Set Access Facilities in OODBs},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170076},
doi = {10.1145/170035.170076},
abstract = {Object-oriented database systems (OODBs) need efficient support for manipulation of complex objects. In particular, support of queries involving evaluations of set predicates is often required in handling complex objects. In this paper, we propose a scheme to apply signature file techniques, which were originally invented for text retrieval, to the support of set value accesses, and quantitatively evaluate their potential capabilities. Two signature file organizations, the sequential signature file and the bit-sliced signature file, are considered and their performance is compared with that of the nested index for queries involving the set inclusion operator (⊆). We develop a detailed cost model and present analytical results clarifying their retrieval, storage, and update costs. Our analysis shows that the bit-sliced signature file is a very promising set access facility in OODBs.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {247–256},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170076,
author = {Ishikawa, Yoshiharu and Kitagawa, Hiroyuki and Ohbo, Nobuo},
title = {Evaluation of Signature Files as Set Access Facilities in OODBs},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170076},
doi = {10.1145/170036.170076},
abstract = {Object-oriented database systems (OODBs) need efficient support for manipulation of complex objects. In particular, support of queries involving evaluations of set predicates is often required in handling complex objects. In this paper, we propose a scheme to apply signature file techniques, which were originally invented for text retrieval, to the support of set value accesses, and quantitatively evaluate their potential capabilities. Two signature file organizations, the sequential signature file and the bit-sliced signature file, are considered and their performance is compared with that of the nested index for queries involving the set inclusion operator (⊆). We develop a detailed cost model and present analytical results clarifying their retrieval, storage, and update costs. Our analysis shows that the bit-sliced signature file is a very promising set access facility in OODBs.},
journal = {SIGMOD Rec.},
month = jun,
pages = {247–256},
numpages = {10}
}

@inproceedings{10.1145/170035.170077,
author = {Curewitz, Kenneth M. and Krishnan, P. and Vitter, Jeffrey Scott},
title = {Practical Prefetching via Data Compression},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170077},
doi = {10.1145/170035.170077},
abstract = {An important issue that affects response time performance in current OODB and hypertext systems is the I/O involved in moving objects from slow memory to cache. A promising way to tackle this problem is to use prefetching, in which we predict the user's next page requests and get those pages into cache in the background. Current databases perform limited prefetching using techniques derived from older virtual memory systems. A novel idea of using data compression techniques for prefetching was recently advocated in [KrV, ViK], in which prefetchers based on the Lempel-Ziv data compressor (the UNIX compress command) were shown theoretically to be optimal in the limit. In this paper we analyze the practical aspects of using data compression techniques for prefetching. We adapt three well-known data compressors to get three simple, deterministic, and universal prefetchers. We simulate our prefetchers on sequences of page accesses derived from the OO1 and OO7 benchmarks and from CAD applications, and demonstrate significant reductions in fault-rate. We examine the important issues of cache replacement, size of the data structure used by the prefetcher, and problems arising from bursts of “fast” page requests (that leave virtually no time between adjacent requests for prefetching and book keeping). We conclude that prediction for prefetching based on data compression techniques holds great promise.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {257–266},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170077,
author = {Curewitz, Kenneth M. and Krishnan, P. and Vitter, Jeffrey Scott},
title = {Practical Prefetching via Data Compression},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170077},
doi = {10.1145/170036.170077},
abstract = {An important issue that affects response time performance in current OODB and hypertext systems is the I/O involved in moving objects from slow memory to cache. A promising way to tackle this problem is to use prefetching, in which we predict the user's next page requests and get those pages into cache in the background. Current databases perform limited prefetching using techniques derived from older virtual memory systems. A novel idea of using data compression techniques for prefetching was recently advocated in [KrV, ViK], in which prefetchers based on the Lempel-Ziv data compressor (the UNIX compress command) were shown theoretically to be optimal in the limit. In this paper we analyze the practical aspects of using data compression techniques for prefetching. We adapt three well-known data compressors to get three simple, deterministic, and universal prefetchers. We simulate our prefetchers on sequences of page accesses derived from the OO1 and OO7 benchmarks and from CAD applications, and demonstrate significant reductions in fault-rate. We examine the important issues of cache replacement, size of the data structure used by the prefetcher, and problems arising from bursts of “fast” page requests (that leave virtually no time between adjacent requests for prefetching and book keeping). We conclude that prediction for prefetching based on data compression techniques holds great promise.},
journal = {SIGMOD Rec.},
month = jun,
pages = {257–266},
numpages = {10}
}

@inproceedings{10.1145/170035.170078,
author = {Hellerstein, Joseph M. and Stonebraker, Michael},
title = {Predicate Migration: Optimizing Queries with Expensive Predicates},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170078},
doi = {10.1145/170035.170078},
abstract = {The traditional focus of relational query optimization schemes has been on the choice of join methods and join orders. Restrictions have typically been handled in query optimizers by “predicate pushdown” rules, which apply restrictions in some random order before as many joins as possible. These rules work under the assumption that restriction is essentially a zero-time operation. However, today's extensible and object-oriented database systems allow users to define time-consuming functions, which may be used in a query's restriction and join predicates. Furthermore, SQL has long supported subquery predicates, which may be arbitrarily time-consuming to check. Thus restrictions should not be considered zero-time operations, and the model of query optimization must be enhanced.In this paper we develop a theory for moving expensive predicates in a query plan so that the total cost of the plan — including the costs of both joins and restrictions — is minimal. We present an algorithm to implement the theory, as well as results of our implementation in POSTGRES. Our experience with the newly enhanced POSTGRES query optimizer demonstrates that correctly optimizing queries with expensive predicates often produces plans that are orders of magnitude faster than plans generated by a traditional query optimizer. The additional complexity of considering expensive predicates during optimization is found to be manageably small.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {267–276},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170078,
author = {Hellerstein, Joseph M. and Stonebraker, Michael},
title = {Predicate Migration: Optimizing Queries with Expensive Predicates},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170078},
doi = {10.1145/170036.170078},
abstract = {The traditional focus of relational query optimization schemes has been on the choice of join methods and join orders. Restrictions have typically been handled in query optimizers by “predicate pushdown” rules, which apply restrictions in some random order before as many joins as possible. These rules work under the assumption that restriction is essentially a zero-time operation. However, today's extensible and object-oriented database systems allow users to define time-consuming functions, which may be used in a query's restriction and join predicates. Furthermore, SQL has long supported subquery predicates, which may be arbitrarily time-consuming to check. Thus restrictions should not be considered zero-time operations, and the model of query optimization must be enhanced.In this paper we develop a theory for moving expensive predicates in a query plan so that the total cost of the plan — including the costs of both joins and restrictions — is minimal. We present an algorithm to implement the theory, as well as results of our implementation in POSTGRES. Our experience with the newly enhanced POSTGRES query optimizer demonstrates that correctly optimizing queries with expensive predicates often produces plans that are orders of magnitude faster than plans generated by a traditional query optimizer. The additional complexity of considering expensive predicates during optimization is found to be manageably small.},
journal = {SIGMOD Rec.},
month = jun,
pages = {267–276},
numpages = {10}
}

@inproceedings{10.1145/170035.170079,
author = {G\"{u}ting, Ralf Hartmut},
title = {Second-Order Signature: A Tool for Specifying Data Models, Query Processing, and Optimization},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170079},
doi = {10.1145/170035.170079},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {277–286},
numpages = {10},
keywords = {data model, extensibility, algebra, functional programming, polymorphism, optimization, query processing, signature, type system},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170079,
author = {G\"{u}ting, Ralf Hartmut},
title = {Second-Order Signature: A Tool for Specifying Data Models, Query Processing, and Optimization},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170079},
doi = {10.1145/170036.170079},
journal = {SIGMOD Rec.},
month = jun,
pages = {277–286},
numpages = {10},
keywords = {optimization, data model, query processing, functional programming, algebra, polymorphism, type system, signature, extensibility}
}

@inproceedings{10.1145/170035.170080,
author = {Blakeley, Jos\'{e} A. and McKenna, William J. and Graefe, Goetz},
title = {Experiences Building the Open OODB Query Optimizer},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170080},
doi = {10.1145/170035.170080},
abstract = {This paper reports our experiences building the query optimizer for TI's Open OODB system. To the best of our knowledge, it is the first working object query optimizer to be based on a complete extensible optimization framework including logical algebra, execution algorithms, property enforcers, logical transformation rules, implementation rules, and selectivity and cost estimation. Our algebra incorporates a new materialize operator with its corresponding logical transformation and implementation rules that enable the optimization of path expressions. Initial experiments on queries obtained from the object query optimization literature demonstrate that our optimizer is able to derive plans that are as efficient as, and often substantially more efficient than, the plans generated by other query optimization strategies. These experiments demonstrate that our initial choices for populating each part of our optimization framework are reasonable. Our experience also shows that having a complete optimization framework is crucial for two reasons. First, it allows the optimizer to discover plans that cannot be revealed by exploring only the alternatives provided by the logical algebra and its transformations. Second, it helps and forces the database system designer to consider all parts of the framework and to maintain a good balance of choices when incorporating a new logical operator, execution algorithm, transformation rule, or implementation rule. The Open OODB query optimizer was constructed using the Volcano Optimizer Generator, demonstrating that this second-generation optimizer generator enables rapid development of efficient and effective query optimizers for non-standard data models and systems.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {287–296},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170080,
author = {Blakeley, Jos\'{e} A. and McKenna, William J. and Graefe, Goetz},
title = {Experiences Building the Open OODB Query Optimizer},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170080},
doi = {10.1145/170036.170080},
abstract = {This paper reports our experiences building the query optimizer for TI's Open OODB system. To the best of our knowledge, it is the first working object query optimizer to be based on a complete extensible optimization framework including logical algebra, execution algorithms, property enforcers, logical transformation rules, implementation rules, and selectivity and cost estimation. Our algebra incorporates a new materialize operator with its corresponding logical transformation and implementation rules that enable the optimization of path expressions. Initial experiments on queries obtained from the object query optimization literature demonstrate that our optimizer is able to derive plans that are as efficient as, and often substantially more efficient than, the plans generated by other query optimization strategies. These experiments demonstrate that our initial choices for populating each part of our optimization framework are reasonable. Our experience also shows that having a complete optimization framework is crucial for two reasons. First, it allows the optimizer to discover plans that cannot be revealed by exploring only the alternatives provided by the logical algebra and its transformations. Second, it helps and forces the database system designer to consider all parts of the framework and to maintain a good balance of choices when incorporating a new logical operator, execution algorithm, transformation rule, or implementation rule. The Open OODB query optimizer was constructed using the Volcano Optimizer Generator, demonstrating that this second-generation optimizer generator enables rapid development of efficient and effective query optimizers for non-standard data models and systems.},
journal = {SIGMOD Rec.},
month = jun,
pages = {287–296},
numpages = {10}
}

@inproceedings{10.1145/170035.170081,
author = {O'Neil, Elizabeth J. and O'Neil, Patrick E. and Weikum, Gerhard},
title = {The LRU-K Page Replacement Algorithm for Database Disk Buffering},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170081},
doi = {10.1145/170035.170081},
abstract = {This paper introduces a new approach to database disk buffering, called the LRU-K method. The basic idea of LRU-K is to keep track of the times of the last K references to popular database pages, using this information to statistically estimate the interarrival times of references on a page by page basis. Although the LRU-K approach performs optimal statistical inference under relatively standard assumptions, it is fairly simple and incurs little bookkeeping overhead. As we demonstrate with simulation experiments, the LRU-K algorithm surpasses conventional buffering algorithms in discriminating between frequently and infrequently referenced pages. In fact, LRU-K can approach the behavior of buffering algorithms in which page sets with known access frequencies are manually assigned to different buffer pools of specifically tuned sizes. Unlike such customized buffering algorithms however, the LRU-K method is self-tuning, and does not rely on external hints about workload characteristics. Furthermore, the LRU-K algorithm adapts in real time to changing patterns of access.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {297–306},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170081,
author = {O'Neil, Elizabeth J. and O'Neil, Patrick E. and Weikum, Gerhard},
title = {The LRU-K Page Replacement Algorithm for Database Disk Buffering},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170081},
doi = {10.1145/170036.170081},
abstract = {This paper introduces a new approach to database disk buffering, called the LRU-K method. The basic idea of LRU-K is to keep track of the times of the last K references to popular database pages, using this information to statistically estimate the interarrival times of references on a page by page basis. Although the LRU-K approach performs optimal statistical inference under relatively standard assumptions, it is fairly simple and incurs little bookkeeping overhead. As we demonstrate with simulation experiments, the LRU-K algorithm surpasses conventional buffering algorithms in discriminating between frequently and infrequently referenced pages. In fact, LRU-K can approach the behavior of buffering algorithms in which page sets with known access frequencies are manually assigned to different buffer pools of specifically tuned sizes. Unlike such customized buffering algorithms however, the LRU-K method is self-tuning, and does not rely on external hints about workload characteristics. Furthermore, the LRU-K algorithm adapts in real time to changing patterns of access.},
journal = {SIGMOD Rec.},
month = jun,
pages = {297–306},
numpages = {10}
}

@inproceedings{10.1145/170035.170082,
author = {Orji, Cyril U. and Solworth, Jon A.},
title = {Doubly Distorted Mirrors},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170082},
doi = {10.1145/170035.170082},
abstract = {Traditional mirrored disk systems provide high reliability by multiplexing disks. Performance is improved with parallel reads and shorter read seeks. However, writes must be performed by both disks, limiting performance.Doubly distorted mirrors increase the number of physical writes per logical write from 2 to 3, but performs logical writes more efficiently. This reduces the cost of a random logical write to 1/3 of the cost of a read. Moreover, much of the write cost can be absorbed in the rotational latency of the reads, performing under certain conditions all the writes for free. Doubly distorted mirrors achieves a 135% performance improvement over traditional mirrors in the TP1 benchmark. Although these techniques require a disk cache for writes, the cache need not be safe nor is recovery time impacted very much.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {307–316},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170082,
author = {Orji, Cyril U. and Solworth, Jon A.},
title = {Doubly Distorted Mirrors},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170082},
doi = {10.1145/170036.170082},
abstract = {Traditional mirrored disk systems provide high reliability by multiplexing disks. Performance is improved with parallel reads and shorter read seeks. However, writes must be performed by both disks, limiting performance.Doubly distorted mirrors increase the number of physical writes per logical write from 2 to 3, but performs logical writes more efficiently. This reduces the cost of a random logical write to 1/3 of the cost of a read. Moreover, much of the write cost can be absorbed in the rotational latency of the reads, performing under certain conditions all the writes for free. Doubly distorted mirrors achieves a 135% performance improvement over traditional mirrors in the TP1 benchmark. Although these techniques require a disk cache for writes, the cache need not be safe nor is recovery time impacted very much.},
journal = {SIGMOD Rec.},
month = jun,
pages = {307–316},
numpages = {10}
}

@inproceedings{10.1145/170035.170083,
author = {Hou, Robert Y. and Patt, Yale N.},
title = {Comparing Rebuild Algorithms for Mirrored and RAID5 Disk Arrays},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170083},
doi = {10.1145/170035.170083},
abstract = {Several disk array architectures have been proposed to provide high throughput for transaction processing applications. When a single disk in a redundant array fails, the array continues to operate, albeit in a degraded mode with a corresponding reduction in performance. In addition, the lost data must be rebuilt to a spare disk in a timely manner to reduce the probability of permanent data loss. Several researchers have proposed and examined algorithms for rebuilding the failed disk in a disk array with parity.We examine the use of these algorithms to rebuild a mirrored disk array and compare the rebuild time and performance of the RAID5 and mirrored arrays. Redirection of Reads provides comparable average response times and better rebuild times than Piggybacking for a mirrored array, whereas these two algorithms perform similarly for a RAID5 array. In our experiments comparing the two architectures, a mirrored array has more disks than a RAID5 array and can sustain 150% more I/Os per second during the rebuild process. Even if the size of the RAID5 array is increased to match the mirrored array, the mirrored array reduces response times by up to 60% and rebuild times by up to 45%.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {317–326},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170083,
author = {Hou, Robert Y. and Patt, Yale N.},
title = {Comparing Rebuild Algorithms for Mirrored and RAID5 Disk Arrays},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170083},
doi = {10.1145/170036.170083},
abstract = {Several disk array architectures have been proposed to provide high throughput for transaction processing applications. When a single disk in a redundant array fails, the array continues to operate, albeit in a degraded mode with a corresponding reduction in performance. In addition, the lost data must be rebuilt to a spare disk in a timely manner to reduce the probability of permanent data loss. Several researchers have proposed and examined algorithms for rebuilding the failed disk in a disk array with parity.We examine the use of these algorithms to rebuild a mirrored disk array and compare the rebuild time and performance of the RAID5 and mirrored arrays. Redirection of Reads provides comparable average response times and better rebuild times than Piggybacking for a mirrored array, whereas these two algorithms perform similarly for a RAID5 array. In our experiments comparing the two architectures, a mirrored array has more disks than a RAID5 array and can sustain 150% more I/Os per second during the rebuild process. Even if the size of the RAID5 array is increased to match the mirrored array, the mirrored array reduces response times by up to 60% and rebuild times by up to 45%.},
journal = {SIGMOD Rec.},
month = jun,
pages = {317–326},
numpages = {10}
}

@inproceedings{10.1145/170035.170084,
author = {Litwin, Witold and Neimat, Marie-Anne and Schneider, Donovan A.},
title = {LH: Linear Hashing for Distributed Files},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170084},
doi = {10.1145/170035.170084},
abstract = {LH* generalizes Linear Hashing to parallel or distributed RAM and disk files. An LH* file can be created from objects provided by any number of distributed and autonomous clients. It can grow gracefully, one bucket at a time, to virtually any number of servers. The number of messages per insertion is one in general, and three in the worst case. The number of messages per retrieval is two in general, and four in the worst case. The load factor can be about constant, 65-95%, depending on the file parameters. The file can also support parallel operations. An LH* file can be much faster than a single site disk file, and/or can hold a much larger number of objects. It can be more efficient than any file with a centralized directory, or a static parallel or distributed hash file.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {327–336},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170084,
author = {Litwin, Witold and Neimat, Marie-Anne and Schneider, Donovan A.},
title = {LH: Linear Hashing for Distributed Files},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170084},
doi = {10.1145/170036.170084},
abstract = {LH* generalizes Linear Hashing to parallel or distributed RAM and disk files. An LH* file can be created from objects provided by any number of distributed and autonomous clients. It can grow gracefully, one bucket at a time, to virtually any number of servers. The number of messages per insertion is one in general, and three in the worst case. The number of messages per retrieval is two in general, and four in the worst case. The load factor can be about constant, 65-95%, depending on the file parameters. The file can also support parallel operations. An LH* file can be much faster than a single site disk file, and/or can hold a much larger number of objects. It can be more efficient than any file with a centralized directory, or a static parallel or distributed hash file.},
journal = {SIGMOD Rec.},
month = jun,
pages = {327–336},
numpages = {10}
}

@inproceedings{10.1145/170035.170085,
author = {Johnson, Theodore and Krishna, Padmashree},
title = {Lazy Updates for Distributed Search Structure},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170085},
doi = {10.1145/170035.170085},
abstract = {Very large database systems require distributed storage, which means that they need distributed search structures for fast and efficient access to the data. In this paper, we present an approach to maintaining distributed data structures that uses lazy updates, which take advantage of the semantics of the search structure operations to allow for scalable and low-overhead replication. Lazy updates can be used to design distributed search structures that support very high levels of concurrency. The alternatives to lazy update algorithms (eager updates) use synchronization to ensure consistency, while lazy update algorithms avoid blocking. Since lazy updates avoid the use of synchronization, they are much easier to implement than eager update algorithms. We demonstrate the application of lazy updates to the dB-tree, which is a distributed B+ tree that replicates its interior nodes for highly parallel access. We develop a correctness theory for lazy updates so that our algorithms can be applied to other distributed search structures.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {337–346},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170085,
author = {Johnson, Theodore and Krishna, Padmashree},
title = {Lazy Updates for Distributed Search Structure},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170085},
doi = {10.1145/170036.170085},
abstract = {Very large database systems require distributed storage, which means that they need distributed search structures for fast and efficient access to the data. In this paper, we present an approach to maintaining distributed data structures that uses lazy updates, which take advantage of the semantics of the search structure operations to allow for scalable and low-overhead replication. Lazy updates can be used to design distributed search structures that support very high levels of concurrency. The alternatives to lazy update algorithms (eager updates) use synchronization to ensure consistency, while lazy update algorithms avoid blocking. Since lazy updates avoid the use of synchronization, they are much easier to implement than eager update algorithms. We demonstrate the application of lazy updates to the dB-tree, which is a distributed B+ tree that replicates its interior nodes for highly parallel access. We develop a correctness theory for lazy updates so that our algorithms can be applied to other distributed search structures.},
journal = {SIGMOD Rec.},
month = jun,
pages = {337–346},
numpages = {10}
}

@inproceedings{10.1145/170035.170086,
author = {Li, Jianzhong and Rotem, Doron and Srivastava, Jaideep},
title = {Algorithms for Loading Parallel Grid Files},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170086},
doi = {10.1145/170035.170086},
abstract = {The paper describes three fast loading algorithms for grid files on a parallel shared nothing architecture. The algorithms use dynamic programming and sampling to effectively partition the data file among the processors to achieve maximum parallelism in answering range queries. Each processor then constructs in parallel its own portion of the grid file. Analytical results and simulations are given for the three algorithms.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {347–356},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170086,
author = {Li, Jianzhong and Rotem, Doron and Srivastava, Jaideep},
title = {Algorithms for Loading Parallel Grid Files},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170086},
doi = {10.1145/170036.170086},
abstract = {The paper describes three fast loading algorithms for grid files on a parallel shared nothing architecture. The algorithms use dynamic programming and sampling to effectively partition the data file among the processors to achieve maximum parallelism in answering range queries. Each processor then constructs in parallel its own portion of the grid file. Analytical results and simulations are given for the three algorithms.},
journal = {SIGMOD Rec.},
month = jun,
pages = {347–356},
numpages = {10}
}

@inproceedings{10.1145/170035.170087,
author = {Vadaparty, K. and Aslandogan, Y. A. and Ozsoyoglu, G.},
title = {Towards a Unified Visual Database Access},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170087},
doi = {10.1145/170035.170087},
abstract = {Since the development of QBE, over fifty visual query languages have been proposed to facilitate easy database access. Although these languages have introduced some very useful paradigms, a number of these have some severe limitations, such as: (a) not extending beyond the relational model (b) not considering negation and safety, formally (c) using ad hoc constructs, with no analysis of expressivity or complexity done, etc. Note that visual database access is an important issue being revisted, with the emergence of different flavors of object-oriented databases. We believe that there is a need for developing a unified visual query language.Specifically, our goal is to develop a visual query language that has the following properties: (i) It has a few core constructs using which “expert-users” can define new (derived) constructs easily (ii) “Normal users” can use easily either the core or the derived constructs for database querying (iii) It can implement representative constructs of other (textual or visual) query language straightforwardly, and (iv) It has formal semantics, with its theoretical properties, such as complexity, analyzed.We believe that we make a first step towards the above goal by introducing a new logical construct called restricted universal quantifier and combining it with the hierarchical structure of windows to develop a Visual Query Language, called VQL. The core constructs of VQL can encode easily a number of representative constructs of different (about six visual and four non-visual) relational, nested and object-oriented query languages. We also study the theoretical aspects such as safety, complexity, etc., of VQL.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {357–366},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170087,
author = {Vadaparty, K. and Aslandogan, Y. A. and Ozsoyoglu, G.},
title = {Towards a Unified Visual Database Access},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170087},
doi = {10.1145/170036.170087},
abstract = {Since the development of QBE, over fifty visual query languages have been proposed to facilitate easy database access. Although these languages have introduced some very useful paradigms, a number of these have some severe limitations, such as: (a) not extending beyond the relational model (b) not considering negation and safety, formally (c) using ad hoc constructs, with no analysis of expressivity or complexity done, etc. Note that visual database access is an important issue being revisted, with the emergence of different flavors of object-oriented databases. We believe that there is a need for developing a unified visual query language.Specifically, our goal is to develop a visual query language that has the following properties: (i) It has a few core constructs using which “expert-users” can define new (derived) constructs easily (ii) “Normal users” can use easily either the core or the derived constructs for database querying (iii) It can implement representative constructs of other (textual or visual) query language straightforwardly, and (iv) It has formal semantics, with its theoretical properties, such as complexity, analyzed.We believe that we make a first step towards the above goal by introducing a new logical construct called restricted universal quantifier and combining it with the hierarchical structure of windows to develop a Visual Query Language, called VQL. The core constructs of VQL can encode easily a number of representative constructs of different (about six visual and four non-visual) relational, nested and object-oriented query languages. We also study the theoretical aspects such as safety, complexity, etc., of VQL.},
journal = {SIGMOD Rec.},
month = jun,
pages = {357–366},
numpages = {10}
}

@inproceedings{10.1145/170035.170089,
author = {Watters, Aaron},
title = {Interpreting a Reconstructed Relational Calculus (Extended Abstract)},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170089},
doi = {10.1145/170035.170089},
abstract = {This paper describes a method for answering all relational calculus queries under the assumption that the domain of data values is sufficiently large. The method extends recent theoretical results that use extended relation representations to answer domain dependent queries, without the use of auxilliary variables or invented constants or an explicit enumeration of the active domain. The method is shown to be logically correct and to have polynomial data complexity. By identifying relational algebra operations with relational calculus queries this approach extends relational algebra to a full boolean algebra, where intersection, union, and difference are defined between any two relations, whether or not they are union compatible. An example illustrates that this approach can be useful in distributed query optimization.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {367–376},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170089,
author = {Watters, Aaron},
title = {Interpreting a Reconstructed Relational Calculus (Extended Abstract)},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170089},
doi = {10.1145/170036.170089},
abstract = {This paper describes a method for answering all relational calculus queries under the assumption that the domain of data values is sufficiently large. The method extends recent theoretical results that use extended relation representations to answer domain dependent queries, without the use of auxilliary variables or invented constants or an explicit enumeration of the active domain. The method is shown to be logically correct and to have polynomial data complexity. By identifying relational algebra operations with relational calculus queries this approach extends relational algebra to a full boolean algebra, where intersection, union, and difference are defined between any two relations, whether or not they are union compatible. An example illustrates that this approach can be useful in distributed query optimization.},
journal = {SIGMOD Rec.},
month = jun,
pages = {367–376},
numpages = {10}
}

@inproceedings{10.1145/170035.170090,
author = {Beeri, Catriel and Milo, Tova},
title = {On the Power of Algebras with Recursion},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170090},
doi = {10.1145/170035.170090},
abstract = {We consider the relationship between the deductive and the functional/algebraic query language paradigms. Previous works considered this subject for a non-recursive algebra, or an algebra with a fixed point operation, and the corresponding class of deductive queries is that defined by stratified programs. We consider here algebraic languages extended by general recursive definitions. We also consider languages that allow non-restricted use of negation. It turns out that recursion and negation in the algebraic paradigm need to be studied together. The semantics used for the comparison is the valid semantics, although other well-known declarative semantics can also be used to derive similar results. We show that the class of queries expressed by general deduction with negation can be captured using algebra with recursive definitions.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {377–387},
numpages = {11},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170090,
author = {Beeri, Catriel and Milo, Tova},
title = {On the Power of Algebras with Recursion},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170090},
doi = {10.1145/170036.170090},
abstract = {We consider the relationship between the deductive and the functional/algebraic query language paradigms. Previous works considered this subject for a non-recursive algebra, or an algebra with a fixed point operation, and the corresponding class of deductive queries is that defined by stratified programs. We consider here algebraic languages extended by general recursive definitions. We also consider languages that allow non-restricted use of negation. It turns out that recursion and negation in the algebraic paradigm need to be studied together. The semantics used for the comparison is the valid semantics, although other well-known declarative semantics can also be used to derive similar results. We show that the class of queries expressed by general deduction with negation can be captured using algebra with recursive definitions.},
journal = {SIGMOD Rec.},
month = jun,
pages = {377–387},
numpages = {11}
}

@inproceedings{10.1145/170035.170092,
author = {Alonso, Rafael and Korth, Henry F.},
title = {Database System Issues in Nomadic Computing},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170092},
doi = {10.1145/170035.170092},
abstract = {Mobile computers and wireless networks are emerging technologies that will soon be available to a wide variety of computer users. Unlike earlier generations of laptop computers, the new generation of mobile computers can be an integrated part of a distributed computing environment, one in which users change physical location frequently. The result is a new computing paradigm, nomadic computing. This paradigm will affect the design of much of our current systems software, including that of database systems.This paper discusses in some detail the impact of nomadic computing on a number of traditional database system concepts. In particular, we point out how the reliance on short-lived batteries changes the cost assumptions underlying query processing. In these systems, power consumption competes with resource utilization in the definition of cost metrics. We also discuss how the likelihood of temporary disconnection forces consideration of alternative transaction processing protocols. The limited screen space of mobile computers along with the advent of pen-based computing provides new opportunities and new constraints on database interfaces and languages. Lastly, we believe that the movement of computers and data among networks potentially belonging to distinct, autonomous organizations creates serious security problems.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {388–392},
numpages = {5},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170092,
author = {Alonso, Rafael and Korth, Henry F.},
title = {Database System Issues in Nomadic Computing},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170092},
doi = {10.1145/170036.170092},
abstract = {Mobile computers and wireless networks are emerging technologies that will soon be available to a wide variety of computer users. Unlike earlier generations of laptop computers, the new generation of mobile computers can be an integrated part of a distributed computing environment, one in which users change physical location frequently. The result is a new computing paradigm, nomadic computing. This paradigm will affect the design of much of our current systems software, including that of database systems.This paper discusses in some detail the impact of nomadic computing on a number of traditional database system concepts. In particular, we point out how the reliance on short-lived batteries changes the cost assumptions underlying query processing. In these systems, power consumption competes with resource utilization in the definition of cost metrics. We also discuss how the likelihood of temporary disconnection forces consideration of alternative transaction processing protocols. The limited screen space of mobile computers along with the advent of pen-based computing provides new opportunities and new constraints on database interfaces and languages. Lastly, we believe that the movement of computers and data among networks potentially belonging to distinct, autonomous organizations creates serious security problems.},
journal = {SIGMOD Rec.},
month = jun,
pages = {388–392},
numpages = {5}
}

@inproceedings{10.1145/170035.170093,
author = {Dayal, Umesh and Garcia-Molina, Hector and Hsu, Mei and Kao, Ben and Shan, Ming-Chien},
title = {Third Generation TP Monitors: A Database Challenge},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170093},
doi = {10.1145/170035.170093},
abstract = {In a 1976 book, “Algorithms + Data Structures = Programs” [15], Niklaus Wirth defined programs to be algorithms and data structures. Of course, by now we know that man does not live from programs alone, and that there is a second fundamental computer science equation: “Programs + Databases = Information Systems.”Database researchers have traditionally focused on the database component of the equation, providing shared and persistent repositories for the data that programs need and produce. As a matter of fact, a lot of us have worked hard to hide or ignore the programs component. For instance, non-procedural languages like SQL and relational algebra have been the holy grail of the database field, letting us describe the data in the way we want without need to write messy programs. The magic wand of transactions makes programs that execute concurrently with our non-procedural statements suddenly disappear: these other programs appear as atomic actions that are either executed before we started looking at our data, or will be executed after we are all done with our work. The wonders of fault tolerance and automatic recovery guarantee that we never have to concern ourselves with our statements failing or being interrupted. The data we need will always be there for us, and our statements will always run to completion.Unfortunately, the real programs that operate on databases are in many cases more complex than the classical ones like “withdraw 100 dollars from my account” or “find me all my blue eyed great-grandfathers.” For one, programs may be much longer, requiring many database interactions. Furthermore, programs need to interact with other concurrent programs, getting results from and to them. They may also need to be aware of their environment, perhaps monitoring the execution of    another program, or taking corrective action when some system components fail. Of course, this is not to say that transactions and non-procedural query languages have not been great contributions. In many cases, they are all that is needed to program one's application. But beyond that there are many cases when one must deal with multiple concurrent applications. Indeed, a critical problem facing complex enterprises is the automation of complex business processes. Enterprises today are drowning in an ocean of data, with a few isolated islands of automation consisting of heterogeneous databases and legacy application programs, each of which automates some point function (e.g., order entry, inventory, accounting, billing) within the enterprise. As the enterprise attempts to automate its business processes, these isolated islands have to be bridged: complex information systems must be developed that need to span many of these databases and application programs. Traditional database systems do not provide the supporting environment for this.Our programming languages colleagues have been working on the programs component of our fundamental equation, but the database component has traditionally been ignored or hidden. There has been a lot of recent interest on languages that support persistent objects, but often the goal is to make the database that holds the objects look as little as possible like a database. That is, the persistent objects are to be handled just as if they were volatile objects, even though they are not. Also, the programming languages researchers have borrowed the notions of transactions and serializable schedules to hide as much as possible concurrent execution and failures of programs. Finally, traditional programming languages (there are exceptions[4, 13]) have focused on “programming in the small,” as opposed to “programming in the large.” The goal of the former is to program single applications or to solve   single problems, as opposed to programming an entire enterprise and all of its interacting applications.Researchers from both camps have recently been addressing both components of the “Programs + Databases” equation. For example, database researchers have been adding triggers and procedures to database objects[2], resulting in so called active databases. These are important steps in the right direction (other related steps are listed below), but still do not address the full programming in the large problem.In our opinion, the only software providers that have tackled both components of the “Programs + Databases” equation, and have a proven track record with real applications, are the Transaction Processing Monitor (TPM) builders[9].},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {393–397},
numpages = {5},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170093,
author = {Dayal, Umesh and Garcia-Molina, Hector and Hsu, Mei and Kao, Ben and Shan, Ming-Chien},
title = {Third Generation TP Monitors: A Database Challenge},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170093},
doi = {10.1145/170036.170093},
abstract = {In a 1976 book, “Algorithms + Data Structures = Programs” [15], Niklaus Wirth defined programs to be algorithms and data structures. Of course, by now we know that man does not live from programs alone, and that there is a second fundamental computer science equation: “Programs + Databases = Information Systems.”Database researchers have traditionally focused on the database component of the equation, providing shared and persistent repositories for the data that programs need and produce. As a matter of fact, a lot of us have worked hard to hide or ignore the programs component. For instance, non-procedural languages like SQL and relational algebra have been the holy grail of the database field, letting us describe the data in the way we want without need to write messy programs. The magic wand of transactions makes programs that execute concurrently with our non-procedural statements suddenly disappear: these other programs appear as atomic actions that are either executed before we started looking at our data, or will be executed after we are all done with our work. The wonders of fault tolerance and automatic recovery guarantee that we never have to concern ourselves with our statements failing or being interrupted. The data we need will always be there for us, and our statements will always run to completion.Unfortunately, the real programs that operate on databases are in many cases more complex than the classical ones like “withdraw 100 dollars from my account” or “find me all my blue eyed great-grandfathers.” For one, programs may be much longer, requiring many database interactions. Furthermore, programs need to interact with other concurrent programs, getting results from and to them. They may also need to be aware of their environment, perhaps monitoring the execution of    another program, or taking corrective action when some system components fail. Of course, this is not to say that transactions and non-procedural query languages have not been great contributions. In many cases, they are all that is needed to program one's application. But beyond that there are many cases when one must deal with multiple concurrent applications. Indeed, a critical problem facing complex enterprises is the automation of complex business processes. Enterprises today are drowning in an ocean of data, with a few isolated islands of automation consisting of heterogeneous databases and legacy application programs, each of which automates some point function (e.g., order entry, inventory, accounting, billing) within the enterprise. As the enterprise attempts to automate its business processes, these isolated islands have to be bridged: complex information systems must be developed that need to span many of these databases and application programs. Traditional database systems do not provide the supporting environment for this.Our programming languages colleagues have been working on the programs component of our fundamental equation, but the database component has traditionally been ignored or hidden. There has been a lot of recent interest on languages that support persistent objects, but often the goal is to make the database that holds the objects look as little as possible like a database. That is, the persistent objects are to be handled just as if they were volatile objects, even though they are not. Also, the programming languages researchers have borrowed the notions of transactions and serializable schedules to hide as much as possible concurrent execution and failures of programs. Finally, traditional programming languages (there are exceptions[4, 13]) have focused on “programming in the small,” as opposed to “programming in the large.” The goal of the former is to program single applications or to solve   single problems, as opposed to programming an entire enterprise and all of its interacting applications.Researchers from both camps have recently been addressing both components of the “Programs + Databases” equation. For example, database researchers have been adding triggers and procedures to database objects[2], resulting in so called active databases. These are important steps in the right direction (other related steps are listed below), but still do not address the full programming in the large problem.In our opinion, the only software providers that have tackled both components of the “Programs + Databases” equation, and have a proven track record with real applications, are the Transaction Processing Monitor (TPM) builders[9].},
journal = {SIGMOD Rec.},
month = jun,
pages = {393–397},
numpages = {5}
}

@inproceedings{10.1145/170035.170096,
author = {Egenhofer, Max J.},
title = {What's Special about Spatial? Database Requirements for Vehicle Navigation in Geographic Space},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170096},
doi = {10.1145/170035.170096},
abstract = {Geographic databases are becoming a popular subject for research projects. It is acknowledged that database requirements for such applications as Geographic Information Systems (GISs), computer cartography, remote-sensing image databases, and emergency routing/dispatching are distinct from those for traditional database applications; however, the specific database needs due to the properties of geographic data are frequently overlooked. Here, we describe a specific problem domain, a vehicle navigation system, and analyze the properties of data to be modeled appropriately and managed efficiently by database management systems. Vehicle navigation systems need continuous, fast access to very large databases. Through the combination of real-time access and geographic data, we identify particularly challenging database requirements of this application with respect to data models, query languages, and query processing.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {398–402},
numpages = {5},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170096,
author = {Egenhofer, Max J.},
title = {What's Special about Spatial? Database Requirements for Vehicle Navigation in Geographic Space},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170096},
doi = {10.1145/170036.170096},
abstract = {Geographic databases are becoming a popular subject for research projects. It is acknowledged that database requirements for such applications as Geographic Information Systems (GISs), computer cartography, remote-sensing image databases, and emergency routing/dispatching are distinct from those for traditional database applications; however, the specific database needs due to the properties of geographic data are frequently overlooked. Here, we describe a specific problem domain, a vehicle navigation system, and analyze the properties of data to be modeled appropriately and managed efficiently by database management systems. Vehicle navigation systems need continuous, fast access to very large databases. Through the combination of real-time access and geographic data, we identify particularly challenging database requirements of this application with respect to data models, query languages, and query processing.},
journal = {SIGMOD Rec.},
month = jun,
pages = {398–402},
numpages = {5}
}

@inproceedings{10.1145/170035.170099,
author = {Ordille, Joann J. and Miller, Barton P.},
title = {Database Challenges in Global Information Systems},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170099},
doi = {10.1145/170035.170099},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {403–407},
numpages = {5},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170099,
author = {Ordille, Joann J. and Miller, Barton P.},
title = {Database Challenges in Global Information Systems},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170099},
doi = {10.1145/170036.170099},
journal = {SIGMOD Rec.},
month = jun,
pages = {403–407},
numpages = {5}
}

@inproceedings{10.1145/170035.170101,
author = {Zdonik, Stanley B.},
title = {Incremental Database Systems: Databases from the Ground Up},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170101},
doi = {10.1145/170035.170101},
abstract = {This paper discusses a new approach to database management systems that is better suited to a wide class of new applications such as scientific, hypermedia, and financial applications. These applications are characterized by their need to store large amounts of raw, unstructured data. Our premise is that, in these situations, database systems need a way to store data without imposing a schema, and a way to provide a schema incrementally as we process the data. This requires that the raw data be mapped in complex ways to an evolving schema.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {408–412},
numpages = {5},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170101,
author = {Zdonik, Stanley B.},
title = {Incremental Database Systems: Databases from the Ground Up},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170101},
doi = {10.1145/170036.170101},
abstract = {This paper discusses a new approach to database management systems that is better suited to a wide class of new applications such as scientific, hypermedia, and financial applications. These applications are characterized by their need to store large amounts of raw, unstructured data. Our premise is that, in these situations, database systems need a way to store data without imposing a schema, and a way to provide a schema incrementally as we process the data. This requires that the raw data be mapped in complex ways to an evolving schema.},
journal = {SIGMOD Rec.},
month = jun,
pages = {408–412},
numpages = {5}
}

@inproceedings{10.1145/170035.170103,
author = {Carey, Michael J. and Haas, Laura M. and Livny, Miron},
title = {Tapes Hold Data, Too: Challenges of Tuples on Tertiary Store},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170103},
doi = {10.1145/170035.170103},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {413–417},
numpages = {5},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170103,
author = {Carey, Michael J. and Haas, Laura M. and Livny, Miron},
title = {Tapes Hold Data, Too: Challenges of Tuples on Tertiary Store},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170103},
doi = {10.1145/170036.170103},
journal = {SIGMOD Rec.},
month = jun,
pages = {413–417},
numpages = {5}
}

@inproceedings{10.1145/170035.170105,
author = {Jagadish, H. V.},
title = {Issues in Multimedia Databases},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170105},
doi = {10.1145/170035.170105},
abstract = {Multimedia is a popular term these days, and the database community, naturally, is talking about multimedia databases. The reason multimedia is getting so much attention is clear: technology trends are now beginning to make it possible to store and display, at a reasonable price, audio and still images through a computer. It is expected that video storage will also be affordable in the near future. The purpose of this panel is to explore what new challenges this multimedia explosion brings to the database community.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {419},
numpages = {1},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170105,
author = {Jagadish, H. V.},
title = {Issues in Multimedia Databases},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170105},
doi = {10.1145/170036.170105},
abstract = {Multimedia is a popular term these days, and the database community, naturally, is talking about multimedia databases. The reason multimedia is getting so much attention is clear: technology trends are now beginning to make it possible to store and display, at a reasonable price, audio and still images through a computer. It is expected that video storage will also be affordable in the near future. The purpose of this panel is to explore what new challenges this multimedia explosion brings to the database community.},
journal = {SIGMOD Rec.},
month = jun,
pages = {419},
numpages = {1}
}

@inproceedings{10.1145/170035.170108,
author = {Motro, Amihai},
title = {What to Teach about Databases},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170108},
doi = {10.1145/170035.170108},
abstract = {Lately, there has been increased pressure on universities to place more emphasis on teaching, relative to research. Hence, this is a good time to discuss the state of database instruction in universities. The annual SIGMOD Conference brings together both the “producers” and the “consumers” of database education (i.e., university professors, and professionals representing research and development organizations). Hence, this is a good place to hold such a discussion.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {420},
numpages = {1},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170108,
author = {Motro, Amihai},
title = {What to Teach about Databases},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170108},
doi = {10.1145/170036.170108},
abstract = {Lately, there has been increased pressure on universities to place more emphasis on teaching, relative to research. Hence, this is a good time to discuss the state of database instruction in universities. The annual SIGMOD Conference brings together both the “producers” and the “consumers” of database education (i.e., university professors, and professionals representing research and development organizations). Hence, this is a good place to hold such a discussion.},
journal = {SIGMOD Rec.},
month = jun,
pages = {420},
numpages = {1}
}

@inproceedings{10.1145/170035.170111,
author = {Shan, Ming-Chien},
title = {Pegasus Architecture and Design Principles},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170111},
doi = {10.1145/170035.170111},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {422–425},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170111,
author = {Shan, Ming-Chien},
title = {Pegasus Architecture and Design Principles},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170111},
doi = {10.1145/170036.170111},
journal = {SIGMOD Rec.},
month = jun,
pages = {422–425},
numpages = {4}
}

@inproceedings{10.1145/170035.170113,
author = {Bukhres, Omran and Chen, Jiansan and Pezzoli, Rob},
title = {An InterBase System at BNR},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170113},
doi = {10.1145/170035.170113},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {426–429},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170113,
author = {Bukhres, Omran and Chen, Jiansan and Pezzoli, Rob},
title = {An InterBase System at BNR},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170113},
doi = {10.1145/170036.170113},
journal = {SIGMOD Rec.},
month = jun,
pages = {426–429},
numpages = {4}
}

@inproceedings{10.1145/170035.170115,
author = {Klein, Johannes and Upton, Francis},
title = {Open DECdtm: Constraint Based Transaction Management},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170115},
doi = {10.1145/170035.170115},
abstract = {Open DECdtm offers portable transaction management services layered on OSF DCE which support the application (TX), resource manager (XA), and transactional DCE RPC (TxRPC) interfaces specified by X/Open. Open DECdtm also provides interoperability with OSI Transaction Processing (OSI TP) and OpenVMS systems using the DECdtm OpenVMS protocol. Protocols executed by Open DECdtm are specified by constraints. This simplifies the development of transactional gateways between different data transfer protocols and transaction models.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {430–433},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170115,
author = {Klein, Johannes and Upton, Francis},
title = {Open DECdtm: Constraint Based Transaction Management},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170115},
doi = {10.1145/170036.170115},
abstract = {Open DECdtm offers portable transaction management services layered on OSF DCE which support the application (TX), resource manager (XA), and transactional DCE RPC (TxRPC) interfaces specified by X/Open. Open DECdtm also provides interoperability with OSI Transaction Processing (OSI TP) and OpenVMS systems using the DECdtm OpenVMS protocol. Protocols executed by Open DECdtm are specified by constraints. This simplifies the development of transactional gateways between different data transfer protocols and transaction models.},
journal = {SIGMOD Rec.},
month = jun,
pages = {430–433},
numpages = {4}
}

@inproceedings{10.1145/170035.170118,
author = {Wiederhold, Gio},
title = {Intelligent Integration of Information},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170118},
doi = {10.1145/170035.170118},
abstract = {This paper describes and classifies methods to transform data to information in a three-layer, mediated architecture. The layers can be characterized from the top down as information-consuming applications, mediators which perform intelligent integration of information (I3), and data, knowledge and simulation resources.The objective of modules in the I3 architecture is to provide end users' apoplications with information obtained through selection, abstraction, fusion, caching, extrapolation, and pruning of data. The data is obtained from many diverse and heterogeneous sources. The I3 objective requires the establishment of a consensual information system architecture, so that many participants and technologies can contribute. An attempt to provide such a range of services within a single, tightly integrated system is unlikely to survive technological or environmental change.This paper focuses on the computational models needed to support the mediating functions in this architecture and introduces initial applicatiions. The architecture has been motivated in [Wied:92C].},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {434–437},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170118,
author = {Wiederhold, Gio},
title = {Intelligent Integration of Information},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170118},
doi = {10.1145/170036.170118},
abstract = {This paper describes and classifies methods to transform data to information in a three-layer, mediated architecture. The layers can be characterized from the top down as information-consuming applications, mediators which perform intelligent integration of information (I3), and data, knowledge and simulation resources.The objective of modules in the I3 architecture is to provide end users' apoplications with information obtained through selection, abstraction, fusion, caching, extrapolation, and pruning of data. The data is obtained from many diverse and heterogeneous sources. The I3 objective requires the establishment of a consensual information system architecture, so that many participants and technologies can contribute. An attempt to provide such a range of services within a single, tightly integrated system is unlikely to survive technological or environmental change.This paper focuses on the computational models needed to support the mediating functions in this architecture and introduces initial applicatiions. The architecture has been motivated in [Wied:92C].},
journal = {SIGMOD Rec.},
month = jun,
pages = {434–437},
numpages = {4}
}

@inproceedings{10.1145/170035.170120,
author = {Vieille, Laurent},
title = {A Deductive and Object-Oriented Database System: Why and How?},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170120},
doi = {10.1145/170035.170120},
abstract = {This talk will outline the principles, the architecture and the potential target applications of a Deductive and Object-Oriented Database System (DOOD). Such systems combine the novel functionalities (relying on the associated technology) developed in deductive database projects, the ability to manipulate the complex objects appearing in many applications and the architectural advances achieved by Object-Oriented DBMS's.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {438},
numpages = {1},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170120,
author = {Vieille, Laurent},
title = {A Deductive and Object-Oriented Database System: Why and How?},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170120},
doi = {10.1145/170036.170120},
abstract = {This talk will outline the principles, the architecture and the potential target applications of a Deductive and Object-Oriented Database System (DOOD). Such systems combine the novel functionalities (relying on the associated technology) developed in deductive database projects, the ability to manipulate the complex objects appearing in many applications and the architectural advances achieved by Object-Oriented DBMS's.},
journal = {SIGMOD Rec.},
month = jun,
pages = {438},
numpages = {1}
}

@inproceedings{10.1145/170035.170124,
author = {Stonebraker, Michael},
title = {The Miro DBMS},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170124},
doi = {10.1145/170035.170124},
abstract = {This short paper explains the key object-relational (OR) DBMS technology used by the Miro DBMS.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {439},
numpages = {1},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170124,
author = {Stonebraker, Michael},
title = {The Miro DBMS},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170124},
doi = {10.1145/170036.170124},
abstract = {This short paper explains the key object-relational (OR) DBMS technology used by the Miro DBMS.},
journal = {SIGMOD Rec.},
month = jun,
pages = {439},
numpages = {1}
}

@inproceedings{10.1145/170035.170127,
author = {V\'{e}lez, Fernando},
title = {Modularity and Tuning Mechanisms in the O2 System},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170127},
doi = {10.1145/170035.170127},
abstract = {The O2 System is a commercial Object-Oriented Database Management System with a complete development environment and a set of user interface tools. In this presentation, we focus on the modularity and application tuning facilities of the system.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {440},
numpages = {1},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170127,
author = {V\'{e}lez, Fernando},
title = {Modularity and Tuning Mechanisms in the O2 System},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170127},
doi = {10.1145/170036.170127},
abstract = {The O2 System is a commercial Object-Oriented Database Management System with a complete development environment and a set of user interface tools. In this presentation, we focus on the modularity and application tuning facilities of the system.},
journal = {SIGMOD Rec.},
month = jun,
pages = {440},
numpages = {1}
}

@inproceedings{10.1145/170035.170128,
author = {Wade, Andrew E.},
title = {Single Logical View over Enterprise-Wide Distributed Databases},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170128},
doi = {10.1145/170035.170128},
abstract = {Two trends in today's corporate world demand distribution: downsizing from centralized mainframe single database environments; and wider integration, connecting finance, engineering, manufacturing information systems for enterprise-wide modeling and operations optimization. The resulting environment consists of multiple databases, at the group level, department level, and corporate level, with the need to manage dependencies among data in all of them. The solution is full distribution, providing a single logical view to objects anywhere, from anywhere. Users see a logical model of objects connected to objects, with atomic transactions and propagating methods, even if composite objects are split among multiple databases, each under separate administrative control, on multiple, heterogeneous platforms, operating systems, and network protocols. Support for production environments includes multiple schemas, which may be shared among databases, private, or encrypted, dynamic addition of schemas, and schema evolution. Finally, the logical view must remain valid, and applications must continue to work, as the mapping to the physical environment changes, moving objects and databases to new platforms.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {441},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170128,
author = {Wade, Andrew E.},
title = {Single Logical View over Enterprise-Wide Distributed Databases},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170128},
doi = {10.1145/170036.170128},
abstract = {Two trends in today's corporate world demand distribution: downsizing from centralized mainframe single database environments; and wider integration, connecting finance, engineering, manufacturing information systems for enterprise-wide modeling and operations optimization. The resulting environment consists of multiple databases, at the group level, department level, and corporate level, with the need to manage dependencies among data in all of them. The solution is full distribution, providing a single logical view to objects anywhere, from anywhere. Users see a logical model of objects connected to objects, with atomic transactions and propagating methods, even if composite objects are split among multiple databases, each under separate administrative control, on multiple, heterogeneous platforms, operating systems, and network protocols. Support for production environments includes multiple schemas, which may be shared among databases, private, or encrypted, dynamic addition of schemas, and schema evolution. Finally, the logical view must remain valid, and applications must continue to work, as the mapping to the physical environment changes, moving objects and databases to new platforms.},
journal = {SIGMOD Rec.},
month = jun,
pages = {441},
numpages = {4}
}

@inproceedings{10.1145/170035.170129,
author = {Mohan, C.},
title = {IBM's Relational DBMS Products: Features and Technologies},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170129},
doi = {10.1145/170035.170129},
abstract = {This paper very briefly summarizes the features and technologies implemented in the IBM relational DBMS products. The topics covered include record and index management, concurrency control and recovery methods, commit protocols, query optimization and execution techniques, high availability and support for parallelism and distributed data. Some indications of likely future product directions are also given.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {445–448},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170129,
author = {Mohan, C.},
title = {IBM's Relational DBMS Products: Features and Technologies},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170129},
doi = {10.1145/170036.170129},
abstract = {This paper very briefly summarizes the features and technologies implemented in the IBM relational DBMS products. The topics covered include record and index management, concurrency control and recovery methods, commit protocols, query optimization and execution techniques, high availability and support for parallelism and distributed data. Some indications of likely future product directions are also given.},
journal = {SIGMOD Rec.},
month = jun,
pages = {445–448},
numpages = {4}
}

@inproceedings{10.1145/170035.170130,
author = {Fushimi, Shinya and Kitsuregawa, Masaru},
title = {GREO: A Commercial Database Processor Based on a Pipelined Hardware Sorter},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170130},
doi = {10.1145/170035.170130},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {449–452},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170130,
author = {Fushimi, Shinya and Kitsuregawa, Masaru},
title = {GREO: A Commercial Database Processor Based on a Pipelined Hardware Sorter},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170130},
doi = {10.1145/170036.170130},
journal = {SIGMOD Rec.},
month = jun,
pages = {449–452},
numpages = {4}
}

@inproceedings{10.1145/170035.170131,
author = {Tseng, Emy and Reiner, David},
title = {Parallel Database Processing on the KSR1 Computer},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170131},
doi = {10.1145/170035.170131},
abstract = {The Kendall Square Research high performance computer (KSR1) provides a spectrum of parallel database processing techniques to achieve scalability and performance in a shared memory environment. The techniques include running multiple transactions in parallel, decomposing queries into parallel subqueries, running multiple instances of the DBMS and partitioning data over disks. These techniques enable on-line transactions to be run in parallel at high throughput rates and decision-support queries to be parallelized and executed very rapidly.This paper focuses upon two of the parallel database processing techniques used on the KSR1—the Kendall Square Query Decomposer and the Oracle Parallel Server. The Query Decomposer intercepts costly decision support queries and decomposes them into subqueries which are executed in parallel. Parallel Server enables multiple ORACLE instances to run simultaneously on the same database.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {453–455},
numpages = {3},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170131,
author = {Tseng, Emy and Reiner, David},
title = {Parallel Database Processing on the KSR1 Computer},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170131},
doi = {10.1145/170036.170131},
abstract = {The Kendall Square Research high performance computer (KSR1) provides a spectrum of parallel database processing techniques to achieve scalability and performance in a shared memory environment. The techniques include running multiple transactions in parallel, decomposing queries into parallel subqueries, running multiple instances of the DBMS and partitioning data over disks. These techniques enable on-line transactions to be run in parallel at high throughput rates and decision-support queries to be parallelized and executed very rapidly.This paper focuses upon two of the parallel database processing techniques used on the KSR1—the Kendall Square Query Decomposer and the Oracle Parallel Server. The Query Decomposer intercepts costly decision support queries and decomposes them into subqueries which are executed in parallel. Parallel Server enables multiple ORACLE instances to run simultaneously on the same database.},
journal = {SIGMOD Rec.},
month = jun,
pages = {453–455},
numpages = {3}
}

@inproceedings{10.1145/170035.170133,
author = {Jin, W. Woody and Rusinkiewicz, Marek and Ness, Linda and Sheth, Amit},
title = {Concurrency Control and Recovery of Multidatabase Work Flows in Telecommunication Applications},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170133},
doi = {10.1145/170035.170133},
abstract = {In a research and technology application project at Bellcore, we used multidatabase transactions to model multisystem work flows of telecommunication applications. During the project a prototype scheduler for executing multi-database transactions was developed. Two of the issues addressed in this project were concurrent execution of multi-database transactions and their failure recovery. This paper discusses our use of properties of the application and the telecommunication systems to develop simple and efficient solutions to the concurrency control and recovery problems.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {456–459},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170133,
author = {Jin, W. Woody and Rusinkiewicz, Marek and Ness, Linda and Sheth, Amit},
title = {Concurrency Control and Recovery of Multidatabase Work Flows in Telecommunication Applications},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170133},
doi = {10.1145/170036.170133},
abstract = {In a research and technology application project at Bellcore, we used multidatabase transactions to model multisystem work flows of telecommunication applications. During the project a prototype scheduler for executing multi-database transactions was developed. Two of the issues addressed in this project were concurrent execution of multi-database transactions and their failure recovery. This paper discusses our use of properties of the application and the telecommunication systems to develop simple and efficient solutions to the concurrency control and recovery problems.},
journal = {SIGMOD Rec.},
month = jun,
pages = {456–459},
numpages = {4}
}

@inproceedings{10.1145/170035.170136,
author = {Sherman, Mark},
title = {Architecture of the Encina Distributed Transaction Processing Family},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170136},
doi = {10.1145/170035.170136},
abstract = {This paper discusses how the Encina® family of distributed transaction processing software can be used to build reliable, distributed applications. We start with the toolkit components of Encina and how they are used for implementing ACID properties. We then consider how the toolkit can be applied in building higher level components in a DCE environment. We conclude with a discussion of the Encina Monitor, which provides a framework for organizing a collection of machines and servers.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {460–463},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170136,
author = {Sherman, Mark},
title = {Architecture of the Encina Distributed Transaction Processing Family},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170136},
doi = {10.1145/170036.170136},
abstract = {This paper discusses how the Encina® family of distributed transaction processing software can be used to build reliable, distributed applications. We start with the toolkit components of Encina and how they are used for implementing ACID properties. We then consider how the toolkit can be applied in building higher level components in a DCE environment. We conclude with a discussion of the Encina Monitor, which provides a framework for organizing a collection of machines and servers.},
journal = {SIGMOD Rec.},
month = jun,
pages = {460–463},
numpages = {4}
}

@inproceedings{10.1145/170035.170138,
author = {Colton, Malcolm},
title = {Replicated Data in a Distributed Environment},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170138},
doi = {10.1145/170035.170138},
abstract = {Replication Server is a forthcoming Sybase product that dynamically maintains subsets of data in a distributed environment, providing several transaction models to maintain loose consistency. Replication Server is constrasted with existing products which provide location-transparent reads and high-consistency coordinated commits Replication Server makes it possible to build systems that are much more robust in the face of system component failures. By moving transactions rather than data, and by locating data at the point of processing, it maximizes the use of network bandwidth, enabling the deployment of robust, high-performance applications at a lower cost than with traditional approaches.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {464–466},
numpages = {3},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170138,
author = {Colton, Malcolm},
title = {Replicated Data in a Distributed Environment},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170138},
doi = {10.1145/170036.170138},
abstract = {Replication Server is a forthcoming Sybase product that dynamically maintains subsets of data in a distributed environment, providing several transaction models to maintain loose consistency. Replication Server is constrasted with existing products which provide location-transparent reads and high-consistency coordinated commits Replication Server makes it possible to build systems that are much more robust in the face of system component failures. By moving transactions rather than data, and by locating data at the point of processing, it maximizes the use of network bandwidth, enabling the deployment of robust, high-performance applications at a lower cost than with traditional approaches.},
journal = {SIGMOD Rec.},
month = jun,
pages = {464–466},
numpages = {3}
}

@inproceedings{10.1145/170035.170141,
author = {Singhal, Anoop and Arlein, Robert M. and Lo, Chi-Yuan},
title = {DDB: An Object Oriented Design Data Manager for VLSI CAD},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170141},
doi = {10.1145/170035.170141},
abstract = {In this paper we present an object oriented data model for VLSI/CAD data. A design data manager (DDB) based on such a model has been implemented under the UNIX/C++ environment. It has been used by a set of diverse VLSI/CAD applications of our organization. Benchmarks have shown it to perform better as compared to commercial object oriented database systems. In conjunction with the ease of data access, the data manger served to improve software productivity and a modular program architecture for our CAD system.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {467–470},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170141,
author = {Singhal, Anoop and Arlein, Robert M. and Lo, Chi-Yuan},
title = {DDB: An Object Oriented Design Data Manager for VLSI CAD},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170141},
doi = {10.1145/170036.170141},
abstract = {In this paper we present an object oriented data model for VLSI/CAD data. A design data manager (DDB) based on such a model has been implemented under the UNIX/C++ environment. It has been used by a set of diverse VLSI/CAD applications of our organization. Benchmarks have shown it to perform better as compared to commercial object oriented database systems. In conjunction with the ease of data access, the data manger served to improve software productivity and a modular program architecture for our CAD system.},
journal = {SIGMOD Rec.},
month = jun,
pages = {467–470},
numpages = {4}
}

@inproceedings{10.1145/170035.170145,
author = {Nassif, Rodolphe and Mitchusson, Don},
title = {Issues and Approaches for Migration/Cohabitation between Legacy and New Systems},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170145},
doi = {10.1145/170035.170145},
abstract = {Corporate Subject Data Bases (CSDB) are being introduced to reduce data redundancy, maintain the integrity of the data, provide a uniform data access interface, and have data readily available to make business decisions. During the transition phase, there is a need to maintain Legacy Systems (LS), CSDB, and to synchronize between them. Choosing the right granularity for migration of data and functionality is essential to the success of the migration strategy. Technologies being used to support the transition to CSDB include relational systems supporting stored procedures, remote procedures, expert systems, object-oriented approach, reengineering tools, and data transition tools. For our Customer CSDB to be deployed in 1993, cleanup of data occurs during initial load of the CSDB. Nightly updates are needed during the transition phase to account for operations executed through LS. There is a lack of an integrated set of tools to help in the transition phase.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {471–474},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170145,
author = {Nassif, Rodolphe and Mitchusson, Don},
title = {Issues and Approaches for Migration/Cohabitation between Legacy and New Systems},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170145},
doi = {10.1145/170036.170145},
abstract = {Corporate Subject Data Bases (CSDB) are being introduced to reduce data redundancy, maintain the integrity of the data, provide a uniform data access interface, and have data readily available to make business decisions. During the transition phase, there is a need to maintain Legacy Systems (LS), CSDB, and to synchronize between them. Choosing the right granularity for migration of data and functionality is essential to the success of the migration strategy. Technologies being used to support the transition to CSDB include relational systems supporting stored procedures, remote procedures, expert systems, object-oriented approach, reengineering tools, and data transition tools. For our Customer CSDB to be deployed in 1993, cleanup of data occurs during initial load of the CSDB. Nightly updates are needed during the transition phase to account for operations executed through LS. There is a lack of an integrated set of tools to help in the transition phase.},
journal = {SIGMOD Rec.},
month = jun,
pages = {471–474},
numpages = {4}
}

@inproceedings{10.1145/170035.170146,
author = {Thieman, James R.},
title = {The International Directory Network and Connected Data Information Systems for Research in the Earth and Space Sciences},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170146},
doi = {10.1145/170035.170146},
abstract = {Many researchers are becoming aware of the International Directory Network (IDN), an interconnected federation of international directories to Earth and space science data. These directories may become distributed nodes of a single, virtual master data directory of the future. Not as many are aware, however, of the many Earth-and-space-sciece-relevant information systems which can be accessed automatically from the directories. After determining potentially useful data sets in various disciplines through IDN directories it is becoming increasingly possible to get detailed information about the correlative possibilities of these data sets through the connected guide/catalog and inventory systems. Such capabilities as data set browse, subsetting, analysis, etc. are available now and will be improving in the future.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {475–478},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170146,
author = {Thieman, James R.},
title = {The International Directory Network and Connected Data Information Systems for Research in the Earth and Space Sciences},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170146},
doi = {10.1145/170036.170146},
abstract = {Many researchers are becoming aware of the International Directory Network (IDN), an interconnected federation of international directories to Earth and space science data. These directories may become distributed nodes of a single, virtual master data directory of the future. Not as many are aware, however, of the many Earth-and-space-sciece-relevant information systems which can be accessed automatically from the directories. After determining potentially useful data sets in various disciplines through IDN directories it is becoming increasingly possible to get detailed information about the correlative possibilities of these data sets through the connected guide/catalog and inventory systems. Such capabilities as data set browse, subsetting, analysis, etc. are available now and will be improving in the future.},
journal = {SIGMOD Rec.},
month = jun,
pages = {475–478},
numpages = {4}
}

@inproceedings{10.1145/170035.170147,
author = {Mukhopadhyay, Debajyoti},
title = {Interoperability Using APPC},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170147},
doi = {10.1145/170035.170147},
abstract = {The complex and competitive business world of today needs to access data for various operations from different systems placed at different geographical locations. In order to fulfil this need, one should have a reliable distributed computing environment. Various components of that environment may be supplied by different vendors. This means that the computing environment not only needs to be distributed but also requires interoperability. Today, Interoperability is no more just an idea but a reality. There is also a growing need to support interactions among various systems in a dialog mode. Integrating distributed systems can only help to achieve the goal of developing a reliable distributed computing environment. In this paper, a conceptual framework for an architecture is described in conjunction with Advanced Program-to-Program Communications LU 6.2 protocol to handle that challenge. This architecture discusses the required contracting services for integrating distributed systems. This contracting service has three major components: the contract interaction services, the contract support services, and the communications infrastructure services.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {479–482},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170147,
author = {Mukhopadhyay, Debajyoti},
title = {Interoperability Using APPC},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170147},
doi = {10.1145/170036.170147},
abstract = {The complex and competitive business world of today needs to access data for various operations from different systems placed at different geographical locations. In order to fulfil this need, one should have a reliable distributed computing environment. Various components of that environment may be supplied by different vendors. This means that the computing environment not only needs to be distributed but also requires interoperability. Today, Interoperability is no more just an idea but a reality. There is also a growing need to support interactions among various systems in a dialog mode. Integrating distributed systems can only help to achieve the goal of developing a reliable distributed computing environment. In this paper, a conceptual framework for an architecture is described in conjunction with Advanced Program-to-Program Communications LU 6.2 protocol to handle that challenge. This architecture discusses the required contracting services for integrating distributed systems. This contracting service has three major components: the contract interaction services, the contract support services, and the communications infrastructure services.},
journal = {SIGMOD Rec.},
month = jun,
pages = {479–482},
numpages = {4}
}

@inproceedings{10.1145/170035.170148,
author = {Sheth, Amit P. and Karabatis, George},
title = {Multidatabase Interdependencies in Industry},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170148},
doi = {10.1145/170035.170148},
abstract = {In this paper we address the problem of data consistency between interrelated data. In industrial environments, lack of consistent data creates difficulties in interoperation between systems and often requires manual interventions to restart operations that fail due to inconsistent data. We report the results of a study to understand applicability, adequacy and advantages of a framework we had proposed earlier to specify interdatabase dependencies in multidatabase environments. We studied several existing Bellcore systems and identified examples of interdependent data. The examples demonstrate that the framework allows precise and detailed specification of complex interdependencies that lead to efficient strategies to enforce the consistency requirements among the corporate data managed in multiple databases. We believe that our specification framework can help in the maintenance of data that meet a business's consistency needs, reduce time consuming and costly manual operations, and provide data of better quality to end users.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {483–486},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170148,
author = {Sheth, Amit P. and Karabatis, George},
title = {Multidatabase Interdependencies in Industry},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170148},
doi = {10.1145/170036.170148},
abstract = {In this paper we address the problem of data consistency between interrelated data. In industrial environments, lack of consistent data creates difficulties in interoperation between systems and often requires manual interventions to restart operations that fail due to inconsistent data. We report the results of a study to understand applicability, adequacy and advantages of a framework we had proposed earlier to specify interdatabase dependencies in multidatabase environments. We studied several existing Bellcore systems and identified examples of interdependent data. The examples demonstrate that the framework allows precise and detailed specification of complex interdependencies that lead to efficient strategies to enforce the consistency requirements among the corporate data managed in multiple databases. We believe that our specification framework can help in the maintenance of data that meet a business's consistency needs, reduce time consuming and costly manual operations, and provide data of better quality to end users.},
journal = {SIGMOD Rec.},
month = jun,
pages = {483–486},
numpages = {4}
}

@inproceedings{10.1145/170035.170149,
author = {Cohen, David and Larson, Gary and Berke, Larry},
title = {Role of Interoperability in Business Application Development},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170149},
doi = {10.1145/170035.170149},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {487–490},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170149,
author = {Cohen, David and Larson, Gary and Berke, Larry},
title = {Role of Interoperability in Business Application Development},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170149},
doi = {10.1145/170036.170149},
journal = {SIGMOD Rec.},
month = jun,
pages = {487–490},
numpages = {4}
}

@inproceedings{10.1145/170035.170150,
author = {Woelk, Darrell and Attie, Paul and Cannata, Phil and Meredith, Greg and Sheth, Amit and Singh, Munindar and Tomlinson, Christine},
title = {Task Scheduling Using Intertask Dependencies in Carnot},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170150},
doi = {10.1145/170035.170150},
abstract = {The Carnot Project at MCC is addressing the problem of logically unifying physically-distributed, enterprise-wide, heterogeneous information. Carnot will provide a user with the means to navigate information efficiently and transparently, to update that information consistently, and to write applications easily for large, heterogeneous, distributed information systems. A prototype has been implemented which provides services for (a) enterprise modeling and model integration to create an enterprise-wide view, (b) semantic expansion of queries on the view to queries on individual resources, and (c) inter-resource consistency management. This paper describes the Carnot approach to transaction processing in environments where heterogeneous, distributed, and autonomous systems are required to coordinate the update of the local information under their control. In this approach, subtransactions are represented as a set of tasks and a set of intertask dependencies that capture the semantics of a particular relaxed transaction model. A scheduler has been implemented which schedules the execution of these tasks in the Carnot environment so that all intertask dependencies are satisfied.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {491–494},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170150,
author = {Woelk, Darrell and Attie, Paul and Cannata, Phil and Meredith, Greg and Sheth, Amit and Singh, Munindar and Tomlinson, Christine},
title = {Task Scheduling Using Intertask Dependencies in Carnot},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170150},
doi = {10.1145/170036.170150},
abstract = {The Carnot Project at MCC is addressing the problem of logically unifying physically-distributed, enterprise-wide, heterogeneous information. Carnot will provide a user with the means to navigate information efficiently and transparently, to update that information consistently, and to write applications easily for large, heterogeneous, distributed information systems. A prototype has been implemented which provides services for (a) enterprise modeling and model integration to create an enterprise-wide view, (b) semantic expansion of queries on the view to queries on individual resources, and (c) inter-resource consistency management. This paper describes the Carnot approach to transaction processing in environments where heterogeneous, distributed, and autonomous systems are required to coordinate the update of the local information under their control. In this approach, subtransactions are represented as a set of tasks and a set of intertask dependencies that capture the semantics of a particular relaxed transaction model. A scheduler has been implemented which schedules the execution of these tasks in the Carnot environment so that all intertask dependencies are satisfied.},
journal = {SIGMOD Rec.},
month = jun,
pages = {491–494},
numpages = {4}
}

@inproceedings{10.1145/170035.170151,
author = {Mannai, Dhamir N. and Bugrara, Khaled},
title = {Enhancing Inter-Operability and Data Sharing in Medical Information Systems},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170151},
doi = {10.1145/170035.170151},
abstract = {Clinical care generates an immense amount of patient data that has been archived and manipulated by computer-based information systems. Such computer-based medical record systems improved the accessibility of clinical information and made several studies of such information possible. Unfortunately, the care provider's task of retrieving, integrating, and interpreting only those portions of the patient's record that are relevant to a specific clinical problem is actually becoming increasingly difficult. This difficulty can be attributed primarily to the large variety of minimum data sets, the heterogeneous formats used to store the data, the heterogeneous data access methods and procedures, the varying granularity of access to data, the different rigid views of the data, and the lack of inter-operability among the information repositories of such data sets. Recognizing the aforementioned issues, we are engaged in a project to build a multi-database environment tailored for the interoperability of medical information systems. The main building blocks of such a system are a multi-disciplinary minimum data set and a catalogue for the support of interoperability and customization functions. In this paper, we report on the design approach used and describe the general architecture of the system.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {495–498},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170151,
author = {Mannai, Dhamir N. and Bugrara, Khaled},
title = {Enhancing Inter-Operability and Data Sharing in Medical Information Systems},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170151},
doi = {10.1145/170036.170151},
abstract = {Clinical care generates an immense amount of patient data that has been archived and manipulated by computer-based information systems. Such computer-based medical record systems improved the accessibility of clinical information and made several studies of such information possible. Unfortunately, the care provider's task of retrieving, integrating, and interpreting only those portions of the patient's record that are relevant to a specific clinical problem is actually becoming increasingly difficult. This difficulty can be attributed primarily to the large variety of minimum data sets, the heterogeneous formats used to store the data, the heterogeneous data access methods and procedures, the varying granularity of access to data, the different rigid views of the data, and the lack of inter-operability among the information repositories of such data sets. Recognizing the aforementioned issues, we are engaged in a project to build a multi-database environment tailored for the interoperability of medical information systems. The main building blocks of such a system are a multi-disciplinary minimum data set and a catalogue for the support of interoperability and customization functions. In this paper, we report on the design approach used and describe the general architecture of the system.},
journal = {SIGMOD Rec.},
month = jun,
pages = {495–498},
numpages = {4}
}

@inproceedings{10.1145/170035.170152,
author = {Woyna, Mark A. and Christiansen, John H. and Hield, Christopher W. and Simunich, Kathy Lee},
title = {Modeling Battlefield Sensor Environments with an Object Database Management System},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170152},
doi = {10.1145/170035.170152},
abstract = {The Visual Intelligence and Electronic Warfare Simulation (VIEWS) Workbench software system has been developed by Argonne National Laboratory (ANL) to enable Army intelligence and electronic warfare (IEW) analysts at Unix workstations to conveniently build detailed IEW battlefield scenarios, or “sensor environments”, to drive the Army's high-resolution IEW sensor performance models. VIEWS is fully object-oriented, including the underlying database.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {499–501},
numpages = {3},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170152,
author = {Woyna, Mark A. and Christiansen, John H. and Hield, Christopher W. and Simunich, Kathy Lee},
title = {Modeling Battlefield Sensor Environments with an Object Database Management System},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170152},
doi = {10.1145/170036.170152},
abstract = {The Visual Intelligence and Electronic Warfare Simulation (VIEWS) Workbench software system has been developed by Argonne National Laboratory (ANL) to enable Army intelligence and electronic warfare (IEW) analysts at Unix workstations to conveniently build detailed IEW battlefield scenarios, or “sensor environments”, to drive the Army's high-resolution IEW sensor performance models. VIEWS is fully object-oriented, including the underlying database.},
journal = {SIGMOD Rec.},
month = jun,
pages = {499–501},
numpages = {3}
}

@inproceedings{10.1145/170035.171529,
author = {Schaller, Tony},
title = {The INtersect Concept for Multidatabase System Integration in the Pharmaceutical Industry},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171529},
doi = {10.1145/170035.171529},
abstract = {The industry trends facing information systems in the 1990's involve a matrix of complex requirements. The migration from centralized mainframe computing to desktop personal computing has created opportunities and challenges in moving access to and control of information closer to the end-user. Within this new wave of computing, there has been a drive to move the access control for information to the desktop of the user. Although, graphical user standards were introduced in order to ease of use and provide a consistent look and feel to desktop applications. This provided some assistance in shielding the complexity of the various systems, however it did not address the difficulties presented in accessing heterogeneous database systems on various platforms.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {502–504},
numpages = {3},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171529,
author = {Schaller, Tony},
title = {The INtersect Concept for Multidatabase System Integration in the Pharmaceutical Industry},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171529},
doi = {10.1145/170036.171529},
abstract = {The industry trends facing information systems in the 1990's involve a matrix of complex requirements. The migration from centralized mainframe computing to desktop personal computing has created opportunities and challenges in moving access to and control of information closer to the end-user. Within this new wave of computing, there has been a drive to move the access control for information to the desktop of the user. Although, graphical user standards were introduced in order to ease of use and provide a consistent look and feel to desktop applications. This provided some assistance in shielding the complexity of the various systems, however it did not address the difficulties presented in accessing heterogeneous database systems on various platforms.},
journal = {SIGMOD Rec.},
month = jun,
pages = {502–504},
numpages = {3}
}

@inproceedings{10.1145/170035.171533,
author = {Gemis, Marc and Paredaens, Jan and Thyssens, Inge and Van den Bussche, Jan},
title = {GOOD: A Graph-Oriented Object Database System},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171533},
doi = {10.1145/170035.171533},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {505–510},
numpages = {6},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171533,
author = {Gemis, Marc and Paredaens, Jan and Thyssens, Inge and Van den Bussche, Jan},
title = {GOOD: A Graph-Oriented Object Database System},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171533},
doi = {10.1145/170036.171533},
journal = {SIGMOD Rec.},
month = jun,
pages = {505–510},
numpages = {6}
}

@inproceedings{10.1145/170035.171537,
author = {Consens, Mariano and Mendelzon, Alberto},
title = {Hy+: A Hygraph-Based Query and Visualization System},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171537},
doi = {10.1145/170035.171537},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {511–516},
numpages = {6},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171537,
author = {Consens, Mariano and Mendelzon, Alberto},
title = {Hy+: A Hygraph-Based Query and Visualization System},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171537},
doi = {10.1145/170036.171537},
journal = {SIGMOD Rec.},
month = jun,
pages = {511–516},
numpages = {6}
}

@inproceedings{10.1145/170035.171539,
author = {Chu, Wesley W. and Merzbacher, Matthew and Berkovich, Ladislav},
title = {The Design and Implementation of CoBase},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171539},
doi = {10.1145/170035.171539},
abstract = {CoBase, a cooperative database, is a new type of distributed database that integrates knowledge base technology with database systems to provide cooperative (approximate and conceptual) query answering. Based on the database schema and application characteristics, data are organized into conceptual (type abstraction) hierarchies. The higher levels of the hierarchy provide a more abstract data representation than the lower levels. Generalization (moving up in the hierarchy), specialization (moving down in the hierarchy) and association (moving between hierarchies) are the three key operations in deriving cooperative query answers.Relaxation in CoBase can also be specified explicitly in the query by the user or calling program through cooperative operators. We have extended SQL to CSQL by adding cooperative primitives. We describe the CoBase software implementation, including an inter-module data protocol that provides a uniform module interface. This modular approach provides flexibility in adding new relaxation modules and simplifies software maintenance.CoBase uses LOOM as its knowledge representation and inference system and supports relational data bases (e.g. Oracle and Sybase). We have demonstrated the feasibility and functionality of CoBase on top of a Transportation Database. The CoBase methodology has also been adopted in the multi-media medical distributed database project at UCLA, which provides approximate query answers to medical queries.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {517–522},
numpages = {6},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171539,
author = {Chu, Wesley W. and Merzbacher, Matthew and Berkovich, Ladislav},
title = {The Design and Implementation of CoBase},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171539},
doi = {10.1145/170036.171539},
abstract = {CoBase, a cooperative database, is a new type of distributed database that integrates knowledge base technology with database systems to provide cooperative (approximate and conceptual) query answering. Based on the database schema and application characteristics, data are organized into conceptual (type abstraction) hierarchies. The higher levels of the hierarchy provide a more abstract data representation than the lower levels. Generalization (moving up in the hierarchy), specialization (moving down in the hierarchy) and association (moving between hierarchies) are the three key operations in deriving cooperative query answers.Relaxation in CoBase can also be specified explicitly in the query by the user or calling program through cooperative operators. We have extended SQL to CSQL by adding cooperative primitives. We describe the CoBase software implementation, including an inter-module data protocol that provides a uniform module interface. This modular approach provides flexibility in adding new relaxation modules and simplifies software maintenance.CoBase uses LOOM as its knowledge representation and inference system and supports relational data bases (e.g. Oracle and Sybase). We have demonstrated the feasibility and functionality of CoBase on top of a Transportation Database. The CoBase methodology has also been adopted in the multi-media medical distributed database project at UCLA, which provides approximate query answers to medical queries.},
journal = {SIGMOD Rec.},
month = jun,
pages = {517–522},
numpages = {6}
}

@inproceedings{10.1145/170035.171541,
author = {Keller, Arthur M. and Jensen, Richard and Agarwal, Shailesh},
title = {Persistence Software: Bridging Object-Oriented Programming and Relational Databases},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171541},
doi = {10.1145/170035.171541},
abstract = {Building object-oriented applications which access relational data introduces a number of technical issues for developers who are making the transition to C++. We describe these issues and discuss how we have addressed them in Persistence, an application development tool that uses an automatic code generator to merge C++ applications with relational data. We use client-side caching to provide the application program with efficient access to the data.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {523–528},
numpages = {6},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171541,
author = {Keller, Arthur M. and Jensen, Richard and Agarwal, Shailesh},
title = {Persistence Software: Bridging Object-Oriented Programming and Relational Databases},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171541},
doi = {10.1145/170036.171541},
abstract = {Building object-oriented applications which access relational data introduces a number of technical issues for developers who are making the transition to C++. We describe these issues and discuss how we have addressed them in Persistence, an application development tool that uses an automatic code generator to merge C++ applications with relational data. We use client-side caching to provide the application program with efficient access to the data.},
journal = {SIGMOD Rec.},
month = jun,
pages = {523–528},
numpages = {6}
}

@inproceedings{10.1145/170035.171543,
author = {K\"{u}pper, D. and Storbel, M. and R\"{o}sner, D.},
title = {NAUDA: A Cooperative Natural Language Interface to Relational Databases},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171543},
doi = {10.1145/170035.171543},
abstract = {The NAUDA1 System is a cooperative natural (German) language database interface for relational databases. The project is carried out at FAW2 - funded by the state of Baden-W\"{u}rttemberg and IBM Germany. This paper describes the extension of a natural language interface to relational databases with respect to its cooperative behavior. We argue that cooperative support of users is especially important for a complex domain such as environmental protection. In order to enrich traditional database reports our system provides dialog-oriented features such as over-answering (providing more information than explicitly requested) and handling of presupposition failure, as well as presentation-oriented features such as natural language responses and geographical maps. Additional information by the system includes (meta-) information about the domain.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {529–533},
numpages = {5},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171543,
author = {K\"{u}pper, D. and Storbel, M. and R\"{o}sner, D.},
title = {NAUDA: A Cooperative Natural Language Interface to Relational Databases},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171543},
doi = {10.1145/170036.171543},
abstract = {The NAUDA1 System is a cooperative natural (German) language database interface for relational databases. The project is carried out at FAW2 - funded by the state of Baden-W\"{u}rttemberg and IBM Germany. This paper describes the extension of a natural language interface to relational databases with respect to its cooperative behavior. We argue that cooperative support of users is especially important for a complex domain such as environmental protection. In order to enrich traditional database reports our system provides dialog-oriented features such as over-answering (providing more information than explicitly requested) and handling of presupposition failure, as well as presentation-oriented features such as natural language responses and geographical maps. Additional information by the system includes (meta-) information about the domain.},
journal = {SIGMOD Rec.},
month = jun,
pages = {529–533},
numpages = {5}
}

@inproceedings{10.1145/170035.171545,
author = {Bukhres, O. and Chen, J. and Elmagarmid, A. and Liu, X. and Mullen, J.},
title = {InterBase: A Multidatabase Prototype Systems},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171545},
doi = {10.1145/170035.171545},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {534–539},
numpages = {6},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171545,
author = {Bukhres, O. and Chen, J. and Elmagarmid, A. and Liu, X. and Mullen, J.},
title = {InterBase: A Multidatabase Prototype Systems},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171545},
doi = {10.1145/170036.171545},
journal = {SIGMOD Rec.},
month = jun,
pages = {534–539},
numpages = {6}
}

@inproceedings{10.1145/170035.171546,
author = {Su, Stanley Y. W. and Lam, Herman X. and Eddula, Srinivasa and Arroyo, Javier and Prasad, Neeta and Zhuang, Ronghao},
title = {OSAM*.KBMS: An Object-Oriented Knowledge Base Management System for Supporting Advanced Applications},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171546},
doi = {10.1145/170035.171546},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {540–541},
numpages = {2},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171546,
author = {Su, Stanley Y. W. and Lam, Herman X. and Eddula, Srinivasa and Arroyo, Javier and Prasad, Neeta and Zhuang, Ronghao},
title = {OSAM*.KBMS: An Object-Oriented Knowledge Base Management System for Supporting Advanced Applications},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171546},
doi = {10.1145/170036.171546},
journal = {SIGMOD Rec.},
month = jun,
pages = {540–541},
numpages = {2}
}

@inproceedings{10.1145/170035.171548,
author = {Moenkeberg, Axel and Zabback, Peter and Hasse, Christof and Weikum, Gerhard},
title = {The COMFORT Prototype: A Step towards Automated Database Performance Tuning},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171548},
doi = {10.1145/170035.171548},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {542–543},
numpages = {2},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171548,
author = {Moenkeberg, Axel and Zabback, Peter and Hasse, Christof and Weikum, Gerhard},
title = {The COMFORT Prototype: A Step towards Automated Database Performance Tuning},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171548},
doi = {10.1145/170036.171548},
journal = {SIGMOD Rec.},
month = jun,
pages = {542–543},
numpages = {2}
}

@inproceedings{10.1145/170035.171550,
author = {Ramakrishnan, Raghu and Roth, William G. and Seshadri, Praveen and Srivastava, Divesh and Sudarshan, S.},
title = {The CORAL Deductive Database System},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171550},
doi = {10.1145/170035.171550},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {544–545},
numpages = {2},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171550,
author = {Ramakrishnan, Raghu and Roth, William G. and Seshadri, Praveen and Srivastava, Divesh and Sudarshan, S.},
title = {The CORAL Deductive Database System},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171550},
doi = {10.1145/170036.171550},
journal = {SIGMOD Rec.},
month = jun,
pages = {544–545},
numpages = {2}
}

@inproceedings{10.1145/170035.171552,
author = {Andersson, M. and Auddino, A-M. and Dupont, Y. and Fontana, E. and Gentile, M. and Spaccapietra, S.},
title = {The “SUPER” Project},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171552},
doi = {10.1145/170035.171552},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {546–547},
numpages = {2},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171552,
author = {Andersson, M. and Auddino, A-M. and Dupont, Y. and Fontana, E. and Gentile, M. and Spaccapietra, S.},
title = {The “SUPER” Project},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171552},
doi = {10.1145/170036.171552},
journal = {SIGMOD Rec.},
month = jun,
pages = {546–547},
numpages = {2}
}

@inproceedings{10.1145/170035.171555,
author = {Polyachenko, Boris E. and Andon, Filipp I.},
title = {Instrumental Complex of Parallel Software System Development and Operating Environment Support for Distributed Processing within Multitransputer Systems, TRANSSOFT},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171555},
doi = {10.1145/170035.171555},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {548–549},
numpages = {2},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171555,
author = {Polyachenko, Boris E. and Andon, Filipp I.},
title = {Instrumental Complex of Parallel Software System Development and Operating Environment Support for Distributed Processing within Multitransputer Systems, TRANSSOFT},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171555},
doi = {10.1145/170036.171555},
journal = {SIGMOD Rec.},
month = jun,
pages = {548–549},
numpages = {2}
}

@inproceedings{10.1145/170035.171558,
author = {Cacace, F. and Ceri, S. and Crespi-Reghizzi, S. and Fraternali, P. and Paraboschi, S. and Tanca, L.},
title = {The LOGRES Prototype},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171558},
doi = {10.1145/170035.171558},
abstract = {Logres is a new-generation database system integrating features from deductive and object-oriented databases [1, 2, 3, 4, 5]. The data model of Logres supports structural and semantic complexity through a rich collection of concepts from object-oriented models. The rule language allows for the manipulation of complex objects, the generation of new objects, and the definition of passive and active constraints. The application of set of rules to database states is controlled by means of qualifiers, which dictate the side effects of rules; qualifiers are the unique procedural feature of Logres, otherwise a fully declarative language.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {550–551},
numpages = {2},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171558,
author = {Cacace, F. and Ceri, S. and Crespi-Reghizzi, S. and Fraternali, P. and Paraboschi, S. and Tanca, L.},
title = {The LOGRES Prototype},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171558},
doi = {10.1145/170036.171558},
abstract = {Logres is a new-generation database system integrating features from deductive and object-oriented databases [1, 2, 3, 4, 5]. The data model of Logres supports structural and semantic complexity through a rich collection of concepts from object-oriented models. The rule language allows for the manipulation of complex objects, the generation of new objects, and the definition of passive and active constraints. The application of set of rules to database states is controlled by means of qualifiers, which dictate the side effects of rules; qualifiers are the unique procedural feature of Logres, otherwise a fully declarative language.},
journal = {SIGMOD Rec.},
month = jun,
pages = {550–551},
numpages = {2}
}

@inproceedings{10.1145/170035.171560,
author = {Ford, Steve and Blakeley, Jos\'{e} A. and Bannon, Thomas J.},
title = {Open OODB: A Modular Object-Oriented DBMS},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171560},
doi = {10.1145/170035.171560},
abstract = {The Open OODB project, part of the DARPA Persistent Object Base (POB) Program, is an effort to build an open, extensible object-oriented database management system (OODB) in which database functionality can be tailored for particular applications within an incrementally improvable framework. The system is designed to serve both as a platform for research and as a testbed that can meet the needs of demanding, next generation database applications. The Open OODB project goals are to describe the design space of OODBs; build an architectural framework that enables configuring independently useful modules to form an OODB; verify the suitability of this open approach by implementing an OODB to these specifications; and identify opportunities for building consensus that can lead to much-needed OODB standards. The motivating factors in this approach were that our previous experience in object-oriented databases had convinced us that different applications have differing requirements, and that a monolithic system is unlikely to meet the needs of many demanding kinds of applications.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {552–553},
numpages = {2},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171560,
author = {Ford, Steve and Blakeley, Jos\'{e} A. and Bannon, Thomas J.},
title = {Open OODB: A Modular Object-Oriented DBMS},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171560},
doi = {10.1145/170036.171560},
abstract = {The Open OODB project, part of the DARPA Persistent Object Base (POB) Program, is an effort to build an open, extensible object-oriented database management system (OODB) in which database functionality can be tailored for particular applications within an incrementally improvable framework. The system is designed to serve both as a platform for research and as a testbed that can meet the needs of demanding, next generation database applications. The Open OODB project goals are to describe the design space of OODBs; build an architectural framework that enables configuring independently useful modules to form an OODB; verify the suitability of this open approach by implementing an OODB to these specifications; and identify opportunities for building consensus that can lead to much-needed OODB standards. The motivating factors in this approach were that our previous experience in object-oriented databases had convinced us that different applications have differing requirements, and that a monolithic system is unlikely to meet the needs of many demanding kinds of applications.},
journal = {SIGMOD Rec.},
month = jun,
pages = {552–553},
numpages = {2}
}

@inproceedings{10.1145/170035.171562,
author = {Hasan, W. and Heytens, M. and Kolovson, C. and Neimat, M.-A. and Potamianos, S. and Schneider, D.},
title = {Papyrus GIS Demonstration},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171562},
doi = {10.1145/170035.171562},
abstract = {The goal of the Papyrus project [3] is to provide tools and services to enable the integration and parallelization of specialized data managers so that data-intensive applications can be constructed easily and efficiently. In our terminology, a data manager (DM) is a set of specialized methods that manage persistent data. A collection of functions defines the interface to a DM and provides the only means of accessing its persistent data.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {554–555},
numpages = {2},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171562,
author = {Hasan, W. and Heytens, M. and Kolovson, C. and Neimat, M.-A. and Potamianos, S. and Schneider, D.},
title = {Papyrus GIS Demonstration},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171562},
doi = {10.1145/170036.171562},
abstract = {The goal of the Papyrus project [3] is to provide tools and services to enable the integration and parallelization of specialized data managers so that data-intensive applications can be constructed easily and efficiently. In our terminology, a data manager (DM) is a set of specialized methods that manage persistent data. A collection of functions defines the interface to a DM and provides the only means of accessing its persistent data.},
journal = {SIGMOD Rec.},
month = jun,
pages = {554–555},
numpages = {2}
}

@inproceedings{10.1145/170035.171563,
author = {Rakow, Thomas C. and Muth, Peter},
title = {The V3 Video Server--Managing Analog and Digital Video Clips},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171563},
doi = {10.1145/170035.171563},
abstract = {The V3 Video Server is a demonstration showing a multimedia application developed on top of the VODAK database management system. VODAK is a prototype of an object-oriented and distributed database management system (DBMS) developed at GMD-IPSI. The V3 Video Server allows a user to interactively store, retrieve, manipulate, and present analog and short digital video clips. A video clip consists of a sequence of pictures and corresponding sound. Several attributes like author, title, and a set of keywords are annotated. The highlights of the demonstration are as follows. (1) It is shown that an object-oriented database management systems is very useful for the development of multimedia applications. (2) The video server gives valuable hints for the development of an object-oriented database management system in direction to a multimedia database management system.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {556–557},
numpages = {2},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171563,
author = {Rakow, Thomas C. and Muth, Peter},
title = {The V3 Video Server--Managing Analog and Digital Video Clips},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171563},
doi = {10.1145/170036.171563},
abstract = {The V3 Video Server is a demonstration showing a multimedia application developed on top of the VODAK database management system. VODAK is a prototype of an object-oriented and distributed database management system (DBMS) developed at GMD-IPSI. The V3 Video Server allows a user to interactively store, retrieve, manipulate, and present analog and short digital video clips. A video clip consists of a sequence of pictures and corresponding sound. Several attributes like author, title, and a set of keywords are annotated. The highlights of the demonstration are as follows. (1) It is shown that an object-oriented database management systems is very useful for the development of multimedia applications. (2) The video server gives valuable hints for the development of an object-oriented database management system in direction to a multimedia database management system.},
journal = {SIGMOD Rec.},
month = jun,
pages = {556–557},
numpages = {2}
}

@inproceedings{10.1145/170035.171564,
author = {Muth, Peter and Rakow, Thomas C.},
title = {VODAK Open Nested Transactions—Visualizing Database Internals},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171564},
doi = {10.1145/170035.171564},
abstract = {VODAK is a prototype of an object-oriented, distributed database system developed during the past five years at the Integrated Publication and Information Systems Institute (IPSI). The aim of demonstrating VODAK Open Nested Transactions is to provide insights into internals of database systems that are usually hidden from application programmers and users. By utilizing semantics of methods, VODAK Open Nested Transactions increase the degree of parallelism between concurrent transactions compared to conventional transaction management schemes. Demonstrating the difference in parallelism provides users with a “feeling” for internal database mechanisms, application programmers with information about the impact of transaction management on performance, and system developers with ideas how to improve their systems with respect to transaction management.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {558–559},
numpages = {2},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171564,
author = {Muth, Peter and Rakow, Thomas C.},
title = {VODAK Open Nested Transactions—Visualizing Database Internals},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171564},
doi = {10.1145/170036.171564},
abstract = {VODAK is a prototype of an object-oriented, distributed database system developed during the past five years at the Integrated Publication and Information Systems Institute (IPSI). The aim of demonstrating VODAK Open Nested Transactions is to provide insights into internals of database systems that are usually hidden from application programmers and users. By utilizing semantics of methods, VODAK Open Nested Transactions increase the degree of parallelism between concurrent transactions compared to conventional transaction management schemes. Demonstrating the difference in parallelism provides users with a “feeling” for internal database mechanisms, application programmers with information about the impact of transaction management on performance, and system developers with ideas how to improve their systems with respect to transaction management.},
journal = {SIGMOD Rec.},
month = jun,
pages = {558–559},
numpages = {2}
}

@inproceedings{10.1145/170035.171565,
author = {Luniewski, Allen and Schwarz, Peter and Shoens, Kurt and Stamos, Jim and Thomas, John},
title = {Information Organization Using Rufus},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171565},
doi = {10.1145/170035.171565},
abstract = {Computer system users today are inundated with a flood of semi-structured information, such as documents, electronic mail, programs, and images. Today, this information is typically stored in filesystems that provide limited support for organizing, searching, and operating upon this data, all operations that are vital to the ability of users to effectively use this data. Database systems provide good function for organizing, searching, managing and writing applications on structured data. Current database systems are inappropriate for semi-structured information because moving the data into the database breaks all existing applications that use the data. The Rufus system attacks the problems of semi-structured information by using database function to help users manage semi-structured information without requiring that the user's information reside in the database.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {560–561},
numpages = {2},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171565,
author = {Luniewski, Allen and Schwarz, Peter and Shoens, Kurt and Stamos, Jim and Thomas, John},
title = {Information Organization Using Rufus},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171565},
doi = {10.1145/170036.171565},
abstract = {Computer system users today are inundated with a flood of semi-structured information, such as documents, electronic mail, programs, and images. Today, this information is typically stored in filesystems that provide limited support for organizing, searching, and operating upon this data, all operations that are vital to the ability of users to effectively use this data. Database systems provide good function for organizing, searching, managing and writing applications on structured data. Current database systems are inappropriate for semi-structured information because moving the data into the database breaks all existing applications that use the data. The Rufus system attacks the problems of semi-structured information by using database function to help users manage semi-structured information without requiring that the user's information reside in the database.},
journal = {SIGMOD Rec.},
month = jun,
pages = {560–561},
numpages = {2}
}

@inproceedings{10.1145/170035.171566,
author = {Arens, Yigal and Knoblock, Craig},
title = {SIMS: Retrieving and Integrating Information from Multiple Sources},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.171566},
doi = {10.1145/170035.171566},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {562–563},
numpages = {2},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.171566,
author = {Arens, Yigal and Knoblock, Craig},
title = {SIMS: Retrieving and Integrating Information from Multiple Sources},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.171566},
doi = {10.1145/170036.171566},
journal = {SIGMOD Rec.},
month = jun,
pages = {562–563},
numpages = {2}
}

