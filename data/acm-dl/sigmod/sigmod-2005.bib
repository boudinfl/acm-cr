@inproceedings{10.1145/1066157.1066159,
author = {Johnson, Theodore and Muthukrishnan, S. and Rozenbaum, Irina},
title = {Sampling Algorithms in a Stream Operator},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066159},
doi = {10.1145/1066157.1066159},
abstract = {Complex queries over high speed data streams often need to rely on approximations to keep up with their input. The research community has developed a rich literature on approximate streaming algorithms for this application. Many of these algorithms produce samples of the input stream, providing better properties than conventional random sampling. In this paper, we abstract the stream sampling process and design a new stream sample operator. We show how it can be used to implement a wide variety of algorithms that perform sampling and sampling-based aggregations. Also, we show how to implement the operator in Gigascope - a high speed stream database specialized for IP network monitoring applications. As an example study, we apply the operator within such an enhanced Gigascope to perform subset-sum sampling which is of great interest for IP network management. We evaluate this implemention on a live, high speed internet traffic data stream and find that (a) the operator is a flexible, versatile addition to Gigascope suitable for tuning and algorithm engineering, and (b) the operator imposes only a small evaluation overhead. This is the first operational implementation we know of, for a wide variety of stream sampling algorithms at line speed within a data stream management system.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {1–12},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066160,
author = {Balazinska, Magdalena and Balakrishnan, Hari and Madden, Samuel and Stonebraker, Michael},
title = {Fault-Tolerance in the Borealis Distributed Stream Processing System},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066160},
doi = {10.1145/1066157.1066160},
abstract = {We present a replication-based approach to fault-tolerant distributed stream processing in the face of node failures, network failures, and network partitions. Our approach aims to reduce the degree of inconsistency in the system while guaranteeing that available inputs capable of being processed are processed within a specified time threshold. This threshold allows a user to trade availability for consistency: a larger time threshold decreases availability but limits inconsistency, while a smaller threshold increases availability but produces more inconsistent results based on partial data. In addition, when failures heal, our scheme corrects previously produced results, ensuring eventual consistency.Our scheme uses a data-serializing operator to ensure that all replicas process data in the same order, and thus remain consistent in the absence of failures. To regain consistency after a failure heals, we experimentally compare approaches based on checkpoint/redo and undo/redo techniques and illustrate the performance trade-offs between these schemes.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {13–24},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066161,
author = {Cormode, Graham and Garofalakis, Minos and Muthukrishnan, S. and Rastogi, Rajeev},
title = {Holistic Aggregates in a Networked World: Distributed Tracking of Approximate Quantiles},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066161},
doi = {10.1145/1066157.1066161},
abstract = {While traditional database systems optimize for performance on one-shot queries, emerging large-scale monitoring applications require continuous tracking of complex aggregates and data-distribution summaries over collections of physically-distributed streams. Thus, effective solutions have to be simultaneously space efficient (at each remote site), communication efficient (across the underlying communication network), and provide continuous, guaranteed-quality estimates. In this paper, we propose novel algorithmic solutions for the problem of continuously tracking complex holistic aggregates in such a distributed-streams setting --- our primary focus is on approximate quantile summaries, but our approach is more broadly applicable and can handle other holistic-aggregate functions (e.g., "heavy-hitters" queries). We present the first known distributed-tracking schemes for maintaining accurate quantile estimates with provable approximation guarantees, while simultaneously optimizing the storage space at each remote site as well as the communication cost across the network. In a nutshell, our algorithms employ a combination of local tracking at remote sites and simple prediction models for local site behavior in order to produce highly communication- and space-efficient solutions. We perform extensive experiments with real and synthetic data to explore the various tradeoffs and understand the role of prediction models in our schemes. The results clearly validate our approach, revealing significant savings over naive solutions as well as our analytical worst-case guarantees.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {25–36},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066163,
author = {Huang, Zhengli and Du, Wenliang and Chen, Biao},
title = {Deriving Private Information from Randomized Data},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066163},
doi = {10.1145/1066157.1066163},
abstract = {Randomization has emerged as a useful technique for data disguising in privacy-preserving data mining. Its privacy properties have been studied in a number of papers. Kargupta et al. challenged the randomization schemes, and they pointed out that randomization might not be able to preserve privacy. However, it is still unclear what factors cause such a security breach, how they affect the privacy preserving property of the randomization, and what kinds of data have higher risk of disclosing their private contents even though they are randomized.We believe that the key factor is the correlations among attributes. We propose two data reconstruction methods that are based on data correlations. One method uses the Principal Component Analysis (PCA) technique, and the other method uses the Bayes Estimate (BE) technique. We have conducted theoretical and experimental analysis on the relationship between data correlations and the amount of private information that can be disclosed based our proposed data reconstructions schemes. Our studies have shown that when the correlations are high, the original data can be reconstructed more accurately, i.e., more private information can be disclosed.To improve privacy, we propose a modified randomization scheme, in which we let the correlation of random noises "similar" to the original data. Our results have shown that the reconstruction accuracy of both PCA-based and BE-based schemes become worse as the similarity increases.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {37–48},
numpages = {12},
keywords = {randomization, privacy-preserving data mining, bayes estimate, PCA},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066164,
author = {LeFevre, Kristen and DeWitt, David J. and Ramakrishnan, Raghu},
title = {Incognito: Efficient Full-Domain K-Anonymity},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066164},
doi = {10.1145/1066157.1066164},
abstract = {A number of organizations publish microdata for purposes such as public health and demographic research. Although attributes that clearly identify individuals, such as Name and Social Security Number, are generally removed, these databases can sometimes be joined with other public databases on attributes such as Zipcode, Sex, and Birthdate to re-identify individuals who were supposed to remain anonymous. "Joining" attacks are made easier by the availability of other, complementary, databases over the Internet.K-anonymization is a technique that prevents joining attacks by generalizing and/or suppressing portions of the released microdata so that no individual can be uniquely distinguished from a group of size k. In this paper, we provide a practical framework for implementing one model of k-anonymization, called full-domain generalization. We introduce a set of algorithms for producing minimal full-domain generalizations, and show that these algorithms perform up to an order of magnitude faster than previous algorithms on two real-life databases.Besides full-domain generalization, numerous other models have also been proposed for k-anonymization. The second contribution in this paper is a single taxonomy that categorizes previous models and introduces some promising new alternatives.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {49–60},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066165,
author = {Lakshmanan, Laks V. S. and Ng, Raymond T. and Ramesh, Ganesh},
title = {To Do or Not to Do: The Dilemma of Disclosing Anonymized Data},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066165},
doi = {10.1145/1066157.1066165},
abstract = {Decision makers of companies often face the dilemma of whether to release data for knowledge discovery, vis a vis the risk of disclosing proprietary or sensitive information. While there are various "sanitization" methods, in this paper we focus on anonymization, given its widespread use in practice. We give due diligence to the question of "just how safe the anonymized data is", in terms of protecting the true identities of the data objects. We consider both the scenarios when the hacker has no information, and more realistically, when the hacker may have partial information about items in the domain. We conduct our analyses in the context of frequent set mining. We propose to capture the prior knowledge of the hacker by means of a belief function, where an educated guess of the frequency of each item is assumed. For various classes of belief functions, which correspond to different degrees of prior knowledge, we derive formulas for computing the expected number of "cracks". While obtaining the exact values for the more general situations is computationally hard, we propose a heuristic called the O-estimate. It is easy to compute, and is shown to be accurate empirically with real benchmark datasets. Finally, based on the O-estimates, we propose a recipe for the decision makers to resolve their dilemma.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {61–72},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066167,
author = {Koutrika, Georgia and Ioannidis, Yannis},
title = {Constrained Optimalities in Query Personalization},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066167},
doi = {10.1145/1066157.1066167},
abstract = {Personalization is a powerful mechanism that helps users to cope with the abundance of information on the Web. Database query personalization achieves this by dynamically constructing queries that return results of high interest to the user. This, however, may conflict with other constraints on the query execution time and/or result size that may be imposed by the search context, such as the device used, the network connection, etc. For example, if the user is accessing information using a mobile phone, then it is desirable to construct a personalized query that executes quickly and returns a handful of answers. Constrained Query Personalization (CQP) is an integrated approach to database query answering that dynamically takes into account the queries issued, the user's interest in the results, response time, and result size in order to build personalized queries. In this paper, we introduce CQP as a family of constrained optimization problems, where each time one of the parameters of concern is optimized while the others remain within the bounds of range constraints. Taking into account some key (exact or approximate) properties of these parameters, we map CQP to a state search problem and provide several algorithms for the discovery of optimal solutions. Experimental results demonstrate the effectiveness of the proposed techniques and the appropriateness of the overall approach.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {73–84},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066168,
author = {Dong, Xin and Halevy, Alon and Madhavan, Jayant},
title = {Reference Reconciliation in Complex Information Spaces},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066168},
doi = {10.1145/1066157.1066168},
abstract = {Reference reconciliation is the problem of identifying when different references (i.e., sets of attribute values) in a dataset correspond to the same real-world entity. Most previous literature assumed references to a single class that had a fair number of attributes (e.g., research publications). We consider complex information spaces: our references belong to multiple related classes and each reference may have very few attribute values. A prime example of such a space is Personal Information Management, where the goal is to provide a coherent view of all the information on one's desktop.Our reconciliation algorithm has three principal features. First, we exploit the associations between references to design new methods for reference comparison. Second, we propagate information between reconciliation decisions to accumulate positive and negative evidences. Third, we gradually enrich references by merging attribute values. Our experiments show that (1) we considerably improve precision and recall over standard methods on a diverse set of personal information datasets, and (2) there are advantages to using our algorithm even on a standard citation dataset benchmark.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {85–96},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066169,
author = {Sinha, Vineet and Karger, David R.},
title = {Magnet: Supporting Navigation in Semistructured Data Environments},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066169},
doi = {10.1145/1066157.1066169},
abstract = {With the growing importance of systems containing arbitrary semi-structured relationships, the need for supporting users searching in such repositories has grown. Currently support for users' search needs either has required domain-specific user interfaces or has required users to be schema experts. We have developed a general-purpose tool that offers users helpful navigation and refinement options for seeking information in these semistructured repositories. We show how a tool can be built without requiring domain-specific assumptions about the information being explored. In addition to describing a general approach to the problem, we provide a set of natural, general-purpose refinement tactics, many generalized from past work on textual information retrieval.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {97–106},
numpages = {10},
keywords = {navigation, searching/browsing, semistructured data, information retrieval, metadata},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066171,
author = {Babu, Shivnath and Bizarro, Pedro and DeWitt, David},
title = {Proactive Re-Optimization},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066171},
doi = {10.1145/1066157.1066171},
abstract = {Traditional query optimizers rely on the accuracy of estimated statistics to choose good execution plans. This design often leads to suboptimal plan choices for complex queries, since errors in estimates for intermediate subexpressions grow exponentially in the presence of skewed and correlated data distributions. Reoptimization is a promising technique to cope with such mistakes. Current re-optimizers first use a traditional optimizer to pick a plan, and then react to estimation errors and resulting suboptimalities detected in the plan during execution. The effectiveness of this approach is limited because traditional optimizers choose plans unaware of issues affecting reoptimization. We address this problem using proactive reoptimization, a new approach that incorporates three techniques: i) the uncertainty in estimates of statistics is computed in the form of bounding boxes around these estimates, ii) these bounding boxes are used to pick plans that are robust to deviations of actual values from their estimates, and iii) accurate measurements of statistics are collected quickly and efficiently during query execution. We present an extensive evaluation of these techniques using a prototype proactive re-optimizer named Rio. In our experiments Rio outperforms current re-optimizers by up to a factor of three.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {107–118},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066172,
author = {Babcock, Brian and Chaudhuri, Surajit},
title = {Towards a Robust Query Optimizer: A Principled and Practical Approach},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066172},
doi = {10.1145/1066157.1066172},
abstract = {Research on query optimization has focused almost exclusively on reducing query execution time, while important qualities such as consistency and predictability have largely been ignored, even though most database users consider these qualities to be at least as important as raw performance. In this paper, we explore how the query optimization process can be made more robust, focusing on the important subproblem of cardinality estimation. The robust cardinality estimation technique that we propose allows for a user- or application-specified trade-off between performance and predictability, and it captures multi-dimensional correlations while remaining space- and time-efficient.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {119–130},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066173,
author = {Li, Chengkai and Chang, Kevin Chen-Chuan and Ilyas, Ihab F. and Song, Sumin},
title = {RankSQL: Query Algebra and Optimization for Relational Top-k Queries},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066173},
doi = {10.1145/1066157.1066173},
abstract = {This paper introduces RankSQL, a system that provides a systematic and principled framework to support efficient evaluations of ranking (top-k) queries in relational database systems (RDBMS), by extending relational algebra and query optimization. Previously, top-k query processing is studied in the middleware scenario or in RDBMS in a "piecemeal" fashion, i.e., focusing on specific operator or sitting outside the core of query engines. In contrast, we aim to support ranking as a first-class database construct. As a key insight, the new ranking relationship can be viewed as another logical property of data, parallel to the "membership" property of relational data model. While membership is essentially supported in RDBMS, the same support for ranking is clearly lacking. We address the fundamental integration of ranking in RDBMS in a way similar to how membership, i.e., Boolean filtering, is supported. We extend relational algebra by proposing a rank-relational model to capture the ranking property, and introducing new and extended operators to support ranking as a first-class construct. Enabled by the extended algebra, we present a pipelined and incremental execution model of ranking query plans (that cannot be expressed traditionally) based on a fundamental ranking principle. To optimize top-k queries, we propose a dimensional enumeration algorithm to explore the extended plan space by enumerating plans along two dual dimensions: ranking and membership. We also propose a sampling-based method to estimate the cardinality of rank-aware operators, for costing plans. Our experiments show the validity of our framework and the accuracy of the proposed estimation model.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {131–142},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066175,
author = {Bohannon, Philip and Fan, Wenfei and Flaster, Michael and Rastogi, Rajeev},
title = {A Cost-Based Model and Effective Heuristic for Repairing Constraints by Value Modification},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066175},
doi = {10.1145/1066157.1066175},
abstract = {Data integrated from multiple sources may contain inconsistencies that violate integrity constraints. The constraint repair problem attempts to find "low cost" changes that, when applied, will cause the constraints to be satisfied. While in most previous work repair cost is stated in terms of tuple insertions and deletions, we follow recent work to define a database repair as a set of value modifications. In this context, we introduce a novel cost framework that allows for the application of techniques from record-linkage to the search for good repairs. We prove that finding minimal-cost repairs in this model is NP-complete in the size of the database, and introduce an approach to heuristic repair-construction based on equivalence classes of attribute values. Following this approach, we define two greedy algorithms. While these simple algorithms take time cubic in the size of the database, we develop optimizations inspired by algorithms for duplicate-record detection that greatly improve scalability. We evaluate our framework and algorithms on synthetic and real data, and show that our proposed optimizations greatly improve performance at little or no cost in repair quality.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {143–154},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066176,
author = {Fuxman, Ariel and Fazli, Elham and Miller, Ren\'{e}e J.},
title = {ConQuer: Efficient Management of Inconsistent Databases},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066176},
doi = {10.1145/1066157.1066176},
abstract = {Although integrity constraints have long been used to maintain data consistency, there are situations in which they may not be enforced or satisfied. In this paper, we present ConQuer, a system for efficient and scalable answering of SQL queries on databases that may violate a set of constraints. ConQuer permits users to postulate a set of key constraints together with their queries. The system rewrites the queries to retrieve all (and only) data that is consistent with respect to the constraints. The rewriting is into SQL, so the rewritten queries can be efficiently optimized and executed by commercial database systems.We study the overhead of resolving inconsistencies dynamically (at query time). In particular, we present a set of performance experiments that compare the efficiency of the rewriting strategies used by ConQuer. The experiments use queries taken from the TPC-H workload. We show that the overhead is not onerous, and the consistent query answers can often be computed within twice the time required to obtain the answers to the original (non-rewritten) query.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {155–166},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066177,
author = {Melnik, Sergey and Bernstein, Philip A. and Halevy, Alon and Rahm, Erhard},
title = {Supporting Executable Mappings in Model Management},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066177},
doi = {10.1145/1066157.1066177},
abstract = {Model management is an approach to simplify the programming of metadata-intensive applications. It offers developers powerful operators, such as Compose, Diff, and Merge, that are applied to models, such as database schemas or interface specifications, and to mappings between models. Prior model management solutions focused on a simple class of mappings that do not have executable semantics. Yet many metadata applications require that mappings be executable, expressed in SQL, XSLT, or other data transformation languages.In this paper, we develop a semantics for model-management operators that allows applying the operators to executable mappings. Our semantics captures previously-proposed desiderata and is language-independent: the effect of the operators is expressed in terms of what they do to the instances of models and mappings. We describe an implemented prototype in which mappings are represented as dependencies between relational schemas, and discuss algebraic optimization of model-management scripts.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {167–178},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066179,
author = {DeHaan, David and Larson, Per-Ake and Zhou, Jingren},
title = {Stacked Indexed Views in Microsoft SQL Server},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066179},
doi = {10.1145/1066157.1066179},
abstract = {Appropriately selected materialized views (also called indexed views) can speed up query execution by orders of magnitude. Most database systems limit support for materialized views to select-project-join expressions, possibly with a group-by, over base tables because this class of views can be efficiently maintained incrementally and thus kept up to date with the underlying source tables. However, limiting views to reference only base tables restricts the class of queries that can be supported by materialized views. View stacking (also called views on views) relaxes one restriction by allowing a materialized view to reference both base tables and other materialized views. This extends materialized view support to additional types of queries. This paper describes a prototype implementation of stacked views within Microsoft SQL Server and explains which classes of queries can be supported. To support view matching for stacked views, a signature mechanism was added to the optimizer. This mechanism turned out to be beneficial also for regular views by significantly speeding up view matching.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {179–190},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066180,
author = {Cao, Bin and Badia, Antonio},
title = {A Nested Relational Approach to Processing SQL Subqueries},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066180},
doi = {10.1145/1066157.1066180},
abstract = {One of the most powerful features of SQL is the use of nested queries. Most research work on the optimization of nested queries focuses on aggregate subqueries. However, the solutions proposed for non-aggregate subqueries are still limited, especially for queries having multiple subqueries and null values. In this paper, we show that existing approaches to queries containing non-aggregate subqueries proposed in the literature (including rewrites) are not adequate. We then propose a new efficient approach, the nested relational approach, based on the nested relational algebra. Our approach directly unnests non-aggregate subqueries using hash joins, and treats all subqueries in a uniform manner, being able to deal with nested queries of any type and any level. We report on experimental work that confirms that existing approaches have difficulties dealing with non-aggregate subqueries, and that our approach offers better performance. We also discuss some possibilities for algebraic optimization and the issue of integrating our approach in a relational database system.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {191–202},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066181,
author = {Chan, Chee-Yong and Eng, Pin-Kwang and Tan, Kian-Lee},
title = {Stratified Computation of Skylines with Partially-Ordered Domains},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066181},
doi = {10.1145/1066157.1066181},
abstract = {In this paper, we study the evaluation of skyline queries with partially-ordered attributes. Because such attributes lack a total ordering, traditional index-based evaluation algorithms (e.g., NN and BBS) that are designed for totally-ordered attributes can no longer prune the space as effectively. Our solution is to transform each partially-ordered attribute into a two-integer domain that allows us to exploit index-based algorithms to compute skyline queries on the transformed space. Based on this framework, we propose three novel algorithms: BBS+ is a straightforward adaptation of BBS using the framework, and SDC (Stratification by Dominance Classification) and SDC+ are optimized to handle false positives and support progressive evaluation. Both SDC and SDC+ exploit a dominance relationship to organize the data into strata. While SDC generates its strata at run time, SDC+ partitions the data into strata offline. We also design two dominance classification strategies (MinPC and MaxPC) to further optimize the performance of SDC and SDC+. We implemented the proposed schemes and evaluated their efficiency. Our results show that our proposed techniques outperform existing approaches by a wide margin, with SDC+-MinPC giving the best performance in terms of both response time as well as progressiveness. To the best of our knowledge, this is the first paper to address the problem of skyline query evaluation involving partially-ordered attribute domains.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {203–214},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066183,
author = {Dittrich, Jens-Peter and Fischer, Peter M. and Kossmann, Donald},
title = {AGILE: Adaptive Indexing for Context-Aware Information Filters},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066183},
doi = {10.1145/1066157.1066183},
abstract = {Information filtering has become a key technology for modern information systems. The goal of an information filter is to route messages to the right recipients (possibly none) according to declarative rules called profiles. In order to deal with high volumes of messages, several index structures have been proposed in the past. The challenge addressed in this paper is to carry out stateful information filtering in which profiles refer to values in a database or to previous messages. The difficulty is that database update streams need to be processed in addition to messages. This paper presents AGILE, a way to extend existing index structures so that the indexes adapt to the message/update workload and show good performance in all situations. Performance experiments show that AGILE is overall the clear winner as compared to the best existing approaches. In extreme situations in which it is not the winner, the overheads are small.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {215–226},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066184,
author = {Bruno, Nicolas and Chaudhuri, Surajit},
title = {Automatic Physical Database Tuning: A Relaxation-Based Approach},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066184},
doi = {10.1145/1066157.1066184},
abstract = {In recent years there has been considerable research on automated selection of physical design in database systems. In current solutions, candidate access paths are heuristically chosen based on the structure of each input query, and a subsequent bottom-up search is performed to identify the best overall configuration. To handle large workloads and multiple kinds of physical structures, recent techniques have become increasingly complex: they exhibit many special cases, shortcuts, and heuristics that make it very difficult to analyze and extract properties. In this paper we critically examine the architecture of current solutions. We then design a new framework for the physical design problem that significantly reduces the assumptions and heuristics used in previous approaches. While simplicity and uniformity are important contributions in themselves, we report extensive experimental results showing that our approach could result in comparable (and, in many cases, considerably better) recommendations than state-of-the-art commercial alternatives.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {227–238},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066185,
author = {Consens, Mariano P. and Barbosa, Denilson and Teisanu, Adrian and Mignet, Laurent},
title = {Goals and Benchmarks for Autonomic Configuration Recommenders},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066185},
doi = {10.1145/1066157.1066185},
abstract = {We are witnessing an explosive increase in the complexity of the information systems we rely upon, Autonomic systems address this challenge by continuously configuring and tuning themselves. Recently, a number of autonomic features have been incorporated into commercial RDBMS; tools for recommending database configurations (i.e., indexes, materialized views, partitions) for a given workload are prominent examples of this promising trend.In this paper, we introduce a flexible characterization of the performance goals of configuration recommenders and develop an experimental evaluation approach to benchmark the effectiveness of these autonomic tools. We focus on exploratory queries and present extensive experimental results using both real and synthetic data that demonstrate the validity of the approach introduced. Our results identify a specific index configuration based on single-column indexes as a very useful baseline for comparisons in the exploratory setting. Furthermore, the experimental results demonstrate the unfulfilled potential for achieving improvements of several orders of magnitude.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {239–250},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066187,
author = {Agrawal, Rakesh and Srikant, Ramakrishnan and Thomas, Dilys},
title = {Privacy Preserving OLAP},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066187},
doi = {10.1145/1066157.1066187},
abstract = {We present techniques for privacy-preserving computation of multidimensional aggregates on data partitioned across multiple clients. Data from different clients is perturbed (randomized) in order to preserve privacy before it is integrated at the server. We develop formal notions of privacy obtained from data perturbation and show that our perturbation provides guarantees against privacy breaches. We develop and analyze algorithms for reconstructing counts of subcubes over perturbed data. We also evaluate the tradeoff between privacy guarantees and reconstruction accuracy and show the practicality of our approach.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {251–262},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066188,
author = {Chen, Zhimin and Narasayya, Vivek},
title = {Efficient Computation of Multiple Group by Queries},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066188},
doi = {10.1145/1066157.1066188},
abstract = {Data analysts need to understand the quality of data in the warehouse. This is often done by issuing many Group By queries on the sets of columns of interest. Since the volume of data in these warehouses can be large, and tables in a data warehouse often contain many columns, this analysis typically requires executing a large number of Group By queries, which can be expensive. We show that the performance of today's database systems for such data analysis is inadequate. We also show that the problem is computationally hard, and develop efficient techniques for solving it. We demonstrate significant speedup over existing approaches on today's commercial database systems.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {263–274},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066189,
author = {Jahangiri, Mehrdad and Sacharidis, Dimitris and Shahabi, Cyrus},
title = {SHIFT-SPLIT: I/O Efficient Maintenance of Wavelet-Transformed Multidimensional Data},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066189},
doi = {10.1145/1066157.1066189},
abstract = {The Discrete Wavelet Transform is a proven tool for a wide range of database applications. However, despite broad acceptance, some of its properties have not been fully explored and thus not exploited, particularly for two common forms of multidimensional decomposition. We introduce two novel operations for wavelet transformed data, termed SHIFT and SPLIT, based on the properties of wavelet trees, which work directly in the wavelet domain. We demonstrate their significance and usefulness by analytically proving six important results in four common data maintenance scenarios, i.e., transformation of massive datasets, appending data, approximation of data streams and partial data reconstruction, leading to significant I/O cost reduction in all cases. Furthermore, we show how these operations can be further improved in combination with the optimal coefficient-to-disk-block allocation strategy. Our exhaustive set of empirical experiments with real-world datasets verifies our claims.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {275–286},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066191,
author = {Manjhi, Amit and Nath, Suman and Gibbons, Phillip B.},
title = {Tributaries and Deltas: Efficient and Robust Aggregation in Sensor Network Streams},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066191},
doi = {10.1145/1066157.1066191},
abstract = {Existing energy-efficient approaches to in-network aggregation in sensor networks can be classified into two categories, tree-based and multi-path-based, with each having unique strengths and weaknesses. In this paper, we introduce Tributary-Delta, a novel approach that combines the advantages of the tree and multi-path approaches by running them simultaneously in different regions of the network. We present schemes for adjusting the regions in response to changes in network conditions, and show how many useful aggregates can be readily computed within this new framework. We then show how a difficult aggregate for this context---finding frequent items---can be efficiently computed within the framework. To this end, we devise the first algorithm for frequent items (and for quantiles) that provably minimizes the worst case total communication for non-regular trees. In addition, we give a multi-path algorithm for frequent items that is considerably more accurate than previous approaches. These algorithms form the basis for our efficient Tributary-Delta frequent items algorithm. Through extensive simulation with real-world and synthetic data, we show the significant advantages of our techniques. For example, in computing Count under realistic loss rates, our techniques reduce answer error by up to a factor of 3 compared to any previous technique.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {287–298},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066192,
author = {Zhang, Rui and Koudas, Nick and Ooi, Beng Chin and Srivastava, Divesh},
title = {Multiple Aggregations over Data Streams},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066192},
doi = {10.1145/1066157.1066192},
abstract = {Monitoring aggregates on IP traffic data streams is a compelling application for data stream management systems. The need for exploratory IP traffic data analysis naturally leads to posing related aggregation queries on data streams, that differ only in the choice of grouping attributes. In this paper, we address this problem of efficiently computing multiple aggregations over high speed data streams, based on a two-level LFTA/HFTA DSMS architecture, inspired by Gigascope.Our first contribution is the insight that in such a scenario, additionally computing and maintaining fine-granularity aggregation queries (phantoms) at the LFTA has the benefit of supporting shared computation. Our second contribution is an investigation into the problem of identifying beneficial LFTA configurations of phantoms and user-queries. We formulate this problem as a cost optimization problem, which consists of two sub-optimization problems: how to choose phantoms and how to allocate space for them in the LFTA. We formally show the hardness of determining the optimal configuration, and propose cost greedy heuristics for these independent sub-problems based on detailed analyses. Our final contribution is a thorough experimental study, based on real IP traffic data, as well as synthetic data, to demonstrate the effectiveness of our techniques for identifying beneficial configurations.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {299–310},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066193,
author = {Li, Jin and Maier, David and Tufte, Kristin and Papadimos, Vassilis and Tucker, Peter A.},
title = {Semantics and Evaluation Techniques for Window Aggregates in Data Streams},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066193},
doi = {10.1145/1066157.1066193},
abstract = {A windowed query operator breaks a data stream into possibly overlapping subsets of data and computes a result over each. Many stream systems can evaluate window aggregate queries. However, current stream systems suffer from a lack of an explicit definition of window semantics. As a result, their implementations unnecessarily confuse window definition with physical stream properties. This confusion complicates the stream system, and even worse, can hurt performance both in terms of memory usage and execution time. To address this problem, we propose a framework for defining window semantics, which can be used to express almost all types of windows of which we are aware, and which is easily extensible to other types of windows that may occur in the future. Based on this definition, we explore a one-pass query evaluation strategy, the Window-ID (WID) approach, for various types of window aggregate queries. WID significantly reduces both required memory space and execution time for a large class of window definitions. In addition, WID can leverage punctuations to gracefully handle disorder. Our experimental study shows that WID has better execution-time performance than existing window aggregate query evaluation options that retain and reprocess tuples, and has better latency-accuracy tradeoffs for disordered input streams compared to using a fixed delay for handling disorder.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {311–322},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066195,
author = {Linga, Prakash and Crainiceanu, Adina and Gehrke, Johannes and Shanmugasudaram, Jayavel},
title = {Guaranteeing Correctness and Availability in P2P Range Indices},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066195},
doi = {10.1145/1066157.1066195},
abstract = {New and emerging P2P applications require sophisticated range query capability and also have strict requirements on query correctness, system availability and item availability. While there has been recent work on developing new P2P range indices, none of these indices guarantee correctness and availability. In this paper, we develop new techniques that can provably guarantee the correctness and availability of P2P range indices. We develop our techniques in the context of a general P2P indexing framework that can be instantiated with most P2P index structures from the literature. As a specific instantiation, we implement P-Ring, an existing P2P range index, and show how it can be extended to guarantee correctness and availability. We quantitatively evaluate our techniques using a real distributed implementation.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {323–334},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066196,
author = {Sun, Xiaowei and Wang, Rui and Salzberg, Betty and Zou, Chendong},
title = {Online B-Tree Merging},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066196},
doi = {10.1145/1066157.1066196},
abstract = {Many scenarios involve merging of two B-tree indexes, both covering the same key range. Increasing demand for continuous availability and high performance requires that such merging be done online, with minimal interference to normal user transactions. In this paper we present an online B-tree merging method, in which the merging of leaf pages in two B-trees are piggybacked lazily with normal user transactions, thus making the merging I/O efficient and allowing user transactions to access only one index instead of both. The concurrency control mechanism is designed to interfere as little as possible with ongoing user transactions. Merging is made forward recoverable by following a conventional logging protocol, with a few extensions. Should a system failure occur, both indexes being merged can be recovered to a consistent state and no merging work is lost. Experiments and analysis show the I/O savings and the performance, and compare variations on the basic algorithm.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {335–346},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066197,
author = {Beyer, Kevin and Cochrane, Roberta J. and Josifovski, Vanja and Kleewein, Jim and Lapis, George and Lohman, Guy and Lyle, Bob and \"{O}zcan, Fatma and Pirahesh, Hamid and Seemann, Normen and Truong, Tuong and Van der Linden, Bert and Vickery, Brian and Zhang, Chun},
title = {System RX: One Part Relational, One Part XML},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066197},
doi = {10.1145/1066157.1066197},
abstract = {This paper describes the overall architecture and design aspects of a hybrid relational and XML database system called System RX. We believe that such a system is fundamental in the evolution of enterprise data management solutions: XML and relational data will co-exist and complement each other in enterprise solutions. Furthermore, a successful XML repository requires much of the same infrastructure that already exists in a relational database management system. Finally, XML query languages have considerable conceptual and functional overlap with relational dataflow engines. System RX is the first truly hybrid system that comingles XML and relational data, giving them equal footing. The new support for XML includes native support for storage and indexing as well as query compilation and evaluation support for the latest industry-standard query languages, SQL/XML and XQuery. By building a hybrid system, we leverage more than 20 years of data management research to advance XML technology to the same standards expected from mature relational systems.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {347–358},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066199,
author = {Xie, Junyi and Yang, Jun and Chen, Yuguo},
title = {On Joining and Caching Stochastic Streams},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066199},
doi = {10.1145/1066157.1066199},
abstract = {We consider the problem of joining data streams using limited cache memory, with the goal of producing as many result tuples as possible from the cache. Many cache replacement heuristics have been proposed in the past. Their performance often relies on implicit assumptions about the input streams, e.g., that the join attribute values follow a relatively stationary distribution. However, in general and in practice, streams often exhibit more complex behaviors, such as increasing trends and random walks, rendering these "hardwired" heuristics inadequate.In this paper, we propose a framework that is able to exploit known or observed statistical properties of input streams to make cache replacement decisions aimed at maximizing the expected number of result tuples. To illustrate the complexity of the solution space, we show that even an algorithm that considers, at every time step, all possible sequences of future replacement decisions may not be optimal. We then identify a condition between two candidate tuples under which an optimal algorithm would always choose one tuple over the other to replace. We develop a heuristic that behaves consistently with an optimal algorithm whenever this condition is satisfied. We show through experiments that our heuristic outperforms previous ones.As another evidence of the generality of our framework, we show that the classic caching/paging problem for static objects can be reduced to a stream join problem and analyzed under our framework, yielding results that agree with or extend classic ones.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {359–370},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066200,
author = {Tao, Yufei and Yiu, Man Lung and Papadias, Dimitris and Hadjieleftheriou, Marios and Mamoulis, Nikos},
title = {RPJ: Producing Fast Join Results on Streams through Rate-Based Optimization},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066200},
doi = {10.1145/1066157.1066200},
abstract = {We consider the problem of "progressively" joining relations whose records are continuously retrieved from remote sources through an unstable network that may incur temporary failures. The objectives are to (i) start reporting the first output tuples as soon as possible (before the participating relations are completely received), and (ii) produce the remaining results at a fast rate. We develop a new algorithm RPJ (<u>R</u>ate-based <u>P</u>rogressive <u>J</u>oin) based on solid theoretical analysis. RPJ maximizes the output rate by optimizing its execution according to the characteristics of the join relations (e.g., data distribution, tuple arrival pattern, etc.). Extensive experiments prove that our technique delivers results significantly faster than the previous methods.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {371–382},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066201,
author = {Harizopoulos, Stavros and Shkapenyuk, Vladislav and Ailamaki, Anastassia},
title = {QPipe: A Simultaneously Pipelined Relational Query Engine},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066201},
doi = {10.1145/1066157.1066201},
abstract = {Relational DBMS typically execute concurrent queries independently by invoking a set of operator instances for each query. To exploit common data retrievals and computation in concurrent queries, researchers have proposed a wealth of techniques, ranging from buffering disk pages to constructing materialized views and optimizing multiple queries. The ideas proposed, however, are inherently limited by the query-centric philosophy of modern engine designs. Ideally, the query engine should proactively coordinate same-operator execution among concurrent queries, thereby exploiting common accesses to memory and disks as well as common intermediate result computation.This paper introduces on-demand simultaneous pipelining (OSP), a novel query evaluation paradigm for maximizing data and work sharing across concurrent queries at execution time. OSP enables proactive, dynamic operator sharing by pipelining the operator's output simultaneously to multiple parent nodes. This paper also introduces QPipe, a new operator-centric relational engine that effortlessly supports OSP. Each relational operator is encapsulated in a micro-engine serving query tasks from a queue, naturally exploiting all data and work sharing opportunities. Evaluation of QPipe built on top of BerkeleyDB shows that QPipe achieves a 2x speedup over a commercial DBMS when running a workload consisting of TPC-H queries.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {383–394},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066203,
author = {Zhu, Qingbo and Hsu, Windsor W.},
title = {Fossilized Index: The Linchpin of Trustworthy Non-Alterable Electronic Records},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066203},
doi = {10.1145/1066157.1066203},
abstract = {As critical records are increasingly stored in electronic form, which tends to make for easy destruction and clandestine modification, it is imperative that they be properly managed to preserve their trustworthiness, i.e., their ability to provide irrefutable proof and accurate details of events that have occurred. The need for proper record keeping is further underscored by the recent corporate misconduct and ensuing attempts to destroy incriminating records. Currently, the industry practice and regulatory requirements (e.g., SEC Rule 17a-4) rely on storing records in WORM storage to immutably preserve the records. In this paper, we contend that simply storing records in WORM storage is increasingly inadequate to ensure that they are trustworthy. Specifically, with the large volume of records that are typical today, meeting the ever more stringent query response time requires the use of direct access mechanisms such as indexes. Relying on indexes for accessing records could, however, provide a means for effectively altering or deleting records, even those stored in WORM storage.In this paper, we establish the key requirements for a fossilized index that protects the records from such logical modification. We also analyze current indexing methods to determine how they fall short of these requirements. Based on our insights, we propose the Generalized Hash Tree (GHT). Using both theoretical analysis and simulations with real system data, we demonstrate that the GHT can satisfy the requirements of a fossilized index with performance and cost that are comparable to regular indexing techniques such as the B-tree. We further note that as records are indexed on multiple fields to facilitate search and retrieval, the records can be reconstructed from the corresponding index entries even after the records expire and are disposed of, Therefore, we also present a novel method to eliminate this disclosure risk by allowing an index entry to be effectively disposed of when its record expires.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {395–406},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066204,
author = {Pang, HweeHwa and Jain, Arpit and Ramamritham, Krithi and Tan, Kian-Lee},
title = {Verifying Completeness of Relational Query Results in Data Publishing},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066204},
doi = {10.1145/1066157.1066204},
abstract = {In data publishing, the owner delegates the role of satisfying user queries to a third-party publisher. As the publisher may be untrusted or susceptible to attacks, it could produce incorrect query results. In this paper, we introduce a scheme for users to verify that their query results are complete (i.e., no qualifying tuples are omitted) and authentic (i.e., all the result values originated from the owner). The scheme supports range selection on key and non-key attributes, project as well as join queries on relational databases. Moreover, the proposed scheme complies with access control policies, is computationally secure, and can be implemented efficiently.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {407–418},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066205,
author = {Lin, Yi and Kemme, Bettina and Pati\~{n}o-Mart\'{\i}nez, Marta and Jim\'{e}nez-Peris, Ricardo},
title = {Middleware Based Data Replication Providing Snapshot Isolation},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066205},
doi = {10.1145/1066157.1066205},
abstract = {Many cluster based replication solutions have been proposed providing scalability and fault-tolerance. Many of these solutions perform replica control in a middleware on top of the database replicas. In such a setting concurrency control is a challenge and is often performed on a table basis. Additionally, some systems put severe requirements on transaction programs (e.g., to declare all objects to be accessed in advance). This paper addresses these issues and presents a middleware-based replication scheme which provides the popular snapshot isolation level at the same tuple-level granularity as database systems like PostgreSQL and Oracle, without any need to declare transaction properties in advance. Both read-only and update transactions can be executed at any replica while providing data consistency at all times. Our approach provides what we call "1-copy-snapshot-isolation" as long as the underlying database replicas provide snapshot isolation. We have implemented our approach as a replicated middleware on top of PostgreSQL replicas. By providing a standard JDBC interface, the middleware is completely transparent to the client program. Fault-tolerance is provided by automatically reconnecting clients in case of crashes. Our middleware shows good performance in terms of response times and scalability.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {419–430},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066207,
author = {Weis, Melanie and Naumann, Felix},
title = {DogmatiX Tracks down Duplicates in XML},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066207},
doi = {10.1145/1066157.1066207},
abstract = {Duplicate detection is the problem of detecting different entries in a data source representing the same real-world entity. While research abounds in the realm of duplicate detection in relational data, there is yet little work for duplicates in other, more complex data models, such as XML. In this paper, we present a generalized framework for duplicate detection, dividing the problem into three components: candidate definition defining which objects are to be compared, duplicate definition defining when two duplicate candidates are in fact duplicates, and duplicate detection specifying how to efficiently find those duplicates.Using this framework, we propose an XML duplicate detection method, DogmatiX, which compares XML elements based not only on their direct data values, but also on the similarity of their parents, children, structure, etc. We propose heuristics to determine which of these to choose, as well as a similarity measure specifically geared towards the XML data model. An evaluation of our algorithm using several heuristics validates our approach.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {431–442},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066208,
author = {Sawires, Arsany and Tatemura, Junichi and Po, Oliver and Agrawal, Divyakant and Candan, K. Sel\c{C}uk},
title = {Incremental Maintenance of Path-Expression Views},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066208},
doi = {10.1145/1066157.1066208},
abstract = {Caching data by maintaining materialized views typically requires updating the cache appropriately to reflect dynamic source updates. Extensive research has addressed the problem of incremental view maintenance for relational data but only few works have addressed it for semi-structured data. In this paper we address the problem of incremental maintenance of views defined over XML documents using path-expressions. The approach described in this paper has the following main features that distinguish it from the previous works: (1) The view specification language is powerful and standardized enough to be used in realistic applications. (2) The size of the auxiliary data maintained with the views depends on the expression size and the answer size regardless of the source data size.(3) No source schema is assumed to exist; the source data can be any general well-formed XML document. Experimental evaluation is conducted to assess the performance benefits of the proposed approach.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {443–454},
numpages = {12},
keywords = {XML views, path expressions, caching, incremental view maintenance},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066209,
author = {Chen, Ting and Lu, Jiaheng and Ling, Tok Wang},
title = {On Boosting Holism in XML Twig Pattern Matching Using Structural Indexing Techniques},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066209},
doi = {10.1145/1066157.1066209},
abstract = {Searching for all occurrences of a twig pattern in an XML document is an important operation in XML query processing. Recently a holistic method TwigStack. [2] has been proposed. The method avoids generating large intermediate results which do not contribute to the final answer and is CPU and I/O optimal when twig patterns only have ancestor-descendant relationships. Another important direction of XML query processing is to build structural indexes [3][8][13][15] over XML documents to avoid unnecessary scanning of source documents. We regard XML structural indexing as a technique to partition XML documents and call it streaming scheme in our paper. In this paper we develop a method to perform holistic twig pattern matching on XML documents partitioned using various streaming schemes. Our method avoids unnecessary scanning of irrelevant portion of XML documents. More importantly, depending on different streaming schemes used, it can process a large class of twig patterns consisting of both ancestor-descendant and parent-child relationships and avoid generating redundant intermediate results. Our experiments demonstrate the applicability and the performance advantages of our approach.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {455–466},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066211,
author = {Tung, Anthony K. H. and Xu, Xin and Ooi, Beng Chin},
title = {CURLER: Finding and Visualizing Nonlinear Correlation Clusters},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066211},
doi = {10.1145/1066157.1066211},
abstract = {While much work has been done in finding linear correlation among subsets of features in high-dimensional data, work on detecting nonlinear correlation has been left largely untouched. In this paper, we present an algorithm for finding and visualizing nonlinear correlation clusters in the subspace of high-dimensional databases.Unlike the detection of linear correlation in which clusters are of unique orientations, finding nonlinear correlation clusters of varying orientations requires merging clusters of possibly very different orientations. Combined with the fact that spatial proximity must be judged based on a subset of features that are not originally known, deciding which clusters to be merged during the clustering process becomes a challenge. To avoid this problem, we propose a novel concept called co-sharing level which captures both spatial proximity and cluster orientation when judging similarity between clusters. Based on this concept, we develop an algorithm which not only detects nonlinear correlation clusters but also provides a way to visualize them. Experiments on both synthetic and real-life datasets are done to show the effectiveness of our method.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {467–478},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066212,
author = {Hu, Haibo and Xu, Jianliang and Lee, Dik Lun},
title = {A Generic Framework for Monitoring Continuous Spatial Queries over Moving Objects},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066212},
doi = {10.1145/1066157.1066212},
abstract = {This paper proposes a generic framework for monitoring continuous spatial queries over moving objects. The framework distinguishes itself from existing work by being the first to address the location update issue and to provide a common interface for monitoring mixed types of queries. Based on the notion of safe region, the client location update strategy is developed based on the queries being monitored. Thus, it significantly reduces the wireless communication and query reevaluation costs required to maintain the up-to-date query results. We propose algorithms for query evaluation/reevaluation and for safe region computation in this framework. Enhancements are also proposed to take advantage of two practical mobility assumptions: maximum speed and steady movement. The experimental results show that our framework substantially outperforms the traditional periodic monitoring scheme in terms of monitoring accuracy and CPU time while achieving a close-to-optimal wireless communication cost. The framework also can scale up to a large monitoring system and is robust under various object mobility patterns.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {479–490},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066213,
author = {Chen, Lei and \"{O}zsu, M. Tamer and Oria, Vincent},
title = {Robust and Fast Similarity Search for Moving Object Trajectories},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066213},
doi = {10.1145/1066157.1066213},
abstract = {An important consideration in similarity-based retrieval of moving object trajectories is the definition of a distance function. The existing distance functions are usually sensitive to noise, shifts and scaling of data that commonly occur due to sensor failures, errors in detection techniques, disturbance signals, and different sampling rates. Cleaning data to eliminate these is not always possible. In this paper, we introduce a novel distance function, Edit Distance on Real sequence (EDR) which is robust against these data imperfections. Analysis and comparison of EDR with other popular distance functions, such as Euclidean distance, Dynamic Time Warping (DTW), Edit distance with Real Penalty (ERP), and Longest Common Subsequences (LCSS), indicate that EDR is more robust than Euclidean distance, DTW and ERP, and it is on average 50% more accurate than LCSS. We also develop three pruning techniques to improve the retrieval efficiency of EDR and show that these techniques can be combined effectively in a search, increasing the pruning power significantly. The experimental results confirm the superior efficiency of the combined methods.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {491–502},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066215,
author = {Beyer, Kevin and Chamb\'{e}rlin, Don and Colby, Latha S. and \"{O}zcan, Fatma and Pirahesh, Hamid and Xu, Yu},
title = {Extending XQuery for Analytics},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066215},
doi = {10.1145/1066157.1066215},
abstract = {XQuery is a query language under development by the W3C XML Query Working Group. The language contains constructs for navigating, searching, and restructuring XML data. With XML gaining importance as the standard for representing business data, XQuery must support the types of queries that are common in business analytics. One such class of queries is OLAP-style aggregation queries. Although these queries are expressible in XQuery Version 1, the lack of explicit grouping constructs makes the construction of these queries non-intuitive and places a burden on the XQuery engine to recognize and optimize the implicit grouping constructs. Furthermore, although the flexibility of the XML data model provides an opportunity for advanced forms of grouping that are not easily represented in relational systems, these queries are difficult to express using the current XQuery syntax. In this paper, we provide a proposal for extending the XQuery FLWOR expression with explicit syntax for grouping and for numbering of results. We show that these new XQuery constructs not only simplify the construction and evaluation of queries requiring grouping and ranking but also enable complex analytic queries such as moving-window aggregation and rollups along dynamic hierarchies to be expressed without additional language extensions.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {503–514},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066216,
author = {Catania, Barbara and Ooi, Beng Chin and Wang, Wenqiang and Wang, Xiaoling},
title = {Lazy XML Updates: Laziness as a Virtue, of Update and Structural Join Efficiency},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066216},
doi = {10.1145/1066157.1066216},
abstract = {XML documents are normally stored as plain text files. Hence, the natural and most convenient way to update XML documents is to simply edit the text files. But efficient query evaluation algorithms require XML documents to be indexed. Every element is given a unique identifier based on its location in the document or its preorder-traversal order, and this identifier is later used as (part of) the key in the index. Reassigning orders of possibly a large number of elements is therefore necessary when the original XML documents are updated. Immutable dynamic labeling schemes have been proposed to solve this problem, that, however, require very long labels and may decrease query performance. If we consider a real-world scenario, we note that many relatively small ad-hoc XML segments are inserted/deleted into/from an existing XML database. In this paper, we start from this consideration and we propose a new lazy approach to handle XML updates that also improves query performance. The lazy approach: (i) completely avoids reassigning existing element orders after updates; (ii) improves query processing by taking advantages from segments. Experimental results show that our approach is much more efficient in handling updates than using immutable labeling and, at the same time, it also improves the performance of recently defined structural join algorithms.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {515–526},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066217,
author = {Xu, Yu and Papakonstantinou, Yannis},
title = {Efficient Keyword Search for Smallest LCAs in XML Databases},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066217},
doi = {10.1145/1066157.1066217},
abstract = {Keyword search is a proven, user-friendly way to query HTML documents in the World Wide Web. We propose keyword search in XML documents, modeled as labeled trees, and describe corresponding efficient algorithms. The proposed keyword search returns the set of smallest trees containing all keywords, where a tree is designated as "smallest" if it contains no tree that also contains all keywords. Our core contribution, the Indexed Lookup Eager algorithm, exploits key properties of smallest trees in order to outperform prior algorithms by orders of magnitude when the query contains keywords with significantly different frequencies. The Scan Eager variant is tuned for the case where the keywords have similar frequencies. We analytically and experimentally evaluate two variants of the Eager algorithm, along with the Stack algorithm [13]. We also present the XKSearch system, which utilizes the Indexed Lookup Eager, Scan Eager and Stack algorithms and a demo of which on DBLP data is available at http://www.db.ucsd.edu/projects/xksearch. Finally, we extend the Indexed Lookup Eager algorithm to answer Lowest Common Ancestor (LCA) queries.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {527–538},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066219,
author = {Deutsch, Alin and Marcus, Monica and Sui, Liying and Vianu, Victor and Zhou, Dayou},
title = {A Verifier for Interactive, Data-Driven Web Applications},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066219},
doi = {10.1145/1066157.1066219},
abstract = {We present WAVE, a verifier for interactive, database-driven Web applications specified using high-level modeling tools such as WebML. WAVE is complete for a broad class of applications and temporal properties. For other applications, WAVE can be used as an incomplete verifier, as commonly done in software verification. Our experiments on four representative data-driven applications and a battery of common properties yielded surprisingly good verification times, on the order of seconds. This suggests that interactive applications controlled by database queries may be unusually well suited to automatic verification. They also show that the coupling of model checking with database optimization techniques used in the implementation of WAVE can be extremely effective. This is significant both to the database area and to automatic verification in general.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {539–550},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066220,
author = {Cho, Junghoo and Roy, Sourashis and Adams, Robert E.},
title = {Page Quality: In Search of an Unbiased Web Ranking},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066220},
doi = {10.1145/1066157.1066220},
abstract = {In a number of recent studies [4, 8] researchers have found that because search engines repeatedly return currently popular pages at the top of search results, popular pages tend to get even more popular, while unpopular pages get ignored by an average user. This "rich-get-richer" phenomenon is particularly problematic for new and high-quality pages because they may never get a chance to get users' attention, decreasing the overall quality of search results in the long run. In this paper, we propose a new ranking function, called page quality that can alleviate the problem of popularity-based ranking. We first present a formal framework to study the search engine bias by discussing what is an "ideal" way to measure the intrinsic quality of a page. We then compare how PageRank, the current ranking metric used by major search engines, differs from this ideal quality metric. This framework will help us investigate the search engine bias in more concrete terms and provide clear understanding why PageRank is effective in many cases and exactly when it is problematic. We then propose a practical way to estimate the intrinsic page quality to avoid the inherent bias of PageRank. We derive our proposed quality estimator through a careful analysis of a reasonable web user model, and we present experimental results that show the potential of our proposed estimator. We believe that our quality estimator has the potential to alleviate the rich-get-richer phenomenon and help new and high-quality pages get the attention that they deserve.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {551–562},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066222,
author = {Jermaine, Christopher and Dobra, Alin and Arumugam, Subramanian and Joshi, Shantanu and Pol, Abhijit},
title = {A Disk-Based Join with Probabilistic Guarantees},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066222},
doi = {10.1145/1066157.1066222},
abstract = {One of the most common operations in analytic query processing is the application of an aggregate function to the result of a relational join. We describe an algorithm for computing the answer to such a query over large, disk-based input tables. The key innovation of our algorithm is that at all times, it provides an online, statistical estimator for the eventual answer to the query, as well as probabilistic confidence bounds. Thus, a user can monitor the progress of the join throughout its execution and stop the join when satisfied with the estimate's accuracy, or run the algorithm to completion with a total time requirement that is not much longer than other common join algorithms. This contrasts with other online join algorithms, which either do not offer such statistical guarantees or can only offer guarantees so long as the input data can fit into core memory.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {563–574},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066223,
author = {Chaudhuri, Surajit and Kaushik, Raghav and Ramamurthy, Ravishankar},
title = {When Can We Trust Progress Estimators for SQL Queries?},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066223},
doi = {10.1145/1066157.1066223},
abstract = {The problem of estimating progress for long-running queries has recently been introduced. We analyze the characteristics of the progress estimation problem, from the perspective of providing robust, worst-case guarantees. Our first result is that in the worst case, no progress estimation algorithm can yield anything even moderately better than the trivial guarantee that identifies the progress as lying between 0% and 100%. In such cases, we introduce an estimator that can optimally bound the error. However, we show that in many "good" scenarios, it is possible to design effective progress estimators with small error bounds. We then demonstrate empirically that these "good" scenarios are common in practice and discuss possible ways of combining the estimators.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {575–586},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066224,
author = {Pol, Abhijit and Jermaine, Christopher},
title = {Relational Confidence Bounds Are Easy with the Bootstrap},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066224},
doi = {10.1145/1066157.1066224},
abstract = {Statistical estimation and approximate query processing have become increasingly prevalent applications for database systems. However, approximation is usually of little use without some sort of guarantee on estimation accuracy, or "confidence bound." Analytically deriving probabilistic guarantees for database queries over sampled data is a daunting task, not suitable for the faint of heart, and certainly beyond the expertise of the typical database system end-user. This paper considers the problem of incorporating into a database system a powerful "plug-in" method for computing confidence bounds on the answer to relational database queries over sampled or incomplete data. This statistical tool, called the bootstrap, is simple enough that it can be used by a data-base programmer with a rudimentary mathematical background, but general enough that it can be applied to almost any statistical inference problem. Given the power and ease-of-use of the bootstrap, we argue that the algorithms presented for supporting the bootstrap should be incorporated into any database system which is intended to support analytic processing.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {587–598},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066226,
author = {Sakurai, Yasushi and Papadimitriou, Spiros and Faloutsos, Christos},
title = {BRAID: Stream Mining through Group Lag Correlations},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066226},
doi = {10.1145/1066157.1066226},
abstract = {The goal is to monitor multiple numerical streams, and determine which pairs are correlated with lags, as well as the value of each such lag. Lag correlations (and anti-correlations) are frequent, and very interesting in practice: For example, a decrease in interest rates typically precedes an increase in house sales by a few months; higher amounts of fluoride in the drinking water may lead to fewer dental cavities, some years later. Additional settings include network analysis, sensor monitoring, financial data analysis, and moving object tracking. Such data streams are often correlated (or anti-correlated), but with an unknown lag.We propose BRAID, a method to detect lag correlations between data streams. BRAID can handle data streams of semi-infinite length, incrementally, quickly, and with small resource consumption. We also provide a theoretical analysis, which, based on Nyquist's sampling theorem, shows that BRAID can estimate lag correlations with little, and often with no error at all. Our experiments on real and realistic data show that BRAID detects the correct lag perfectly most of the time (the largest relative error was about 1%); while it is up to 40,000 times faster than the naive implementation.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {599–610},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066227,
author = {Govindaraju, Naga K. and Raghuvanshi, Nikunj and Manocha, Dinesh},
title = {Fast and Approximate Stream Mining of Quantiles and Frequencies Using Graphics Processors},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066227},
doi = {10.1145/1066157.1066227},
abstract = {We present algorithms for fast quantile and frequency estimation in large data streams using graphics processors (GPUs). We exploit the high computation power and memory bandwidth of graphics processors and present a new sorting algorithm that performs rasterization operations on the GPUs. We use sorting as the main computational component for histogram approximation and construction of ε-approximate quantile and frequency summaries. Our algorithms for numerical statistics computation on data streams are deterministic, applicable to fixed or variable-sized sliding windows and use a limited memory footprint. We use GPU as a co-processor and minimize the data transmission between the CPU and GPU by taking into account the low bus bandwidth. We implemented our algorithms on a PC with a NVIDIA GeForce FX 6800 Ultra GPU and a 3.4 GHz Pentium IV CPU and applied them to large data streams consisting of more than 100 million values. We also compared the performance of our GPU-based algorithms with optimized implementations of prior CPU-based algorithms. Overall, our results demonstrate that the graphics processors available on a commodity computer system are efficient stream-processor and useful co-processors for mining data streams.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {611–622},
numpages = {12},
keywords = {quantiles, sorting, sliding windows, graphics processors, frequencies, memory bandwidth, data streams},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066228,
author = {Zhang, Minghua and Kao, Ben and Cheung, David W. and Yip, Kevin Y.},
title = {Mining Periodic Patterns with Gap Requirement from Sequences},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066228},
doi = {10.1145/1066157.1066228},
abstract = {We study a problem of mining frequently occurring periodic patterns with a gap requirement from sequences. Given a character sequence S of length L and a pattern P of length l, we consider P a frequently occurring pattern in S if the probability of observing P given a randomly picked length-l subsequence of S exceeds a certain threshold. In many applications, particularly those related to bioinformatics, interesting patterns are periodic with a gap requirement. That is to say, the characters in P should match subsequences of S in such a way that the matching characters in S are separated by gaps of more or less the same size. We show the complexity of the mining problem and discuss why traditional mining algorithms are computationally infeasible. We propose practical algorithms for solving the problem, and study their characteristics. We also present a case study in which we apply our algorithms on some DNA sequences. We discuss some interesting patterns obtained from the case study.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {623–633},
numpages = {11},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066230,
author = {Mouratidis, Kyriakos and Papadias, Dimitris and Hadjieleftheriou, Marios},
title = {Conceptual Partitioning: An Efficient Method for Continuous Nearest Neighbor Monitoring},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066230},
doi = {10.1145/1066157.1066230},
abstract = {Given a set of objects P and a query point q, a k nearest neighbor (k-NN) query retrieves the k objects in P that lie closest to q. Even though the problem is well-studied for static datasets, the traditional methods do not extend to highly dynamic environments where multiple continuous queries require real-time results, and both objects and queries receive frequent location updates. In this paper we propose conceptual partitioning (CPM), a comprehensive technique for the efficient monitoring of continuous NN queries. CPM achieves low running time by handling location updates only from objects that fall in the vicinity of some query (and ignoring the rest). It can be used with multiple, static or moving queries, and it does not make any assumptions about the object moving patterns. We analyze the performance of CPM and show that it outperforms the current state-of-the-art algorithms for all problem settings. Finally, we extend our framework to aggregate NN (ANN) queries, which monitor the data objects that minimize the aggregate distance with respect to a set of query points (e.g., the objects with the minimum sum of distances to all query points).},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {634–645},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066231,
author = {Denny, Matthew and Franklin, Michael J.},
title = {Predicate Result Range Caching for Continuous Queries},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066231},
doi = {10.1145/1066157.1066231},
abstract = {Many analysis and monitoring applications require the repeated execution of expensive modeling functions over streams of rapidly changing data. These applications can often be expressed declaratively, but the continuous query processors developed to date are not designed to optimize queries with expensive functions. To speed up such queries, we present CASPER: the CAching System for PrEdicate Result ranges. CASPER computes and caches predicate result ranges, which are ranges of stream input values where the system knows the results of expensive predicate evaluations. Over time, CASPER expands ranges so that they are more likely to contain future stream values. This paper presents the CASPER architecture, as well as algorithms for computing and expanding ranges for a large class of predicates. We demonstrate the effectiveness of CASPER using a prototype implementation and a financial application using real bond market data.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {646–657},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066232,
author = {Golab, Lukasz and \"{O}zsu, M. Tamer},
title = {Update-Pattern-Aware Modeling and Processing of Continuous Queries},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066232},
doi = {10.1145/1066157.1066232},
abstract = {A defining characteristic of continuous queries over on-line data streams, possibly bounded by sliding windows, is the potentially infinite and time-evolving nature of their inputs and outputs. New items continually arrive on the input streams and new results are continually produced. Additionally, inputs expire by falling out of range of their sliding windows and results expire when they cease to satisfy the query. This impacts continuous query processing in two ways. First, data stream systems allow tables to be queried alongside data streams, but in terms of query semantics, it is not clear how updates of tables are different from insertions and deletions caused by the movement of the sliding windows. Second, many interesting queries need to store state, which must be kept up-to-date as time goes on. Therefore, query processing efficiency depends highly on the amount of overhead involved in state maintenance.In this paper, we show that the above issues can be solved by understanding the update patterns of continuous queries and exploiting them during query processing. We propose a classification that defines four types of update characteristics. Using our classification, we present a definition of continuous query semantics that clearly states the role of relations. We then propose the notion of update-pattern-aware query processing, where physical implementations of query operators, including the data structures used for storing intermediate state, vary depending on the update patterns of their inputs and outputs. When tested on IP traffic logs, our update-pattern-aware query plans routinely outperform the existing techniques by an order of magnitude.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {658–669},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066234,
author = {Cong, Gao and Tan, Kian-Lee and Tung, Anthony K. H. and Xu, Xin},
title = {Mining Top-K Covering Rule Groups for Gene Expression Data},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066234},
doi = {10.1145/1066157.1066234},
abstract = {In this paper, we propose a novel algorithm to discover the top-k covering rule groups for each row of gene expression profiles. Several experiments on real bioinformatics datasets show that the new top-k covering rule mining algorithm is orders of magnitude faster than previous association rule mining algorithms.Furthermore, we propose a new classification method RCBT. RCBT classifier is constructed from the top-k covering rule groups. The rule groups generated for building RCBT are bounded in number. This is in contrast to existing rule-based classification methods like CBA [19] which despite generating excessive number of redundant rules, is still unable to cover some training data with the discovered rules. Experiments show that the RCBT classifier can match or outperform other state-of-the-art classifiers on several benchmark gene expression datasets. In addition, the top-k covering rule groups themselves provide insights into the mechanisms responsible for diseases directly.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {670–681},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066235,
author = {Wu, Huanmei and Salzberg, Betty and Sharp, Gregory C and Jiang, Steve B and Shirato, Hiroki and Kaeli, David},
title = {Subsequence Matching on Structured Time Series Data},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066235},
doi = {10.1145/1066157.1066235},
abstract = {Subsequence matching in time series databases is a useful technique, with applications in pattern matching, prediction, and rule discovery. Internal structure within the time series data can be used to improve these tasks, and provide important insight into the problem domain. This paper introduces our research effort in using the internal structure of a time series directly in the matching process. This idea is applied to the problem domain of respiratory motion data in cancer radiation treatment. We propose a comprehensive solution for analysis, clustering, and online prediction of respiratory motion using subsequence similarity matching. In this system, a motion signal is captured in real time as a data stream, and is analyzed immediately for treatment and also saved in a database for future study. A piecewise linear representation of the signal is generated from a finite state model, and is used as a query for subsequence matching. To ensure that the query subsequence is representative, we introduce the concept of subsequence stability, which can be used to dynamically adjust the query subsequence length. To satisfy the special needs of similarity matching over breathing patterns, a new subsequence similarity measure is introduced. This new measure uses a weighted L1 distance function to capture the relative importance of each source stream, amplitude, frequency, and proximity in time. From the subsequence similarity measure, stream and patient similarity can be defined, which are then used for offline and online applications. The matching results are analyzed and applied for motion prediction and correlation discovery. While our system has been customized for use in radiation therapy, our approach to time series modeling is general enough for application domains with structured time series data.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {682–693},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066236,
author = {Zhao, Lizhuang and Zaki, Mohammed J.},
title = {TRICLUSTER: An Effective Algorithm for Mining Coherent Clusters in 3D Microarray Data},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066236},
doi = {10.1145/1066157.1066236},
abstract = {In this paper we introduce a novel algorithm called TRICLUSTER, for mining coherent clusters in three-dimensional (3D) gene expression datasets. TRICLUSTER can mine arbitrarily positioned and overlapping clusters, and depending on different parameter values, it can mine different types of clusters, including those with constant or similar values along each dimension, as well as scaling and shifting expression patterns. TRICLUSTER relies on graph-based approach to mine all valid clusters. For each time slice, i.e., a gene\texttimes{}sample matrix, it constructs the range multigraph, a compact representation of all similar value ranges between any two sample columns. It then searches for constrained maximal cliques in this multigraph to yield the set of bi-clusters for this time slice. Then TRICLUSTER constructs another graph using the biclusters (as vertices) from each time slice; mining cliques from this graph yields the final set of triclusters. Optionally, TRICLUSTER merges/deletes some clusters having large overlaps. We present a useful set of metrics to evaluate the clustering quality, and we show that TRICLUSTER can find significant triclusters in the real microarray datasets.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {694–705},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066238,
author = {Athitsos, Vassilis and Hadjieleftheriou, Marios and Kollios, George and Sclaroff, Stan},
title = {Query-Sensitive Embeddings},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066238},
doi = {10.1145/1066157.1066238},
abstract = {A common problem in many types of databases is retrieving the most similar matches to a query object. Finding those matches in a large database can be too slow to be practical, especially in domains where objects are compared using computationally expensive similarity (or distance) measures. This paper proposes a novel method for approximate nearest neighbor retrieval in such spaces. Our method is embedding-based, meaning that it constructs a function that maps objects into a real vector space. The mapping preserves a large amount of the proximity structure of the original space, and it can be used to rapidly obtain a short list of likely matches to the query. The main novelty of our method is that it constructs, together with the embedding, a query-sensitive distance measure that should be used when measuring distances in the vector space. The term "query-sensitive" means that the distance measure changes depending on the current query object. We report experiments with an image database of handwritten digits, and a time-series database. In both cases, the proposed method outperforms existing state-of-the-art embedding methods, meaning that it provides significantly better trade-offs between efficiency and retrieval accuracy.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {706–717},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066239,
author = {Lee, JeongKyu and Oh, JungHwan and Hwang, Sae},
title = {STRG-Index: Spatio-Temporal Region Graph Indexing for Large Video Databases},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066239},
doi = {10.1145/1066157.1066239},
abstract = {In this paper, we propose new graph-based data structure and indexing to organize and retrieve video data. Several researches have shown that a graph can be a better candidate for modeling semantically rich and complicated multimedia data. However, there are few methods that consider the temporal feature of video data, which is a distinguishable and representative characteristic when compared with other multimedia (i.e., images). In order to consider the temporal feature effectively and efficiently, we propose a new graph-based data structure called Spatio-Temporal Region Graph (STRG). Unlike existing graph-based data structures which provide only spatial features, the proposed STRG further provides temporal features, which represent temporal relationships among spatial objects. The STRG is decomposed into its subgraphs in which redundant subgraphs are eliminated to reduce the index size and search time, because the computational complexity of graph matching (subgraph isomorphism) is NP-complete. In addition, a new distance measure, called Extended Graph Edit Distance (EGED), is introduced in both non-metric and metric spaces for matching and indexing respectively. Based on STRG and EGED, we propose a new indexing method STRG-Index, which is faster and more accurate since it uses tree structure and clustering algorithm. We compare the STRG-Index with the M-tree, which is a popular tree-based indexing method for multimedia data. The STRG-Index outperforms the M-tree for various query loads in terms of cost and speed.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {718–729},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066240,
author = {Shen, Heng Tao and Ooi, Beng Chin and Zhou, Xiaofang},
title = {Towards Effective Indexing for Very Large Video Sequence Database},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066240},
doi = {10.1145/1066157.1066240},
abstract = {With rapid advances in video processing technologies and ever fast increments in network bandwidth, the popularity of video content publishing and sharing has made similarity search an indispensable operation to retrieve videos of user interests. The video similarity is usually measured by the percentage of similar frames shared by two video sequences, and each frame is typically represented as a high-dimensional feature vector. Unfortunately, high complexity of video content has posed the following major challenges for fast retrieval: (a) effective and compact video representations, (b) efficient similarity measurements, and (c) efficient indexing on the compact representations. In this paper, we propose a number of methods to achieve fast similarity search for very large video database. First, each video sequence is summarized into a small number of clusters, each of which contains similar frames and is represented by a novel compact model called Video Triplet (ViTri). ViTri models a cluster as a tightly bounded hypersphere described by its position, radius, and density. The ViTri similarity is measured by the volume of intersection between two hyperspheres multiplying the minimal density, i.e., the estimated number of similar frames shared by two clusters. The total number of similar frames is then estimated to derive the overall similarity between two video sequences. Hence the time complexity of video similarity measure can be reduced greatly. To further reduce the number of similarity computations on ViTris, we introduce a new one dimensional transformation technique which rotates and shifts the original axis system using PCA in such a way that the original inter-distance between two high-dimensional vectors can be maximally retained after mapping. An efficient B+-tree is then built on the transformed one dimensional values of ViTris' positions. Such a transformation enables B+-tree to achieve its optimal performance by quickly filtering a large portion of non-similar ViTris. Our extensive experiments on real large video datasets prove the effectiveness of our proposals that outperform existing methods significantly.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {730–741},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066242,
author = {Kanne, Carl-Christian and Brantner, Matthias and Moerkotte, Guido},
title = {Cost-Sensitive Reordering of Navigational Primitives},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066242},
doi = {10.1145/1066157.1066242},
abstract = {We present a method to evaluate path queries based on the novel concept of partial path instances. Our method (1) maximizes performance by means of sequential scans or asynchronous I/O, (2) does not require a special storage format, (3) relies on simple navigational primitives on trees, and (4) can be complemented by existing logical and physical optimizations such as duplicate elimination, duplicate prevention and path rewriting.We use a physical algebra which separates those navigation operations that require I/O from those that do not. All I/O operations necessary for the evaluation of a path are isolated in a single operator, which may employ efficient I/O scheduling strategies such as sequential scans or asynchronous I/O.Performance results for queries from the XMark benchmark show that reordering the navigation operations can increase performance up to a factor of four.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {742–753},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066243,
author = {Yang, Rui and Kalnis, Panos and Tung, Anthony K. H.},
title = {Similarity Evaluation on Tree-Structured Data},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066243},
doi = {10.1145/1066157.1066243},
abstract = {Tree-structured data are becoming ubiquitous nowadays and manipulating them based on similarity is essential for many applications. The generally accepted similarity measure for trees is the edit distance. Although similarity search has been extensively studied, searching for similar trees is still an open problem due to the high complexity of computing the tree edit distance. In this paper, we propose to transform tree-structured data into an approximate numerical multidimensional vector which encodes the original structure information. We prove that the L1 distance of the corresponding vectors, whose computational complexity is O(|T1| + |T2|), forms a lower bound for the edit distance between trees. Based on the theoretical analysis, we describe a novel algorithm which embeds the proposed distance into a filter-and-refine framework to process similarity search on tree-structured data. The experimental results show that our algorithm reduces dramatically the distance computation cost. Our method is especially suitable for accelerating similarity query processing on large trees in massive datasets.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {754–765},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066244,
author = {Yan, Xifeng and Yu, Philip S. and Han, Jiawei},
title = {Substructure Similarity Search in Graph Databases},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066244},
doi = {10.1145/1066157.1066244},
abstract = {Advanced database systems face a great challenge raised by the emergence of massive, complex structural data in bioinformatics, chem-informatics, and many other applications. The most fundamental support needed in these applications is the efficient search of complex structured data. Since exact matching is often too restrictive, similarity search of complex structures becomes a vital operation that must be supported efficiently.In this paper, we investigate the issues of substructure similarity search using indexed features in graph databases. By transforming the edge relaxation ratio of a query graph into the maximum allowed missing features, our structural filtering algorithm, called Grafil, can filter many graphs without performing pairwise similarity computations. It is further shown that using either too few or too many features can result in poor filtering performance. Thus the challenge is to design an effective feature set selection strategy for filtering. By examining the effect of different feature selection mechanisms, we develop a multi-filter composition strategy, where each filter uses a distinct and complementary subset of the features. We identify the criteria to form effective feature sets for filtering, and demonstrate that combining features with similar size and selectivity can improve the filtering and search performance significantly. Moreover, the concept presented in Grafil can be applied to searching approximate non-consecutive sequences, trees, and other complicated structures as well.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {766–777},
numpages = {12},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066246,
author = {Halevy, Alon Y. and Ashish, Naveen and Bitton, Dina and Carey, Michael and Draper, Denise and Pollock, Jeff and Rosenthal, Arnon and Sikka, Vishal},
title = {Enterprise Information Integration: Successes, Challenges and Controversies},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066246},
doi = {10.1145/1066157.1066246},
abstract = {The goal of EII systems is to provide uniform access to multiple data sources without having to first load them into a data warehouse. Since the late 1990's, several EII products have appeared in the marketplace and significant experience has been accumulated from fielding such systems. This collection of articles, by individuals who were involved in this industry in various ways, describes some of these experiences and points to the challenges ahead.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {778–787},
numpages = {10},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066247,
author = {Maluf, David A. and Bell, David G. and Ashish, Naveen},
title = {Lean Middleware},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066247},
doi = {10.1145/1066157.1066247},
abstract = {This paper describes an approach to achieving data integration across multiple sources in an enterprise, in a manner that does not require heavy investment in database and middleware maintenance. This "lean" approach to integration leads to cost-effectiveness and scalability of data integration in the enterprise.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {788–791},
numpages = {4},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066249,
author = {Bernstein, Philip A.},
title = {The Many Roles of Meta Data in Data Integration},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066249},
doi = {10.1145/1066157.1066249},
abstract = {This paper is a short introduction to an industrial session on the use of meta data to address data integration problems in large enterprises. The main topics are data discovery, version and configuration management, and mapping development.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {792},
numpages = {1},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066250,
author = {Hauch, Randall and Miller, Alex and Cardwell, Rob},
title = {Information Intelligence: Metadata for Information Discovery, Access, and Integration},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066250},
doi = {10.1145/1066157.1066250},
abstract = {Integrating enterprise information requires an accurate, precise and complete understanding of the disparate data sources, the needs of the information consumers, and how these map to the semantic business concepts of the enterprise. We describe how MetaMatrix captures and manages this metadata through the use of the OMG's MOF architecture and multiple domain-specific modeling languages, and how this semantic and syntactic metadata is then used for a variety of purposes, including accessing data in real-time from the underlying enterprise systems, integrating it, and returning it as information expected by consumers.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {793–798},
numpages = {6},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066251,
author = {Friedrich, John R},
title = {Meta-Data Version and Configuration Management in Multi-Vendor Environments},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066251},
doi = {10.1145/1066157.1066251},
abstract = {Nearly all components that comprise modern information technology, such as Computer Aided Software Engineering (CASE) tools, Enterprise Application Integration (EAI) environments, Extract/Transform/Load (ETL) engines. Warehouses, EII, and Business Intelligence (BI), contain a great deal of meta-data, which often drive much of the tool's functionality. These metadata are distributed and duplicated, are often times actively interacting with the tools as they process data, and are generally represented in a variety of methodologies. Meta-data exchange and reuse is now becoming commonplace. This article is based upon the real challenges found in these complicated meta-data environments, and identifies the often overlooked distinctions and importance of meta-data version and configuration management (CM), including the extensive use of automated meta-data comparison, mapping comparison, mapping generation and mapping update functions, which comprise a complete meta-data CM environment. Also addressed is the reality that most repositories are not up to the task of true version and configuration management, and thus true impact and lineage analysis, as their emphasis has been on the development a single enterprise architecture and the concept of "a single version of the truth."},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {799–804},
numpages = {6},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066252,
author = {Haas, Laura M. and Hern\'{a}ndez, Mauricio A. and Ho, Howard and Popa, Lucian and Roth, Mary},
title = {Clio Grows up: From Research Prototype to Industrial Tool},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066252},
doi = {10.1145/1066157.1066252},
abstract = {Clio, the IBM Research system for expressing declarative schema mappings, has progressed in the past few years from a research prototype into a technology that is behind some of IBM's mapping technology. Clio provides a declarative way of specifying schema mappings between either XML or relational schemas. Mappings are compiled into an abstract query graph representation that captures the transformation semantics of the mappings. The query graph can then be serialized into different query languages, depending on the kind of schemas and systems involved in the mapping. Clio currently produces XQuery, XSLT, SQL, and SQL/XML queries. In this paper, we revisit the architecture and algorithms behind Clio. We then discuss some implementation issues, optimizations needed for scalability, and general lessons learned in the road towards creating an industrial-strength tool.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {805–810},
numpages = {6},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066254,
author = {Choy, David M.},
title = {Integration of Structured and Unstructured Data in IBM Content Manager},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066254},
doi = {10.1145/1066157.1066254},
abstract = {Integration of structured and unstructured data goes much deeper than supporting large objects in a database. Through an architecture overview of the IBM Content Manager, this paper examines some of the requirements, challenges, and solutions in managing a large volume of content and in support of a wide range of content applications. The discussion touches upon system architecture, data model, and access control.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {811–816},
numpages = {6},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066255,
author = {Bosworth, Adam},
title = {Database Issues for the 21st Century},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066255},
doi = {10.1145/1066157.1066255},
abstract = {The Web has democratized and revolutionized computing in the last 10 years. Acting as a universal communications mechanism, it has let anyone talk to anyone (for example via email, IM and voip), anyone talk to any application (the Web), anyone talk to some very limited forms of information (Blogs/RSS), any application talk to anyone (Spam), and any application talk to any other application (Web Services).},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {817},
numpages = {1},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066256,
author = {Sedlar, Eric},
title = {Managing Structure in Bits &amp; Pieces: The Killer Use Case for XML},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066256},
doi = {10.1145/1066157.1066256},
abstract = {This paper asserts that for databases to manage a significantly greater percentage of the world's data, managing structural information must get significantly easier. XML technologies provide a widely accepted basis for significant advances in managing data structure. Topics include schema design, evolution, and versioning; managing related applications; and application architecture.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {818–821},
numpages = {4},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066258,
author = {Zaman, Kazi A. and Schneider, Donovan A.},
title = {Modeling and Querying Multidimensional Data Sources in Siebel Analytics: A Federated Relational System},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066258},
doi = {10.1145/1066157.1066258},
abstract = {Large organizations have a multitude of data sources across the enterprise and want to obtain business value from all of them. While the majority of these data sources may be consolidated in an enterprise data warehouse, many business units have their own data marts where analysis is carried out against data stored in multidimensional data structures. It is often critical to pose queries which span both these sources. This is a challenge since these sources have differing models and query languages (SQL vs MDX). The Siebel Analytics Server enables this requirement to be fulfilled. In this paper, we describe how the multidimensional metadata is modeled relationally within Siebel Analytics, efficient SQL to MDX translation algorithms and the conversion protocols required to convert a multidimensional result into a relational rowset.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {822–827},
numpages = {6},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066259,
author = {Liu, Zhen Hua and Krishnaprasad, Muralidhar and Arora, Vikas},
title = {Native Xquery Processing in Oracle XMLDB},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066259},
doi = {10.1145/1066157.1066259},
abstract = {With XQuery becoming the standard language for querying XML, and the relational SQL platform being recognized as an important platform to store and process XML, the SQL/XML standard is integrating XML query capability into the SQL system by introducing new SQL functions and constructs such as XMLQuery() and XMLTable. This paper discusses the Oracle XMLDB XQuery architecture for supporting XQuery in the Oracle ORDBMS kernel which has the XQuery processing tightly integrated with the SQL/XML engine using native XQuery compilation, optimization and execution techniques.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {828–833},
numpages = {6},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066260,
author = {Ordonez, Carlos},
title = {Optimizing Recursive Queries in SQL},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066260},
doi = {10.1145/1066157.1066260},
abstract = {Recursion represents an important addition to the SQL language. This work focuses on the optimization of linear recursive queries in SQL. To provide an abstract framework for discussion, we focus on computing the transitive closure of a graph. Three optimizations are studied: (1) Early evaluation of row selection conditions. (2) Eliminating duplicate rows in intermediate tables. (3) Defining an enhanced index to accelerate join computation. Optimizations are evaluated on two types of graphs: binary trees and sparse graphs. Binary trees represent an ideal graph with no cycles and a linear number of edges. Sparse graphs represent an average case with some cycles and a linear number of edges. In general, the proposed optimizations produce a significant reduction in the evaluation time of recursive queries.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {834–839},
numpages = {6},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066262,
author = {Cuomo, Gennaro (Jerry)},
title = {IBM SOA "on the Edge"},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066262},
doi = {10.1145/1066157.1066262},
abstract = {This paper introduces the concept of an SOA edge server and a set of complementary design patterns designed to optimize performance, improve manageability and enable customers to cost effectively deploy SOA applications into complex, mission-critical, high-volume distributed environments.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {840–843},
numpages = {4},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066263,
author = {Patrick, Paul},
title = {Impact of SOA on Enterprise Information Architectures},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066263},
doi = {10.1145/1066157.1066263},
abstract = {Enterprises are looking to find new and cost effective means to leverage existing investments in IT infrastructure and incorporate new capabilities in order to improve business productivity. As a means to improve the integration of applications hosted both internal and external to the enterprise, enterprises are turning to Service Oriented Architectures.In this paper, we describe some of the major aspects associated with the introduction of a Service Oriented Architecture and the impact that it can have on an enterprise's information architecture. We outline the concept of exposing data sources as services and discuss the critical integration aspects that need to be addressed including data access, data transformation, and integration into an over arching enterprise security scheme. The paper suggests alternatives, utilizing a Service Oriented Architecture approach, to promote flexible, extensible, and evolvable information architectures.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {844–848},
numpages = {5},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@dataset{10.1145/review-1066157.1066263_R39910,
author = {Burns, Robert C.},
title = {Review ID:R39910 for DOI: 10.1145/1066157.1066263},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1066157.1066263_R39910}
}

@inproceedings{10.1145/1066157.1066264,
author = {Sikka, Vishal},
title = {Data and Metadata Management in Service-Oriented Architectures: Some Open Challenges},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066264},
doi = {10.1145/1066157.1066264},
abstract = {Over the last decade, the role of information technology in enterprises has been transforming from one of providing automation services to one of enabling business innovation. IT's charter is now closely aligned with the business goals and processes in a company and to support this charter, enterprise application architecture is shifting towards what's commonly referred to as a services-oriented architecture (SOA), or an enterprise-services architecture [1, 2]. In this talk, I want to discuss the shift to this new architecture and some ramifications of this, in particular some challenges posed by this shift for our research community to pursue.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {849–850},
numpages = {2},
keywords = {model-driven development, metadata management, service-oriented architecture, data management},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066265,
author = {Brambilla, Marco and Ceri, Stefano and Fraternali, Piero and Acerbis, Roberto and Bongio, Aldo},
title = {Model-Driven Design of Service-Enabled Web Applications},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066265},
doi = {10.1145/1066157.1066265},
abstract = {Significant efforts are currently invested in application integration to enable the interaction and composition of business processes of different companies, yielding complex; multi-party processes. Web service standards, based on WSDL, have been adopted as a process-to-process communication paradigm. This paper presents an industrial experience in integrating data-intensive and process-intensive Web applications through Web services. Design of sites and of Web services interaction exploits modern Web engineering methods, including conceptual modeling, model verification, visual data marshalling and automatic code generation. In particular, the applied method is based on a declarative model for specifying data-intensive Web applications that enact complex interactions, driven by the user, with remote processes implemented as services. We describe the internal architecture of the CASE tool that has been used, and give an overview of three industrial applications developed with the described approach.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {851–856},
numpages = {6},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066267,
author = {Campbell, David},
title = {Service Oriented Database Architecture: APP Server-Lite?},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066267},
doi = {10.1145/1066157.1066267},
abstract = {As the capabilities and service levels of enterprise database systems have evolved, they have collided with incumbent technologies such as TP-Monitors or Message Oriented Middleware (MOM). We believe this trend will continue and have architected the upcoming release of SQL Server to advance this technology trend. This paper describes the Service Oriented Database Architecture (SODA) developed for the Microsoft SQL Server DBMS. First, it motivates the need for building Service Oriented Architecture (SOA) features directly into a database engine. Second, it describes a set of features in SQL Server that have been designed for SOA use. Finally, it concludes with some thoughts on how SODA can enable multiple service deployment topologies.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {857–862},
numpages = {6},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066268,
author = {Thome, Bob and Gawlick, Dieter and Pratt, Maria},
title = {Event Processing with an Oracle Database},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066268},
doi = {10.1145/1066157.1066268},
abstract = {In this paper, we examine how active database technology developed over the past few years has been put to use to solve real world problems. We note how the technology had to be extended beyond the feature set originally identified in early research to meet these real-world needs, and discuss why this technology was best suited to solving these problems.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {863–867},
numpages = {5},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066269,
author = {Gallagher, Bill and Jacobs, Dean and Langen, Anno},
title = {A High-Performance, Transactional Filestore for Application Servers},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066269},
doi = {10.1145/1066157.1066269},
abstract = {There is a class of data, including messages and business workflow state, for which conventional monolithic databases are less than ideal. Performance and scalability of Application Server systems can be dramatically increased by distributing such data across transactional filestores, each of which is bound to a server instance in a cluster. This paper describes a high-performance, transactional filestore that has been developed for the BEA WebLogic Application ServerTM and benchmarks it against a database. The filestore uses a novel, platform-independent disk scheduling algorithm to minimize the latency of small, synchronous writes to disk.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {868–872},
numpages = {5},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066271,
author = {Luo, Chang and Thakkar, Hetal and Wang, Haixun and Zaniolo, Carlo},
title = {A Native Extension of SQL for Mining Data Streams},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066271},
doi = {10.1145/1066157.1066271},
abstract = {ESL1 enables users to develop stream applications in an SQL-like high level language that provides the ease-of-use of a declarative language, which is Turing complete in terms of expressive power [11].},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {873–875},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066272,
author = {Koudas, Nick and Marathe, Amit and Srivastava, Divesh},
title = {SPIDER: Flexible Matching in Databases},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066272},
doi = {10.1145/1066157.1066272},
abstract = {We present a prototype system, SPIDER, developed at AT&amp;T Labs-Research, which supports flexible string attribute value matching in large databases. We discuss the design principles on which SPIDER is based, describe the basic techniques encompassed by the tool and provide a description of the demo.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {876–878},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066273,
author = {Wang, Wei and Wang, Chen and Zhu, Yongtai and Shi, Baile and Pei, Jian and Yan, Xifeng and Han, Jiawei},
title = {GraphMiner: A Structural Pattern-Mining System for Large Disk-Based Graph Databases and Its Applications},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066273},
doi = {10.1145/1066157.1066273},
abstract = {Mining frequent structural patterns from graph databases is an important research problem with broad applications. Recently, we developed an effective index structure, ADI, and efficient algorithms for mining frequent patterns from large, disk-based graph databases [5], as well as constraint-based mining techniques. The techniques have been integrated into a research prototype system--- GraphMiner. In this paper, we describe a demo of GraphMiner which showcases the technical details of the index structure and the mining algorithms including their efficient implementation, the mining performance and the comparison with some state-of-the-art methods, the constraint-based graph-pattern mining techniques and the procedure of constrained graph mining, as well as mining real data sets in novel applications.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {879–881},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066274,
author = {Ahmad, Yanif and Berg, Bradley and Cetintemel, Uundefinedur and Humphrey, Mark and Hwang, Jeong-Hyon and Jhingran, Anjali and Maskey, Anurag and Papaemmanouil, Olga and Rasin, Alexander and Tatbul, Nesime and Xing, Wenjuan and Xing, Ying and Zdonik, Stan},
title = {Distributed Operation in the Borealis Stream Processing Engine},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066274},
doi = {10.1145/1066157.1066274},
abstract = {Borealis is a distributed stream processing engine that is being developed at Brandeis University, Brown University, and MIT. Borealis inherits core stream processing functionality from Aurora and inter-node communication functionality from Medusa.We propose to demonstrate some of the key aspects of distributed operation in Borealis, using a multi-player network game as the underlying application. The demonstration will illustrate the dynamic resource management, query optimization and high availability mechanisms employed by Borealis, using visual performance-monitoring tools as well as the gaming experience.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {882–884},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066275,
author = {Rizvi, Shariq and Jeffery, Shawn R. and Krishnamurthy, Sailesh and Franklin, Michael J. and Burkhart, Nathan and Edakkunni, Anil and Liang, Linus},
title = {Events on the Edge},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066275},
doi = {10.1145/1066157.1066275},
abstract = {The emergence of large-scale receptor-based systems has enabled applications to execute complex business logic over data generated from monitoring the physical world. An important functionality required by these applications is the detection and response to complex events, often in real-time. Bridging the gap between low-level receptor technology and such high-level needs of applications remains a significant challenge.We demonstrate our solution to this problem in the context of HiFi, a system we are building to solve the data management problems of large-scale receptor-based systems. Specifically, we show how HiFi generates simple events out of receptor data at its edges and provides high-functionality complex event processing mechanisms for sophisticated event detection using a real-world library scenario.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {885–887},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066276,
author = {Bouganim, Luc and Cremarenco, Cosmin and Ngoc, Fran\c{c}ois Dang and Dieu, Nicolas and Pucheral, Philippe},
title = {Safe Data Sharing and Data Dissemination on Smart Devices},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066276},
doi = {10.1145/1066157.1066276},
abstract = {The erosion of trust put in traditional database servers and in Database Service Providers (DSP), the growing interest for different forms of data dissemination and the concern for protecting children from suspicious Internet content are different factors that lead to move the access control from servers to clients. Due to the intrinsic untrustworthiness of client devices, client-based access control solutions rely on data encryption. The data are kept encrypted at the server and a client is granted access to subparts of them according to the decryption keys in its possession. Several variations of this basic model have been proposed (e.g., [1, 6]) but they have in common to minimize the trust required on the client at the cost of a static way of sharing data. Indeed, whatever the granularity of sharing, the dataset is split in subsets reflecting a current sharing situation, each encrypted with a different key. Once the dataset is encrypted, changes in the access control rules definition may impact the subset boundaries, hence incurring a partial re-encryption of the dataset and a potential redistribution of keys.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {888–890},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066277,
author = {Boulos, Jihad and Dalvi, Nilesh and Mandhani, Bhushan and Mathur, Shobhit and Re, Chris and Suciu, Dan},
title = {MYSTIQ: A System for Finding More Answers by Using Probabilities},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066277},
doi = {10.1145/1066157.1066277},
abstract = {MystiQ is a system that uses probabilistic query semantics [3] to find answers in large numbers of data sources of less than perfect quality. There are many reasons why the data originating from many different sources may be of poor quality, and therefore difficult to query: the same data item may have different representation in different sources; the schema alignments needed by a query system are imperfect and noisy; different sources may contain contradictory information, and, in particular, their combined data may violate some global integrity constraints; fuzzy matches between objects from different sources may return false positives or negatives. Even in such environment, users some-times want to ask complex, structurally rich queries, using query constructs typically found in SQL queries: joins, subqueries, existential/universal quantifiers, aggregate and group-by queries: for example scientists may use such queries to query multiple scientific data sources, or a law enforcement agency may use it in order to find rare associations from multiple data sources. If standard query semantics were applied to such queries, all but the most trivial queries will return an empty answer.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {891–893},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066278,
author = {Jahangiri, Mehrdad and Shahabi, Cyrus},
title = {ProDA: A Suite of Web-Services for Progressive Data Analysis},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066278},
doi = {10.1145/1066157.1066278},
abstract = {Online Scientific Applications (OSA) require statistical analysis of large multidimensional datasets. Towards this end, we have designed and developed a data storage and retrieval system, called ProDA, which deploys wavelet transform and provides fast approximate answers with progressively increasing accuracy in support of the OSA queries. ProDA employs a standard web-service infrastructure to enable remote users to interact with their data. These web-services enable wavelet transformation of large multidimensional datasets as well as inserting, updating, and exact, approximate and progressive querying of these datasets in the wavelet domain. We demonstrate the features of ProDA on a massive atmospheric dataset provided to us by NASA/JPL.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {894–896},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066280,
author = {Iacob, Ionut E. and Dekhtyar, Alex},
title = {A Framework for Processing Complex Document-Centric XML with Overlapping Structures},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066280},
doi = {10.1145/1066157.1066280},
abstract = {Management of multihierarchical XML encodings has attracted attention of a number of researchers both in databases [8] and in humanities[10]. Encoding documents using multiple hierarchies can yield overlapping markup. Previously proposed solutions to management of document-centric XML with overlapping markup rely on the XML expertise of humans and their ability to maintain correct schemas for complex markup languages.We demonstrate a unified solution for management of complex, multihierarchical document-centric XML. Our framework includes software for storing, parsing, in-memory access, editing and querying, multihierarchical XML documents with conflicting structures.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {897–899},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066281,
author = {Li, Yunyao and Yang, Huahai and Jagadish, H. V.},
title = {NaLIX: An Interactive Natural Language Interface for Querying XML},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066281},
doi = {10.1145/1066157.1066281},
abstract = {Database query languages can be intimidating to the non-expert, leading to the immense recent popularity for keyword based search in spite of its significant limitations. The holy grail has been the development of a natural language query interface. We present NaLIX, a generic interactive natural language query interface to an XML database. Our system can accept an arbitrary English language sentence as query input, which can include aggregation, nesting, and value joins, among other things. This query is translated, potentially after reformulation, into an XQuery expression that can be evaluated against an XML database. The translation is done through mapping grammatical proximity of natural language parsed tokens to proximity of corresponding elements in the result XML. In this demonstration, we show that NaLIX, while far from being able to pass the Turing test, is perfectly usable in practice, and able to handle even quite complex queries in a variety of application domains. In addition, we also demonstrate how carefully designed features in NaLIX facilitate the interactive query process and improve the usability of the interface.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {900–902},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066282,
author = {Braga, Daniele and Campi, Alessandro and Ceri, Stefano and Raffio, Alessandro},
title = {XQBE: A Visual Environment for Learning XML Query Languages},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066282},
doi = {10.1145/1066157.1066282},
abstract = {XQBE (XQuery By Example) is a visual XML query language which, coherently with the hierarchical XML data model, uses tree-shaped structures to express queries and transformations over XML documents. These structures are annotated to express selection predicates; explicit bindings between the nodes of such structures visualize the input/output mappings.XQuery and XSLT, the standard query and transformation languages for XML, happen to be too complex for most occasional or unskilled users who might need to specify queries, schema mappings, or document transformations, if they are only aware of the basics of the XML data model. The implementation of XQBE allows to generate the XQuery and XSLT translations of the visual queries, assisting the user in several aspects of the interaction (e.g. providing interactive access to schema information); therefore, XQBE provides an integrated environment where users can edit the visual queries and their textual counterparts, executing them on several engines. Alternating among different representations of the same query is valuable for training beginners, as we have experienced in our database courses.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {903–905},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066283,
author = {Aumueller, David and Do, Hong-Hai and Massmann, Sabine and Rahm, Erhard},
title = {Schema and Ontology Matching with COMA++},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066283},
doi = {10.1145/1066157.1066283},
abstract = {We demonstrate the schema and ontology matching tool COMA++. It extends our previous prototype COMA utilizing a composite approach to combine different match algorithms [3]. COMA++ implements significant improvements and offers a comprehensive infrastructure to solve large real-world match problems. It comes with a graphical interface enabling a variety of user interactions. Using a generic data representation, COMA++ uniformly supports schemas and ontologies, e.g. the powerful standard languages W3C XML Schema and OWL. COMA++ includes new approaches for ontology matching, in particular the utilization of shared taxonomies. Furthermore, different match strategies can be applied including various forms of reusing previously determined match results and a so-called fragment-based match approach which decomposes a large match problem into smaller problems. Finally, COMA++ cannot only be used to solve match problems but also to comparatively evaluate the effectiveness of different match algorithms and strategies.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {906–908},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066284,
author = {Morishima, Atsuyuki and Okawara, Toshiaki and Tanaka, Jun'ichi and Ishikawa, Ken'ichi},
title = {SMART: A Tool for Semantic-Driven Creation of Complex XML Mappings},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066284},
doi = {10.1145/1066157.1066284},
abstract = {We focus on the problem of data transformations, i.e., how to transform data to another structure to adapt it to new application requirements or given environments. Here, we define data transformation as the process of taking as input two schemas A and B and an instance of A, and producing an instance of B. Today, data transformations are required in many situations: to integrate multiple information sources, to construct and receive data for Web services, and to migrate data from legacy systems to new systems, from local databases to data warehouses. This demonstration focuses on XML transformations, since XML is the de facto standard for data exchange.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {909–911},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066285,
author = {Fletcher, George H. L. and Wyss, Catharine M.},
title = {Relational Data Mapping in MIQIS},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066285},
doi = {10.1145/1066157.1066285},
abstract = {We demonstrate a prototype of the relational data mapping module of MIQIS, a formal framework for investigating information flow in peer-to-peer database management systems. Data maps constitute effective mappings between structured data sources. These mappings are the `glue' for facilitating large scale ad-hoc information sharing between autonomous peers, and automating their discovery is one of the fundamental unsolved challenges for information interoperability and sharing. Our approach to automating data map discovery utilizes heuristic search within a space delineated by basic relational transformation operators. A novelty of our approach is that these operators include data to metadata transformations (and vice versa). This approach leverages new perspectives on the data mapping problem, and generalizes previous approaches such as token-based schema matching.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {912–914},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066286,
author = {Leone, Nicola and Greco, Gianluigi and Ianni, Giovambattista and Lio, Vincenzino and Terracina, Giorgio and Eiter, Thomas and Faber, Wolfgang and Fink, Michael and Gottlob, Georg and Rosati, Riccardo and Lembo, Domenico and Lenzerini, Maurizio and Ruzzi, Marco and Kalka, Edyta and Nowicki, Bartosz and Staniszkis, Witold},
title = {The INFOMIX System for Advanced Integration of Incomplete and Inconsistent Data},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066286},
doi = {10.1145/1066157.1066286},
abstract = {The task of an information integration system is to combine data residing at different sources, providing the user with a unified view of them, called global schema. Users formulate queries over the global schema, and the system suitably queries the sources, providing an answer to the user, who is not obliged to have any information about the sources. Recent developments in IT such as the expansion of the Internet and the World Wide Web, have made available to users a huge number of information sources, generally autonomous, heterogeneous and widely distributed: as a consequence, information integration has emerged as a crucial issue in many application domains, e.g., distributed databases, cooperative information systems, data warehousing, or on-demand computing. Recent estimates view information integration to be a $10 Billion market by 2006 [14].},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {915–917},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066287,
author = {Chaudhuri, Surajit and Ganjam, Kris and Ganti, Venky and Kapoor, Rahul and Narasayya, Vivek and Vassilakis, Theo},
title = {Data Cleaning in Microsoft SQL Server 2005},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066287},
doi = {10.1145/1066157.1066287},
abstract = {When collecting and combining data from various sources into a data warehouse, ensuring high data quality and consistency becomes a significant, often expensive, challenge. Common data quality problems include inconsistent data conventions amongst sources such as different abbreviations or synonyms; data entry errors such as spelling mistakes; missing, incomplete, outdated or otherwise incorrect attribute values. These data defects generally manifest themselves as foreign-key mismatches and approximately duplicate records, both of which make further data mining and decision support analyses either impossible or suspect. We demonstrate two new data cleansing operators, Fuzzy Lookup and Fuzzy Grouping, which address these problems in a scalable and domain-independent manner. These operators are implemented within Microsoft SQL Server 2005 Integration Services. Our demo will explain their functionality and highlight multiple real-world scenarios in which they can be used to achieve high data quality.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {918–920},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066289,
author = {Cai, Yuhan and Dong, Xin Luna and Halevy, Alon and Liu, Jing Michelle and Madhavan, Jayant},
title = {Personal Information Management with SEMEX},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066289},
doi = {10.1145/1066157.1066289},
abstract = {The explosion of information available in digital form has made search a hot research topic for the Information Management Community. While most of the research on search is focused on the WWW, individual computer users have developed their own vast collections of data on their desktops, and these collections are in critical need for good search and query tools. The problem is exacerbated by the proliferation of varied electronic devices (laptops, PDAs, cellphones) that are at our disposal, which often hold subsets or variations of our data. In fact, several recent venues have noted Personal Information Management (PIM) as an area of growing interest to the data management community [1, 8, 6]},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {921–923},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066290,
author = {Liu, Guodong and Zhang, Jingdan and Wang, Wei and McMillan, Leonard},
title = {A System for Analyzing and Indexing Human-Motion Databases},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066290},
doi = {10.1145/1066157.1066290},
abstract = {We demonstrate a data-driven approach for representing, compressing, and indexing human-motion databases. Our modeling approach is based on piecewise-linear components that are determined via a divisive clustering method. Selection of the appropriate linear model is determined automatically via a classifier using a subspace of the most significant, or principle features (markers). We show that, after offline training, our model can accurately estimate and classify human motions. We can also construct indexing structures for motion sequences according to their transition trajectories through these linear components. Our method not only provides indices for whole and/or partial motion sequences, but also serves as a compressed representation for the entire motion database. Our method also tends to be immune to temporal variations, and thus avoids the expense of time-warping.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {924–926},
numpages = {3},
keywords = {piecewise linear modeling, motion capture, motion compression, motion database indexing},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066291,
author = {He, Bin and Zhang, Zhen and Chang, Kevin Chen-Chuan},
title = {MetaQuerier: Querying Structured Web Sources on-the-Fly},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066291},
doi = {10.1145/1066157.1066291},
abstract = {Recently, we witness the rapid growth and thus the prevalence of databases on the Web. Our recent survey [2] in April 2004 estimated 450,000 online databases. On this deep Web, myriad online databases provide dynamic query-based data access through their query interfaces, instead of static URL links. As the door to the deep Web, it is essential to integrate these query interfaces for integrating the deep Web.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {927–929},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066292,
author = {Agrawal, Sanjay and Chaudhuri, Surajit and Kollar, Lubor and Marathe, Arun and Narasayya, Vivek and Syamala, Manoj},
title = {Database Tuning Advisor for Microsoft SQL Server 2005: Demo},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066292},
doi = {10.1145/1066157.1066292},
abstract = {Database Tuning Advisor (DTA) is a physical database design tool that is part of Microsoft's SQL Server 2005 relational database management system. Previously known as "Index Tuning Wizard" in SQL Server 7.0 and SQL Server 2000, DTA adds new functionality that is not available in other contemporary physical design tuning tools. Novel aspects of DTA that will be demonstrated include: (a) Ability to take into account both performance and manageability requirements of DBAs (b) Fully integrated recommendations for indexes, materialized views and horizontal partitioning (c) Transparently leverage a test server to offload tuning load from production server and (d) Easy programmability and scriptability.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {930–932},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066293,
author = {Haas, P. and Kandil, M. and Lerner, A. and Markl, V. and Popivanov, I. and Raman, V. and Zilio, D.},
title = {Automated Statistics Collection in Action},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066293},
doi = {10.1145/1066157.1066293},
abstract = {If presented with inaccurate statistics, even the most sophisticated query optimizers make mistakes. They may wrongly estimate the output cardinality of a certain operation and thus make sub-optimal plan choices based on that cardinality. Maintaining accurate statistics is hard, both because each table may need a specifically parameterized set of statistics and because statistics get outdated as the database changes. Automated Statistic Collection (ASC) is a new component in IBM DB2 UDB that, without any DBA intervention, observes and analyzes the effects of faulty statistics and, in response, it triggers actions that continuously repair the latter. In this demonstration, we will show how ASC works to alleviate the DBA from the task of maintaining fresh, accurate statistics in several challenging scenarios. ASC is able to reconfigure the statistics collection parameters (e.g, number of frequent values for a column, or correlations between certain column pairs) on a per-table basis. ASC can also detect and guard against outdated statistics caused by high updates/inserts/deletes rates in volatile, dynamic databases. We will also show how ASC works from the inside: from how cardinality mis-estimations are introduced in different kind of operators, to how this error is propagated to later operations in the plan, to how this influences plan choices inside the optimizer.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {933–935},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066294,
author = {Babu, Shivnath and Bizarro, Pedro and DeWitt, David},
title = {Proactive Re-Optimization with Rio},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066294},
doi = {10.1145/1066157.1066294},
abstract = {Traditional query optimizers rely on the accuracy of estimated statistics of intermediate subexpressions to choose good query execution plans. This design often leads to suboptimal plan choices for complex queries since errors in estimates grow exponentially in the presence of skewed and correlated data distributions. We propose to demonstrate the Rio prototype database system that uses proactive re-optimization to address the problems with traditional optimizers. Rio supports three new techniques:1. Intervals of uncertainty are considered around estimates of statistics during plan enumeration and costing2. These intervals are used to pick execution plans that are robust to deviations of actual values of statistics from estimated values, or to defer the choice of execution plan until the uncertainty in estimates can be resolved3. Statistics of intermediate subexpressions are collected quickly, accurately, and efficiently during query executionThese three features are fully functional in the current Rio prototype which is built using the Predator open-source DBMS [5]. In this proposal, we first describe the novel features of Rio, then we use an example query to illustrate the main aspects of our demonstration.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {936–938},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066295,
author = {Lomet, David and Barga, Roger and Mokbel, Mohamed F. and Shegalov, German and Wang, Rui and Zhu, Yunyue},
title = {Immortal DB: Transaction Time Support for SQL Server},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066295},
doi = {10.1145/1066157.1066295},
abstract = {Immortal DB builds transaction time database support into the SQL Server engine, not in middleware. Transaction time databases retain and provide access to prior states of a database. An update "inserts" a new record while preserving the old version. The system supports as of queries returning records current at the specified time. It also supports snapshot isolation concurrency control. Versions are stamped with the times of their updating transactions. The timestamp order agrees with transaction serialization order. Lazy timestamping propagates timestamps to all updates of a transaction after commit. All versions are kept in an integrated storage structure, with historical versions initially stored with current data. Time-splits of pages permit large histories to be maintained, and enable time based indexing. We demonstrate Immortal DB with a moving objects application that tracks cars in the Seattle area.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {939–941},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066296,
author = {Chiticariu, Laura and Tan, Wang-Chiew and Vijayvargiya, Gaurav},
title = {DBNotes: A Post-It System for Relational Databases Based on Provenance},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066296},
doi = {10.1145/1066157.1066296},
abstract = {We demonstrate DBNotes, a Post-It note system for relational databases where every piece of data may be associated with zero or more notes (or annotations). These annotations are transparently propagated along as data is being transformed. The method by which annotations are propagated is based on provenance (aka lineage): the annotations associated with a piece of data d in the result of a transformation consist of the annotations associated with each piece of data in the source where d is copied from. One immediate application of this system is to use annotations to systematically trace the provenance and flow of data. If every piece of source data is attached with an annotation that describes its address (i.e., origins), then the annotations of a piece of data in the result of a transformation describe its provenance. Hence, one can easily determine the provenance of data through a sequence of transformation steps simply by examining the annotations. Annotations can also be used to store additional information about data. Since a database schema is often proprietary, the ability to insert new information about data without having to change the underlying schema is a useful feature. For example, an error report could be attached to an erroneous piece of data, and this error report will be propagated to other databases along transformations, thus notifying other users of the error. Overall, the annotations on the result of a transformation can also provide an estimate on the quality of the resulting database.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {942–944},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066298,
author = {Rys, Michael and Chamberlin, Don and Florescu, Daniela},
title = {XML and Relational Database Management Systems: The inside Story},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066298},
doi = {10.1145/1066157.1066298},
abstract = {As XML has evolved from a document markup language to a widely-used format for exchange of structured and semistructured data, managing large amounts of XML data has become increasingly important. A number of companies, including both established database vendors and startups, have recently announced new XML database systems or new XML functionality integrated into existing database systems. This tutorial will provide an insight into how XML functionality fits into relational database management systems as seen by three major relational vendors: IBM, Microsoft and Oracle.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {945–947},
numpages = {3},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066299,
author = {Beyer, Kevin and \"{O}zcan, Fatma and Saiprasad, Sundar and Van der Linden, Bert},
title = {DB2/XML: Designing for Evolution},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066299},
doi = {10.1145/1066157.1066299},
abstract = {DB2 provides native XML storage, indexing, navigation and query processing through both SQL/XML and XQuery using the XML data type introduced by SQL/XML. In this tutorial we focus on DB2's XML support for schema evolution, especially DB2's schema repository and document-level validation.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {948–952},
numpages = {5},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066300,
author = {Murthy, Ravi and Liu, Zhen Hua and Krishnaprasad, Muralidhar and Chandrasekar, Sivasankaran and Tran, Anh-Tuan and Sedlar, Eric and Florescu, Daniela and Kotsovolos, Susan and Agarwal, Nipun and Arora, Vikas and Krishnamurthy, Viswanathan},
title = {Towards an Enterprise XML Architecture},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066300},
doi = {10.1145/1066157.1066300},
abstract = {XML is being increasingly used in diverse domains ranging from data and application integration to content management. Oracle provides an enterprise wide platform for managing all types of XML content. Within the Oracle database and the application server, the XML content can be efficiently stored using a variety of storage and indexing methods and it can be processed using multiple standard languages within different programmatic environments.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {953–957},
numpages = {5},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066301,
author = {Rys, Michael},
title = {XML and Relational Database Management Systems: Inside Microsoft® SQL Server™ 2005},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066301},
doi = {10.1145/1066157.1066301},
abstract = {Microsoft® SQL Server™ has a long history of XML support dating back to 1999. While first concentrating on enabling the transport of relational data via XML with the SQL Server 2000 release, SQL Server 2005 now additionally provides native XML storage and query support. This part of the tutorial will provide an insight into how SQL Server 2005 fits XML functionality into its core relational database management framework.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {958–962},
numpages = {5},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066303,
author = {Suciu, Dan and Dalvi, Nilesh},
title = {Foundations of Probabilistic Answers to Queries},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066303},
doi = {10.1145/1066157.1066303},
abstract = {Overview: Probabilistic query answering is a fundamental set of techniques that underlies several, very recent database applications: exploratory queries in databases, novel IR-style approaches to data integration, querying information extracted from the Web, queries over sensor networks, data acquisition, querying data sources that violate integrity constraints, controlling information disclosure in data exchange, and reasoning about privacy breaches in data mining. This is a surprisingly diverse range of applications, most of which have either emerged recently, or have seen a recent increased interest, and which all share a common fundamental abstraction: that an item being in the answer to a query is no longer a boolean value, but a probabilistic event. It this authors belief that this is a new paradigm in query answering, whose foundations lie in random graphs, and 0/1-laws in finite model theory. The results from these fields, and their relevance to the probabilistic query answering method, are very little known in the database research community, and the theoretical research papers or books that describe them are not very popular in the systems database research community.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {963},
numpages = {1},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066305,
author = {Chaudhuri, Surajit and Weikum, Gerhard},
title = {Foundations of Automated Database Tuning},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066305},
doi = {10.1145/1066157.1066305},
abstract = {1. The Challenge of Total Cost of-Ownership Our society is more dependent on information systems than ever before. However, managing the information systems infrastructure in a cost-effective manner is a growing challenge. The total cost of ownership (TCO) of information technology is increasingly dominated by people costs. In fact, mistakes in operations and administration of information systems are the single most reasons for system outage and unacceptable performance. For information systems to provide value to their customers, we must reduce the complexity associated with their deployment and usage.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {964–965},
numpages = {2},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066307,
author = {Murphy, Robert F. and Faloutsos, Christos},
title = {Research Issues in Protein Location Image Databases},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066307},
doi = {10.1145/1066157.1066307},
abstract = {Which proteins have similar locations within cells? How many distinct location patters do cells display? How do we answer these questions quickly, from a large collection of microscope images such as in on-line journals?},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {966–967},
numpages = {2},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

@inproceedings{10.1145/1066157.1066309,
author = {Shasha, Dennis},
title = {Computing for Biologists: Lessons from Some Successful Case Studies},
year = {2005},
isbn = {1595930604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1066157.1066309},
doi = {10.1145/1066157.1066309},
abstract = {My presentation will be online at the address http://cs.nyu.edu/cs/faculty/shasha/papers/sigmodtut05.ppt in addition to at the SIGMOD site. The presentation discusses computational techniques that have helped biologists, including combinatorial design to support a disciplined experimental design, visualization techniques to display the interaction among multiple inputs, and the discovery of gene function through the search through related species, and others.In this writeup, I confine myself to informal remarks describing both social and technical lessons I have learned while working with biologists. I intersperse these comments with references to relevant papers when appropriate.The tutorial is meant to appeal to researchers and practitioners in databases, data mining, and combinatorial algorithms as well as to natural scientists, especially biologists.},
booktitle = {Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data},
pages = {968–969},
numpages = {2},
location = {Baltimore, Maryland},
series = {SIGMOD '05}
}

