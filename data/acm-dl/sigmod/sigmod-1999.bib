@inproceedings{10.1145/304182.304183,
author = {Mamoulis, Nikos and Papadias, Dimitris},
title = {Integration of Spatial Join Algorithms for Processing Multiple Inputs},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304183},
doi = {10.1145/304182.304183},
abstract = {Several techniques that compute the join between two spatial datasets have been proposed during the last decade. Among these methods, some consider existing indices for the joined inputs, while others treat datasets with no index, providing solutions for the case where at least one input comes as an intermediate result of another database operator. In this paper we analyze previous work on spatial joins and propose a novel algorithm, called slot index spatial join (SISJ), that efficiently computes the spatial join between two inputs, only one of which is indexed by an R-tree. Going one step further, we show how SISJ and other spatial join algorithms can be implemented as operators in a database environment that joins more than two spatial datasets. We study the differences between relational and spatial multiway joins, and propose a dynamic programming algorithm that optimizes the execution of complex spatial queries.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {1–12},
numpages = {12},
keywords = {spatial joins, spatial query processing, query optimization},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304183,
author = {Mamoulis, Nikos and Papadias, Dimitris},
title = {Integration of Spatial Join Algorithms for Processing Multiple Inputs},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304183},
doi = {10.1145/304181.304183},
abstract = {Several techniques that compute the join between two spatial datasets have been proposed during the last decade. Among these methods, some consider existing indices for the joined inputs, while others treat datasets with no index, providing solutions for the case where at least one input comes as an intermediate result of another database operator. In this paper we analyze previous work on spatial joins and propose a novel algorithm, called slot index spatial join (SISJ), that efficiently computes the spatial join between two inputs, only one of which is indexed by an R-tree. Going one step further, we show how SISJ and other spatial join algorithms can be implemented as operators in a database environment that joins more than two spatial datasets. We study the differences between relational and spatial multiway joins, and propose a dynamic programming algorithm that optimizes the execution of complex spatial queries.},
journal = {SIGMOD Rec.},
month = jun,
pages = {1–12},
numpages = {12},
keywords = {spatial query processing, query optimization, spatial joins}
}

@inproceedings{10.1145/304182.304184,
author = {Acharya, Swarup and Poosala, Viswanath and Ramaswamy, Sridhar},
title = {Selectivity Estimation in Spatial Databases},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304184},
doi = {10.1145/304182.304184},
abstract = {Selectivity estimation of queries is an important and well-studied problem in relational database systems. In this paper, we examine selectivity estimation in the context of Geographic Information Systems, which manage spatial data such as points, lines, poly-lines and polygons. In particular, we focus on point and range queries over two-dimensional rectangular data. We propose several techniques based on using spatial indices, histograms, binary space partitionings (BSPs), and the novel notion of spatial skew. Our techniques carefully partition the input rectangles into subsets and approximate each partition accurately. We present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques. We evaluate them using synthetic as well as real-life TIGER datasets. Based on our experiments, we identify a BSP based partitioning that we call Min-Skew which consistently provides the most accurate selectivity estimates for spatial queries. The Min-Skew partitioning can be constructed efficiently, occupies very little space, and provides accurate selectivity estimates over a broad range of spatial queries.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {13–24},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304184,
author = {Acharya, Swarup and Poosala, Viswanath and Ramaswamy, Sridhar},
title = {Selectivity Estimation in Spatial Databases},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304184},
doi = {10.1145/304181.304184},
abstract = {Selectivity estimation of queries is an important and well-studied problem in relational database systems. In this paper, we examine selectivity estimation in the context of Geographic Information Systems, which manage spatial data such as points, lines, poly-lines and polygons. In particular, we focus on point and range queries over two-dimensional rectangular data. We propose several techniques based on using spatial indices, histograms, binary space partitionings (BSPs), and the novel notion of spatial skew. Our techniques carefully partition the input rectangles into subsets and approximate each partition accurately. We present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques. We evaluate them using synthetic as well as real-life TIGER datasets. Based on our experiments, we identify a BSP based partitioning that we call Min-Skew which consistently provides the most accurate selectivity estimates for spatial queries. The Min-Skew partitioning can be constructed efficiently, occupies very little space, and provides accurate selectivity estimates over a broad range of spatial queries.},
journal = {SIGMOD Rec.},
month = jun,
pages = {13–24},
numpages = {12}
}

@inproceedings{10.1145/304182.304185,
author = {Chakrabarti, Kaushik and Mehrotra, Sharad},
title = {Efficient Concurrency Control in Multidimensional Access Methods},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304185},
doi = {10.1145/304182.304185},
abstract = {The importance of multidimensional index structures to numerous emerging database applications is well established. However, before these index structures can be supported as access methods (AMs) in a “commercial-strength” database management system (DBMS), efficient techniques to provide transactional access to data via the index structure must be developed. Concurrent accesses to data via index structures introduce the problem of protecting ranges specified in the retrieval from phantom insertions and deletions (the phantom problem). This paper presents a dynamic granular locking approach to phantom protection in Generalized Search Trees(GiSTs), an index structure supporting an extensible set of queries and data types. The granular locking technique offers a high degree of concurrency and has a low lock overhead. Our experiments show that the granular locking technique (1) scales well under various system loads and (2) similar to the B-tree case, provides a significantly more efficient implementation compared to predicate locking for multidimensional AMs as well. Since a wide variety of multidimensional index structures can be implemented using GiST, the developed algorithms provide a general solution to concurrency control in multidimensional AMs. To the best of our knowledge, this paper provides the first such solution based on granular locking.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {25–36},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304185,
author = {Chakrabarti, Kaushik and Mehrotra, Sharad},
title = {Efficient Concurrency Control in Multidimensional Access Methods},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304185},
doi = {10.1145/304181.304185},
abstract = {The importance of multidimensional index structures to numerous emerging database applications is well established. However, before these index structures can be supported as access methods (AMs) in a “commercial-strength” database management system (DBMS), efficient techniques to provide transactional access to data via the index structure must be developed. Concurrent accesses to data via index structures introduce the problem of protecting ranges specified in the retrieval from phantom insertions and deletions (the phantom problem). This paper presents a dynamic granular locking approach to phantom protection in Generalized Search Trees(GiSTs), an index structure supporting an extensible set of queries and data types. The granular locking technique offers a high degree of concurrency and has a low lock overhead. Our experiments show that the granular locking technique (1) scales well under various system loads and (2) similar to the B-tree case, provides a significantly more efficient implementation compared to predicate locking for multidimensional AMs as well. Since a wide variety of multidimensional index structures can be implemented using GiST, the developed algorithms provide a general solution to concurrency control in multidimensional AMs. To the best of our knowledge, this paper provides the first such solution based on granular locking.},
journal = {SIGMOD Rec.},
month = jun,
pages = {25–36},
numpages = {12}
}

@inproceedings{10.1145/304182.304186,
author = {Jagadish, H. V. and Lakshmanan, Laks V. S. and Srivastava, Divesh},
title = {Snakes and Sandwiches: Optimal Clustering Strategies for a Data Warehouse},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304186},
doi = {10.1145/304182.304186},
abstract = {Physical layout of data is a crucial determinant of performance in a data warehouse. The optimal clustering of data on disk, for minimizing expected I/O, depends on the query workload. In practice, we often have a reasonable sense of the likelihood of different classes of queries, e.g., 40% of the queries concern calls made from some specific telephone number in some month. In this paper, we address the problem of finding an optimal clustering of records of a fact table on disk, given an expected workload in the form of a probability distribution over query classes.Attributes in a data warehouse fact table typically have hierarchies defined on them (by means of auxiliary dimension tables). The product of the dimensional hierarchy levels forms a lattice and leads to a natural notion of query classes. Optimal clustering in this context is a combinatorially explosive problem with a huge search space (doubly exponential in number of hierarchy levels). We identify an important subclass of clustering strategies called lattice paths, and present a dynamic programming algorithm for finding the optimal lattice path clustering, in time linear in the lattice size. We additionally propose a technique called snaking, which when applied to a lattice path, always reduces its cost. For a representative class of star schemas, we show that for every workload, there is a snaked lattice path which is globally optimal. Further, we prove that the clustering obtained by applying snaking to the optimal lattice path is never much worse than the globally optimal snaked lattice path clustering. We complement our analyses and validate the practical utility of our techniques with experiments using TPC-D benchmark data.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {37–48},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304186,
author = {Jagadish, H. V. and Lakshmanan, Laks V. S. and Srivastava, Divesh},
title = {Snakes and Sandwiches: Optimal Clustering Strategies for a Data Warehouse},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304186},
doi = {10.1145/304181.304186},
abstract = {Physical layout of data is a crucial determinant of performance in a data warehouse. The optimal clustering of data on disk, for minimizing expected I/O, depends on the query workload. In practice, we often have a reasonable sense of the likelihood of different classes of queries, e.g., 40% of the queries concern calls made from some specific telephone number in some month. In this paper, we address the problem of finding an optimal clustering of records of a fact table on disk, given an expected workload in the form of a probability distribution over query classes.Attributes in a data warehouse fact table typically have hierarchies defined on them (by means of auxiliary dimension tables). The product of the dimensional hierarchy levels forms a lattice and leads to a natural notion of query classes. Optimal clustering in this context is a combinatorially explosive problem with a huge search space (doubly exponential in number of hierarchy levels). We identify an important subclass of clustering strategies called lattice paths, and present a dynamic programming algorithm for finding the optimal lattice path clustering, in time linear in the lattice size. We additionally propose a technique called snaking, which when applied to a lattice path, always reduces its cost. For a representative class of star schemas, we show that for every workload, there is a snaked lattice path which is globally optimal. Further, we prove that the clustering obtained by applying snaking to the optimal lattice path is never much worse than the globally optimal snaked lattice path clustering. We complement our analyses and validate the practical utility of our techniques with experiments using TPC-D benchmark data.},
journal = {SIGMOD Rec.},
month = jun,
pages = {37–48},
numpages = {12}
}

@inproceedings{10.1145/304182.304187,
author = {Ankerst, Mihael and Breunig, Markus M. and Kriegel, Hans-Peter and Sander, J\"{o}rg},
title = {OPTICS: Ordering Points to Identify the Clustering Structure},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304187},
doi = {10.1145/304182.304187},
abstract = {Cluster analysis is a primary method for database mining. It is either used as a stand-alone tool to get insight into the distribution of a data set, e.g. to focus further analysis and data processing, or as a preprocessing step for other algorithms operating on the detected clusters. Almost all of the well-known clustering algorithms require input parameters which are hard to determine but have a significant influence on the clustering result. Furthermore, for many real-data sets there does not even exist a global parameter setting for which the result of the clustering algorithm describes the intrinsic clustering structure accurately. We introduce a new algorithm for the purpose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of parameter settings. It is a versatile basis for both automatic and interactive cluster analysis. We show how to automatically and efficiently extract not only 'traditional' clustering information (e.g. representative points, arbitrary shaped clusters), but also the intrinsic clustering structure. For medium sized data sets, the cluster-ordering can be represented graphically and for very large data sets, we introduce an appropriate visualization technique. Both are suitable for interactive exploration of the intrinsic clustering structure offering additional insights into the distribution and correlation of the data.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {49–60},
numpages = {12},
keywords = {database mining, visualization, cluster analysis},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304187,
author = {Ankerst, Mihael and Breunig, Markus M. and Kriegel, Hans-Peter and Sander, J\"{o}rg},
title = {OPTICS: Ordering Points to Identify the Clustering Structure},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304187},
doi = {10.1145/304181.304187},
abstract = {Cluster analysis is a primary method for database mining. It is either used as a stand-alone tool to get insight into the distribution of a data set, e.g. to focus further analysis and data processing, or as a preprocessing step for other algorithms operating on the detected clusters. Almost all of the well-known clustering algorithms require input parameters which are hard to determine but have a significant influence on the clustering result. Furthermore, for many real-data sets there does not even exist a global parameter setting for which the result of the clustering algorithm describes the intrinsic clustering structure accurately. We introduce a new algorithm for the purpose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of parameter settings. It is a versatile basis for both automatic and interactive cluster analysis. We show how to automatically and efficiently extract not only 'traditional' clustering information (e.g. representative points, arbitrary shaped clusters), but also the intrinsic clustering structure. For medium sized data sets, the cluster-ordering can be represented graphically and for very large data sets, we introduce an appropriate visualization technique. Both are suitable for interactive exploration of the intrinsic clustering structure offering additional insights into the distribution and correlation of the data.},
journal = {SIGMOD Rec.},
month = jun,
pages = {49–60},
numpages = {12},
keywords = {cluster analysis, database mining, visualization}
}

@inproceedings{10.1145/304182.304188,
author = {Aggarwal, Charu C. and Wolf, Joel L. and Yu, Philip S. and Procopiuc, Cecilia and Park, Jong Soo},
title = {Fast Algorithms for Projected Clustering},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304188},
doi = {10.1145/304182.304188},
abstract = {The clustering problem is well known in the database literature for its numerous applications in problems such as customer segmentation, classification and trend analysis. Unfortunately, all known algorithms tend to break down in high dimensional spaces because of the inherent sparsity of the points. In such high dimensional spaces not all dimensions may be relevant to a given cluster. One way of handling this is to pick the closely correlated dimensions and find clusters in the corresponding subspace. Traditional feature selection algorithms attempt to achieve this. The weakness of this approach is that in typical high dimensional data mining applications different sets of points may cluster better for different subsets of dimensions. The number of dimensions in each such cluster-specific subspace may also vary. Hence, it may be impossible to find a single small subset of dimensions for all the clusters. We therefore discuss a generalization of the clustering problem, referred to as the projected clustering problem, in which the subsets of dimensions selected are specific to the clusters themselves. We develop an algorithmic framework for solving the projected clustering problem, and test its performance on synthetic data.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {61–72},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304188,
author = {Aggarwal, Charu C. and Wolf, Joel L. and Yu, Philip S. and Procopiuc, Cecilia and Park, Jong Soo},
title = {Fast Algorithms for Projected Clustering},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304188},
doi = {10.1145/304181.304188},
abstract = {The clustering problem is well known in the database literature for its numerous applications in problems such as customer segmentation, classification and trend analysis. Unfortunately, all known algorithms tend to break down in high dimensional spaces because of the inherent sparsity of the points. In such high dimensional spaces not all dimensions may be relevant to a given cluster. One way of handling this is to pick the closely correlated dimensions and find clusters in the corresponding subspace. Traditional feature selection algorithms attempt to achieve this. The weakness of this approach is that in typical high dimensional data mining applications different sets of points may cluster better for different subsets of dimensions. The number of dimensions in each such cluster-specific subspace may also vary. Hence, it may be impossible to find a single small subset of dimensions for all the clusters. We therefore discuss a generalization of the clustering problem, referred to as the projected clustering problem, in which the subsets of dimensions selected are specific to the clusters themselves. We develop an algorithmic framework for solving the projected clustering problem, and test its performance on synthetic data.},
journal = {SIGMOD Rec.},
month = jun,
pages = {61–72},
numpages = {12}
}

@inproceedings{10.1145/304182.304189,
author = {Lomet, David and Tuttle, Mark},
title = {Logical Logging to Extend Recovery to New Domains},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304189},
doi = {10.1145/304182.304189},
abstract = {Recovery can be extended to new domains at reduced logging cost by exploiting “logical” log operations. During recovery, a logical log operation may read data values from any recoverable object, not solely from values on the log or from the updated object. Hence, we needn't log these values, a substantial saving. In [8], we developed a redo recovery theory that deals with general log operations and proved that the stable database remains recoverable when it is explained in terms of an installation graph. This graph was used to derived a write graph that determines a flush order for cached objects that ensures that the database remains recoverable. In this paper, we introduce a refined write graph that permits more flexible cache management that flushes smaller sets of objects. Using this write graph, we show how: (i) the cache manager can inject its own operations to break up atomic flush sets; and (ii) the recovery process can avoid redoing operations whose effects aren't needed by exploiting generalized recovery LSNs. These advances permit more cost-effective recovery for, e.g., files and applications.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {73–84},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304189,
author = {Lomet, David and Tuttle, Mark},
title = {Logical Logging to Extend Recovery to New Domains},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304189},
doi = {10.1145/304181.304189},
abstract = {Recovery can be extended to new domains at reduced logging cost by exploiting “logical” log operations. During recovery, a logical log operation may read data values from any recoverable object, not solely from values on the log or from the updated object. Hence, we needn't log these values, a substantial saving. In [8], we developed a redo recovery theory that deals with general log operations and proved that the stable database remains recoverable when it is explained in terms of an installation graph. This graph was used to derived a write graph that determines a flush order for cached objects that ensures that the database remains recoverable. In this paper, we introduce a refined write graph that permits more flexible cache management that flushes smaller sets of objects. Using this write graph, we show how: (i) the cache manager can inject its own operations to break up atomic flush sets; and (ii) the recovery process can avoid redoing operations whose effects aren't needed by exploiting generalized recovery LSNs. These advances permit more cost-effective recovery for, e.g., files and applications.},
journal = {SIGMOD Rec.},
month = jun,
pages = {73–84},
numpages = {12}
}

@inproceedings{10.1145/304182.304190,
author = {Shanmugasundaram, Jayavel and Nithrakashyap, Arvind and Sivasankaran, Rajendran and Ramamritham, Krithi},
title = {Efficient Concurrency Control for Broadcast Environments},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304190},
doi = {10.1145/304182.304190},
abstract = {A crucial consideration in environments where data is broadcast to clients is the low bandwidth available for clients to communicate with servers. Advanced applications in such environments do need to read data that is mutually consistent as well as current. However, given the asymmetric communication capabilities and the needs of clients in mobile environments, traditional serializability-based approaches are too restrictive, unnecessary, and impractical. We thus propose the use of a weaker correctness criterion called update consistency and outline mechanisms based on this criterion that ensure (1) the mutual consistency of data maintained by the server and read by clients, and (2) the currency of data read by clients. Using these mechanisms, clients can obtain data that is current and mutually consistent “off the air”, i.e., without contacting the server to, say, obtain locks. Experimental results show a substantial reduction in response times as compared to existing (serializability-based) approaches. A further attractive feature of the approach is that if caching is possible at a client, weaker forms of currency can be obtained while still satisfying the mutual consistency of data.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {85–96},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304190,
author = {Shanmugasundaram, Jayavel and Nithrakashyap, Arvind and Sivasankaran, Rajendran and Ramamritham, Krithi},
title = {Efficient Concurrency Control for Broadcast Environments},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304190},
doi = {10.1145/304181.304190},
abstract = {A crucial consideration in environments where data is broadcast to clients is the low bandwidth available for clients to communicate with servers. Advanced applications in such environments do need to read data that is mutually consistent as well as current. However, given the asymmetric communication capabilities and the needs of clients in mobile environments, traditional serializability-based approaches are too restrictive, unnecessary, and impractical. We thus propose the use of a weaker correctness criterion called update consistency and outline mechanisms based on this criterion that ensure (1) the mutual consistency of data maintained by the server and read by clients, and (2) the currency of data read by clients. Using these mechanisms, clients can obtain data that is current and mutually consistent “off the air”, i.e., without contacting the server to, say, obtain locks. Experimental results show a substantial reduction in response times as compared to existing (serializability-based) approaches. A further attractive feature of the approach is that if caching is possible at a client, weaker forms of currency can be obtained while still satisfying the mutual consistency of data.},
journal = {SIGMOD Rec.},
month = jun,
pages = {85–96},
numpages = {12}
}

@inproceedings{10.1145/304182.304191,
author = {Breitbart, Yuri and Komondoor, Raghavan and Rastogi, Rajeev and Seshadri, S. and Silberschatz, Avi},
title = {Update Propagation Protocols for Replicated Databates},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304191},
doi = {10.1145/304182.304191},
abstract = {Replication is often used in many distributed systems to provide a higher level of performance, reliability and availability. Lazy replica update protocols, which propagate updates to replicas through independent transactions after the original transaction commits, have become popular with database vendors due to their superior performance characteristics. However, if lazy protocols are used indiscriminately, they can result in non-serializable executions. In this paper, we propose two new lazy update protocols that guarantee serializability but impose a much weaker requirement on data placement than earlier protocols. Further, many naturally occurring distributed systems, like distributed data warehouses, satisfy this requirement. We also extend our lazy update protocols to eliminate all requirements on data placement. The extension is a hybrid protocol that propagates as many updates as possible in a lazy fashion. We implemented our protocols on the Datablitz database system product developed at Bell Labs. We also conducted an extensive performance study which shows that our protocols outperform existing protocols over a wide range of workloads.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {97–108},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304191,
author = {Breitbart, Yuri and Komondoor, Raghavan and Rastogi, Rajeev and Seshadri, S. and Silberschatz, Avi},
title = {Update Propagation Protocols for Replicated Databates},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304191},
doi = {10.1145/304181.304191},
abstract = {Replication is often used in many distributed systems to provide a higher level of performance, reliability and availability. Lazy replica update protocols, which propagate updates to replicas through independent transactions after the original transaction commits, have become popular with database vendors due to their superior performance characteristics. However, if lazy protocols are used indiscriminately, they can result in non-serializable executions. In this paper, we propose two new lazy update protocols that guarantee serializability but impose a much weaker requirement on data placement than earlier protocols. Further, many naturally occurring distributed systems, like distributed data warehouses, satisfy this requirement. We also extend our lazy update protocols to eliminate all requirements on data placement. The extension is a hybrid protocol that propagates as many updates as possible in a lazy fashion. We implemented our protocols on the Datablitz database system product developed at Bell Labs. We also conducted an extensive performance study which shows that our protocols outperform existing protocols over a wide range of workloads.},
journal = {SIGMOD Rec.},
month = jun,
pages = {97–108},
numpages = {12}
}

@inproceedings{10.1145/304182.304192,
author = {Jamil, Hasan M.},
title = {Belief Reasoning in MLS Deductive Databases},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304192},
doi = {10.1145/304182.304192},
abstract = {It is envisaged that the application of the multilevel security (MLS) scheme will enhance flexibility and effectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view definitions on a per user basis. However, as advances in this area are being made and ideas crystallized, the concomitant weaknesses of the MLS databases are also surfacing. We insist that the critical problem with the current model is that the belief at a higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported. Critics also argue that it is imperative for MLS database users to theorize about the belief of others, perhaps at different security levels, an apparatus that is currently missing and the absence of which is seriously felt.The impetus for our current research is this need to provide an adequate framework for belief reasoning in MLS databases. We demonstrate that a prudent application of the concept of inheritance in a deductive database setting will help capture the notion of declarative belief and belief reasoning in MLS databases in an elegant way. To this end, we develop a function to compute belief in multiple modes which can be used to reason about the beliefs of other users. We strive to develop a poised and practical logical characterization of MLS databases for the first time based on the inherently difficult concept of non-monotonic inheritance. We present an extension of the acclaimed Datalog language, called the MultiLog, and show that Datalog is a special case of our language. We also suggest an implementation scheme for MultiLog as a front-end for CORAL.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {109–120},
numpages = {12},
keywords = {MLS database, deductive databases, reasoning, beleif assertion, inheritance and overriding},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304192,
author = {Jamil, Hasan M.},
title = {Belief Reasoning in MLS Deductive Databases},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304192},
doi = {10.1145/304181.304192},
abstract = {It is envisaged that the application of the multilevel security (MLS) scheme will enhance flexibility and effectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view definitions on a per user basis. However, as advances in this area are being made and ideas crystallized, the concomitant weaknesses of the MLS databases are also surfacing. We insist that the critical problem with the current model is that the belief at a higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported. Critics also argue that it is imperative for MLS database users to theorize about the belief of others, perhaps at different security levels, an apparatus that is currently missing and the absence of which is seriously felt.The impetus for our current research is this need to provide an adequate framework for belief reasoning in MLS databases. We demonstrate that a prudent application of the concept of inheritance in a deductive database setting will help capture the notion of declarative belief and belief reasoning in MLS databases in an elegant way. To this end, we develop a function to compute belief in multiple modes which can be used to reason about the beliefs of other users. We strive to develop a poised and practical logical characterization of MLS databases for the first time based on the inherently difficult concept of non-monotonic inheritance. We present an extension of the acclaimed Datalog language, called the MultiLog, and show that Datalog is a special case of our language. We also suggest an implementation scheme for MultiLog as a front-end for CORAL.},
journal = {SIGMOD Rec.},
month = jun,
pages = {109–120},
numpages = {12},
keywords = {beleif assertion, MLS database, reasoning, inheritance and overriding, deductive databases}
}

@inproceedings{10.1145/304182.304193,
author = {Adali, S. and Sapino, M. L. and Subrahmanian, V. S.},
title = {A Multimedia Presentation Algebra},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304193},
doi = {10.1145/304182.304193},
abstract = {Over the last few years, there has been a tremendous increase in the number of interactive multimedia presentations prepared by different individuals and organizations. In this paper, we present an algebra for querying multimedia presentation databases. In contrast to the relational algebra, an algebra for interactive multimedia presentations must operate on trees whose branches reflect different possible playouts of a family of presentations. The query language supports selection type operations for locating objects and presentation paths that are of interest to the user, join type operations for combining presentations from multiple databases into a single presentation, and finally set theoretic operations for comparing different databases. The algebra operations can be used to locate presentations with specific properties and also for creating new presentations by borrowing different components from existing ones. We prove a host of equivalence results for queries in this algebra which may be used to build query optimizers for interactive presentation databases.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {121–132},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304193,
author = {Adali, S. and Sapino, M. L. and Subrahmanian, V. S.},
title = {A Multimedia Presentation Algebra},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304193},
doi = {10.1145/304181.304193},
abstract = {Over the last few years, there has been a tremendous increase in the number of interactive multimedia presentations prepared by different individuals and organizations. In this paper, we present an algebra for querying multimedia presentation databases. In contrast to the relational algebra, an algebra for interactive multimedia presentations must operate on trees whose branches reflect different possible playouts of a family of presentations. The query language supports selection type operations for locating objects and presentation paths that are of interest to the user, join type operations for combining presentations from multiple databases into a single presentation, and finally set theoretic operations for comparing different databases. The algebra operations can be used to locate presentations with specific properties and also for creating new presentations by borrowing different components from existing ones. We prove a host of equivalence results for queries in this algebra which may be used to build query optimizers for interactive presentation databases.},
journal = {SIGMOD Rec.},
month = jun,
pages = {121–132},
numpages = {12}
}

@inproceedings{10.1145/304182.304194,
author = {Jagadish, H. V. and Lakshmanan, Laks V. S. and Milo, Tova and Srivastava, Divesh and Vista, Dimitra},
title = {Querying Network Directories},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304194},
doi = {10.1145/304182.304194},
abstract = {Heirarchically structured directories have recently proliferated with the growth of the Internet, and are being used to store not only address books and contact information for people, but also personal profiles, network resource information, and network and service policies. These systems provide a means for managing scale and heterogeneity, while allowing for conceptual unity and autonomy across multiple directory servers in the network, in a way for superior to what conventional relational or object-oriented databases offer. Yet, in deployed systems today, much of the data is modeled in an ad hoc manner, and many of the more sophisticated “queries” involve navigational access.In this paper, we develop the core of a formal data model for network directories, and propose a sequence of efficiently computable query languages with increasing expressive power. The directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. Answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. We present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the efficiency of each algorithm in terms of its I/O complexity. Our data model and query languages share the flexibility and utility of the recent proposals for semi-structured data models, while at the same time effectively addressing the specific needs of network directory applications, which we demonstrate by means of a representative real-life example.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {133–144},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304194,
author = {Jagadish, H. V. and Lakshmanan, Laks V. S. and Milo, Tova and Srivastava, Divesh and Vista, Dimitra},
title = {Querying Network Directories},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304194},
doi = {10.1145/304181.304194},
abstract = {Heirarchically structured directories have recently proliferated with the growth of the Internet, and are being used to store not only address books and contact information for people, but also personal profiles, network resource information, and network and service policies. These systems provide a means for managing scale and heterogeneity, while allowing for conceptual unity and autonomy across multiple directory servers in the network, in a way for superior to what conventional relational or object-oriented databases offer. Yet, in deployed systems today, much of the data is modeled in an ad hoc manner, and many of the more sophisticated “queries” involve navigational access.In this paper, we develop the core of a formal data model for network directories, and propose a sequence of efficiently computable query languages with increasing expressive power. The directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. Answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. We present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the efficiency of each algorithm in terms of its I/O complexity. Our data model and query languages share the flexibility and utility of the recent proposals for semi-structured data models, while at the same time effectively addressing the specific needs of network directory applications, which we demonstrate by means of a representative real-life example.},
journal = {SIGMOD Rec.},
month = jun,
pages = {133–144},
numpages = {12}
}

@inproceedings{10.1145/304182.304195,
author = {Hidber, Christian},
title = {Online Association Rule Mining},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304195},
doi = {10.1145/304182.304195},
abstract = {We present a novel algorithm to compute large itemsets online. The user is free to change the support threshold any time during the first scan of the transaction sequence. The algorithm maintains a superset of all large itemsets and for each itemset a shrinking, deterministic interval on its support. After at most 2 scans the algorithm terminates with the precise support for each large itemset. Typically our algorithm is by an order of magnitude more memory efficient than Apriori or DIC.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {145–156},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304195,
author = {Hidber, Christian},
title = {Online Association Rule Mining},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304195},
doi = {10.1145/304181.304195},
abstract = {We present a novel algorithm to compute large itemsets online. The user is free to change the support threshold any time during the first scan of the transaction sequence. The algorithm maintains a superset of all large itemsets and for each itemset a shrinking, deterministic interval on its support. After at most 2 scans the algorithm terminates with the precise support for each large itemset. Typically our algorithm is by an order of magnitude more memory efficient than Apriori or DIC.},
journal = {SIGMOD Rec.},
month = jun,
pages = {145–156},
numpages = {12}
}

@inproceedings{10.1145/304182.304196,
author = {Lakshmanan, Laks V. S. and Ng, Raymond and Han, Jiawei and Pang, Alex},
title = {Optimization of Constrained Frequent Set Queries with 2-Variable Constraints},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304196},
doi = {10.1145/304182.304196},
abstract = {Currently, there is tremendous interest in providing ad-hoc mining capabilities in database management systems. As a first step towards this goal, in [15] we proposed an architecture for supporting constraint-based, human-centered, exploratory mining of various kinds of rules including associations, introduced the notion of constrained frequent set queries (CFQs), and developed effective pruning optimizations for CFQs with 1-variable (1-var) constraints.While 1-var constraints are useful for constraining the antecedent and consequent separately, many natural examples of CFQs illustrate the need for constraining the antecedent and consequent jointly, for which 2-variable (2-var) constraints are indispensable. Developing pruning optimizations for CFQs with 2-var constraints is the subject of this paper. But this is a difficult problem because: (i) in 2-var constraints, both variables keep changing and, unlike 1-var constraints, there is no fixed target for pruning; (ii) as we show, “conventional” monotonicity-based optimization techniques do not apply effectively to 2-var constraints.The contributions are as follows. (1) We introduce a notion of quasi-succinctness, which allows a quasi-succinct 2-var constraint to be reduced to two succinct 1-var constraints for pruning. (2) We characterize the class of 2-var constraints that are quasi-succinct. (3) We develop heuristic techniques for non-quasi-succinct constraints. Experimental results show the effectiveness of all our techniques. (4) We propose a query optimizer for CFQs and show that for a large class of constraints, the computation strategy generated by the optimizer is ccc-optimal, i.e., minimizing the effort incurred w.r.t. constraint checking and support counting.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {157–168},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304196,
author = {Lakshmanan, Laks V. S. and Ng, Raymond and Han, Jiawei and Pang, Alex},
title = {Optimization of Constrained Frequent Set Queries with 2-Variable Constraints},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304196},
doi = {10.1145/304181.304196},
abstract = {Currently, there is tremendous interest in providing ad-hoc mining capabilities in database management systems. As a first step towards this goal, in [15] we proposed an architecture for supporting constraint-based, human-centered, exploratory mining of various kinds of rules including associations, introduced the notion of constrained frequent set queries (CFQs), and developed effective pruning optimizations for CFQs with 1-variable (1-var) constraints.While 1-var constraints are useful for constraining the antecedent and consequent separately, many natural examples of CFQs illustrate the need for constraining the antecedent and consequent jointly, for which 2-variable (2-var) constraints are indispensable. Developing pruning optimizations for CFQs with 2-var constraints is the subject of this paper. But this is a difficult problem because: (i) in 2-var constraints, both variables keep changing and, unlike 1-var constraints, there is no fixed target for pruning; (ii) as we show, “conventional” monotonicity-based optimization techniques do not apply effectively to 2-var constraints.The contributions are as follows. (1) We introduce a notion of quasi-succinctness, which allows a quasi-succinct 2-var constraint to be reduced to two succinct 1-var constraints for pruning. (2) We characterize the class of 2-var constraints that are quasi-succinct. (3) We develop heuristic techniques for non-quasi-succinct constraints. Experimental results show the effectiveness of all our techniques. (4) We propose a query optimizer for CFQs and show that for a large class of constraints, the computation strategy generated by the optimizer is ccc-optimal, i.e., minimizing the effort incurred w.r.t. constraint checking and support counting.},
journal = {SIGMOD Rec.},
month = jun,
pages = {157–168},
numpages = {12}
}

@inproceedings{10.1145/304182.304197,
author = {Gehrke, Johannes and Ganti, Venkatesh and Ramakrishnan, Raghu and Loh, Wei-Yin},
title = {BOAT—Optimistic Decision Tree Construction},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304197},
doi = {10.1145/304182.304197},
abstract = {Classification is an important data mining problem. Given a training database of records, each tagged with a class label, the goal of classification is to build a concise model that can be used to predict the class label of future, unlabeled records. A very popular class of classifiers are decision trees. All current algorithms to construct decision trees, including all main-memory algorithms, make one scan over the training database per level of the tree.We introduce a new algorithm (BOAT) for decision tree construction that improves upon earlier algorithms in both performance and functionality. BOAT constructs several levels of the tree in only two scans over the training database, resulting in an average performance gain of 300% over previous work. The key to this performance improvement is a novel optimistic approach to tree construction in which we construct an initial tree using a small subset of the data and refine it to arrive at the final tree. We guarantee that any difference with respect to the “real” tree (i.e., the tree that would be constructed by examining all the data in a traditional way) is detected and corrected. The correction step occasionally requires us to make additional scans over subsets of the data; typically, this situation rarely arises, and can be addressed with little added cost.Beyond offering faster tree construction, BOAT is the first scalable algorithm with the ability to incrementally update the tree with respect to both insertions and deletions over the dataset. This property is valuable in dynamic environments such as data warehouses, in which the training dataset changes over time. The BOAT update operation is much cheaper than completely rebuilding the tree, and the resulting tree is guaranteed to be identical to the tree that would be produced by a complete re-build.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {169–180},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304197,
author = {Gehrke, Johannes and Ganti, Venkatesh and Ramakrishnan, Raghu and Loh, Wei-Yin},
title = {BOAT—Optimistic Decision Tree Construction},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304197},
doi = {10.1145/304181.304197},
abstract = {Classification is an important data mining problem. Given a training database of records, each tagged with a class label, the goal of classification is to build a concise model that can be used to predict the class label of future, unlabeled records. A very popular class of classifiers are decision trees. All current algorithms to construct decision trees, including all main-memory algorithms, make one scan over the training database per level of the tree.We introduce a new algorithm (BOAT) for decision tree construction that improves upon earlier algorithms in both performance and functionality. BOAT constructs several levels of the tree in only two scans over the training database, resulting in an average performance gain of 300% over previous work. The key to this performance improvement is a novel optimistic approach to tree construction in which we construct an initial tree using a small subset of the data and refine it to arrive at the final tree. We guarantee that any difference with respect to the “real” tree (i.e., the tree that would be constructed by examining all the data in a traditional way) is detected and corrected. The correction step occasionally requires us to make additional scans over subsets of the data; typically, this situation rarely arises, and can be addressed with little added cost.Beyond offering faster tree construction, BOAT is the first scalable algorithm with the ability to incrementally update the tree with respect to both insertions and deletions over the dataset. This property is valuable in dynamic environments such as data warehouses, in which the training dataset changes over time. The BOAT update operation is much cheaper than completely rebuilding the tree, and the resulting tree is guaranteed to be identical to the tree that would be produced by a complete re-build.},
journal = {SIGMOD Rec.},
month = jun,
pages = {169–180},
numpages = {12}
}

@inproceedings{10.1145/304182.304198,
author = {Aboulnaga, Ashraf and Chaudhuri, Surajit},
title = {Self-Tuning Histograms: Building Histograms without Looking at Data},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304198},
doi = {10.1145/304182.304198},
abstract = {In this paper, we introduce self-tuning histograms. Although similar in structure to traditional histograms, these histograms infer data distributions not by examining the data or a sample thereof, but by using feedback from the query execution engine about the actual selectivity of range selection operators to progressively refine the histogram. Since the cost of building and maintaining self-tuning histograms is independent of the data size, self-tuning histograms provide a remarkably inexpensive way to construct histograms for large data sets with little up-front costs. Self-tuning histograms are particularly attractive as an alternative to multi-dimensional traditional histograms that capture dependencies between attributes but are prohibitively expensive to build and maintain. In this paper, we describe the techniques for initializing and refining self-tuning histograms. Our experimental results show that self-tuning histograms provide a low-cost alternative to traditional multi-dimensional histograms with little loss of accuracy for data distributions with low to moderate skew.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {181–192},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304198,
author = {Aboulnaga, Ashraf and Chaudhuri, Surajit},
title = {Self-Tuning Histograms: Building Histograms without Looking at Data},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304198},
doi = {10.1145/304181.304198},
abstract = {In this paper, we introduce self-tuning histograms. Although similar in structure to traditional histograms, these histograms infer data distributions not by examining the data or a sample thereof, but by using feedback from the query execution engine about the actual selectivity of range selection operators to progressively refine the histogram. Since the cost of building and maintaining self-tuning histograms is independent of the data size, self-tuning histograms provide a remarkably inexpensive way to construct histograms for large data sets with little up-front costs. Self-tuning histograms are particularly attractive as an alternative to multi-dimensional traditional histograms that capture dependencies between attributes but are prohibitively expensive to build and maintain. In this paper, we describe the techniques for initializing and refining self-tuning histograms. Our experimental results show that self-tuning histograms provide a low-cost alternative to traditional multi-dimensional histograms with little loss of accuracy for data distributions with low to moderate skew.},
journal = {SIGMOD Rec.},
month = jun,
pages = {181–192},
numpages = {12}
}

@inproceedings{10.1145/304182.304199,
author = {Vitter, Jeffrey Scott and Wang, Min},
title = {Approximate Computation of Multidimensional Aggregates of Sparse Data Using Wavelets},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304199},
doi = {10.1145/304182.304199},
abstract = {Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries.In this paper, we present a novel method that provides approximate answers to high-dimensional OLAP aggregation queries in massive sparse data sets in a time-efficient and space-efficient manner. We construct a compact data cube, which is an approximate and space-efficient representation of the underlying multidimensional array, based upon a multiresolution wavelet decomposition. In the on-line phase, each aggregation query can generally be answered using the compact data cube in one I/O or a smalll number of I/Os, depending upon the desired accuracy.We present two I/O-efficient algorithms to construct the compact data cube for the important case of sparse high-dimensional arrays, which often arise in practice. The traditional histogram methods are infeasible for the massive high-dimensional data sets in OLAP applications. Previously developed wavelet techniques are efficient only for dense data. Our on-line query processing algorithm is very fast and capable of refining answers as the user demands more accuracy. Experiments on real data show that our method provides significantly more accurate results for typical OLAP aggregation queries than other efficient approximation techniques such as random sampling.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {193–204},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304199,
author = {Vitter, Jeffrey Scott and Wang, Min},
title = {Approximate Computation of Multidimensional Aggregates of Sparse Data Using Wavelets},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304199},
doi = {10.1145/304181.304199},
abstract = {Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries.In this paper, we present a novel method that provides approximate answers to high-dimensional OLAP aggregation queries in massive sparse data sets in a time-efficient and space-efficient manner. We construct a compact data cube, which is an approximate and space-efficient representation of the underlying multidimensional array, based upon a multiresolution wavelet decomposition. In the on-line phase, each aggregation query can generally be answered using the compact data cube in one I/O or a smalll number of I/Os, depending upon the desired accuracy.We present two I/O-efficient algorithms to construct the compact data cube for the important case of sparse high-dimensional arrays, which often arise in practice. The traditional histogram methods are infeasible for the massive high-dimensional data sets in OLAP applications. Previously developed wavelet techniques are efficient only for dense data. Our on-line query processing algorithm is very fast and capable of refining answers as the user demands more accuracy. Experiments on real data show that our method provides significantly more accurate results for typical OLAP aggregation queries than other efficient approximation techniques such as random sampling.},
journal = {SIGMOD Rec.},
month = jun,
pages = {193–204},
numpages = {12}
}

@inproceedings{10.1145/304182.304200,
author = {Lee, Ju-Hong and Kim, Deok-Hwan and Chung, Chin-Wan},
title = {Multi-Dimensional Selectivity Estimation Using Compressed Histogram Information},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304200},
doi = {10.1145/304182.304200},
abstract = {The database query optimizer requires the estimation of the query selectivity to find the most efficient access plan. For queries referencing multiple attributes from the same relation, we need a multi-dimensional selectivity estimation technique when the attributes are dependent each other because the selectivity is determined by the joint data distribution of the attributes. Additionally, for multimedia databases, there are intrinsic requirements for the multi-dimensional selectivity estimation because feature vectors are stored in multi-dimensional indexing trees. In the 1-dimensional case, a histogram is practically the most preferable. In the multi-dimensional case, however, a histogram is not adequate because of high storage overhead and high error rates.In this paper, we propose a novel approach for the multi-dimensional selectivity estimation. Compressed information from a large number of small-sized histogram buckets is maintained using the discrete cosine transform. This enables low error rates and low storage overheads even in high dimensions. In addition, this approach has the advantage of supporting dynamic data updates by eliminating the overhead for periodical reconstructions of the compressed information. Extensive experimental results show advantages of the proposed approach.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {205–214},
numpages = {10},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304200,
author = {Lee, Ju-Hong and Kim, Deok-Hwan and Chung, Chin-Wan},
title = {Multi-Dimensional Selectivity Estimation Using Compressed Histogram Information},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304200},
doi = {10.1145/304181.304200},
abstract = {The database query optimizer requires the estimation of the query selectivity to find the most efficient access plan. For queries referencing multiple attributes from the same relation, we need a multi-dimensional selectivity estimation technique when the attributes are dependent each other because the selectivity is determined by the joint data distribution of the attributes. Additionally, for multimedia databases, there are intrinsic requirements for the multi-dimensional selectivity estimation because feature vectors are stored in multi-dimensional indexing trees. In the 1-dimensional case, a histogram is practically the most preferable. In the multi-dimensional case, however, a histogram is not adequate because of high storage overhead and high error rates.In this paper, we propose a novel approach for the multi-dimensional selectivity estimation. Compressed information from a large number of small-sized histogram buckets is maintained using the discrete cosine transform. This enables low error rates and low storage overheads even in high dimensions. In addition, this approach has the advantage of supporting dynamic data updates by eliminating the overhead for periodical reconstructions of the compressed information. Extensive experimental results show advantages of the proposed approach.},
journal = {SIGMOD Rec.},
month = jun,
pages = {205–214},
numpages = {10}
}

@inproceedings{10.1145/304182.304201,
author = {Chan, Chee-Yong and Ioannidis, Yannis E.},
title = {An Efficient Bitmap Encoding Scheme for Selection Queries},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304201},
doi = {10.1145/304182.304201},
abstract = {Bitmap indexes are useful in processing complex queries in decision support systems, and they have been implemented in several commercial database systems. A key design parameter for bitmap indexes is the encoding scheme, which determines the bits that are set to 1 in each bitmap in an index. While the relative performance of the two existing bitmap encoding schemes for simple selection queries of the form “v1 ≤ A ≤ v2” is known (specifically, one of the encoding schemes is better for processing equality queries; i.e., v1 = v2, while the other is better for processing range queries; i.e., v1 &lt; v2), it remains an open question whether these two encoding schemes are indeed optimal for their respective query classes in the sense that there is no other encoding scheme with better space-time tradeoff. In this paper, we establish a number of optimality results for the existing encoding schemes; in particular, we prove that neither of the two known schemes is optimal for the class of two-sided range queries. We also propose a new encoding scheme and prove that it is optimal for that class. Finally, we present an experimental study comparing the performance of the new encoding scheme with that of the existing ones as well as four hybrid encoding schemes for both simple selection queries and the more general class of membership queries of the form “A ∈ {v1, v2, .…, vk}”. These results demonstrate that the new encoding scheme has an overall better space-time performance than existing schemes.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {215–226},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304201,
author = {Chan, Chee-Yong and Ioannidis, Yannis E.},
title = {An Efficient Bitmap Encoding Scheme for Selection Queries},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304201},
doi = {10.1145/304181.304201},
abstract = {Bitmap indexes are useful in processing complex queries in decision support systems, and they have been implemented in several commercial database systems. A key design parameter for bitmap indexes is the encoding scheme, which determines the bits that are set to 1 in each bitmap in an index. While the relative performance of the two existing bitmap encoding schemes for simple selection queries of the form “v1 ≤ A ≤ v2” is known (specifically, one of the encoding schemes is better for processing equality queries; i.e., v1 = v2, while the other is better for processing range queries; i.e., v1 &lt; v2), it remains an open question whether these two encoding schemes are indeed optimal for their respective query classes in the sense that there is no other encoding scheme with better space-time tradeoff. In this paper, we establish a number of optimality results for the existing encoding schemes; in particular, we prove that neither of the two known schemes is optimal for the class of two-sided range queries. We also propose a new encoding scheme and prove that it is optimal for that class. Finally, we present an experimental study comparing the performance of the new encoding scheme with that of the existing ones as well as four hybrid encoding schemes for both simple selection queries and the more general class of membership queries of the form “A ∈ {v1, v2, .…, vk}”. These results demonstrate that the new encoding scheme has an overall better space-time performance than existing schemes.},
journal = {SIGMOD Rec.},
month = jun,
pages = {215–226},
numpages = {12}
}

@inproceedings{10.1145/304182.304202,
author = {Wu, Ming-Chuan},
title = {Query Optimization for Selections Using Bitmaps},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304202},
doi = {10.1145/304182.304202},
abstract = {Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {227–238},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304202,
author = {Wu, Ming-Chuan},
title = {Query Optimization for Selections Using Bitmaps},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304202},
doi = {10.1145/304181.304202},
abstract = {Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.},
journal = {SIGMOD Rec.},
month = jun,
pages = {227–238},
numpages = {12}
}

@inproceedings{10.1145/304182.304203,
author = {Blohsfeld, Bj\"{o}rn and Korus, Dieter and Seeger, Bernhard},
title = {A Comparison of Selectivity Estimators for Range Queries on Metric Attributes},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304203},
doi = {10.1145/304182.304203},
abstract = {In this paper, we present a comparison of nonparametric estimation methods for computing approximations of the selectivities of queries, in particular range queries. In contrast to previous studies, the focus of our comparison is on metric attributes with large domains which occur for example in spatial and temporal databases. We also assume that only small sample sets of the required relations are available for estimating the selectivity. In addition to the popular histogram estimators, our comparison includes so-called kernel estimation methods. Although these methods have been proven to be among the most accurate estimators known in statistics, they have not been considered for selectivity estimation of database queries, so far. We first show how to generate kernel estimators that deliver accurate approximate selectivities of queries. Thereafter, we reveal that two parameters, the number of samples and the so-called smoothing parameter, are important for the accuracy of both kernel estimators and histogram estimators. For histogram estimators, the smoothing parameter determines the number of bins (histogram classes). We first present the optimal smoothing parameter as a function of the number of samples and show how to compute approximations of the optimal parameter. Moreover, we propose a new selectivity estimator that can be viewed as an hybrid of histogram and kernel estimators. Experimental results show the performance of different estimators in practice. We found in our experiments that kernel estimators are most efficient for continuously distributed data sets, whereas for our real data sets the hybrid technique is most promising.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {239–250},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304203,
author = {Blohsfeld, Bj\"{o}rn and Korus, Dieter and Seeger, Bernhard},
title = {A Comparison of Selectivity Estimators for Range Queries on Metric Attributes},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304203},
doi = {10.1145/304181.304203},
abstract = {In this paper, we present a comparison of nonparametric estimation methods for computing approximations of the selectivities of queries, in particular range queries. In contrast to previous studies, the focus of our comparison is on metric attributes with large domains which occur for example in spatial and temporal databases. We also assume that only small sample sets of the required relations are available for estimating the selectivity. In addition to the popular histogram estimators, our comparison includes so-called kernel estimation methods. Although these methods have been proven to be among the most accurate estimators known in statistics, they have not been considered for selectivity estimation of database queries, so far. We first show how to generate kernel estimators that deliver accurate approximate selectivities of queries. Thereafter, we reveal that two parameters, the number of samples and the so-called smoothing parameter, are important for the accuracy of both kernel estimators and histogram estimators. For histogram estimators, the smoothing parameter determines the number of bins (histogram classes). We first present the optimal smoothing parameter as a function of the number of samples and show how to compute approximations of the optimal parameter. Moreover, we propose a new selectivity estimator that can be viewed as an hybrid of histogram and kernel estimators. Experimental results show the performance of different estimators in practice. We found in our experiments that kernel estimators are most efficient for continuously distributed data sets, whereas for our real data sets the hybrid technique is most promising.},
journal = {SIGMOD Rec.},
month = jun,
pages = {239–250},
numpages = {12}
}

@inproceedings{10.1145/304182.304204,
author = {Manku, Gurmeet Singh and Rajagopalan, Sridhar and Lindsay, Bruce G.},
title = {Random Sampling Techniques for Space Efficient Online Computation of Order Statistics of Large Datasets},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304204},
doi = {10.1145/304182.304204},
abstract = {In a recent paper [MRL98], we had described a general framework for single pass approximate quantile finding algorithms. This framework included several known algorithms as special cases. We had identified a new algorithm, within the framework, which had a significantly smaller requirement for main memory than other known algorithms. In this paper, we address two issues left open in our earlier paper.First, all known and space efficient algorithms for approximate quantile finding require advance knowledge of the length of the input sequence. Many important database applications employing quantiles cannot provide this information. In this paper, we present a novel non-uniform random sampling scheme and an extension of our framework. Together, they form the basis of a new algorithm which computes approximate quantiles without knowing the input sequence length.Second, if the desired quantile is an extreme value (e.g., within the top 1% of the elements), the space requirements of currently known algorithms are overly pessimistic. We provide a simple algorithm which estimates extreme values using less space than required by the earlier more general technique for computing all quantiles. Our principal observation here is that random sampling is quantifiably better when estimating extreme values than is the case with the median.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {251–262},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304204,
author = {Manku, Gurmeet Singh and Rajagopalan, Sridhar and Lindsay, Bruce G.},
title = {Random Sampling Techniques for Space Efficient Online Computation of Order Statistics of Large Datasets},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304204},
doi = {10.1145/304181.304204},
abstract = {In a recent paper [MRL98], we had described a general framework for single pass approximate quantile finding algorithms. This framework included several known algorithms as special cases. We had identified a new algorithm, within the framework, which had a significantly smaller requirement for main memory than other known algorithms. In this paper, we address two issues left open in our earlier paper.First, all known and space efficient algorithms for approximate quantile finding require advance knowledge of the length of the input sequence. Many important database applications employing quantiles cannot provide this information. In this paper, we present a novel non-uniform random sampling scheme and an extension of our framework. Together, they form the basis of a new algorithm which computes approximate quantiles without knowing the input sequence length.Second, if the desired quantile is an extreme value (e.g., within the top 1% of the elements), the space requirements of currently known algorithms are overly pessimistic. We provide a simple algorithm which estimates extreme values using less space than required by the earlier more general technique for computing all quantiles. Our principal observation here is that random sampling is quantifiably better when estimating extreme values than is the case with the median.},
journal = {SIGMOD Rec.},
month = jun,
pages = {251–262},
numpages = {12}
}

@inproceedings{10.1145/304182.304206,
author = {Chaudhuri, Surajit and Motwani, Rajeev and Narasayya, Vivek},
title = {On Random Sampling over Joins},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304206},
doi = {10.1145/304182.304206},
abstract = {A major bottleneck in implementing sampling as a primitive relational operation is the inefficiency of sampling the output of a query. It is not even known whether it is possible to generate a sample of a join tree without first evaluating the join tree completely. We undertake a detailed study of this problem and attempt to analyze it in a variety of settings. We present theoretical results explaining the difficulty of this problem and setting limits on the efficiency that can be achieved. Based on new insights into the interaction between join and sampling, we develop join sampling techniques for the settings where our negative results do not apply. Our new sampling algorithms are significantly more efficient than those known earlier. We present experimental evaluation of our techniques on Microsoft's SQL Server 7.0.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {263–274},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304206,
author = {Chaudhuri, Surajit and Motwani, Rajeev and Narasayya, Vivek},
title = {On Random Sampling over Joins},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304206},
doi = {10.1145/304181.304206},
abstract = {A major bottleneck in implementing sampling as a primitive relational operation is the inefficiency of sampling the output of a query. It is not even known whether it is possible to generate a sample of a join tree without first evaluating the join tree completely. We undertake a detailed study of this problem and attempt to analyze it in a variety of settings. We present theoretical results explaining the difficulty of this problem and setting limits on the efficiency that can be achieved. Based on new insights into the interaction between join and sampling, we develop join sampling techniques for the settings where our negative results do not apply. Our new sampling algorithms are significantly more efficient than those known earlier. We present experimental evaluation of our techniques on Microsoft's SQL Server 7.0.},
journal = {SIGMOD Rec.},
month = jun,
pages = {263–274},
numpages = {12}
}

@inproceedings{10.1145/304182.304207,
author = {Acharya, Swarup and Gibbons, Phillip B. and Poosala, Viswanath and Ramaswamy, Sridhar},
title = {Join Synopses for Approximate Query Answering},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304207},
doi = {10.1145/304182.304207},
abstract = {In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known. We also present efficient algorithms for incrementally maintaining join synopses in the presence of updates to the base relations. Our extensive set of experiments on the TPC-D benchmark database show the effectiveness of join synopses and various other techniques proposed in this paper.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {275–286},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304207,
author = {Acharya, Swarup and Gibbons, Phillip B. and Poosala, Viswanath and Ramaswamy, Sridhar},
title = {Join Synopses for Approximate Query Answering},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304207},
doi = {10.1145/304181.304207},
abstract = {In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known. We also present efficient algorithms for incrementally maintaining join synopses in the presence of updates to the base relations. Our extensive set of experiments on the TPC-D benchmark database show the effectiveness of join synopses and various other techniques proposed in this paper.},
journal = {SIGMOD Rec.},
month = jun,
pages = {275–286},
numpages = {12}
}

@inproceedings{10.1145/304182.304208,
author = {Haas, Peter J. and Hellerstein, Joseph M.},
title = {Ripple Joins for Online Aggregation},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304208},
doi = {10.1145/304182.304208},
abstract = {We present a new family of join algorithms, called ripple joins, for online processing of multi-table aggregation queries in a relational database management system (DBMS). Such queries arise naturally in interactive exploratory decision-support applications.Traditional offline join algorithms are designed to minimize the time to completion of the query. In contrast, ripple joins are designed to minimize the time until an acceptably precise estimate of the query result is available, as measured by the length of a confidence interval. Ripple joins are adaptive, adjusting their behavior during processing in accordance with the statistical properties of the data. Ripple joins also permit the user to dynamically trade off the two key performance factors of on-line aggregation: the time between successive updates of the running aggregate, and the amount by which the confidence-interval length decreases at each update. We show how ripple joins can be implemented in an existing DBMS using iterators, and we give an overview of the methods used to compute confidence intervals and to adaptively optimize the ripple join “aspect-ratio” parameters. In experiments with an initial implementation of our algorithms in the POSTGRES DBMS, the time required to produce reasonably precise online estimates was up to two orders of magnitude smaller than the time required for the best offline join algorithms to produce exact answers.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {287–298},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304208,
author = {Haas, Peter J. and Hellerstein, Joseph M.},
title = {Ripple Joins for Online Aggregation},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304208},
doi = {10.1145/304181.304208},
abstract = {We present a new family of join algorithms, called ripple joins, for online processing of multi-table aggregation queries in a relational database management system (DBMS). Such queries arise naturally in interactive exploratory decision-support applications.Traditional offline join algorithms are designed to minimize the time to completion of the query. In contrast, ripple joins are designed to minimize the time until an acceptably precise estimate of the query result is available, as measured by the length of a confidence interval. Ripple joins are adaptive, adjusting their behavior during processing in accordance with the statistical properties of the data. Ripple joins also permit the user to dynamically trade off the two key performance factors of on-line aggregation: the time between successive updates of the running aggregate, and the amount by which the confidence-interval length decreases at each update. We show how ripple joins can be implemented in an existing DBMS using iterators, and we give an overview of the methods used to compute confidence intervals and to adaptively optimize the ripple join “aspect-ratio” parameters. In experiments with an initial implementation of our algorithms in the POSTGRES DBMS, the time required to produce reasonably precise online estimates was up to two orders of magnitude smaller than the time required for the best offline join algorithms to produce exact answers.},
journal = {SIGMOD Rec.},
month = jun,
pages = {287–298},
numpages = {12}
}

@inproceedings{10.1145/304182.304209,
author = {Ives, Zachary G. and Florescu, Daniela and Friedman, Marc and Levy, Alon and Weld, Daniel S.},
title = {An Adaptive Query Execution System for Data Integration},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304209},
doi = {10.1145/304182.304209},
abstract = {Query processing in data integration occurs over network-bound, autonomous data sources. This requires extensions to traditional optimization and execution techniques for three reasons: there is an absence of quality statistics about the data, data transfer rates are unpredictable and bursty, and slow or unavailable data sources can often be replaced by overlapping or mirrored sources. This paper presents the Tukwila data integration system, designed to support adaptivity at its core using a two-pronged approach. Interleaved planning and execution with partial optimization allows Tukwila to quickly recover from decisions based on inaccurate estimates. During execution, Tukwila uses adaptive query operators such as the double pipelined hash join, which produces answers quickly, and the dynamic collector, which robustly and efficiently computes unions across overlapping data sources. We demonstrate that the Tukwila architecture extends previous innovations in adaptive execution (such as query scrambling, mid-execution re-optimization, and choose nodes), and we present experimental evidence that our techniques result in behavior desirable for a data integration system.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {299–310},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304209,
author = {Ives, Zachary G. and Florescu, Daniela and Friedman, Marc and Levy, Alon and Weld, Daniel S.},
title = {An Adaptive Query Execution System for Data Integration},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304209},
doi = {10.1145/304181.304209},
abstract = {Query processing in data integration occurs over network-bound, autonomous data sources. This requires extensions to traditional optimization and execution techniques for three reasons: there is an absence of quality statistics about the data, data transfer rates are unpredictable and bursty, and slow or unavailable data sources can often be replaced by overlapping or mirrored sources. This paper presents the Tukwila data integration system, designed to support adaptivity at its core using a two-pronged approach. Interleaved planning and execution with partial optimization allows Tukwila to quickly recover from decisions based on inaccurate estimates. During execution, Tukwila uses adaptive query operators such as the double pipelined hash join, which produces answers quickly, and the dynamic collector, which robustly and efficiently computes unions across overlapping data sources. We demonstrate that the Tukwila architecture extends previous innovations in adaptive execution (such as query scrambling, mid-execution re-optimization, and choose nodes), and we present experimental evidence that our techniques result in behavior desirable for a data integration system.},
journal = {SIGMOD Rec.},
month = jun,
pages = {299–310},
numpages = {12}
}

@inproceedings{10.1145/304182.304210,
author = {Florescu, Daniela and Levy, Alon and Manolescu, Ioana and Suciu, Dan},
title = {Query Optimization in the Presence of Limited Access Patterns},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304210},
doi = {10.1145/304182.304210},
abstract = {We consider the problem of query optimization in the presence of limitations on access patterns to the data (i.e., when one must provide values for one of the attributes of a relation in order to obtain tuples). We show that in the presence of limited access patterns we must search a space of annotated query plans, where the annotations describe the inputs that must be given to the plan. We describe a theoretical and experimental analysis of the resulting search space and a novel query optimization algorithm that is designed to perform well under the different conditions that may arise. The algorithm searches the set of annotated query plans, pruning invalid and non-viable plans as early as possible in the search space, and it also uses a best-first search strategy in order to produce a first complete plan early in the search. We describe experiments to illustrate the performance of our algorithm.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {311–322},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304210,
author = {Florescu, Daniela and Levy, Alon and Manolescu, Ioana and Suciu, Dan},
title = {Query Optimization in the Presence of Limited Access Patterns},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304210},
doi = {10.1145/304181.304210},
abstract = {We consider the problem of query optimization in the presence of limitations on access patterns to the data (i.e., when one must provide values for one of the attributes of a relation in order to obtain tuples). We show that in the presence of limited access patterns we must search a space of annotated query plans, where the annotations describe the inputs that must be given to the plan. We describe a theoretical and experimental analysis of the resulting search space and a novel query optimization algorithm that is designed to perform well under the different conditions that may arise. The algorithm searches the set of annotated query plans, pruning invalid and non-viable plans as early as possible in the search space, and it also uses a best-first search strategy in order to produce a first complete plan early in the search. We describe experiments to illustrate the performance of our algorithm.},
journal = {SIGMOD Rec.},
month = jun,
pages = {311–322},
numpages = {12}
}

@inproceedings{10.1145/304182.304211,
author = {Marathe, Arunprasad P. and Salem, Kenneth},
title = {Query Processing Techniques for Arrays},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304211},
doi = {10.1145/304182.304211},
abstract = {Arrays are an appropriate data model for images, gridded output from computational models, and other types of data. This paper describes an approach to array query processing. Queries are expressed in AML, a logical algebra that is easily extended with user-defined functions to support a wide variety of array operations. For example, compression, filtering, and algebraic operations on images can be described. We show how AML expressions involving such operations can be treated declaratively and subjected to useful rewrite optimizations. We also describe a plan generator that produces efficient iterator-based plans from rewritten AML expressions.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {323–334},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304211,
author = {Marathe, Arunprasad P. and Salem, Kenneth},
title = {Query Processing Techniques for Arrays},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304211},
doi = {10.1145/304181.304211},
abstract = {Arrays are an appropriate data model for images, gridded output from computational models, and other types of data. This paper describes an approach to array query processing. Queries are expressed in AML, a logical algebra that is easily extended with user-defined functions to support a wide variety of array operations. For example, compression, filtering, and algebraic operations on images can be described. We show how AML expressions involving such operations can be treated declaratively and subjected to useful rewrite optimizations. We also describe a plan generator that produces efficient iterator-based plans from rewritten AML expressions.},
journal = {SIGMOD Rec.},
month = jun,
pages = {323–334},
numpages = {12}
}

@inproceedings{10.1145/304182.304212,
author = {Chang, Chen-Chuan K. and Garc\'{\i}a-Molina, H\'{e}ctor},
title = {Mind Your Vocabulary: Query Mapping across Heterogeneous Information Sources},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304212},
doi = {10.1145/304182.304212},
abstract = {In this paper we present a mechanism for translating constraint queries, i.e., Boolean expressions of constraints, across heterogeneous information sources. Integrating such systems is difficult in part because they use a wide range of constraints as the vocabulary for formulating queries. We describe algorithms that apply user-provided mapping rules to translate query constraints into ones that are understood and supported in another context, e.g., that use the proper operators and value formats. We show that the translated queries minimally subsume the original ones. Furthermore, the translated queries are also the most compact possible. Unlike other query mapping work, we effectively consider inter-dependencies among constraints, i.e., we handle constraints that cannot be translated independently. Furthermore, when constraints are not fully supported, our framework explores relaxations (semantic rewritings) into the closest supported version. Our most sophisticated algorithm (Algorithm TDQM) does not blindly convert queries to DNF (which would be easier to translate, but expensive); instead it performs a top-down mapping of a query tree, and does local query structure conversion only when necessary.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {335–346},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304212,
author = {Chang, Chen-Chuan K. and Garc\'{\i}a-Molina, H\'{e}ctor},
title = {Mind Your Vocabulary: Query Mapping across Heterogeneous Information Sources},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304212},
doi = {10.1145/304181.304212},
abstract = {In this paper we present a mechanism for translating constraint queries, i.e., Boolean expressions of constraints, across heterogeneous information sources. Integrating such systems is difficult in part because they use a wide range of constraints as the vocabulary for formulating queries. We describe algorithms that apply user-provided mapping rules to translate query constraints into ones that are understood and supported in another context, e.g., that use the proper operators and value formats. We show that the translated queries minimally subsume the original ones. Furthermore, the translated queries are also the most compact possible. Unlike other query mapping work, we effectively consider inter-dependencies among constraints, i.e., we handle constraints that cannot be translated independently. Furthermore, when constraints are not fully supported, our framework explores relaxations (semantic rewritings) into the closest supported version. Our most sophisticated algorithm (Algorithm TDQM) does not blindly convert queries to DNF (which would be easier to translate, but expensive); instead it performs a top-down mapping of a query tree, and does local query structure conversion only when necessary.},
journal = {SIGMOD Rec.},
month = jun,
pages = {335–346},
numpages = {12}
}

@inproceedings{10.1145/304182.304213,
author = {Mayr, Tobias and Seshadri, Praveen},
title = {Client-Site Query Extensions},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304213},
doi = {10.1145/304182.304213},
abstract = {We explore the execution of queries with client-site user-defined functions (UDFs). Many UDFs can only be executed at the client site, for reasons of scalability, security, confidentiality, or availability of resources. How should a query with client-site UDFs be executed? We demonstrate that the standard execution technique for server-site UDFs performs poorly. Instead, we adapt well-known distributed database algorithms and apply them to client-site UDFs. The resulting query execution techniques are implemented in the Cornell Predator database system, and we present performance results to demonstrate their effectiveness. We also consider the question of query optimization in the context of client-site UDFs. The known techniques for expensive UDFs are inadequate because they do not take the location of the UDF into account. We present an extension of traditional 'System-R' optimizers that suitably optimize queries with client-site operations.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {347–358},
numpages = {12},
keywords = {user-defined functions, distributed query-processing, client-site extensions},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304213,
author = {Mayr, Tobias and Seshadri, Praveen},
title = {Client-Site Query Extensions},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304213},
doi = {10.1145/304181.304213},
abstract = {We explore the execution of queries with client-site user-defined functions (UDFs). Many UDFs can only be executed at the client site, for reasons of scalability, security, confidentiality, or availability of resources. How should a query with client-site UDFs be executed? We demonstrate that the standard execution technique for server-site UDFs performs poorly. Instead, we adapt well-known distributed database algorithms and apply them to client-site UDFs. The resulting query execution techniques are implemented in the Cornell Predator database system, and we present performance results to demonstrate their effectiveness. We also consider the question of query optimization in the context of client-site UDFs. The known techniques for expensive UDFs are inadequate because they do not take the location of the UDF into account. We present an extension of traditional 'System-R' optimizers that suitably optimize queries with client-site operations.},
journal = {SIGMOD Rec.},
month = jun,
pages = {347–358},
numpages = {12},
keywords = {user-defined functions, distributed query-processing, client-site extensions}
}

@inproceedings{10.1145/304182.304214,
author = {Beyer, Kevin and Ramakrishnan, Raghu},
title = {Bottom-up Computation of Sparse and Iceberg CUBE},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304214},
doi = {10.1145/304182.304214},
abstract = {We introduce the Iceberg-CUBE problem as a reformulation of the datacube (CUBE) problem. The Iceberg-CUBE problem is to compute only those group-by partitions with an aggregate value (e.g., count) above some minimum support threshold. The result of Iceberg-CUBE can be used (1) to answer group-by queries with a clause such as HAVING COUNT(*) &gt;= X, where X is greater than the threshold, (2) for mining multidimensional association rules, and (3) to complement existing strategies for identifying interesting subsets of the CUBE for precomputation.We present a new algorithm (BUC) for Iceberg-CUBE computation. BUC builds the CUBE bottom-up; i.e., it builds the CUBE by starting from a group-by on a single attribute, then a group-by on a pair of attributes, then a group-by on three attributes, and so on. This is the opposite of all techniques proposed earlier for computing the CUBE, and has an important practical advantage: BUC avoids computing the larger group-bys that do not meet minimum support. The pruning in BUC is similar to the pruning in the Apriori algorithm for association rules, except that BUC trades some pruning for locality of reference and reduced memory requirements. BUC uses the same pruning strategy when computing sparse, complete CUBEs.We present a thorough performance evaluation over a broad range of workloads. Our evaluation demonstrates that (in contrast to earlier assumptions) minimizing the aggregations or the number of sorts is not the most important aspect of the sparse CUBE problem. The pruning in BUC, combined with an efficient sort method, enables BUC to outperform all previous algorithms for sparse CUBEs, even for computing entire CUBEs, and to dramatically improve Iceberg-CUBE computation.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {359–370},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304214,
author = {Beyer, Kevin and Ramakrishnan, Raghu},
title = {Bottom-up Computation of Sparse and Iceberg CUBE},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304214},
doi = {10.1145/304181.304214},
abstract = {We introduce the Iceberg-CUBE problem as a reformulation of the datacube (CUBE) problem. The Iceberg-CUBE problem is to compute only those group-by partitions with an aggregate value (e.g., count) above some minimum support threshold. The result of Iceberg-CUBE can be used (1) to answer group-by queries with a clause such as HAVING COUNT(*) &gt;= X, where X is greater than the threshold, (2) for mining multidimensional association rules, and (3) to complement existing strategies for identifying interesting subsets of the CUBE for precomputation.We present a new algorithm (BUC) for Iceberg-CUBE computation. BUC builds the CUBE bottom-up; i.e., it builds the CUBE by starting from a group-by on a single attribute, then a group-by on a pair of attributes, then a group-by on three attributes, and so on. This is the opposite of all techniques proposed earlier for computing the CUBE, and has an important practical advantage: BUC avoids computing the larger group-bys that do not meet minimum support. The pruning in BUC is similar to the pruning in the Apriori algorithm for association rules, except that BUC trades some pruning for locality of reference and reduced memory requirements. BUC uses the same pruning strategy when computing sparse, complete CUBEs.We present a thorough performance evaluation over a broad range of workloads. Our evaluation demonstrates that (in contrast to earlier assumptions) minimizing the aggregations or the number of sorts is not the most important aspect of the sparse CUBE problem. The pruning in BUC, combined with an efficient sort method, enables BUC to outperform all previous algorithms for sparse CUBEs, even for computing entire CUBEs, and to dramatically improve Iceberg-CUBE computation.},
journal = {SIGMOD Rec.},
month = jun,
pages = {359–370},
numpages = {12}
}

@inproceedings{10.1145/304182.304215,
author = {Kotidis, Yannis and Roussopoulos, Nick},
title = {DynaMat: A Dynamic View Management System for Data Warehouses},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304215},
doi = {10.1145/304182.304215},
abstract = {Pre-computation and materialization of views with aggregate functions is a common technique in Data Warehouses. Due to the complex structure of the warehouse and the different profiles of the users who submit queries, there is need for tools that will automate the selection and management of the materialized data. In this paper we present DynaMat, a system that dynamically materializes information at multiple levels of granularity in order to match the demand (workload) but also takes into account the maintenance restrictions for the warehouse, such as down time to update the views and space availability. DynaMat unifies the view selection and the view maintenance problems under a single framework using a novel “goodness” measure for the materialized views. DynaMat constantly monitors incoming queries and materializes the best set of views subject to the space constraints. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window. We compare DynaMat against a system that is given all queries in advance and the pre-computed optimal static view selection. The comparison is made based on a new metric, the Detailed Cost Savings Ratio introduced for quantifying the benefits of view materialization against incoming queries. These experiments show that DynaMat's dynamic view selection outperforms the optimal static view selection and thus, any sub-optimal static algorithm that has appeared in the literature.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {371–382},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304215,
author = {Kotidis, Yannis and Roussopoulos, Nick},
title = {DynaMat: A Dynamic View Management System for Data Warehouses},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304215},
doi = {10.1145/304181.304215},
abstract = {Pre-computation and materialization of views with aggregate functions is a common technique in Data Warehouses. Due to the complex structure of the warehouse and the different profiles of the users who submit queries, there is need for tools that will automate the selection and management of the materialized data. In this paper we present DynaMat, a system that dynamically materializes information at multiple levels of granularity in order to match the demand (workload) but also takes into account the maintenance restrictions for the warehouse, such as down time to update the views and space availability. DynaMat unifies the view selection and the view maintenance problems under a single framework using a novel “goodness” measure for the materialized views. DynaMat constantly monitors incoming queries and materializes the best set of views subject to the space constraints. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window. We compare DynaMat against a system that is given all queries in advance and the pre-computed optimal static view selection. The comparison is made based on a new metric, the Detailed Cost Savings Ratio introduced for quantifying the benefits of view materialization against incoming queries. These experiments show that DynaMat's dynamic view selection outperforms the optimal static view selection and thus, any sub-optimal static algorithm that has appeared in the literature.},
journal = {SIGMOD Rec.},
month = jun,
pages = {371–382},
numpages = {12}
}

@inproceedings{10.1145/304182.304216,
author = {Labio, Wilburt Juan and Yerneni, Ramana and Garcia-Molina, Hector},
title = {Shrinking the Warehouse Update Window},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304216},
doi = {10.1145/304182.304216},
abstract = {Warehouse views need to be updated when source data changes. Due to the constantly increasing size of warehouses and the rapid rates of change, there is increasing pressure to reduce the time taken for updating the warehouse views. In this paper we focus on reducing this “update window” by minimizing the work required to compute and install a batch of updates. Various strategies have been proposed in the literature for updating a single warehouse view. These algorithms typically cannot be extended to come up with good strategies for updating an entire set of views. We develop an efficient algorithm that selects an optimal update strategy for any single warehouse view. Based on this algorithm, we develop an algorithm for selecting strategies to update a set of views. The performance of these algorithms is studied with experiments involving warehouse views based on TPC-D queries.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {383–394},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304216,
author = {Labio, Wilburt Juan and Yerneni, Ramana and Garcia-Molina, Hector},
title = {Shrinking the Warehouse Update Window},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304216},
doi = {10.1145/304181.304216},
abstract = {Warehouse views need to be updated when source data changes. Due to the constantly increasing size of warehouses and the rapid rates of change, there is increasing pressure to reduce the time taken for updating the warehouse views. In this paper we focus on reducing this “update window” by minimizing the work required to compute and install a batch of updates. Various strategies have been proposed in the literature for updating a single warehouse view. These algorithms typically cannot be extended to come up with good strategies for updating an entire set of views. We develop an efficient algorithm that selects an optimal update strategy for any single warehouse view. Based on this algorithm, we develop an algorithm for selecting strategies to update a set of views. The performance of these algorithms is studied with experiments involving warehouse views based on TPC-D queries.},
journal = {SIGMOD Rec.},
month = jun,
pages = {383–394},
numpages = {12}
}

@inproceedings{10.1145/304182.304217,
author = {Natsev, Apostol and Rastogi, Rajeev and Shim, Kyuseok},
title = {WALRUS: A Similarity Retrieval Algorithm for Image Databases},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304217},
doi = {10.1145/304182.304217},
abstract = {Traditional approaches for content-based image querying typically compute a single signature for each image based on color histograms, texture, wavelet tranforms etc., and return as the query result, images whose signatures are closest to the signature of the query image. Therefore, most traditional methods break down when images contain similar objects that are scaled differently or at different locations, or only certain regions of the image match.In this paper, we propose WALRUS (WAveLet-based Retrieval of User-specified Scenes), a novel similarity retrieval algorithm that is robust to scaling and translation of objects within an image. WALRUS employs a novel similarity model in which each image is first decomposed into its regions, and the similarity measure between a pair of images is then defined to be the fraction of the area of the two images covered by matching regions from the images. In order to extract regions for an image, WALRUS considers sliding windows of varying sizes and then clusters them based on the proximity of their signatures. An efficient dynamic programming algorithm is used to compute wavelet-based signatures for the sliding windows. Experimental results on real-life data sets corroborate the effectiveness of WALRUS's similarity model that performs similarity matching at a region rather than an image granularity.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {395–406},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304217,
author = {Natsev, Apostol and Rastogi, Rajeev and Shim, Kyuseok},
title = {WALRUS: A Similarity Retrieval Algorithm for Image Databases},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304217},
doi = {10.1145/304181.304217},
abstract = {Traditional approaches for content-based image querying typically compute a single signature for each image based on color histograms, texture, wavelet tranforms etc., and return as the query result, images whose signatures are closest to the signature of the query image. Therefore, most traditional methods break down when images contain similar objects that are scaled differently or at different locations, or only certain regions of the image match.In this paper, we propose WALRUS (WAveLet-based Retrieval of User-specified Scenes), a novel similarity retrieval algorithm that is robust to scaling and translation of objects within an image. WALRUS employs a novel similarity model in which each image is first decomposed into its regions, and the similarity measure between a pair of images is then defined to be the fraction of the area of the two images covered by matching regions from the images. In order to extract regions for an image, WALRUS considers sliding windows of varying sizes and then clusters them based on the proximity of their signatures. An efficient dynamic programming algorithm is used to compute wavelet-based signatures for the sliding windows. Experimental results on real-life data sets corroborate the effectiveness of WALRUS's similarity model that performs similarity matching at a region rather than an image granularity.},
journal = {SIGMOD Rec.},
month = jun,
pages = {395–406},
numpages = {12}
}

@inproceedings{10.1145/304182.304218,
author = {Aggarwal, Charu C. and Wolf, Joel L. and Yu, Philip S.},
title = {A New Method for Similarity Indexing of Market Basket Data},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304218},
doi = {10.1145/304182.304218},
abstract = {In recent years, many data mining methods have been proposed for finding useful and structured information from market basket data. The association rule model was recently proposed in order to discover useful patterns and dependencies in such data. This paper discusses a method for indexing market basket data efficiently for similarity search. The technique is likely to be very useful in applications which utilize the similarity in customer buying behavior in order to make peer recommendations. We propose an index called the signature table, which is very flexible in supporting a wide range of similarity functions. The construction of the index structure is independent of the similarity function, which can be specified at query time. The resulting similarity search algorithm shows excellent scalability with increasing memory availability and database size.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {407–418},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304218,
author = {Aggarwal, Charu C. and Wolf, Joel L. and Yu, Philip S.},
title = {A New Method for Similarity Indexing of Market Basket Data},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304218},
doi = {10.1145/304181.304218},
abstract = {In recent years, many data mining methods have been proposed for finding useful and structured information from market basket data. The association rule model was recently proposed in order to discover useful patterns and dependencies in such data. This paper discusses a method for indexing market basket data efficiently for similarity search. The technique is likely to be very useful in applications which utilize the similarity in customer buying behavior in order to make peer recommendations. We propose an index called the signature table, which is very flexible in supporting a wide range of similarity functions. The construction of the index structure is independent of the similarity function, which can be specified at query time. The resulting similarity search algorithm shows excellent scalability with increasing memory availability and database size.},
journal = {SIGMOD Rec.},
month = jun,
pages = {407–418},
numpages = {12}
}

@inproceedings{10.1145/304182.304219,
author = {Keim, Daniel A.},
title = {Efficient Geometry-Based Similarity Search of 3D Spatial Databases},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304219},
doi = {10.1145/304182.304219},
abstract = {Searching a database of 3D-volume objects for objects which are similar to a given 3D search object is an important problem which arises in number of database applications — for example, in Medicine and CAD. In this paper, we present a new geometry-based solution to the problem of searching for similar 3D-volume objects. The problem is motivated from a real application in the medical domain where volume similarity is used as a basis for surgery decisions. Our solution for an efficient similarity search on large databases of 3D volume objects is based on a new geometric index structure. The basic idea of our new approach is to use the concept of hierarchical approximations of the 3D objects to speed up the search process. We formally show the correctness of our new approach and introduce two instantiations of our general idea, which are based on cuboid and octree approximations. We finally provide a performance evaluation of our new index structure revealing significant performance improvements over existing approaches.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {419–430},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304219,
author = {Keim, Daniel A.},
title = {Efficient Geometry-Based Similarity Search of 3D Spatial Databases},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304219},
doi = {10.1145/304181.304219},
abstract = {Searching a database of 3D-volume objects for objects which are similar to a given 3D search object is an important problem which arises in number of database applications — for example, in Medicine and CAD. In this paper, we present a new geometry-based solution to the problem of searching for similar 3D-volume objects. The problem is motivated from a real application in the medical domain where volume similarity is used as a basis for surgery decisions. Our solution for an efficient similarity search on large databases of 3D volume objects is based on a new geometric index structure. The basic idea of our new approach is to use the concept of hierarchical approximations of the 3D objects to speed up the search process. We formally show the correctness of our new approach and introduce two instantiations of our general idea, which are based on cuboid and octree approximations. We finally provide a performance evaluation of our new index structure revealing significant performance improvements over existing approaches.},
journal = {SIGMOD Rec.},
month = jun,
pages = {419–430},
numpages = {12}
}

@inproceedings{10.1145/304182.304220,
author = {Deutsch, Alin and Fernandez, Mary and Suciu, Dan},
title = {Storing Semistructured Data with STORED},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304220},
doi = {10.1145/304182.304220},
abstract = {Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {431–442},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304220,
author = {Deutsch, Alin and Fernandez, Mary and Suciu, Dan},
title = {Storing Semistructured Data with STORED},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304220},
doi = {10.1145/304181.304220},
abstract = {Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.},
journal = {SIGMOD Rec.},
month = jun,
pages = {431–442},
numpages = {12}
}

@inproceedings{10.1145/304182.304221,
author = {Yerneni, Ramana and Li, Chen and Garcia-Molina, Hector and Ullman, Jeffrey},
title = {Computing Capabilities of Mediators},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304221},
doi = {10.1145/304182.304221},
abstract = {Existing data-integration systems based on the mediation architecture employ a variety of mechanisms to describe the query-processing capabilities of sources. However, these systems do not compute the capabilities of the mediators based on the capabilities of the sources they integrate. In this paper, we proposed a framework to capture a rich variety of query-processing capabilities of data sources and mediators. We present algorithms to compute the set of supported queries of a mediator, based on the capability limitations of its sources. Our algorithms take into consideration a variety of query-processing techniques employed by mediators to enhance the set of supported queries.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {443–454},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304221,
author = {Yerneni, Ramana and Li, Chen and Garcia-Molina, Hector and Ullman, Jeffrey},
title = {Computing Capabilities of Mediators},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304221},
doi = {10.1145/304181.304221},
abstract = {Existing data-integration systems based on the mediation architecture employ a variety of mechanisms to describe the query-processing capabilities of sources. However, these systems do not compute the capabilities of the mediators based on the capabilities of the sources they integrate. In this paper, we proposed a framework to capture a rich variety of query-processing capabilities of data sources and mediators. We present algorithms to compute the set of supported queries of a mediator, based on the capability limitations of its sources. Our algorithms take into consideration a variety of query-processing techniques employed by mediators to enhance the set of supported queries.},
journal = {SIGMOD Rec.},
month = jun,
pages = {443–454},
numpages = {12}
}

@inproceedings{10.1145/304182.304222,
author = {Papakonstantinou, Yannis and Vassalos, Vasilis},
title = {Query Rewriting for Semistructured Data},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304222},
doi = {10.1145/304182.304222},
abstract = {We address the problem of query rewriting for TSL, a language for querying semistructured data. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing containment mappings, the chase, and query composition — techniques that were developed for structured, relational data. We also develop an algorithm for equivalence checking of TSL queries.We show that the algorithm is sound and complete for TSL, i.e., it always finds every non-trivial TSL rewriting query of q, and we discuss its complexity. We extend the rewriting algorithm to use some forms of structural constraints (such as DTDs) and find more opportunities for query rewriting.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {455–466},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304222,
author = {Papakonstantinou, Yannis and Vassalos, Vasilis},
title = {Query Rewriting for Semistructured Data},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304222},
doi = {10.1145/304181.304222},
abstract = {We address the problem of query rewriting for TSL, a language for querying semistructured data. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing containment mappings, the chase, and query composition — techniques that were developed for structured, relational data. We also develop an algorithm for equivalence checking of TSL queries.We show that the algorithm is sound and complete for TSL, i.e., it always finds every non-trivial TSL rewriting query of q, and we discuss its complexity. We extend the rewriting algorithm to use some forms of structural constraints (such as DTDs) and find more opportunities for query rewriting.},
journal = {SIGMOD Rec.},
month = jun,
pages = {455–466},
numpages = {12}
}

@inproceedings{10.1145/304182.304223,
author = {Embley, D. W. and Jiang, Y. and Ng, Y.-K.},
title = {Record-Boundary Discovery in Web Documents},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304223},
doi = {10.1145/304182.304223},
abstract = {Extraction of information from unstructured or semistructured Web documents often requires a recognition and delimitation of records. (By “record” we mean a group of information relevant to some entity.) Without first chunking documents that contain multiple records according to record boundaries, extraction of record information will not likely succeed. In this paper we describe a heuristic approach to discovering record boundaries in Web documents. In our approach, we capture the structure of a document as a tree of nested HTML tags, locate the subtree containing the records of interest, identify candidate separator tags within the subtree using five independent heuristics, and select a consensus separator tag based on a combined heuristic. Our approach is fast (runs linearly for practical cases within the context of the larger data-extraction problem) and accurate (100% in the experiments we conducted).},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {467–478},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304223,
author = {Embley, D. W. and Jiang, Y. and Ng, Y.-K.},
title = {Record-Boundary Discovery in Web Documents},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304223},
doi = {10.1145/304181.304223},
abstract = {Extraction of information from unstructured or semistructured Web documents often requires a recognition and delimitation of records. (By “record” we mean a group of information relevant to some entity.) Without first chunking documents that contain multiple records according to record boundaries, extraction of record information will not likely succeed. In this paper we describe a heuristic approach to discovering record boundaries in Web documents. In our approach, we capture the structure of a document as a tree of nested HTML tags, locate the subtree containing the records of interest, identify candidate separator tags within the subtree using five independent heuristics, and select a consensus separator tag based on a combined heuristic. Our approach is fast (runs linearly for practical cases within the context of the larger data-extraction problem) and accurate (100% in the experiments we conducted).},
journal = {SIGMOD Rec.},
month = jun,
pages = {467–478},
numpages = {12}
}

@inproceedings{10.1145/304182.304224,
author = {Callan, Jamie and Connell, Margaret and Du, Aiqun},
title = {Automatic Discovery of Language Models for Text Databases},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304224},
doi = {10.1145/304182.304224},
abstract = {The proliferation of text databases within large organizations and on the Internet makes it difficult for a person to know which databases to search. Given language models that describe the contents of each database, a database selection algorithm such as GIOSS can provide assistance by automatically selecting appropriate databases for an information need. Current practice is that each database provides its language model upon request, but this cooperative approach has important limitations.This paper demonstrates that cooperation is not required. Instead, the database selection service can construct its own language models by sampling database contents via the normal process of running queries and retrieving documents. Although random sampling is not possible, it can be approximated with carefully selected queries. This sampling approach avoids the limitations that characterize the cooperative approach, and also enables additional capabilities. Experimental results demonstrate that accurate language models can be learned from a relatively small number of queries and documents.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {479–490},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304224,
author = {Callan, Jamie and Connell, Margaret and Du, Aiqun},
title = {Automatic Discovery of Language Models for Text Databases},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304224},
doi = {10.1145/304181.304224},
abstract = {The proliferation of text databases within large organizations and on the Internet makes it difficult for a person to know which databases to search. Given language models that describe the contents of each database, a database selection algorithm such as GIOSS can provide assistance by automatically selecting appropriate databases for an information need. Current practice is that each database provides its language model upon request, but this cooperative approach has important limitations.This paper demonstrates that cooperation is not required. Instead, the database selection service can construct its own language models by sampling database contents via the normal process of running queries and retrieving documents. Although random sampling is not possible, it can be approximated with carefully selected queries. This sampling approach avoids the limitations that characterize the cooperative approach, and also enables additional capabilities. Experimental results demonstrate that accurate language models can be learned from a relatively small number of queries and documents.},
journal = {SIGMOD Rec.},
month = jun,
pages = {479–490},
numpages = {12}
}

@inproceedings{10.1145/304182.304225,
author = {Davulcu, Hasan and Freire, Juliana and Kifer, Michael and Ramakrishnan, I. V.},
title = {A Layered Architecture for Querying Dynamic Web Content},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304225},
doi = {10.1145/304182.304225},
abstract = {The design of webbases, database systems for supporting Web-based applications, is currently an active area of research. In this paper, we propose a 3-year architecture for designing and implementing webbases for querying dynamic Web content(i.e., data that can only be extracted by filling out multiple forms). The lowest layer, virtual physical layer, provides navigation independence by shielding the user from the complexities associated with retrieving data from raw Web sources. Next, the traditional logical layer supports site independence. The top layer is analogous to the external schema layer in traditional databases.Within this architectural framework we address two problems unique to webbases — retrieving dynamic Web content in the virtual physical layer and querying of the external schema by the end user. The layered architecture makes it possible to automate data extraction to a much greater degree than in existing proposals. Wrappers for the virtual physical schema can be created semi-automatically, by asking the webbase designer to navigate through the sites of interest — we call this approach mapping by example. Thus, the webbase designer need not have expertise in the language that maps the physical schema to the raw Web (this should be contrasted to other approaches, which require expertise in various Web-enabled flavors of SQL). For the external schema layer, we propose a semantic extension of the universal relation interface. This interface provides powerful, yet reasonably simple, ad hoc querying capabilities for the end user compared to the currently prevailing “canned” form-based interfaces on the one hand or complex Web-enabling extensions of SQL on the other. Finally, we discuss the implementation of the proposed architecture.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {491–502},
numpages = {12},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304225,
author = {Davulcu, Hasan and Freire, Juliana and Kifer, Michael and Ramakrishnan, I. V.},
title = {A Layered Architecture for Querying Dynamic Web Content},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304225},
doi = {10.1145/304181.304225},
abstract = {The design of webbases, database systems for supporting Web-based applications, is currently an active area of research. In this paper, we propose a 3-year architecture for designing and implementing webbases for querying dynamic Web content(i.e., data that can only be extracted by filling out multiple forms). The lowest layer, virtual physical layer, provides navigation independence by shielding the user from the complexities associated with retrieving data from raw Web sources. Next, the traditional logical layer supports site independence. The top layer is analogous to the external schema layer in traditional databases.Within this architectural framework we address two problems unique to webbases — retrieving dynamic Web content in the virtual physical layer and querying of the external schema by the end user. The layered architecture makes it possible to automate data extraction to a much greater degree than in existing proposals. Wrappers for the virtual physical schema can be created semi-automatically, by asking the webbase designer to navigate through the sites of interest — we call this approach mapping by example. Thus, the webbase designer need not have expertise in the language that maps the physical schema to the raw Web (this should be contrasted to other approaches, which require expertise in various Web-enabled flavors of SQL). For the external schema layer, we propose a semantic extension of the universal relation interface. This interface provides powerful, yet reasonably simple, ad hoc querying capabilities for the end user compared to the currently prevailing “canned” form-based interfaces on the one hand or complex Web-enabling extensions of SQL on the other. Finally, we discuss the implementation of the proposed architecture.},
journal = {SIGMOD Rec.},
month = jun,
pages = {491–502},
numpages = {12}
}

@inproceedings{10.1145/304182.304226,
author = {Seshadri, Praveen},
title = {“Honey, I Shrunk the Database”: Footprint, Mobility, and Beyond},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304226},
doi = {10.1145/304182.304226},
abstract = {The small-scale mobile computing market (palmtops, handhelds, smartphones, smartcards, etc.) is showing the most explosive growth in the history of computing. Within the last year, most database vendors have announced plans to build “small footprint” versions of their DBMS products to run on these small and mobile platforms. What are the real database systems issues? What are the likely markets? What are the challenges, and the dangers? Every specific vendor seems to have a different answer to these questions. They all agree on only one thing: there is a tremendous opportunity here. Our motivation for this panel is to get a number of the central decision-makers into a room and debate the issues.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {503},
numpages = {1},
keywords = {small-footprint database, mobile computing},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304226,
author = {Seshadri, Praveen},
title = {“Honey, I Shrunk the Database”: Footprint, Mobility, and Beyond},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304226},
doi = {10.1145/304181.304226},
abstract = {The small-scale mobile computing market (palmtops, handhelds, smartphones, smartcards, etc.) is showing the most explosive growth in the history of computing. Within the last year, most database vendors have announced plans to build “small footprint” versions of their DBMS products to run on these small and mobile platforms. What are the real database systems issues? What are the likely markets? What are the challenges, and the dangers? Every specific vendor seems to have a different answer to these questions. They all agree on only one thing: there is a tremendous opportunity here. Our motivation for this panel is to get a number of the central decision-makers into a room and debate the issues.},
journal = {SIGMOD Rec.},
month = jun,
pages = {503},
numpages = {1},
keywords = {small-footprint database, mobile computing}
}

@inproceedings{10.1145/304182.304227,
author = {Garofalakis, Minos N. and Ramaswamy, Sridhar and Rastogi, Rajeev and Shim, Kyuseok},
title = {Of Crawlers, Portals, Mice, and Men: Is There More to Mining the Web?},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304227},
doi = {10.1145/304182.304227},
abstract = {The World Wide Web is rapidly emerging as an important medium for transacting commerce as well as for the dissemination of information related to a wide range of topics (e.g., business, government, recreation). According to most predictions, the majority of human information will be available on the Web in ten years. These huge amounts of data raise a grand challenge for the database community, namely, how to turn the Web into a more useful information utility. This is exactly the subject that will be addressed by this panel.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {504},
numpages = {1},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304227,
author = {Garofalakis, Minos N. and Ramaswamy, Sridhar and Rastogi, Rajeev and Shim, Kyuseok},
title = {Of Crawlers, Portals, Mice, and Men: Is There More to Mining the Web?},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304227},
doi = {10.1145/304181.304227},
abstract = {The World Wide Web is rapidly emerging as an important medium for transacting commerce as well as for the dissemination of information related to a wide range of topics (e.g., business, government, recreation). According to most predictions, the majority of human information will be available on the Web in ten years. These huge amounts of data raise a grand challenge for the database community, namely, how to turn the Web into a more useful information utility. This is exactly the subject that will be addressed by this panel.},
journal = {SIGMOD Rec.},
month = jun,
pages = {504},
numpages = {1}
}

@inproceedings{10.1145/304182.304228,
author = {\"{O}zsu, M. Tamer},
title = {Data Management Issues in Electronic Commerce},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304228},
doi = {10.1145/304182.304228},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {505},
numpages = {1},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304228,
author = {\"{O}zsu, M. Tamer},
title = {Data Management Issues in Electronic Commerce},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304228},
doi = {10.1145/304181.304228},
journal = {SIGMOD Rec.},
month = jun,
pages = {505},
numpages = {1}
}

@inproceedings{10.1145/304182.304229,
author = {D\"{u}llmann, Dirk},
title = {Petabyte Databases},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304229},
doi = {10.1145/304182.304229},
abstract = {This paper describes the use of Object-Database Management Systems (ODBMS)for the storage of High-Energy Physics (HEP) data.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {506},
numpages = {1},
keywords = {object-databases, high energy physics, very large databases},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304229,
author = {D\"{u}llmann, Dirk},
title = {Petabyte Databases},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304229},
doi = {10.1145/304181.304229},
abstract = {This paper describes the use of Object-Database Management Systems (ODBMS)for the storage of High-Energy Physics (HEP) data.},
journal = {SIGMOD Rec.},
month = jun,
pages = {506},
numpages = {1},
keywords = {very large databases, object-databases, high energy physics}
}

@inproceedings{10.1145/304182.304230,
author = {Mohan, C.},
title = {A Database Perspective on Lotus Domino/Notes},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304230},
doi = {10.1145/304182.304230},
abstract = {In this one-page summary, I introduce the database aspects of Lotus Domino/Notes. These database features are covered in detail in the corresponding SIGMOD99 tutorial available at www.almaden.ibm.com/u/mohan/domino_sigmod99.ps.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {507},
numpages = {1},
keywords = {ARIES, semi-structured data, Notes, replication, heterogeneous data access, recovery, groupware, logging, Lotus Domino},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304230,
author = {Mohan, C.},
title = {A Database Perspective on Lotus Domino/Notes},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304230},
doi = {10.1145/304181.304230},
abstract = {In this one-page summary, I introduce the database aspects of Lotus Domino/Notes. These database features are covered in detail in the corresponding SIGMOD99 tutorial available at www.almaden.ibm.com/u/mohan/domino_sigmod99.ps.},
journal = {SIGMOD Rec.},
month = jun,
pages = {507},
numpages = {1},
keywords = {Notes, groupware, heterogeneous data access, recovery, logging, semi-structured data, Lotus Domino, ARIES, replication}
}

@inproceedings{10.1145/304182.304231,
author = {Chakrabarti, Soumen},
title = {Hypertext Databases and Data Mining},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304231},
doi = {10.1145/304182.304231},
abstract = {The volume of unstructured text and hypertext data far exceeds that of structured data. Text and hypertext are used for digital libraries, product catalogs, reviews, newsgroups, medical reports, customer service reports, and the like. Currently measured in billions of dollars, the worldwide internet activity is expected to reach a trillion dollars by 2002. Database researchers have kept some cautious distance from this action. The goal of this tutorial is to expose database researchers to text and hypertext information retrieval (IR) and mining systems, and to discuss emerging issues in the overlapping areas of databases, hypertext, and data mining.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {508},
numpages = {1},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304231,
author = {Chakrabarti, Soumen},
title = {Hypertext Databases and Data Mining},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304231},
doi = {10.1145/304181.304231},
abstract = {The volume of unstructured text and hypertext data far exceeds that of structured data. Text and hypertext are used for digital libraries, product catalogs, reviews, newsgroups, medical reports, customer service reports, and the like. Currently measured in billions of dollars, the worldwide internet activity is expected to reach a trillion dollars by 2002. Database researchers have kept some cautious distance from this action. The goal of this tutorial is to expose database researchers to text and hypertext information retrieval (IR) and mining systems, and to discuss emerging issues in the overlapping areas of databases, hypertext, and data mining.},
journal = {SIGMOD Rec.},
month = jun,
pages = {508},
numpages = {1}
}

@inproceedings{10.1145/304182.304232,
author = {Hinneburg, Alexander and Keim, Daniel A.},
title = {Clustering Methods for Large Databases: From the Past to the Future},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304232},
doi = {10.1145/304182.304232},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {509},
numpages = {1},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304232,
author = {Hinneburg, Alexander and Keim, Daniel A.},
title = {Clustering Methods for Large Databases: From the Past to the Future},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304232},
doi = {10.1145/304181.304232},
journal = {SIGMOD Rec.},
month = jun,
pages = {509},
numpages = {1}
}

@inproceedings{10.1145/304182.304233,
author = {Suciu, Dan},
title = {Managing Web Data},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304233},
doi = {10.1145/304182.304233},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {510},
numpages = {1},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304233,
author = {Suciu, Dan},
title = {Managing Web Data},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304233},
doi = {10.1145/304181.304233},
journal = {SIGMOD Rec.},
month = jun,
pages = {510},
numpages = {1}
}

@inproceedings{10.1145/304182.304234,
author = {Carey, M. and Chamberlin, D. and Doole, D. and Rielau, S. and Mattos, N. and Narayanan, S. and Vance, B. and Swagerman, R.},
title = {O-O, What's Happening to DB2?},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304234},
doi = {10.1145/304182.304234},
abstract = {In this presentation, we will describe a collection of new object-relational features that have been added to IBM's DB2 Universal Database (UDB) system. The features to be described include support for structured types, object references, and hierarchies of typed tables and views. These features will be covered from the perspective of a database designer or end user. In addition to presenting the features presently available in DB2 UDB V5.2, which became available in Fall 1998, we will discuss the expected evolution and impact of this technology over time.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {511–512},
numpages = {2},
keywords = {DB2, SQL99, object-relational database systems, SQL3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304234,
author = {Carey, M. and Chamberlin, D. and Doole, D. and Rielau, S. and Mattos, N. and Narayanan, S. and Vance, B. and Swagerman, R.},
title = {O-O, What's Happening to DB2?},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304234},
doi = {10.1145/304181.304234},
abstract = {In this presentation, we will describe a collection of new object-relational features that have been added to IBM's DB2 Universal Database (UDB) system. The features to be described include support for structured types, object references, and hierarchies of typed tables and views. These features will be covered from the perspective of a database designer or end user. In addition to presenting the features presently available in DB2 UDB V5.2, which became available in Fall 1998, we will discuss the expected evolution and impact of this technology over time.},
journal = {SIGMOD Rec.},
month = jun,
pages = {511–512},
numpages = {2},
keywords = {SQL3, object-relational database systems, SQL99, DB2}
}

@inproceedings{10.1145/304182.304235,
author = {Krishnamurthy, Vishu and Banerjee, Sandeepan and Nori, Anil},
title = {Bringing Object-Relational Technology to the Mainstream},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304235},
doi = {10.1145/304182.304235},
abstract = {Over the last few years, Oracle has evolved its flagship relational database system into an Object-Relational system by adding an extensible type system, object storage, an object cache, an extensible query and indexing framework, support for multimedia datatypes, a server-based scalable Java virtual machine, as well as enhancing its SQL DDL and DML language. These extensions were done with the practical goal of bringing objects to mainstream use.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {513–514},
numpages = {2},
keywords = {AQ, interMedia, data cartidges, multimedia, SQL3, extensibility, iFS, object-relational},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304235,
author = {Krishnamurthy, Vishu and Banerjee, Sandeepan and Nori, Anil},
title = {Bringing Object-Relational Technology to the Mainstream},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304235},
doi = {10.1145/304181.304235},
abstract = {Over the last few years, Oracle has evolved its flagship relational database system into an Object-Relational system by adding an extensible type system, object storage, an object cache, an extensible query and indexing framework, support for multimedia datatypes, a server-based scalable Java virtual machine, as well as enhancing its SQL DDL and DML language. These extensions were done with the practical goal of bringing objects to mainstream use.},
journal = {SIGMOD Rec.},
month = jun,
pages = {513–514},
numpages = {2},
keywords = {iFS, SQL3, interMedia, extensibility, object-relational, multimedia, data cartidges, AQ}
}

@inproceedings{10.1145/304182.304236,
author = {Brown, Paul},
title = {Implementing the Spirit of SQL-99},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304236},
doi = {10.1145/304182.304236},
abstract = {This paper describes the current INFORMIX IDS/UD release (9.2 or Centaur) and compares and contrasts its functionality with the features of the SQL-99 language standard. INFORMIX and Illustra have been shipping DBMSs implementing the spirit of the SQL-99 standard for five years. In this paper, we review our experience working with ORDBMS technology, and argue that while SQL-99 is a huge improvement over SQL-92, substantial further work is necessary to make object-relational DBMSs truly useful. Specifically, we describe several interesting pieces of functionality unique to IDS/UD, and several dilemmas our customers have encountered that the standard does not address.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {515–518},
numpages = {4},
keywords = {SQL, INFORMIX, language standards, object-relational database},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304236,
author = {Brown, Paul},
title = {Implementing the Spirit of SQL-99},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304236},
doi = {10.1145/304181.304236},
abstract = {This paper describes the current INFORMIX IDS/UD release (9.2 or Centaur) and compares and contrasts its functionality with the features of the SQL-99 language standard. INFORMIX and Illustra have been shipping DBMSs implementing the spirit of the SQL-99 standard for five years. In this paper, we review our experience working with ORDBMS technology, and argue that while SQL-99 is a huge improvement over SQL-92, substantial further work is necessary to make object-relational DBMSs truly useful. Specifically, we describe several interesting pieces of functionality unique to IDS/UD, and several dilemmas our customers have encountered that the standard does not address.},
journal = {SIGMOD Rec.},
month = jun,
pages = {515–518},
numpages = {4},
keywords = {SQL, INFORMIX, object-relational database, language standards}
}

@inproceedings{10.1145/304182.304239,
author = {Baulier, J. and Bohannon, P. and Gogate, S. and Gupta, C. and Haldar, S.},
title = {DataBlitz Storage Manager: Main-Memory Database Performance for Critical Applications},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304239},
doi = {10.1145/304182.304239},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {519–520},
numpages = {2},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304239,
author = {Baulier, J. and Bohannon, P. and Gogate, S. and Gupta, C. and Haldar, S.},
title = {DataBlitz Storage Manager: Main-Memory Database Performance for Critical Applications},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304239},
doi = {10.1145/304181.304239},
journal = {SIGMOD Rec.},
month = jun,
pages = {519–520},
numpages = {2}
}

@inproceedings{10.1145/304182.304240,
author = {Ravi Kanth, K. V. and Ravada, Siva and Sharma, Jayant and Banerjee, Jay},
title = {Indexing Medium-Dimensionality Data in Oracle},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304240},
doi = {10.1145/304182.304240},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {521–522},
numpages = {2},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304240,
author = {Ravi Kanth, K. V. and Ravada, Siva and Sharma, Jayant and Banerjee, Jay},
title = {Indexing Medium-Dimensionality Data in Oracle},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304240},
doi = {10.1145/304181.304240},
journal = {SIGMOD Rec.},
month = jun,
pages = {521–522},
numpages = {2}
}

@inproceedings{10.1145/304182.304241,
author = {Kohler, Walt},
title = {EMC Information Sharing: Direct Access to MVS Data from UNIX and NT},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304241},
doi = {10.1145/304182.304241},
abstract = {In this extended abstract we briefly describe EMC's information sharing technology that enables UNIX and NT systems to directly access MVS mainframe datasets and how this technology can be used to directly access an MVS DB2 database.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {523–524},
numpages = {2},
keywords = {MVS, direct access, DB2, UNIX, NT, information sharing, database extractor},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304241,
author = {Kohler, Walt},
title = {EMC Information Sharing: Direct Access to MVS Data from UNIX and NT},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304241},
doi = {10.1145/304181.304241},
abstract = {In this extended abstract we briefly describe EMC's information sharing technology that enables UNIX and NT systems to directly access MVS mainframe datasets and how this technology can be used to directly access an MVS DB2 database.},
journal = {SIGMOD Rec.},
month = jun,
pages = {523–524},
numpages = {2},
keywords = {direct access, UNIX, DB2, database extractor, information sharing, MVS, NT}
}

@inproceedings{10.1145/304182.304242,
author = {Greer, Rick},
title = {Daytona and the Fourth-Generation Language Cymbal},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304242},
doi = {10.1145/304182.304242},
abstract = {The Daytona™ data management system is used by AT&amp;T to solve a wide spectrum of data management problems. For example, Daytona is managing a 4 terabyte data warehouse whose largest table contains over 10 billion rows. Daytona's architecture is based on translating its high-level query language Cymbal (which includes SQL as a subset) completely into C and then compiling that C into object code. The system resulting from this architecture is fast, powerful, easy to use and administer, reliable and open to UNIX™ tools. In particular, two forms of data compression plus robust horizontal partitioning enable Daytona to handle terabytes with ease.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {525–526},
numpages = {2},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304242,
author = {Greer, Rick},
title = {Daytona and the Fourth-Generation Language Cymbal},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304242},
doi = {10.1145/304181.304242},
abstract = {The Daytona™ data management system is used by AT&amp;T to solve a wide spectrum of data management problems. For example, Daytona is managing a 4 terabyte data warehouse whose largest table contains over 10 billion rows. Daytona's architecture is based on translating its high-level query language Cymbal (which includes SQL as a subset) completely into C and then compiling that C into object code. The system resulting from this architecture is fast, powerful, easy to use and administer, reliable and open to UNIX™ tools. In particular, two forms of data compression plus robust horizontal partitioning enable Daytona to handle terabytes with ease.},
journal = {SIGMOD Rec.},
month = jun,
pages = {525–526},
numpages = {2}
}

@inproceedings{10.1145/304182.304243,
author = {Hammond, Brad},
title = {Merge Replication in Microsoft's SQL Server 7.0},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304243},
doi = {10.1145/304182.304243},
abstract = {SQL Server 7.0 offers three different styles of replication that we call Transactional Replication, Snapshot Replication, and Merge Replication. Merge Replication means that data changes can be performed at any replica, and that the changes performed at multiple replicas are later merged together. Because Merge Replication allows updates to disconnected replicas, it is particularly well suited to applications that require a lot of autonomy. A special process called the Merge Agent propagates changes between replicas, filters data as appropriate, and detects and handles conflicts according to user-specified rules.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {527},
numpages = {1},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304243,
author = {Hammond, Brad},
title = {Merge Replication in Microsoft's SQL Server 7.0},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304243},
doi = {10.1145/304181.304243},
abstract = {SQL Server 7.0 offers three different styles of replication that we call Transactional Replication, Snapshot Replication, and Merge Replication. Merge Replication means that data changes can be performed at any replica, and that the changes performed at multiple replicas are later merged together. Because Merge Replication allows updates to disconnected replicas, it is particularly well suited to applications that require a lot of autonomy. A special process called the Merge Agent propagates changes between replicas, filters data as appropriate, and detects and handles conflicts according to user-specified rules.},
journal = {SIGMOD Rec.},
month = jun,
pages = {527},
numpages = {1}
}

@inproceedings{10.1145/304182.304244,
author = {TimesTen Team, CORPORATE},
title = {In-Memory Data Management for Consumer Transactions the Timesten Approach},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304244},
doi = {10.1145/304182.304244},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {528–529},
numpages = {2},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304244,
author = {TimesTen Team, CORPORATE},
title = {In-Memory Data Management for Consumer Transactions the Timesten Approach},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304244},
doi = {10.1145/304181.304244},
journal = {SIGMOD Rec.},
month = jun,
pages = {528–529},
numpages = {2}
}

@inproceedings{10.1145/304182.304245,
author = {Ojjeh, Bassel},
title = {Microsoft Site Server (Commerce Ed.): Talk-Slides Available at the Conference},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304245},
doi = {10.1145/304182.304245},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {530},
numpages = {1},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304245,
author = {Ojjeh, Bassel},
title = {Microsoft Site Server (Commerce Ed.): Talk-Slides Available at the Conference},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304245},
doi = {10.1145/304181.304245},
journal = {SIGMOD Rec.},
month = jun,
pages = {530},
numpages = {1}
}

@inproceedings{10.1145/304182.304246,
author = {Rajaraman, Anand},
title = {E-Commerce Database Issues and Experience: (Talk-Slides Available at at the Conference)},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304246},
doi = {10.1145/304182.304246},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {531},
numpages = {1},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304246,
author = {Rajaraman, Anand},
title = {E-Commerce Database Issues and Experience: (Talk-Slides Available at at the Conference)},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304246},
doi = {10.1145/304181.304246},
journal = {SIGMOD Rec.},
month = jun,
pages = {531},
numpages = {1}
}

@inproceedings{10.1145/304182.304248,
author = {Bergstraesser, Thomas and Bernstein, Philip A. and Pal, Shankar and Shutt, David},
title = {Versions and Workspaces in Microsoft Repository},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304248},
doi = {10.1145/304182.304248},
abstract = {This paper describes the version and workspace features of Microsoft Repository, a layer that implements fine-grained objects and relationships on top of Microsoft SQL Server. It supports branching and merging of versions, delta storage, checkout-checkin, and single-version views for version-unaware applications.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {532–533},
numpages = {2},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304248,
author = {Bergstraesser, Thomas and Bernstein, Philip A. and Pal, Shankar and Shutt, David},
title = {Versions and Workspaces in Microsoft Repository},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304248},
doi = {10.1145/304181.304248},
abstract = {This paper describes the version and workspace features of Microsoft Repository, a layer that implements fine-grained objects and relationships on top of Microsoft SQL Server. It supports branching and merging of versions, delta storage, checkout-checkin, and single-version views for version-unaware applications.},
journal = {SIGMOD Rec.},
month = jun,
pages = {532–533},
numpages = {2}
}

@inproceedings{10.1145/304182.304250,
author = {Do, Lyman and Ram, Prabhu and Drew, Pamela},
title = {The Need for Distributed Asynchronous Transactions},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304250},
doi = {10.1145/304182.304250},
abstract = {The theme of the paper is to promote research on asynchronous transactions. We discuss our experience of executing synchronous transactions on a large distributed production system in The Boeing Company. Due to the poor performance of synchronous transactions in our environment, it motivated the exploration of asynchronous transactions as an alternate solution. This paper presents the requirements and benefits/limitations of asynchronous transactions. Open issues related to large scale deployments of asynchronous transactions are also discussed.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {534–535},
numpages = {2},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304250,
author = {Do, Lyman and Ram, Prabhu and Drew, Pamela},
title = {The Need for Distributed Asynchronous Transactions},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304250},
doi = {10.1145/304181.304250},
abstract = {The theme of the paper is to promote research on asynchronous transactions. We discuss our experience of executing synchronous transactions on a large distributed production system in The Boeing Company. Due to the poor performance of synchronous transactions in our environment, it motivated the exploration of asynchronous transactions as an alternate solution. This paper presents the requirements and benefits/limitations of asynchronous transactions. Open issues related to large scale deployments of asynchronous transactions are also discussed.},
journal = {SIGMOD Rec.},
month = jun,
pages = {534–535},
numpages = {2}
}

@inproceedings{10.1145/304182.304568,
author = {Jarke, Matthias and Quix, Christoph and Blees, Guido and Lehmann, Dirk and Michalk, Gunter and Stierl, Stefan},
title = {Improving OLTP Data Quality Using Data Warehouse Mechanisms},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304568},
doi = {10.1145/304182.304568},
abstract = {Research and products for the integration of heterogeneous legacy source databases in data warehousing have addressed numerous data quality problems in or between the sources. Such a solution is marketed by Team4 for the decision support of mobile sales representatives, using advanced view maintenance and replication management techniques in an environment based on relational data warehouse technology and Lotus Notes-based client systems. However, considering total information supply chain management, the capture of poor operational data, to be cleaned later in the data warehouse, appears sub-optimal. Based on the observation that decision support clients are often closely linked to operational data entry, we have addressed the problem of mapping the data warehouse data quality techniques back to data quality measures for improving OLTP data. The solution requires a warehouse-to-OLTP workflow which employs a combination of view maintenance and view update techniques.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {536–537},
numpages = {2},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304568,
author = {Jarke, Matthias and Quix, Christoph and Blees, Guido and Lehmann, Dirk and Michalk, Gunter and Stierl, Stefan},
title = {Improving OLTP Data Quality Using Data Warehouse Mechanisms},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304568},
doi = {10.1145/304181.304568},
abstract = {Research and products for the integration of heterogeneous legacy source databases in data warehousing have addressed numerous data quality problems in or between the sources. Such a solution is marketed by Team4 for the decision support of mobile sales representatives, using advanced view maintenance and replication management techniques in an environment based on relational data warehouse technology and Lotus Notes-based client systems. However, considering total information supply chain management, the capture of poor operational data, to be cleaned later in the data warehouse, appears sub-optimal. Based on the observation that decision support clients are often closely linked to operational data entry, we have addressed the problem of mapping the data warehouse data quality techniques back to data quality measures for improving OLTP data. The solution requires a warehouse-to-OLTP workflow which employs a combination of view maintenance and view update techniques.},
journal = {SIGMOD Rec.},
month = jun,
pages = {536–537},
numpages = {2}
}

@inproceedings{10.1145/304182.304569,
author = {Trisolini, Stefano M. and Lenzerini, Maurizio and Nardi, Daniele},
title = {Data Integration and Warehousing in Telecom Italia},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304569},
doi = {10.1145/304182.304569},
abstract = {We discuss the main methodological and technological issues arosen in the last years in the development of the enterprise integrated database of Telecom Italia and, subsequently in the management of the primary data store for Telecom Italia data warehouse applications.The two efforts, although driven by different needs and requirements can be regarded as a continous development of an integrated view of the enterprise data. We review the experience accumulated in the integration of over 50 internal databases, highlighting the benefits and drawbacks of this scenario for data warehousing and discuss the development of a large dedicated data store to support the analysis of data about customers and phone traffic.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {538–539},
numpages = {2},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304569,
author = {Trisolini, Stefano M. and Lenzerini, Maurizio and Nardi, Daniele},
title = {Data Integration and Warehousing in Telecom Italia},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304569},
doi = {10.1145/304181.304569},
abstract = {We discuss the main methodological and technological issues arosen in the last years in the development of the enterprise integrated database of Telecom Italia and, subsequently in the management of the primary data store for Telecom Italia data warehouse applications.The two efforts, although driven by different needs and requirements can be regarded as a continous development of an integrated view of the enterprise data. We review the experience accumulated in the integration of over 50 internal databases, highlighting the benefits and drawbacks of this scenario for data warehousing and discuss the development of a large dedicated data store to support the analysis of data about customers and phone traffic.},
journal = {SIGMOD Rec.},
month = jun,
pages = {538–539},
numpages = {2}
}

@inproceedings{10.1145/304182.304570,
author = {Liu, Ling and Han, Wei and Buttler, David and Pu, Calton and Tang, Wei},
title = {An XJML-Based Wrapper Generator for Web Information Extraction},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304570},
doi = {10.1145/304182.304570},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {540–543},
numpages = {4},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304570,
author = {Liu, Ling and Han, Wei and Buttler, David and Pu, Calton and Tang, Wei},
title = {An XJML-Based Wrapper Generator for Web Information Extraction},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304570},
doi = {10.1145/304181.304570},
journal = {SIGMOD Rec.},
month = jun,
pages = {540–543},
numpages = {4}
}

@inproceedings{10.1145/304182.304571,
author = {Altinel, Mehmet and Aksoy, Demet and Baby, Thomas and Franklin, Michael and Shapiro, William and Zdonik, Stan},
title = {DBIS-Toolkit: Adaptable Middleware for Large Scale Data Delivery},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304571},
doi = {10.1145/304182.304571},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {544–546},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304571,
author = {Altinel, Mehmet and Aksoy, Demet and Baby, Thomas and Franklin, Michael and Shapiro, William and Zdonik, Stan},
title = {DBIS-Toolkit: Adaptable Middleware for Large Scale Data Delivery},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304571},
doi = {10.1145/304181.304571},
journal = {SIGMOD Rec.},
month = jun,
pages = {544–546},
numpages = {3}
}

@inproceedings{10.1145/304182.304572,
author = {Wolfson, Ouri and Sistla, Prasad and Xu, Bo and Zhou, Jutai and Chamberlain, Sam},
title = {DOMINO: Databases fOr MovINg Objects Tracking},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304572},
doi = {10.1145/304182.304572},
abstract = {Consider a database that represents information about moving objects and their location. For example, for a database representing the location of taxi-cabs a typical query may be: retrieve the free cabs that are currently within 1 mile of 33 N. Michigan Ave., Chicago (to pick-up a customer); or for a trucking company database a typical query may be: retrieve the trucks that are currently within 1 mile of truck ABT312 (which needs assistance); or for a database representing the current location of objects in a battlefield a typical query may be: retrieve the friendly helicopters that are in a given region, or, retrieve the friendly helicopters that are expected to enter the region within the next 10 minutes. The queries may originate from the moving objects, or from stationary users. We will refer to applications with the above characteristics as moving-objects-database (MOD) applications, and to queries as the ones mentioned above as MOD queries.In the military MOD applications arise in the context of the digital battlefield (see [1]), and in the civilian industry they arise in transportation systems. For example, Omnitracs developed by Qualcomm (see[2]) is a commercial system used by the transportation industry, which enables MOD functionality. It provides location management by connecting vehicles (e.g. trucks), via satellites, to company databases. The vehicles are equipped with a Global Positioning System (GPS), and they automatically and periodically report their location.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {547–549},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304572,
author = {Wolfson, Ouri and Sistla, Prasad and Xu, Bo and Zhou, Jutai and Chamberlain, Sam},
title = {DOMINO: Databases fOr MovINg Objects Tracking},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304572},
doi = {10.1145/304181.304572},
abstract = {Consider a database that represents information about moving objects and their location. For example, for a database representing the location of taxi-cabs a typical query may be: retrieve the free cabs that are currently within 1 mile of 33 N. Michigan Ave., Chicago (to pick-up a customer); or for a trucking company database a typical query may be: retrieve the trucks that are currently within 1 mile of truck ABT312 (which needs assistance); or for a database representing the current location of objects in a battlefield a typical query may be: retrieve the friendly helicopters that are in a given region, or, retrieve the friendly helicopters that are expected to enter the region within the next 10 minutes. The queries may originate from the moving objects, or from stationary users. We will refer to applications with the above characteristics as moving-objects-database (MOD) applications, and to queries as the ones mentioned above as MOD queries.In the military MOD applications arise in the context of the digital battlefield (see [1]), and in the civilian industry they arise in transportation systems. For example, Omnitracs developed by Qualcomm (see[2]) is a commercial system used by the transportation industry, which enables MOD functionality. It provides location management by connecting vehicles (e.g. trucks), via satellites, to company databases. The vehicles are equipped with a Global Positioning System (GPS), and they automatically and periodically report their location.},
journal = {SIGMOD Rec.},
month = jun,
pages = {547–549},
numpages = {3}
}

@inproceedings{10.1145/304182.304573,
author = {Braumandl, Reinhard and Kemper, Alfons and Kossmann, Donald},
title = {Database Patchwork on the Internet},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304573},
doi = {10.1145/304182.304573},
abstract = {Naturally, data processing requires three kinds of resources:
the data itself,the functionality (i.e. database operations) andthe machines on which to run the operations.Because of the Internet we believe that in the long run there will be alternative providers for all of these three resources for any given application. Data providers will bring more and more data and more and more different kinds of data to the net. Likewise, function providers will develop new methods to process and work with the data; e.g., function providers might develop new algorithms to compress data or to produce thumbnails out of large images and try to sell these on the Internet. It is also conceivable, that some people allow other people to use spare cycles of their idle machines in the Internet (as in the Condor system of the University of Wisconsin) or that some companies (cycle providers) even specialize on selling computing time to businesses that occasionally need to carry out very complex operations for which regular hardware is not sufficient.At the University of Passau, we are currently developing a distributed database system to be used in the Internet. The goal is to ultimately have a system which is able to run on any machine, manage any kind of data, import any kind of data from other systems and import any kind of database operations. The system is entirely written in Java. One of the most important features of the system is that it is capable of dynamically loading (external) query operators, written in Java and supplied by any function provider, and executing these query operators in concert with pre-defined and other external operators in order to evaluate a query. Compared to object-relational database systems, which allow to integrate external data and functionality by the means of extensions (datablades, extenders or cartridges) or heterogeneous database systems such as Garlic [MS97] or Tsimmis [GMPQ+97], our approach makes it possible to place external query operators anywhere in a query evaluation plan as opposed to restricting the placement of external operations to the “access level” of plans. It would, for example, be possible to make our system execute a completely new relational join method, if somebody finds a new join method which is worth-while implementing. Because our system is written in Java, it is highly portable and could be used by data, function and cycle providers with almost no effort. Furthermore, our query engine is, of course, completely distributed providing all the required infrastructure for server-server communication, name services, etc.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {550–552},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304573,
author = {Braumandl, Reinhard and Kemper, Alfons and Kossmann, Donald},
title = {Database Patchwork on the Internet},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304573},
doi = {10.1145/304181.304573},
abstract = {Naturally, data processing requires three kinds of resources:
the data itself,the functionality (i.e. database operations) andthe machines on which to run the operations.Because of the Internet we believe that in the long run there will be alternative providers for all of these three resources for any given application. Data providers will bring more and more data and more and more different kinds of data to the net. Likewise, function providers will develop new methods to process and work with the data; e.g., function providers might develop new algorithms to compress data or to produce thumbnails out of large images and try to sell these on the Internet. It is also conceivable, that some people allow other people to use spare cycles of their idle machines in the Internet (as in the Condor system of the University of Wisconsin) or that some companies (cycle providers) even specialize on selling computing time to businesses that occasionally need to carry out very complex operations for which regular hardware is not sufficient.At the University of Passau, we are currently developing a distributed database system to be used in the Internet. The goal is to ultimately have a system which is able to run on any machine, manage any kind of data, import any kind of data from other systems and import any kind of database operations. The system is entirely written in Java. One of the most important features of the system is that it is capable of dynamically loading (external) query operators, written in Java and supplied by any function provider, and executing these query operators in concert with pre-defined and other external operators in order to evaluate a query. Compared to object-relational database systems, which allow to integrate external data and functionality by the means of extensions (datablades, extenders or cartridges) or heterogeneous database systems such as Garlic [MS97] or Tsimmis [GMPQ+97], our approach makes it possible to place external query operators anywhere in a query evaluation plan as opposed to restricting the placement of external operations to the “access level” of plans. It would, for example, be possible to make our system execute a completely new relational join method, if somebody finds a new join method which is worth-while implementing. Because our system is written in Java, it is highly portable and could be used by data, function and cycle providers with almost no effort. Furthermore, our query engine is, of course, completely distributed providing all the required infrastructure for server-server communication, name services, etc.},
journal = {SIGMOD Rec.},
month = jun,
pages = {550–552},
numpages = {3}
}

@inproceedings{10.1145/304182.304574,
author = {Rundensteiner, E. A. and Koeller, A. and Zhang, X. and Lee, A. J. and Nica, A. and Van Wyk, A. and Lee, Y.},
title = {Evolvable View Environment (EVE): Non-Equivalent View Maintenance under Schema Changes},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304574},
doi = {10.1145/304182.304574},
abstract = {Supporting independent ISs and integrating them in distributed data warehouses (materialized views) is becoming more important with the growth of the WWW. However, views defined over autonomous ISs are susceptible to schema changes. In the EVE project we are developing techniques to support the maintenance of data warehouses defined over distributed dynamic ISs [5, 6, 7]. The EVE system is the first to allow views to survive schema changes of their underlying ISs while also adapting to changing data in those sources. EVE achieves this is two steps: applying view query rewriting algorithms that exploit information about alternative ISs and the information they contain, and incrementally adapting the view extent to the view definition changes. Those processes are referred to as view synchronization and view adaption, respectively. They increase the survivability of materialized views in changing environments and reduce the necessity of human interaction in system maintenance.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {553–555},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304574,
author = {Rundensteiner, E. A. and Koeller, A. and Zhang, X. and Lee, A. J. and Nica, A. and Van Wyk, A. and Lee, Y.},
title = {Evolvable View Environment (EVE): Non-Equivalent View Maintenance under Schema Changes},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304574},
doi = {10.1145/304181.304574},
abstract = {Supporting independent ISs and integrating them in distributed data warehouses (materialized views) is becoming more important with the growth of the WWW. However, views defined over autonomous ISs are susceptible to schema changes. In the EVE project we are developing techniques to support the maintenance of data warehouses defined over distributed dynamic ISs [5, 6, 7]. The EVE system is the first to allow views to survive schema changes of their underlying ISs while also adapting to changing data in those sources. EVE achieves this is two steps: applying view query rewriting algorithms that exploit information about alternative ISs and the information they contain, and incrementally adapting the view extent to the view definition changes. Those processes are referred to as view synchronization and view adaption, respectively. They increase the survivability of materialized views in changing environments and reduce the necessity of human interaction in system maintenance.},
journal = {SIGMOD Rec.},
month = jun,
pages = {553–555},
numpages = {3}
}

@inproceedings{10.1145/304182.304575,
author = {Ng, Raymond and Lakshmanan, Laks V. S. and Han, Jiawei and Mah, Teresa},
title = {Exploratory Mining via Constrained Frequent Set Queries},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304575},
doi = {10.1145/304182.304575},
abstract = {Although there have been many studies on data mining, to date there have been few research prototypes or commercial systems supporting comprehensive query-driven mining, which encourages interactive exploration of the data. Our thesis is that constraint constructs and the optimization they induce play a pivotal role in mining queries, thus substantially enhancing the usefulness and performance of the mining system. This is based on the analogy of declarative query languages like SQL and query optimization which have made relational databases so successful. To this end, our proposed demo is not yet another data mining system, but of a new paradigm in data mining - mining with constraints, as the important first step towards supporting ad-hoc mining in DBMS.In this demo, we will show a prototype exploratory mining system that implements constraint-based mining query optimization methods proposed in [5]. We will demonstrate how a user can interact with the system for exploratory data mining and how efficiently the system may execute optimized data mining queries. The prototype system will include all the constraint pushing techniques for mining association rules outlined in [5], and will include additional capabilities for mining other kinds of rules for which the computation of constrained frequent sets forms the core first step.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {556–558},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304575,
author = {Ng, Raymond and Lakshmanan, Laks V. S. and Han, Jiawei and Mah, Teresa},
title = {Exploratory Mining via Constrained Frequent Set Queries},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304575},
doi = {10.1145/304181.304575},
abstract = {Although there have been many studies on data mining, to date there have been few research prototypes or commercial systems supporting comprehensive query-driven mining, which encourages interactive exploration of the data. Our thesis is that constraint constructs and the optimization they induce play a pivotal role in mining queries, thus substantially enhancing the usefulness and performance of the mining system. This is based on the analogy of declarative query languages like SQL and query optimization which have made relational databases so successful. To this end, our proposed demo is not yet another data mining system, but of a new paradigm in data mining - mining with constraints, as the important first step towards supporting ad-hoc mining in DBMS.In this demo, we will show a prototype exploratory mining system that implements constraint-based mining query optimization methods proposed in [5]. We will demonstrate how a user can interact with the system for exploratory data mining and how efficiently the system may execute optimized data mining queries. The prototype system will include all the constraint pushing techniques for mining association rules outlined in [5], and will include additional capabilities for mining other kinds of rules for which the computation of constrained frequent sets forms the core first step.},
journal = {SIGMOD Rec.},
month = jun,
pages = {556–558},
numpages = {3}
}

@inproceedings{10.1145/304182.304576,
author = {Adelberg, Brad and Denny, Matthew},
title = {Nodose Version 2.0},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304576},
doi = {10.1145/304182.304576},
abstract = {This paper describes a tool, called Nodose, we have developed to expedite the creation of robust wrappers. Nodose allows non-programmers to build components that can convert data from the source format to XML or another generic format. Further, the generated code performs a set of statistical checks at runtime that attempt to find extraction errors before they are propogated back to users.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {559–561},
numpages = {3},
keywords = {wrapper generation, data extraction, error detection},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304576,
author = {Adelberg, Brad and Denny, Matthew},
title = {Nodose Version 2.0},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304576},
doi = {10.1145/304181.304576},
abstract = {This paper describes a tool, called Nodose, we have developed to expedite the creation of robust wrappers. Nodose allows non-programmers to build components that can convert data from the source format to XML or another generic format. Further, the generated code performs a set of statistical checks at runtime that attempt to find extraction errors before they are propogated back to users.},
journal = {SIGMOD Rec.},
month = jun,
pages = {559–561},
numpages = {3},
keywords = {error detection, wrapper generation, data extraction}
}

@inproceedings{10.1145/304182.304577,
author = {Barga, Roger and Lomet, David B.},
title = {Phoenix: Making Applications Robust},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304577},
doi = {10.1145/304182.304577},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {562–564},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304577,
author = {Barga, Roger and Lomet, David B.},
title = {Phoenix: Making Applications Robust},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304577},
doi = {10.1145/304181.304577},
journal = {SIGMOD Rec.},
month = jun,
pages = {562–564},
numpages = {3}
}

@inproceedings{10.1145/304182.304578,
author = {Li, Wen-Syan and Vu, Quoc and Chang, Edward and Agrawal, Divyakant and Hirata, Kyoji and Mukherjea, Sougata and Wu, Yi-Leh and Bufi, Corey and Chang, Chen-Chuan Kevin and Hara, Yoshinori and Ito, Reiko and Kimura, Yutaka and Shimazu, Kezuyuki and Saito, Yukiyoshi},
title = {PowerBookmarks: A System for Personalizable Web Information Organization, Sharing, and Management},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304578},
doi = {10.1145/304182.304578},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {565–567},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304578,
author = {Li, Wen-Syan and Vu, Quoc and Chang, Edward and Agrawal, Divyakant and Hirata, Kyoji and Mukherjea, Sougata and Wu, Yi-Leh and Bufi, Corey and Chang, Chen-Chuan Kevin and Hara, Yoshinori and Ito, Reiko and Kimura, Yutaka and Shimazu, Kezuyuki and Saito, Yukiyoshi},
title = {PowerBookmarks: A System for Personalizable Web Information Organization, Sharing, and Management},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304578},
doi = {10.1145/304181.304578},
journal = {SIGMOD Rec.},
month = jun,
pages = {565–567},
numpages = {3}
}

@inproceedings{10.1145/304182.304579,
author = {Rundensteiner, E. A. and Claypool, K. and Li, M. and Chen, L. and Zhang, Z. and Natarajan, C. and Jin, J. and De Lima, S. and Weiner, S.},
title = {SERF: ODMG-Based Generic Re-Structuring Facility},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304579},
doi = {10.1145/304182.304579},
abstract = {The age of information management and with it the advent of increasingly sophisticated technologies have kindled a need in the database community and others to re-structure existing systems and move forward to make use of these new technologies. Legacy application systems are being transformed to newer state-of-the-art systems, information sources are being mapped from one data model to another, a diversity of data sources are being transformed to load, cleanse and consolidate data into modern data-warehouses [CR99].Re-structuring is thus a critical task for a variety of applications. For this reason, most object-oriented database systems (OODB) today support some form of re-structuring support [Tec94, Obj93, BKKK87]. This existing support of current OODBs [BKKK87, Tec94, Obj93] is limited to a pre-defined taxonomy of simple fixed-semantic schema evolution operations. However, such simple changes, typically to individual types only, are not sufficient for many advanced applications [Br\'{e}96]. More radical changes, such as combining two types of redefining the relationship between two types, are either very difficult or even impossible to achieve with current commercial database technology [Tec94, Obj93]. In fact, most OODBs would typically require the user to write ad-hoc programs to accomplish such transformations. Research that has begun to look into the issue of complex changes [Br\'{e}96, Ler96] is still limited by providing a fixed set of some selected (even if now more complex) operations.To address these limitations of the current restructuring technology, we have proposed the SERF framework which aims at providing a rich environment for doing complex user-defined transformations flexibly, easily and correctly [CJR98b].  The goal of our work is to increase the usability and utility of the SERF framework and its applicability to re-structuring problems beyond OODB evolution. Towards that end, we provide re-usable transformations via the notion of SERF Templates that can be packaged into libraries, thereby increasing the portability of these transformations. We also now have a first cut at providing an assurance of consistency for the users of this system, a semantic optimizer that provides some performance improvements via enhanced query optimization techniques with emphasis on the re-structuring primitives [CNR99]. In this demo we give an overview of the SERF framework, its current status and the enhancements that are planned for the future. We also present an example of the application of SERF to a domain other than schema evolution, i.e., the web restructuring.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {568–570},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304579,
author = {Rundensteiner, E. A. and Claypool, K. and Li, M. and Chen, L. and Zhang, Z. and Natarajan, C. and Jin, J. and De Lima, S. and Weiner, S.},
title = {SERF: ODMG-Based Generic Re-Structuring Facility},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304579},
doi = {10.1145/304181.304579},
abstract = {The age of information management and with it the advent of increasingly sophisticated technologies have kindled a need in the database community and others to re-structure existing systems and move forward to make use of these new technologies. Legacy application systems are being transformed to newer state-of-the-art systems, information sources are being mapped from one data model to another, a diversity of data sources are being transformed to load, cleanse and consolidate data into modern data-warehouses [CR99].Re-structuring is thus a critical task for a variety of applications. For this reason, most object-oriented database systems (OODB) today support some form of re-structuring support [Tec94, Obj93, BKKK87]. This existing support of current OODBs [BKKK87, Tec94, Obj93] is limited to a pre-defined taxonomy of simple fixed-semantic schema evolution operations. However, such simple changes, typically to individual types only, are not sufficient for many advanced applications [Br\'{e}96]. More radical changes, such as combining two types of redefining the relationship between two types, are either very difficult or even impossible to achieve with current commercial database technology [Tec94, Obj93]. In fact, most OODBs would typically require the user to write ad-hoc programs to accomplish such transformations. Research that has begun to look into the issue of complex changes [Br\'{e}96, Ler96] is still limited by providing a fixed set of some selected (even if now more complex) operations.To address these limitations of the current restructuring technology, we have proposed the SERF framework which aims at providing a rich environment for doing complex user-defined transformations flexibly, easily and correctly [CJR98b].  The goal of our work is to increase the usability and utility of the SERF framework and its applicability to re-structuring problems beyond OODB evolution. Towards that end, we provide re-usable transformations via the notion of SERF Templates that can be packaged into libraries, thereby increasing the portability of these transformations. We also now have a first cut at providing an assurance of consistency for the users of this system, a semantic optimizer that provides some performance improvements via enhanced query optimization techniques with emphasis on the re-structuring primitives [CNR99]. In this demo we give an overview of the SERF framework, its current status and the enhancements that are planned for the future. We also present an example of the application of SERF to a domain other than schema evolution, i.e., the web restructuring.},
journal = {SIGMOD Rec.},
month = jun,
pages = {568–570},
numpages = {3}
}

@inproceedings{10.1145/304182.304580,
author = {Zhou, Tong and Liu, Ling and Pu, Calton},
title = {TAM: A System for Dynamic Transactional Activity Management},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304580},
doi = {10.1145/304182.304580},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {571–573},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304580,
author = {Zhou, Tong and Liu, Ling and Pu, Calton},
title = {TAM: A System for Dynamic Transactional Activity Management},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304580},
doi = {10.1145/304181.304580},
journal = {SIGMOD Rec.},
month = jun,
pages = {571–573},
numpages = {3}
}

@inproceedings{10.1145/304182.304581,
author = {Acharya, Swarup and Gibbons, Phillip B. and Poosala, Viswanath and Ramaswamy, Sridhar},
title = {The Aqua Approximate Query Answering System},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304581},
doi = {10.1145/304182.304581},
abstract = {Aqua is a system for providing fast, approximate answers to aggregate queries, which are very common in OLAP applications. It has been designed to run on top of any commercial relational DBMS. Aqua precomputes synopses (special statistical summaries) of the original data and stores them in the DBMS. It provides approximate answers along with quality guarantees by rewriting the queries to run on these synopses. Finally, Aqua keeps the synopses up-to-date as the database changes, using fast incremental maintenance techniques.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {574–576},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304581,
author = {Acharya, Swarup and Gibbons, Phillip B. and Poosala, Viswanath and Ramaswamy, Sridhar},
title = {The Aqua Approximate Query Answering System},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304581},
doi = {10.1145/304181.304581},
abstract = {Aqua is a system for providing fast, approximate answers to aggregate queries, which are very common in OLAP applications. It has been designed to run on top of any commercial relational DBMS. Aqua precomputes synopses (special statistical summaries) of the original data and stores them in the DBMS. It provides approximate answers along with quality guarantees by rewriting the queries to run on these synopses. Finally, Aqua keeps the synopses up-to-date as the database changes, using fast incremental maintenance techniques.},
journal = {SIGMOD Rec.},
month = jun,
pages = {574–576},
numpages = {3}
}

@inproceedings{10.1145/304182.304582,
author = {Brodsky, Alexander and Segal, Victor E. and Chen, Jia and Exarkhopoulo, Paval A.},
title = {The CCUBE Constraint Object-Oriented Database System},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304582},
doi = {10.1145/304182.304582},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {577–579},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304582,
author = {Brodsky, Alexander and Segal, Victor E. and Chen, Jia and Exarkhopoulo, Paval A.},
title = {The CCUBE Constraint Object-Oriented Database System},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304582},
doi = {10.1145/304181.304582},
journal = {SIGMOD Rec.},
month = jun,
pages = {577–579},
numpages = {3}
}

@inproceedings{10.1145/304182.304583,
author = {Bonnet, Phillippe and Buza, Kyle and Chan, Zhiyuan and Cheng, Victor and Chung, Randolph and Hickey, Takako and Kennedy, Ryan and Mahashin, Daniel and Mayr, Tobias and Oprencak, Ivan and Seshadri, Praveen and Siu, Hubert},
title = {The Cornell Jaguar Project: Adding Mobility to PREDATOR},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304583},
doi = {10.1145/304182.304583},
abstract = {The Cornell Jaguar Project is exploring a variety of issues related to mobility and query processing. One broad theme is to break down the traditional client and server boundaries, leading to ubiquitous query processing. Another theme is to extend database and query processing techniques to small-scale and mobile devices. The project builds on and extends the Cornell PREDATOR database engine.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {580–581},
numpages = {2},
keywords = {mobile computing, ubiquitous query processing},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304583,
author = {Bonnet, Phillippe and Buza, Kyle and Chan, Zhiyuan and Cheng, Victor and Chung, Randolph and Hickey, Takako and Kennedy, Ryan and Mahashin, Daniel and Mayr, Tobias and Oprencak, Ivan and Seshadri, Praveen and Siu, Hubert},
title = {The Cornell Jaguar Project: Adding Mobility to PREDATOR},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304583},
doi = {10.1145/304181.304583},
abstract = {The Cornell Jaguar Project is exploring a variety of issues related to mobility and query processing. One broad theme is to break down the traditional client and server boundaries, leading to ubiquitous query processing. Another theme is to extend database and query processing techniques to small-scale and mobile devices. The project builds on and extends the Cornell PREDATOR database engine.},
journal = {SIGMOD Rec.},
month = jun,
pages = {580–581},
numpages = {2},
keywords = {mobile computing, ubiquitous query processing}
}

@inproceedings{10.1145/304182.304584,
author = {Roussopoulos, Nick and Kotidis, Yannis and Sismanis, Yannis},
title = {The Active MultiSync Controller of the Cubetree Storage Organization},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304584},
doi = {10.1145/304182.304584},
abstract = {The Cubetree Storage Organization (CSO)1 logically and physically clusters materialized-views data, multi-dimensional indices on them, and computed aggregate values all in one compact and tight storage structure that uses a fraction of the conventional table-based space. This is a breakthrough technology for storing and accessing multi-dimensional data in terms of storage reduction, query performance and incremental bulk update speed. CSO has been extended with an Active MultiSync controller for synchronizing multiple concurrent access and continuous asynchronous online updates for a non-stop data warehouse.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {582–583},
numpages = {2},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304584,
author = {Roussopoulos, Nick and Kotidis, Yannis and Sismanis, Yannis},
title = {The Active MultiSync Controller of the Cubetree Storage Organization},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304584},
doi = {10.1145/304181.304584},
abstract = {The Cubetree Storage Organization (CSO)1 logically and physically clusters materialized-views data, multi-dimensional indices on them, and computed aggregate values all in one compact and tight storage structure that uses a fraction of the conventional table-based space. This is a breakthrough technology for storing and accessing multi-dimensional data in terms of storage reduction, query performance and incremental bulk update speed. CSO has been extended with an Active MultiSync controller for synchronizing multiple concurrent access and continuous asynchronous online updates for a non-stop data warehouse.},
journal = {SIGMOD Rec.},
month = jun,
pages = {582–583},
numpages = {2}
}

@inproceedings{10.1145/304182.304585,
author = {B\"{o}hlen, Michael and Bukauskas, Linas and Dyreson, Curtis},
title = {The Jungle Database Search Engine},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304585},
doi = {10.1145/304182.304585},
abstract = {Information spread in in databases cannot be found by current search engines. A database search engine is capable to access and advertise database on the WWW. Jungle is a database search engine prototype developed at Aalborg University. Operating through JDBC connections to remote databases, Jungle extracts and indexes database data and meta-data, building a data store of database information. This information is used to evaluate and optimize queries in the AQUA query language. AQUA is a natural and intuitive database query language that helps users to search for information without knowing how that information is structured. This paper gives an overview of AQUA and describes the implementation of Jungle.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {584–586},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304585,
author = {B\"{o}hlen, Michael and Bukauskas, Linas and Dyreson, Curtis},
title = {The Jungle Database Search Engine},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304585},
doi = {10.1145/304181.304585},
abstract = {Information spread in in databases cannot be found by current search engines. A database search engine is capable to access and advertise database on the WWW. Jungle is a database search engine prototype developed at Aalborg University. Operating through JDBC connections to remote databases, Jungle extracts and indexes database data and meta-data, building a data store of database information. This information is used to evaluate and optimize queries in the AQUA query language. AQUA is a natural and intuitive database query language that helps users to search for information without knowing how that information is structured. This paper gives an overview of AQUA and describes the implementation of Jungle.},
journal = {SIGMOD Rec.},
month = jun,
pages = {584–586},
numpages = {3}
}

@inproceedings{10.1145/304182.304586,
author = {Vossen, Gottfried and Weske, Mathias},
title = {The WASA2 Object-Oriented Workflow Management System},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304586},
doi = {10.1145/304182.304586},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {587–589},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304586,
author = {Vossen, Gottfried and Weske, Mathias},
title = {The WASA2 Object-Oriented Workflow Management System},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304586},
doi = {10.1145/304181.304586},
journal = {SIGMOD Rec.},
month = jun,
pages = {587–589},
numpages = {3}
}

@inproceedings{10.1145/304182.304587,
author = {Cruz, Isabel F. and James, Kimberly M.},
title = {A User-Centered Interface for Querying Distributed Multimedia Databases},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304587},
doi = {10.1145/304182.304587},
abstract = {Facilitating information retrieval in the vastly growing realm of digital media has become increasingly difficult. DelaunayMM seeks to assist all users in finding relevant information through an interactive interface that supports pre- and post-query refinement, and a customizable multimedia information display. This project leverages the strengths of visual query languages with a resourceful framework to provide users with a single intuitive interface. The interface and its supporting framework are described in this paper.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {590–592},
numpages = {3},
keywords = {distributed database access, multimedia querying, customizable user interfaces},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304587,
author = {Cruz, Isabel F. and James, Kimberly M.},
title = {A User-Centered Interface for Querying Distributed Multimedia Databases},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304587},
doi = {10.1145/304181.304587},
abstract = {Facilitating information retrieval in the vastly growing realm of digital media has become increasingly difficult. DelaunayMM seeks to assist all users in finding relevant information through an interactive interface that supports pre- and post-query refinement, and a customizable multimedia information display. This project leverages the strengths of visual query languages with a resourceful framework to provide users with a single intuitive interface. The interface and its supporting framework are described in this paper.},
journal = {SIGMOD Rec.},
month = jun,
pages = {590–592},
numpages = {3},
keywords = {multimedia querying, distributed database access, customizable user interfaces}
}

@inproceedings{10.1145/304182.304589,
author = {Bouguettaya, Athman and Benatallah, Boualem and Hendra, Lily and Beard, James and Smith, Kevin and Quzzani, Mourad},
title = {World Wide Database—Integrating the Web, CORBA and Databases},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304589},
doi = {10.1145/304182.304589},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {594–596},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304589,
author = {Bouguettaya, Athman and Benatallah, Boualem and Hendra, Lily and Beard, James and Smith, Kevin and Quzzani, Mourad},
title = {World Wide Database—Integrating the Web, CORBA and Databases},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304589},
doi = {10.1145/304181.304589},
journal = {SIGMOD Rec.},
month = jun,
pages = {594–596},
numpages = {3}
}

@inproceedings{10.1145/304182.304590,
author = {Baru, Chaitan and Gupta, Amarnath and Lud\"{a}scher, Bertram and Marciano, Richard and Papakonstantinou, Yannis and Velikhov, Pavel and Chu, Vincent},
title = {XML-Based Information Mediation with MIX},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304590},
doi = {10.1145/304182.304590},
abstract = {The MIX mediator system, MIXm, is developed as part of the MIX Project at the San Diego Supercomputer Center, and the University of California, San Diego.1 MIXm uses XML as the common model for data exchange. Mediator views are expressed in XMAS (XML Matching And Structuring Language), a declarative XML query language. To facilitate user-friendly query formulation and for optimization purposes, MIXm employs XML DTDs as a structural description (in effect, a “schema”) of the exchanged data. The novel features of the system include:
Data exchange and integration solely relies on XML, i.e., instance and schema information is represented by XML documents and XML DTDs, respectively. XML queries are denoted in XMAS, which builds upon ideas of languages like XML-QL, MSL, Yat, and UnQL. Additionally, XMAS features powerful grouping and order constructs for generating new integrated XML “objects” from existing ones.The graphical user interface BBQ (Blended Browsing and Querying) is driven by the mediator view DTD and integrates browsing and querying of XML data. Complex queries can be constructed in an intuitive way, resembling QBE. Due to the nested nature of XML data and DTDs, BBQ provides graphical means to specify the nesting and grouping of query results.Query evaluation can be demand-driven, i.e., by the user's navigation into the mediated view.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {597–599},
numpages = {3},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304590,
author = {Baru, Chaitan and Gupta, Amarnath and Lud\"{a}scher, Bertram and Marciano, Richard and Papakonstantinou, Yannis and Velikhov, Pavel and Chu, Vincent},
title = {XML-Based Information Mediation with MIX},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304590},
doi = {10.1145/304181.304590},
abstract = {The MIX mediator system, MIXm, is developed as part of the MIX Project at the San Diego Supercomputer Center, and the University of California, San Diego.1 MIXm uses XML as the common model for data exchange. Mediator views are expressed in XMAS (XML Matching And Structuring Language), a declarative XML query language. To facilitate user-friendly query formulation and for optimization purposes, MIXm employs XML DTDs as a structural description (in effect, a “schema”) of the exchanged data. The novel features of the system include:
Data exchange and integration solely relies on XML, i.e., instance and schema information is represented by XML documents and XML DTDs, respectively. XML queries are denoted in XMAS, which builds upon ideas of languages like XML-QL, MSL, Yat, and UnQL. Additionally, XMAS features powerful grouping and order constructs for generating new integrated XML “objects” from existing ones.The graphical user interface BBQ (Blended Browsing and Querying) is driven by the mediator view DTD and integrates browsing and querying of XML data. Complex queries can be constructed in an intuitive way, resembling QBE. Due to the nested nature of XML data and DTDs, BBQ provides graphical means to specify the nesting and grouping of query results.Query evaluation can be demand-driven, i.e., by the user's navigation into the mediated view.},
journal = {SIGMOD Rec.},
month = jun,
pages = {597–599},
numpages = {3}
}

