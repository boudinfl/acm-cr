@inproceedings{10.1145/342009.335372,
author = {Han, Jiawei and Pei, Jian and Yin, Yiwen},
title = {Mining Frequent Patterns without Candidate Generation},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335372},
doi = {10.1145/342009.335372},
abstract = {Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist prolific patterns and/or long patterns.In this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {1–12},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335375,
author = {Riedel, Erik and Faloutsos, Christos and Ganger, Gregory R. and Nagle, David F.},
title = {Data Mining on an OLTP System (Nearly) for Free},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335375},
doi = {10.1145/342009.335375},
abstract = {This paper proposes a scheme for scheduling disk requests that takes advantage of the ability of high-level functions to operate directly at individual disk drives. We show that such a scheme makes it possible to support a Data Mining workload on an OLTP system almost for free: there is only a small impact on the throughput and response time of the existing workload. Specifically, we show that an OLTP system has the disk resources to consistently provide one third of its sequential bandwidth to a background Data Mining task with close to zero impact on OLTP throughput and response time at high transaction loads. At low transaction loads, we show much lower impact than observed in previous work. This means that a production OLTP system can be used for Data Mining tasks without the expense of a second dedicated system. Our scheme takes advantage of close interaction with the on-disk scheduler by reading blocks for the Data Mining workload as the disk head “passes over” them while satisfying demand blocks from the OLTP request stream. We show that this scheme provides a consistent level of throughput for the background workload even at very high foreground loads. Such a scheme is of most benefit in combination with an Active Disk environment that allows the background Data Mining application to also take advantage of the processing power and memory available directly on the disk drives.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {13–21},
numpages = {9},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335376,
author = {Shenoy, Pradeep and Haritsa, Jayant R. and Sudarshan, S. and Bhalotia, Gaurav and Bawa, Mayank and Shah, Devavrat},
title = {Turbo-Charging Vertical Mining of Large Databases},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335376},
doi = {10.1145/342009.335376},
abstract = {In a vertical representation of a market-basket database, each item is associated with a column of values representing the transactions in which it is present. The association-rule mining algorithms that have been recently proposed for this representation show performance improvements over their classical horizontal counterparts, but are either efficient only for certain database sizes, or assume particular characteristics of the database contents, or are applicable only to specific kinds of database schemas. We present here a new vertical mining algorithm called VIPER, which is general-purpose, making no special requirements of the underlying database. VIPER stores data in compressed bit-vectors called “snakes” and integrates a number of novel optimizations for efficient snake generation, intersection, counting and storage. We analyze the performance of VIPER for a range of synthetic database workloads. Our experimental results indicate significant performance gains, especially for large databases, over previously proposed vertical and horizontal mining algorithms. In fact, there are even workload regions where VIPER outperforms an optimal, but practically infeasible, horizontal mining algorithm.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {22–33},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335378,
author = {Lomet, David B.},
title = {High Speed On-Line Backup When Using Logical Log Operations},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335378},
doi = {10.1145/342009.335378},
abstract = {Media recovery protects a database from failures of the stable medium by maintaining an extra copy of the database, called the backup, and a media recovery log. When a failure occurs, the database is “restored” from the backup, and the media recovery log is used to roll forward the database to the desired time, usually the current time. Backup must be both fast and “on-line”, i.e. concurrent with on-going update activity. Conventional online backup sequentially copies from the stable database, almost independent of the database cache manager, but requires page-oriented log operations. But results of logical operations must be flushed to a stable database (a backup is a stable database) in a constrained order to guarantee recovery. This order is not naturally achieved for the backup by a cache manager concerned only with crash recovery. We describe a “full speed” backup, only loosely coupled to the cache manager, and hence similar to current online backups, but effective for general logical log operations. This requires additional logging of cached objects to guarantee media recoverability. We then show how logging can be greatly reduced when log operations have a constrained form which nonetheless provides very useful additional logging efficiency for database systems.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {34–45},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335379,
author = {Labio, Wilburt Juan and Wiener, Janet L. and Garcia-Molina, Hector and Gorelik, Vlad},
title = {Efficient Resumption of Interrupted Warehouse Loads},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335379},
doi = {10.1145/342009.335379},
abstract = {Data warehouses collect large quantities of data from distributed sources into a single repository. A typical load to create or maintain a warehouse processes GBs of data, takes hours or even days to execute, and involves many complex and user-defined transformations of the data (e.g., find duplicates, resolve data inconsistencies, and add unique keys). If the load fails, a possible approach is to “redo” the entire load. A better approach is to resume the incomplete load from where it was interrupted. Unfortunately, traditional algorithms for resuming the load either impose unacceptable overhead during normal operation, or rely on the specifics of transformations. We develop a resumption algorithm called DR that imposes no overhead and relies only on the high-level properties of the transformations. We show that DR can lead to a ten-fold reduction in resumption time by performing experiments using commercial software.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {46–57},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335381,
author = {Lakhamraju, Mohana K. and Rastogi, Rajeev and Seshadri, S. and Sudarshan, S.},
title = {On-Line Reorganization in Object Databases},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335381},
doi = {10.1145/342009.335381},
abstract = {Reorganization of objects in an object databases is an important component of several operations like compaction, clustering, and schema evolution. The high availability requirements (24 \texttimes{} 7 operation) of certain application domains requires reorganization to be performed on-line with minimal interference to concurrently executing transactions.In this paper, we address the problem of on-line reorganization in object databases, where a set of objects have to be migrated from one location to another. Specifically, we consider the case where objects in the database may contain physical references to other objects. Relocating an object in this case involves finding the set of objects (parents) that refer to it, and modifying the references in each parent. We propose an algorithm called the Incremental Reorganization Algorithm (IRA) that achieves the above task with minimal interference to concurrently executing transactions. The IRA algorithm holds locks on at most two distinct objects at any point of time. We have implemented IRA on Brahma, a storage manager developed at IIT Bombay, and conducted an extensive performance study. Our experiments reveal that IRA makes on-line reorganization feasible, with very little impact on the response times of concurrently executing transactions and on overall system throughput. We also describe how the IRA algorithm can handle system failures.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {58–69},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335383,
author = {Aggarwal, Charu C. and Yu, Philip S.},
title = {Finding Generalized Projected Clusters in High Dimensional Spaces},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335383},
doi = {10.1145/342009.335383},
abstract = {High dimensional data has always been a challenge for clustering algorithms because of the inherent sparsity of the points. Recent research results indicate that in high dimensional data, even the concept of proximity or clustering may not be meaningful. We discuss very general techniques for projected clustering which are able to construct clusters in arbitrarily aligned subspaces of lower dimensionality. The subspaces are specific to the clusters themselves. This definition is substantially more general and realistic than currently available techniques which limit the method to only projections from the original set of attributes. The generalized projected clustering technique may also be viewed as a way of trying to redefine clustering for high dimensional applications by searching for hidden subspaces with clusters which are created by inter-attribute correlations. We provide a new concept of using extended cluster feature vectors in order to make the algorithm scalable for very large databases. The running time and space requirements of the algorithm are adjustable, and are likely ta tradeoff with better accuracy.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {70–81},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335384,
author = {Palmer, Christopher R. and Faloutsos, Christos},
title = {Density Biased Sampling: An Improved Method for Data Mining and Clustering},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335384},
doi = {10.1145/342009.335384},
abstract = {Data mining in large data sets often requires a sampling or summarization step to form an in-core representation of the data that can be processed more efficiently. Uniform random sampling is frequently used in practice and also frequently criticized because it will miss small clusters. Many natural phenomena are known to follow Zipf's distribution and the inability of uniform sampling to find small clusters is of practical concern. Density Biased Sampling is proposed to probabilistically under-sample dense regions and over-sample light regions. A weighted sample is used to preserve the densities of the original data. Density biased sampling naturally includes uniform sampling as a special case. A memory efficient algorithm is proposed that approximates density biased sampling using only a single scan of the data. We empirically evaluate density biased sampling using synthetic data sets that exhibit varying cluster size distributions finding up to a factor of six improvement over uniform sampling.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {82–92},
numpages = {11},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335388,
author = {Breunig, Markus M. and Kriegel, Hans-Peter and Ng, Raymond T. and Sander, J\"{o}rg},
title = {LOF: Identifying Density-Based Local Outliers},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335388},
doi = {10.1145/342009.335388},
abstract = {For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using real-world datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {93–104},
numpages = {12},
keywords = {database mining, outlier detection},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335390,
author = {Zaharioudakis, Markos and Cochrane, Roberta and Lapis, George and Pirahesh, Hamid and Urata, Monica},
title = {Answering Complex SQL Queries Using Automatic Summary Tables},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335390},
doi = {10.1145/342009.335390},
abstract = {We investigate the problem of using materialized views to answer SQL queries. We focus on modern decision-support queries, which involve joins, arithmetic operations and other (possibly user-defined) functions, aggregation (often along multiple dimensions), and nested subqueries. Given the complexity of such queries, the vast amounts of data upon which they operate, and the requirement for interactive response times, the use of materialized views (MVs) of similar complexity is often mandatory for acceptable performance. We present a novel algorithm that is able to rewrite a user query so that it will access one or more of the available MVs instead of the base tables. The algorithm extends prior work by addressing the new sources of complexity mentioned above, that is, complex expressions, multidimensional aggregation, and nested subqueries. It does so by relying on a graphical representation of queries and a bottom-up, pair-wise matching of nodes from the query and MV graphs. This approach offers great modularity and extensibility, allowing for the rewriting of a large class of queries.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {105–116},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335391,
author = {Cho, Junghoo and Garcia-Molina, Hector},
title = {Synchronizing a Database to Improve Freshness},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335391},
doi = {10.1145/342009.335391},
abstract = {In this paper we study how to refresh a local copy of an autonomous data source to maintain the copy up-to-date. As the size of the data grows, it becomes more difficult to maintain the copy  fresh, “making it crucial to synchronize the copy effectively. We define two freshness metrics, change models of the underlying data, and synchronization policies. We analytically study how effective the various policies are. We also experimentally verify our analysis, based on data collected from 270 web sites for more than 4 months, and we show that our new policy improves the  freshness” very significantly compared to current policies in use.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {117–128},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335393,
author = {Salem, Kenneth and Beyer, Kevin and Lindsay, Bruce and Cochrane, Roberta},
title = {How to Roll a Join: Asynchronous Incremental View Maintenance},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335393},
doi = {10.1145/342009.335393},
abstract = {Incremental refresh of a materialized join view is often less expensive than a full, non-incremental refresh. However, it is still a potentially costly atomic operation. This paper presents an algorithm that performs incremental view maintenance as a series of small, asynchronous steps. The size of each step can be controlled to limit contention between the refresh process and concurrent operations that access the materialized view or the underlying relations. The algorithm supports point-in-time refresh, which allows a materialized view to be refreshed to any time between the last refresh and the present.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {129–140},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335395,
author = {Christophides, Vassilis and Cluet, Sophie and Sim\`{e}on, J\'{e}rundefinedme},
title = {On Wrapping Query Languages and Efficient XML Integration},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335395},
doi = {10.1145/342009.335395},
abstract = {Modern applications (Web portals, digital libraries, etc.) require integrated access to various information sources (from traditional DBMS to semistructured Web repositories), fast deployment and low maintenance cost in a rapidly evolving environment. Because of its flexibility, there is an increasing interest in using XML as a middleware model for such applications. XML enables fast wrapping and declarative integration. However, query processing in XML-based integration systems is still penalized by the lack of an algebra with adequate optimization properties and the difficulty to understand source query capabilities. In this paper, we propose an algebraic approach to support efficient XML query evaluation. We define a general purpose algebra suitable for semistructured on XML query languages. We show how this algebra can be used, with appropriate type information, to also wrap more structured query languages such as OQL or SQL. Finally, we develop new optimization techniques for XML-based integration systems.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {141–152},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335405,
author = {Liefke, Hartmut and Suciu, Dan},
title = {XMill: An Efficient Compressor for XML Data},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335405},
doi = {10.1145/342009.335405},
abstract = {We describe a tool for compressing XML data, with applications in data exchange and archiving, which usually achieves about twice the compression ratio of gzip at roughly the same speed. The compressor, called XMill, incorporates and combines existing compressors in order to apply them to heterogeneous XML data: it uses zlib, the library function for gzip, a collection of datatype specific compressors for simple data types, and, possibly, user defined compressors for application specific data types.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {153–164},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335409,
author = {Garofalakis, Minos and Gionis, Aristides and Rastogi, Rajeev and Seshadri, S. and Shim, Kyuseok},
title = {XTRACT: A System for Extracting Document Type Descriptors from XML Documents},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335409},
doi = {10.1145/342009.335409},
abstract = {XML is rapidly emerging as the new standard for data representation and exchange on the Web. An XML document can be accompanied by a Document Type Descriptor (DTD) which plays the role of a schema for an XML data collection. DTDs contain valuable information on the structure of documents and thus have a crucial role in the efficient storage of XML data, as well as the effective formulation and optimization of XML queries. In this paper, we propose XTRACT, a novel system for inferring a DTD schema for a database of XML documents. Since the DTD syntax incorporates the full expressive power of regular expressions, naive approaches typically fail to produce concise and intuitive DTDs. Instead, the XTRACT inference algorithms employ a sequence of sophisticated steps that involve: (1) finding patterns in the input sequences and replacing them with regular expressions to generate “general” candidate DTDs, (2) factoring candidate DTDs using adaptations of algorithms from the logic optimization literature, and (3) applying the Minimum Description Length (MDL) principle to find the best DTD among the candidates. The results of our experiments with real-life and synthetic DTDs demonstrate the effectiveness of XTRACT's approach in inferring concise and semantically meaningful DTD schemas for XML databases.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {165–176},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335412,
author = {Faloutsos, Christos and Seeger, Bernhard and Traina, Agma and Traina, Caetano},
title = {Spatial Join Selectivity Using Power Laws},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335412},
doi = {10.1145/342009.335412},
abstract = {We discovered a surprising law governing the spatial join selectivity across two sets of points. An example of such a spatial join is “find the libraries that are within 10 miles of schools”. Our law dictates that the number of such qualifying pairs follows a power law, whose exponent we call “pair-count exponent” (PC). We show that this law also holds for self-spatial-joins (“find schools within 5 miles of other schools”) in addition to the general case that the two point-sets are distinct. Our law holds for many real datasets, including diverse environments (geographic datasets, feature vectors from biology data, galaxy data from astronomy).In addition, we introduce the concept of the Box-Occupancy-Product-Sum (BOPS) plot, and we show that it can compute the pair-count exponent in a timely manner, reducing the run time by orders of magnitude, from quadratic to linear. Due to the pair-count exponent and our analysis (Law 1), we can achieve accurate selectivity estimates in constant time (O(1)) without the need for sampling or other expensive operations. The relative error in selectivity is about 30% with our fast BOPS method, and even better (about 10%), if we use the slower, quadratic method.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {177–188},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335414,
author = {Corral, Antonio and Manolopoulos, Yannis and Theodoridis, Yannis and Vassilakopoulos, Michael},
title = {Closest Pair Queries in Spatial Databases},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335414},
doi = {10.1145/342009.335414},
abstract = {This paper addresses the problem of finding the K closest pairs between two spatial data sets, where each set is stored in a structure belonging in the R-tree family. Five different algorithms (four recursive and one iterative) are presented for solving this problem. The case of 1 closest pair is treated as a special case. An extensive study, based on experiments performed with synthetic as well as with real point data sets, is presented. A wide range of values for the basic parameters affecting the performance of the algorithms, especially the effect of overlap between the two data sets, is explored. Moreover, an algorithmic as well as an experimental comparison with existing incremental algorithms addressing the same problem is presented. In most settings, the new algorithms proposed clearly outperform the existing ones.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {189–200},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335415,
author = {Korn, Flip and Muthukrishnan, S.},
title = {Influence Sets Based on Reverse Nearest Neighbor Queries},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335415},
doi = {10.1145/342009.335415},
abstract = {Inherent in the operation of many decision support and continuous referral systems is the notion of the “influence” of a data point on the database. This notion arises in examples such as finding the set of customers affected by the opening of a new store outlet location, notifying the subset of subscribers to a digital library who will find a newly added document most relevant, etc. Standard approaches to determining the influence set of a data point involve range searching and nearest neighbor queries.In this paper, we formalize a novel notion of influence based on reverse neighbor queries and its variants. Since the nearest neighbor relation is not symmetric, the set of points that are closest to a query point (i.e., the nearest neighbors) differs from the set of points that have the query point as their nearest neighbor (called the reverse nearest neighbors). Influence sets based on reverse nearest neighbor (RNN) queries seem to capture the intuitive notion of influence from our motivating examples.We present a general approach for solving RNN queries and an efficient R-tree based method for large data sets, based on this approach. Although the RNN query appears to be natural, it has not been studied previously. RNN queries are of independent interest, and as such should be part of the suite of available queries for processing spatial and multimedia data. In our experiments with real geographical data, the proposed method appears to scale logarithmically, whereas straightforward sequential scan scales linearly. Our experimental study also shows that approaches based on range searching or nearest neighbors are ineffective at finding influence sets of our interest.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {201–212},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335413,
author = {Rodr\'{\i}guez-Mart\'{\i}nez, Manuel and Roussopoulos, Nick},
title = {MOCHA: A Self-Extensible Database Middleware System for Distributed Data Sources},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335413},
doi = {10.1145/342009.335413},
abstract = {We present MOCHA, a new self-extensible database middleware system designed to interconnect distributed data sources. MOCHA is designed to scale to large environments and is based on the idea that some of the user-defined functionality in the system should be deployed by the middleware system itself. This is realized by shipping Java code implementing either advanced data types or tailored query operators to remote data sources and have it executed remotely. Optimized query plans push the evaluation of powerful data-reducing operators to the data source sites while executing data-inflating operators near the client's site. The Volume Reduction Factor is a new and more explicit metric introduced in this paper to select the best site to execute query operators and is shown to be more accurate than the standard selectivity factor alone. MOCHA has been implemented in Java and runs on top of Informix and Oracle. We present the architecture of MOCHA, the ideas behind it, and a performance study using scientific data and queries. The results of this study demonstrate that MOCHA provides a more flexible, scalable and efficient framework for distributed query processing compared to those in existing middleware solutions.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {213–224},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335416,
author = {Lee, Mong Li and Kitsuregawa, Masaru and Ooi, Beng Chin and Tan, Kian-Lee and Mondal, Anirban},
title = {Towards Self-Tuning Data Placement in Parallel Database Systems},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335416},
doi = {10.1145/342009.335416},
abstract = {Parallel database systems are increasingly being deployed to support the performance demands of end-users. While declustering data across multiple nodes facilitates parallelism, initial data placement may not be optimal due to skewed workloads and changing access patterns. To prevent performance degradation, the placement of data must be reorganized, and this must be done on-line to minimize disruption to the system.In this paper, we consider a dynamic self-tuning approach to reorganization in a shared nothing system. We introduce a new index-based method that faciliates fast and efficient migration of data. Our solution incorporates a globally height-balanced structure and load tracking at different levels of granularity. We conducted an extensive performance study, and implemented the methods on the Fujitsu AP3000 machine. Both the simulation and empirical results demonstratic that our proposed method is indeed scalable and effective in correcting any deterioration in system throughput.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {225–236},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335418,
author = {Litwin, Witold and Schwarz, Thomas},
title = {LH*RS: A High-Availability Scalable Distributed Data Structure Using Reed Solomon Codes},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335418},
doi = {10.1145/342009.335418},
abstract = {LH*RS is a new high-availability Scalable Distributed Data Structure (SDDS). The data storage scheme and the search performance of LH*RS are basically these of LH*. LH*RS manages in addition the parity information to tolerate the unavailability of k ⪈ 1 server sites. The value of k scales with the file, to prevent the reliability decline. The parity calculus uses the Reed -Solomon Codes. The storage and access performance overheads to provide the high-availability are about the smallest possible. The scheme should prove attractive to data-intensive applications.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {237–248},
numpages = {12},
keywords = {high-availability, SDDS, scalable, Reed-Solomon Codes},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335419,
author = {Roy, Prasan and Seshadri, S. and Sudarshan, S. and Bhobe, Siddhesh},
title = {Efficient and Extensible Algorithms for Multi Query Optimization},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335419},
doi = {10.1145/342009.335419},
abstract = {Complex queries are becoming commonplace, with the growing use of decision support systems. These complex queries often have a lot of common sub-expressions, either within a single query, or across multiple such queries run as a batch. Multiquery optimization aims at exploiting common sub-expressions to reduce evaluation cost. Multi-query optimization has hither-to been viewed as impractical, since earlier algorithms were exhaustive, and explore a doubly exponential search space.In this paper we demonstrate that multi-query optimization using heuristics is practical, and provides significant benefits. We propose three cost-based heuristic algorithms: Volcano-SH and Volcano-RU, which are based on simple modifications to the Volcano search strategy, and a greedy heuristic. Our greedy heuristic incorporates novel optimizations that improve efficiency greatly. Our algorithms are designed to be easily added to existing optimizers. We present a performance study comparing the algorithms, using workloads consisting of queries from the TPC-D benchmark. The study shows that our algorithms provide significant benefits over traditional optimization, at a very acceptable overhead in optimization time.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {249–260},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335420,
author = {Avnur, Ron and Hellerstein, Joseph M.},
title = {Eddies: Continuously Adaptive Query Processing},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335420},
doi = {10.1145/342009.335420},
abstract = {In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates promising results, with eddies performing nearly as well as a static optimizer/executor in static scenarios, and providing dramatic improvements in dynamic execution environments.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {261–272},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335421,
author = {Popa, Lucian and Deutsch, Alin and Sahuguet, Arnaud and Tannen, Val},
title = {A Chase Too Far?},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335421},
doi = {10.1145/342009.335421},
abstract = {In a previous paper we proposed a novel method for generating alternative query plans that uses chasing (and back-chasing) with logical constraints. The method brings together use of indexes, use of materialized views, semantic optimization and join elimination (minimization). Each of these techniques is known separately to be beneficial to query optimization. The novelty of our approach is in allowing these techniques to interact systematically, eg. non-trivial use of indexes and materialized views may be enabled only by semantic constraints.We have implemented our method for a variety of schemas and queries. We examine how far we can push the method in term of complexity of both schemas and queries. We propose a technique for reducing the size of the search space by “stratifying” the sets of constraints used in the (back)chase. The experimental results demonstrate that our method is practical (i.e., feasible and worthwhile).},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {273–284},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335422,
author = {Goldman, Roy and Widom, Jennifer},
title = {WSQ/DSQ: A Practical Approach for Combined Querying of Databases and the Web},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335422},
doi = {10.1145/342009.335422},
abstract = {We present WSQ/DSQ (pronounced “wisk-disk”), a new approach for combining the query facilities of traditional databases with existing search engines on the Web. WSQ, for Web-Supported (Database) Queries, leverages results from Web searches to enhance SQL queries over a relational database. DSQ, for Database-Supported (Web) Queries, uses information stored in the database to enhance and explain Web searches. This paper focuses primarily on WSQ, describing a simple, low-overhead way to support WSQ in a relational DBMS, and demonstrating the utility of WSQ with a number of interesting queries and results. The queries supported by WSQ are enabled by two virtual tables, whose tuples represent Web search results generated dynamically during query execution. WSQ query execution may involve many high-latency calls to one or more search engines, during which the query processor is idle. We present a lightweight technique called asynchronous iteration that can be integrated easily into a standard sequential query processor to enable concurrency between query processing and multiple Web search requests. Asynchronous iteration has broader applications than WSQ alone, and it opens up many interesting query optimization issues. We have developed a prototype implementation of WSQ by extending a DBMS with virtual tables and asynchronous iteration; performance results are reported.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {285–296},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335423,
author = {Agrawal, Rakesh and Wimmers, Edward L.},
title = {A Framework for Expressing and Combining Preferences},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335423},
doi = {10.1145/342009.335423},
abstract = {The advent of the World Wide Web has created an explosion in the available on-line information. As the range of potential choices expand, the time and effort required to sort through them also expands. We propose a formal framework for expressing and combining user preferences to address this problem. Preferences can be used to focus search queries and to order the search results. A preference is expressed by the user for an entity which is described by a set of named fields; each field can take on values from a certain type. The * symbol may be used to match any element of that type. A set of preferences can be combined using a generic combine operator which is instantiated with a value function, thus providing a great deal of flexibility. Same preferences can be combined in more than one way and a combination of preferences yields another preference thus providing the closure property. We demonstrate the power of our framework by illustrating how a currently popular personalization system and a real-life application can be realized as special cases of our framework. We also discuss implementation of the framework in a relational setting.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {297–306},
numpages = {10},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335424,
author = {Barclay, Tom and Gray, Jim and Slutz, Don},
title = {Microsoft TerraServer: A Spatial Data Warehouse},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335424},
doi = {10.1145/342009.335424},
abstract = {Microsoft® TerraServer stores aerial, satellite, and topographic images of the earth in a SQL database available via the Internet. It is the world's largest online atlas, combining eight terabytes of image data from the United States Geological Survey (USGS) and SPIN-2. Internet browsers provide intuitive spatial and text interfaces to the data. Users need no special hardware, software, or knowledge to locate and browse imagery. This paper describes how terabytes of “Internet unfriendly” geo-spatial images were scrubbed and edited into hundreds of millions of “Internet friendly” image tiles and loaded into a SQL data warehouse. All meta-data and imagery are stored in the SQL database.TerraServer demonstrates that general-purpose relational database technology can manage large scale image repositories, and shows that web browsers can be a good geo-spatial image presentation system.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {307–318},
numpages = {12},
keywords = {geo-spatial, internet, image databases, VLDB},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335426,
author = {Forlizzi, Luca and G\"{u}ting, Ralf Hartmut and Nardelli, Enrico and Schneider, Markus},
title = {A Data Model and Data Structures for Moving Objects Databases},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335426},
doi = {10.1145/342009.335426},
abstract = {We consider spatio-temporal databases supporting spatial objects with continuously changing position and extent, termed moving objects databases. We formally define a data model for such databases that includes complex evolving spatial structures such as line networks or multi-component regions with holes. The data model is given as a collection of data types and operations which can be plugged as attribute types into any DBMS data model (e.g. relational, or object-oriented) to obtain a complete model and query language. A particular novel concept is the sliced representation which represents a temporal development as a set of units, where unit types for spatial and other data types represent certain “simple” functions of time. We also show how the model can be mapped into concrete physical data structures in a DBMS environment.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {319–330},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335427,
author = {\v{S}altenis, Simonas and Jensen, Christian S. and Leutenegger, Scott T. and Lopez, Mario A.},
title = {Indexing the Positions of Continuously Moving Objects},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335427},
doi = {10.1145/342009.335427},
abstract = {The coming years will witness dramatic advances in wireless communications as well as positioning technologies. As a result, tracking the changing positions of objects capable of continuous movement is becoming increasingly feasible and necessary. The present paper proposes a novel, R*-tree based indexing technique that supports the efficient querying of the current and projected future positions of such moving objects. The technique is capable of indexing objects moving in one-, two-, and three-dimensional space. Update algorithms enable the index to accommodate a dynamic data set, where objects may appear and disappear, and where changes occur in the anticipated positions of existing objects. A comprehensive performance study is reported.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {331–342},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335428,
author = {Shin, Hyoseop and Moon, Bongki and Lee, Sukho},
title = {Adaptive Multi-Stage Distance Join Processing},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335428},
doi = {10.1145/342009.335428},
abstract = {A spatial distance join is a relatively new type of operation introduced for spatial and multimedia database applications. Additional requirements for ranking and stopping cardinality are often combined with the spatial distance join in on-line query processing or internet search environments. These requirements pose new challenges as well as opportunities for more efficient processing of spatial distance join queries. In this paper, we first present an efficient k-distance join algorithm that uses spatial indexes such as R-trees. Bi-directional node expansion and plane-sweeping techniques are used for fast pruning of distant pairs, and the plane-sweeping is further optimized by novel strategies for selecting a sweeping axis and direction. Furthermore, we propose adaptive multi-stage algorithms for k-distance join and incremental distance join operations. Our performance study shows that the proposed adaptive multi-stage algorithms outperform previous work by up to an order of magnitude for both k-distance join and incremental distance join queries, under various operational conditions.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {343–354},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335429,
author = {Cho, Junghoo and Shivakumar, Narayanan and Garcia-Molina, Hector},
title = {Finding Replicated Web Collections},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335429},
doi = {10.1145/342009.335429},
abstract = {Many web documents (such as JAVA FAQs) are being replicated on the Internet. Often entire document collections (such as hyperlinked Linux manuals) are being replicated many times. In this paper, we make the case for identifying replicated documents and collections to improve web crawlers, archivers, and ranking functions used in search engines. The paper describes how to efficiently identify replicated documents and hyperlinked document collections. The challenge is to identify these replicas from an input data set of several tens of millions of web pages and several hundreds of gigabytes of textual data. We also present two real-life case studies where we used replication information to improve a crawler and a search engine. We report these results for a data set of 25 million web pages (about 150 gigabytes of HTML data) crawled from the web.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {355–366},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335430,
author = {Labrinidis, Alexandros and Roussopoulos, Nick},
title = {WebView Materialization},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335430},
doi = {10.1145/342009.335430},
abstract = {A WebView is a web page automatically created from base data typically stored in a DBMS. Given the multi-tiered architecture behind database-backed web servers, we have the option of materializing a WebView inside the DBMS, at the web server, or not at all, always computing it on the fly (virtual). Since WebViews must be up to date, materialized WebViews are immediately refreshed with every update on the base data. In this paper we compare the three materialization policies (materialized inside the DBMS, materialized at the web server and virtual) analytically, through a detailed cost model, and quantitatively, through extensive experiments on an implemented system. Our results indicate that materializing at the web server is a more scalable solution and can facilitate an order of magnitude more users than the virtual and materialized inside the DBMS policies, even under high update workloads.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {367–378},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335432,
author = {Chen, Jianjun and DeWitt, David J. and Tian, Feng and Wang, Yuan},
title = {NiagaraCQ: A Scalable Continuous Query System for Internet Databases},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335432},
doi = {10.1145/342009.335432},
abstract = {Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system's performance and scalability.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {379–390},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335433,
author = {Chang, Yuan-Chi and Bergman, Lawrence and Castelli, Vittorio and Li, Chung-Sheng and Lo, Ming-Ling and Smith, John R.},
title = {The Onion Technique: Indexing for Linear Optimization Queries},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335433},
doi = {10.1145/342009.335433},
abstract = {This paper describes the Onion technique, a special indexing structure for linear optimization queries. Linear optimization queries ask for top-N records subject to the maximization or minimization of linearly weighted sum of record attribute values. Such query appears in many applications employing linear models and is an effective way to summarize representative cases, such as the top-50 ranked colleges. The Onion indexing is based on a geometric property of convex hull, which guarantees that the optimal value can always be found at one or more of its vertices. The Onion indexing makes use of this property to construct convex hulls in layers with outer layers enclosing inner layers geometrically. A data record is indexed by its layer number or equivalently its depth in the layered convex hull. Queries with linear weightings issued at run time are evaluated from the outmost layer inwards. We show experimentally that the Onion indexing achieves orders of magnitude speedup against sequential linear scan when N is small compared to the cardinality of the set. The Onion technique also enables progressive retrieval, which processes and returns ranked results in a progressive manner. Furthermore, the proposed indexing can be extended into a hierarchical organization of data to accommodate both global and local queries.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {391–402},
numpages = {12},
keywords = {database indexing, linear optimization},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335434,
author = {Jagadish, H. V. and Koudas, Nick and Srivastava, Divesh},
title = {On Effective Multi-Dimensional Indexing for Strings},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335434},
doi = {10.1145/342009.335434},
abstract = {As databases have expanded in scope from storing purely business data to include XML documents, product catalogs, e-mail messages, and directory data, it has become increasingly important to search databases based on wild-card string matching: prefix matching, for example, is more common (and useful) than exact matching, for such data. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the dimensions. Traditional multi-dimensional index structures, designed with (fixed length) numeric data in mind, are not suitable for matching unbounded length string data.In this paper, we describe a general technique for adapting a multi-dimensional index structure for wild-card indexing of unbounded length string data. The key ideas are (a) a carefully developed mapping function from strings to rational numbers, (b) representing an unbounded length string in an index leaf page by a fixed length offset to an external key, and (c) storing multiple elided tries, one per dimension, in an index page to prune search during traversal of index pages. These basic ideas affect all index algorithms. In this paper, we present efficient algorithms for different types of string matching.While our technique is applicable to a wide range of multi-dimensional index structures, we instantiate our generic techniques by adapting the 2-dimensional R-tree to string data. We demonstrate the space effectiveness and time benefits of using the string R-tree both analytically and experimentally.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {403–414},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335436,
author = {Oh, JungHwan and Hua, Kien A.},
title = {Efficient and Cost-Effective Techniques for Browsing and Indexing Large Video Databases},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335436},
doi = {10.1145/342009.335436},
abstract = {We present in this paper a fully automatic content-based approach to organizing and indexing video data. Our methodology involves three steps:
Step 1: We segment each video into shots using a Camera-Tracking technique. This process also extracts the feature vector for each shot, which consists of two statistical variances VarBA and VarOA. These values capture how much things are changing in the background and foreground areas of the video shot.Step 2: For each video, We apply a fully automatic method to build a browsing hierarchy using the shots identified in Step 1.Step 3: Using the VarBA and VarOA values obtained in Step 1, we build an index table to support a variance-based video similarity model. That is, video scenes/shots are retrieved based on given values of VarBA and VarOA.The above three inter-related techniques offer an integrated framework for modeling, browsing, and searching large video databases. Our experimental results indicate that they have many advantages over existing methods.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {415–426},
numpages = {12},
keywords = {video browsing, video similarity model, shot detection, video retrieval, video indexing},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335437,
author = {Ramaswamy, Sridhar and Rastogi, Rajeev and Shim, Kyuseok},
title = {Efficient Algorithms for Mining Outliers from Large Data Sets},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335437},
doi = {10.1145/342009.335437},
abstract = {In this paper, we propose a novel formulation for distance-based outliers that is based on the distance of a point from its kth nearest neighbor. We rank each point on the basis of its distance to its kth nearest neighbor and declare the top n points in this ranking to be outliers. In addition to developing relatively straightforward solutions to finding such outliers based on the classical nested-loop join and index join algorithms, we develop a highly efficient partition-based algorithm for mining outliers. This algorithm first partitions the input data set into disjoint subsets, and then prunes entire partitions as soon as it is determined that they cannot contain outliers. This results in substantial savings in computation. We present the results of an extensive experimental study on real-life and synthetic data sets. The results from a real-life NBA database highlight and reveal several expected and unexpected aspects of the database. The results from a study on synthetic data sets demonstrate that the partition-based algorithm scales well with respect to both data set size and data set dimensionality.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {427–438},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335438,
author = {Agrawal, Rakesh and Srikant, Ramakrishnan},
title = {Privacy-Preserving Data Mining},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335438},
doi = {10.1145/342009.335438},
abstract = {A fruitful direction for future data mining research will be the development of techniques that incorporate privacy concerns. Specifically, we address the following question. Since the primary task in data mining is the development of models about aggregated data, can we develop accurate models without access to precise information in individual data records? We consider the concrete case of building a decision-tree classifier from training data in which the values of individual records have been perturbed. The resulting data records look very different from the original records and the distribution of data values is also very different from the original distribution. While it is not possible to accurately estimate original values in individual data records, we propose a novel reconstruction procedure to accurately estimate the distribution of original data values. By using these reconstructed distributions, we are able to build classifiers whose accuracy is comparable to the accuracy of classifiers built with the original data.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {439–450},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335439,
author = {Szalay, Alexander S. and Kunszt, Peter Z. and Thakar, Ani and Gray, Jim and Slutz, Don and Brunner, Robert J.},
title = {Designing and Mining Multi-Terabyte Astronomy Archives: The Sloan Digital Sky Survey},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335439},
doi = {10.1145/342009.335439},
abstract = {The next-generation astronomy digital archives will cover most of the sky at fine resolution in many wavelengths, from X-rays, through ultraviolet, optical, and infrared. The archives will be stored at diverse geographical locations. One of the first of these projects, the Sloan Digital Sky Survey (SDSS) is creating a 5-wavelength catalog over 10,000 square degrees of the sky (see http://www.sdss.org/). The 200 million objects in the multi-terabyte database will have mostly numerical attributes in a 100+ dimensional space. Points in this space have highly correlated distributions.The archive will enable astronomers to explore the data interactively. Data access will be aided by multidimensional spatial and attribute indices. The data will be partitioned in many ways. Small tag objects consisting of the most popular attributes will accelerate frequent searches. Splitting the data among multiple servers will allow parallel, scalable I/O and parallel data analysis. Hashing techniques will allow efficient clustering, and pair-wise comparison algorithms that should parallelize nicely. Randomly sampled subsets will allow de-bugging otherwise large queries at the desktop. Central servers will operate a data pump to support sweep searches touching most of the data. The anticipated queries will require special operators related to angular distances and complex similarity tests of object properties, like shapes, colors, velocity vectors, or temporal behaviors. These issues pose interesting data management challenges.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {451–462},
numpages = {12},
keywords = {scalable, Internet, data mining, astronomy, archive, database, data analysis},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335448,
author = {Gunopulos, Dimitrios and Kollios, George and Tsotras, Vassilis J. and Domeniconi, Carlotta},
title = {Approximating Multi-Dimensional Aggregate Range Queries over Real Attributes},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335448},
doi = {10.1145/342009.335448},
abstract = {Finding approximate answers to multi-dimensional range queries over real valued attributes has significant applications in data exploration and database query optimization. In this paper we consider the following problem: given a table of d attributes whose domain is the real numbers, and a query that specifies a range in each dimension, find a good approximation of the number of records in the table that satisfy the query.We present a new histogram technique that is designed to approximate the density of multi-dimensional datasets with real attributes. Our technique finds buckets of variable size, and allows the buckets to overlap. Overlapping buckets allow more efficient approximation of the density. The size of the cells is based on the local density of the data. This technique leads to a faster and more compact approximation of the data distribution. We also show how to generalize kernel density estimators, and how to apply them on the multi-dimensional query approximation problem.Finally, we compare the accuracy of the proposed techniques with existing techniques using real and synthetic datasets.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {463–474},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335449,
author = {Rao, Jun and Ross, Kenneth A.},
title = {Making B+- Trees Cache Conscious in Main Memory},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335449},
doi = {10.1145/342009.335449},
abstract = {Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees. However, CSS-Trees are designed for decision support workloads with relatively static data. Although B+-Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers. Nevertheless, for applications that require incremental updates, traditional B+-Trees perform well.Our goal is to make B+-Trees as cache conscious as CSS-Trees without increasing their update cost too much. We propose a new indexing technique called “Cache Sensitive B+-Trees” (CSB+-Trees). It is a variant of B+-Trees that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node. The rest of the children can be found by adding an offset to that address. Since only one child pointer is stored explicitly, the utilization of a cache line is high. CSB+-Trees support incremental updates in a way similar to B+-Trees.We also introduce two variants of CSB+-Trees. Segmented CSB+-Trees divide the child nodes into segments. Nodes within the same segment are stored contiguously and only pointers to the beginning of each segment are stored explicitly in each node. Segmented CSB+-Trees can reduce the copying cost when there is a split since only one segment needs to be moved. Full CSB+-Trees preallocate space for the full node group and thus reduce the split cost. Our performance studies show that CSB+-Trees are useful for a wide range of applications.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {475–486},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335450,
author = {Acharya, Swarup and Gibbons, Phillip B. and Poosala, Viswanath},
title = {Congressional Samples for Approximate Answering of Group-by Queries},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335450},
doi = {10.1145/342009.335450},
abstract = {In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex decision support queries using precomputed summary statistics, such as samples. Decision support queries routinely segment the data into groups and then aggregate the information in each group (group-by queries). Depending on the data, there can be a wide disparity between the number of data items in each group. As a result, approximate answers based on uniform random samples of the data can result in poor accuracy for groups with very few data items, since such groups will be represented in the sample by very few (often zero) tuples.In this paper, we propose a general class of techniques for obtaining fast, highly-accurate answers for group-by queries. These techniques rely on precomputed non-uniform (biased) samples of the data. In particular, we propose congressional samples, a hybrid union of uniform and biased samples. Given a fixed amount of space, congressional samples seek to maximize the accuracy for all possible group-by queries on a set of columns. We present a one pass algorithm for constructing a congressional sample and use this technique to also incrementally maintain the sample up-to-date without accessing the base relation. We also evaluate query rewriting strategies for providing approximate answers from congressional samples. Finally, we conduct an extensive set of experiments on the TPC-D database, which demonstrates the efficacy of the techniques proposed.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {487–498},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335451,
author = {Waas, Florian and Galindo-Legaria, C\'{e}sar},
title = {Counting, Enumerating, and Sampling of Execution Plans in a Cost-Based Query Optimizer},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335451},
doi = {10.1145/342009.335451},
abstract = {Testing an SQL database system by running large sets of deterministic or stochastic SQL statements is common practice in commercial database development. However, code defects often remain undetected as the query optimizer's choice of an execution plan is not only depending on the query but strongly influenced by a large number of parameters describing the database and the hardware environment. Modifying these parameters in order to steer the optimizer to select other plans is difficult since this means anticipating often complex search strategies implemented in the optimizer.In this paper we devise algorithms for counting, exhaustive generation, and uniform sampling of plans from the complete search space. Our techniques allow extensive validation of both generation of alternatives, and execution algorithms with plans other than the optimized one—if two candidate plans fail to produce the same results, then either the optimizer considered an invalid plan, or the execution code is faulty. When the space of alternatives becomes too large for exhaustive testing, which can occur even with a handful of joins, uniform random sampling provides a mechanism for unbiased testing.The technique is implemented in Microsoft's SQL Server, where it is an integral part of the validation and testing process.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {499–509},
numpages = {11},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335452,
author = {Wattez, Fanny and Cluet, Sophie and Benzaken, V\'{e}ronique and Ferran, Guy and Fiegel, Christian},
title = {Benchmarking Queries over Trees: Learning the Hard Truth the Hard Way},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335452},
doi = {10.1145/342009.335452},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {510–511},
numpages = {2},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335454,
author = {Lehner, Wolfgang and Sidle, Richard and Pirahesh, Hamid and Cochrane, Roberta Wolfgang},
title = {Maintenance of Cube Automatic Summary Tables},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335454},
doi = {10.1145/342009.335454},
abstract = {Materialized views (or Automatic Summary Tables—ASTs) are commonly used to improve the performance of aggregation queries by orders of magnitude. In contrast to regular tables, ASTs are synchronized by the database system. In this paper, we present techniques for maintaining cube ASTs. Our implementation is based on IBM DB2 UDB.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {512–513},
numpages = {2},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335455,
author = {Huber, Val},
title = {Challenges in Automating Declarative Business Rules to Enable Rapid Business Response},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335455},
doi = {10.1145/342009.335455},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {514},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335457,
author = {Ross, Ronald G.},
title = {Expressing Business Rules},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335457},
doi = {10.1145/342009.335457},
abstract = {Business rules are formal statements about the data and processes of  an  enterprise.  My  overall  approach  to  business  rules  is  described in [1, 2]. Here, I will briefly discuss things we have learned about the expression of business rules in the last several years. This will shed light on where we stand in understanding business rules today.First, it is clearly important to separate analysis-level expression of business rules from their design-level expression. Most of what I will say here is aimed toward the design level, but let me start with the analysis level.Effective expression of business rules at the analysis level requires formative guidelines or Business Rule Statement Templates. Such language  templates  are  now  offered  commercially  (by  my  company and others). Think of these language templates as text or sentence patterns, to ensure higher clarity and consistency. These templates are important for making the business rule approach practical.At the design level, how business rules are expressed to users must be cleanly separated from how they are represented inside the system. What is good for one is not good for the other.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {515–516},
numpages = {2},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335459,
author = {Kintzer, Eric},
title = {Going beyond Personalization: Rule Engines at Work},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335459},
doi = {10.1145/342009.335459},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {517},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335461,
author = {Hsiao, Hui-I and Narang, Inderpal},
title = {DLFM: A Transactional Resource Manager},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335461},
doi = {10.1145/342009.335461},
abstract = {The DataLinks technology developed at IBM Almaden Research Center and now available in DB2 UDB 5.2 introduces a new data type called DATALINK for a database to reference and manage files stored external to the database. An external file is put under a database control by “linking” the file to the database. Control to a file can also be removed by “unlinking” it. The technology provides transactional semantics with respect to linking or unlinking the file when DATALINK value is stored or updated. Further more, it provides the following set of properties: (1) managing access control to linked files, (2) enforcing referential integrity, such as referenced file cannot be deleted or renamed as long as it is referenced from the RDBMS, and (3) providing coordinated backup and recovery of RDBMS data with the file data.DataLinks File Manager (DLFM) is a key component of the DataLinks technology. DLFM is a sophisticated SQL application with a set of daemon processes residing at a file server node that work cooperatively with the host database server(s) to manage external files. To reduce the number of messages between database server and DLFM, DLFM maintains a set of meta data on the file system and the files that are under database control. One of the major decisions we made was to build DLFM on top of an existing database manager, such as DB2, instead of implementing a proprietary persistent data store. We have mixed feelings about using the RDBMS to build such a resource manager. One of the major challenges is to support transactional semantics for DLFM operations. To do this, we implemented the two-phase commit protocol in DLFM and designed an innovative scheme to enable rolling back transaction update after local database commit. Also a major gotchas is that the RDBMS' cost based optimizer generates the access plan, which does not take into account the locking costs of a concurrent workload. Using the RDBMS as a black box can cause “havoc” in terms of causing the lock timeouts and deadlocks and reducing the throughput of a concurrent workload. To solve the problem, we came up with a simple but effective way of influencing the optimizer to generate access plans matching the needs of DLFM implementation. Also several precautions had to be taken to ensure that lock escalation did not take place; next key locking was disabled to avoid deadlocks on heavily used indexes and SQL tables; and timeout mechanism was applied to break global deadlocks.We were able to run 100-client workload for 24 hours without much deadlock/timeout problem in system test. This paper describes the motivation for building the DLFM and the lessons that we have learned from this experience.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {518–528},
numpages = {11},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335462,
author = {Ponnekanti, Nagavamsi and Kodavalla, Hanuma},
title = {Online Index Rebuild},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335462},
doi = {10.1145/342009.335462},
abstract = {In this paper we present an efficient method to do online rebuild of a B+-tree index. This method has been implemented in Sybase Adaptive Server Enterprise (ASE) Version 12.0. It provides high concurrency, does minimal amount of logging, has good performance and does not deadlock with other index operations. It copies the index rows to newly allocated pages in the key order so that good space utilization and clustering are achieved. The old pages are deallocated during the process. Our algorithm differs from the previously published online index rebuild algorithms in two ways. It rebuilds multiple leaf pages and then propagates the changes to higher levels. Also, while propagating the leaf level changes to higher levels, level 11 pages are reorganized, eliminating the need for a separate pass. Our performance study shows that our approach results in significant reduction in logging and CPU time. Also, our approach uses the same concurrency control mechanism as split and shrink operations, which made it attractive for implementation.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {529–538},
numpages = {10},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335463,
author = {Annamalai, Melliyal and Chopra, Rajiv and DeFazio, Samuel and Mavris, Susan},
title = {Indexing Images in Oracle8i},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335463},
doi = {10.1145/342009.335463},
abstract = {Content-based retrieval of images is the ability to retrieve images that are similar to a query image. Oracle8i Visual Information Retrieval provides this facility based on technology licensed from Virage, Inc. This product is built on top of Oracle8i interMedia which enables storage, retrieval and management of images, audios and videos. Images are matched using attributes such as color, texture and structure and efficient content-based retrieval is provided using indexes of an image index type. The design of the index type is based on a multi-level filtering algorithm. The filters reduce the search space so that the expensive comparison algorithm operates on a small subset of the data. Bitmap indexes are used to evaluate the first filter resulting in a design which performs well and is scalable. The image index type is built using Oracle8i extensible indexing technology, allowing users to create, use, and drop instances of this index type as they would any other standard index. In this paper we present an overview of the product, the design of the image index type, and some performance results of our product.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {539–547},
numpages = {9},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335465,
author = {Weininger, Andreas},
title = {Handling Very Large Databases with Informix Extended Parallel Server},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335465},
doi = {10.1145/342009.335465},
abstract = {In this paper, we investigate which problems exist in very large real databases and describe which mechanisms are provided by Informix Extended Parallel Server (XPS) for dealing with these problems. Currently the largest customer XPS database contains 27 TB of data. A database server that has to handle such an amount of data has to provide mechanisms which allow achieving adequate performance and easing the usability. We will present mechanisms which address both of these issues and illustrate them with examples from real customer systems.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {548–549},
numpages = {2},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335466,
author = {Chen, Chung-Min and Cochinwala, Munir and Petrone, Claudio and Pucci, Marc and Samtani, Sunil and Santa, Patrizia},
title = {Internet Traffic Warehouse},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335466},
doi = {10.1145/342009.335466},
abstract = {We report on a network traffic warehousing project at Telcordia. The warehouse supports a variety of applications that require access to Internet traffic data. The applications include Service Level Agreement (SLA), web traffic analysis, network capacity engineering and planning, and billing. We describe the design of the warehouse and the issues encountered in building the warehouse.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {550–558},
numpages = {9},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335468,
author = {Ordonez, Carlos and Cereghini, Paul},
title = {SQLEM: Fast Clustering in SQL Using the EM Algorithm},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335468},
doi = {10.1145/342009.335468},
abstract = {Clustering is one of the most important tasks performed in Data Mining applications. This paper presents an efficient SQL implementation of the EM algorithm to perform clustering in very large databases. Our version can effectively handle high dimensional data, a high number of clusters and more importantly, a very large number of data records. We present three strategies to implement EM in SQL: horizontal, vertical and a hybrid one. We expect this work to be useful for data mining programmers and users who want to cluster large data sets inside a relational DBMS.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {559–570},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335469,
author = {Jhingran, Anant},
title = {Anatomy of a Real E-Commerce System},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335469},
doi = {10.1145/342009.335469},
abstract = {Today's E-Commerce systems are a complex assembly of databases, web servers, home grown glue code, and networking services for security and scalability. The trend is towards larger pieces of these coming together in bundled offerings from leading software vendors, and the networking/hardware being offered through service delivery companies. In this paper we examine the bundle by looking in detail at IBM's WebSphere, Commerce Edition, and its deployment at a major customer site.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {571–572},
numpages = {2},
keywords = {Web applications, databases, e-commerce, middleware},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335471,
author = {Ramakrishnan, Raghu},
title = {From Browsing to Interacting: DBMS Support for Responsive Websites},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335471},
doi = {10.1145/342009.335471},
abstract = {Internet websites increasingly rely on database management systems. There are several reasons for this trend:
As sites grow larger, managing the content becomes impossible without the use of a DBMS to keep track of the nature, origin, authorship, and modification history of each article.As sites become more interactive, tracking and logging user activity and user contributions creates valuable new data, which again is best managed using a DBMS. The emerging paradigm of Customer-Centric e-Business places a premium on engaging users, building a relationship with them across visits, and leveraging their expertise and feed-back. Supporting this paradigm means that we not only have to track what users visit on a site, we also have to enable them to offer opinions and contribute to the content of the website in various ways; naturally, this requires us to use a DBMS.In order to personalize a user's experience, a site must dynamically construct (or at least fine-tune) each page as it is delivered, taking into account information about the user's past activity and the nature of the content on the current page. In other words, personalization is made possible by utilizing the information (about content and user activity) that we already indicated is best managed using a DBMS.In summary, as websites go beyond a passive collection of pages to be browsed and seek to present users with a personalized, interactive experience, the role of database management systems becomes central.In this talk, I will present an overview of these issues, including a discussion of related techniques such as cookies and web server logs for tracking user activity.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {573},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335473,
author = {Hellerstein, Joseph M. and Kriegel, Hans-Peter and Comet, David and Falsutsos, Christos and Ramakrishnan, Raghu and Brown, Paul},
title = {Index Research (Panel Session): Forest or Trees?},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335473},
doi = {10.1145/342009.335473},
abstract = {Indexes and access methods have been a staple of database research — and indeed of computer science in general — for decades. A glance at the contents of this year's SIGMOD and PODS proceedings shows another bumper crop of indexing papers.Given the hundreds of indexing papers published in the database literature, a pause for reflection seems in order. From a scientific perspective, it is natural to ask why definitive indexing solutions have eluded us for so many years. What is the grand challenge in indexing? What basic complexities or intricacies underlie this large body of work? What would constitute a successful completion of this research agenda, and what steps will best move us in that direction? Or is it the case that the problem space branches in so many ways that we should expect to continuously need to solve variants of the indexing problem?From the practitioner's perspective, the proliferation of indexing solutions in the literature may be more confusing than helpful. Comprehensively evaluating the research to date is a near-impossible task. An evaluation has to include both functionality (applicability to the practitioner's problem, integration with other data management services like buffer management, query processing and transactions) as well as performance for the practitioner's workloads. Unfortunately, there are no standard benchmarks for advanced indexing problems, and there has been relatively little work on methodologies for index experimentation and customization. How should the research community promote technology transfer in this area? Are the new extensibility interfaces in object-relational DBMSs conducive to this effort?},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {574},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335474,
author = {Nori, Anil K.},
title = {Application Architecture (Panel Session): 2Tier or 3Tier? What is DBMS's Role?},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335474},
doi = {10.1145/342009.335474},
abstract = {Experienced panelist will share their views on application architecture, specially, as it relates to database systems. The discussion will focus on what technologies and mechanisms are necessary for developing web applications, and where these mechanisms should reside.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {575},
numpages = {1},
keywords = {applications, world-wide web, databases},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335476,
author = {Widom, Jennifer and Bosworth, Adam and Lindsay, Bruce and Stonebraker, Michael and Suciu, Dan and Carey, Michael J.},
title = {Of XML and Databases (Panel Session): Where's the Beef?},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335476},
doi = {10.1145/342009.335476},
abstract = {This panel will examine the implications of the XML revolution, which is currently raging on the web, for database systems research and development.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {576},
numpages = {1},
keywords = {World-Wide Web, XML, databases, semistructured data},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335477,
author = {Bratsberg, Svein Erik and Torbj\o{}rnsen, \O{}ystein},
title = {Designing an Ultra Highly Available DBMS (Tutorial Session)},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335477},
doi = {10.1145/342009.335477},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {577},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335479,
author = {Gal, Avigdor},
title = {Data Management in ECommerce (Tutorial Session): The Good, the Bad, and the Ugly},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335479},
doi = {10.1145/342009.335479},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {578},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335480,
author = {Blakeley, Jos\'{e} A. and Deshpande, Anand},
title = {Data Access (Tutorial Session)},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335480},
doi = {10.1145/342009.335480},
abstract = {With an explosion of data on the web, consistent data access to diverse data sources has become a challenging task. In this tutorial will present topics of interest to database researchers and developers building: interoperable middle-ware, gateways, distributed heterogeneous query processors, federated databases, data source wrappers, mediators, and DBMS extensions.All of these require access to diverse information through common data access abstractions, powerful APIs, and common data exchange formats. With the emergence of the web, database applications are being run over the intranet and the extranet. This tutorial presents an overview of existing and emerging data access technologies. We will concentrate on some of the technical challenges that have to be addressed to enable uniform data access across various platforms and some of the issues that went into the design of these data access strategies.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {579},
numpages = {1},
keywords = {database connectivity, data access},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335482,
author = {Shukla, Shridhar and Deshpande, Anand},
title = {LDAP Directory Services- Just Another Database Application? (Tutorial Session)},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335482},
doi = {10.1145/342009.335482},
abstract = {The key driving force behind general-purpose enterprise directory services is for providing a central repository for commonly and widely used information such as users, groups, network service access information and profiles, security information, etc. Acceptance of the Lightweight Directory Access Protocol (LDAP) as an access protocol has facilitated widespread integration of these directory services into the network infrastructure and applications.Both directory and relational databases are data repositories sharing the characteristic that they have mechanisms for dealing with schema and structure of information and are suitable for systematically organized data. This tutorial describes characteristics of directories such as schema information, query language and support, storage mechanisms required, typical requirements imposed by applications, etc. We then explain the differences between a directory and relational database, and show how the two are required to co-exist in a typical enterprise.An essential characteristic assumed for information stored in directories is that it is relatively static and that the queries are mostly read only. We describe typical directory applications to validate this assumption and project the requirements imposed on them as these applications evolve. We then describe areas of overlap between traditional databases and directories, describe some database and directory integration solutions adopted in the market, and identify areas in which directory deployment can benefit from the experience gathered by the database community.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {580},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335484,
author = {Wolfson, Ouri},
title = {Research Issues in Moving Objects Databases (Tutorial Session)},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335484},
doi = {10.1145/342009.335484},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {581},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335492,
author = {Ives, Zachary and Levy, Alon and Madhavan, Jayant and Pottinger, Rachel and Saroiu, Stefan and Tatarinov, Igor and Betzler, Shiori and Chen, Qiong and Jaslikowska, Ewa and Su, Jing and Theodora Yeung, Wai Tak},
title = {Self-Organizing Data Sharing Communities with SAGRES},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335492},
doi = {10.1145/342009.335492},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {582},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335494,
author = {Fegaras, Leonidas and Srinivasan, Chandrasekhar and Rajendran, Arvind and Maier, David},
title = {λ-DB: An ODMG-Based Object-Oriented DBMS},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335494},
doi = {10.1145/342009.335494},
abstract = {The λ-DB project at the University of Texas at Arlington aims at developing frameworks and prototype systems that address the new query optimization challenges for object-oriented and object-relational databases, such as query nesting, multiple collection types, methods, and arbitrary nesting of collections. We have already developed a theoretical framework for query optimization based on an effective calculus, called the monoid comprehension calculus [4]. The system reported here is a fully operational ODMG 2.0 [2] OODB management system, based on this framework. Our system can handle most ODL declarations and can process most OQL query forms. λ-DB is not ODMG compliant. Instead it supports its own C++ binding that provides a seamless integration between OQL and C++ with low  impedance mismatch. It allows C++ variables to be used in queries and results of queries to be passed back to C++ programs. Programs expressed in our C++ binding are compiled by a preprocessor that performs query optimization at compile time, rather than run-time, as it is proposed by ODMG. In addition to compiled queries, λ-DB provides an interpreter that evaluates ad-hoc OQL queries at run-time.The λ-DB system architecture is shown in Figure 1. The λ-DB evaluation engine is written in SDL (the SHORE Data Language) of the SHORE object management system [1], developed at the University of Wisconsin. ODL schemas are translated into SDL schemas in a straightforward way and are stored in the system catalog. The λ-DB OQL compiler is a C++ preprocessor that accepts a language called λ-OQL, which is C++ code with embedded DML commands to perform transactions, queries, updates, etc. The preprocessor translates λ-OQL programs into C++ code that contains calls to the λ-DB evaluation engine. We also provide a visual query formulation interface, called VOODOO, and a translator from visual queries to OQL text, which can be sent to the λ-DB OQL interpreter for evaluation.Even though a lot of effort has been made to make the implementation of our system simple enough for other database researchers to use and extend, our system is quite sophisticated since it employs current state-of-the-art query optimization technologies as well as new advanced experimental optimization techniques which we have developed through the years, such as query unnesting [3]. The λ-DB OODBMS is available as an open source software through the web at http://lambda.uta.edu/lambda-DB.html},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {583},
numpages = {4},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335495,
author = {Wang, Jason T. L. and Wang, Xiong and Shasha, Dennis and Shapiro, Bruce A. and Zhang, Kaizhong and Ma, Qicheng and Weinberg, Zasha},
title = {An Approximate Search Engine for Structural Databases},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335495},
doi = {10.1145/342009.335495},
abstract = {When a person interested in a topic enters a keyword into a Web search engine, the response is nearly instantaneous (and sometimes overwhelming). The impressive speed is due to clever inverted index structures, caching, and a domain-independent knowledge of strings. Our project seeks to construct algorithms, data structures, and software that approach the speed of keyword-based search engines for queries on structural databases.A structural database is one whose data objects include trees, graphs, or a set of interrelated labeled points in two, three, or higher dimensional space. Examples include databases holding (i) protein secondary and tertiary structure, (ii) phylogenetic trees, (iii) neuroanatomical networks, (iv) parse trees, (v) molecular diagrams, and (vi) XML documents. Comparison queries on such databases require solving variants of the graph isomorphism or subisomorphism problems (for which all known algorithms are exponential), so we have explored a large heuristic space.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {584},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335496,
author = {Rundensteiner, Elke A. and Claypool, Kajal T. and Chen, Li and Su, Hong and Oenoki, Keiji},
title = {SERFing the Web: Web Site Management Made Easy},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335496},
doi = {10.1145/342009.335496},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {585},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.335497,
author = {Merialdo, Paolo and Atzeni, Paolo and Magnante, Marco and Mecca, Giansalvatore and Pecorone, Marco},
title = {HOMER: A Model-Based CASE Tool for Data-Intensive Web Sites},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335497},
doi = {10.1145/342009.335497},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {586},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336560,
author = {Chen, Songting and Diao, Yanlei and Lu, Hongjun and Tian, Zengping},
title = {FACT: A Learning Based Web Query Processing System},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336560},
doi = {10.1145/342009.336560},
abstract = {FACT (Fast and ACcuraTe) is a query processing system aimed at providing users with facilities so that they can get the query results from the Web in a database-like fashion. The system takes user queries in the form of keywords (free text) and returns segments of Web pages that contain the required information. It works as follows. The input from a user is passed to a general- purpose search engine to obtain a set of URLs of Web pages that may contain the required information. The system later locates the query results from the Web pages reachable from these URLs. Since queries expressed in keywords may be not able to express query requirements precisely or may not guarantee the discovery of required information inherently, the user is asked to first browse a few pages, during which the system learns from her/him about the exact query requirements and heuristics of finding the required information through a series of hyperlinks. The system will process the rest URLs and present the results in the form of segments of Web pages to the user.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {587},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336562,
author = {van den Bercken, Jochen and Dittrich, Jens-Peter and Seeger, Bernhard},
title = {Javax.XXL: A Prototype for a Library of Query Processing Algorithms},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336562},
doi = {10.1145/342009.336562},
abstract = {Therefore, index structures can easily be used in queries. A typical example is a join cursor which consumes the outputs of two underlying cursors. Most of our work is however not dedicated to the area of relational databases, but mainly refers to spatial and temporal data. For spatial databases, for example, we provide several implementations of spatial join algorithms [3]. The cursor-based processing is however the major advantage of XXL in contrast to approaches like LEDA [6] and TPIE [7]. For more information on XXL see http://www.mathematik.uni-marburg.de/DBS/xxl.We will demonstrate the latest version of XXL using examples to show its core functionality. We will concentrate on three key aspects of XXL.Usage: We show how easily state-of-the-art spatial join-algorithms can be implemented in XXL using data from different sources.Reuse: We will demonstrate how to support different joins, e.g. spatial and temporal joins, using the same generic algorithm like Plug&amp;Join [1].Comparability: We will demonstrate how XXL serves as an ideal testbed to compare query processing algorithms and index structures.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {588},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336564,
author = {Sarawagi, Sunita and Sathe, Gayatri},
title = {I3: Intelligent, Interactive Investigation of OLAP Data Cubes},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336564},
doi = {10.1145/342009.336564},
abstract = {The goal of the i3(eye cube) project is to enhance multidimensional database products with a suite of advanced operators to automate data analysis tasks that are currently handled through manual exploration. Most OLAP products are rather simplistic and rely heavily on the user's intuition to manually drive the discovery process. Such ad hoc user-driven exploration gets tedious and error-prone as data dimensionality and size increases. We first investigated how and why analysts currently explore the data cube and then automated them using advanced operators that can be invoked interactively like existing simple operators.Our proposed suite of extensions appear in the form of a toolkit attached with a OLAP product. At this demo we will present three such operators: DIFF, RELAX and INFORM with illustrations from real-life datasets.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {589},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336568,
author = {Galhardas, Helena and Florescu, Daniela and Shasha, Dennis and Simon, Eric},
title = {AJAX: An Extensible Data Cleaning Tool},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336568},
doi = {10.1145/342009.336568},
abstract = {@@@@ groups together matching pairs with a high similarity value by applying a given grouping criteria (e.g. by transitive closure). Finally, ging collapses each individual cluster into a tuple of the resulting data source. AJAX provides @@@@ for specifying data cleaning programs, which consists of SQL statements enriched with a set of specific primitives to express these transformations.AJAX also @@@@. It allows the user to interact with an executing data cleaning program to handle exceptional cases and to inspect intermediate results. Finally, AJAX provides @@@@ @@@@ that permits users to determine the source and processing of data for debugging purposes.We will present the AJAX system applied to two real world problems: the consolidation of a telecommunication database, and the conversion of a dirty database of bibliographic references into a set of clean, normalized, and redundancy free relational tables maintaining the same data.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {590},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336570,
author = {Jarke, M. and Quix, C. and Calvanese, D. and Lenzerini, M. and Franconi, E. and Ligoudistianos, S. and Vassiliadis, P. and Vassiliou, Y.},
title = {Concept Based Design of Data Warehouses: The DWQ Demonstrators},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336570},
doi = {10.1145/342009.336570},
abstract = {The ESPRIT Project DWQ (Foundations of Data Warehouse Quality) aimed at improving the quality of DW design and operation through systematic enrichment of the semantic foundations of data warehousing. Logic-based knowledge representation and reasoning techniques were developed to control accuracy, consistency, and completeness via advanced conceptual modeling techniques for source integration, data reconciliation, and multi-dimensional aggregation. This is complemented by quantitative optimization techniques for view materialization, optimizing timeliness and responsiveness without losing the semantic advantages from the conceptual approach. At the operational level, query rewriting and materialization refreshment algorithms exploit the knowledge developed at design time. The demonstration shows the interplay of these tools under a shared metadata repository, based on an example extracted from an application at Telecom Italia.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {591},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336572,
author = {Pei, Jian and Mao, Runying and Hu, Kan and Zhu, Hua},
title = {Towards Data Mining Benchmarking: A Test Bed for Performance Study of Frequent Pattern Mining},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336572},
doi = {10.1145/342009.336572},
abstract = {Performance benchmarking has played an important role in the research and development in relational DBMS, object-relational DBMS, data warehouse systems, etc. We believe that benchmarking data mining algorithms is a long overdue task, and it will play an important role in the research and development of data mining systems as well.Frequent pattern mining forms a core component in mining associations, correlations, sequential patterns, partial periodicity, etc., which are of great potential value in applications. There have been a lot of methods proposed and developed for efficient frequent pattern mining in various kinds of databases, including transaction databases, time-series databases, etc. However, so far there is no serious performance benchmarking study of different  frequent pattern mining methods.To facilitate an analytical comparison of different frequent mining methods, we have constructed an open test bed for performance study of a set of recently developed, popularly used methods for mining frequent patterns in transaction databases and mining sequential patterns in sequence databases, with different data characteristics. The testbed consists of the following components.A synthetic data generator, which can generate large sets of synthetic data in various kinds of data distributions. A few large data sets from real world applications will also be provided.A good set of typical frequent pattern mining methods, ranging from classical algorithms to recent studies. The method are grouped into three  classes: frequent pattern mining, max-pattern mining, and sequential pattern mining. For frequent pattern mining, we will demonstrate Apriori, hashing, partitioning, sampling, TreeProjection, and FP-growth. For maximal pattern mining, we will demonstrate MaxMiner, TreeProjection, and FP-growth-max. For sequential pattern mining, we will demonstrate GSP and FreeSpan.A set of performance curves. These algorithms their running speeds, scalabilities, bottlenecks, and performance on different data distributions, will be compared and demonstrated upon request. Some performance curves from our pre-conference experimental evaluations will also be shown.An open testbed. Our goal is to construct an extensible test bed which integrates the above components and supports an open-ended testing service. Researchers can upload the object codes of their mining algorithms, and run them in the test bed using these data sets. The architecture is shown in Figure 1.This testbed is our first step towards benchmarking data mining algorithms. By doing so, performance of different algorithms can be reported consistently, on the same platform, and in the same environment. After the demo, we plan to make the testbed available on the WWW so that it may, hopefully, benefit further research and development of efficient data mining methods.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {592},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336573,
author = {Hsu, Wynne and Lee, Mong Li and Goh, Kheng Guan},
title = {Image Mining in IRIS: Integrated Retinal Information System},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336573},
doi = {10.1145/342009.336573},
abstract = {There is an increasing demand for systems that can automatically analyze images and extract semantically meaningful information. IRIS, an Integrated Retinal Information system, has been developed to provide medical professionals easy and unified access to the screening, trend and progression of diabetic-related eye diseases in a diabetic patient database. This paper shows how mining techniques can be used to accurately extract features in the retinal images. In particular, we apply a classification approach to determine the conditions for tortuousity in retinal blood vessels.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {593},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336575,
author = {Rodr\'{\i}guez-Mart\'{\i}nez, Manuel and Roussopoulos, Nick and McGann, John M. and Kelley, Stephen and Katz, Vadim and Song, Zhexuan and J\'{a}J\'{a}, Joseph},
title = {MOCHA: A Database Middleware System Featuring Automatic Deployment of Application-Specific Functionality},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336575},
doi = {10.1145/342009.336575},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {594},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336577,
author = {Gillmann, Michael and Weissenfels, Jeanine and Shegalov, German and Wonner, Wolfgang and Weikum, Gerhard},
title = {A Goal-Driven Auto-Configuration Tool for the Distributed Workflow Management System Mentorlite},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336577},
doi = {10.1145/342009.336577},
abstract = {The Mentor-lite prototype has been developed within the research project “Architecture, Configuration, and Administration of Large Workflow Management Systems” funded by the German Science Foundation (DFG). It has evolved from its predecessor Mentor [1], but aims at a simpler architecture. The main goal of Mentor-lite has been to build a light-weight, extensible, and tailorable workflow management system (WFMS) with small footprint and easy-to-use administration capabilities. Our approach is to provide only kernel functionality inside the workflow engine, and consider system components like history management and worklist management as extensions on top of the kernel. The key point to retain the light-weight nature is that these extensions are implemented as workflows themselves.The workflow specifications are interpreted at runtime, which is a crucial prerequisite for flexible exception handling and dynamic modifications during runtime. The interpreter performs a stepwise execution of the workflow specification according to its formal semantics. For each step, the activities to be performed by the step are determined and started.Mentor-lite supports a protocol for distributed execution of workflows spread across multiple workflow engines. This support is crucial for workflows that span large, decentralized enterprises with largely autonomous organizational units or even cross multiple enterprises to form so-called “virtual enterprises”. A communication manager is responsible for sending and receiving synchronization messages between the engines. In order to guarantee a consistent global state even in the presence of site or network failures, we have built reliable message queues using the CORBA Object Transaction Services.For administration, Mentor-lite provides a Java-based workbench for workflow design, workflow partitioning across multiple workflow servers, and a Java-based runtime monitoring tool.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {595},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336578,
author = {Yang, Jun and Ying, Huacheng C. and Widom, Jennifer},
title = {TIP: A Temporal Extension to Informix},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336578},
doi = {10.1145/342009.336578},
abstract = {Commercial relational database systems today provide only limited temporal support. To address the needs of applications requiring rich temporal data and queries, we have built TIP (Temporal Information Processor), a temporal extension to the Informix database system based on its DataBlade technology. Our TIP DataBlade extends Informix with a rich set of datatypes and routines that facilitate temporal modeling and querying. TIP provides both C and Java libraries for client applications to access a TIP-enabled database, and provides end-users with a GUI interface for querying and browsing temporal data.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {596},
numpages = {6},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336579,
author = {Liu, Ling and Pu, Calton and Buttler, David and Han, Wei and Paques, Henrique and Tang, Wei},
title = {AQR-Toolkit: An Adaptive Query Routing Middleware for Distributed Data Intensive Systems},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336579},
doi = {10.1145/342009.336579},
abstract = {Query routing is an intelligent service that can direct query requests to appropriate servers that are capable of answering the queries. The goal of a query routing system is to provide efficient associative access to a large, heterogeneous, distributed collection of information providers by routing a user query to the most relevant information sources that can provide the best answer. Effective query routing not only minimizes the query response time and the overall processing cost, but also eliminates a lot of unnecessary communication overhead over the global networks and over the individual information sources.The AQR-Toolkit divides the query routing task into two cooperating processes: query refinement and source selection. It is well known that a broadly defined query inevitably produces many false positives. Query refinement provides mechanisms to help the user formulate queries that will return more useful results and that can be processed efficiently. As a complimentary process, source selection reduces false negatives by identifying and locating a set of relevant information providers from a large collection of available sources. By pruning irrelevant information sources, source selection also reduces the overhead of contacting the information servers that do not contribute to the answer of the query.The system architecture of AQR-Toolkit consists of a hierarchical network (a directed acyclic graph) with external information providers at the leaves and query routers as mediating nodes. The end-point information providers support query-based access to their documents. At a query router node, a user may browse and query the meta information about information providers registered at that query router or make use of the router's facilitates for query refinement and source selection.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {597},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336583,
author = {Li, Chung-Sheng and Bergman, Lawrence D. and Chang, Yuan-Chi and Castelli, Vittorio and Smith, John R.},
title = {SPIRE: A Progressive Content-Based Spatial Image Retrieval Engine},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336583},
doi = {10.1145/342009.336583},
abstract = {In this demo, we will show the implementation of a content-based SPatial Image Retrieval Engine (SPIRE) for multimodal unstructured data. This architecture provides a framework for retrieving multi-modal data including image, image sequence, time series and parametric data from large archives. Dramatic speedup (from a factor of 4 to 35) has been achieved for many search operations such as template matching, texture feature extraction. This framework has been applied and validated in solar flares and petroleum exploration in which spatial and spatial-temporal phenomena are located.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {598},
numpages = {1},
keywords = {content-based retrieval, multimedia database, digital library},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336587,
author = {Goh, Chong Leng and Ooi, Beng Chin and Bressan, Stephane and Tan, Kian-Lee},
title = {Integrating Replacement Policies in StorM: An Extensible Approach},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336587},
doi = {10.1145/342009.336587},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {599},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336589,
author = {Oria, Vincent and \"{O}zsu, M. Tamer and Iglinski, Paul J. and Lin, Shu and Yao, Bin},
title = {DISIMA: A Distributed and Interoperable Image Database System},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336589},
doi = {10.1145/342009.336589},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {600},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@inproceedings{10.1145/342009.336590,
author = {Revesz, Peter and Chen, Rui and Kanjamala, Pradip and Li, Yiming and Liu, Yuguo and Wang, Yonghui},
title = {The MLPQ/GIS Constraint Database System},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.336590},
doi = {10.1145/342009.336590},
abstract = {MLPQ/GIS [4,6] is a constraint database [5] system like CCUBE [1] and DEDALE [3] but with a special emphases on spatio-temporal data. Features include data entry tools (first four icons in Fig. 1), icon-based queries such as @@@@ Intersection, @@@@ Union, @@@@ Area, @@@@ Buffer, @@@@ Max and @@@@ Min, which optimize linear objective functions, and @@@@ for Datalog queries. For example, in Fig. 1 we loaded and displayed a constraint database that represents the midwest United States and loaded two contraint relations describing the movements of two persons. The query icon opened a dialog box into which we entered the query which finds (t, i) pairs such that the two people are in the same state i at the same time t.MLPQ/GIS can animate [2] spatio-temporal objects that are linear constraint relations over and t.Users can also display in discrete color zones (isometric maps) any spatially distributed variable z that is a linear function and For example, Fig. 2 shows the mean annual air temperature Nebraska. Animation and isometric map display can be combined.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {601},
numpages = {1},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

