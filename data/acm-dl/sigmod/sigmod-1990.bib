@inproceedings{10.1145/93597.93611,
author = {Lipton, Richard J. and Naughton, Jeffrey F. and Schneider, Donovan A.},
title = {Practical Selectivity Estimation through Adaptive Sampling},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.93611},
doi = {10.1145/93597.93611},
abstract = {Recently we have proposed an adaptive, random sampling algorithm for general query size estimation. In earlier work we analyzed the asymptotic efficiency and accuracy of the algorithm, in this paper we investigate its practicality as applied to selects and joins. First, we extend our previous analysis to provide significantly improved bounds on the amount of sampling necessary for a given level of accuracy. Next, we provide “sanity bounds” to deal with queries for which the underlying data is extremely skewed or the query result is very small. Finally, we report on the performance of the estimation algorithm as implemented in a host language on a commercial relational system. The results are encouraging, even with this loose coupling between the estimation algorithm and the DBMS.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {1–11},
numpages = {11},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.93611,
author = {Lipton, Richard J. and Naughton, Jeffrey F. and Schneider, Donovan A.},
title = {Practical Selectivity Estimation through Adaptive Sampling},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.93611},
doi = {10.1145/93605.93611},
abstract = {Recently we have proposed an adaptive, random sampling algorithm for general query size estimation. In earlier work we analyzed the asymptotic efficiency and accuracy of the algorithm, in this paper we investigate its practicality as applied to selects and joins. First, we extend our previous analysis to provide significantly improved bounds on the amount of sampling necessary for a given level of accuracy. Next, we provide “sanity bounds” to deal with queries for which the underlying data is extremely skewed or the query result is very small. Finally, we report on the performance of the estimation algorithm as implemented in a host language on a commercial relational system. The results are encouraging, even with this loose coupling between the estimation algorithm and the DBMS.},
journal = {SIGMOD Rec.},
month = may,
pages = {1–11},
numpages = {11}
}

@inproceedings{10.1145/93597.93614,
author = {King, Roger and Morfeq, Ali},
title = {Bayan: An Arabic Text Database Management System},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.93614},
doi = {10.1145/93597.93614},
abstract = {Most existing databases lack features which allow for the convenient manipulation of text. It is even more difficult to use them if the text language is not based on the Roman alphabet. The Arabic language is a very good example of this case. Many projects have attempted to use conventional database systems for Arabic data manipulation (including text data), but because of Arabic's many differences with English, these projects have met with limited success. In the Bayan project, the approach has been different. Instead of simply trying to adopt an environment to Arabic, the properties of the Arabic language were the starting point and everything was designed to meet the needs of Arabic, thus avoiding the shortcomings of other projects. A text database management system was designed to overcome the shortcomings of conventional database management systems in manipulating text data. Bayan's data model is based on an object-oriented approach which helps the extensibility of the system for future use. In Bayan, we designed the database with the Arabic text properties in mind. We designed it to support the way Arabic words are derived, classified, and constructed. Furthermore, linguistic algorithms (for word generation and morphological decomposition of words) were designed, leading to a formalization of rules of Arabic language writing and sentence construction. A user interface was designed on top of this environment. A new representation of the Arabic characters was designed, a complete Arabic keyboard layout was created, and a window-based Arabic user interface was also designed.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {12–23},
numpages = {12},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.93614,
author = {King, Roger and Morfeq, Ali},
title = {Bayan: An Arabic Text Database Management System},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.93614},
doi = {10.1145/93605.93614},
abstract = {Most existing databases lack features which allow for the convenient manipulation of text. It is even more difficult to use them if the text language is not based on the Roman alphabet. The Arabic language is a very good example of this case. Many projects have attempted to use conventional database systems for Arabic data manipulation (including text data), but because of Arabic's many differences with English, these projects have met with limited success. In the Bayan project, the approach has been different. Instead of simply trying to adopt an environment to Arabic, the properties of the Arabic language were the starting point and everything was designed to meet the needs of Arabic, thus avoiding the shortcomings of other projects. A text database management system was designed to overcome the shortcomings of conventional database management systems in manipulating text data. Bayan's data model is based on an object-oriented approach which helps the extensibility of the system for future use. In Bayan, we designed the database with the Arabic text properties in mind. We designed it to support the way Arabic words are derived, classified, and constructed. Furthermore, linguistic algorithms (for word generation and morphological decomposition of words) were designed, leading to a formalization of rules of Arabic language writing and sentence construction. A user interface was designed on top of this environment. A new representation of the Arabic characters was designed, a complete Arabic keyboard layout was created, and a window-based Arabic user interface was also designed.},
journal = {SIGMOD Rec.},
month = may,
pages = {12–23},
numpages = {12}
}

@inproceedings{10.1145/93597.93616,
author = {Gyssens, Marc and Paredaens, Jan and Gucht, Dirk Van},
title = {A Graph-Oriented Object Model for Database End-User Interfaces},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.93616},
doi = {10.1145/93597.93616},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {24–33},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.93616,
author = {Gyssens, Marc and Paredaens, Jan and Gucht, Dirk Van},
title = {A Graph-Oriented Object Model for Database End-User Interfaces},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.93616},
doi = {10.1145/93605.93616},
journal = {SIGMOD Rec.},
month = may,
pages = {24–33},
numpages = {10}
}

@inproceedings{10.1145/93597.93618,
author = {Agrawal, R. and Gehani, N. H. and Srinivasan, J.},
title = {OdeView: The Graphical Interface to Ode},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.93618},
doi = {10.1145/93597.93618},
abstract = {OdeView is the graphical front end for Ode, an object-oriented database system and environment. Ode's data model supports data encapsulation, type inheritance, and complex objects. OdeView provides facilities for examining the database schema (i.e., the object type or class hierarchy), examining class definitions, browsing objects, following chains of references starting from an object, synchronized browsing, displaying selected portions of objects (projection), and retrieving objects with specific characteristics (selection).OdeView does not need to know about the internals of Ode objects. Consequently, the internals of specific classes are not hardwired into OdeView and new classes can be added to the Ode database without requiring any changes to or recompilation of OdeView. Just as OdeView does not know about the object internals, class functions (methods) for displaying objects are written without knowing about the specifics of the windowing software used by OdeView or the graphical user interface provided by it.In this paper, we present OdeView, and discuss its design and implementation.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {34–43},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.93618,
author = {Agrawal, R. and Gehani, N. H. and Srinivasan, J.},
title = {OdeView: The Graphical Interface to Ode},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.93618},
doi = {10.1145/93605.93618},
abstract = {OdeView is the graphical front end for Ode, an object-oriented database system and environment. Ode's data model supports data encapsulation, type inheritance, and complex objects. OdeView provides facilities for examining the database schema (i.e., the object type or class hierarchy), examining class definitions, browsing objects, following chains of references starting from an object, synchronized browsing, displaying selected portions of objects (projection), and retrieving objects with specific characteristics (selection).OdeView does not need to know about the internals of Ode objects. Consequently, the internals of specific classes are not hardwired into OdeView and new classes can be added to the Ode database without requiring any changes to or recompilation of OdeView. Just as OdeView does not know about the object internals, class functions (methods) for displaying objects are written without knowing about the specifics of the windowing software used by OdeView or the graphical user interface provided by it.In this paper, we present OdeView, and discuss its design and implementation.},
journal = {SIGMOD Rec.},
month = may,
pages = {34–43},
numpages = {10}
}

@inproceedings{10.1145/93597.93620,
author = {Ullman, Jeffrey D. and Yannakakis, Mihalis},
title = {The Input/Output Complexity of Transitive Closure},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.93620},
doi = {10.1145/93597.93620},
abstract = {Suppose a directed graph has its arcs stored in secondary memory, and we wish to compute its transitive closure, also storing the result in secondary memory. We assume that an amount of main memory capable of holding s “values” is available, and that s lies between n, the number of nodes of the graph, and e, the number of arcs. The cost measure we use for algorithms is the I/O complexity of Kung and Hong, where we count 1 every time a value is moved into main memory from secondary memory, or vice versa.In the dense case, where e is close to n2, we show that I/O equal to Ο(n3 / √s) is sufficient to compute the transitive closure of an n-node graph, using main memory of size s. Moreover, it is necessary for any algorithm that is “standard,” in a sense to be defined precisely in the paper. Roughly, “standard” means that paths are constructed only by concatenating arcs and previously discovered paths. This class includes the usual algorithms that work for the generalization of transitive closure to semiring problems. For the sparse case, we show that I/O equal to Ο(n2 √e/s) is sufficient, although the algorithm we propose meets our definition of “standard” only if the underlying graph is acyclic. We also show that Ω(n2 √e/s) is necessary for any standard algorithm in the sparse case. That settles the I/O complexity of the sparse/acyclic case, for standard algorithms. It is unknown whether this complexity can be achieved in the sparse, cyclic case, by a standard algorithm, and it is unknown whether the bound can be beaten by nonstandard algorithms.We then consider a special kind of standard algorithm, in which paths are constructed only by concatenating arcs and old paths, never by concatenating two old paths. This restriction seems essential if we are to take advantage of sparseness. Unfortunately, we show that almost another factor of n I/O is necessary. That is, there is an algorithm in this class using I/O Ο(n3 √e/s) for arbitrary sparse graphs, including cyclic ones. Moreover, every algorithm in the restricted class must use Ω(n3 √e/s/log3 n) I/O, on some cyclic graphs.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {44–53},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.93620,
author = {Ullman, Jeffrey D. and Yannakakis, Mihalis},
title = {The Input/Output Complexity of Transitive Closure},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.93620},
doi = {10.1145/93605.93620},
abstract = {Suppose a directed graph has its arcs stored in secondary memory, and we wish to compute its transitive closure, also storing the result in secondary memory. We assume that an amount of main memory capable of holding s “values” is available, and that s lies between n, the number of nodes of the graph, and e, the number of arcs. The cost measure we use for algorithms is the I/O complexity of Kung and Hong, where we count 1 every time a value is moved into main memory from secondary memory, or vice versa.In the dense case, where e is close to n2, we show that I/O equal to Ο(n3 / √s) is sufficient to compute the transitive closure of an n-node graph, using main memory of size s. Moreover, it is necessary for any algorithm that is “standard,” in a sense to be defined precisely in the paper. Roughly, “standard” means that paths are constructed only by concatenating arcs and previously discovered paths. This class includes the usual algorithms that work for the generalization of transitive closure to semiring problems. For the sparse case, we show that I/O equal to Ο(n2 √e/s) is sufficient, although the algorithm we propose meets our definition of “standard” only if the underlying graph is acyclic. We also show that Ω(n2 √e/s) is necessary for any standard algorithm in the sparse case. That settles the I/O complexity of the sparse/acyclic case, for standard algorithms. It is unknown whether this complexity can be achieved in the sparse, cyclic case, by a standard algorithm, and it is unknown whether the bound can be beaten by nonstandard algorithms.We then consider a special kind of standard algorithm, in which paths are constructed only by concatenating arcs and old paths, never by concatenating two old paths. This restriction seems essential if we are to take advantage of sparseness. Unfortunately, we show that almost another factor of n I/O is necessary. That is, there is an algorithm in this class using I/O Ο(n3 √e/s) for arbitrary sparse graphs, including cyclic ones. Moreover, every algorithm in the restricted class must use Ω(n3 √e/s/log3 n) I/O, on some cyclic graphs.},
journal = {SIGMOD Rec.},
month = may,
pages = {44–53},
numpages = {10}
}

@inproceedings{10.1145/93597.93621,
author = {Shen, Yeh-Heng},
title = {IDLOG: Extending the Expressive Power of Deductive Database Languages},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.93621},
doi = {10.1145/93597.93621},
abstract = {The expressive power of pure deductive database languages, such as DATALOG and stratified DATALOGS, is limited in a sense that some useful queries such as functions involving aggregation are not definable in these languages. Our concern in this paper is to provide a uniform logic framework for deductive databases with greater expressive power. It has been shown that with a linear ordering on the domain of the database, the expressive power of some database languages can be enhanced so that some functions involving aggregation can be defined. Yet, a direct implementation of the linear ordering in deductive database languages may seem unintuitive, and may not be very efficient to use in practice. We propose a logic for deductive databases which employs the notion of “identifying each tuple in a relation”. Through the use of these tuple-identifications, different linear orderings are defined as a result. This intuitively explains the reason why our logic has greater expressive power. The proposed logic language is non-deterministu in nature. However, non-determinism is not the real reason for the enhanced expressive power. A deterministic subset of the programs in this language is computational complete in the sense that it defines all the computable deterministic queries. Although the problem of deciding whether a program is in this subset is in general undecidable, we do provide a rather general sufficient test for identifying such programs. Also discussed in this paper is an extended notion of queries which allows both the input and the output of a query to contain interpreted constants of an infinite domain. We show that extended queries involving aggregation can also be defined in the language.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {54–63},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.93621,
author = {Shen, Yeh-Heng},
title = {IDLOG: Extending the Expressive Power of Deductive Database Languages},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.93621},
doi = {10.1145/93605.93621},
abstract = {The expressive power of pure deductive database languages, such as DATALOG and stratified DATALOGS, is limited in a sense that some useful queries such as functions involving aggregation are not definable in these languages. Our concern in this paper is to provide a uniform logic framework for deductive databases with greater expressive power. It has been shown that with a linear ordering on the domain of the database, the expressive power of some database languages can be enhanced so that some functions involving aggregation can be defined. Yet, a direct implementation of the linear ordering in deductive database languages may seem unintuitive, and may not be very efficient to use in practice. We propose a logic for deductive databases which employs the notion of “identifying each tuple in a relation”. Through the use of these tuple-identifications, different linear orderings are defined as a result. This intuitively explains the reason why our logic has greater expressive power. The proposed logic language is non-deterministu in nature. However, non-determinism is not the real reason for the enhanced expressive power. A deterministic subset of the programs in this language is computational complete in the sense that it defines all the computable deterministic queries. Although the problem of deciding whether a program is in this subset is in general undecidable, we do provide a rather general sufficient test for identifying such programs. Also discussed in this paper is an extended notion of queries which allows both the input and the output of a query to contain interpreted constants of an infinite domain. We show that extended queries involving aggregation can also be defined in the language.},
journal = {SIGMOD Rec.},
month = may,
pages = {54–63},
numpages = {10}
}

@inproceedings{10.1145/93597.93622,
author = {Saraiya, Yatin P.},
title = {Hard Problems for Simple Logic Programs},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.93622},
doi = {10.1145/93597.93622},
abstract = {A number of optimizations have been proposed for Datalog programs involving a single intensional predicate (“single-IDB programs”). Examples include the detection of commutativity and separability ([Naug88],[RSUV89], [Ioan89a]) in linear logic programs, and the detection of ZYT-linearizability ([ZYT88], [RSUV89], [Sara89], [Sara90]) in nonlinear programs. We show that the natural generalizations of the commutativity and ZYT-linearizability problems (respectively, the sequencability and base-case linearizability problems) are undecidable. Our constructions involve the simulation of context-free grammars using single-IDB programs that have a bounded number of initialisation rules. The constructions may be used to show that containment (or equivalence) is undecidable for such programs, even if the programs are linear, or if each program contains a single recursive rule. These results tighten those of [Shmu87] and [Abit89].},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {64–73},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.93622,
author = {Saraiya, Yatin P.},
title = {Hard Problems for Simple Logic Programs},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.93622},
doi = {10.1145/93605.93622},
abstract = {A number of optimizations have been proposed for Datalog programs involving a single intensional predicate (“single-IDB programs”). Examples include the detection of commutativity and separability ([Naug88],[RSUV89], [Ioan89a]) in linear logic programs, and the detection of ZYT-linearizability ([ZYT88], [RSUV89], [Sara89], [Sara90]) in nonlinear programs. We show that the natural generalizations of the commutativity and ZYT-linearizability problems (respectively, the sequencability and base-case linearizability problems) are undecidable. Our constructions involve the simulation of context-free grammars using single-IDB programs that have a bounded number of initialisation rules. The constructions may be used to show that containment (or equivalence) is undecidable for such programs, even if the programs are linear, or if each program contains a single recursive rule. These results tighten those of [Shmu87] and [Abit89].},
journal = {SIGMOD Rec.},
month = may,
pages = {64–73},
numpages = {10}
}

@inproceedings{10.1145/93597.93624,
author = {Wang, Ke},
title = {Polynomial Time Designs toward Both BCNF and Efficient Data Manipulation},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.93624},
doi = {10.1145/93597.93624},
abstract = {We define the independence-reducibility based on a modification of key dependencies, which has better computational properties and is more practically useful than the original one based on key dependencies. Using this modification as a tool, we design BCNF databases that are highly desirable with respect to updates and/or query answering. In particular, given a set U of attributes and a set F of functional dependencies over U, we characterize when F can be embedded in a database scheme over U that is independent and is BCNF with respect to F, a polynomial time algorithm that tests this characterization and produces such a database scheme whenever possible is presented. The produced database scheme contains the fewest possible number of relation schemes. Then we show that designs of embedding constant-time-maintainable BCNF schemes and of embedding independence-reducible schemes share exactly the same method with the above design. Finally, a simple modification of this method yields a polynomial time algorithm for designing embedding separable BCNF schemes.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {74–83},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.93624,
author = {Wang, Ke},
title = {Polynomial Time Designs toward Both BCNF and Efficient Data Manipulation},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.93624},
doi = {10.1145/93605.93624},
abstract = {We define the independence-reducibility based on a modification of key dependencies, which has better computational properties and is more practically useful than the original one based on key dependencies. Using this modification as a tool, we design BCNF databases that are highly desirable with respect to updates and/or query answering. In particular, given a set U of attributes and a set F of functional dependencies over U, we characterize when F can be embedded in a database scheme over U that is independent and is BCNF with respect to F, a polynomial time algorithm that tests this characterization and produces such a database scheme whenever possible is presented. The produced database scheme contains the fewest possible number of relation schemes. Then we show that designs of embedding constant-time-maintainable BCNF schemes and of embedding independence-reducible schemes share exactly the same method with the above design. Finally, a simple modification of this method yields a polynomial time algorithm for designing embedding separable BCNF schemes.},
journal = {SIGMOD Rec.},
month = may,
pages = {74–83},
numpages = {10}
}

@inproceedings{10.1145/93597.93626,
author = {Atzeni, Paolo and Torlone, Riccardo},
title = {Efficient Updates to Independent Schemes in the Weak Instance Model},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.93626},
doi = {10.1145/93597.93626},
abstract = {The weak instance model is a framework to consider the relations in a database as a whole, regardless of the way attributes are grouped in the individual relations. Queries and updates can be performed involving any set of attributes. The management of updates is based on a lattice structure on the set of legal states, and inconsistencies and ambiguities can ariseIn the general case, the test for inconsistency and determinism may involve the application of the chase algorithm to the whole database. In this paper it is shown how, for the highly significant class of independent schemes, updates can be handled efficiently, considering only the relevant portion of the database.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {84–93},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.93626,
author = {Atzeni, Paolo and Torlone, Riccardo},
title = {Efficient Updates to Independent Schemes in the Weak Instance Model},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.93626},
doi = {10.1145/93605.93626},
abstract = {The weak instance model is a framework to consider the relations in a database as a whole, regardless of the way attributes are grouped in the individual relations. Queries and updates can be performed involving any set of attributes. The management of updates is based on a lattice structure on the set of legal states, and inconsistencies and ambiguities can ariseIn the general case, the test for inconsistency and determinism may involve the application of the chase algorithm to the whole database. In this paper it is shown how, for the highly significant class of independent schemes, updates can be handled efficiently, considering only the relevant portion of the database.},
journal = {SIGMOD Rec.},
month = may,
pages = {84–93},
numpages = {10}
}

@inproceedings{10.1145/93597.98719,
author = {Salzberg, Betty and Tsukerman, Alex and Gray, Jim and Stuewart, Michael and Uren, Susan and Vaughan, Bonnie},
title = {FastSort: A Distributed Single-Input Single-Output External Sort},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98719},
doi = {10.1145/93597.98719},
abstract = {External single-input single-output sorts can use multiple processors each with a large tournament replacement-selection in memory, and each with private disks to sort an input stream in linear elapsed time. Of course, increased numbers of processors, memories, and disks are required as the input file size grows. This paper analyzes the algorithm and reports the performance of an implementation.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {94–101},
numpages = {8},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98719,
author = {Salzberg, Betty and Tsukerman, Alex and Gray, Jim and Stuewart, Michael and Uren, Susan and Vaughan, Bonnie},
title = {FastSort: A Distributed Single-Input Single-Output External Sort},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98719},
doi = {10.1145/93605.98719},
abstract = {External single-input single-output sorts can use multiple processors each with a large tournament replacement-selection in memory, and each with private disks to sort an input stream in linear elapsed time. Of course, increased numbers of processors, memories, and disks are required as the input file size grows. This paper analyzes the algorithm and reports the performance of an implementation.},
journal = {SIGMOD Rec.},
month = may,
pages = {94–101},
numpages = {8}
}

@inproceedings{10.1145/93597.98720,
author = {Graefe, Goetz},
title = {Encapsulation of Parallelism in the Volcano Query Processing System},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98720},
doi = {10.1145/93597.98720},
abstract = {Volcano is a new dataflow query processing system we have developed for database systems research and education. The uniform interface between operators makes Volcano extensible by new operators. All operators are designed and coded as if they were meant for a single-process system only. When attempting to parallelize Volcano, we had to choose between two models of parallelization, called here the bracket and operator models. We describe the reasons for not choosing the bracket model, introduce the novel operator model, and provide details of Volcano's exchange operator that parallelizes all other operators. It allows intra-operator parallelism on partitioned datasets and both vertical and horizontal inter-operator parallelism. The exchange operator encapsulates all parallelism issues and therefore makes implementation of parallel database algorithms significantly easier and more robust. Included in this encapsulation is the translation between demand-driven dataflow within processes and data-driven dataflow between processes. Since the interface between Volcano operators is similar to the one used in “real,” commercial systems, the techniques described here can be used to parallelize other query processing engines.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {102–111},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98720,
author = {Graefe, Goetz},
title = {Encapsulation of Parallelism in the Volcano Query Processing System},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98720},
doi = {10.1145/93605.98720},
abstract = {Volcano is a new dataflow query processing system we have developed for database systems research and education. The uniform interface between operators makes Volcano extensible by new operators. All operators are designed and coded as if they were meant for a single-process system only. When attempting to parallelize Volcano, we had to choose between two models of parallelization, called here the bracket and operator models. We describe the reasons for not choosing the bracket model, introduce the novel operator model, and provide details of Volcano's exchange operator that parallelizes all other operators. It allows intra-operator parallelism on partitioned datasets and both vertical and horizontal inter-operator parallelism. The exchange operator encapsulates all parallelism issues and therefore makes implementation of parallel database algorithms significantly easier and more robust. Included in this encapsulation is the translation between demand-driven dataflow within processes and data-driven dataflow between processes. Since the interface between Volcano operators is similar to the one used in “real,” commercial systems, the techniques described here can be used to parallelize other query processing engines.},
journal = {SIGMOD Rec.},
month = may,
pages = {102–111},
numpages = {10}
}

@inproceedings{10.1145/93597.98721,
author = {Bernstein, Philip A. and Hsu, Meichun and Mann, Bruce},
title = {Implementing Recoverable Requests Using Queues},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98721},
doi = {10.1145/93597.98721},
abstract = {Transactions have been rigorously defined and extensively studied in the database and transaction processing literature, but little has been said about the handling of the requests for transaction execution in commercial TP systems, especially distributed ones, managing the flow of requests is often as important as executing the transactions themselves.This paper studies fault-tolerant protocols for managing the flow of transaction requests between clients that issue requests and servers that process them. We discuss how to implement these protocols using transactions and recoverable queuing systems. Queuing systems are used to move requests reliably between clients and servers. The protocols use queuing systems to ensure that the server processes each request exactly once and that a client processes each reply at least once. We treat request-reply protocols for single-transaction requests, for multi-transaction requests, and for requests that require interaction with the display after the request is submitted.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {112–122},
numpages = {11},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98721,
author = {Bernstein, Philip A. and Hsu, Meichun and Mann, Bruce},
title = {Implementing Recoverable Requests Using Queues},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98721},
doi = {10.1145/93605.98721},
abstract = {Transactions have been rigorously defined and extensively studied in the database and transaction processing literature, but little has been said about the handling of the requests for transaction execution in commercial TP systems, especially distributed ones, managing the flow of requests is often as important as executing the transactions themselves.This paper studies fault-tolerant protocols for managing the flow of transaction requests between clients that issue requests and servers that process them. We discuss how to implement these protocols using transactions and recoverable queuing systems. Queuing systems are used to move requests reliably between clients and servers. The protocols use queuing systems to ensure that the server processes each request exactly once and that a client processes each reply at least once. We treat request-reply protocols for single-transaction requests, for multi-transaction requests, and for requests that require interaction with the display after the request is submitted.},
journal = {SIGMOD Rec.},
month = may,
pages = {112–122},
numpages = {11}
}

@inproceedings{10.1145/93597.98722,
author = {Solworth, Jon A. and Orji, Cyril U.},
title = {Write-Only Disk Caches},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98722},
doi = {10.1145/93597.98722},
abstract = {With recent declines in the cost of semiconductor memory and the increasing need for high performance I/O disk systems, it makes sense to consider the design of large caches. In this paper, we consider the effect of caching writes. We show that cache sizes in the range of a few percent allow writes to be performed at negligible or no cost and independently of locality considerations.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {123–132},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98722,
author = {Solworth, Jon A. and Orji, Cyril U.},
title = {Write-Only Disk Caches},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98722},
doi = {10.1145/93605.98722},
abstract = {With recent declines in the cost of semiconductor memory and the increasing need for high performance I/O disk systems, it makes sense to consider the design of large caches. In this paper, we consider the effect of caching writes. We show that cache sizes in the range of a few percent allow writes to be performed at negligible or no cost and independently of locality considerations.},
journal = {SIGMOD Rec.},
month = may,
pages = {123–132},
numpages = {10}
}

@inproceedings{10.1145/93597.98723,
author = {Wolfson, Ouri and Ozeri, Aya},
title = {A New Paradigm for Parallel and Distributed Rule-Processing},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98723},
doi = {10.1145/93597.98723},
abstract = {This paper is concerned with the parallel evaluation of datalog rule programs, mainly by processors that are interconnected by a communication network. We introduce a paradigm, called data-reduction, for the parallel evaluation of a general datalog program. Several parallelization strategies discussed previously in [CW, GST, W, WS] are special cases of this paradigm. The paradigm parallelizes the evaluation by partitioning among the processors the instantiations of the rules. After presenting the paradigm, we discuss the following issues, that we see fundamental for parallelization strategies derived from the paradigm properties of the strategies that enable a reduction in the communication overhead, decomposability, load balancing, and application to programs with negation. We prove that decomposability, a concept introduced previously in [WS, CW], is undecidable.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {133–142},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98723,
author = {Wolfson, Ouri and Ozeri, Aya},
title = {A New Paradigm for Parallel and Distributed Rule-Processing},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98723},
doi = {10.1145/93605.98723},
abstract = {This paper is concerned with the parallel evaluation of datalog rule programs, mainly by processors that are interconnected by a communication network. We introduce a paradigm, called data-reduction, for the parallel evaluation of a general datalog program. Several parallelization strategies discussed previously in [CW, GST, W, WS] are special cases of this paradigm. The paradigm parallelizes the evaluation by partitioning among the processors the instantiations of the rules. After presenting the paradigm, we discuss the following issues, that we see fundamental for parallelization strategies derived from the paradigm properties of the strategies that enable a reduction in the communication overhead, decomposability, load balancing, and application to programs with negation. We prove that decomposability, a concept introduced previously in [WS, CW], is undecidable.},
journal = {SIGMOD Rec.},
month = may,
pages = {133–142},
numpages = {10}
}

@inproceedings{10.1145/93597.98724,
author = {Ganguly, Sumit and Silberschatz, Avi and Tsur, Shalom},
title = {A Framework for the Parallel Processing of Datalog Queries},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98724},
doi = {10.1145/93597.98724},
abstract = {This paper presents several complementary methods for the parallel, bottom-up evaluation of Datalog queries. We introduce the notion of a discriminating predicate, based on hash functions, that partitions the computation between the processors in order to achieve parallelism. A parallelization scheme with the property of non-redundant computation (no duplication of computation by processors) is then studied in detail. The mapping of Datalog programs onto a network of processors, such that the results is a non-redundant computation, is also studied. The methods reported in this paper clearly demonstrate the trade-offs between redundancy and interprocessor-communication for this class of problems.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {143–152},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98724,
author = {Ganguly, Sumit and Silberschatz, Avi and Tsur, Shalom},
title = {A Framework for the Parallel Processing of Datalog Queries},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98724},
doi = {10.1145/93605.98724},
abstract = {This paper presents several complementary methods for the parallel, bottom-up evaluation of Datalog queries. We introduce the notion of a discriminating predicate, based on hash functions, that partitions the computation between the processors in order to achieve parallelism. A parallelization scheme with the property of non-redundant computation (no duplication of computation by processors) is then studied in detail. The mapping of Datalog programs onto a network of processors, such that the results is a non-redundant computation, is also studied. The methods reported in this paper clearly demonstrate the trade-offs between redundancy and interprocessor-communication for this class of problems.},
journal = {SIGMOD Rec.},
month = may,
pages = {143–152},
numpages = {10}
}

@inproceedings{10.1145/93597.98725,
author = {Kogan, Boris and Jajodia, S.},
title = {Concurrency Control in Multilevel-Secure Databases Based on Replicated Architecture},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98725},
doi = {10.1145/93597.98725},
abstract = {In a multilevel secure database management system based on the replicated architecture, there is a separate database management system to manage data at or below each security level, and lower level data are replicated in all databases containing higher level data. In this paper, we address the open issue of concurrency control in such a system. We give a secure protocol that guarantees one-copy serializability of concurrent transaction executions and can be implemented in such a way that the size of the trusted code (including the code required for concurrency and recovery) is small.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {153–162},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98725,
author = {Kogan, Boris and Jajodia, S.},
title = {Concurrency Control in Multilevel-Secure Databases Based on Replicated Architecture},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98725},
doi = {10.1145/93605.98725},
abstract = {In a multilevel secure database management system based on the replicated architecture, there is a separate database management system to manage data at or below each security level, and lower level data are replicated in all databases containing higher level data. In this paper, we address the open issue of concurrency control in such a system. We give a secure protocol that guarantees one-copy serializability of concurrent transaction executions and can be implemented in such a way that the size of the trusted code (including the code required for concurrency and recovery) is small.},
journal = {SIGMOD Rec.},
month = may,
pages = {153–162},
numpages = {10}
}

@inproceedings{10.1145/93597.98726,
author = {Badrinath, B. R. and Ramamritham, Krithi},
title = {Performance Evaluation of Semantics-Based Multilevel Concurrency Control Protocols},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98726},
doi = {10.1145/93597.98726},
abstract = {For next generation information systems, concurrency control mechanisms are required to handle high level abstract operations and to meet high throughput demands. The currently available single level concurrency control mechanisms for reads and writes are inadequate for future complex information systems. In this paper, we will present a new multilevel concurrency protocol that uses a semantics-based notion of conflict, which is weaker than commutativity, called recoverability. Further, operations are scheduled according to relative conflict, a conflict notion based on the structure of operations.Performance evaluation via extensive simulation studies show that with our multilevel concurrency control protocol, the performance improvement is significant when compared to that of a single level two-phase locking based concurrency control scheme or to that of a multilevel concurrency control scheme based on commutativity alone. Further, simulation studies show that our new multilevel concurrency control protocol performs better even with resource contention.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {163–172},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98726,
author = {Badrinath, B. R. and Ramamritham, Krithi},
title = {Performance Evaluation of Semantics-Based Multilevel Concurrency Control Protocols},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98726},
doi = {10.1145/93605.98726},
abstract = {For next generation information systems, concurrency control mechanisms are required to handle high level abstract operations and to meet high throughput demands. The currently available single level concurrency control mechanisms for reads and writes are inadequate for future complex information systems. In this paper, we will present a new multilevel concurrency protocol that uses a semantics-based notion of conflict, which is weaker than commutativity, called recoverability. Further, operations are scheduled according to relative conflict, a conflict notion based on the structure of operations.Performance evaluation via extensive simulation studies show that with our multilevel concurrency control protocol, the performance improvement is significant when compared to that of a single level two-phase locking based concurrency control scheme or to that of a multilevel concurrency control scheme based on commutativity alone. Further, simulation studies show that our new multilevel concurrency control protocol performs better even with resource contention.},
journal = {SIGMOD Rec.},
month = may,
pages = {163–172},
numpages = {10}
}

@inproceedings{10.1145/93597.98727,
author = {Motro, Amihai and Yuan, Qiuhui},
title = {Querying Database Knowledge},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98727},
doi = {10.1145/93597.98727},
abstract = {The role of database knowledge is usually limited to the evaluation of data queries. In this paper we argue that when this knowledge is of substantial volume and complexity, there is genuine need to query this repository of information. Moreover, since users of the database may not be able to distinguish between information that is data and information that is knowledge, access to knowledge and data should be provided with a single, coherent instrument. We provide an informal review of various kinds of knowledge queries, with possible syntax and semantics. We then formalize a framework of knowledge-rich databases, and a simple query language consisting of a pair of retrieve and describe statements. The retrieve statement is for querying the data (it corresponds to the basic retrieval statement of various knowledge-rich database systems). The describe statement is for querying the knowledge. Essentially, it inquires about the meaning of a concept under specified circumstances. We provide algorithms for evaluating sound and finite knowledge answers to describe queries, and we demonstrate them with examples.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {173–183},
numpages = {11},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98727,
author = {Motro, Amihai and Yuan, Qiuhui},
title = {Querying Database Knowledge},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98727},
doi = {10.1145/93605.98727},
abstract = {The role of database knowledge is usually limited to the evaluation of data queries. In this paper we argue that when this knowledge is of substantial volume and complexity, there is genuine need to query this repository of information. Moreover, since users of the database may not be able to distinguish between information that is data and information that is knowledge, access to knowledge and data should be provided with a single, coherent instrument. We provide an informal review of various kinds of knowledge queries, with possible syntax and semantics. We then formalize a framework of knowledge-rich databases, and a simple query language consisting of a pair of retrieve and describe statements. The retrieve statement is for querying the data (it corresponds to the basic retrieval statement of various knowledge-rich database systems). The describe statement is for querying the knowledge. Essentially, it inquires about the meaning of a concept under specified circumstances. We provide algorithms for evaluating sound and finite knowledge answers to describe queries, and we demonstrate them with examples.},
journal = {SIGMOD Rec.},
month = may,
pages = {173–183},
numpages = {11}
}

@inproceedings{10.1145/93597.98728,
author = {Laenens, Els and Sacca, Domenico and Vermeir, Dirk},
title = {Extending Logic Programming},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98728},
doi = {10.1145/93597.98728},
abstract = {An extension of logic programming, called “ordered logic programming”, which includes some abstractions of the object-oriented paradigm, is presented. An ordered program consists of a number of modules (objects), where each module is composed by a number of rules possibly with negated head predicates. A sort of “isa” hierarchy can be defined among the modules in order to allow for rule inheritance. Therefore, every module sees its own rules as local rules and the rules of the other modules to which it is connected by the “isa” hierarchy as global rules. In this way, as local rules may hide global rules, it is possible to deal with default properties and exceptions. This new approach represents a novel attempt to combine the logic paradigm with the object-oriented one in knowledge base systems. Moreover, this approach provides a new ground for explaining some recent proposals of semantics for classical logic programs with negation in the rule bodies and gives an interesting semantics to logic programs with negated rule heads.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {184–193},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98728,
author = {Laenens, Els and Sacca, Domenico and Vermeir, Dirk},
title = {Extending Logic Programming},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98728},
doi = {10.1145/93605.98728},
abstract = {An extension of logic programming, called “ordered logic programming”, which includes some abstractions of the object-oriented paradigm, is presented. An ordered program consists of a number of modules (objects), where each module is composed by a number of rules possibly with negated head predicates. A sort of “isa” hierarchy can be defined among the modules in order to allow for rule inheritance. Therefore, every module sees its own rules as local rules and the rules of the other modules to which it is connected by the “isa” hierarchy as global rules. In this way, as local rules may hide global rules, it is possible to deal with default properties and exceptions. This new approach represents a novel attempt to combine the logic paradigm with the object-oriented one in knowledge base systems. Moreover, this approach provides a new ground for explaining some recent proposals of semantics for classical logic programs with negation in the rule bodies and gives an interesting semantics to logic programs with negated rule heads.},
journal = {SIGMOD Rec.},
month = may,
pages = {184–193},
numpages = {10}
}

@inproceedings{10.1145/93597.98729,
author = {Chrysanthis, Panayiotis K. and Ramamritham, Krithi},
title = {ACTA: A Framework for Specifying and Reasoning about Transaction Structure and Behavior},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98729},
doi = {10.1145/93597.98729},
abstract = {Recently, a number of extensions to the traditional transaction model have been proposed to support new information-intensive applications such as CAD/CAM and software development. However, these extended models capture only a subset of interactions that can be found in such applications, and represent only some of the points within the spectrum of interactions possible in competitive and cooperative environments.ACTA is a formalizable framework developed for characterizing the whole spectrum of interactions. The ACTA framework is not yet another transaction model, but is intended to unify the existing models. ACTA allows for specifying the structure and the behavior of transactions as well as for reasoning about the concurrency and recovery properties of the transactions. In ACTA, the semantics of interactions are expressed in terms of transactions' effects on the commit and abort of other transactions and on objects' state and concurrency status (i.e., synchronization state). Its ability to capture the semantics of previously proposed transaction models is indicative of its generality. The reasoning capabilities of this framework have also been tested by using the framework to study the properties of a new model that is derived by combining two existing transaction models.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {194–203},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98729,
author = {Chrysanthis, Panayiotis K. and Ramamritham, Krithi},
title = {ACTA: A Framework for Specifying and Reasoning about Transaction Structure and Behavior},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98729},
doi = {10.1145/93605.98729},
abstract = {Recently, a number of extensions to the traditional transaction model have been proposed to support new information-intensive applications such as CAD/CAM and software development. However, these extended models capture only a subset of interactions that can be found in such applications, and represent only some of the points within the spectrum of interactions possible in competitive and cooperative environments.ACTA is a formalizable framework developed for characterizing the whole spectrum of interactions. The ACTA framework is not yet another transaction model, but is intended to unify the existing models. ACTA allows for specifying the structure and the behavior of transactions as well as for reasoning about the concurrency and recovery properties of the transactions. In ACTA, the semantics of interactions are expressed in terms of transactions' effects on the commit and abort of other transactions and on objects' state and concurrency status (i.e., synchronization state). Its ability to capture the semantics of previously proposed transaction models is indicative of its generality. The reasoning capabilities of this framework have also been tested by using the framework to study the properties of a new model that is derived by combining two existing transaction models.},
journal = {SIGMOD Rec.},
month = may,
pages = {194–203},
numpages = {10}
}

@inproceedings{10.1145/93597.98730,
author = {Dayal, Umeshwar and Hsu, Meichun and Ladin, Rivka},
title = {Organizing Long-Running Activities with Triggers and Transactions},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98730},
doi = {10.1145/93597.98730},
abstract = {This paper addresses the problem of organising and controlling activities that involve multiple steps of processing and that typically are of long duration. We explore the use of triggers and transactions to specify and organize such long-running activities. Triggers offer data- or event-driven specification of control flow, and thus provide a flexible and modular framework with which the control structures of the activities can be extended or modified. We describe a model based on event-condition-action rules and coupling modes. The execution of these rules is governed by an extended nested transaction model. Through a detailed example, we illustrate the utility of the various features of the model for chaining related steps without sacrificing concurrency, for enforcing integrity constraints, and for providing flexible failure and exception handling.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {204–214},
numpages = {11},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98730,
author = {Dayal, Umeshwar and Hsu, Meichun and Ladin, Rivka},
title = {Organizing Long-Running Activities with Triggers and Transactions},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98730},
doi = {10.1145/93605.98730},
abstract = {This paper addresses the problem of organising and controlling activities that involve multiple steps of processing and that typically are of long duration. We explore the use of triggers and transactions to specify and organize such long-running activities. Triggers offer data- or event-driven specification of control flow, and thus provide a flexible and modular framework with which the control structures of the activities can be extended or modified. We describe a model based on event-condition-action rules and coupling modes. The execution of these rules is governed by an extended nested transaction model. Through a detailed example, we illustrate the utility of the various features of the model for chaining related steps without sacrificing concurrency, for enforcing integrity constraints, and for providing flexible failure and exception handling.},
journal = {SIGMOD Rec.},
month = may,
pages = {204–214},
numpages = {11}
}

@inproceedings{10.1145/93597.98731,
author = {Breitbart, Yuri and Silberschatz, Avi and Thompson, Glenn R.},
title = {Reliable Transaction Management in a Multidatabase System},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98731},
doi = {10.1145/93597.98731},
abstract = {A model of a multidatabase system is defined in which each local DBMS uses the two-phase locking protocol Locks are released by a global transaction only after the transaction commits or aborts at each local site. Failures may occur during the processing of transactions. We design a fault tolerant transaction management algorithm and recovery procedures that retain global database consistency. We also show that our algorithms ensure freedom from global deadlocks of any kind.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {215–224},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98731,
author = {Breitbart, Yuri and Silberschatz, Avi and Thompson, Glenn R.},
title = {Reliable Transaction Management in a Multidatabase System},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98731},
doi = {10.1145/93605.98731},
abstract = {A model of a multidatabase system is defined in which each local DBMS uses the two-phase locking protocol Locks are released by a global transaction only after the transaction commits or aborts at each local site. Failures may occur during the processing of transactions. We design a fault tolerant transaction management algorithm and recovery procedures that retain global database consistency. We also show that our algorithms ensure freedom from global deadlocks of any kind.},
journal = {SIGMOD Rec.},
month = may,
pages = {215–224},
numpages = {10}
}

@inproceedings{10.1145/93597.98732,
author = {Cacace, F. and Ceri, S. and Crespi-Reghizzi, S. and Tanca, L. and Zicari, R.},
title = {Integrating Object-Oriented Data Modelling with a Rule-Based Programming Paradigm},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98732},
doi = {10.1145/93597.98732},
abstract = {LOGRES is a new project for the development of extended database systems which is based on the integration of the object-oriented data modelling paradigm and of the rule-based approach for the specification of queries and updates.The data model supports generalization hierarchies and object sharing, the rule-based language extends Datalog to support generalized type constructors (sets, multisets, and sequences), rule-based integrity constraints are automatically produced by analyzing schema definitions. Modularization is a fundamental feature, as modules encapsulate queries and updates, when modules are applied to a LOGRES database, their side effects can be controlled.The LOGRES project is a follow-up of the ALGRES project, and takes advantage of the ALGRES programming environment for the development of a fast prototype.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {225–236},
numpages = {12},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98732,
author = {Cacace, F. and Ceri, S. and Crespi-Reghizzi, S. and Tanca, L. and Zicari, R.},
title = {Integrating Object-Oriented Data Modelling with a Rule-Based Programming Paradigm},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98732},
doi = {10.1145/93605.98732},
abstract = {LOGRES is a new project for the development of extended database systems which is based on the integration of the object-oriented data modelling paradigm and of the rule-based approach for the specification of queries and updates.The data model supports generalization hierarchies and object sharing, the rule-based language extends Datalog to support generalized type constructors (sets, multisets, and sequences), rule-based integrity constraints are automatically produced by analyzing schema definitions. Modularization is a fundamental feature, as modules encapsulate queries and updates, when modules are applied to a LOGRES database, their side effects can be controlled.The LOGRES project is a follow-up of the ALGRES project, and takes advantage of the ALGRES programming environment for the development of a fast prototype.},
journal = {SIGMOD Rec.},
month = may,
pages = {225–236},
numpages = {12}
}

@inproceedings{10.1145/93597.98733,
author = {Kiernan, G. and de Maindreville, C. and Simon, E.},
title = {Making Deductive Databases a Practical Technology: A Step Forward},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98733},
doi = {10.1145/93597.98733},
abstract = {Deductive databases provide a formal framework to study rule-based query languages that are extensions of first-order logic. However, deductive database languages and their current implementations do not seem appropriate for improving the development of real applications or even sample of them. Our goal is to make deductive database technology practical. The design and implementation of the RDL1 system, presented in this paper, constitute a step toward this goal. Our approach is based on the integration of a production rule language within a relational database system, the development of a rule-based programming environment and the support of system extensibility using Abstract Data Types. We discuss important practical experience gained during the implementation of the system. Also, comparisons with related work such as LDL, STARBURST and POSTGRES are given.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {237–246},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98733,
author = {Kiernan, G. and de Maindreville, C. and Simon, E.},
title = {Making Deductive Databases a Practical Technology: A Step Forward},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98733},
doi = {10.1145/93605.98733},
abstract = {Deductive databases provide a formal framework to study rule-based query languages that are extensions of first-order logic. However, deductive database languages and their current implementations do not seem appropriate for improving the development of real applications or even sample of them. Our goal is to make deductive database technology practical. The design and implementation of the RDL1 system, presented in this paper, constitute a step toward this goal. Our approach is based on the integration of a production rule language within a relational database system, the development of a rule-based programming environment and the support of system extensibility using Abstract Data Types. We discuss important practical experience gained during the implementation of the system. Also, comparisons with related work such as LDL, STARBURST and POSTGRES are given.},
journal = {SIGMOD Rec.},
month = may,
pages = {237–246},
numpages = {10}
}

@inproceedings{10.1145/93597.98734,
author = {Mumick, I. S. and Finkelstein, S. J. and Pirahesh, Hamid and Ramakrishnan, Raghu},
title = {Magic is Relevant},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98734},
doi = {10.1145/93597.98734},
abstract = {We define the magic-sets transformation for traditional relational systems (with duplicates, aggregation and grouping), as well as for relational systems extended with recursion. We compare the magic-sets rewriting to traditional optimization techniques for nonrecursive queries, and use performance experiments to argue that the magic-sets transformation is often a better optimization technique.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {247–258},
numpages = {12},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98734,
author = {Mumick, I. S. and Finkelstein, S. J. and Pirahesh, Hamid and Ramakrishnan, Raghu},
title = {Magic is Relevant},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98734},
doi = {10.1145/93605.98734},
abstract = {We define the magic-sets transformation for traditional relational systems (with duplicates, aggregation and grouping), as well as for relational systems extended with recursion. We compare the magic-sets rewriting to traditional optimization techniques for nonrecursive queries, and use performance experiments to argue that the magic-sets transformation is often a better optimization technique.},
journal = {SIGMOD Rec.},
month = may,
pages = {247–258},
numpages = {12}
}

@inproceedings{10.1145/93597.98735,
author = {Widom, Jennifer and Finkelstein, S. J.},
title = {Set-Oriented Production Rules in Relational Database Systems},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98735},
doi = {10.1145/93597.98735},
abstract = {We propose incorporating a production rules facility into a relational database system. Such a facility allows definition of database operations that are automatically executed whenever certain conditions are met. In keeping with the set-oriented approach of relational data manipulation languages, our production rules are also set-oriented—they are triggered by sets of changes to the database and may perform sets of changes. The condition and action parts of our production rules may refer to the current state of the database as well as to the sets of changes triggering the rules. We define a syntax for production rule definition as an extension to SQL. A model of system behavior is used to give an exact semantics for production rule execution, taking into account externally-generated operations, self-triggering rules, and simultaneous triggering of multiple rules.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {259–270},
numpages = {12},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98735,
author = {Widom, Jennifer and Finkelstein, S. J.},
title = {Set-Oriented Production Rules in Relational Database Systems},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98735},
doi = {10.1145/93605.98735},
abstract = {We propose incorporating a production rules facility into a relational database system. Such a facility allows definition of database operations that are automatically executed whenever certain conditions are met. In keeping with the set-oriented approach of relational data manipulation languages, our production rules are also set-oriented—they are triggered by sets of changes to the database and may perform sets of changes. The condition and action parts of our production rules may refer to the current state of the database as well as to the sets of changes triggering the rules. We define a syntax for production rule definition as an extension to SQL. A model of system behavior is used to give an exact semantics for production rule execution, taking into account externally-generated operations, self-triggering rules, and simultaneous triggering of multiple rules.},
journal = {SIGMOD Rec.},
month = may,
pages = {259–270},
numpages = {12}
}

@inproceedings{10.1145/93597.98736,
author = {Hanson, Eric N. and Chaabouni, Moez and Kim, Chang-Ho and Wang, Yu-Wang},
title = {A Predicate Matching Algorithm for Database Rule Systems},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98736},
doi = {10.1145/93597.98736},
abstract = {Forward-chaining rule systems must test each newly asserted fact against a collection of predicates to find those rules that match the fact. Expert system rule engines use a simple combination of hashing and sequential search for this matching. We introduce an algorithm for finding the matching predicates that is more efficient than the standard algorithm when the number of predicates is large. We focus on equality and inequality predicates on totally ordered domains. This algorithm is well-suited for database rule systems, where predicate-testing speed is critical. A key component of the algorithm is the interval binary search tree (IBS-tree). The IBS-tree is designed to allow efficient retrieval of all intervals (e.g. range predicates) that overlap a point, while allowing dynamic insertion and deletion of intervals. The algorithm could also be used to improve the performance of forward-chaining inference engines for large expert systems applications.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {271–280},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98736,
author = {Hanson, Eric N. and Chaabouni, Moez and Kim, Chang-Ho and Wang, Yu-Wang},
title = {A Predicate Matching Algorithm for Database Rule Systems},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98736},
doi = {10.1145/93605.98736},
abstract = {Forward-chaining rule systems must test each newly asserted fact against a collection of predicates to find those rules that match the fact. Expert system rule engines use a simple combination of hashing and sequential search for this matching. We introduce an algorithm for finding the matching predicates that is more efficient than the standard algorithm when the number of predicates is large. We focus on equality and inequality predicates on totally ordered domains. This algorithm is well-suited for database rule systems, where predicate-testing speed is critical. A key component of the algorithm is the interval binary search tree (IBS-tree). The IBS-tree is designed to allow efficient retrieval of all intervals (e.g. range predicates) that overlap a point, while allowing dynamic insertion and deletion of intervals. The algorithm could also be used to improve the performance of forward-chaining inference engines for large expert systems applications.},
journal = {SIGMOD Rec.},
month = may,
pages = {271–280},
numpages = {10}
}

@inproceedings{10.1145/93597.98737,
author = {Stonebraker, Michael and Jhingran, Anant and Goh, Jeffrey and Potamianos, Spyros},
title = {On Rules, Procedure, Caching and Views in Data Base Systems},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98737},
doi = {10.1145/93597.98737},
abstract = {This paper demonstrates that a simple rule system can be constructed that supports a more powerful view system than available in current commercial systems. Not only can views be specified by using rules but also special semantics for resolving ambiguous view updates are simply additional rules. Moreover, procedural data types as proposed in POSTGRES are also efficiently simulated by the same rules system. Lastly, caching of the action part of certain rules is a possible performance enhancement and can be applied to materialize views as well as to cache procedural data items. Hence, we conclude that a rule system is a fundamental concept in a next generation DBMS, and it subsumes both views and procedures as special cases.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {281–290},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98737,
author = {Stonebraker, Michael and Jhingran, Anant and Goh, Jeffrey and Potamianos, Spyros},
title = {On Rules, Procedure, Caching and Views in Data Base Systems},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98737},
doi = {10.1145/93605.98737},
abstract = {This paper demonstrates that a simple rule system can be constructed that supports a more powerful view system than available in current commercial systems. Not only can views be specified by using rules but also special semantics for resolving ambiguous view updates are simply additional rules. Moreover, procedural data types as proposed in POSTGRES are also efficiently simulated by the same rules system. Lastly, caching of the action part of certain rules is a possible performance enhancement and can be applied to materialize views as well as to cache procedural data items. Hence, we conclude that a rule system is a fundamental concept in a next generation DBMS, and it subsumes both views and procedures as special cases.},
journal = {SIGMOD Rec.},
month = may,
pages = {281–290},
numpages = {10}
}

@inproceedings{10.1145/93597.98738,
author = {Rosenthal, Arnon and Galindo-Legaria, Cesar},
title = {Query Graphs, Implementing Trees, and Freely-Reorderable Outerjoins},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98738},
doi = {10.1145/93597.98738},
abstract = {We determine when a join/outerjoin query can be expressed unambiguously as a query graph, without an explicit specification of the order of evaluation. To do so, we first characterize the set of expression trees that implement a given join/outerjoin query graph, and investigate the existence of transformations among the various trees. Our main theorem is that a join/outerjoin query is freely reorderable if the query graph derived from it falls within a particular class, every tree that “implements” such a graph evaluates to the same result.The result has applications to language design and query optimization. Languages that generate queries within such a class do not require the user to indicate priority among join operations, and hence may present a simplified syntax. And it is unnecessary to add extensive analyses to a conventional query optimizer in order to generate legal reorderings for a freely-reorderable language.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {291–299},
numpages = {9},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98738,
author = {Rosenthal, Arnon and Galindo-Legaria, Cesar},
title = {Query Graphs, Implementing Trees, and Freely-Reorderable Outerjoins},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98738},
doi = {10.1145/93605.98738},
abstract = {We determine when a join/outerjoin query can be expressed unambiguously as a query graph, without an explicit specification of the order of evaluation. To do so, we first characterize the set of expression trees that implement a given join/outerjoin query graph, and investigate the existence of transformations among the various trees. Our main theorem is that a join/outerjoin query is freely reorderable if the query graph derived from it falls within a particular class, every tree that “implements” such a graph evaluates to the same result.The result has applications to language design and query optimization. Languages that generate queries within such a class do not require the user to indicate priority among join operations, and hence may present a simplified syntax. And it is unnecessary to add extensive analyses to a conventional query optimizer in order to generate legal reorderings for a freely-reorderable language.},
journal = {SIGMOD Rec.},
month = may,
pages = {291–299},
numpages = {9}
}

@inproceedings{10.1145/93597.98739,
author = {Shekita, Eugene J. and Carey, Michael J.},
title = {A Performance Evaluation of Pointer-Based Joins},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98739},
doi = {10.1145/93597.98739},
abstract = {In this paper we describe three pointer-based join algorithms that are simple variants of the nested-loops, sort-merge, and hybrid-hash join algorithms used in relational database systems. Each join algorithm is described and an analysis is carried out to compare the performance of the pointer-based algorithms to their standard, non-pointer-based counterparts. The results of the analysis show that the pointer-based algorithms can provide significant performance gains in many situations. The results also show that the pointer-based nested-loops join algorithm, which is perhaps the most natural pointer-based join algorithm to consider using in an object-oriented database system, performs quite poorly on most medium to large joins.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {300–311},
numpages = {12},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98739,
author = {Shekita, Eugene J. and Carey, Michael J.},
title = {A Performance Evaluation of Pointer-Based Joins},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98739},
doi = {10.1145/93605.98739},
abstract = {In this paper we describe three pointer-based join algorithms that are simple variants of the nested-loops, sort-merge, and hybrid-hash join algorithms used in relational database systems. Each join algorithm is described and an analysis is carried out to compare the performance of the pointer-based algorithms to their standard, non-pointer-based counterparts. The results of the analysis show that the pointer-based algorithms can provide significant performance gains in many situations. The results also show that the pointer-based nested-loops join algorithm, which is perhaps the most natural pointer-based join algorithm to consider using in an object-oriented database system, performs quite poorly on most medium to large joins.},
journal = {SIGMOD Rec.},
month = may,
pages = {300–311},
numpages = {12}
}

@inproceedings{10.1145/93597.98740,
author = {Ioannidis, Y. E. and Kang, Younkyung},
title = {Randomized Algorithms for Optimizing Large Join Queries},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98740},
doi = {10.1145/93597.98740},
abstract = {Query optimization for relational database systems is a combinatorial optimization problem, which makes exhaustive search unacceptable as the query size grows. Randomized algorithms, such as Simulated Annealing (SA) and Iterative Improvement (II), are viable alternatives to exhaustive search. We have adapted these algorithms to the optimization of project-select-join queries. We have tested them on large queries of various types with different databases, concluding that in most cases SA identifies a lower cost access plan than II. To explain this result, we have studied the shape of the cost function over the solution space associated with such queries and we have conjectured that it resembles a 'cup' with relatively small variations at the bottom. This has inspired a new Two Phase Optimization algorithm, which is a combination of Simulated Annealing and Iterative Improvement. Experimental results show that Two Phase Optimization outperforms the original algorithms in terms of both output quality and running time.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {312–321},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98740,
author = {Ioannidis, Y. E. and Kang, Younkyung},
title = {Randomized Algorithms for Optimizing Large Join Queries},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98740},
doi = {10.1145/93605.98740},
abstract = {Query optimization for relational database systems is a combinatorial optimization problem, which makes exhaustive search unacceptable as the query size grows. Randomized algorithms, such as Simulated Annealing (SA) and Iterative Improvement (II), are viable alternatives to exhaustive search. We have adapted these algorithms to the optimization of project-select-join queries. We have tested them on large queries of various types with different databases, concluding that in most cases SA identifies a lower cost access plan than II. To explain this result, we have studied the shape of the cost function over the solution space associated with such queries and we have conjectured that it resembles a 'cup' with relatively small variations at the bottom. This has inspired a new Two Phase Optimization algorithm, which is a combination of Simulated Annealing and Iterative Improvement. Experimental results show that Two Phase Optimization outperforms the original algorithms in terms of both output quality and running time.},
journal = {SIGMOD Rec.},
month = may,
pages = {312–321},
numpages = {10}
}

@inproceedings{10.1145/93597.98741,
author = {Beckmann, Norbert and Kriegel, Hans-Peter and Schneider, Ralf and Seeger, Bernhard},
title = {The R*-Tree: An Efficient and Robust Access Method for Points and Rectangles},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98741},
doi = {10.1145/93597.98741},
abstract = {The R-tree, one of the most popular access methods for rectangles, is based on the heuristic optimization of the area of the enclosing rectangle in each inner node. By running numerous experiments in a standardized testbed under highly varying data, queries and operations, we were able to design the R*-tree which incorporates a combined optimization of area, margin and overlap of each enclosing rectangle in the directory. Using our standardized testbed in an exhaustive performance comparison, it turned out that the R*-tree clearly outperforms the existing R-tree variants. Guttman's linear and quadratic R-tree and Greene's variant of the R-tree. This superiority of the R*-tree holds for different types of queries and operations, such as map overlay, for both rectangles and multidimensional points in all experiments. From a practical point of view the R*-tree is very attractive because of the following two reasons 1 it efficiently supports point and spatial data at the same time and 2 its implementation cost is only slightly higher than that of other R-trees.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {322–331},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98741,
author = {Beckmann, Norbert and Kriegel, Hans-Peter and Schneider, Ralf and Seeger, Bernhard},
title = {The R*-Tree: An Efficient and Robust Access Method for Points and Rectangles},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98741},
doi = {10.1145/93605.98741},
abstract = {The R-tree, one of the most popular access methods for rectangles, is based on the heuristic optimization of the area of the enclosing rectangle in each inner node. By running numerous experiments in a standardized testbed under highly varying data, queries and operations, we were able to design the R*-tree which incorporates a combined optimization of area, margin and overlap of each enclosing rectangle in the directory. Using our standardized testbed in an exhaustive performance comparison, it turned out that the R*-tree clearly outperforms the existing R-tree variants. Guttman's linear and quadratic R-tree and Greene's variant of the R-tree. This superiority of the R*-tree holds for different types of queries and operations, such as map overlay, for both rectangles and multidimensional points in all experiments. From a practical point of view the R*-tree is very attractive because of the following two reasons 1 it efficiently supports point and spatial data at the same time and 2 its implementation cost is only slightly higher than that of other R-trees.},
journal = {SIGMOD Rec.},
month = may,
pages = {322–331},
numpages = {10}
}

@inproceedings{10.1145/93597.98742,
author = {Jagadish, H. V.},
title = {Linear Clustering of Objects with Multiple Attributes},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98742},
doi = {10.1145/93597.98742},
abstract = {There is often a need to map a multi-dimensional space on to a one-dimensional space. For example, this kind of mapping has been proposed to permit the use of one-dimensional indexing techniques to a multi-dimensional index space such as in a spatial database. This kind of mapping is also of value in assigning physical storage, such as assigning buckets to records that have been indexed on multiple attributes, to minimize the disk access effort.In this paper, we discuss what the desired properties of such a mapping are, and evaluate, through analysis and simulation, several mappings that have been proposed in the past. We present a mapping based on Hilbert's space-filling curve, which out-performs previously proposed mappings on average over a variety of different operating conditions.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {332–342},
numpages = {11},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98742,
author = {Jagadish, H. V.},
title = {Linear Clustering of Objects with Multiple Attributes},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98742},
doi = {10.1145/93605.98742},
abstract = {There is often a need to map a multi-dimensional space on to a one-dimensional space. For example, this kind of mapping has been proposed to permit the use of one-dimensional indexing techniques to a multi-dimensional index space such as in a spatial database. This kind of mapping is also of value in assigning physical storage, such as assigning buckets to records that have been indexed on multiple attributes, to minimize the disk access effort.In this paper, we discuss what the desired properties of such a mapping are, and evaluate, through analysis and simulation, several mappings that have been proposed in the past. We present a mapping based on Hilbert's space-filling curve, which out-performs previously proposed mappings on average over a variety of different operating conditions.},
journal = {SIGMOD Rec.},
month = may,
pages = {332–342},
numpages = {11}
}

@inproceedings{10.1145/93597.98743,
author = {Orenstein, Jack},
title = {A Comparison of Spatial Query Processing Techniques for Native and Parameter Spaces},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98743},
doi = {10.1145/93597.98743},
abstract = {Spatial queries can be evaluated in native space or in a parameter space. In the latter case, data objects are transformed into points and query objects are transformed into search regions. The requirement for different data and query representations may prevent the use of parameter-space searching in some applications. Native-space and parameter-space searching are compared in the context of a z order-based spatial access method. Experimental results show that when there is a single query object, searching in parameter space can be faster than searching in native space, if the data and query objects are large enough, and if sufficient redundancy is used for the query representation. The result is, however, less accurate than the native space result. When there are multiple query objects, native-space searching is better initially, but as the number of query objects increases, parameter space searching with low redundancy is superior. Native-space searching is much more accurate for multiple-object queries.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {343–352},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98743,
author = {Orenstein, Jack},
title = {A Comparison of Spatial Query Processing Techniques for Native and Parameter Spaces},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98743},
doi = {10.1145/93605.98743},
abstract = {Spatial queries can be evaluated in native space or in a parameter space. In the latter case, data objects are transformed into points and query objects are transformed into search regions. The requirement for different data and query representations may prevent the use of parameter-space searching in some applications. Native-space and parameter-space searching are compared in the context of a z order-based spatial access method. Experimental results show that when there is a single query object, searching in parameter space can be faster than searching in native space, if the data and query objects are large enough, and if sufficient redundancy is used for the query representation. The result is, however, less accurate than the native space result. When there are multiple query objects, native-space searching is better initially, but as the number of query objects increases, parameter space searching with low redundancy is superior. Native-space searching is much more accurate for multiple-object queries.},
journal = {SIGMOD Rec.},
month = may,
pages = {343–352},
numpages = {10}
}

@inproceedings{10.1145/93597.98744,
author = {Lomet, David and Salzberg, Betty},
title = {The Performance of a Multiversion Access Method},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98744},
doi = {10.1145/93597.98744},
abstract = {The Time-Split B-tree is an integrated index structure for a versioned timestamped database. It gradually migrates data from a current database to an historical database, records migrating when nodes split. Records valid at the split time are placed in both an historical node and a current node. This implies some redundancy. Using both analysis and simulation, we characterise the amount of redundancy, the space utilization, and the record addition (insert or update) performance for a spectrum of different rates of insertion versus update. Three splitting policies are studied which alter the conditions under which either time splits or key space splits are performed.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {353–363},
numpages = {11},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98744,
author = {Lomet, David and Salzberg, Betty},
title = {The Performance of a Multiversion Access Method},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98744},
doi = {10.1145/93605.98744},
abstract = {The Time-Split B-tree is an integrated index structure for a versioned timestamped database. It gradually migrates data from a current database to an historical database, records migrating when nodes split. Records valid at the split time are placed in both an historical node and a current node. This implies some redundancy. Using both analysis and simulation, we characterise the amount of redundancy, the space utilization, and the record addition (insert or update) performance for a spectrum of different rates of insertion versus update. Three splitting policies are studied which alter the conditions under which either time splits or key space splits are performed.},
journal = {SIGMOD Rec.},
month = may,
pages = {353–363},
numpages = {11}
}

@inproceedings{10.1145/93597.98745,
author = {Kemper, Alfons and Moerkotte, Guido},
title = {Access Support in Object Bases},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98745},
doi = {10.1145/93597.98745},
abstract = {In this work access support relations are introduced as a means for optimizing query processing in object-oriented database systems. The general idea is to maintain redundant separate structures (disassociated from the object representation) to store object references that are frequently traversed in database queries. The proposed access support relation technique is no longer restricted to relate an object (tuple) to an atomic value (attribute value) as in conventional indexing. Rather, access support relations relate objects with each other and can span over reference chains which may contain collection-valued components in order to support queries involving path expressions. We present several alternative extensions of access support relations for a given path expression, the best of which has to be determined according to the application-specific database usage profile. An analytical cost model for access support relations and their application is developed. This analytical cost model is, in particular, used to determine the best access support relation extension and decomposition with respect to the specific database configuration and application profile.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {364–374},
numpages = {11},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98745,
author = {Kemper, Alfons and Moerkotte, Guido},
title = {Access Support in Object Bases},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98745},
doi = {10.1145/93605.98745},
abstract = {In this work access support relations are introduced as a means for optimizing query processing in object-oriented database systems. The general idea is to maintain redundant separate structures (disassociated from the object representation) to store object references that are frequently traversed in database queries. The proposed access support relation technique is no longer restricted to relate an object (tuple) to an atomic value (attribute value) as in conventional indexing. Rather, access support relations relate objects with each other and can span over reference chains which may contain collection-valued components in order to support queries involving path expressions. We present several alternative extensions of access support relations for a given path expression, the best of which has to be determined according to the application-specific database usage profile. An analytical cost model for access support relations and their application is developed. This analytical cost model is, in particular, used to determine the best access support relation extension and decomposition with respect to the specific database configuration and application profile.},
journal = {SIGMOD Rec.},
month = may,
pages = {364–374},
numpages = {11}
}

@inproceedings{10.1145/93597.98746,
author = {Olken, Frank and Rotem, Doron and Xu, Ping},
title = {Random Sampling from Hash Files},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98746},
doi = {10.1145/93597.98746},
abstract = {In this paper we discuss simple random sampling from hash files on secondary storage. We consider both iterative and batch sampling algorithms from both static and dynamic hashing methods. The static methods considered are open addressing hash files and hash files with separate overflow chains. The dynamic hashing methods considered are Linear Hash files [Lit80] and Extendible Hash files [FNPS79]. We give the cost of sampling in terms of the cost of successfully searching a hash file and show how to exploit features of the dynamic hashing methods to improve sampling efficiency.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {375–386},
numpages = {12},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98746,
author = {Olken, Frank and Rotem, Doron and Xu, Ping},
title = {Random Sampling from Hash Files},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98746},
doi = {10.1145/93605.98746},
abstract = {In this paper we discuss simple random sampling from hash files on secondary storage. We consider both iterative and batch sampling algorithms from both static and dynamic hashing methods. The static methods considered are open addressing hash files and hash files with separate overflow chains. The dynamic hashing methods considered are Linear Hash files [Lit80] and Extendible Hash files [FNPS79]. We give the cost of sampling in terms of the cost of successfully searching a hash file and show how to exploit features of the dynamic hashing methods to improve sampling efficiency.},
journal = {SIGMOD Rec.},
month = may,
pages = {375–386},
numpages = {12}
}

@inproceedings{10.1145/93597.98747,
author = {Cha, Sang K. and Wiederhold, Gio},
title = {Kaleidoscope: A Cooperative Menu-Guided Query Interface},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98747},
doi = {10.1145/93597.98747},
abstract = {Querying databases to obtain information requires the user's knowledge of query language and underlying data. However, because the knowledge in human long-term memory is imprecise, incomplete, and often incorrect, user queries are subject to various types of failure. These may include spelling mistakes, the violation of the syntax and semantics of a query language, and the misconception of the entities and relationships in a database.Kaleidoscope is a cooperative query interface whose knowledge guides users to avoid most failure during query creation. We call this type of cooperative behavior intraquery guidance. To enable this early, active engagement in the user's process of query creation, Kaleidoscope reduces the granularity of user-system interaction via a context-sensitive menu. The system generates valid query constituents as menu choices step-by-step by interpreting a language grammar, and the user creates a query following this menu guidance[2]. For instance, it takes four steps to create the following query [Q1] Who/1 authored/2 'Al'/3 journal papers/(3+) in 'Postquery COOP'/4At each of such steps, as the user selects one of menu choices, the system updates its partial query status window. If a choice is unique as in (3+), it is taken automatically. To guide the user's entry of values, the system provides a pop-up menu for each value domain.With Kaleidoscope's process of choice generation tightly controlled by the system's knowledge of query language and underlying data, users need not remember the query language and the underlying database structure but merely recognize or identify the constituents coming one after another that match their intended query. The system provides additional guidance for users to avoid creating semantically inconsistent queries. It informs the user of any derived predicates on the completion of a user-selected predicate. To illustrate this, consider a partially constructed SQL query [Q2] SELECT * FROM professor p#1 WHERE p#1 dept = 'CS' AND p#1 salary &lt; 40000Suppose that the system has an integrity constraint [IC] FROM professor p IF p dept = 'CS' AND p salary &lt; 45000 THEN p rank = 'Assistant'This rules states that a CS professor whose salary is less than 45000 is an assistant professor. With the replacement of rule variable p in IC by Q2's range variable p#1, IC's leading two predicates subsume Q2's query condition, producing p#1 rank = 'Assistant'. Because this derived predicate is not subsumed by Q2's query condition, the system suspects that the user may not know of it and presents it to the user.Derived predicates, together with user-selected ones, constrain the user's further conjunctive extension of the partial query condition. For example, the system prunes the field rank (as well as the field dept) in the conjunctive extension of Q2, because the derived condition restricts the value of this field to a constant.As shown in examples, we apply Kaleidoscope's approach to two linear-syntax languages in different levels of abstraction SQL[1] and a query language whose syntax and semantics cover a subset of wh-queries. To implement the intraquery guidance, we extend context-free grammar by associating context variables with each grammar symbol and attaching several types of procedural decorations to grammar rules. This extension enables the system to capture the semantic constraints and its user-guiding actions in a domain-independent grammar. As the grammar is interpreted, the database-specific information is fed from the system's lexicon and knowledge base. The current implementation of Kaleidoscope runs on a XEROX-1186 LISP machine with a SUN server configured with a relational DBMS.The approach of Kaleidoscope is based on the normative system assumption. The system presents its capability transparently to the user in a context-dependent manner during the user's query creation. This makes the system usable even with a small amount of stored knowledge.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {387},
numpages = {1},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98747,
author = {Cha, Sang K. and Wiederhold, Gio},
title = {Kaleidoscope: A Cooperative Menu-Guided Query Interface},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98747},
doi = {10.1145/93605.98747},
abstract = {Querying databases to obtain information requires the user's knowledge of query language and underlying data. However, because the knowledge in human long-term memory is imprecise, incomplete, and often incorrect, user queries are subject to various types of failure. These may include spelling mistakes, the violation of the syntax and semantics of a query language, and the misconception of the entities and relationships in a database.Kaleidoscope is a cooperative query interface whose knowledge guides users to avoid most failure during query creation. We call this type of cooperative behavior intraquery guidance. To enable this early, active engagement in the user's process of query creation, Kaleidoscope reduces the granularity of user-system interaction via a context-sensitive menu. The system generates valid query constituents as menu choices step-by-step by interpreting a language grammar, and the user creates a query following this menu guidance[2]. For instance, it takes four steps to create the following query [Q1] Who/1 authored/2 'Al'/3 journal papers/(3+) in 'Postquery COOP'/4At each of such steps, as the user selects one of menu choices, the system updates its partial query status window. If a choice is unique as in (3+), it is taken automatically. To guide the user's entry of values, the system provides a pop-up menu for each value domain.With Kaleidoscope's process of choice generation tightly controlled by the system's knowledge of query language and underlying data, users need not remember the query language and the underlying database structure but merely recognize or identify the constituents coming one after another that match their intended query. The system provides additional guidance for users to avoid creating semantically inconsistent queries. It informs the user of any derived predicates on the completion of a user-selected predicate. To illustrate this, consider a partially constructed SQL query [Q2] SELECT * FROM professor p#1 WHERE p#1 dept = 'CS' AND p#1 salary &lt; 40000Suppose that the system has an integrity constraint [IC] FROM professor p IF p dept = 'CS' AND p salary &lt; 45000 THEN p rank = 'Assistant'This rules states that a CS professor whose salary is less than 45000 is an assistant professor. With the replacement of rule variable p in IC by Q2's range variable p#1, IC's leading two predicates subsume Q2's query condition, producing p#1 rank = 'Assistant'. Because this derived predicate is not subsumed by Q2's query condition, the system suspects that the user may not know of it and presents it to the user.Derived predicates, together with user-selected ones, constrain the user's further conjunctive extension of the partial query condition. For example, the system prunes the field rank (as well as the field dept) in the conjunctive extension of Q2, because the derived condition restricts the value of this field to a constant.As shown in examples, we apply Kaleidoscope's approach to two linear-syntax languages in different levels of abstraction SQL[1] and a query language whose syntax and semantics cover a subset of wh-queries. To implement the intraquery guidance, we extend context-free grammar by associating context variables with each grammar symbol and attaching several types of procedural decorations to grammar rules. This extension enables the system to capture the semantic constraints and its user-guiding actions in a domain-independent grammar. As the grammar is interpreted, the database-specific information is fed from the system's lexicon and knowledge base. The current implementation of Kaleidoscope runs on a XEROX-1186 LISP machine with a SUN server configured with a relational DBMS.The approach of Kaleidoscope is based on the normative system assumption. The system presents its capability transparently to the user in a context-dependent manner during the user's query creation. This makes the system usable even with a small amount of stored knowledge.},
journal = {SIGMOD Rec.},
month = may,
pages = {387},
numpages = {1}
}

@inproceedings{10.1145/93597.98748,
author = {Consens, Mariano P. and Mendelzon, Alberto O.},
title = {The G+/GraphLog Visual Query System},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98748},
doi = {10.1145/93597.98748},
abstract = {The video presentation “The G+/GraphLog Visual Query System” gives an overview of the capabilities of the ongoing implementation of the G+ Visual Query System for visualizing both data and queries as graphs. The system provides an environment for expressing queries in GraphLog [Con89, CM89, CM90], as well as for browsing, displaying and editing graphs. The visual query system also supports displaying the answers in several different ways.Graphs are a very natural representation for data in many application domains, for example, transportation networks, project scheduling, parts hierarchies, family trees, concept hierarchies, and Hypertext. From a broader perspective, many databases can be naturally viewed as graphs. In particular, any relational database in which we can identify one or more sets of objects of interest and relationships between them can be represented by mapping these objects into nodes and relationships into edges. In the case of semantic and object-oriented databases, there is a natural mapping of objects to nodes and attributes to edges.GraphLog is a visual query language, based on a graph representation of both data and queries, that has evolved from the earlier language G+ [CMW87, CMW89, MW89]. GraphLog queries ask for patterns that must be present or absent in the database graph. Each such pattern, called a query graph, defines new edges that are added to the graph whenever the pattern is found. GraphLog queries are sets of query graphs, called graphical queries. If, when looking at a query graph in a graphical query, we do not find an edge label in the database, then there must exist another query graph in the graphical query defining that edge. The language also supports computing aggregate functions and summarizing along paths.The G+ Visual Query System is currently implemented in Smalltalk-80™, and runs on Sun 3, Sun 4 and Macintosh II workstations. A Graph Editor is available for editing query graphs and displaying database graphs. It supports graph “cutting and pasting”, as well as text editing of node and edge labels, node and edge repositioning and re-shaping, storage and retrieval of graphs as text files, etc. Automatic graph layout is also provided. For editing collections of graphs (such as graphical queries) a Graph Browser is available.The first answer mode supported by the G+ Visual Query System is to return as the result of a GraphLog query a graph with the new edges defined by the graphical query added to the database graph.An alternative way of visualizing answers is by high-lighting on the database graph, one at a time, the paths (or just the nodes) described by the query. This mode is particularly useful to locate interesting starting points for browsing.Rather than viewing the answers superimposed on the database graph, the user may choose to view them in a Graph Browser. The Graph Browser contains the set of subgraphs of the database graph that were found to satisfy the query.Finally, the user may select to collect all the subgraphs of the database graph that satisfy the query together into one new graph. This graph (as well as any other result graph from any of the above mentioned answer modes) in turn may be queried, providing a mechanism for iterative filtering of irrelevant information until a manageable subgraph is obtained.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {388},
numpages = {1},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98748,
author = {Consens, Mariano P. and Mendelzon, Alberto O.},
title = {The G+/GraphLog Visual Query System},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98748},
doi = {10.1145/93605.98748},
abstract = {The video presentation “The G+/GraphLog Visual Query System” gives an overview of the capabilities of the ongoing implementation of the G+ Visual Query System for visualizing both data and queries as graphs. The system provides an environment for expressing queries in GraphLog [Con89, CM89, CM90], as well as for browsing, displaying and editing graphs. The visual query system also supports displaying the answers in several different ways.Graphs are a very natural representation for data in many application domains, for example, transportation networks, project scheduling, parts hierarchies, family trees, concept hierarchies, and Hypertext. From a broader perspective, many databases can be naturally viewed as graphs. In particular, any relational database in which we can identify one or more sets of objects of interest and relationships between them can be represented by mapping these objects into nodes and relationships into edges. In the case of semantic and object-oriented databases, there is a natural mapping of objects to nodes and attributes to edges.GraphLog is a visual query language, based on a graph representation of both data and queries, that has evolved from the earlier language G+ [CMW87, CMW89, MW89]. GraphLog queries ask for patterns that must be present or absent in the database graph. Each such pattern, called a query graph, defines new edges that are added to the graph whenever the pattern is found. GraphLog queries are sets of query graphs, called graphical queries. If, when looking at a query graph in a graphical query, we do not find an edge label in the database, then there must exist another query graph in the graphical query defining that edge. The language also supports computing aggregate functions and summarizing along paths.The G+ Visual Query System is currently implemented in Smalltalk-80™, and runs on Sun 3, Sun 4 and Macintosh II workstations. A Graph Editor is available for editing query graphs and displaying database graphs. It supports graph “cutting and pasting”, as well as text editing of node and edge labels, node and edge repositioning and re-shaping, storage and retrieval of graphs as text files, etc. Automatic graph layout is also provided. For editing collections of graphs (such as graphical queries) a Graph Browser is available.The first answer mode supported by the G+ Visual Query System is to return as the result of a GraphLog query a graph with the new edges defined by the graphical query added to the database graph.An alternative way of visualizing answers is by high-lighting on the database graph, one at a time, the paths (or just the nodes) described by the query. This mode is particularly useful to locate interesting starting points for browsing.Rather than viewing the answers superimposed on the database graph, the user may choose to view them in a Graph Browser. The Graph Browser contains the set of subgraphs of the database graph that were found to satisfy the query.Finally, the user may select to collect all the subgraphs of the database graph that satisfy the query together into one new graph. This graph (as well as any other result graph from any of the above mentioned answer modes) in turn may be queried, providing a mechanism for iterative filtering of irrelevant information until a manageable subgraph is obtained.},
journal = {SIGMOD Rec.},
month = may,
pages = {388},
numpages = {1}
}

@inproceedings{10.1145/93597.98749,
author = {Agrawal, R. and Gehani, N. H. and Srinivasan, J.},
title = {OdeView: A User-Friendly Graphical Interface to Ode},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98749},
doi = {10.1145/93597.98749},
abstract = {OdeView is the graphical front end for Ode, an object-oriented database system and environment. It is intended for users who do not want to write programs in Ode's database programming language O++ to interact with Ode but instead want to use a friendlier interface to Ode. OdeView is based on the graphical direct manipulation paradigm that involves selection of items from pop-up menus and icons that can be clicked on and dragged. OdeView provides facilities for examining the database schema examining class definitions, browsing objects, following chains of references, displaying selected portions of objects or selecting a subset of the ways in which an object can be displayed (projection), and retrieving specific objects (selection).Upon entering OdeView, the user is presented with a scrollable “database” window containing the names and iconified images of the current Ode databases. The user can select a database to interact with by using the mouse to click on the appropriate icon. OdeView then opens a “class relationship” window which displays the hierarchy relationship between the object classes database. The hierarchy relationship between classes is a set of dags.The user can zoom in and zoom out to examine this dag at various levels of detail. The user can also examine a class in detail by clicking at the node labeled with the class of interest. Clicking results in the opening of a “class information” window that has three scrollable subwindows, one showing its superclasses, the second its subclasses, and the third showing the meta data associated with this class.The class information window also has a button, clicking which shows the class definition. The user may continue schema browsing by selecting another node in the schema graph, or may click on one of the superclasses or subclasses. Associated with each class in Ode a the set of persistent objects of that class, called cluster. The class definition window has an “objects” button that allows users to browse through the objects in the cluster. Clicking this button opens the “object set” window which consists of two parts the control and object panels. The control panel consists of buttons reset, next, and previous to sequence through the objects. The object panel has buttons to view the object, projection (to view parts of the object), and to specify the selection criteria.An Ode object can be displayed in one or more formats depending upon the semantics of the display function associated with the corresponding class. The object set window supplies one button each for each of the object display formats. For example, an employee object can be displayed textually or in pictorial form, the object panel for employee will provides appropriate buttons to see these displays. An object may contain embedded references to other objects. The object panel of an object set window provides buttons for viewing these referenced objects. The basic browsing paradigm encouraged by OdeView is to start from an object and then explore the related objects in the database by following the embedded chains of references. To speed up such repetitive navigations, OdeView supports synchronized browsing. Once the user has displayed a network of objects and the user applies a sequencing operation to any object in this network, the sequencing operation is automatically propagated over the network.OdeView is implemented using X-Windows and HP-Widgets on a SUN workstation running the UNIX system. The video takes the viewers on a tour of OdeView, showing how a user interacts with OdeView to examine the database schema and the objects in the database.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {389},
numpages = {1},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98749,
author = {Agrawal, R. and Gehani, N. H. and Srinivasan, J.},
title = {OdeView: A User-Friendly Graphical Interface to Ode},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98749},
doi = {10.1145/93605.98749},
abstract = {OdeView is the graphical front end for Ode, an object-oriented database system and environment. It is intended for users who do not want to write programs in Ode's database programming language O++ to interact with Ode but instead want to use a friendlier interface to Ode. OdeView is based on the graphical direct manipulation paradigm that involves selection of items from pop-up menus and icons that can be clicked on and dragged. OdeView provides facilities for examining the database schema examining class definitions, browsing objects, following chains of references, displaying selected portions of objects or selecting a subset of the ways in which an object can be displayed (projection), and retrieving specific objects (selection).Upon entering OdeView, the user is presented with a scrollable “database” window containing the names and iconified images of the current Ode databases. The user can select a database to interact with by using the mouse to click on the appropriate icon. OdeView then opens a “class relationship” window which displays the hierarchy relationship between the object classes database. The hierarchy relationship between classes is a set of dags.The user can zoom in and zoom out to examine this dag at various levels of detail. The user can also examine a class in detail by clicking at the node labeled with the class of interest. Clicking results in the opening of a “class information” window that has three scrollable subwindows, one showing its superclasses, the second its subclasses, and the third showing the meta data associated with this class.The class information window also has a button, clicking which shows the class definition. The user may continue schema browsing by selecting another node in the schema graph, or may click on one of the superclasses or subclasses. Associated with each class in Ode a the set of persistent objects of that class, called cluster. The class definition window has an “objects” button that allows users to browse through the objects in the cluster. Clicking this button opens the “object set” window which consists of two parts the control and object panels. The control panel consists of buttons reset, next, and previous to sequence through the objects. The object panel has buttons to view the object, projection (to view parts of the object), and to specify the selection criteria.An Ode object can be displayed in one or more formats depending upon the semantics of the display function associated with the corresponding class. The object set window supplies one button each for each of the object display formats. For example, an employee object can be displayed textually or in pictorial form, the object panel for employee will provides appropriate buttons to see these displays. An object may contain embedded references to other objects. The object panel of an object set window provides buttons for viewing these referenced objects. The basic browsing paradigm encouraged by OdeView is to start from an object and then explore the related objects in the database by following the embedded chains of references. To speed up such repetitive navigations, OdeView supports synchronized browsing. Once the user has displayed a network of objects and the user applies a sequencing operation to any object in this network, the sequencing operation is automatically propagated over the network.OdeView is implemented using X-Windows and HP-Widgets on a SUN workstation running the UNIX system. The video takes the viewers on a tour of OdeView, showing how a user interacts with OdeView to examine the database schema and the objects in the database.},
journal = {SIGMOD Rec.},
month = may,
pages = {389},
numpages = {1}
}

@inproceedings{10.1145/93597.98750,
author = {Blum, Bruce I. and Semmel, Ralph D.},
title = {The INA: A Simple Query Language with Only Attribute Names},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98750},
doi = {10.1145/93597.98750},
abstract = {Current query languages, such as SQL, assume that the user is familiar with the database schema including the attribute names, types, and relation associations. When a user has imperfect knowledge of this information (or when he balks at the data-processing orientation of the required statements), he normally asks an experienced analyst to perform his and hoc query. The Intelligent Navigational Assistant (INA) was developed for the U S Army as a prototype query tool that permits the users to specify requests using only domain terms familiar to them. Once a request is made, it is converted into SQL for processing1,2To facilitate query formulation, the INA supports an interface that allows the user to identify attributes without relation associations (i.e., treats the data model as a universal relation). Because an attribute may appear in many relations, one of the principal tasks of the INA is the determination of the appropriate relation bindings. To aid in the selection of terms, the INA maintains a user vocabulary and provides facilities for browsing the vocabulary and examining term definitions. Thus, the INA has two primary functions it provides an easy-to-use interface for query definition, and it converts a request into SQL.The INA prototype has been implemented as a PC-resident knowledge-based system linked to a host-based DBMS. Its knowledge base is the logical schema of the target database, and the query transformation relies on the dependencies implicit in that schema. Supporting the knowledge-processing functions are the query definition interface, various tools to manage the target data model description, and facilities for communicating with other computers. The system was developed using TEDIUM@@@@,3 and the user interface and query resolution mechanism are extensions of earlier work with Tequila4 (which accessed the semantically-richer TEDIUM@@@@ data model)Work on the INA began in 1987 and was terminated in 1988. The system was demonstrated as a prototype with an Army-supplied logical model consisting of approximately 40 relations and 200 attributes. After query definition, reformation, and user acceptance, the SQL queries were submitted to the mainframe for processing. In those tests, the INA often produced better queries than those manually coded by analysts. The INA currently is undergoing a beta test with a much larger database schema. Its algorithms are described in reference 5, and reference 3 contains details regarding its implementation and semantic data model. Current research includes the development of improved query resolution algorithms based on an enriched semantic data model},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {390},
numpages = {1},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98750,
author = {Blum, Bruce I. and Semmel, Ralph D.},
title = {The INA: A Simple Query Language with Only Attribute Names},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98750},
doi = {10.1145/93605.98750},
abstract = {Current query languages, such as SQL, assume that the user is familiar with the database schema including the attribute names, types, and relation associations. When a user has imperfect knowledge of this information (or when he balks at the data-processing orientation of the required statements), he normally asks an experienced analyst to perform his and hoc query. The Intelligent Navigational Assistant (INA) was developed for the U S Army as a prototype query tool that permits the users to specify requests using only domain terms familiar to them. Once a request is made, it is converted into SQL for processing1,2To facilitate query formulation, the INA supports an interface that allows the user to identify attributes without relation associations (i.e., treats the data model as a universal relation). Because an attribute may appear in many relations, one of the principal tasks of the INA is the determination of the appropriate relation bindings. To aid in the selection of terms, the INA maintains a user vocabulary and provides facilities for browsing the vocabulary and examining term definitions. Thus, the INA has two primary functions it provides an easy-to-use interface for query definition, and it converts a request into SQL.The INA prototype has been implemented as a PC-resident knowledge-based system linked to a host-based DBMS. Its knowledge base is the logical schema of the target database, and the query transformation relies on the dependencies implicit in that schema. Supporting the knowledge-processing functions are the query definition interface, various tools to manage the target data model description, and facilities for communicating with other computers. The system was developed using TEDIUM@@@@,3 and the user interface and query resolution mechanism are extensions of earlier work with Tequila4 (which accessed the semantically-richer TEDIUM@@@@ data model)Work on the INA began in 1987 and was terminated in 1988. The system was demonstrated as a prototype with an Army-supplied logical model consisting of approximately 40 relations and 200 attributes. After query definition, reformation, and user acceptance, the SQL queries were submitted to the mainframe for processing. In those tests, the INA often produced better queries than those manually coded by analysts. The INA currently is undergoing a beta test with a much larger database schema. Its algorithms are described in reference 5, and reference 3 contains details regarding its implementation and semantic data model. Current research includes the development of improved query resolution algorithms based on an enriched semantic data model},
journal = {SIGMOD Rec.},
month = may,
pages = {390},
numpages = {1}
}

@inproceedings{10.1145/93597.98751,
author = {Kuntz, Michel},
title = {Pasta-3: A Graphical Direct Manipulation Interface for Knowledge Base Management Systems},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98751},
doi = {10.1145/93597.98751},
abstract = {Pasta-3 is an end-user interface for D/KBMSs based on the graphical Direct Manipulation (DM) interaction paradigm, which relies on a bit-mapped, multi-window screen and a mouse to implement clickable icons as the main representation of information. This style of interaction enables end users to learn quickly and remember easily how the system works. Pasta-3 gives complete access to the D/KBMS, since its users can carry out all manipulation tasks through it schema definition, schema and data browsing, query formulation, and updating. These tasks can be freely mixed, combined, and switched Pasta-3 interfaces to the KB2 knowledge base system, implemented in Prolog and built over the EDUCE system which provides a tight coupling to a relational DBMS KB2 uses the Entity-Relationship data model, extended with inheritance and deduction rules. KB2 was developed by the KB Group at ECRC.Pasta-3 uses Direct Manipulation in the strong sense of the term DM of the actual graphical representations of the application data and not just DM of commands operating on that data. Besides the high degree of integration in the overall design, major innovations with respect to earlier work include enhanced schema browsing with active functionalities to facilitate correct user understanding of the KB structure, “synchronized” data browsing that exploits the underlying semantic data model to make browsing more powerful, and a graphical query language providing full expressive power (including certain recursive queries, nested subsequeries, quantification).Pasta-3 provides interactive design support that has significant ergonomic advantages over the usual approach to this problem. In Pasta-3 different types of schema information — the basic E-R diagram, and inheritance lattices, the properties of each E-R item — are displayed in separate windows, which makes accurate reading of such information much less difficult than in the usual case where all these layers are thrown together in a single graph, which makes misinterpretation hard to avoid.For schema and data browsing, Pasta-3 offers facilities that build more semantics into the browsing processes. One type of schema browsing tool is a subgraph computation capability which automatically finds and displays the paths that connect arbitrary E-R items. This helps end users to correctly perceive the schema structure. Data browsing includes “synchronised” browsing, a functionality which shows simultaneously data from several Entities all sharing the same Relationship and indicates which values from each Entity are associated with given values from the others.Pasta-3's DM query language replaces the textual language without loss of expressive power it offers a new, sophisticated DM editing capability for the same formal constructs. Query specification takes place in a window containing icons representing the components of the query expression which can be created, destroyed, and modified all by clicking and dragging through the mouse. Queries can be recursive and involve logical variables, quantification, and subqueries. Expressions mixing both KB2 statements and Prolog predicates can also be formulated.The video shows Pasta-3 actually being used, in real time and under normal conditions. It includes sequences demonstrating all three major functionalities schema design browsing, and querying. It gives an example of the subgraph computation capability and builds a simple query from scratch, going through all the steps needed to do so. The demonstration also includes work with other types of Pasta-3 windows (e g property sheets).The video has an English-language sound track explaining everything that is seen on the screen. The camera zooms in and out in order to show full screen overviews (giving a good idea of the general “feel” of the interface) and close-ups of work with mouse and icons (allowing the viewer to see as much detail in the video as an actual user would, seated in front of the workstation).},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {391},
numpages = {1},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98751,
author = {Kuntz, Michel},
title = {Pasta-3: A Graphical Direct Manipulation Interface for Knowledge Base Management Systems},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98751},
doi = {10.1145/93605.98751},
abstract = {Pasta-3 is an end-user interface for D/KBMSs based on the graphical Direct Manipulation (DM) interaction paradigm, which relies on a bit-mapped, multi-window screen and a mouse to implement clickable icons as the main representation of information. This style of interaction enables end users to learn quickly and remember easily how the system works. Pasta-3 gives complete access to the D/KBMS, since its users can carry out all manipulation tasks through it schema definition, schema and data browsing, query formulation, and updating. These tasks can be freely mixed, combined, and switched Pasta-3 interfaces to the KB2 knowledge base system, implemented in Prolog and built over the EDUCE system which provides a tight coupling to a relational DBMS KB2 uses the Entity-Relationship data model, extended with inheritance and deduction rules. KB2 was developed by the KB Group at ECRC.Pasta-3 uses Direct Manipulation in the strong sense of the term DM of the actual graphical representations of the application data and not just DM of commands operating on that data. Besides the high degree of integration in the overall design, major innovations with respect to earlier work include enhanced schema browsing with active functionalities to facilitate correct user understanding of the KB structure, “synchronized” data browsing that exploits the underlying semantic data model to make browsing more powerful, and a graphical query language providing full expressive power (including certain recursive queries, nested subsequeries, quantification).Pasta-3 provides interactive design support that has significant ergonomic advantages over the usual approach to this problem. In Pasta-3 different types of schema information — the basic E-R diagram, and inheritance lattices, the properties of each E-R item — are displayed in separate windows, which makes accurate reading of such information much less difficult than in the usual case where all these layers are thrown together in a single graph, which makes misinterpretation hard to avoid.For schema and data browsing, Pasta-3 offers facilities that build more semantics into the browsing processes. One type of schema browsing tool is a subgraph computation capability which automatically finds and displays the paths that connect arbitrary E-R items. This helps end users to correctly perceive the schema structure. Data browsing includes “synchronised” browsing, a functionality which shows simultaneously data from several Entities all sharing the same Relationship and indicates which values from each Entity are associated with given values from the others.Pasta-3's DM query language replaces the textual language without loss of expressive power it offers a new, sophisticated DM editing capability for the same formal constructs. Query specification takes place in a window containing icons representing the components of the query expression which can be created, destroyed, and modified all by clicking and dragging through the mouse. Queries can be recursive and involve logical variables, quantification, and subqueries. Expressions mixing both KB2 statements and Prolog predicates can also be formulated.The video shows Pasta-3 actually being used, in real time and under normal conditions. It includes sequences demonstrating all three major functionalities schema design browsing, and querying. It gives an example of the subgraph computation capability and builds a simple query from scratch, going through all the steps needed to do so. The demonstration also includes work with other types of Pasta-3 windows (e g property sheets).The video has an English-language sound track explaining everything that is seen on the screen. The camera zooms in and out in order to show full screen overviews (giving a good idea of the general “feel” of the interface) and close-ups of work with mouse and icons (allowing the viewer to see as much detail in the video as an actual user would, seated in front of the workstation).},
journal = {SIGMOD Rec.},
month = may,
pages = {391},
numpages = {1}
}

@inproceedings{10.1145/93597.98752,
author = {Kent, Bill and Lyngback, Peter and Mathur, Samir and Wilkinson, Kevin},
title = {The Iris Database System},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98752},
doi = {10.1145/93597.98752},
abstract = {Iris is an object-oriented database management system being developed at Hewlett-Packard Laboratories [1], [3]. This videotape provides an overview of the Iris data model and a summary of our experiences in converting a computer-integrated manufacturing application to Iris. An abstract of the videotape follows.Iris is intended to meet the needs of new and emerging database applications such as office and engineering information systems, knowledge-based systems, manufacturing applications, and hardware and software design. These applications require a rich set of capabilities that are not supported by the current generation (i.e., relational) DBMSs.The Iris data model is an object and function model. It provides three basic constructs objects, types and functions. As with other object systems, Iris objects have a unique identifier and can only be accessed and manipulated through functions. Objects are classified by type. Objects that belong to the same type share common functions. Types are organized into a hierarchy with inherited functions. In Iris, functions are used to model properties of objects, relationships among objects and operations on objects. Thus, the behavior of an Iris object is completely specified through its participation in functions.Iris provides good separation among its three basic notions. This simplifies the data model making it easier to learn and easier to implement since there are fewer constructs than other object models. In addition, it facilitates Iris support for the following desirable features. Schema evolution: new types and functions may be added at any time. Object evolution: Iris objects may have multiple types and may acquire and lose types dynamically. Object participation in functions may be required or optional (e g, everyone has birthdate but not everyone has a phone number). Data independence: the implementation of a function is defined separately from its interface. Thus, the implementation of a function may change without affecting applications that use it. Functional extensibility: an Iris function may be implemented as a stored table, computed as an Iris expression, or computed as a subroutine in a general-purpose programming language. Thus, any computation can be expressed as an Iris function Schema and data uniformity: the metadata is modeled and manipulated using the primitives of the data model. Also, system functions (create type, delete object, etc) are invoked in the same manner as user functions. Thus, users need learn only one interface. Set processing: Iris supports set-at-a-time processing for efficient retrieval and update of collections of objects.To evaluate the usefulness of the Iris prototype, a project was undertaken to convert a large relational application to Iris [2]. The relational system contained nearly 200 relations and 2500 attributes. When transcribed to Iris, the schema size was reduced by over a third. There are two reasons for this large reduction. First, in the relational schema, many attributes were simply foreign keys required for joins. In the Iris schema, function inheritance through the type hierarchy eliminates the need for many of these foreign keys. A second reason for the schema reduction was that compound keys were replaced by object references. This permitted several attributes in a relation to be replaced by a single identifierIt was noted that application programs were easier to read and develop using the Iris schema. The Iris OSQL (Object SQL) language was a fairly natural interface for users familiar with SQL. The use of function composition and function inheritance and a large number of joins that, in the relational system, must be expressed by comparing keys. The function-orientation of Iris encouraged code sharing in that deriving and sharing new functions was simplified.Finally, since there are few tools and methodologies for using object-oriented database management systems, the ability of the Iris schema to easily evolve was valuable in iteratively refining the Iris schema. Also, the Iris Graphical Editor was a useful tool in graphically displaying the schema and browsing function definition and instances.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {392},
numpages = {1},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98752,
author = {Kent, Bill and Lyngback, Peter and Mathur, Samir and Wilkinson, Kevin},
title = {The Iris Database System},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98752},
doi = {10.1145/93605.98752},
abstract = {Iris is an object-oriented database management system being developed at Hewlett-Packard Laboratories [1], [3]. This videotape provides an overview of the Iris data model and a summary of our experiences in converting a computer-integrated manufacturing application to Iris. An abstract of the videotape follows.Iris is intended to meet the needs of new and emerging database applications such as office and engineering information systems, knowledge-based systems, manufacturing applications, and hardware and software design. These applications require a rich set of capabilities that are not supported by the current generation (i.e., relational) DBMSs.The Iris data model is an object and function model. It provides three basic constructs objects, types and functions. As with other object systems, Iris objects have a unique identifier and can only be accessed and manipulated through functions. Objects are classified by type. Objects that belong to the same type share common functions. Types are organized into a hierarchy with inherited functions. In Iris, functions are used to model properties of objects, relationships among objects and operations on objects. Thus, the behavior of an Iris object is completely specified through its participation in functions.Iris provides good separation among its three basic notions. This simplifies the data model making it easier to learn and easier to implement since there are fewer constructs than other object models. In addition, it facilitates Iris support for the following desirable features. Schema evolution: new types and functions may be added at any time. Object evolution: Iris objects may have multiple types and may acquire and lose types dynamically. Object participation in functions may be required or optional (e g, everyone has birthdate but not everyone has a phone number). Data independence: the implementation of a function is defined separately from its interface. Thus, the implementation of a function may change without affecting applications that use it. Functional extensibility: an Iris function may be implemented as a stored table, computed as an Iris expression, or computed as a subroutine in a general-purpose programming language. Thus, any computation can be expressed as an Iris function Schema and data uniformity: the metadata is modeled and manipulated using the primitives of the data model. Also, system functions (create type, delete object, etc) are invoked in the same manner as user functions. Thus, users need learn only one interface. Set processing: Iris supports set-at-a-time processing for efficient retrieval and update of collections of objects.To evaluate the usefulness of the Iris prototype, a project was undertaken to convert a large relational application to Iris [2]. The relational system contained nearly 200 relations and 2500 attributes. When transcribed to Iris, the schema size was reduced by over a third. There are two reasons for this large reduction. First, in the relational schema, many attributes were simply foreign keys required for joins. In the Iris schema, function inheritance through the type hierarchy eliminates the need for many of these foreign keys. A second reason for the schema reduction was that compound keys were replaced by object references. This permitted several attributes in a relation to be replaced by a single identifierIt was noted that application programs were easier to read and develop using the Iris schema. The Iris OSQL (Object SQL) language was a fairly natural interface for users familiar with SQL. The use of function composition and function inheritance and a large number of joins that, in the relational system, must be expressed by comparing keys. The function-orientation of Iris encouraged code sharing in that deriving and sharing new functions was simplified.Finally, since there are few tools and methodologies for using object-oriented database management systems, the ability of the Iris schema to easily evolve was valuable in iteratively refining the Iris schema. Also, the Iris Graphical Editor was a useful tool in graphically displaying the schema and browsing function definition and instances.},
journal = {SIGMOD Rec.},
month = may,
pages = {392},
numpages = {1}
}

