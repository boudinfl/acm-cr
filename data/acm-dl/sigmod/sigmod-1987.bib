@inproceedings{10.1145/38713.38767,
author = {Neff, R. K.},
title = {Data Bases, Compound Objects, and Networked Workstations: Beyond Distributed Computing (Abstract)},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38767},
doi = {10.1145/38713.38767},
abstract = {Requirements for future data base systems are developed from the perspective of the user of a networked workstation who naturally deals with compound objects. Objects considered include full text, diagrams, maps, sound recordings, images from film and video and of art objects, spreadsheets, etc. Searching requirements and strategies over multi-objects are also considered. The context of such data base systems is the library, in its electronic or digital version. Comments are presented with respect to the digital learning environment of the future. Current related projects at Berkeley are described.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {1},
numpages = {1},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38767,
author = {Neff, R. K.},
title = {Data Bases, Compound Objects, and Networked Workstations: Beyond Distributed Computing (Abstract)},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38767},
doi = {10.1145/38714.38767},
abstract = {Requirements for future data base systems are developed from the perspective of the user of a networked workstation who naturally deals with compound objects. Objects considered include full text, diagrams, maps, sound recordings, images from film and video and of art objects, spreadsheets, etc. Searching requirements and strategies over multi-objects are also considered. The context of such data base systems is the library, in its electronic or digital version. Comments are presented with respect to the digital learning environment of the future. Current related projects at Berkeley are described.},
journal = {SIGMOD Rec.},
month = dec,
pages = {1},
numpages = {1}
}

@inproceedings{10.1145/38713.38716,
author = {Ingenthron, Kurt},
title = {Thoughts on Database Research: A User Perspective},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38716},
doi = {10.1145/38713.38716},
abstract = {The future of computer aided design is in object oriented programming. If the database community hopes to participate in this future, it must reexamine some basic assumptions about the architecture of database systems. Database system functionality can be added to object systems but if the performance cost is too high, it will never survive. Below are some suggestions for what can be done at a reasonable performance cost.The object oriented paradigm provides a more practical approach to the partitioning of the global database than horizontal and vertical partitioning of relational tables. Each partition should itself be an independent database containing related data such as the geometry of a part or the spacial relationship of parts in an assembly. A meta-database would be used to control access to collections of these partitions. A collection of partitions comprise the database for a user's design session.The overhead of traditional database transaction management is not acceptable for high performance CAD systems. With the partitioning scheme described above, transaction management can be performed at a partition/session granularity. Once the user has composed the collection of partitions, he has a single user database. There is no need for concurrency control or transaction logging except at the meta-database level. This type of transaction management can in fact be more functional than traditional transaction management, allowing for versioning, long transactions, integrity checking and archival.Object oriented databases need a message model, not a data model. Any object which responds to the same messages as an object of “Duck” class (walk and quack) is, for all intents and purposes, a duck. An attempt to design a data model based on instance variables of an object or based on collections of objects of like class violates the data abstraction facilities of object oriented languages and diminishes their power. An attempt to implement a relational database system with an object oriented language yields a relational database system where you get abstract data types for free. It does not yield an object oriented database system.For object oriented queries, the message is the media. A query can be transformed into an execution plan consisting of messages sent to database objects. Optimization decisions can be made by sending messages to referenced objects. Collection classes can be implemented for new access methods with cost and selectivity methods to provide optimization information. In this way, the query language can grow with the application.Data representation is an important aspect of object oriented systems. Most object systems are typeless in that all instance variables of an object are object references. For performance sake, object systems should provide enough of a type mechanism to allow simple data items (integers, floats, characters … ) to be represented in the form intrinsic to the machine. Methods can then be compiled for access to typed data.In conclusion, object systems provide enormous potential for the development of CAD systems. Performance influences the approach taken to an application. WYSIWYG publishing applications were not attempted until performance was adequate. Functionality is what sells CAD systems. Database system functionality can be added to object systems at a reasonable cost.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {2},
numpages = {1},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38716,
author = {Ingenthron, Kurt},
title = {Thoughts on Database Research: A User Perspective},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38716},
doi = {10.1145/38714.38716},
abstract = {The future of computer aided design is in object oriented programming. If the database community hopes to participate in this future, it must reexamine some basic assumptions about the architecture of database systems. Database system functionality can be added to object systems but if the performance cost is too high, it will never survive. Below are some suggestions for what can be done at a reasonable performance cost.The object oriented paradigm provides a more practical approach to the partitioning of the global database than horizontal and vertical partitioning of relational tables. Each partition should itself be an independent database containing related data such as the geometry of a part or the spacial relationship of parts in an assembly. A meta-database would be used to control access to collections of these partitions. A collection of partitions comprise the database for a user's design session.The overhead of traditional database transaction management is not acceptable for high performance CAD systems. With the partitioning scheme described above, transaction management can be performed at a partition/session granularity. Once the user has composed the collection of partitions, he has a single user database. There is no need for concurrency control or transaction logging except at the meta-database level. This type of transaction management can in fact be more functional than traditional transaction management, allowing for versioning, long transactions, integrity checking and archival.Object oriented databases need a message model, not a data model. Any object which responds to the same messages as an object of “Duck” class (walk and quack) is, for all intents and purposes, a duck. An attempt to design a data model based on instance variables of an object or based on collections of objects of like class violates the data abstraction facilities of object oriented languages and diminishes their power. An attempt to implement a relational database system with an object oriented language yields a relational database system where you get abstract data types for free. It does not yield an object oriented database system.For object oriented queries, the message is the media. A query can be transformed into an execution plan consisting of messages sent to database objects. Optimization decisions can be made by sending messages to referenced objects. Collection classes can be implemented for new access methods with cost and selectivity methods to provide optimization information. In this way, the query language can grow with the application.Data representation is an important aspect of object oriented systems. Most object systems are typeless in that all instance variables of an object are object references. For performance sake, object systems should provide enough of a type mechanism to allow simple data items (integers, floats, characters … ) to be represented in the form intrinsic to the machine. Methods can then be compiled for access to typed data.In conclusion, object systems provide enormous potential for the development of CAD systems. Performance influences the approach taken to an application. WYSIWYG publishing applications were not attempted until performance was adequate. Functionality is what sells CAD systems. Database system functionality can be added to object systems at a reasonable cost.},
journal = {SIGMOD Rec.},
month = dec,
pages = {2},
numpages = {1}
}

@inproceedings{10.1145/38713.38722,
author = {Ioannidis, Yannis E. and Wong, Eugene},
title = {Query Optimization by Simulated Annealing},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38722},
doi = {10.1145/38713.38722},
abstract = {Query optimizers of future database management systems are likely to face large access plan spaces in their task. Exhaustively searching such access plan spaces is unacceptable. We propose a query optimization algorithm based on simulated annealing, which is a probabilistic hill climbing algorithm. We show the specific formulation of the algorithm for the case of optimizing complex non-recursive queries that arise in the study of linear recursion. The query answer is explicitly represented and manipulated within the closed semiring of linear relational operators. The optimization algorithm is applied to a state space that is constructed from the equivalent algebraic forms of the query answer. A prototype of the simulated annealing algorithm has been built and few experiments have been performed for a limited class of relational operators. Our initial experience is that, in general, the algorithm converges to processing strategies that are very close to the optimal. Moreover, the traditional processing strategies (e.g., the semi-naive evaluation) have been found to be, in general, suboptimal.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {9–22},
numpages = {14},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38722,
author = {Ioannidis, Yannis E. and Wong, Eugene},
title = {Query Optimization by Simulated Annealing},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38722},
doi = {10.1145/38714.38722},
abstract = {Query optimizers of future database management systems are likely to face large access plan spaces in their task. Exhaustively searching such access plan spaces is unacceptable. We propose a query optimization algorithm based on simulated annealing, which is a probabilistic hill climbing algorithm. We show the specific formulation of the algorithm for the case of optimizing complex non-recursive queries that arise in the study of linear recursion. The query answer is explicitly represented and manipulated within the closed semiring of linear relational operators. The optimization algorithm is applied to a state space that is constructed from the equivalent algebraic forms of the query answer. A prototype of the simulated annealing algorithm has been built and few experiments have been performed for a limited class of relational operators. Our initial experience is that, in general, the algorithm converges to processing strategies that are very close to the optimal. Moreover, the traditional processing strategies (e.g., the semi-naive evaluation) have been found to be, in general, suboptimal.},
journal = {SIGMOD Rec.},
month = dec,
pages = {9–22},
numpages = {14}
}

@inproceedings{10.1145/38713.38723,
author = {Ganski, Richard A. and Wong, Harry K. T.},
title = {Optimization of Nested SQL Queries Revisited},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38723},
doi = {10.1145/38713.38723},
abstract = {Current methods of evaluating nested queries in the SQL language can be inefficient in a variety of query and data base contexts. Previous research in the area of nested query optimization which sought methods of reducing evaluation costs is summarized, including a classification scheme for nested queries, algorithms designed to transform each type of query to a logically equivalent form which may then be evaluated more efficiently, and a description of a major bug in one of these algorithms. Further examination reveals another bug in the same algorithm. Solutions to these bugs are proposed and incorporated into a new transformation algorithm, and extensions are proposed which will allow the transformation algorithms to handle a larger class of predicates. A recursive algorithm for processing a general nested query is presented and the action of this algorithm is demonstrated. This algorithm can be used to transform any nested query.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {23–33},
numpages = {11},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38723,
author = {Ganski, Richard A. and Wong, Harry K. T.},
title = {Optimization of Nested SQL Queries Revisited},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38723},
doi = {10.1145/38714.38723},
abstract = {Current methods of evaluating nested queries in the SQL language can be inefficient in a variety of query and data base contexts. Previous research in the area of nested query optimization which sought methods of reducing evaluation costs is summarized, including a classification scheme for nested queries, algorithms designed to transform each type of query to a logically equivalent form which may then be evaluated more efficiently, and a description of a major bug in one of these algorithms. Further examination reveals another bug in the same algorithm. Solutions to these bugs are proposed and incorporated into a new transformation algorithm, and extensions are proposed which will allow the transformation algorithms to handle a larger class of predicates. A recursive algorithm for processing a general nested query is presented and the action of this algorithm is demonstrated. This algorithm can be used to transform any nested query.},
journal = {SIGMOD Rec.},
month = dec,
pages = {23–33},
numpages = {11}
}

@inproceedings{10.1145/38713.38724,
author = {Abiteboul, Serge and Kanellakis, Paris and Grahne, Gosta},
title = {On the Representation and Querying of Sets of Possible Worlds},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38724},
doi = {10.1145/38713.38724},
abstract = {We represent a set of possible worlds using an incomplete information database. The representation techniques that we study form a hierarchy, which generalizes relations of constants. This hierarchy ranges from the very simple Codd-table, (i e , a relation of constants and distinct variables called nulls, which stand for values present but unknown), to much more complex mechanisms involving views on conditioned-tables, (i e , queries on Codd-tables together with conditions). The views we consider are the queries that have polynomial data-complexity on complete information databases. Our conditions are conjunctions of equalities and inequalities.(1) We provide matching upper and lower bounds on the data-complexity of testing containement, membership, and uniqueness for sets of possible worlds and we fully classify these problems with respect to our representation hierarchy. The most surprising result in this classification is that it is complete in Π2p, whether a set of possible worlds represented by a Codd-table is a subset of a set of possible worlds represented by a Codd-table with one conjuction of inequalities.(2) We investigate the data-complexity of querying incomplete information databases. We examine both asking for certain facts and for possible facts. Our approach is algebraic but our bounds also apply to logical databases. We show that asking for a certain fact is coNP-complete, even for a fixed first order query on a Codd-table. We thus strengthen a lower bound of [16], who showed that this holds for a Codd-table with a conjunction of inequalities. For each fixed positive existential query we present a polynomial algorithm solving the bounded possible fact problem of this query on conditioned-tables. We show that our approach is, in a sense, the best possible, by deriving two NP-completeness lower bounds for the bounded possible fact problem when the fixed query contains either negation or recursion.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {34–48},
numpages = {15},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38724,
author = {Abiteboul, Serge and Kanellakis, Paris and Grahne, Gosta},
title = {On the Representation and Querying of Sets of Possible Worlds},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38724},
doi = {10.1145/38714.38724},
abstract = {We represent a set of possible worlds using an incomplete information database. The representation techniques that we study form a hierarchy, which generalizes relations of constants. This hierarchy ranges from the very simple Codd-table, (i e , a relation of constants and distinct variables called nulls, which stand for values present but unknown), to much more complex mechanisms involving views on conditioned-tables, (i e , queries on Codd-tables together with conditions). The views we consider are the queries that have polynomial data-complexity on complete information databases. Our conditions are conjunctions of equalities and inequalities.(1) We provide matching upper and lower bounds on the data-complexity of testing containement, membership, and uniqueness for sets of possible worlds and we fully classify these problems with respect to our representation hierarchy. The most surprising result in this classification is that it is complete in Π2p, whether a set of possible worlds represented by a Codd-table is a subset of a set of possible worlds represented by a Codd-table with one conjuction of inequalities.(2) We investigate the data-complexity of querying incomplete information databases. We examine both asking for certain facts and for possible facts. Our approach is algebraic but our bounds also apply to logical databases. We show that asking for a certain fact is coNP-complete, even for a fixed first order query on a Codd-table. We thus strengthen a lower bound of [16], who showed that this holds for a Codd-table with a conjunction of inequalities. For each fixed positive existential query we present a polynomial algorithm solving the bounded possible fact problem of this query on conditioned-tables. We show that our approach is, in a sense, the best possible, by deriving two NP-completeness lower bounds for the bounded possible fact problem when the fixed query contains either negation or recursion.},
journal = {SIGMOD Rec.},
month = dec,
pages = {34–48},
numpages = {15}
}

@inproceedings{10.1145/38713.38725,
author = {Sacca, Domenico and Zaniolo, Carlo},
title = {Magic Counting Methods},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38725},
doi = {10.1145/38713.38725},
abstract = {The problem considered is that of implementing recursive queries, expressed in a logic-based language, by efficient fixpoint computations. In particular, the situation is studied where the initial bindings in the recursive predicate can be used to restrict the search space and ensure safety of execution. Two key techniques previously proposed to solve this problem are (i) the highly efficient counting method, and (ii) the magic set method which is safe in a wider range of situations than (i). In this paper, we present a family of methods, called the magic counting methods, that combines the advantages of (i) and (ii). This is made possible by the similarity of the strategies used by the counting method and the magic set method for propagating the bindings. This paper introduces these new methods, examines their computational complexity, and illustrates the trade-offs between the family members and their superiority with respect to the old methods.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {49–59},
numpages = {11},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38725,
author = {Sacca, Domenico and Zaniolo, Carlo},
title = {Magic Counting Methods},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38725},
doi = {10.1145/38714.38725},
abstract = {The problem considered is that of implementing recursive queries, expressed in a logic-based language, by efficient fixpoint computations. In particular, the situation is studied where the initial bindings in the recursive predicate can be used to restrict the search space and ensure safety of execution. Two key techniques previously proposed to solve this problem are (i) the highly efficient counting method, and (ii) the magic set method which is safe in a wider range of situations than (i). In this paper, we present a family of methods, called the magic counting methods, that combines the advantages of (i) and (ii). This is made possible by the similarity of the strategies used by the counting method and the magic set method for propagating the bindings. This paper introduces these new methods, examines their computational complexity, and illustrates the trade-offs between the family members and their superiority with respect to the old methods.},
journal = {SIGMOD Rec.},
month = dec,
pages = {49–59},
numpages = {11}
}

@inproceedings{10.1145/38713.38726,
author = {Aly, Hussien and Ozsoyoglu, Z. Meral},
title = {Non-Deterministic Modelling of Logical Queries in Deductive Databases},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38726},
doi = {10.1145/38713.38726},
abstract = {We propose a technique based on Petri Nets formalism to model logic queries in deductive databases. The model is called PNLP (Petri Net model for Logic Programs), and it has a simple formal description and a graphical representation. The PNLP model explicitly represents the relationships between rules and predicates. It is general and flexible enough to demonstrate the flow of control in different algorithms used to evaluate recursive logic queries. In fact the model unifies the level of description of these algorithms, and facilitates identifying similarities and differences between them. The inherent non-determinism in the PNLP model may also be useful in recognizing the parallelism within Horn-clause logic programs. In this paper, the PNLP model is described, and its functionality is demonstrated by modeling several existing algorithms for recursive query evaluation.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {60–72},
numpages = {13},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38726,
author = {Aly, Hussien and Ozsoyoglu, Z. Meral},
title = {Non-Deterministic Modelling of Logical Queries in Deductive Databases},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38726},
doi = {10.1145/38714.38726},
abstract = {We propose a technique based on Petri Nets formalism to model logic queries in deductive databases. The model is called PNLP (Petri Net model for Logic Programs), and it has a simple formal description and a graphical representation. The PNLP model explicitly represents the relationships between rules and predicates. It is general and flexible enough to demonstrate the flow of control in different algorithms used to evaluate recursive logic queries. In fact the model unifies the level of description of these algorithms, and facilitates identifying similarities and differences between them. The inherent non-determinism in the PNLP model may also be useful in recognizing the parallelism within Horn-clause logic programs. In this paper, the PNLP model is described, and its functionality is demonstrated by modeling several existing algorithms for recursive query evaluation.},
journal = {SIGMOD Rec.},
month = dec,
pages = {60–72},
numpages = {13}
}

@inproceedings{10.1145/38713.38727,
author = {Han, Jiawei and Henschen, Lawrence J.},
title = {Handling Redundancy in the Processing of Recursive Database Queries},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38727},
doi = {10.1145/38713.38727},
abstract = {Redundancy may exist in the processing of recursive database queries at four different levels precompilation level, iteration level, tuple processing level and file accessing level. Techniques for reducing redundant work at each level are studied. In the precompilation level, the optimization techniques include removing redundant parts in a rule cluster, simplifying recursive clusters and sharing common subexpressions among rules. At the iteration level, the techniques discussed are the use of frontier relations and the counting method. At the tuple processing level, we use merging and filtering methods to exclude processed drivers from database reaccessing. Finally, at the file accessing level, I/O cost can be further reduced by level relaxation. We conclude that even for complex recursion, redundant database processing can be considerably reduced or eliminated by developing appropriate algorithms.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {73–81},
numpages = {9},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38727,
author = {Han, Jiawei and Henschen, Lawrence J.},
title = {Handling Redundancy in the Processing of Recursive Database Queries},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38727},
doi = {10.1145/38714.38727},
abstract = {Redundancy may exist in the processing of recursive database queries at four different levels precompilation level, iteration level, tuple processing level and file accessing level. Techniques for reducing redundant work at each level are studied. In the precompilation level, the optimization techniques include removing redundant parts in a rule cluster, simplifying recursive clusters and sharing common subexpressions among rules. At the iteration level, the techniques discussed are the use of frontier relations and the counting method. At the tuple processing level, we use merging and filtering methods to exclude processed drivers from database reaccessing. Finally, at the file accessing level, I/O cost can be further reduced by level relaxation. We conclude that even for complex recursion, redundant database processing can be considerably reduced or eliminated by developing appropriate algorithms.},
journal = {SIGMOD Rec.},
month = dec,
pages = {73–81},
numpages = {9}
}

@inproceedings{10.1145/38713.38728,
author = {Daniels, Dean S. and Spector, Alfred Z. and Thompson, Dean S.},
title = {Distributed Logging for Transaction Processing},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38728},
doi = {10.1145/38713.38728},
abstract = {Increased interest in using workstations and small processors for distributed transaction processing raises the question of how to implement the logs needed for transaction recovery. Although logs can be implemented with data written to duplexed disks on each processing node, this paper argues there are advantages if log data is written to multiple log server nodes. A simple analysis of expected logging loads leads to the conclusion that a high performance, microprocessor based processing node can support a log server if it uses efficient communication protocols and low latency, non volatile storage to buffer log data. The buffer is needed to reduce the processing time per log record and to increase throughput to the logging disk. An interface to the log servers using simple, robust, and efficient protocols is presented. Also described are the disk data structures that the log servers use. This paper concludes with a brief discussion of remaining design issues, the status of a prototype implementation, and plans for its completion.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {82–96},
numpages = {15},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38728,
author = {Daniels, Dean S. and Spector, Alfred Z. and Thompson, Dean S.},
title = {Distributed Logging for Transaction Processing},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38728},
doi = {10.1145/38714.38728},
abstract = {Increased interest in using workstations and small processors for distributed transaction processing raises the question of how to implement the logs needed for transaction recovery. Although logs can be implemented with data written to duplexed disks on each processing node, this paper argues there are advantages if log data is written to multiple log server nodes. A simple analysis of expected logging loads leads to the conclusion that a high performance, microprocessor based processing node can support a log server if it uses efficient communication protocols and low latency, non volatile storage to buffer log data. The buffer is needed to reduce the processing time per log record and to increase throughput to the logging disk. An interface to the log servers using simple, robust, and efficient protocols is presented. Also described are the disk data structures that the log servers use. This paper concludes with a brief discussion of remaining design issues, the status of a prototype implementation, and plans for its completion.},
journal = {SIGMOD Rec.},
month = dec,
pages = {82–96},
numpages = {15}
}

@inproceedings{10.1145/38713.38729,
author = {Herman, Gary and Lee, K. C. and Weinrib, Abel},
title = {The Datacycle Architecture for Very High Throughput Database Systems},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38729},
doi = {10.1145/38713.38729},
abstract = {The evolutionary trend toward a database-driven public communications network has motivated research into database architectures capable of executing thousands of transactions per second. In this paper we introduce the Datacycle architecture, an attempt to exploit the enormous transmission bandwidth of optical systems to permit the implementation of high throughput multiprocessor database systems. The architecture has the potential for unlimited query throughput, simplified data management, rapid execution of complex queries, and efficient concurrency control. We describe the logical operation of the architecture and discuss implementation issues in the context of a prototype system currently under construction.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {97–103},
numpages = {7},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38729,
author = {Herman, Gary and Lee, K. C. and Weinrib, Abel},
title = {The Datacycle Architecture for Very High Throughput Database Systems},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38729},
doi = {10.1145/38714.38729},
abstract = {The evolutionary trend toward a database-driven public communications network has motivated research into database architectures capable of executing thousands of transactions per second. In this paper we introduce the Datacycle architecture, an attempt to exploit the enormous transmission bandwidth of optical systems to permit the implementation of high throughput multiprocessor database systems. The architecture has the potential for unlimited query throughput, simplified data management, rapid execution of complex queries, and efficient concurrency control. We describe the logical operation of the architecture and discuss implementation issues in the context of a prototype system currently under construction.},
journal = {SIGMOD Rec.},
month = dec,
pages = {97–103},
numpages = {7}
}

@inproceedings{10.1145/38713.38730,
author = {Lehman, Tobin J. and Carey, Michael J.},
title = {A Recovery Algorithm for a High-Performance Memory-Resident Database System},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38730},
doi = {10.1145/38713.38730},
abstract = {With memory prices dropping and memory sizes increasing accordingly, a number of researchers are addressing the problem of designing high-performance database systems for managing memory-resident data. In this paper we address the recovery problem in the context of such a system. We argue that existing database recovery schemes fall short of meeting the requirements of such a system, and we present a new recovery mechanism which is designed to overcome their shortcomings. The proposed mechanism takes advantage of a few megabytes of reliable memory in order to organize recovery information on a per “object” basis. As a result, it is able to amortize the cost of checkpoints over a controllable number of updates, and it is also able to separate post-crash recovery into two phases—high-speed recovery of data which is needed immediately by transactions, and background recovery of the remaining portions of the database. A simple performance analysis is undertaken, and the results suggest our mechanism should perform well in a high-performance, memory-resident database environment.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {104–117},
numpages = {14},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38730,
author = {Lehman, Tobin J. and Carey, Michael J.},
title = {A Recovery Algorithm for a High-Performance Memory-Resident Database System},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38730},
doi = {10.1145/38714.38730},
abstract = {With memory prices dropping and memory sizes increasing accordingly, a number of researchers are addressing the problem of designing high-performance database systems for managing memory-resident data. In this paper we address the recovery problem in the context of such a system. We argue that existing database recovery schemes fall short of meeting the requirements of such a system, and we present a new recovery mechanism which is designed to overcome their shortcomings. The proposed mechanism takes advantage of a few megabytes of reliable memory in order to organize recovery information on a per “object” basis. As a result, it is able to amortize the cost of checkpoints over a controllable number of updates, and it is also able to separate post-crash recovery into two phases—high-speed recovery of data which is needed immediately by transactions, and background recovery of the remaining portions of the database. A simple performance analysis is undertaken, and the results suggest our mechanism should perform well in a high-performance, memory-resident database environment.},
journal = {SIGMOD Rec.},
month = dec,
pages = {104–117},
numpages = {14}
}

@inproceedings{10.1145/38713.38731,
author = {Nixon, Brian and Chung, Lawrence and Mylopoulos, John and Lauzon, David and Borgida, Alex and Stanley, M.},
title = {Implementation of a Compiler for a Semantic Data Model: Experiences with Taxis},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38731},
doi = {10.1145/38713.38731},
abstract = {The features of a compiler for the Taxis design language are described and discussed. Taxis offers an entity-based framework for designing interactive information systems and supports generalisation, classification and aggregation as abstraction mechanisms. Its features include multiple inheritance of attributes, isA hierarchies of transactions, metaclasses, typed attributes, a procedural exception-handling mechanism and an iteration construct based on the abstraction mechanisms supported Developing a compiler for the language involved dealing with the problems of efficiently representing and accessing a large collection of entities, performing (static) type checking and representing isA hierarchies of transactions.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {118–131},
numpages = {14},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38731,
author = {Nixon, Brian and Chung, Lawrence and Mylopoulos, John and Lauzon, David and Borgida, Alex and Stanley, M.},
title = {Implementation of a Compiler for a Semantic Data Model: Experiences with Taxis},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38731},
doi = {10.1145/38714.38731},
abstract = {The features of a compiler for the Taxis design language are described and discussed. Taxis offers an entity-based framework for designing interactive information systems and supports generalisation, classification and aggregation as abstraction mechanisms. Its features include multiple inheritance of attributes, isA hierarchies of transactions, metaclasses, typed attributes, a procedural exception-handling mechanism and an iteration construct based on the abstraction mechanisms supported Developing a compiler for the language involved dealing with the problems of efficiently representing and accessing a large collection of entities, performing (static) type checking and representing isA hierarchies of transactions.},
journal = {SIGMOD Rec.},
month = dec,
pages = {118–131},
numpages = {14}
}

@inproceedings{10.1145/38713.38732,
author = {Lyngbaek, Peter and Vianu, Victor},
title = {Mapping a Semantic Database Model to the Relational Model},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38732},
doi = {10.1145/38713.38732},
abstract = {The connection between semantic database models and the relational model is formally investigated using the Iris Data Model, which has been implemented using relational database techniques. The results focus on properties of relational schemas that are translations of Iris schemas. Two new types of constraints, cross-product constraints and multiplicity constraints are introduced to characterize the relational translations of Iris schemas. The connection established between Iris and relational schemas also yields new, unexpected information about Iris schemas. In particular, a notion of equivalence of Iris schemas is defined using their relational translations, and a result is obtained on simplifying the type structure of Iris schemas.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {132–142},
numpages = {11},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38732,
author = {Lyngbaek, Peter and Vianu, Victor},
title = {Mapping a Semantic Database Model to the Relational Model},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38732},
doi = {10.1145/38714.38732},
abstract = {The connection between semantic database models and the relational model is formally investigated using the Iris Data Model, which has been implemented using relational database techniques. The results focus on properties of relational schemas that are translations of Iris schemas. Two new types of constraints, cross-product constraints and multiplicity constraints are introduced to characterize the relational translations of Iris schemas. The connection established between Iris and relational schemas also yields new, unexpected information about Iris schemas. In particular, a notion of equivalence of Iris schemas is defined using their relational translations, and a result is obtained on simplifying the type structure of Iris schemas.},
journal = {SIGMOD Rec.},
month = dec,
pages = {132–142},
numpages = {11}
}

@inproceedings{10.1145/38713.38733,
author = {Roth, Mark A. and Korth, Henry F.},
title = {The Design of ¬ 1NF Relational Databases into Nested Normal Form},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38733},
doi = {10.1145/38713.38733},
abstract = {We develop new algorithms for the design of non first normal form relational databases that are in nested normal form. Previously, a set of given multivalued dependencies and those multivalued dependencies implied by given functional dependencies were used to obtain a nested normal form decomposition of a scheme. This method ignored the semantic distinction between functional and multivalued dependencies and utilized only full multivalued dependencies in the design process. We propose new algorithms which take advantage of this distinction, and use embedded multivalued dependencies to enhance the decomposition. This results in further elimination of redundancy due to functional dependencies in nested normal form designs.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {143–159},
numpages = {17},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38733,
author = {Roth, Mark A. and Korth, Henry F.},
title = {The Design of ¬ 1NF Relational Databases into Nested Normal Form},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38733},
doi = {10.1145/38714.38733},
abstract = {We develop new algorithms for the design of non first normal form relational databases that are in nested normal form. Previously, a set of given multivalued dependencies and those multivalued dependencies implied by given functional dependencies were used to obtain a nested normal form decomposition of a scheme. This method ignored the semantic distinction between functional and multivalued dependencies and utilized only full multivalued dependencies in the design process. We propose new algorithms which take advantage of this distinction, and use embedded multivalued dependencies to enhance the decomposition. This results in further elimination of redundancy due to functional dependencies in nested normal form designs.},
journal = {SIGMOD Rec.},
month = dec,
pages = {143–159},
numpages = {17}
}

@inproceedings{10.1145/38713.38734,
author = {Graefe, Goetz and DeWitt, David J.},
title = {The EXODUS Optimizer Generator},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38734},
doi = {10.1145/38713.38734},
abstract = {This paper presents the design and an initial performance evaluation of the query optimizer generator designed for the EXODUS extensible database system. Algebraic transformation rules are translated into an executable query optimizer, which transforms query trees and selects methods for executing operations according to cost functions associated with the methods. The search strategy avoids exhaustive search and it modifies itself to take advantage of past experience. Computational results show that an optimizer generated for a relational system produces access plans almost as good as those produced by exhaustive search, with the search time cut to a small fraction.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {160–172},
numpages = {13},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38734,
author = {Graefe, Goetz and DeWitt, David J.},
title = {The EXODUS Optimizer Generator},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38734},
doi = {10.1145/38714.38734},
abstract = {This paper presents the design and an initial performance evaluation of the query optimizer generator designed for the EXODUS extensible database system. Algebraic transformation rules are translated into an executable query optimizer, which transforms query trees and selects methods for executing operations according to cost functions associated with the methods. The search strategy avoids exhaustive search and it modifies itself to take advantage of past experience. Computational results show that an optimizer generated for a relational system produces access plans almost as good as those produced by exhaustive search, with the search time cut to a small fraction.},
journal = {SIGMOD Rec.},
month = dec,
pages = {160–172},
numpages = {13}
}

@inproceedings{10.1145/38713.38735,
author = {Freytag, Johann Christoph},
title = {A Rule-Based View of Query Optimization},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38735},
doi = {10.1145/38713.38735},
abstract = {The query optimizer is an important system component of a relational database management system (DBMS). It is the responsibility of this component to translate the user-submitted query - usually written in a non-procedural language - into an efficient query evaluation plan (QEP) which is then executed against the database. The research literature describes a wide variety of optimization strategies for different query languages and implementation environments. However, very little is known about how to design and structure the query optimization component to implement these strategies.This paper proposes a first step towards the design of a modular query optimizer. We describe its operations by transformation rules which generate different QEPs from initial query specifications. As we distinguish different aspects of the query optimization process, our hope is that the approach taken in this paper will contribute to the more general goal of a modular query optimizer as part of an extensible database management system.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {173–180},
numpages = {8},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38735,
author = {Freytag, Johann Christoph},
title = {A Rule-Based View of Query Optimization},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38735},
doi = {10.1145/38714.38735},
abstract = {The query optimizer is an important system component of a relational database management system (DBMS). It is the responsibility of this component to translate the user-submitted query - usually written in a non-procedural language - into an efficient query evaluation plan (QEP) which is then executed against the database. The research literature describes a wide variety of optimization strategies for different query languages and implementation environments. However, very little is known about how to design and structure the query optimization component to implement these strategies.This paper proposes a first step towards the design of a modular query optimizer. We describe its operations by transformation rules which generate different QEPs from initial query specifications. As we distinguish different aspects of the query optimization process, our hope is that the approach taken in this paper will contribute to the more general goal of a modular query optimizer as part of an extensible database management system.},
journal = {SIGMOD Rec.},
month = dec,
pages = {173–180},
numpages = {8}
}

@inproceedings{10.1145/38713.38736,
author = {Shenoy, Sreekumar T. and Ozsoyoglu, Z. Meral},
title = {A System for Semantic Query Optimization},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38736},
doi = {10.1145/38713.38736},
abstract = {This paper describes a scheme to utilize semantic integrity constraints in optimizing a user specified query. The scheme uses a graph theoretic approach to identify redundant join clauses and redundant restriction clauses specified in a user query. An algorithm is suggested to eliminate such redundant joins and avoid unnecessary restrictions. In addition to these eliminations, the algorithm aims to introduce as many restrictions on indexed attributes as possible, thus yielding an equivalent, but potentially more profitable, form of the original query.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {181–195},
numpages = {15},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38736,
author = {Shenoy, Sreekumar T. and Ozsoyoglu, Z. Meral},
title = {A System for Semantic Query Optimization},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38736},
doi = {10.1145/38714.38736},
abstract = {This paper describes a scheme to utilize semantic integrity constraints in optimizing a user specified query. The scheme uses a graph theoretic approach to identify redundant join clauses and redundant restriction clauses specified in a user query. An algorithm is suggested to eliminate such redundant joins and avoid unnecessary restrictions. In addition to these eliminations, the algorithm aims to introduce as many restrictions on indexed attributes as possible, thus yielding an equivalent, but potentially more profitable, form of the original query.},
journal = {SIGMOD Rec.},
month = dec,
pages = {181–195},
numpages = {15}
}

@inproceedings{10.1145/38713.38737,
author = {Paul, H. B. and Schek, H. J. and Scholl, M. H.},
title = {Architecture and Implementation of the Darmstadt Database Kernel System},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38737},
doi = {10.1145/38713.38737},
abstract = {The multi-layered architecture of the DArmStadt Data Base System (DASDBS) for advanced applications is introduced DASDBS is conceived as a family of application-specific database systems on top of a common database kernel system. The main design problem considered here is, What features are common enough to be integrated into the kernel and what features are rather application-specific? Kernel features must be simple enough to be efficiently implemented and to serve a broad class of clients, yet powerful enough to form a convenient basis for application-oriented layers. Our kernel provides mechanisms to efficiently store hierarchically structured complex objects, and offers operations which are set-oriented and can be processed in a single scan through the objects. To achieve high concurrency in a layered system, a multi-level transaction methodology is applied. First experiences with our current implementation and some lessons we have learned from it are reported.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {196–207},
numpages = {12},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38737,
author = {Paul, H. B. and Schek, H. J. and Scholl, M. H.},
title = {Architecture and Implementation of the Darmstadt Database Kernel System},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38737},
doi = {10.1145/38714.38737},
abstract = {The multi-layered architecture of the DArmStadt Data Base System (DASDBS) for advanced applications is introduced DASDBS is conceived as a family of application-specific database systems on top of a common database kernel system. The main design problem considered here is, What features are common enough to be integrated into the kernel and what features are rather application-specific? Kernel features must be simple enough to be efficiently implemented and to serve a broad class of clients, yet powerful enough to form a convenient basis for application-oriented layers. Our kernel provides mechanisms to efficiently store hierarchically structured complex objects, and offers operations which are set-oriented and can be processed in a single scan through the objects. To achieve high concurrency in a layered system, a multi-level transaction methodology is applied. First experiences with our current implementation and some lessons we have learned from it are reported.},
journal = {SIGMOD Rec.},
month = dec,
pages = {196–207},
numpages = {12}
}

@inproceedings{10.1145/38713.38738,
author = {Richardson, Joel E. and Carey, Michael J.},
title = {Programming Constructs for Database System Implementation in EXODUS},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38738},
doi = {10.1145/38713.38738},
abstract = {The goal of the EXODUS extensible DBMS project is to enable the rapid development of a wide spectrum of high-performance, application-specific database systems EXODUS provides certain kernel facilities for use by all applications and a set of tools to aid the database implementor (DBI) in generating new database system software. Some of the DBI's work is supported by EXODUS tools which generate database components from a specification. However, components such as new abstract data types, access methods, and database operations must be explicitly coded by the DBI. This paper analyzes the major programming problems faced by the DBI, describing the collection of programming language constructs that EXODUS provides for simplifying the DBI's task. These constructs have been embedded in the E programming language, an extension of C++ designed specifically for implementing DBMS software.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {208–219},
numpages = {12},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38738,
author = {Richardson, Joel E. and Carey, Michael J.},
title = {Programming Constructs for Database System Implementation in EXODUS},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38738},
doi = {10.1145/38714.38738},
abstract = {The goal of the EXODUS extensible DBMS project is to enable the rapid development of a wide spectrum of high-performance, application-specific database systems EXODUS provides certain kernel facilities for use by all applications and a set of tools to aid the database implementor (DBI) in generating new database system software. Some of the DBI's work is supported by EXODUS tools which generate database components from a specification. However, components such as new abstract data types, access methods, and database operations must be explicitly coded by the DBI. This paper analyzes the major programming problems faced by the DBI, describing the collection of programming language constructs that EXODUS provides for simplifying the DBI's task. These constructs have been embedded in the E programming language, an extension of C++ designed specifically for implementing DBMS software.},
journal = {SIGMOD Rec.},
month = dec,
pages = {208–219},
numpages = {12}
}

@inproceedings{10.1145/38713.38739,
author = {Lindsay, Bruce and McPherson, John and Pirahesh, Hamid},
title = {A Data Management Extension Architecture},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38739},
doi = {10.1145/38713.38739},
abstract = {A database management system architecture is described that facilitates the implementation of data management extensions for relational database systems. The architecture defines two classes of data management extensions alternative ways of storing relations called relation “storage methods”, and access paths, integrity constraints, or triggers which are “attachments” to relations. Generic sets of operations are defined for storage methods and attachments, and these operations must be provided in order to add a new storage method or attachment type to the system. The data management extension architecture also provides common services for coordination of storage method and attachment execution. This article describes the data management extension architecture along with some implementation issues and techniques.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {220–226},
numpages = {7},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38739,
author = {Lindsay, Bruce and McPherson, John and Pirahesh, Hamid},
title = {A Data Management Extension Architecture},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38739},
doi = {10.1145/38714.38739},
abstract = {A database management system architecture is described that facilitates the implementation of data management extensions for relational database systems. The architecture defines two classes of data management extensions alternative ways of storing relations called relation “storage methods”, and access paths, integrity constraints, or triggers which are “attachments” to relations. Generic sets of operations are defined for storage methods and attachments, and these operations must be provided in order to add a new storage method or attachment type to the system. The data management extension architecture also provides common services for coordination of storage method and attachment execution. This article describes the data management extension architecture along with some implementation issues and techniques.},
journal = {SIGMOD Rec.},
month = dec,
pages = {220–226},
numpages = {7}
}

@inproceedings{10.1145/38713.38740,
author = {Jajodia, Sushil and Mutchler, David},
title = {Dynamic Voting},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38740},
doi = {10.1145/38713.38740},
abstract = {In a voting-based algorithm, a replicated file can be updated in a partition if it contains a majority of copies. In this paper, we propose an extension of this scheme which permits a file to be updated in a partition provided it contains a majority of up-to-date copies. Our scheme not only preserves mutual consistency of the replicated file, but provides improvement in its availability as well. We develop a stochastic model which gives insight into the improvements afforded by our scheme over the voting scheme.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {227–238},
numpages = {12},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38740,
author = {Jajodia, Sushil and Mutchler, David},
title = {Dynamic Voting},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38740},
doi = {10.1145/38714.38740},
abstract = {In a voting-based algorithm, a replicated file can be updated in a partition if it contains a majority of copies. In this paper, we propose an extension of this scheme which permits a file to be updated in a partition provided it contains a majority of up-to-date copies. Our scheme not only preserves mutual consistency of the replicated file, but provides improvement in its availability as well. We develop a stochastic model which gives insight into the improvements afforded by our scheme over the voting scheme.},
journal = {SIGMOD Rec.},
month = dec,
pages = {227–238},
numpages = {12}
}

@inproceedings{10.1145/38713.38741,
author = {Haerder, Theo and Rothermel, Kurt},
title = {Concepts for Transaction Recovery in Nested Transactions},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38741},
doi = {10.1145/38713.38741},
abstract = {The concept of nested transactions offers more decomposable execution units and finer grained control over recovery and concurrency as compared to 'flat' transactions. To exploit these advantages, especially transaction recovery has to be refined and adjusted to the requirements of the control structure.In this paper, we investigate transaction recovery for nested transactions. Therefore, a model for nested transaction is introduced allowing for synchronous and asynchronous transaction invocation as well as single call and conversational interfaces. For the resulting four parameter combinations, the properties and dependencies of transaction recovery are explored if a transaction is 'unit of recovery' and if savepoints within transactions are used to gain finer recovery units.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {239–248},
numpages = {10},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38741,
author = {Haerder, Theo and Rothermel, Kurt},
title = {Concepts for Transaction Recovery in Nested Transactions},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38741},
doi = {10.1145/38714.38741},
abstract = {The concept of nested transactions offers more decomposable execution units and finer grained control over recovery and concurrency as compared to 'flat' transactions. To exploit these advantages, especially transaction recovery has to be refined and adjusted to the requirements of the control structure.In this paper, we investigate transaction recovery for nested transactions. Therefore, a model for nested transaction is introduced allowing for synchronous and asynchronous transaction invocation as well as single call and conversational interfaces. For the resulting four parameter combinations, the properties and dependencies of transaction recovery are explored if a transaction is 'unit of recovery' and if savepoints within transactions are used to gain finer recovery units.},
journal = {SIGMOD Rec.},
month = dec,
pages = {239–248},
numpages = {10}
}

@inproceedings{10.1145/38713.38742,
author = {Garcia-Molina, Hector and Salem, Kenneth},
title = {Sagas},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38742},
doi = {10.1145/38713.38742},
abstract = {Long lived transactions (LLTs) hold on to database resources for relatively long periods of time, significantly delaying the termination of shorter and more common transactions. To alleviate these problems we propose the notion of a saga. A LLT is a saga if it can be written as a sequence of transactions that can be interleaved with other transactions. The database management system guarantees that either all the transactions in a saga are successfully completed or compensating transactions are run to amend a partial execution. Both the concept of saga and its implementation are relatively simple, but they have the potential to improve performance significantly. We analyze the various implementation issues related to sagas, including how they can be run on an existing system that does not directly support them. We also discuss techniques for database and LLT design that make it feasible to break up LLTs into sagas.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {249–259},
numpages = {11},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38742,
author = {Garcia-Molina, Hector and Salem, Kenneth},
title = {Sagas},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38742},
doi = {10.1145/38714.38742},
abstract = {Long lived transactions (LLTs) hold on to database resources for relatively long periods of time, significantly delaying the termination of shorter and more common transactions. To alleviate these problems we propose the notion of a saga. A LLT is a saga if it can be written as a sequence of transactions that can be interleaved with other transactions. The database management system guarantees that either all the transactions in a saga are successfully completed or compensating transactions are run to amend a partial execution. Both the concept of saga and its implementation are relatively simple, but they have the potential to improve performance significantly. We analyze the various implementation issues related to sagas, including how they can be run on an existing system that does not directly support them. We also discuss techniques for database and LLT design that make it feasible to break up LLTs into sagas.},
journal = {SIGMOD Rec.},
month = dec,
pages = {249–259},
numpages = {11}
}

@inproceedings{10.1145/38713.38743,
author = {Freeston, Michael},
title = {The BANG File: A New Kind of Grid File},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38743},
doi = {10.1145/38713.38743},
abstract = {A new multi-dimensional file structure has been developed in the course of a project to devise ways of improving the support for interactive queries to database and knowledge bases. Christened the 'BANG' file - a Balanced And Nested Grid - the new structure is of the 'grid file' type, but is fundamentally different from previous grid file designs in that it does not share their common underlying properties. It has a tree-structured directory which has the self-balancing property of a B-tree and which, in contrast to previous designs, always expands at the same rate as the data, whatever the form of the data distribution. Its partitioning strategy both accurately reflects the clustering of points in the data space, and is flexible enough to adapt gracefully to changes in the distribution.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {260–269},
numpages = {10},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38743,
author = {Freeston, Michael},
title = {The BANG File: A New Kind of Grid File},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38743},
doi = {10.1145/38714.38743},
abstract = {A new multi-dimensional file structure has been developed in the course of a project to devise ways of improving the support for interactive queries to database and knowledge bases. Christened the 'BANG' file - a Balanced And Nested Grid - the new structure is of the 'grid file' type, but is fundamentally different from previous grid file designs in that it does not share their common underlying properties. It has a tree-structured directory which has the self-balancing property of a B-tree and which, in contrast to previous designs, always expands at the same rate as the data, whatever the form of the data distribution. Its partitioning strategy both accurately reflects the clustering of points in the data space, and is flexible enough to adapt gracefully to changes in the distribution.},
journal = {SIGMOD Rec.},
month = dec,
pages = {260–269},
numpages = {10}
}

@inproceedings{10.1145/38713.38744,
author = {Nelson, Randal C. and Samet, Hanan},
title = {A Population Analysis for Hierarchical Data Structures},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38744},
doi = {10.1145/38713.38744},
abstract = {A new method termed population analysis is presented for approximating the distribution of node occupancies in hierarchical data structures which store a variable number of geometric data items per node. The basic idea is to describe a dynamic data structure as a set of populations which are permitted to transform into one another according to certain rules. The transformation rules are used to obtain a set of equations describing a population distribution which is stable under insertion of additional information into the structure. These equations can then be solved, either analytically or numerically, to obtain the population distribution. Hierarchical data structures are modeled by letting each population represent the nodes of a given occupancy. A detailed analysis of quadtree data structures for storing point data is presented, and the results are compared to experimental data. Two phenomena referred to as aging and phasing are defined and shown to account for the differences between the experimental results and those predicted by the model. The population technique is compared with statistical methods of analyzing similar data structures.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {270–277},
numpages = {8},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38744,
author = {Nelson, Randal C. and Samet, Hanan},
title = {A Population Analysis for Hierarchical Data Structures},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38744},
doi = {10.1145/38714.38744},
abstract = {A new method termed population analysis is presented for approximating the distribution of node occupancies in hierarchical data structures which store a variable number of geometric data items per node. The basic idea is to describe a dynamic data structure as a set of populations which are permitted to transform into one another according to certain rules. The transformation rules are used to obtain a set of equations describing a population distribution which is stable under insertion of additional information into the structure. These equations can then be solved, either analytically or numerically, to obtain the population distribution. Hierarchical data structures are modeled by letting each population represent the nodes of a given occupancy. A detailed analysis of quadtree data structures for storing point data is presented, and the results are compared to experimental data. Two phenomena referred to as aging and phasing are defined and shown to account for the differences between the experimental results and those predicted by the model. The population technique is compared with statistical methods of analyzing similar data structures.},
journal = {SIGMOD Rec.},
month = dec,
pages = {270–277},
numpages = {8}
}

@inproceedings{10.1145/38713.38745,
author = {Sellis, Timos K.},
title = {Efficiently Supporting Procedures in Relational Database Systems},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38745},
doi = {10.1145/38713.38745},
abstract = {We examine an extended relational database system which supports database procedures as full fledged objects. In particular, we focus on the problems of query processing and efficient support for database procedures. First, a variation to the original INGRES decomposition algorithm is presented. Then, we examine the idea of storing results of previously processed procedures in secondary storage (caching). Using a cache, the cost of processing a query can be reduced by preventing multiple evaluations of the same procedure. Problems associated with cache organizations, such as replacement policies and validation schemes are examined. Another means for reducing the execution cost of queries is indexing. A new indexing scheme for cached results, Partial Indexing, is proposed and analyzed.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {278–291},
numpages = {14},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38745,
author = {Sellis, Timos K.},
title = {Efficiently Supporting Procedures in Relational Database Systems},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38745},
doi = {10.1145/38714.38745},
abstract = {We examine an extended relational database system which supports database procedures as full fledged objects. In particular, we focus on the problems of query processing and efficient support for database procedures. First, a variation to the original INGRES decomposition algorithm is presented. Then, we examine the idea of storing results of previously processed procedures in secondary storage (caching). Using a cache, the cost of processing a query can be reduced by preventing multiple evaluations of the same procedure. Problems associated with cache organizations, such as replacement policies and validation schemes are examined. Another means for reducing the execution cost of queries is indexing. A new indexing scheme for cached results, Partial Indexing, is proposed and analyzed.},
journal = {SIGMOD Rec.},
month = dec,
pages = {278–291},
numpages = {14}
}

@inproceedings{10.1145/38713.38746,
author = {Hardwick, Martin},
title = {Why ROSE is Fast: Five Optimizations in the Design of an Experimental Database System for CAD/CAM Applications},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38746},
doi = {10.1145/38713.38746},
abstract = {ROSE is an experimental database system for CAD/CAM applications that organizes a database into entries and relationships. The data model of ROSE is an extension of the relational model and the data manipulation language is an extension of the relational algebra. Internally, ROSE is organized so that it can use operating system services to implement database system services. In this paper we describe five optimizations that have helped to make ROSE a fast database system for CAD/CAM.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {292–298},
numpages = {7},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38746,
author = {Hardwick, Martin},
title = {Why ROSE is Fast: Five Optimizations in the Design of an Experimental Database System for CAD/CAM Applications},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38746},
doi = {10.1145/38714.38746},
abstract = {ROSE is an experimental database system for CAD/CAM applications that organizes a database into entries and relationships. The data model of ROSE is an extension of the relational model and the data manipulation language is an extension of the relational algebra. Internally, ROSE is organized so that it can use operating system services to implement database system services. In this paper we describe five optimizations that have helped to make ROSE a fast database system for CAD/CAM.},
journal = {SIGMOD Rec.},
month = dec,
pages = {292–298},
numpages = {7}
}

@inproceedings{10.1145/38713.38747,
author = {Kemper, Alfons and Lockemann, Peter C. and Wallrath, Mechtild},
title = {An Object-Oriented System for Engineering Applications},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38747},
doi = {10.1145/38713.38747},
abstract = {One of the most promising approaches to database support of engineering applications is the concept of object-oriented database management. Object-orientation is usually approached from either a behavioral or structural viewpoint. The former emphasizes the application-specific manipulation of technical objects while hiding their structural details whereas the latter concentrates on the structural aspects and their efficient implementation. The thesis of the paper is that the two viewpoints may enter into a fruitful symbiosis where a behaviorally object-oriented system is implemented on top of a structurally object-oriented database system, thereby combining ease of use by the engineer with high database system performance. The thesis will be demonstrated in the paper by a user-friendly interface based on user-definable abstract datatypes and its implementation using a prototype for the non-first-normal-form (NF2) relational model, and will be supported by an engineering example application from off-line robot programming.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {299–310},
numpages = {12},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38747,
author = {Kemper, Alfons and Lockemann, Peter C. and Wallrath, Mechtild},
title = {An Object-Oriented System for Engineering Applications},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38747},
doi = {10.1145/38714.38747},
abstract = {One of the most promising approaches to database support of engineering applications is the concept of object-oriented database management. Object-orientation is usually approached from either a behavioral or structural viewpoint. The former emphasizes the application-specific manipulation of technical objects while hiding their structural details whereas the latter concentrates on the structural aspects and their efficient implementation. The thesis of the paper is that the two viewpoints may enter into a fruitful symbiosis where a behaviorally object-oriented system is implemented on top of a structurally object-oriented database system, thereby combining ease of use by the engineer with high database system performance. The thesis will be demonstrated in the paper by a user-friendly interface based on user-definable abstract datatypes and its implementation using a prototype for the non-first-normal-form (NF2) relational model, and will be supported by an engineering example application from off-line robot programming.},
journal = {SIGMOD Rec.},
month = dec,
pages = {299–310},
numpages = {12}
}

@inproceedings{10.1145/38713.38748,
author = {Banerjee, Jay and Kim, Won and Kim, Hyoung-Joo and Korth, Henry F.},
title = {Semantics and Implementation of Schema Evolution in Object-Oriented Databases},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38748},
doi = {10.1145/38713.38748},
abstract = {Object-oriented programming is well-suited to such data-intensive application domains as CAD/CAM, AI, and OIS (office information systems) with multimedia documents. At MCC we have built a prototype object-oriented database system, called ORION. It adds persistence and sharability to objects created and manipulated in applications implemented in an object-oriented programming environment. One of the important requirements of these applications is schema evolution, that is, the ability to dynamically make a wide variety of changes to the database schema. In this paper, following a brief review of the object-oriented data model that we support in ORION, we establish a framework for supporting schema evolution, define the semantics of schema evolution, and discuss its implementation.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {311–322},
numpages = {12},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38748,
author = {Banerjee, Jay and Kim, Won and Kim, Hyoung-Joo and Korth, Henry F.},
title = {Semantics and Implementation of Schema Evolution in Object-Oriented Databases},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38748},
doi = {10.1145/38714.38748},
abstract = {Object-oriented programming is well-suited to such data-intensive application domains as CAD/CAM, AI, and OIS (office information systems) with multimedia documents. At MCC we have built a prototype object-oriented database system, called ORION. It adds persistence and sharability to objects created and manipulated in applications implemented in an object-oriented programming environment. One of the important requirements of these applications is schema evolution, that is, the ability to dynamically make a wide variety of changes to the database schema. In this paper, following a brief review of the object-oriented data model that we support in ORION, we establish a framework for supporting schema evolution, define the semantics of schema evolution, and discuss its implementation.},
journal = {SIGMOD Rec.},
month = dec,
pages = {311–322},
numpages = {12}
}

@inproceedings{10.1145/38713.38749,
author = {Cruz, Isabel F. and Mendelzon, Alberto O. and Wood, Peter T.},
title = {A Graphical Query Language Supporting Recursion},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38749},
doi = {10.1145/38713.38749},
abstract = {We define a language G for querying data represented as a labeled graph G. By considering G as a relation, this graphical query language can be viewed as a relational query language, and its expressive power can be compared to that of other relational query languages. We do not propose G as an alternative to general purpose relational query languages, but rather as a complementary language in which recursive queries are simple to formulate. The user is aided in this formulation by means of a graphical interface. The provision of regular expressions in G allows recursive queries more general than transitive closure to be posed, although the language is not as powerful as those based on function-free Horn clauses. However, we hope to be able to exploit well-known graph algorithms in evaluating recursive queries efficiently, a topic which has received widespread attention recently.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {323–330},
numpages = {8},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38749,
author = {Cruz, Isabel F. and Mendelzon, Alberto O. and Wood, Peter T.},
title = {A Graphical Query Language Supporting Recursion},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38749},
doi = {10.1145/38714.38749},
abstract = {We define a language G for querying data represented as a labeled graph G. By considering G as a relation, this graphical query language can be viewed as a relational query language, and its expressive power can be compared to that of other relational query languages. We do not propose G as an alternative to general purpose relational query languages, but rather as a complementary language in which recursive queries are simple to formulate. The user is aided in this formulation by means of a graphical interface. The provision of regular expressions in G allows recursive queries more general than transitive closure to be posed, although the language is not as powerful as those based on function-free Horn clauses. However, we hope to be able to exploit well-known graph algorithms in evaluating recursive queries efficiently, a topic which has received widespread attention recently.},
journal = {SIGMOD Rec.},
month = dec,
pages = {323–330},
numpages = {8}
}

@inproceedings{10.1145/38713.38750,
author = {Jagadish, H. V. and Agrawal, Rakesh and Ness, Linda},
title = {A Study of Transitive Closure as a Recursion Mechanism},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38750},
doi = {10.1145/38713.38750},
abstract = {We show that every linearly recursive query can be expressed as a transitive closure possibly preceded and followed by operations already available in relational algebra. This reduction is possible even if there are repeated variables in the recursive literals and if some of the arguments in the recursive literals are constants. Such an equivalence has significant theoretical and practical ramifications. One the one hand it influences the design of expressive notations to capture recursion as an augmentation of relational query languages. On the other hand implementation of deductive databases is impacted in that the design does not have to provide the generality that linear recursion would demand. It suffices to study the single problem of transitive closure and to provide an efficient implementation for it.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {331–344},
numpages = {14},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38750,
author = {Jagadish, H. V. and Agrawal, Rakesh and Ness, Linda},
title = {A Study of Transitive Closure as a Recursion Mechanism},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38750},
doi = {10.1145/38714.38750},
abstract = {We show that every linearly recursive query can be expressed as a transitive closure possibly preceded and followed by operations already available in relational algebra. This reduction is possible even if there are repeated variables in the recursive literals and if some of the arguments in the recursive literals are constants. Such an equivalence has significant theoretical and practical ramifications. One the one hand it influences the design of expressive notations to capture recursion as an augmentation of relational query languages. On the other hand implementation of deductive databases is impacted in that the design does not have to provide the generality that linear recursion would demand. It suffices to study the single problem of transitive closure and to provide an efficient implementation for it.},
journal = {SIGMOD Rec.},
month = dec,
pages = {331–344},
numpages = {14}
}

@inproceedings{10.1145/38713.38751,
author = {Zhang, Weining and Yu, C. T.},
title = {A Necessary Condition for a Doubly Recursive Rule to Be Equivalent to a Linear Recursive Rule},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38751},
doi = {10.1145/38713.38751},
abstract = {Nonlinear recursive queries are usually less efficient in processing than linear recursive queries. It is therefore of interest to transform non-linear recursive queries into linear ones. We obtain a necessary and sufficient condition for a doubly recursive rule of a certain type to be logically equivalent to a single linear recursive rule obtained in a specific way.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {345–356},
numpages = {12},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38751,
author = {Zhang, Weining and Yu, C. T.},
title = {A Necessary Condition for a Doubly Recursive Rule to Be Equivalent to a Linear Recursive Rule},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38751},
doi = {10.1145/38714.38751},
abstract = {Nonlinear recursive queries are usually less efficient in processing than linear recursive queries. It is therefore of interest to transform non-linear recursive queries into linear ones. We obtain a necessary and sufficient condition for a doubly recursive rule of a certain type to be logically equivalent to a single linear recursive rule obtained in a specific way.},
journal = {SIGMOD Rec.},
month = dec,
pages = {345–356},
numpages = {12}
}

@inproceedings{10.1145/38713.38752,
author = {Morgenstern, Matthew},
title = {Security and Inference in Multilevel Database and Knowledge-Base Systems},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38752},
doi = {10.1145/38713.38752},
abstract = {This paper addresses the threat to multilevel security that arises from logical inference and the semantics of the application. Such compromises of security are particularly challenging since they circumvent traditional security mechanisms and rely on a user's knowledge of the application. The problems of inference and security have heretofore been amorphous and difficult to circumscribe. We focus on these problems in the context of a multilevel database system and show their relevance to knowledge-based systems, sometimes referred to as expert systems. Here we establish a framework for studying these inference control problems, describe a representation for relevant semantics of the application, develop criteria for safety and security of a system to prevent these problems, and outline algorithms for enforcing these criteria.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {357–373},
numpages = {17},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38752,
author = {Morgenstern, Matthew},
title = {Security and Inference in Multilevel Database and Knowledge-Base Systems},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38752},
doi = {10.1145/38714.38752},
abstract = {This paper addresses the threat to multilevel security that arises from logical inference and the semantics of the application. Such compromises of security are particularly challenging since they circumvent traditional security mechanisms and rely on a user's knowledge of the application. The problems of inference and security have heretofore been amorphous and difficult to circumscribe. We focus on these problems in the context of a multilevel database system and show their relevance to knowledge-based systems, sometimes referred to as expert systems. Here we establish a framework for studying these inference control problems, describe a representation for relevant semantics of the application, develop criteria for safety and security of a system to prevent these problems, and outline algorithms for enforcing these criteria.},
journal = {SIGMOD Rec.},
month = dec,
pages = {357–373},
numpages = {17}
}

@inproceedings{10.1145/38713.38753,
author = {Stemple, David and Mazumdar, Subhasish and Sheard, Tim},
title = {On the Modes and Meaning of Feedback to Transaction Designers},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38753},
doi = {10.1145/38713.38753},
abstract = {An analysis of database transactions in the presence of database integrity constraints can lead to several modes of feedback to transaction designers. The different kinds of feedback include tests and updates that could be added to the transaction to make it obey the integrity constraints, as well as predicates representing post-conditions guaranteed by a transaction's execution. We discuss the various modes, meanings, and uses of feedback. We also discuss methods of generating feedback from integrity constraints, transaction details and theorems constituting both generic knowledge of database systems and specific knowledge about a particular database. Our methods are based on a running system that generates tailored theories about database systems from their schemas and uses these theories to prove that transactions obey integrity constraints.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {374–386},
numpages = {13},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38753,
author = {Stemple, David and Mazumdar, Subhasish and Sheard, Tim},
title = {On the Modes and Meaning of Feedback to Transaction Designers},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38753},
doi = {10.1145/38714.38753},
abstract = {An analysis of database transactions in the presence of database integrity constraints can lead to several modes of feedback to transaction designers. The different kinds of feedback include tests and updates that could be added to the transaction to make it obey the integrity constraints, as well as predicates representing post-conditions guaranteed by a transaction's execution. We discuss the various modes, meanings, and uses of feedback. We also discuss methods of generating feedback from integrity constraints, transaction details and theorems constituting both generic knowledge of database systems and specific knowledge about a particular database. Our methods are based on a running system that generates tailored theories about database systems from their schemas and uses these theories to prove that transactions obey integrity constraints.},
journal = {SIGMOD Rec.},
month = dec,
pages = {374–386},
numpages = {13}
}

@inproceedings{10.1145/38713.38754,
author = {Rubenstein, W. B. and Kubicar, M. S. and Cattell, R. G. G.},
title = {Benchmarking Simple Database Operations},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38754},
doi = {10.1145/38713.38754},
abstract = {There are two widely-known benchmarks for database management systems the TP1 benchmarks (Anon et al [1985]), designed to measure transaction throughout, and the Wisconsin benchmarks (Bitton, Dewitt, &amp; Turbyfil [1984]), designed to measure the performance of a relational query processor. In our work with databases on engineering workstations, we found neither of these benchmarks a suitable measure for our applications' needs. Instead, our requirements are for response time for simple queries. We propose benchmark measurements to measure response time, specifically designed for the simple, object-oriented queries that engineering database applications perform. We report results from running this benchmark against some database systems we use ourselves, and provide enough detail for others to reproduce the benchmark measurements on other relational, object-oriented, or specialized database systems. We discuss a number of factors that make an order of magnitude improvement in benchmark performance caching the entire database in main memory, avoiding query optimization overhead, using physical links for prejoins, and using an alternative to the generally-accepted database “server” architecture on distributed networks.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {387–394},
numpages = {8},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38754,
author = {Rubenstein, W. B. and Kubicar, M. S. and Cattell, R. G. G.},
title = {Benchmarking Simple Database Operations},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38754},
doi = {10.1145/38714.38754},
abstract = {There are two widely-known benchmarks for database management systems the TP1 benchmarks (Anon et al [1985]), designed to measure transaction throughout, and the Wisconsin benchmarks (Bitton, Dewitt, &amp; Turbyfil [1984]), designed to measure the performance of a relational query processor. In our work with databases on engineering workstations, we found neither of these benchmarks a suitable measure for our applications' needs. Instead, our requirements are for response time for simple queries. We propose benchmark measurements to measure response time, specifically designed for the simple, object-oriented queries that engineering database applications perform. We report results from running this benchmark against some database systems we use ourselves, and provide enough detail for others to reproduce the benchmark measurements on other relational, object-oriented, or specialized database systems. We discuss a number of factors that make an order of magnitude improvement in benchmark performance caching the entire database in main memory, avoiding query optimization overhead, using physical links for prejoins, and using an alternative to the generally-accepted database “server” architecture on distributed networks.},
journal = {SIGMOD Rec.},
month = dec,
pages = {387–394},
numpages = {8}
}

@inproceedings{10.1145/38713.38755,
author = {Gray, Jim and Putzolu, Franco},
title = {The 5 Minute Rule for Trading Memory for Disc Accesses and the 10 Byte Rule for Trading Memory for CPU Time},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38755},
doi = {10.1145/38713.38755},
abstract = {If an item is accessed frequently enough, it should be main memory resident. For current technology, “frequently enough” means about every five minutes.Along a similar vein, one can frequently trade memory space for CPU time. For example, bits can be packed in a byte at the expense of extra instructions to extract the bits. It makes economic sense to spend ten bytes of main memory to save one instruction per second.These results depend on current price ratios of processors, memory and disc accesses. These ratios are changing and hence the constants in the rules are changing.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {395–398},
numpages = {4},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38755,
author = {Gray, Jim and Putzolu, Franco},
title = {The 5 Minute Rule for Trading Memory for Disc Accesses and the 10 Byte Rule for Trading Memory for CPU Time},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38755},
doi = {10.1145/38714.38755},
abstract = {If an item is accessed frequently enough, it should be main memory resident. For current technology, “frequently enough” means about every five minutes.Along a similar vein, one can frequently trade memory space for CPU time. For example, bits can be packed in a byte at the expense of extra instructions to extract the bits. It makes economic sense to spend ten bytes of main memory to save one instruction per second.These results depend on current price ratios of processors, memory and disc accesses. These ratios are changing and hence the constants in the rules are changing.},
journal = {SIGMOD Rec.},
month = dec,
pages = {395–398},
numpages = {4}
}

@inproceedings{10.1145/38713.38756,
author = {Richardson, James P. and Lu, Hongjun and Mikkilineni, Krishna},
title = {Design and Evaluation of Parallel Pipelined Join Algorithms},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38756},
doi = {10.1145/38713.38756},
abstract = {The join operation is the most costly operation in relational database management systems. Distributed and parallel processing can effectively speed up the join operation. In this paper, we describe a number of highly parallel and pipelined multiprocessor join algorithms using sort-merge and hashing techniques. Among them, two algorithms are parallel and pipelined versions of traditional sort-merge join methods, two algorithms use both hashing and sort-merge techniques, and another two are variations of the hybrid hash join algorithms. The performance of those algorithms is evaluated analytically against a generic database machine architecture. The methodology used in the design and evaluation of these algorithms is also discussed.The results of the analysis indicate that using a hashing technique to partition the source relations can dramatically reduce the elapsed time hash-based algorithms outperform sort-merge algorithms in almost all cases because of their high parallelism. Hash-based sort-merge and hybrid hash methods provide similar performance in most cases. With large source relations, the algorithms which replicate the smaller relation usually give better elapsed time. Sharing memory among processors also improves performance somewhat.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {399–409},
numpages = {11},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38756,
author = {Richardson, James P. and Lu, Hongjun and Mikkilineni, Krishna},
title = {Design and Evaluation of Parallel Pipelined Join Algorithms},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38756},
doi = {10.1145/38714.38756},
abstract = {The join operation is the most costly operation in relational database management systems. Distributed and parallel processing can effectively speed up the join operation. In this paper, we describe a number of highly parallel and pipelined multiprocessor join algorithms using sort-merge and hashing techniques. Among them, two algorithms are parallel and pipelined versions of traditional sort-merge join methods, two algorithms use both hashing and sort-merge techniques, and another two are variations of the hybrid hash join algorithms. The performance of those algorithms is evaluated analytically against a generic database machine architecture. The methodology used in the design and evaluation of these algorithms is also discussed.The results of the analysis indicate that using a hashing technique to partition the source relations can dramatically reduce the elapsed time hash-based algorithms outperform sort-merge algorithms in almost all cases because of their high parallelism. Hash-based sort-merge and hybrid hash methods provide similar performance in most cases. With large source relations, the algorithms which replicate the smaller relation usually give better elapsed time. Sharing memory among processors also improves performance somewhat.},
journal = {SIGMOD Rec.},
month = dec,
pages = {399–409},
numpages = {11}
}

@inproceedings{10.1145/38713.38757,
author = {Butler, Margaret H.},
title = {Storage Reclamation in Object Oriented Database Systems},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38757},
doi = {10.1145/38713.38757},
abstract = {When providing data management for nontraditional data, database systems encounter storage reclamation problems similar to those encountered by virtual memory managers. The paging behavior of existing automatic storage reclamation schemes as applied to objects stored in a database management system is one indicator of the performance cost of various features of storage reclamation algorithms. The results of modeling the paging behavior suggest that Mark and Sweep causes many more input/output operations than Copy-Compact. A contributing factor to the expense of Mark and Sweep is that it does not recluster memory as does Copy-Compact. If memory is not reclustered, the average cost of accessing data can go up tremendously. Other algorithms that do not recluster memory also suffer performance problems, namely all reference counting schemes. The main advantage of a reference count scheme is that it does not force a running program to pause for a long period of time while reclamation takes place, it amortizes the cost of reclamation across all accesses. The reclustering of Copy-Compact and the cost amortization of Reference Count are combined to great advantage in Baker's algorithm. This algorithm proves to be the least prohibitive for operating on disk-based data.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {410–425},
numpages = {16},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38757,
author = {Butler, Margaret H.},
title = {Storage Reclamation in Object Oriented Database Systems},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38757},
doi = {10.1145/38714.38757},
abstract = {When providing data management for nontraditional data, database systems encounter storage reclamation problems similar to those encountered by virtual memory managers. The paging behavior of existing automatic storage reclamation schemes as applied to objects stored in a database management system is one indicator of the performance cost of various features of storage reclamation algorithms. The results of modeling the paging behavior suggest that Mark and Sweep causes many more input/output operations than Copy-Compact. A contributing factor to the expense of Mark and Sweep is that it does not recluster memory as does Copy-Compact. If memory is not reclustered, the average cost of accessing data can go up tremendously. Other algorithms that do not recluster memory also suffer performance problems, namely all reference counting schemes. The main advantage of a reference count scheme is that it does not force a running program to pause for a long period of time while reclamation takes place, it amortizes the cost of reclamation across all accesses. The reclustering of Copy-Compact and the cost amortization of Reference Count are combined to great advantage in Baker's algorithm. This algorithm proves to be the least prohibitive for operating on disk-based data.},
journal = {SIGMOD Rec.},
month = dec,
pages = {410–425},
numpages = {16}
}

@inproceedings{10.1145/38713.38758,
author = {Faloutsos, Christos and Sellis, Timos and Roussopoulos, Nick},
title = {Analysis of Object Oriented Spatial Access Methods},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38758},
doi = {10.1145/38713.38758},
abstract = {This paper provides an analysis of R-trees and a variation (R+-trees) that avoids overlapping rectangles in intermediate nodes of the tree. The main contributions of the paper are the following. We provide the first known analysis of R-trees. Although formulas are given for objects in one dimension (line segments), they can be generalized for objects in higher dimensions as well. We show how the transformation of objects to higher dimensions [HINR83] can be effectively used as a tool for the analysis of R- and R+- trees. Finally, we derive formulas for R+-trees and compare the two methods analytically. The results we obtained show that R+-trees require less than half the disk accesses required by a corresponding R-tree when searching files of real life sizes R+-trees are clearly superior in cases where there are few long segments and a lot of small ones.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {426–439},
numpages = {14},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38758,
author = {Faloutsos, Christos and Sellis, Timos and Roussopoulos, Nick},
title = {Analysis of Object Oriented Spatial Access Methods},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38758},
doi = {10.1145/38714.38758},
abstract = {This paper provides an analysis of R-trees and a variation (R+-trees) that avoids overlapping rectangles in intermediate nodes of the tree. The main contributions of the paper are the following. We provide the first known analysis of R-trees. Although formulas are given for objects in one dimension (line segments), they can be generalized for objects in higher dimensions as well. We show how the transformation of objects to higher dimensions [HINR83] can be effectively used as a tool for the analysis of R- and R+- trees. Finally, we derive formulas for R+-trees and compare the two methods analytically. The results we obtained show that R+-trees require less than half the disk accesses required by a corresponding R-tree when searching files of real life sizes R+-trees are clearly superior in cases where there are few long segments and a lot of small ones.},
journal = {SIGMOD Rec.},
month = dec,
pages = {426–439},
numpages = {14}
}

@inproceedings{10.1145/38713.38759,
author = {Hanson, Eric N.},
title = {A Performance Analysis of View Materialization Strategies},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38759},
doi = {10.1145/38713.38759},
abstract = {The conventional way to process commands for relational views is to use query modification to translate the commands into ones on the base relations. An alternative approach has been proposed recently, whereby materialized copies of views are kept, and incrementally updated immediately after each modification of the database. A related scheme exists, in which update of materialized views is deferred until just before data is retrieved from the view. A performance analysis is presented comparing the cost of query modification, immediate view maintenance, and deferred view maintenance. Three different models of the structure of views are given a simple selection and projection of one relation, the natural join of two relations, and an aggregate (e.g. the sum of values in a column) over a selection-projection view. The results show that the choice of the most efficient view maintenance method depends heavily on the structure of the database, the view definition, and the type of query and update activity present.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {440–453},
numpages = {14},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38759,
author = {Hanson, Eric N.},
title = {A Performance Analysis of View Materialization Strategies},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38759},
doi = {10.1145/38714.38759},
abstract = {The conventional way to process commands for relational views is to use query modification to translate the commands into ones on the base relations. An alternative approach has been proposed recently, whereby materialized copies of views are kept, and incrementally updated immediately after each modification of the database. A related scheme exists, in which update of materialized views is deferred until just before data is retrieved from the view. A performance analysis is presented comparing the cost of query modification, immediate view maintenance, and deferred view maintenance. Three different models of the structure of views are given a simple selection and projection of one relation, the natural join of two relations, and an aggregate (e.g. the sum of values in a column) over a selection-projection view. The results show that the choice of the most efficient view maintenance method depends heavily on the structure of the database, the view definition, and the type of query and update activity present.},
journal = {SIGMOD Rec.},
month = dec,
pages = {440–453},
numpages = {14}
}

@inproceedings{10.1145/38713.38760,
author = {Segev, Arie and Shoshani, Arie},
title = {Logical Modeling of Temporal Data},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38760},
doi = {10.1145/38713.38760},
abstract = {In this paper we examine the semantics and develop constructs for temporal data independent of any traditional data model, such as the relational or network data models. Unlike many other works which extend existing models to support temporal data, our purpose is to characterize the properties of temporal data and operators over them without being influenced by traditional models which were not specifically designed to model temporal data. We develop data constructs that represent sequences of temporal values, identify their semantic properties, and define operations over these structures.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {454–466},
numpages = {13},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38760,
author = {Segev, Arie and Shoshani, Arie},
title = {Logical Modeling of Temporal Data},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38760},
doi = {10.1145/38714.38760},
abstract = {In this paper we examine the semantics and develop constructs for temporal data independent of any traditional data model, such as the relational or network data models. Unlike many other works which extend existing models to support temporal data, our purpose is to characterize the properties of temporal data and operators over them without being influenced by traditional models which were not specifically designed to model temporal data. We develop data constructs that represent sequences of temporal values, identify their semantic properties, and define operations over these structures.},
journal = {SIGMOD Rec.},
month = dec,
pages = {454–466},
numpages = {13}
}

@inproceedings{10.1145/38713.38761,
author = {McKenzie, Edwin and Snodgrass, Richard},
title = {Extending the Relational Algebra to Support Transaction Time},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38761},
doi = {10.1145/38713.38761},
abstract = {In this paper we discuss extensions to the conventional relational algebra to support transaction time. We show that these extensions are applicable to historical algebras that support valid time, yielding a temporal algebraic language. Since transaction time concerns the storage of information in the database, the notion of state is central. The extensions are formalized using denotational semantics. The additions preserve the useful properties of the conventional relational algebra.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {467–478},
numpages = {12},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38761,
author = {McKenzie, Edwin and Snodgrass, Richard},
title = {Extending the Relational Algebra to Support Transaction Time},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38761},
doi = {10.1145/38714.38761},
abstract = {In this paper we discuss extensions to the conventional relational algebra to support transaction time. We show that these extensions are applicable to historical algebras that support valid time, yielding a temporal algebraic language. Since transaction time concerns the storage of information in the database, the notion of state is central. The extensions are formalized using denotational semantics. The additions preserve the useful properties of the conventional relational algebra.},
journal = {SIGMOD Rec.},
month = dec,
pages = {467–478},
numpages = {12}
}

@inproceedings{10.1145/38713.38762,
author = {Rubenstein, W. Bradley},
title = {A Database Design for Musical Information},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38762},
doi = {10.1145/38713.38762},
abstract = {As part of our research into a general purpose data management system for musical information, a major focus has been the development of tools to support a data model for music. This paper first outlines the various types of information that fall under the purview of our proposed data manager. We consider extensions to the entity-relationship data model to implement the notion of hierarchical ordering, commonly found in musical data. We then present examples from our schema for representing musical notation in a database, taking advantage of these extensions.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {479–490},
numpages = {12},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38762,
author = {Rubenstein, W. Bradley},
title = {A Database Design for Musical Information},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38762},
doi = {10.1145/38714.38762},
abstract = {As part of our research into a general purpose data management system for musical information, a major focus has been the development of tools to support a data model for music. This paper first outlines the various types of information that fall under the purview of our proposed data manager. We consider extensions to the entity-relationship data model to implement the notion of hierarchical ordering, commonly found in musical data. We then present examples from our schema for representing musical notation in a database, taking advantage of these extensions.},
journal = {SIGMOD Rec.},
month = dec,
pages = {479–490},
numpages = {12}
}

@inproceedings{10.1145/38713.38763,
author = {Hudson, Scott E. and King, Roger},
title = {Object-Oriented Database Support for Software Environments},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38763},
doi = {10.1145/38713.38763},
abstract = {Cactis is an object-oriented, multi-user DBMS developed at the University of Colorado. The implementation is self-adaptive and concurrent, and runs in the Unix/C Sun workstation environment. A central, unique focus of Cactis is the support of functionally-defined data in a manner which provides good performance. Cactis is intended for use in applications which are conducive to an object-oriented approach and involve derived data. Such applications include software environments.Cactis supports the construction of objects and type/subtype hierarchies, which are useful for managing the complex and highly-interrelated data found in software environments. Such data types include programs, requirement specifications, milestone reports, configurations, documentation, and many others. Cactis uses techniques based on attributed graphs to ensure that functionally-defined attributes of objects, such as compilation dependencies, cost calculations, and milestone dependencies can be maintained efficiently. Since it is necessary to dynamically add new tools (such as debuggers and compilers) to a software environment, the DBMS allows the user to extend the type structure. The system also supports an efficient rollback and recovery mechanism, which provides the framework for a software version facility.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {491–503},
numpages = {13},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38763,
author = {Hudson, Scott E. and King, Roger},
title = {Object-Oriented Database Support for Software Environments},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38763},
doi = {10.1145/38714.38763},
abstract = {Cactis is an object-oriented, multi-user DBMS developed at the University of Colorado. The implementation is self-adaptive and concurrent, and runs in the Unix/C Sun workstation environment. A central, unique focus of Cactis is the support of functionally-defined data in a manner which provides good performance. Cactis is intended for use in applications which are conducive to an object-oriented approach and involve derived data. Such applications include software environments.Cactis supports the construction of objects and type/subtype hierarchies, which are useful for managing the complex and highly-interrelated data found in software environments. Such data types include programs, requirement specifications, milestone reports, configurations, documentation, and many others. Cactis uses techniques based on attributed graphs to ensure that functionally-defined attributes of objects, such as compilation dependencies, cost calculations, and milestone dependencies can be maintained efficiently. Since it is necessary to dynamically add new tools (such as debuggers and compilers) to a software environment, the DBMS allows the user to extend the type structure. The system also supports an efficient rollback and recovery mechanism, which provides the framework for a software version facility.},
journal = {SIGMOD Rec.},
month = dec,
pages = {491–503},
numpages = {13}
}

@inproceedings{10.1145/38713.38764,
author = {Croft, W. B. and Stemple, D. W.},
title = {Supporting Office Document Architectures with Constrained Types},
year = {1987},
isbn = {0897912365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/38713.38764},
doi = {10.1145/38713.38764},
abstract = {Data models have been proposed as a means of defining the objects and operations in an office information system. Office documents, because of their highly variable structure and multimedia content, are a difficult class of objects to model. The modeling task is further complicated by document architecture standards used for interchange between systems. We present an approach to data modeling based on constrained type definitions that allows architecture standards to be defined and ensures that individual document types conform to those standards. The ADABTPL model, which is used to define the schema of document types and standards, is described.},
booktitle = {Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data},
pages = {504–509},
numpages = {6},
location = {San Francisco, California, USA},
series = {SIGMOD '87}
}

@article{10.1145/38714.38764,
author = {Croft, W. B. and Stemple, D. W.},
title = {Supporting Office Document Architectures with Constrained Types},
year = {1987},
issue_date = {Dec. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/38714.38764},
doi = {10.1145/38714.38764},
abstract = {Data models have been proposed as a means of defining the objects and operations in an office information system. Office documents, because of their highly variable structure and multimedia content, are a difficult class of objects to model. The modeling task is further complicated by document architecture standards used for interchange between systems. We present an approach to data modeling based on constrained type definitions that allows architecture standards to be defined and ensures that individual document types conform to those standards. The ADABTPL model, which is used to define the schema of document types and standards, is described.},
journal = {SIGMOD Rec.},
month = dec,
pages = {504–509},
numpages = {6}
}

