@inproceedings{10.1145/130283.277952,
author = {Dozier, Jeff},
title = {Keynote Address: Access to Data in NASA's Earth Observing System},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.277952},
doi = {10.1145/130283.277952},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {1},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.277952,
author = {Dozier, Jeff},
title = {Keynote Address: Access to Data in NASA's Earth Observing System},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.277952},
doi = {10.1145/141484.277952},
journal = {SIGMOD Rec.},
month = jun,
pages = {1},
numpages = {1}
}

@inproceedings{10.1145/130283.130284,
author = {Merz, Ulla and King, Roger},
title = {DIRECT: A Query Facility for Multiple Databases},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130284},
doi = {10.1145/130283.130284},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {2},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130284,
author = {Merz, Ulla and King, Roger},
title = {DIRECT: A Query Facility for Multiple Databases},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130284},
doi = {10.1145/141484.130284},
journal = {SIGMOD Rec.},
month = jun,
pages = {2},
numpages = {1}
}

@inproceedings{10.1145/130283.130285,
author = {Celentano, A. and Fugini, M. G. and Pozzi, S.},
title = {Conceptual Document Browsing and Retrieval in Kabiria},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130285},
doi = {10.1145/130283.130285},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {3},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130285,
author = {Celentano, A. and Fugini, M. G. and Pozzi, S.},
title = {Conceptual Document Browsing and Retrieval in Kabiria},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130285},
doi = {10.1145/141484.130285},
journal = {SIGMOD Rec.},
month = jun,
pages = {3},
numpages = {1}
}

@inproceedings{10.1145/130283.130286,
author = {Johnson, Rowland R. and Goldner, Mandy and Lee, Mitch and McKay, Keith and Shectman, Robert and Woodruff, John},
title = {USD - a Database Management System for Scientific Research},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130286},
doi = {10.1145/130283.130286},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {4},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130286,
author = {Johnson, Rowland R. and Goldner, Mandy and Lee, Mitch and McKay, Keith and Shectman, Robert and Woodruff, John},
title = {USD - a Database Management System for Scientific Research},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130286},
doi = {10.1145/141484.130286},
journal = {SIGMOD Rec.},
month = jun,
pages = {4},
numpages = {1}
}

@inproceedings{10.1145/130283.130287,
author = {Gesmann, Michael and Grasnickel, Andreas and H\"{a}rder, Theo and H\"{u}bel, Christoph and K\"{a}fer, Wolfgang and Mitschang, Bernhard and Sch\"{o}ning, Harald},
title = {PRIMA - a Database System Supporting Dynamically Defined Composite Objects},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130287},
doi = {10.1145/130283.130287},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {5},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130287,
author = {Gesmann, Michael and Grasnickel, Andreas and H\"{a}rder, Theo and H\"{u}bel, Christoph and K\"{a}fer, Wolfgang and Mitschang, Bernhard and Sch\"{o}ning, Harald},
title = {PRIMA - a Database System Supporting Dynamically Defined Composite Objects},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130287},
doi = {10.1145/141484.130287},
journal = {SIGMOD Rec.},
month = jun,
pages = {5},
numpages = {1}
}

@inproceedings{10.1145/130283.130288,
author = {Gray, Jim},
title = {Database and Transaction Processing Benchmarks},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130288},
doi = {10.1145/130283.130288},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {6},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130288,
author = {Gray, Jim},
title = {Database and Transaction Processing Benchmarks},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130288},
doi = {10.1145/141484.130288},
journal = {SIGMOD Rec.},
month = jun,
pages = {6},
numpages = {1}
}

@inproceedings{10.1145/130283.130289,
author = {Bancilhon, Fran\c{c}ois},
title = {The O2 Object-Oriented Database System},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130289},
doi = {10.1145/130283.130289},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {7},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130289,
author = {Bancilhon, Fran\c{c}ois},
title = {The O2 Object-Oriented Database System},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130289},
doi = {10.1145/141484.130289},
journal = {SIGMOD Rec.},
month = jun,
pages = {7},
numpages = {1}
}

@inproceedings{10.1145/130283.130290,
author = {Orenstein, Jack},
title = {Architectures for Object Data Management},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130290},
doi = {10.1145/130283.130290},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {8},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130290,
author = {Orenstein, Jack},
title = {Architectures for Object Data Management},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130290},
doi = {10.1145/141484.130290},
journal = {SIGMOD Rec.},
month = jun,
pages = {8},
numpages = {1}
}

@inproceedings{10.1145/130283.130291,
author = {Ganguly, Sumit and Hasan, Waqar and Krishnamurthy, Ravi},
title = {Query Optimization for Parallel Execution},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130291},
doi = {10.1145/130283.130291},
abstract = {The decreasing cost of computing makes it economically viable to reduce the response time of decision support queries by using parallel execution to exploit inexpensive resources. This goal poses the following query optimization problem: Minimize response time subject to constraints on throughput, which we motivate as the dual of the traditional DBMS problem. We address this novel problem in the context of Select-Project-Join queries by extending the execution space, cost model and search algorithm that are widely used in commercial DBMSs. We incorporate the sources and deterrents of parallelism in the traditional execution space. We show that a cost model can predict response time while accounting for the new aspects due to parallelism. We observe that the response  time optimization metric violates a fundamental assumption in the dynamic programming algorithm that is the linchpin in the optimizers of most commercial DBMSs. We extend dynamic programming and show how optimization metrics which correctly predict response time may be designed.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {9–18},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130291,
author = {Ganguly, Sumit and Hasan, Waqar and Krishnamurthy, Ravi},
title = {Query Optimization for Parallel Execution},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130291},
doi = {10.1145/141484.130291},
abstract = {The decreasing cost of computing makes it economically viable to reduce the response time of decision support queries by using parallel execution to exploit inexpensive resources. This goal poses the following query optimization problem: Minimize response time subject to constraints on throughput, which we motivate as the dual of the traditional DBMS problem. We address this novel problem in the context of Select-Project-Join queries by extending the execution space, cost model and search algorithm that are widely used in commercial DBMSs. We incorporate the sources and deterrents of parallelism in the traditional execution space. We show that a cost model can predict response time while accounting for the new aspects due to parallelism. We observe that the response  time optimization metric violates a fundamental assumption in the dynamic programming algorithm that is the linchpin in the optimizers of most commercial DBMSs. We extend dynamic programming and show how optimization metrics which correctly predict response time may be designed.},
journal = {SIGMOD Rec.},
month = jun,
pages = {9–18},
numpages = {10}
}

@inproceedings{10.1145/130283.130292,
author = {Hong, Wei},
title = {Exploiting Inter-Operation Parallelism in XPRS},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130292},
doi = {10.1145/130283.130292},
abstract = {In this paper, we study the scheduling and optimization problems of parallel query processing using interoperation parallelism in a shared-memory environment and propose our solutions for XPRS. We first study the scheduling problem of a set of a continuous sequence of independent tasks that are either from a bushy tree plan of a single query or from the plans of multiple queries, and present a clean and simple scheduling algorithm. Our scheduling algorithm achieves maximum resource utilizations by running an IO-bound task and a CPU-bound task in parallel with carefully calculated degrees of parallelism and maintains the maximum resource utilizations by dynamically adjusting the degrees of parallelism of running tasks whenever necessary. Real performance figures are shown to confirm  the effectiveness of our scheduling algorithm. We also revisit the optimization problem of parallel execution plans of a single query and extend our previous results to consider inter-operation parallelism by introducing a new cost estimation method to the query optimizer based on our scheduling algorithm.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {19–28},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130292,
author = {Hong, Wei},
title = {Exploiting Inter-Operation Parallelism in XPRS},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130292},
doi = {10.1145/141484.130292},
abstract = {In this paper, we study the scheduling and optimization problems of parallel query processing using interoperation parallelism in a shared-memory environment and propose our solutions for XPRS. We first study the scheduling problem of a set of a continuous sequence of independent tasks that are either from a bushy tree plan of a single query or from the plans of multiple queries, and present a clean and simple scheduling algorithm. Our scheduling algorithm achieves maximum resource utilizations by running an IO-bound task and a CPU-bound task in parallel with carefully calculated degrees of parallelism and maintains the maximum resource utilizations by dynamically adjusting the degrees of parallelism of running tasks whenever necessary. Real performance figures are shown to confirm  the effectiveness of our scheduling algorithm. We also revisit the optimization problem of parallel execution plans of a single query and extend our previous results to consider inter-operation parallelism by introducing a new cost estimation method to the query optimizer based on our scheduling algorithm.},
journal = {SIGMOD Rec.},
month = jun,
pages = {19–28},
numpages = {10}
}

@inproceedings{10.1145/130283.130293,
author = {Ghandeharizadeh, Shahram and DeWitt, David J. and Qureshi, Waheed},
title = {A Performance Analysis of Alternative Multi-Attribute Declustering Strategies},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130293},
doi = {10.1145/130283.130293},
abstract = {During the past decade, parallel database systems have gained increased popularity due to their high performance, scalability and availability characteristics. With the predicted future database sizes and the complexity of queries, the scalability of these systems to hundreds and thousands of processors is essential for satisfying the projected demand. Several studies have repeatedly demonstrated that both the performance and scalability of a paralel database system is contingent on the physical layout of data across the processors of the system. If the data is not declustered properly, the execution of an operator might waste resources, reducing the overall processing capability of the system.With earlier, single attribute declustering strategies, such as those found in  Tandem, Teradata, Gamma, and Bubba parallel database systems, a selection query including a range predicate on any attribute other than the partitioning attribute must be sent to all processors containing tuples of the relation. By directing a query with minimal resource requirements to processors that contain no relevant tuples, the system wastes CPU cycles, communication bandwidth, and I/O bandwidth, reducing its overall processing capability. As a solution, several multi-attribute declustering strategies have been proposed. However, the performance of these declustering techniques have not previously been compared to one another nor with a single attribute partitioning strategy. This paper, compares the performance of Multi-Attribute GrId deClustering (MAGIC) strategy and Bubba's  Extended Range Declustering (BERD) strategy with one another and with the range partitioning strategy. Our results indicate that MAGIC outperforms both range and BERD in all experiments conducted in this study.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {29–38},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130293,
author = {Ghandeharizadeh, Shahram and DeWitt, David J. and Qureshi, Waheed},
title = {A Performance Analysis of Alternative Multi-Attribute Declustering Strategies},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130293},
doi = {10.1145/141484.130293},
abstract = {During the past decade, parallel database systems have gained increased popularity due to their high performance, scalability and availability characteristics. With the predicted future database sizes and the complexity of queries, the scalability of these systems to hundreds and thousands of processors is essential for satisfying the projected demand. Several studies have repeatedly demonstrated that both the performance and scalability of a paralel database system is contingent on the physical layout of data across the processors of the system. If the data is not declustered properly, the execution of an operator might waste resources, reducing the overall processing capability of the system.With earlier, single attribute declustering strategies, such as those found in  Tandem, Teradata, Gamma, and Bubba parallel database systems, a selection query including a range predicate on any attribute other than the partitioning attribute must be sent to all processors containing tuples of the relation. By directing a query with minimal resource requirements to processors that contain no relevant tuples, the system wastes CPU cycles, communication bandwidth, and I/O bandwidth, reducing its overall processing capability. As a solution, several multi-attribute declustering strategies have been proposed. However, the performance of these declustering techniques have not previously been compared to one another nor with a single attribute partitioning strategy. This paper, compares the performance of Multi-Attribute GrId deClustering (MAGIC) strategy and Bubba's  Extended Range Declustering (BERD) strategy with one another and with the range partitioning strategy. Our results indicate that MAGIC outperforms both range and BERD in all experiments conducted in this study.},
journal = {SIGMOD Rec.},
month = jun,
pages = {29–38},
numpages = {10}
}

@inproceedings{10.1145/130283.130294,
author = {Pirahesh, Hamid and Hellerstein, Joseph M. and Hasan, Waqar},
title = {Extensible/Rule Based Query Rewrite Optimization in Starburst},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130294},
doi = {10.1145/130283.130294},
abstract = {This paper describes the Query Rewrite facility of the Starburst extensible database system, a novel phase of query optimization. We present a suite of rewrite rules used in Starburst to transform queries into equivalent queries for faster execution, and also describe the production rule engine which is used by Starburst to choose and execute these rules. Examples are provided demonstrating that these Query Rewrite transformations lead to query execution time improvements of orders of magnitude, suggesting that Query Rewrite in general—and these rewrite rules in particular—are an essential step in query optimization for modern database systems.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {39–48},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130294,
author = {Pirahesh, Hamid and Hellerstein, Joseph M. and Hasan, Waqar},
title = {Extensible/Rule Based Query Rewrite Optimization in Starburst},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130294},
doi = {10.1145/141484.130294},
abstract = {This paper describes the Query Rewrite facility of the Starburst extensible database system, a novel phase of query optimization. We present a suite of rewrite rules used in Starburst to transform queries into equivalent queries for faster execution, and also describe the production rule engine which is used by Starburst to choose and execute these rules. Examples are provided demonstrating that these Query Rewrite transformations lead to query execution time improvements of orders of magnitude, suggesting that Query Rewrite in general—and these rewrite rules in particular—are an essential step in query optimization for modern database systems.},
journal = {SIGMOD Rec.},
month = jun,
pages = {39–48},
numpages = {10}
}

@inproceedings{10.1145/130283.130295,
author = {Hanson, Eric N.},
title = {Rule Condition Testing and Action Execution in Ariel},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130295},
doi = {10.1145/130283.130295},
abstract = {This paper describes testing of rule conditions and execution of rule actions in Ariel active DBMS. The Ariel rule system is tightly coupled with query and update processing. Ariel rules can have conditions based on a mix of patterns, events, and transitions. For testing rule conditions, Ariel makes use of a discrimination network composed of a special data structure for testing single-relation selection conditions efficiently, and a modified version of the TREAT algorithm, called A-TREAT, for testing join conditions. The key modification to TREAT (which could also be used in the Rete algorithm) is the use of virtual α-memory nodes which save storage since they contain only the predicate associated with the memory node instead of copies of data matching the predicate. The rule-action executor in Ariel binds the data matching a rule's condition to the action of the rule at rule fire time, and executes the rule action using the query processor.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {49–58},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130295,
author = {Hanson, Eric N.},
title = {Rule Condition Testing and Action Execution in Ariel},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130295},
doi = {10.1145/141484.130295},
abstract = {This paper describes testing of rule conditions and execution of rule actions in Ariel active DBMS. The Ariel rule system is tightly coupled with query and update processing. Ariel rules can have conditions based on a mix of patterns, events, and transitions. For testing rule conditions, Ariel makes use of a discrimination network composed of a special data structure for testing single-relation selection conditions efficiently, and a modified version of the TREAT algorithm, called A-TREAT, for testing join conditions. The key modification to TREAT (which could also be used in the Rete algorithm) is the use of virtual α-memory nodes which save storage since they contain only the predicate associated with the memory node instead of copies of data matching the predicate. The rule-action executor in Ariel binds the data matching a rule's condition to the action of the rule at rule fire time, and executes the rule action using the query processor.},
journal = {SIGMOD Rec.},
month = jun,
pages = {49–58},
numpages = {10}
}

@inproceedings{10.1145/130283.130296,
author = {Aiken, Alexander and Widom, Jennifer and Hellerstein, Joseph M.},
title = {Behavior of Database Production Rules: Termination, Confluence, and Observable Determinism},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130296},
doi = {10.1145/130283.130296},
abstract = {Static analysis methods are given for determining whether arbitrary sets of database production rules are (1) guaranteed to terminate; (2) guaranteed to produce a unique final database state; (3) guaranteed to produce a unique stream of observable actions. When the analysis determines that one of these properties is not guaranteed, it isolates the rules responsible for the problem and determines criteria that, if satisfied, guarantee the property. The analysis methods are presented in the context of the Starburst Rule System; they will form the basis of an interactive development environment for Starburst rule programmers.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {59–68},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130296,
author = {Aiken, Alexander and Widom, Jennifer and Hellerstein, Joseph M.},
title = {Behavior of Database Production Rules: Termination, Confluence, and Observable Determinism},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130296},
doi = {10.1145/141484.130296},
abstract = {Static analysis methods are given for determining whether arbitrary sets of database production rules are (1) guaranteed to terminate; (2) guaranteed to produce a unique final database state; (3) guaranteed to produce a unique stream of observable actions. When the analysis determines that one of these properties is not guaranteed, it isolates the rules responsible for the problem and determines criteria that, if satisfied, guarantee the property. The analysis methods are presented in the context of the Starburst Rule System; they will form the basis of an interactive development environment for Starburst rule programmers.},
journal = {SIGMOD Rec.},
month = jun,
pages = {59–68},
numpages = {10}
}

@inproceedings{10.1145/130283.130297,
author = {Wade, Andrew E.},
title = {Full Distribution in Objectivity/DB},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130297},
doi = {10.1145/130283.130297},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {69},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130297,
author = {Wade, Andrew E.},
title = {Full Distribution in Objectivity/DB},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130297},
doi = {10.1145/141484.130297},
journal = {SIGMOD Rec.},
month = jun,
pages = {69},
numpages = {1}
}

@inproceedings{10.1145/130283.130298,
author = {Barry, Douglas K.},
title = {ITASCA Distributed ODBMS},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130298},
doi = {10.1145/130283.130298},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {70},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130298,
author = {Barry, Douglas K.},
title = {ITASCA Distributed ODBMS},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130298},
doi = {10.1145/141484.130298},
journal = {SIGMOD Rec.},
month = jun,
pages = {70},
numpages = {1}
}

@inproceedings{10.1145/130283.130299,
author = {Cruz, Isabel F.},
title = {DOODLE: A Visual Language for Object-Oriented Databases},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130299},
doi = {10.1145/130283.130299},
abstract = {In this paper we introduce DOODLE, a new visual and declarative language for object-oriented databases. The main principle behind the language is that it is possible to display and query the database with arbitrary pictures. We allow the user to tailor the display of the data to suit the application at hand or her preferences. We want the user-defined visualizations to be stored in the database, and the language to express all kinds of visual manipulations. For extendibility reasons, the language is object-oriented. The semantics of the language is given by a well-known deductive query language for object-oriented databases. We hope that the formal basis of our language will contribute to the theoretical study of database visualizations and visual query languages, a subject that we believe is of great interest, but largely left unexplored.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {71–80},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130299,
author = {Cruz, Isabel F.},
title = {DOODLE: A Visual Language for Object-Oriented Databases},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130299},
doi = {10.1145/141484.130299},
abstract = {In this paper we introduce DOODLE, a new visual and declarative language for object-oriented databases. The main principle behind the language is that it is possible to display and query the database with arbitrary pictures. We allow the user to tailor the display of the data to suit the application at hand or her preferences. We want the user-defined visualizations to be stored in the database, and the language to express all kinds of visual manipulations. For extendibility reasons, the language is object-oriented. The semantics of the language is given by a well-known deductive query language for object-oriented databases. We hope that the formal basis of our language will contribute to the theoretical study of database visualizations and visual query languages, a subject that we believe is of great interest, but largely left unexplored.},
journal = {SIGMOD Rec.},
month = jun,
pages = {71–80},
numpages = {10}
}

@inproceedings{10.1145/130283.130300,
author = {Gehani, N. H. and Jagadish, H. V. and Shmueli, O.},
title = {Event Specification in an Active Object-Oriented Database},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130300},
doi = {10.1145/130283.130300},
abstract = {The concept of a trigger is central to any active database. Upon the occurrence of a trigger event, the trigger is “fired”, i.e, the trigger action is executed. We describe a model and a language for specifying basic and composite trigger events in the context of an object-oriented database. The specified events can be detected efficiently using finite automata.We integrate our model with O++, the database programming language for the ode object database being developed at AT&amp;T Bell Labs. We propose a new Event-Action model, which folds into the event specification the condition part of the well-known Event-Condition-Action model and avoids the multiple coupling modes between the event, condition, and action trigger components.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {81–90},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130300,
author = {Gehani, N. H. and Jagadish, H. V. and Shmueli, O.},
title = {Event Specification in an Active Object-Oriented Database},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130300},
doi = {10.1145/141484.130300},
abstract = {The concept of a trigger is central to any active database. Upon the occurrence of a trigger event, the trigger is “fired”, i.e, the trigger action is executed. We describe a model and a language for specifying basic and composite trigger events in the context of an object-oriented database. The specified events can be detected efficiently using finite automata.We integrate our model with O++, the database programming language for the ode object database being developed at AT&amp;T Bell Labs. We propose a new Event-Action model, which folds into the event specification the condition part of the well-known Event-Condition-Action model and avoids the multiple coupling modes between the event, condition, and action trigger components.},
journal = {SIGMOD Rec.},
month = jun,
pages = {81–90},
numpages = {10}
}

@inproceedings{10.1145/130283.130301,
author = {Lieuwen, Daniel F. and DeWitt, David J.},
title = {A Transformation-Based Approach to Optimizing Loops in Database Programming Languages},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130301},
doi = {10.1145/130283.130301},
abstract = {Database programming languages like O2, E, and O++ include the ability to iterate through a set. Nested iterators can be used to express joins. This paper describes compile-time optimizations similar to relational transformations like join reordering for such programming constructs. This paper also shows how to use a standard transformation-based optimizer to optimize these joins. An optimizer built using the EXODUS Optimizer Generator [GRAE87] was added to the Bell Labs O++ [AGRA89] compiler. We used the resulting optimizing compiler to experimentally validate the ideas in this paper. The experiments show that this technique can significantly improve the performance of database programming languages.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {91–100},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130301,
author = {Lieuwen, Daniel F. and DeWitt, David J.},
title = {A Transformation-Based Approach to Optimizing Loops in Database Programming Languages},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130301},
doi = {10.1145/141484.130301},
abstract = {Database programming languages like O2, E, and O++ include the ability to iterate through a set. Nested iterators can be used to express joins. This paper describes compile-time optimizations similar to relational transformations like join reordering for such programming constructs. This paper also shows how to use a standard transformation-based optimizer to optimize these joins. An optimizer built using the EXODUS Optimizer Generator [GRAE87] was added to the Bell Labs O++ [AGRA89] compiler. We used the resulting optimizing compiler to experimentally validate the ideas in this paper. The experiments show that this technique can significantly improve the performance of database programming languages.},
journal = {SIGMOD Rec.},
month = jun,
pages = {91–100},
numpages = {10}
}

@inproceedings{10.1145/130283.277953,
author = {Rosenthal, Arnon},
title = {What Can We Do to Strengthen the Connection between Theory and System Builders (Panel)},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.277953},
doi = {10.1145/130283.277953},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {101},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.277953,
author = {Rosenthal, Arnon},
title = {What Can We Do to Strengthen the Connection between Theory and System Builders (Panel)},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.277953},
doi = {10.1145/141484.277953},
journal = {SIGMOD Rec.},
month = jun,
pages = {101},
numpages = {1}
}

@inproceedings{10.1145/130283.130302,
author = {Kasi, Jay},
title = {High Performance and Availability through Data Distribution},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130302},
doi = {10.1145/130283.130302},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {102},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130302,
author = {Kasi, Jay},
title = {High Performance and Availability through Data Distribution},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130302},
doi = {10.1145/141484.130302},
journal = {SIGMOD Rec.},
month = jun,
pages = {102},
numpages = {1}
}

@inproceedings{10.1145/130283.130303,
author = {Davison, Wayne},
title = {Parallel Index Building in Informix OnLine 6.0},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130303},
doi = {10.1145/130283.130303},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {103},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130303,
author = {Davison, Wayne},
title = {Parallel Index Building in Informix OnLine 6.0},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130303},
doi = {10.1145/141484.130303},
journal = {SIGMOD Rec.},
month = jun,
pages = {103},
numpages = {1}
}

@inproceedings{10.1145/130283.130304,
author = {Agrawal, D. and El Abbadi, A. and Jeffers, R.},
title = {Using Delayed Commitment in Locking Protocols for Real-Time Databases},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130304},
doi = {10.1145/130283.130304},
abstract = {In this paper, we propose locking protocols that are useful for real-time databases. Our approach is motivated from two main observations. First, locking protocols are widely accepted and used in most database systems. Second, in real-time databases it has been shown that the blocking behavior of transactions in locking protocols results in performance degradation. We use a new relationship between locks called ordered sharing to eliminate blocking that arises in the traditional locking protocols. Ordered sharing eliminates blocking of read and write operations but may result in delayed commitment. Since in real-time databases, timeliness and not response time is the crucial factor, or protocols exploit this delay to allow transactions to execute within the slacks of delayed transactions. We compare the performance of the proposed protocols with the two phase locking protocol for real-time databases. Our experiments indicate that the propose protocols significantly reduce the percentage of missed deadlines in the system for a variety of workloads.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {104–113},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130304,
author = {Agrawal, D. and El Abbadi, A. and Jeffers, R.},
title = {Using Delayed Commitment in Locking Protocols for Real-Time Databases},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130304},
doi = {10.1145/141484.130304},
abstract = {In this paper, we propose locking protocols that are useful for real-time databases. Our approach is motivated from two main observations. First, locking protocols are widely accepted and used in most database systems. Second, in real-time databases it has been shown that the blocking behavior of transactions in locking protocols results in performance degradation. We use a new relationship between locks called ordered sharing to eliminate blocking that arises in the traditional locking protocols. Ordered sharing eliminates blocking of read and write operations but may result in delayed commitment. Since in real-time databases, timeliness and not response time is the crucial factor, or protocols exploit this delay to allow transactions to execute within the slacks of delayed transactions. We compare the performance of the proposed protocols with the two phase locking protocol for real-time databases. Our experiments indicate that the propose protocols significantly reduce the percentage of missed deadlines in the system for a variety of workloads.},
journal = {SIGMOD Rec.},
month = jun,
pages = {104–113},
numpages = {10}
}

@inproceedings{10.1145/130283.130305,
author = {Dan, Asit and Yu, Philip S.},
title = {Performance Analysis of Coherency Control Policies through Lock Retention},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130305},
doi = {10.1145/130283.130305},
abstract = {Buffer coherency control can be achieved through retaining a lock (shared, exclusive, etc.) on each page in the buffer, even after the requesting transaction has committed. Depending upon the lock mode held for retention and the compatibility of lock modes specified, different retention policies can be devised. In addition to tracking the validity of the buffered data granules, additional capabilities can be provided such as deferred writes to support no-force policy on commit, (node) location identification of valid granules to support remote memory accesses, and shared/exclusive lock retention to reduce the number of global lock requests for concurrency control. However, these can have serious implications not only on the performance but also on the recovery complexity. In this paper, five different integrated coherency policies are considered. We classify these policies into three different categories according to their recovery requirements. A performance study based on analytic models is provided to understand the trade-offs on both maximum throughputs and response times of the policies with a similar level of recovery complexity and the performance gain achievable through increasing the level of recovery complexity.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {114–123},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130305,
author = {Dan, Asit and Yu, Philip S.},
title = {Performance Analysis of Coherency Control Policies through Lock Retention},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130305},
doi = {10.1145/141484.130305},
abstract = {Buffer coherency control can be achieved through retaining a lock (shared, exclusive, etc.) on each page in the buffer, even after the requesting transaction has committed. Depending upon the lock mode held for retention and the compatibility of lock modes specified, different retention policies can be devised. In addition to tracking the validity of the buffered data granules, additional capabilities can be provided such as deferred writes to support no-force policy on commit, (node) location identification of valid granules to support remote memory accesses, and shared/exclusive lock retention to reduce the number of global lock requests for concurrency control. However, these can have serious implications not only on the performance but also on the recovery complexity. In this paper, five different integrated coherency policies are considered. We classify these policies into three different categories according to their recovery requirements. A performance study based on analytic models is provided to understand the trade-offs on both maximum throughputs and response times of the policies with a similar level of recovery complexity and the performance gain achievable through increasing the level of recovery complexity.},
journal = {SIGMOD Rec.},
month = jun,
pages = {114–123},
numpages = {10}
}

@inproceedings{10.1145/130283.130306,
author = {Mohan, C. and Pirahesh, Hamid and Lorie, Raymond},
title = {Efficient and Flexible Methods for Transient Versioning of Records to Avoid Locking by Read-Only Transactions},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130306},
doi = {10.1145/130283.130306},
abstract = {We present efficient and flexible methods which permit read-only transactions that do not mind reading a possibly slightly old, but still consistent, version of the data base to execute without acquiring locks. This approach avoids the undesirable interferences between such queries and the typically shorter update transactions that cause unnecessary and costly delays. Indexed access by such queries is also supported, unlike by the earlier methods. Old versions of records are maintained only in a transient fashion. Our methods are characterized by their flexibility (number of versions maintained and the timing of version switches, supporting partial rollbacks, and different recovery and buffering methods) and their efficiency (logging, garbage collection, version  selection, and incremental, record-level versioning). Distributed data base environments are also supported, including commit protocols with the read-only optimization. We also describe efficient methods for garbage collecting unneeded older versions.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {124–133},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130306,
author = {Mohan, C. and Pirahesh, Hamid and Lorie, Raymond},
title = {Efficient and Flexible Methods for Transient Versioning of Records to Avoid Locking by Read-Only Transactions},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130306},
doi = {10.1145/141484.130306},
abstract = {We present efficient and flexible methods which permit read-only transactions that do not mind reading a possibly slightly old, but still consistent, version of the data base to execute without acquiring locks. This approach avoids the undesirable interferences between such queries and the typically shorter update transactions that cause unnecessary and costly delays. Indexed access by such queries is also supported, unlike by the earlier methods. Old versions of records are maintained only in a transient fashion. Our methods are characterized by their flexibility (number of versions maintained and the timing of version switches, supporting partial rollbacks, and different recovery and buffering methods) and their efficiency (logging, garbage collection, version  selection, and incremental, record-level versioning). Distributed data base environments are also supported, including commit protocols with the read-only optimization. We also describe efficient methods for garbage collecting unneeded older versions.},
journal = {SIGMOD Rec.},
month = jun,
pages = {124–133},
numpages = {10}
}

@inproceedings{10.1145/130283.130307,
author = {Low, Chee Chin and Ooi, Beng Chin and Lu, Hongjun},
title = {H-Trees: A Dynamic Associative Search Index for OODB},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130307},
doi = {10.1145/130283.130307},
abstract = {The support of the superclass-subclass concept in object-oriented databases (OODB) makes an instance of a subclass also an instance of its superclass. As a result, the access scope of a query against a class in general includes the access scope of all its subclasses, unless specified otherwise. To support the superclass-subclass relationship efficiently, the index must achieve two objectives. First, the index must support efficient retrieval of instances from a single class. Second, it must also support efficient retrieval of instances from classes in a hierarchy of classes. In this paper, we propose a new index called the H-tree that supports efficient retrieval of instances of a single class as well as retrieval of instances of a class and its subclasses. The unique feature of H-trees is that they capture the superclass-subclass relationships. A performance analysis is conducted and both experimental and analytical results indicate that the H-tree is an efficient indexing structure for OODB.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {134–143},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130307,
author = {Low, Chee Chin and Ooi, Beng Chin and Lu, Hongjun},
title = {H-Trees: A Dynamic Associative Search Index for OODB},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130307},
doi = {10.1145/141484.130307},
abstract = {The support of the superclass-subclass concept in object-oriented databases (OODB) makes an instance of a subclass also an instance of its superclass. As a result, the access scope of a query against a class in general includes the access scope of all its subclasses, unless specified otherwise. To support the superclass-subclass relationship efficiently, the index must achieve two objectives. First, the index must support efficient retrieval of instances from a single class. Second, it must also support efficient retrieval of instances from classes in a hierarchy of classes. In this paper, we propose a new index called the H-tree that supports efficient retrieval of instances of a single class as well as retrieval of instances of a class and its subclasses. The unique feature of H-trees is that they capture the superclass-subclass relationships. A performance analysis is conducted and both experimental and analytical results indicate that the H-tree is an efficient indexing structure for OODB.},
journal = {SIGMOD Rec.},
month = jun,
pages = {134–143},
numpages = {10}
}

@inproceedings{10.1145/130283.130308,
author = {Tsangaris, Manolis M. and Naughton, Jeffrey F.},
title = {On the Performance of Object Clustering Techniques},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130308},
doi = {10.1145/130283.130308},
abstract = {We investigate the performance of some of the best-known object clustering algorithms on four different workloads based upon the tektronix benchmark. For all four workloads, stochastic clustering gave the best performance for a variety of performance metrics. Since stochastic clustering is computationally expensive, it is interesting that for every workload there was at least one cheaper clustering algorithm that matched or almost matched stochastic clustering. Unfortunately, for each workload, the algorithm that approximated stochastic clustering was different. Our experiments also demonstrated that even when the workload and object graph are fixed, the choice of the clustering algorithm depends upon the goals of the system. For example, if the goal is to perform well on traversals of  small portions of the database starting with a cold cache, the important metric is the per-traversal expansion factor, and a well-chosen placement tree will be nearly optimal; if the goal is to achieve a high steady-state performance with a reasonably large cache, the appropriate metric is the number of pages to which the clustering algorithm maps the active portion of the database. For this metric, the PRP clustering algorithm, which only uses access probabilities achieves nearly optimal performance.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {144–153},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130308,
author = {Tsangaris, Manolis M. and Naughton, Jeffrey F.},
title = {On the Performance of Object Clustering Techniques},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130308},
doi = {10.1145/141484.130308},
abstract = {We investigate the performance of some of the best-known object clustering algorithms on four different workloads based upon the tektronix benchmark. For all four workloads, stochastic clustering gave the best performance for a variety of performance metrics. Since stochastic clustering is computationally expensive, it is interesting that for every workload there was at least one cheaper clustering algorithm that matched or almost matched stochastic clustering. Unfortunately, for each workload, the algorithm that approximated stochastic clustering was different. Our experiments also demonstrated that even when the workload and object graph are fixed, the choice of the clustering algorithm depends upon the goals of the system. For example, if the goal is to perform well on traversals of  small portions of the database starting with a cold cache, the important metric is the per-traversal expansion factor, and a well-chosen placement tree will be nearly optimal; if the goal is to achieve a high steady-state performance with a reasonably large cache, the appropriate metric is the number of pages to which the clustering algorithm maps the active portion of the database. For this metric, the PRP clustering algorithm, which only uses access probabilities achieves nearly optimal performance.},
journal = {SIGMOD Rec.},
month = jun,
pages = {144–153},
numpages = {10}
}

@inproceedings{10.1145/130283.130309,
author = {Ley, Michael},
title = {The Term Retrieval Abstract Machine},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130309},
doi = {10.1145/130283.130309},
abstract = {Scans through large collections of complex objects often cannot be avoided. Even if sohphisticated indexing mechanisms are provided, it may be necessary to evaluate simple predicates against data stored on disk for filtering. For traditional record oriented data models i/o and buffer management are the main bottlenecks for this operation, the interpretation of data structures is straightforward and usually not an important cost factor. For heterogeneously shaped complex objects it may become a dominant cost factor.In this paper we demonstrate a technique to make data structure traversal inside of complex objects much cheaper than naive interpretation. We compile navigation necessary to evaluate condition predicates and physical schema information into a program to be  executed by a specialized abstract machine. Our approach is demonstrated for the Feature Term Data Model (FTDM), but the technique is applicable to many other complex data models. Main parts of this paper are dedicated to the method we used to design the Term Retrieval Abstract Machine (TRAM) architecture by partial evaluation of a tuned interpreter.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {154–163},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130309,
author = {Ley, Michael},
title = {The Term Retrieval Abstract Machine},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130309},
doi = {10.1145/141484.130309},
abstract = {Scans through large collections of complex objects often cannot be avoided. Even if sohphisticated indexing mechanisms are provided, it may be necessary to evaluate simple predicates against data stored on disk for filtering. For traditional record oriented data models i/o and buffer management are the main bottlenecks for this operation, the interpretation of data structures is straightforward and usually not an important cost factor. For heterogeneously shaped complex objects it may become a dominant cost factor.In this paper we demonstrate a technique to make data structure traversal inside of complex objects much cheaper than naive interpretation. We compile navigation necessary to evaluate condition predicates and physical schema information into a program to be  executed by a specialized abstract machine. Our approach is demonstrated for the Feature Term Data Model (FTDM), but the technique is applicable to many other complex data models. Main parts of this paper are dedicated to the method we used to design the Term Retrieval Abstract Machine (TRAM) architecture by partial evaluation of a tuned interpreter.},
journal = {SIGMOD Rec.},
month = jun,
pages = {154–163},
numpages = {10}
}

@inproceedings{10.1145/130283.130311,
author = {Bamford, Roger},
title = {Using Multiversioning to Improve Performance without Loss of Consistency (Abstract Only)},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130311},
doi = {10.1145/130283.130311},
abstract = {Oracle first implemented multiversioning, which was called read consistency, in Version 4 (ea. 1983) of the kernel. It supported the old master/new master programming paradigm for both application builders and query-processor implementors while avoiding the reader/writer serialization that results from using automatic share locks. The resulting transaction consistency model was arcane, not exactly serializable, but pragmatic in actual use. In this session, Mr. Bamford will cover read consistency from a number of viewpoints. First, the model will be described and a little detail about Oracles current implementation will be given. Next, the consequences of having read consistency as part of the databases core architecture will be discussed. In particular, the effects on the designs for row locking, query processing, distributed query, distributed cache coherence, referential integrity, and set updates will be described. This will be followed by a comparison of Oracles read consistency model to other common concurrency models, looking at cost and throughput under a variety of workloads, Finally, Mr. Barnford will propose how Oracle intends to carry read consistency into the future as support is added for complex objects, distributed computations, and long transactions.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {164},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130311,
author = {Bamford, Roger},
title = {Using Multiversioning to Improve Performance without Loss of Consistency (Abstract Only)},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130311},
doi = {10.1145/141484.130311},
abstract = {Oracle first implemented multiversioning, which was called read consistency, in Version 4 (ea. 1983) of the kernel. It supported the old master/new master programming paradigm for both application builders and query-processor implementors while avoiding the reader/writer serialization that results from using automatic share locks. The resulting transaction consistency model was arcane, not exactly serializable, but pragmatic in actual use. In this session, Mr. Bamford will cover read consistency from a number of viewpoints. First, the model will be described and a little detail about Oracles current implementation will be given. Next, the consequences of having read consistency as part of the databases core architecture will be discussed. In particular, the effects on the designs for row locking, query processing, distributed query, distributed cache coherence, referential integrity, and set updates will be described. This will be followed by a comparison of Oracles read consistency model to other common concurrency models, looking at cost and throughput under a variety of workloads, Finally, Mr. Barnford will propose how Oracle intends to carry read consistency into the future as support is added for complex objects, distributed computations, and long transactions.},
journal = {SIGMOD Rec.},
month = jun,
pages = {164},
numpages = {1}
}

@inproceedings{10.1145/130283.130310,
author = {Descollonges, Marc},
title = {A Concurrency Model for Transaction Management},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130310},
doi = {10.1145/130283.130310},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {164},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130310,
author = {Descollonges, Marc},
title = {A Concurrency Model for Transaction Management},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130310},
doi = {10.1145/141484.130310},
journal = {SIGMOD Rec.},
month = jun,
pages = {164},
numpages = {1}
}

@inproceedings{10.1145/130283.130312,
author = {Franklin, Michael J. and Zwilling, Michael J. and Tan, C. K. and Carey, Michael J. and DeWitt, David J.},
title = {Crash Recovery in Client-Server EXODUS},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130312},
doi = {10.1145/130283.130312},
abstract = {In this paper, we address the correctness and performance issues that arise when implementing logging and crash recovery in a page-server environment. The issues result from two characteristics of page-server systems: 1) the fact that data is modified and cached in client database buffers that are not accessible by the server, and 2) the performance and cost trade-offs that are inherent in a client-server environment. We describe a recovery system that we have implemented for the client-server version of the EXODUS storage manager. The implementation supports efficient buffer management policies, allows flexibility in the interaction between clients and the server, and reduces the server load by generating log records at clients. We also present a preliminary performance analysis of the implementation.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {165–174},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130312,
author = {Franklin, Michael J. and Zwilling, Michael J. and Tan, C. K. and Carey, Michael J. and DeWitt, David J.},
title = {Crash Recovery in Client-Server EXODUS},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130312},
doi = {10.1145/141484.130312},
abstract = {In this paper, we address the correctness and performance issues that arise when implementing logging and crash recovery in a page-server environment. The issues result from two characteristics of page-server systems: 1) the fact that data is modified and cached in client database buffers that are not accessible by the server, and 2) the performance and cost trade-offs that are inherent in a client-server environment. We describe a recovery system that we have implemented for the client-server version of the EXODUS storage manager. The implementation supports efficient buffer management policies, allows flexibility in the interaction between clients and the server, and reduces the server load by generating log records at clients. We also present a preliminary performance analysis of the implementation.},
journal = {SIGMOD Rec.},
month = jun,
pages = {165–174},
numpages = {10}
}

@inproceedings{10.1145/130283.130313,
author = {Jhingran, Anant and Khedkar, Pratap},
title = {Analysis of Recovery in a Database System Using a Write-Ahead Log Protocol},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130313},
doi = {10.1145/130283.130313},
abstract = {In this paper we examine the recovery time in a database system using a Write-Ahead Log protocol, such as ARIES [9], under the assumption that the buffer replacement policy is strict LRU. In particular, analytical equations for log read time, data I/O, log application, and undo processing time are presented. Our initial model assumes a read/write ratio of one, and a uniform access pattern. This is later generalized to include different read/write ratios, as well as a “hot set” model (i.e. x% of the accesses go to y% of the data). We show that in the uniform access model, recovery is dominated by data I/O costs, but under extreme hot-set conditions, this may no longer be true. Furthermore, since we derive anaytical equations, recovery can be analyzed for any set of parameter conditions not discussed here.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {175–184},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130313,
author = {Jhingran, Anant and Khedkar, Pratap},
title = {Analysis of Recovery in a Database System Using a Write-Ahead Log Protocol},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130313},
doi = {10.1145/141484.130313},
abstract = {In this paper we examine the recovery time in a database system using a Write-Ahead Log protocol, such as ARIES [9], under the assumption that the buffer replacement policy is strict LRU. In particular, analytical equations for log read time, data I/O, log application, and undo processing time are presented. Our initial model assumes a read/write ratio of one, and a uniform access pattern. This is later generalized to include different read/write ratios, as well as a “hot set” model (i.e. x% of the accesses go to y% of the data). We show that in the uniform access model, recovery is dominated by data I/O costs, but under extreme hot-set conditions, this may no longer be true. Furthermore, since we derive anaytical equations, recovery can be analyzed for any set of parameter conditions not discussed here.},
journal = {SIGMOD Rec.},
month = jun,
pages = {175–184},
numpages = {10}
}

@inproceedings{10.1145/130283.130314,
author = {Lomet, David B.},
title = {MLR: A Recovery Method for Multi-Level Systems},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130314},
doi = {10.1145/130283.130314},
abstract = {To achieve high concurrency in a database system has meant building a system that copes well with important special cases. Recent work on multi-level systems suggest a systematic path to high concurrency. A multi-level system using locks permits restrictive low level locks of a subtransaction to be replaced with less restrictive high level locks when sub-transactions commit, enhancing concurrency. This is possible because sub-transactions can be undone via high level compensation actions rather than by restoring a prior lower level state. We describe a recovery scheme, called Multi-Level Recovery (MLR) that logs this high level undo operation with the commit record for the subtransaction that it compensates, posting log records to only a single log. A variant of the method copes with  nested transactions, and both nested and multi-level transactions can be treated in a unified fashion.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {185–194},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130314,
author = {Lomet, David B.},
title = {MLR: A Recovery Method for Multi-Level Systems},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130314},
doi = {10.1145/141484.130314},
abstract = {To achieve high concurrency in a database system has meant building a system that copes well with important special cases. Recent work on multi-level systems suggest a systematic path to high concurrency. A multi-level system using locks permits restrictive low level locks of a subtransaction to be replaced with less restrictive high level locks when sub-transactions commit, enhancing concurrency. This is possible because sub-transactions can be undone via high level compensation actions rather than by restoring a prior lower level state. We describe a recovery scheme, called Multi-Level Recovery (MLR) that logs this high level undo operation with the commit record for the subtransaction that it compensates, posting log records to only a single log. A variant of the method copes with  nested transactions, and both nested and multi-level transactions can be treated in a unified fashion.},
journal = {SIGMOD Rec.},
month = jun,
pages = {185–194},
numpages = {10}
}

@inproceedings{10.1145/130283.130315,
author = {Kamel, Ibrahim and Faloutsos, Christos},
title = {Parallel R-Trees},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130315},
doi = {10.1145/130283.130315},
abstract = {We consider the problem of exploiting parallelism to accelerate the performance of spacial access methods and specifically, R-trees [11]. Our goal is to design a server for spatial data, so that to maximize the throughput of range queries. This can be achieved by (a) maximizing parallelism for large range queries, and (b) by engaging as few disks as possible on point queries [22].We propose a simple hardware architecture consisting of one processor with several disks attached to it. On this architecture, we propose to distribute the nodes of a traditonal R-tree, with cross-disk pointers (“Multiplexed” R-tree). The R-tree code is identical to the one for a single-disk R-tree, with the only addition that we have to decide which disk a newly created R-tree node should  be stored in. We propose and examine several criteria to choose a disk for a new node. The most successful one, termed “proximity index” or PI, estimates the similarity of the new node with the other R-tree nodes already on a disk, and chooses the disk with the lowest similarity. Experimental results show that our scheme consistently outperforms all the other heuristics for node-to-disk assignments, achieving up to 55% gains over the Round Robin one. Experiments also indicate that the multiplexed R-tree with PI heuristic gives better response time than the disk-stripping (=“Super-node”) approach, and imposes lighter load on the I/O sub-system.The speed up of our method is close to linear speed up, increasing with the size of the queries.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {195–204},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130315,
author = {Kamel, Ibrahim and Faloutsos, Christos},
title = {Parallel R-Trees},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130315},
doi = {10.1145/141484.130315},
abstract = {We consider the problem of exploiting parallelism to accelerate the performance of spacial access methods and specifically, R-trees [11]. Our goal is to design a server for spatial data, so that to maximize the throughput of range queries. This can be achieved by (a) maximizing parallelism for large range queries, and (b) by engaging as few disks as possible on point queries [22].We propose a simple hardware architecture consisting of one processor with several disks attached to it. On this architecture, we propose to distribute the nodes of a traditonal R-tree, with cross-disk pointers (“Multiplexed” R-tree). The R-tree code is identical to the one for a single-disk R-tree, with the only addition that we have to decide which disk a newly created R-tree node should  be stored in. We propose and examine several criteria to choose a disk for a new node. The most successful one, termed “proximity index” or PI, estimates the similarity of the new node with the other R-tree nodes already on a disk, and chooses the disk with the lowest similarity. Experimental results show that our scheme consistently outperforms all the other heuristics for node-to-disk assignments, achieving up to 55% gains over the Round Robin one. Experiments also indicate that the multiplexed R-tree with PI heuristic gives better response time than the disk-stripping (=“Super-node”) approach, and imposes lighter load on the I/O sub-system.The speed up of our method is close to linear speed up, increasing with the size of the queries.},
journal = {SIGMOD Rec.},
month = jun,
pages = {195–204},
numpages = {10}
}

@inproceedings{10.1145/130283.130316,
author = {Hoel, Erik G. and Samet, Hanan},
title = {A Qualitative Comparison Study of Data Structures for Large Line Segment Databases},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130316},
doi = {10.1145/130283.130316},
abstract = {A qualitative comparative study is performed of the performance of three popular spatial indexing methods - the R-tree, R+-tree, and the PMR quadtree-in the context of processing spatial queries in large line segment databases. The data is drawn from the TIGER/Line files used by the Bureau of the Census to deal with the road networks in the US. The goal is not to find the best data structure as this is not generally possible. Instead, their comparability is demonstrated and an indication is given as to when and why their performance differs. Tests are conducted with a number of large datasets and performance is tabulated in terms of the complexity of the disk activity in building them, their storage requirements, and the complexity of the disk activity for a number of tasks that include point and window queries, as well as finding the nearest line segment to a given point and an enclosing polygon.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {205–214},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130316,
author = {Hoel, Erik G. and Samet, Hanan},
title = {A Qualitative Comparison Study of Data Structures for Large Line Segment Databases},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130316},
doi = {10.1145/141484.130316},
abstract = {A qualitative comparative study is performed of the performance of three popular spatial indexing methods - the R-tree, R+-tree, and the PMR quadtree-in the context of processing spatial queries in large line segment databases. The data is drawn from the TIGER/Line files used by the Bureau of the Census to deal with the road networks in the US. The goal is not to find the best data structure as this is not generally possible. Instead, their comparability is demonstrated and an indication is given as to when and why their performance differs. Tests are conducted with a number of large datasets and performance is tabulated in terms of the complexity of the disk activity in building them, their storage requirements, and the complexity of the disk activity for a number of tasks that include point and window queries, as well as finding the nearest line segment to a given point and an enclosing polygon.},
journal = {SIGMOD Rec.},
month = jun,
pages = {205–214},
numpages = {10}
}

@inproceedings{10.1145/130283.130317,
author = {Analyti, Anastasia and Pramanik, Sakti},
title = {Fast Search in Main Memory Databases},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130317},
doi = {10.1145/130283.130317},
abstract = {The objective of this paper is to develop and analyze high performance hash based search methods for main memory databases. We define optimal search in main memory databases as the search that requires at most one key comparison to locate a record. Existing hashing techniques become impractical when they are adapted to yield optimal search in main memory databases because of their large directory size. Multi-directory hashing techniques can provide significantly improved directory utilization over single-directory hashing techniques. A multi-directory hashing scheme, called fast search multi-directory hashing, and its generalization, called controlled search multi-directory hashing, are presented. Both methods achieve linearly increasing expected directory size with the number of records. Their performance is compared to existing alternatives.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {215–224},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130317,
author = {Analyti, Anastasia and Pramanik, Sakti},
title = {Fast Search in Main Memory Databases},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130317},
doi = {10.1145/141484.130317},
abstract = {The objective of this paper is to develop and analyze high performance hash based search methods for main memory databases. We define optimal search in main memory databases as the search that requires at most one key comparison to locate a record. Existing hashing techniques become impractical when they are adapted to yield optimal search in main memory databases because of their large directory size. Multi-directory hashing techniques can provide significantly improved directory utilization over single-directory hashing techniques. A multi-directory hashing scheme, called fast search multi-directory hashing, and its generalization, called controlled search multi-directory hashing, are presented. Both methods achieve linearly increasing expected directory size with the number of records. Their performance is compared to existing alternatives.},
journal = {SIGMOD Rec.},
month = jun,
pages = {215–224},
numpages = {10}
}

@inproceedings{10.1145/130283.130318,
author = {Celis, Pedro},
title = {Distribution, Parallelism, and Availability in Nonstop SQL},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130318},
doi = {10.1145/130283.130318},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {225},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130318,
author = {Celis, Pedro},
title = {Distribution, Parallelism, and Availability in Nonstop SQL},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130318},
doi = {10.1145/141484.130318},
journal = {SIGMOD Rec.},
month = jun,
pages = {225},
numpages = {1}
}

@inproceedings{10.1145/130283.130319,
author = {Rabinovich, Michael and Lazowska, Edward D.},
title = {Improving Fault Tolerance and Supporting Partial Writes in Structured Coterie Protocols for Replicated Objects},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130319},
doi = {10.1145/130283.130319},
abstract = {This paper presents a new technique for efficiently controlling replicas in distributed systems. Conventional structured coterie protocols are efficient but incur a penalty of reduced availability in exchange for the performance gain. Further, the performance advantage can only be fully realized when write operations always replace the old data item with the new value instead of updating a portion of the data item. Our new approach significantly improves availability while allowing partial write operations.After presenting our general approach, we apply it to an existing structured coterie protocol and analyze the availability of the resulting protocol. We also show that other classes of protocols can make use of our approach.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {226–235},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130319,
author = {Rabinovich, Michael and Lazowska, Edward D.},
title = {Improving Fault Tolerance and Supporting Partial Writes in Structured Coterie Protocols for Replicated Objects},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130319},
doi = {10.1145/141484.130319},
abstract = {This paper presents a new technique for efficiently controlling replicas in distributed systems. Conventional structured coterie protocols are efficient but incur a penalty of reduced availability in exchange for the performance gain. Further, the performance advantage can only be fully realized when write operations always replace the old data item with the new value instead of updating a portion of the data item. Our new approach significantly improves availability while allowing partial write operations.After presenting our general approach, we apply it to an existing structured coterie protocol and analyze the availability of the resulting protocol. We also show that other classes of protocols can make use of our approach.},
journal = {SIGMOD Rec.},
month = jun,
pages = {226–235},
numpages = {10}
}

@inproceedings{10.1145/130283.130320,
author = {Bhide, Anupam and Goyal, Ambuj and Hsiao, Hui-I and Jhingran, Anant},
title = {An Efficient Scheme for Providing High Availability},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130320},
doi = {10.1145/130283.130320},
abstract = {Replication at the partition level is a promising approach for increasing availability in a Shared Nothing architecture. We propose an algorithm for maintaining replicas with little overhead during normal failure-free processing. Our mechanism updates the secondary replica in an asynchronous manner: entire dirty pages are sent to the secondary at some time before they are discarded from primary's buffer. A log server node (hardened against failures) maintains the log for each node. If a primary node fails, the secondary fetches the log from the log server, applied it to its replica, and brings itself to the primary's last transaction-consistent state. We study the performance of various policies for sending pages to secondary and the corresponding trade-offs between recovery time and overhead during failure-free processing.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {236–245},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130320,
author = {Bhide, Anupam and Goyal, Ambuj and Hsiao, Hui-I and Jhingran, Anant},
title = {An Efficient Scheme for Providing High Availability},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130320},
doi = {10.1145/141484.130320},
abstract = {Replication at the partition level is a promising approach for increasing availability in a Shared Nothing architecture. We propose an algorithm for maintaining replicas with little overhead during normal failure-free processing. Our mechanism updates the secondary replica in an asynchronous manner: entire dirty pages are sent to the secondary at some time before they are discarded from primary's buffer. A log server node (hardened against failures) maintains the log for each node. If a primary node fails, the secondary fetches the log from the log server, applied it to its replica, and brings itself to the primary's last transaction-consistent state. We study the performance of various policies for sending pages to secondary and the corresponding trade-offs between recovery time and overhead during failure-free processing.},
journal = {SIGMOD Rec.},
month = jun,
pages = {236–245},
numpages = {10}
}

@inproceedings{10.1145/130283.130321,
author = {Polyzois, Christos A. and Garcia-Molina, Hector},
title = {Evaluation of Remote Backup Algorithms for Transaction Processing Systems},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130321},
doi = {10.1145/130283.130321},
abstract = {A remote backup is a copy of a primary database maintained at a geographically separate location and is used to increase data availability. Remote backup systems are typically log-based and can be classified into 2-safe and 1-safe, depending on whether transactions commit at both sites simultaneously or they first commit at the primary and are later propagated to the backup. We have built an experimental database system on which we evaluated the performance of the epoch algorithm, a 1-safe algorithm we have developed, and compared it with the 2-safe approach under various conditions. We also report on the use of multiple log streams to propagate information from the primary to the backup.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {246–255},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130321,
author = {Polyzois, Christos A. and Garcia-Molina, Hector},
title = {Evaluation of Remote Backup Algorithms for Transaction Processing Systems},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130321},
doi = {10.1145/141484.130321},
abstract = {A remote backup is a copy of a primary database maintained at a geographically separate location and is used to increase data availability. Remote backup systems are typically log-based and can be classified into 2-safe and 1-safe, depending on whether transactions commit at both sites simultaneously or they first commit at the primary and are later propagated to the backup. We have built an experimental database system on which we evaluated the performance of the epoch algorithm, a 1-safe algorithm we have developed, and compared it with the 2-safe approach under various conditions. We also report on the use of multiple log streams to propagate information from the primary to the backup.},
journal = {SIGMOD Rec.},
month = jun,
pages = {246–255},
numpages = {10}
}

@inproceedings{10.1145/130283.130322,
author = {Lanzelotte, Rosana S. G. and Valduriez, Patrick and Za\"{\i}t, Mohamed},
title = {Optimization of Object-Oriented Recursive Queries Using Cost-Controlled Strategies},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130322},
doi = {10.1145/130283.130322},
abstract = {Object-oriented data models are being extended with recursion to gain expressive power. This complicates the optimization problem which has to deal with recursive queries on complex objects. Because unary operations invoking methods or path expressions on objects may be costly to execute, traditional heuristics for optimizing recursive queries are no longer valid. In this paper we propose a cost-based optimization method which handles object-oriented recursive queries. In particular, it is able to delay the decision of pushing selective operations through recursion until the effect of such a transformation can be measured by a cost model. The approach integrates rewriting and increases the optimization opportunities for recursive queries on objects while allowing for efficient optimization.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {256–265},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130322,
author = {Lanzelotte, Rosana S. G. and Valduriez, Patrick and Za\"{\i}t, Mohamed},
title = {Optimization of Object-Oriented Recursive Queries Using Cost-Controlled Strategies},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130322},
doi = {10.1145/141484.130322},
abstract = {Object-oriented data models are being extended with recursion to gain expressive power. This complicates the optimization problem which has to deal with recursive queries on complex objects. Because unary operations invoking methods or path expressions on objects may be costly to execute, traditional heuristics for optimizing recursive queries are no longer valid. In this paper we propose a cost-based optimization method which handles object-oriented recursive queries. In particular, it is able to delay the decision of pushing selective operations through recursion until the effect of such a transformation can be measured by a cost model. The approach integrates rewriting and increases the optimization opportunities for recursive queries on objects while allowing for efficient optimization.},
journal = {SIGMOD Rec.},
month = jun,
pages = {256–265},
numpages = {10}
}

@inproceedings{10.1145/130283.130323,
author = {K\"{a}fer, Wolfgang and Sch\"{o}ning, Harald},
title = {Realizing a Temporal Complex-Object Data Model},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130323},
doi = {10.1145/130283.130323},
abstract = {Support for temporal data continues to be a requirement posed by many applications such as VLSI design and CAD, but also in conventional applications like banking and sales. Furthermore, the strong demand for complex-object support is known as an inherent fact in design applications, and also emerges for advance “conventional” applications. Thus, new advanced database management systems should include both features, i.e. should support temporal complex-objects. In this paper, we present such a temporal complex-object data model. The central notion of our temporal complex-object data model is a time slice, representing one state of a complex object. We explain the mapping of time slices onto the complex  objects supported by the MAD model (which we use for an example of a non-temporal complex-object data model) as well as the transformation process of operations on temporal complex-objects into MAD model operations. Thereby, the basic properties of the MAD model are a prerequisite for our approach. For example, time slices can only be directly stored, if non-disjunct (i.e. over-lapping) complex objects are easily handled in the underlying complex-object data model.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {266–275},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130323,
author = {K\"{a}fer, Wolfgang and Sch\"{o}ning, Harald},
title = {Realizing a Temporal Complex-Object Data Model},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130323},
doi = {10.1145/141484.130323},
abstract = {Support for temporal data continues to be a requirement posed by many applications such as VLSI design and CAD, but also in conventional applications like banking and sales. Furthermore, the strong demand for complex-object support is known as an inherent fact in design applications, and also emerges for advance “conventional” applications. Thus, new advanced database management systems should include both features, i.e. should support temporal complex-objects. In this paper, we present such a temporal complex-object data model. The central notion of our temporal complex-object data model is a time slice, representing one state of a complex object. We explain the mapping of time slices onto the complex  objects supported by the MAD model (which we use for an example of a non-temporal complex-object data model) as well as the transformation process of operations on temporal complex-objects into MAD model operations. Thereby, the basic properties of the MAD model are a prerequisite for our approach. For example, time slices can only be directly stored, if non-disjunct (i.e. over-lapping) complex objects are easily handled in the underlying complex-object data model.},
journal = {SIGMOD Rec.},
month = jun,
pages = {266–275},
numpages = {10}
}

@inproceedings{10.1145/130283.130324,
author = {Biliris, Alexandros},
title = {The Performance of Three Database Storage Structures for Managing Large Objects},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130324},
doi = {10.1145/130283.130324},
abstract = {This study analyzes the performance of the storage structures and algorithms employed in three experimental database storage systems – EXODUS, Starburst, and EOS – for managing large unstructured general-purpose objects. All three mechanisms are segment-based in that the large object is stored in a sequence of segments, each consisting of physically continuous disk block. To analyze the algorithms we measured object creation time, sequential scan time, storage utilization in the presence of updates, and the I/O cost of random reads, inserts, and deletes.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {276–285},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130324,
author = {Biliris, Alexandros},
title = {The Performance of Three Database Storage Structures for Managing Large Objects},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130324},
doi = {10.1145/141484.130324},
abstract = {This study analyzes the performance of the storage structures and algorithms employed in three experimental database storage systems – EXODUS, Starburst, and EOS – for managing large unstructured general-purpose objects. All three mechanisms are segment-based in that the large object is stored in a sequence of segments, each consisting of physically continuous disk block. To analyze the algorithms we measured object creation time, sequential scan time, storage utilization in the presence of updates, and the I/O cost of random reads, inserts, and deletes.},
journal = {SIGMOD Rec.},
month = jun,
pages = {276–285},
numpages = {10}
}

@inproceedings{10.1145/130283.130325,
author = {Wang, Yun},
title = {Experience from a Real Life Query Optimizer},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130325},
doi = {10.1145/130283.130325},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {286},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130325,
author = {Wang, Yun},
title = {Experience from a Real Life Query Optimizer},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130325},
doi = {10.1145/141484.130325},
journal = {SIGMOD Rec.},
month = jun,
pages = {286},
numpages = {1}
}

@inproceedings{10.1145/130283.130326,
author = {Rengarajan, T. K.},
title = {Rdb/VMS Support for Multi-Media Databases},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130326},
doi = {10.1145/130283.130326},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {287},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130326,
author = {Rengarajan, T. K.},
title = {Rdb/VMS Support for Multi-Media Databases},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130326},
doi = {10.1145/141484.130326},
journal = {SIGMOD Rec.},
month = jun,
pages = {287},
numpages = {1}
}

@inproceedings{10.1145/130283.130327,
author = {Mehrotra, Sharad and Rastogi, Rajeev and Breitbart, Yuri and Korth, Henry F. and Silberschatz, Avi},
title = {The Concurrency Control Problem in Multidatabases: Characteristics and Solutions},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130327},
doi = {10.1145/130283.130327},
abstract = {A Multidatabase System (MDBS) is a collection of local database management systems, each of which may follow a different concurrency control protocol. This heterogeneity makes the task of ensuring global serializability in an MDBS environment difficult. In this paper, we reduce the problem of ensuring global serializability to the problem of ensuring serializability in a centralized database system. We identify characteristics of the concurrency control problem in an MDBS environment, and additional requirements on concurrency control schemes for ensuring global serializability. We then develop a range of concurrency control schemes that ensure global serializability in an MDBS environment, and at the same time meet the requirements. Finally, we study the tradeoffs between the complexities of the various schemes and the degree of concurrency provided by each of them.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {288–297},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130327,
author = {Mehrotra, Sharad and Rastogi, Rajeev and Breitbart, Yuri and Korth, Henry F. and Silberschatz, Avi},
title = {The Concurrency Control Problem in Multidatabases: Characteristics and Solutions},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130327},
doi = {10.1145/141484.130327},
abstract = {A Multidatabase System (MDBS) is a collection of local database management systems, each of which may follow a different concurrency control protocol. This heterogeneity makes the task of ensuring global serializability in an MDBS environment difficult. In this paper, we reduce the problem of ensuring global serializability to the problem of ensuring serializability in a centralized database system. We identify characteristics of the concurrency control problem in an MDBS environment, and additional requirements on concurrency control schemes for ensuring global serializability. We then develop a range of concurrency control schemes that ensure global serializability in an MDBS environment, and at the same time meet the requirements. Finally, we study the tradeoffs between the complexities of the various schemes and the degree of concurrency provided by each of them.},
journal = {SIGMOD Rec.},
month = jun,
pages = {288–297},
numpages = {10}
}

@inproceedings{10.1145/130283.130328,
author = {Shasha, Dennis and Simon, Eric and Valduriez, Patrick},
title = {Simple Rational Guidance for Chopping up Transactions},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130328},
doi = {10.1145/130283.130328},
abstract = {Chopping transactions into pieces is good for performance but may lead to non-serializable executions. Many researchers have reacted to this fact by either inventing new concurrency control mechanisms, weakening serializability, or both. We adopt a different approach.We assume a user who has only the degree 2 and degree 3 consistency options offered by the vast majority of conventional database systems; and knows the set of transactions that may run during a certain interval (users are likely to have such knowledge for online or real-time transactional applications).Given this information, our algorithm finds the finest partitioning of a set of transactions TranSet with the following property; if the partitioned transactions execute serializably, then TranSet executes serializably. This permits users to obtain more concurrency while preserving correctness. Besides obtaining more inter-transaction concurrency, chopping transactions in this way can enhance intra-transaction parallelism.The algorithm is inexpensive, running in O(n x (e + m)) time using a naive implementation where n is the number of edges in the conflict graph among the transactions, and m is the maximum number of accesses of any transaction. This makes it feasible to add as a tuning knob to practical systems.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {298–307},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130328,
author = {Shasha, Dennis and Simon, Eric and Valduriez, Patrick},
title = {Simple Rational Guidance for Chopping up Transactions},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130328},
doi = {10.1145/141484.130328},
abstract = {Chopping transactions into pieces is good for performance but may lead to non-serializable executions. Many researchers have reacted to this fact by either inventing new concurrency control mechanisms, weakening serializability, or both. We adopt a different approach.We assume a user who has only the degree 2 and degree 3 consistency options offered by the vast majority of conventional database systems; and knows the set of transactions that may run during a certain interval (users are likely to have such knowledge for online or real-time transactional applications).Given this information, our algorithm finds the finest partitioning of a set of transactions TranSet with the following property; if the partitioned transactions execute serializably, then TranSet executes serializably. This permits users to obtain more concurrency while preserving correctness. Besides obtaining more inter-transaction concurrency, chopping transactions in this way can enhance intra-transaction parallelism.The algorithm is inexpensive, running in O(n x (e + m)) time using a naive implementation where n is the number of edges in the conflict graph among the transactions, and m is the maximum number of accesses of any transaction. This makes it feasible to add as a tuning knob to practical systems.},
journal = {SIGMOD Rec.},
month = jun,
pages = {298–307},
numpages = {10}
}

@inproceedings{10.1145/130283.130329,
author = {Rahm, Erhard},
title = {Performance Evaluation of Extended Storage Architectures for Transaction Processing},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130329},
doi = {10.1145/130283.130329},
abstract = {The use of non-volatile semiconductor memory within an extended storage hierarchy promises significant performance improvements for transaction processing. Although page-addressable semiconductor memories like extended memory, solid-state disks and disk caches are commercially available since several years, no detailed investigation of their use for transaction processing has been performed so far. We present a comprehensive simulation study that compares the performance of these storage types and of different usage forms. The following usage forms are considered: allocation of entire log and database files in non-volatile semiconductor memory, using a so-called write buffer to perform disk writes asynchronously, and caching of database pages at intermediate storage levels (in addition to main memory caching). Simulation results will be presented for the debit-credit workload frequently used in transaction processing benchmarks.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {308–317},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130329,
author = {Rahm, Erhard},
title = {Performance Evaluation of Extended Storage Architectures for Transaction Processing},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130329},
doi = {10.1145/141484.130329},
abstract = {The use of non-volatile semiconductor memory within an extended storage hierarchy promises significant performance improvements for transaction processing. Although page-addressable semiconductor memories like extended memory, solid-state disks and disk caches are commercially available since several years, no detailed investigation of their use for transaction processing has been performed so far. We present a comprehensive simulation study that compares the performance of these storage types and of different usage forms. The following usage forms are considered: allocation of entire log and database files in non-volatile semiconductor memory, using a so-called write buffer to perform disk writes asynchronously, and caching of database pages at intermediate storage levels (in addition to main memory caching). Simulation results will be presented for the debit-credit workload frequently used in transaction processing benchmarks.},
journal = {SIGMOD Rec.},
month = jun,
pages = {308–317},
numpages = {10}
}

@inproceedings{10.1145/130283.130330,
author = {Cowley, Paula J.},
title = {Scientific Data Management: Real-World Issues and Requirements},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130330},
doi = {10.1145/130283.130330},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {318},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130330,
author = {Cowley, Paula J.},
title = {Scientific Data Management: Real-World Issues and Requirements},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130330},
doi = {10.1145/141484.130330},
journal = {SIGMOD Rec.},
month = jun,
pages = {318},
numpages = {1}
}

@inproceedings{10.1145/130283.130331,
author = {Chou, Hong-Tai},
title = {The Design and Implementation of Persistent Transactions in an Object Database System},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130331},
doi = {10.1145/130283.130331},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {319},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130331,
author = {Chou, Hong-Tai},
title = {The Design and Implementation of Persistent Transactions in an Object Database System},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130331},
doi = {10.1145/141484.130331},
journal = {SIGMOD Rec.},
month = jun,
pages = {319},
numpages = {1}
}

@inproceedings{10.1145/130283.130332,
author = {Harris, Craig and Reddy, Madhu and Woolf, Carl},
title = {A High Performance Multiversion Concurrency Control Protocol for Object Databases},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130332},
doi = {10.1145/130283.130332},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {320},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130332,
author = {Harris, Craig and Reddy, Madhu and Woolf, Carl},
title = {A High Performance Multiversion Concurrency Control Protocol for Object Databases},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130332},
doi = {10.1145/141484.130332},
journal = {SIGMOD Rec.},
month = jun,
pages = {320},
numpages = {1}
}

@inproceedings{10.1145/130283.130333,
author = {Terry, Douglas and Goldberg, David and Nichols, David and Oki, Brian},
title = {Continuous Queries over Append-Only Databases},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130333},
doi = {10.1145/130283.130333},
abstract = {In a database to which data is continually added, users may wish to issue a permanent query and be notified whenever data matches the query. If such continuous queries examine only single records, this can be implemented by examining each record as it arrives. This is very efficient because only the incoming record needs to be scanned. This simple approach does not work for queries involving joins or time. The Tapestry system allows users to issue such queries over a database of mail and bulletin board messages. The user issues a static query, such as “show me all messages that have been replied to by Jones,” as though the database were fixed and unchanging. Tapestry converts  the query into an incremental query that efficiently finds new matches to the original query as new messages are added to the database. This paper describes the techniques used in Tapestry, which do not depend on triggers and thus be implemented on any commercial database that supports SQL. Although Tapestry is designed for filtering mail and news messages, its techniques are applicable to any append-only database.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {321–330},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130333,
author = {Terry, Douglas and Goldberg, David and Nichols, David and Oki, Brian},
title = {Continuous Queries over Append-Only Databases},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130333},
doi = {10.1145/141484.130333},
abstract = {In a database to which data is continually added, users may wish to issue a permanent query and be notified whenever data matches the query. If such continuous queries examine only single records, this can be implemented by examining each record as it arrives. This is very efficient because only the incoming record needs to be scanned. This simple approach does not work for queries involving joins or time. The Tapestry system allows users to issue such queries over a database of mail and bulletin board messages. The user issues a static query, such as “show me all messages that have been replied to by Jones,” as though the database were fixed and unchanging. Tapestry converts  the query into an incremental query that efficiently finds new matches to the original query as new messages are added to the database. This paper describes the techniques used in Tapestry, which do not depend on triggers and thus be implemented on any commercial database that supports SQL. Although Tapestry is designed for filtering mail and news messages, its techniques are applicable to any append-only database.},
journal = {SIGMOD Rec.},
month = jun,
pages = {321–330},
numpages = {10}
}

@inproceedings{10.1145/130283.130334,
author = {Srinivasan, V. and Carey, Michael J.},
title = {Compensation-Based on-Line Query Processing},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130334},
doi = {10.1145/130283.130334},
abstract = {It is well known that using conventional concurrency control techniques for obtaining serializable answers to long-running queries leads to an unacceptable drop in system performance. As a result, most current DBMSs execute such queries under a reduced degree of consistency, thus providing non-serializable answers. In this paper, we present a new and highly concurrent approach for processing large decision support queries in relational databases. In this new approach, called compensation-based query processing, concurrent updates to any data participating in a query are communicated to the query's on-line query processor, which then compensates for these updates so that the final answer reflects changes caused by the updates. Very high concurrency is achieved by locking data only briefly, while still delivering transaction-consistent answers to queries.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {331–340},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130334,
author = {Srinivasan, V. and Carey, Michael J.},
title = {Compensation-Based on-Line Query Processing},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130334},
doi = {10.1145/141484.130334},
abstract = {It is well known that using conventional concurrency control techniques for obtaining serializable answers to long-running queries leads to an unacceptable drop in system performance. As a result, most current DBMSs execute such queries under a reduced degree of consistency, thus providing non-serializable answers. In this paper, we present a new and highly concurrent approach for processing large decision support queries in relational databases. In this new approach, called compensation-based query processing, concurrent updates to any data participating in a query are communicated to the query's on-line query processor, which then compensates for these updates so that the final answer reflects changes caused by the updates. Very high concurrency is achieved by locking data only briefly, while still delivering transaction-consistent answers to queries.},
journal = {SIGMOD Rec.},
month = jun,
pages = {331–340},
numpages = {10}
}

@inproceedings{10.1145/130283.130335,
author = {Haas, Peter J. and Swami, Arun N.},
title = {Sequential Sampling Procedures for Query Size Estimation},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130335},
doi = {10.1145/130283.130335},
abstract = {We provide a procedure, based on random sampling, for estimation of the size of a query result. The procedure is sequential in that sampling terminates after a random number of steps according to a stopping rule that depends upon the observations obtained so far. Enough observations are obtained so that, with a pre-specified probability, the estimate differs from the true size of the query result by no more than a prespecified amount. Unlike previous sequential estimation procedures for queries, our procedure is asymptotically efficient and requires no ad hoc pilot sample or a a priori assumptions about data characteristics. In addition to establishing the asymptotic properties of the estimation procedure, we provide techniques for reducing undercoverage at small sample sizes and show that the sampling cost of the procedure can be reduced through stratified sampling techniques.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {341–350},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130335,
author = {Haas, Peter J. and Swami, Arun N.},
title = {Sequential Sampling Procedures for Query Size Estimation},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130335},
doi = {10.1145/141484.130335},
abstract = {We provide a procedure, based on random sampling, for estimation of the size of a query result. The procedure is sequential in that sampling terminates after a random number of steps according to a stopping rule that depends upon the observations obtained so far. Enough observations are obtained so that, with a pre-specified probability, the estimate differs from the true size of the query result by no more than a prespecified amount. Unlike previous sequential estimation procedures for queries, our procedure is asymptotically efficient and requires no ad hoc pilot sample or a a priori assumptions about data characteristics. In addition to establishing the asymptotic properties of the estimation procedure, we provide techniques for reducing undercoverage at small sample sizes and show that the sampling cost of the procedure can be reduced through stratified sampling techniques.},
journal = {SIGMOD Rec.},
month = jun,
pages = {341–350},
numpages = {10}
}

@inproceedings{10.1145/130283.130336,
author = {Lomet, David and Salzberg, Betty},
title = {Access Method Concurrency with Recovery},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130336},
doi = {10.1145/130283.130336},
abstract = {Providing high concurrency in B+-trees has been studied extensively. But few efforts have been documented for combining concurrency methods with a recovery scheme that preserves well-formed trees across system crashes. We describe an approach for this that works for a class of index trees that is a generalization of the Blink-tree. A major feature of our method is that it works with a range of different recovery methods. It achieves this by decomposing structure changes in an index tree into a sequence of atomic actions, each one leaving the tree well-formed and each working on a separate level of the tree. All atomic actions on levels of the tree above the leaf level are independent of database transactions, and so are of short duration.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {351–360},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130336,
author = {Lomet, David and Salzberg, Betty},
title = {Access Method Concurrency with Recovery},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130336},
doi = {10.1145/141484.130336},
abstract = {Providing high concurrency in B+-trees has been studied extensively. But few efforts have been documented for combining concurrency methods with a recovery scheme that preserves well-formed trees across system crashes. We describe an approach for this that works for a class of index trees that is a generalization of the Blink-tree. A major feature of our method is that it works with a range of different recovery methods. It achieves this by decomposing structure changes in an index tree into a sequence of atomic actions, each one leaving the tree well-formed and each working on a separate level of the tree. All atomic actions on levels of the tree above the leaf level are independent of database transactions, and so are of short duration.},
journal = {SIGMOD Rec.},
month = jun,
pages = {351–360},
numpages = {10}
}

@inproceedings{10.1145/130283.130337,
author = {Mohan, C. and Narang, Inderpal},
title = {Algorithms for Creating Indexes for Very Large Tables without Quiescing Updates},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130337},
doi = {10.1145/130283.130337},
abstract = {As relational DBMSs become more and more popular and as organizations grow, the sizes of individual tables are increasing dramatically. Unfortunately, current DBMSs do not allow updates to be performed on a table while an index (e.g., a B+-tree) is being built for that table, thereby decreasing the systems' availability. This paper describes two algorithms in order to relax this restriction. Our emphasis has been to maximize concurrency, minimize overheads and cover all aspects of the problem. Builds of both unique and nonunique indexes are handled correctly. We also describe techniques for making the index-build operations restartable, without loss of all work, in case a system failure were to interrupt the completion of the creation of the index. In this  connection, we also present algorithms for making a long sort of operation restartable. These include algorithms for the sort and merge phases of sorting.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {361–370},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130337,
author = {Mohan, C. and Narang, Inderpal},
title = {Algorithms for Creating Indexes for Very Large Tables without Quiescing Updates},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130337},
doi = {10.1145/141484.130337},
abstract = {As relational DBMSs become more and more popular and as organizations grow, the sizes of individual tables are increasing dramatically. Unfortunately, current DBMSs do not allow updates to be performed on a table while an index (e.g., a B+-tree) is being built for that table, thereby decreasing the systems' availability. This paper describes two algorithms in order to relax this restriction. Our emphasis has been to maximize concurrency, minimize overheads and cover all aspects of the problem. Builds of both unique and nonunique indexes are handled correctly. We also describe techniques for making the index-build operations restartable, without loss of all work, in case a system failure were to interrupt the completion of the creation of the index. In this  connection, we also present algorithms for making a long sort of operation restartable. These include algorithms for the sort and merge phases of sorting.},
journal = {SIGMOD Rec.},
month = jun,
pages = {361–370},
numpages = {10}
}

@inproceedings{10.1145/130283.130338,
author = {Mohan, C. and Levine, Frank},
title = {ARIES/IM: An Efficient and High Concurrency Index Management Method Using Write-Ahead Logging},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130338},
doi = {10.1145/130283.130338},
abstract = {This paper provides a comprehensive treatment of index management in transaction systems. We present a method, called ARIESIIM (Algorithm for Recovery and Isolation Exploiting Semantics for Index Management), for concurrency control and recovery of B+-trees. ARIES/IM guarantees serializability and uses write-ahead logging for recovery. It supports very high concurrency and good performance by (1) treating as the lock of a key the same lock as the one on the corresponding record data in a data page (e.g., at the record level), (2) not acquiring, in the interest of permitting very high concurrency, commit duration locks on index pages even during index structure modification operations (SMOs) like page splits and page deletions, and (3) allowing   retrievals, inserts, and deletes to go on concurrently with SMOs. During restart recovery, any necessary redos of index changes are always performed in a page-oriented fashion (i.e., without traversing the index tree) and, during normal processing and restart recovery, whenever possible undos are performed in a page-oriented fashion. ARIES/IM permits different granularities of locking to be supported in a flexible manner. A subset of ARIES/IM has been implemented in the OS/2 Extended Edition Database Manager. Since the locking ideas of ARIES/IM have general applicability, some of them have also been implemented in SQL/DS and the VM Shared File System, even though those systems use the shadow-page technique for recovery.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {371–380},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130338,
author = {Mohan, C. and Levine, Frank},
title = {ARIES/IM: An Efficient and High Concurrency Index Management Method Using Write-Ahead Logging},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130338},
doi = {10.1145/141484.130338},
abstract = {This paper provides a comprehensive treatment of index management in transaction systems. We present a method, called ARIESIIM (Algorithm for Recovery and Isolation Exploiting Semantics for Index Management), for concurrency control and recovery of B+-trees. ARIES/IM guarantees serializability and uses write-ahead logging for recovery. It supports very high concurrency and good performance by (1) treating as the lock of a key the same lock as the one on the corresponding record data in a data page (e.g., at the record level), (2) not acquiring, in the interest of permitting very high concurrency, commit duration locks on index pages even during index structure modification operations (SMOs) like page splits and page deletions, and (3) allowing   retrievals, inserts, and deletes to go on concurrently with SMOs. During restart recovery, any necessary redos of index changes are always performed in a page-oriented fashion (i.e., without traversing the index tree) and, during normal processing and restart recovery, whenever possible undos are performed in a page-oriented fashion. ARIES/IM permits different granularities of locking to be supported in a flexible manner. A subset of ARIES/IM has been implemented in the OS/2 Extended Edition Database Manager. Since the locking ideas of ARIES/IM have general applicability, some of them have also been implemented in SQL/DS and the VM Shared File System, even though those systems use the shadow-page technique for recovery.},
journal = {SIGMOD Rec.},
month = jun,
pages = {371–380},
numpages = {10}
}

@inproceedings{10.1145/130283.130339,
author = {Carter, Fred},
title = {Extending INGRES with Methods and Triggers},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130339},
doi = {10.1145/130283.130339},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {381},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130339,
author = {Carter, Fred},
title = {Extending INGRES with Methods and Triggers},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130339},
doi = {10.1145/141484.130339},
journal = {SIGMOD Rec.},
month = jun,
pages = {381},
numpages = {1}
}

@inproceedings{10.1145/130283.130340,
author = {Bigelow, Richard},
title = {Implementation of General Constraints in SIM},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130340},
doi = {10.1145/130283.130340},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {382},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130340,
author = {Bigelow, Richard},
title = {Implementation of General Constraints in SIM},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130340},
doi = {10.1145/141484.130340},
journal = {SIGMOD Rec.},
month = jun,
pages = {382},
numpages = {1}
}

@inproceedings{10.1145/130283.130341,
author = {Cluet, Sophie and Delobel, Claude},
title = {A General Framework for the Optimization of Object-Oriented Queries},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130341},
doi = {10.1145/130283.130341},
abstract = {The goal of this work is to integrate in a general framework the different query optimization techniques that have been proposed in the object-oriented context. As a first step, we focus essentially on the logical aspect of query optimization. In this paper, we propose a formalism (i) that unifies different rewriting formalisms, (ii) that allows easy and exhaustive factorization of duplicated subqueries, and (iii) that supports heuristics in order to reduce the optimization rewriting phase.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {383–392},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130341,
author = {Cluet, Sophie and Delobel, Claude},
title = {A General Framework for the Optimization of Object-Oriented Queries},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130341},
doi = {10.1145/141484.130341},
abstract = {The goal of this work is to integrate in a general framework the different query optimization techniques that have been proposed in the object-oriented context. As a first step, we focus essentially on the logical aspect of query optimization. In this paper, we propose a formalism (i) that unifies different rewriting formalisms, (ii) that allows easy and exhaustive factorization of duplicated subqueries, and (iii) that supports heuristics in order to reduce the optimization rewriting phase.},
journal = {SIGMOD Rec.},
month = jun,
pages = {383–392},
numpages = {10}
}

@inproceedings{10.1145/130283.130342,
author = {Kifer, Michael and Kim, Won and Sagiv, Yehoshua},
title = {Querying Object-Oriented Databases},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130342},
doi = {10.1145/130283.130342},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {393–402},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130342,
author = {Kifer, Michael and Kim, Won and Sagiv, Yehoshua},
title = {Querying Object-Oriented Databases},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130342},
doi = {10.1145/141484.130342},
journal = {SIGMOD Rec.},
month = jun,
pages = {393–402},
numpages = {10}
}

@inproceedings{10.1145/130283.130343,
author = {Orenstein, Jack and Haradhvala, Sam and Margulies, Benson and Sakahara, Don},
title = {Query Processing in the ObjectStore Database System},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130343},
doi = {10.1145/130283.130343},
abstract = {ObjectStore is an object-oriented database system supporting persistence orthogonal to type, transaction management, and associative queries. Collections are provided as objects. The data model is non-1NF, as objects may have embedded collections. Queries are integrated with the host language in the form of query operators whose operands are a collection and a predicate. The predicate may itself contain a (nested) query operating on an embedded collection. Indexes on paths may be added and removed dynamically. Collections, being treated as objects, may be referred to indirectly, e.g., through a by-reference argument. For this reason and others, multiple execution strategies are generated, and a final selection is made just prior to query execution. Nested queries can result in  interleaved execution and strategy selection.},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {403–412},
numpages = {10},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130343,
author = {Orenstein, Jack and Haradhvala, Sam and Margulies, Benson and Sakahara, Don},
title = {Query Processing in the ObjectStore Database System},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130343},
doi = {10.1145/141484.130343},
abstract = {ObjectStore is an object-oriented database system supporting persistence orthogonal to type, transaction management, and associative queries. Collections are provided as objects. The data model is non-1NF, as objects may have embedded collections. Queries are integrated with the host language in the form of query operators whose operands are a collection and a predicate. The predicate may itself contain a (nested) query operating on an embedded collection. Indexes on paths may be added and removed dynamically. Collections, being treated as objects, may be referred to indirectly, e.g., through a by-reference argument. For this reason and others, multiple execution strategies are generated, and a final selection is made just prior to query execution. Nested queries can result in  interleaved execution and strategy selection.},
journal = {SIGMOD Rec.},
month = jun,
pages = {403–412},
numpages = {10}
}

@inproceedings{10.1145/130283.130344,
author = {Melmon, Paul},
title = {The Sybase Open Server},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130344},
doi = {10.1145/130283.130344},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {413},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130344,
author = {Melmon, Paul},
title = {The Sybase Open Server},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130344},
doi = {10.1145/141484.130344},
journal = {SIGMOD Rec.},
month = jun,
pages = {413},
numpages = {1}
}

@inproceedings{10.1145/130283.130345,
author = {Newmann, Scott},
title = {Multi-Vendor Interoperability through SQL Access},
year = {1992},
isbn = {0897915216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130283.130345},
doi = {10.1145/130283.130345},
booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
pages = {414},
numpages = {1},
location = {San Diego, California, USA},
series = {SIGMOD '92}
}

@article{10.1145/141484.130345,
author = {Newmann, Scott},
title = {Multi-Vendor Interoperability through SQL Access},
year = {1992},
issue_date = {June 1, 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/141484.130345},
doi = {10.1145/141484.130345},
journal = {SIGMOD Rec.},
month = jun,
pages = {414},
numpages = {1}
}

