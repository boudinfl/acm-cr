@inproceedings{10.1145/67544.66927,
author = {Salza, Silvio and Terranova, Mario},
title = {Evaluating the Size of Queries on Relational Databases with Non-Uniform Distribution and Stochastic Dependence},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66927},
doi = {10.1145/67544.66927},
abstract = {The paper deals with the problem of evaluating how the originality of the attributes of a relation, i.e. the number of distinct values in each attribute, is affected by relational operations that reduce the cardinality of the relation. This is indeed an interesting problem in research areas such as database design and query optimization. Some authors have shown that non uniform distributions and stochastic dependence significantly affect the originality of the attributes. Therefore the models that have been proposed in the literature, based on uniformity and independence assumptions, in several situation can not be conveniently utilized. In this paper we propose a probabilistic model that overcomes the need of the uniformity and independence assumptions. The model is exact for non uniform distributions when the attributes are independent, and gives approximate results when stochastic dependence is considered. In the latter case the analytical results have been compared with a simulation, and proved to be quite accurate.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {8–14},
numpages = {7},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66927,
author = {Salza, Silvio and Terranova, Mario},
title = {Evaluating the Size of Queries on Relational Databases with Non-Uniform Distribution and Stochastic Dependence},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66927},
doi = {10.1145/66926.66927},
abstract = {The paper deals with the problem of evaluating how the originality of the attributes of a relation, i.e. the number of distinct values in each attribute, is affected by relational operations that reduce the cardinality of the relation. This is indeed an interesting problem in research areas such as database design and query optimization. Some authors have shown that non uniform distributions and stochastic dependence significantly affect the originality of the attributes. Therefore the models that have been proposed in the literature, based on uniformity and independence assumptions, in several situation can not be conveniently utilized. In this paper we propose a probabilistic model that overcomes the need of the uniformity and independence assumptions. The model is exact for non uniform distributions when the attributes are independent, and gives approximate results when stochastic dependence is considered. In the latter case the analytical results have been compared with a simulation, and proved to be quite accurate.},
journal = {SIGMOD Rec.},
month = jun,
pages = {8–14},
numpages = {7}
}

@inproceedings{10.1145/67544.66928,
author = {Kolodner, Elliot and Liskov, Barbara and Weihl, William},
title = {Atomic Garbage Collection: Managing a Stable Heap},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66928},
doi = {10.1145/67544.66928},
abstract = {Modern database systems use transactions to achieve a high degree of fault-tolerance. Many modern programming languages and systems provide garbage collected heap storage, which frees the programmer from the job of explicitly deallocating storage. In this paper we describe integrated garbage collection and recovery algorithms for managing a stable heap in which accessible objects survive both system crashes and media failures.A garbage collector typically both moves and modifies objects which can lead to problems when the heap is stable because a system crash after the start of collection but before enough of the reorganized heap reaches the disk can leave the disk in an inconsistent state. Furthermore, collection has to be coordinated with the recovery system. We present a collection algorithm and recovery system that solves these problems.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {15–25},
numpages = {11},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66928,
author = {Kolodner, Elliot and Liskov, Barbara and Weihl, William},
title = {Atomic Garbage Collection: Managing a Stable Heap},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66928},
doi = {10.1145/66926.66928},
abstract = {Modern database systems use transactions to achieve a high degree of fault-tolerance. Many modern programming languages and systems provide garbage collected heap storage, which frees the programmer from the job of explicitly deallocating storage. In this paper we describe integrated garbage collection and recovery algorithms for managing a stable heap in which accessible objects survive both system crashes and media failures.A garbage collector typically both moves and modifies objects which can lead to problems when the heap is stable because a system crash after the start of collection but before enough of the reorganized heap reaches the disk can leave the disk in an inconsistent state. Furthermore, collection has to be coordinated with the recovery system. We present a collection algorithm and recovery system that solves these problems.},
journal = {SIGMOD Rec.},
month = jun,
pages = {15–25},
numpages = {11}
}

@inproceedings{10.1145/67544.66929,
author = {Dong, Guozhu},
title = {On Distributed Processibility of Datalog Queries by Decomposing Databases},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66929},
doi = {10.1145/67544.66929},
abstract = {We consider distributed or parallel processing of datalog queries. We address this issue by decomposing databases into a number of subdatabases such that the computation of a program on a database can be achieved by unioning its independent evaluations on the subdatabases. More specifically, we identify two kinds of distributed-processible programs according to the properties of database decomposition. (i) A program is disjoint distributive if it is distributed processible over a decomposition consisting of subdatabases with disjoint domains. A characterization of such programs is given in terms of an easily decidable syntactic property called connectivity. (ii) A program is bounded distributive if it is distributed processible over a decomposition consisting of subdatabases with a fixed size. Three interesting characterizations of such a program are presented, the first by bounded recursion, the second by equivalence to a 1-bounded-recursive program, and the third by constant parallel complexity},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {26–35},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66929,
author = {Dong, Guozhu},
title = {On Distributed Processibility of Datalog Queries by Decomposing Databases},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66929},
doi = {10.1145/66926.66929},
abstract = {We consider distributed or parallel processing of datalog queries. We address this issue by decomposing databases into a number of subdatabases such that the computation of a program on a database can be achieved by unioning its independent evaluations on the subdatabases. More specifically, we identify two kinds of distributed-processible programs according to the properties of database decomposition. (i) A program is disjoint distributive if it is distributed processible over a decomposition consisting of subdatabases with disjoint domains. A characterization of such programs is given in terms of an easily decidable syntactic property called connectivity. (ii) A program is bounded distributive if it is distributed processible over a decomposition consisting of subdatabases with a fixed size. Three interesting characterizations of such a program are presented, the first by bounded recursion, the second by equivalence to a 1-bounded-recursive program, and the third by constant parallel complexity},
journal = {SIGMOD Rec.},
month = jun,
pages = {26–35},
numpages = {10}
}

@inproceedings{10.1145/67544.66930,
author = {Agrawal, R. and Gehani, N. H.},
title = {ODE (Object Database and Environment): The Language and the Data Model},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66930},
doi = {10.1145/67544.66930},
abstract = {ODE is a database system and environment based on the object paradigm. It offers one integrated data model for both database and general purpose manipulation. The database is defined, queried and manipulated in the database programming language O++ which is based on C++. O++ borrows and extends the object definition facility of C++, called the class. Classes support data encapsulation and multiple inheritance. We provide facilities for creating persistent and versioned objects, defining sets, and iterating over sets and clusters of persistent objects. We also provide facilities to associate constraints and triggers with objects. This paper presents the linguistic facilities provided in O++ and the data model it supports.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {36–45},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66930,
author = {Agrawal, R. and Gehani, N. H.},
title = {ODE (Object Database and Environment): The Language and the Data Model},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66930},
doi = {10.1145/66926.66930},
abstract = {ODE is a database system and environment based on the object paradigm. It offers one integrated data model for both database and general purpose manipulation. The database is defined, queried and manipulated in the database programming language O++ which is based on C++. O++ borrows and extends the object definition facility of C++, called the class. Classes support data encapsulation and multiple inheritance. We provide facilities for creating persistent and versioned objects, defining sets, and iterating over sets and clusters of persistent objects. We also provide facilities to associate constraints and triggers with objects. This paper presents the linguistic facilities provided in O++ and the data model it supports.},
journal = {SIGMOD Rec.},
month = jun,
pages = {36–45},
numpages = {10}
}

@inproceedings{10.1145/67544.66931,
author = {Ohori, Atsushi and Buneman, Peter and Breazu-Tannen, Val},
title = {Database Programming in Machiavelli—a Polymorphic Language with Static Type Inference},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66931},
doi = {10.1145/67544.66931},
abstract = {Machiavelli is a polymorphically typed programming language in the spirit of ML, but supports an extended method of type inferencing that makes its polymorphism more general and appropriate for database applications. In particular, a function that selects a field undefined of a records is polymorphic in the sense that it can be applied to any record which contains a field undefined with the appropriate type. When combined with a set data type and database operations including join and projection, this provides a natural medium for relational database programming. Moreover, by implementing database objects as reference types and generating the appropriate views — sets of structures with “identity” — we can achieve a degree of static type checking for object-oriented databases.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {46–57},
numpages = {12},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66931,
author = {Ohori, Atsushi and Buneman, Peter and Breazu-Tannen, Val},
title = {Database Programming in Machiavelli—a Polymorphic Language with Static Type Inference},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66931},
doi = {10.1145/66926.66931},
abstract = {Machiavelli is a polymorphically typed programming language in the spirit of ML, but supports an extended method of type inferencing that makes its polymorphism more general and appropriate for database applications. In particular, a function that selects a field undefined of a records is polymorphic in the sense that it can be applied to any record which contains a field undefined with the appropriate type. When combined with a set data type and database operations including join and projection, this provides a natural medium for relational database programming. Moreover, by implementing database objects as reference types and generating the appropriate views — sets of structures with “identity” — we can achieve a degree of static type checking for object-oriented databases.},
journal = {SIGMOD Rec.},
month = jun,
pages = {46–57},
numpages = {12}
}

@inproceedings{10.1145/67544.66932,
author = {Borgida, Alexander and Brachman, Ronald J. and McGuinness, Deborah L. and Resnick, Lori Alperin},
title = {CLASSIC: A Structural Data Model for Objects},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66932},
doi = {10.1145/67544.66932},
abstract = {CLASSIC is a data model that encourages the description of objects not only in terms of their relations to other known objects, but in terms of a level of intensional structure as well. The CLASSIC language of structured descriptions permits i) partial descriptions of individuals, under an 'open world' assumption, ii) answers to queries either as extensional lists of values or as descriptions that necessarily hold of all possible answers, and iii) an easily extensible schema, which can be accessed uniformly with the data. One of the strengths of the approach is that the same language plays multiple roles in the processes of defining and populating the DB, as well as querying and answering.CLASSIC (for which we have a prototype main-memory implementation) can actively discover new information about objects from several sources: it can recognize new classes under which an object falls based on a description of the object, it can propagate some deductive consequences of DB updates, it has simple procedural recognizers, and it supports a limited form of forward-chaining rules to derive new conclusions about known objects.The kind of language of descriptions and queries presented here provides a new arena for the search for languages that are more expressive than conventional DBMS languages, but for which query processing is still tractable. This space of languages differs from the subsets of predicate calculus hitherto explored by deductive databases.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {58–67},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66932,
author = {Borgida, Alexander and Brachman, Ronald J. and McGuinness, Deborah L. and Resnick, Lori Alperin},
title = {CLASSIC: A Structural Data Model for Objects},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66932},
doi = {10.1145/66926.66932},
abstract = {CLASSIC is a data model that encourages the description of objects not only in terms of their relations to other known objects, but in terms of a level of intensional structure as well. The CLASSIC language of structured descriptions permits i) partial descriptions of individuals, under an 'open world' assumption, ii) answers to queries either as extensional lists of values or as descriptions that necessarily hold of all possible answers, and iii) an easily extensible schema, which can be accessed uniformly with the data. One of the strengths of the approach is that the same language plays multiple roles in the processes of defining and populating the DB, as well as querying and answering.CLASSIC (for which we have a prototype main-memory implementation) can actively discover new information about objects from several sources: it can recognize new classes under which an object falls based on a description of the object, it can propagate some deductive consequences of DB updates, it has simple procedural recognizers, and it supports a limited form of forward-chaining rules to derive new conclusions about known objects.The kind of language of descriptions and queries presented here provides a new arena for the search for languages that are more expressive than conventional DBMS languages, but for which query processing is still tractable. This space of languages differs from the subsets of predicate calculus hitherto explored by deductive databases.},
journal = {SIGMOD Rec.},
month = jun,
pages = {58–67},
numpages = {10}
}

@inproceedings{10.1145/67544.66933,
author = {Hou, Wen-Chi and Ozsoyoglu, Gultekin and Taneja, Baldeo K.},
title = {Processing Aggregate Relational Queries with Hard Time Constraints},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66933},
doi = {10.1145/67544.66933},
abstract = {We consider those database environments in which queries have strict timing constraints, and develop a time-constrained query evaluation methodology. For aggregate relational algebra queries, we describe a time constrained query evaluation algorithm. The algorithm, which is implemented in our prototype DBMS, iteratively samples from input relations, and evaluates the associated estimators developed in our previous work, until a stopping criterion (e.g., a time quota or a desired error range) is satisfied.To determine sample sizes at each stage of the iteration (so that the time quota will not be overspent) we need to have (a) accurate sample selectivity estimations of the RA operators in the query, (b) precise time cost formulas, and (c) good time-control strategies. To estimate the sample selectivities of RA operators, we use a runtime sample selectivity estimation and improvement approach which is flexible. For query time estimations, we use time-cost formulas which are adaptive and precise. To use the time quota efficiently, we propose statistical and heuristic time-control strategies to control the risk of overspending the time quota. Preliminary evaluation of the implemented prototype is also presented.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {68–77},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66933,
author = {Hou, Wen-Chi and Ozsoyoglu, Gultekin and Taneja, Baldeo K.},
title = {Processing Aggregate Relational Queries with Hard Time Constraints},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66933},
doi = {10.1145/66926.66933},
abstract = {We consider those database environments in which queries have strict timing constraints, and develop a time-constrained query evaluation methodology. For aggregate relational algebra queries, we describe a time constrained query evaluation algorithm. The algorithm, which is implemented in our prototype DBMS, iteratively samples from input relations, and evaluates the associated estimators developed in our previous work, until a stopping criterion (e.g., a time quota or a desired error range) is satisfied.To determine sample sizes at each stage of the iteration (so that the time quota will not be overspent) we need to have (a) accurate sample selectivity estimations of the RA operators in the query, (b) precise time cost formulas, and (c) good time-control strategies. To estimate the sample selectivities of RA operators, we use a runtime sample selectivity estimation and improvement approach which is flexible. For query time estimations, we use time-cost formulas which are adaptive and precise. To use the time quota efficiently, we propose statistical and heuristic time-control strategies to control the risk of overspending the time quota. Preliminary evaluation of the implemented prototype is also presented.},
journal = {SIGMOD Rec.},
month = jun,
pages = {68–77},
numpages = {10}
}

@inproceedings{10.1145/67544.66934,
author = {Jagadish, H. V.},
title = {Incorporating Hierarchy in a Relational Model of Data},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66934},
doi = {10.1145/67544.66934},
abstract = {We extend the relational model of data to allow classes as attribute values, thereby permitting the representation of hierarchies of objects. Inheritance, including multiple inheritance with exceptions, is clearly supported. Facts regarding classes of objects can be stored and manipulated in the same way as facts regarding object instances. Our model is upwards compatible with the standard relational model.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {78–87},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66934,
author = {Jagadish, H. V.},
title = {Incorporating Hierarchy in a Relational Model of Data},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66934},
doi = {10.1145/66926.66934},
abstract = {We extend the relational model of data to allow classes as attribute values, thereby permitting the representation of hierarchies of objects. Inheritance, including multiple inheritance with exceptions, is clearly supported. Facts regarding classes of objects can be stored and manipulated in the same way as facts regarding object instances. Our model is upwards compatible with the standard relational model.},
journal = {SIGMOD Rec.},
month = jun,
pages = {78–87},
numpages = {10}
}

@inproceedings{10.1145/67544.66935,
author = {Cammarata, Stephanie and Ramachandra, Prasadram and Shane, Darrell},
title = {Extending a Relational Database with Deferred Referential Integrity Checking and Intelligent Joins},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66935},
doi = {10.1145/67544.66935},
abstract = {Interactive use of relational database management systems (DBMS) requires a user to be knowledgeable about the semantics of the application represented in the database. In many cases, however, users are not trained in the application field and are not DBMS experts. Two categories of functionality are problematic for such users: (1) updating a database without violating integrity constraints imposed by the domain and (2) using join operations to retrieve data from more than one relation. We have been conducting research to help an uninformed or casual user interact with a relational DBMS.This paper describes two capabilities to aid an interactive database user who is neither an application specialist nor a DBMS expert. We have developed deferred Referential Integrity Checking (RIC) and Intelligent Join (IJ) which extend the operations of a relational DBMS. These facilities are made possible by explicit representation of database semantics combined with a relational schema. Deferred RIC is a static validation procedure that checks uniqueness of tuples, non-null keys, uniqueness of keys, and inclusion dependencies. IJ allows a user to identify only the “target” data which is to be retrieved without the need to additionally specify “join clauses”. In this paper we present the motivation for these facilities, describe the features of each, and present examples of their use.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {88–97},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66935,
author = {Cammarata, Stephanie and Ramachandra, Prasadram and Shane, Darrell},
title = {Extending a Relational Database with Deferred Referential Integrity Checking and Intelligent Joins},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66935},
doi = {10.1145/66926.66935},
abstract = {Interactive use of relational database management systems (DBMS) requires a user to be knowledgeable about the semantics of the application represented in the database. In many cases, however, users are not trained in the application field and are not DBMS experts. Two categories of functionality are problematic for such users: (1) updating a database without violating integrity constraints imposed by the domain and (2) using join operations to retrieve data from more than one relation. We have been conducting research to help an uninformed or casual user interact with a relational DBMS.This paper describes two capabilities to aid an interactive database user who is neither an application specialist nor a DBMS expert. We have developed deferred Referential Integrity Checking (RIC) and Intelligent Join (IJ) which extend the operations of a relational DBMS. These facilities are made possible by explicit representation of database semantics combined with a relational schema. Deferred RIC is a static validation procedure that checks uniqueness of tuples, non-null keys, uniqueness of keys, and inclusion dependencies. IJ allows a user to identify only the “target” data which is to be retrieved without the need to additionally specify “join clauses”. In this paper we present the motivation for these facilities, describe the features of each, and present examples of their use.},
journal = {SIGMOD Rec.},
month = jun,
pages = {88–97},
numpages = {10}
}

@inproceedings{10.1145/67544.66936,
author = {Copeland, George and Keller, Tom},
title = {A Comparison of High-Availability Media Recovery Techniques},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66936},
doi = {10.1145/67544.66936},
abstract = {We compare two high-availability techniques for recovery from media failures in database systems. Both techniques achieve high availability by having two copies of all data and indexes, so that recovery is immediate. “Mirrored declustering” spreads two copies of each relation across two identical sets of disks. “Interleaved declustering” spreads two copies of each relation across one set of disks while keeping both copies of each tuple on separate disks. Both techniques pay the same costs of doubling storage requirements and requiring updates to be applied to both copies.Mirroring offers greater simplicity and universality. Recovery can be implemented at lower levels of the system software (e.g., the disk controller). For architectures that do not share disks globally, it allows global and local cluster indexes to be independent. Also, mirroring does not require data to be declustered (i.e., spread over multiple disks).Interleaved declustering offers significant improvements in recovery time, mean time to loss of both copies of some data, throughput during normal operation, and response time during recovery. For all architectures, interleaved declustering enables data to be spread over twice as many disks for improved load balancing. We show how tuning for interleaved declustering is simplified because it is dependent only on a few parameters that are usually well known for a specific workload and system configuration.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {98–109},
numpages = {12},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66936,
author = {Copeland, George and Keller, Tom},
title = {A Comparison of High-Availability Media Recovery Techniques},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66936},
doi = {10.1145/66926.66936},
abstract = {We compare two high-availability techniques for recovery from media failures in database systems. Both techniques achieve high availability by having two copies of all data and indexes, so that recovery is immediate. “Mirrored declustering” spreads two copies of each relation across two identical sets of disks. “Interleaved declustering” spreads two copies of each relation across one set of disks while keeping both copies of each tuple on separate disks. Both techniques pay the same costs of doubling storage requirements and requiring updates to be applied to both copies.Mirroring offers greater simplicity and universality. Recovery can be implemented at lower levels of the system software (e.g., the disk controller). For architectures that do not share disks globally, it allows global and local cluster indexes to be independent. Also, mirroring does not require data to be declustered (i.e., spread over multiple disks).Interleaved declustering offers significant improvements in recovery time, mean time to loss of both copies of some data, throughput during normal operation, and response time during recovery. For all architectures, interleaved declustering enables data to be spread over twice as many disks for improved load balancing. We show how tuning for interleaved declustering is simplified because it is dependent only on a few parameters that are usually well known for a specific workload and system configuration.},
journal = {SIGMOD Rec.},
month = jun,
pages = {98–109},
numpages = {12}
}

@inproceedings{10.1145/67544.66937,
author = {Schneider, Donovan A. and DeWitt, David J.},
title = {A Performance Evaluation of Four Parallel Join Algorithms in a Shared-Nothing Multiprocessor Environment},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66937},
doi = {10.1145/67544.66937},
abstract = {In this paper we analyze and compare four parallel join algorithms. Grace and Hybrid hash represent the class of hash-based join methods, Simple hash represents a looping algorithm with hashing, and our last algorithm is the more traditional sort-merge. The performance of each of the algorithms with different tuple distribution policies, the addition of bit vector filters, varying amounts of main-memory for joining, and non-uniformly distributed join attribute values is studied. The Hybrid hash-join algorithm is found to be superior except when the join attribute values of the inner relation are non-uniformly distributed and memory is limited. In this case, a more conservative algorithm such as the sort-merge algorithm should be used. The Gamma database machine serves as the host for the performance comparison.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {110–121},
numpages = {12},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66937,
author = {Schneider, Donovan A. and DeWitt, David J.},
title = {A Performance Evaluation of Four Parallel Join Algorithms in a Shared-Nothing Multiprocessor Environment},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66937},
doi = {10.1145/66926.66937},
abstract = {In this paper we analyze and compare four parallel join algorithms. Grace and Hybrid hash represent the class of hash-based join methods, Simple hash represents a looping algorithm with hashing, and our last algorithm is the more traditional sort-merge. The performance of each of the algorithms with different tuple distribution policies, the addition of bit vector filters, varying amounts of main-memory for joining, and non-uniformly distributed join attribute values is studied. The Hybrid hash-join algorithm is found to be superior except when the join attribute values of the inner relation are non-uniformly distributed and memory is limited. In this case, a more conservative algorithm such as the sort-merge algorithm should be used. The Gamma database machine serves as the host for the performance comparison.},
journal = {SIGMOD Rec.},
month = jun,
pages = {110–121},
numpages = {12}
}

@inproceedings{10.1145/67544.66938,
author = {Carey, Michael J. and Livny, Miron},
title = {Parallelism and Concurrency Control Performance in Distributed Database Machines},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66938},
doi = {10.1145/67544.66938},
abstract = {While several distributed (or 'shared nothing') database machines exist in the form of prototypes or commercial products, and a number of distributed concurrency control algorithms are available, the effect of parallelism on concurrency control performance has received little attention. This paper examines the interplay between parallelism and transaction performance in a distributed database machine context. Four alternative concurrency control algorithms are considered, including two-phase locking, wound-wait, basic timestamp ordering, and optimistic concurrency control. Issues addressed include how performance scales as a function of machine size and the degree to which partitioning the database for intra-transaction parallelism improves performance for the different algorithms. We examine performance from several perspectives, including response time, throughput, and speedup, and we do so over a fairly wide range of system loads. We also examine the performance impact of certain important overhead factors (e.g., communication and process initiation costs) on the four alternative concurrency control algorithms.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {122–133},
numpages = {12},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66938,
author = {Carey, Michael J. and Livny, Miron},
title = {Parallelism and Concurrency Control Performance in Distributed Database Machines},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66938},
doi = {10.1145/66926.66938},
abstract = {While several distributed (or 'shared nothing') database machines exist in the form of prototypes or commercial products, and a number of distributed concurrency control algorithms are available, the effect of parallelism on concurrency control performance has received little attention. This paper examines the interplay between parallelism and transaction performance in a distributed database machine context. Four alternative concurrency control algorithms are considered, including two-phase locking, wound-wait, basic timestamp ordering, and optimistic concurrency control. Issues addressed include how performance scales as a function of machine size and the degree to which partitioning the database for intra-transaction parallelism improves performance for the different algorithms. We examine performance from several perspectives, including response time, throughput, and speedup, and we do so over a fairly wide range of system loads. We also examine the performance impact of certain important overhead factors (e.g., communication and process initiation costs) on the four alternative concurrency control algorithms.},
journal = {SIGMOD Rec.},
month = jun,
pages = {122–133},
numpages = {12}
}

@inproceedings{10.1145/67544.66939,
author = {Kifer, Michael and Lausen, Georg},
title = {F-Logic: A Higher-Order Language for Reasoning about Objects, Inheritance, and Scheme},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66939},
doi = {10.1145/67544.66939},
abstract = {We propose a database logic which accounts in a clean declarative fashion for most of the “object-oriented” features such as object identity, complex objects, inheritance, methods, etc. Furthermore, database schema is part of the object language, which allows the user to browse schema and data using the same declarative formalism. The proposed logic has a formal semantics and a sound and complete resolution-based proof procedure, which makes it also computationally attractive.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {134–146},
numpages = {13},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66939,
author = {Kifer, Michael and Lausen, Georg},
title = {F-Logic: A Higher-Order Language for Reasoning about Objects, Inheritance, and Scheme},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66939},
doi = {10.1145/66926.66939},
abstract = {We propose a database logic which accounts in a clean declarative fashion for most of the “object-oriented” features such as object identity, complex objects, inheritance, methods, etc. Furthermore, database schema is part of the object language, which allows the user to browse schema and data using the same declarative formalism. The proposed logic has a formal semantics and a sound and complete resolution-based proof procedure, which makes it also computationally attractive.},
journal = {SIGMOD Rec.},
month = jun,
pages = {134–146},
numpages = {13}
}

@inproceedings{10.1145/67544.66940,
author = {Hull, Richard and Su, Jianwen},
title = {On Accessing Object-Oriented Databases: Expressive Power, Complexity, and Restrictions},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66940},
doi = {10.1145/67544.66940},
abstract = {A formal framework for studying the expressive power and complexity of OODB queries is developed. Three approaches to modeling sets are articulated and compared. The class of regular OODB schemas supports the explicit representation of set-valued types. Using an object-based semantics for sets, the regular schemas correspond to most implemented OODB systems in the literature; a value-based semantics for sets is also introduced. Without restrictions, both of these approaches support the specification of all computable queries. Assuming that the new operator is prohibited, the query language of the regular OODB schemas under the object-based semantics is complete in PSPACE; and under the value-based semantics it has hyper-exponential complexity. The third approach to modeling sets is given by the algebraic OODB model, in which multi-valued attributes rather than set-valued types are supported. method implementations can use operators stemming from the relational algebra, and do not have side-effects. The query language of algebraic OODBs is more powerful than the relational algebra but has complexity bounded by PTIME. The expressive power and complexity of data access for other variations of OODBs are also considered. Finally, a new relational query language, called algebra + pointwise recursion, is introduced. This is equivalent to the algebraic OODB language, and can compute generalized transitive closure.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {147–158},
numpages = {12},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66940,
author = {Hull, Richard and Su, Jianwen},
title = {On Accessing Object-Oriented Databases: Expressive Power, Complexity, and Restrictions},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66940},
doi = {10.1145/66926.66940},
abstract = {A formal framework for studying the expressive power and complexity of OODB queries is developed. Three approaches to modeling sets are articulated and compared. The class of regular OODB schemas supports the explicit representation of set-valued types. Using an object-based semantics for sets, the regular schemas correspond to most implemented OODB systems in the literature; a value-based semantics for sets is also introduced. Without restrictions, both of these approaches support the specification of all computable queries. Assuming that the new operator is prohibited, the query language of the regular OODB schemas under the object-based semantics is complete in PSPACE; and under the value-based semantics it has hyper-exponential complexity. The third approach to modeling sets is given by the algebraic OODB model, in which multi-valued attributes rather than set-valued types are supported. method implementations can use operators stemming from the relational algebra, and do not have side-effects. The query language of algebraic OODBs is more powerful than the relational algebra but has complexity bounded by PTIME. The expressive power and complexity of data access for other variations of OODBs are also considered. Finally, a new relational query language, called algebra + pointwise recursion, is introduced. This is equivalent to the algebraic OODB language, and can compute generalized transitive closure.},
journal = {SIGMOD Rec.},
month = jun,
pages = {147–158},
numpages = {12}
}

@inproceedings{10.1145/67544.66941,
author = {Abiteboul, Serge and Kanellakis, Paris C.},
title = {Object Identity as a Query Language Primitive},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66941},
doi = {10.1145/67544.66941},
abstract = {We demonstrate the power of object identities (oid's) as a database query language primitive. We develop an object-based data model, whose structural part generalizes most of the known complex-object data models: cyclicity is allowed in both its schemas and instances. Our main contribution is the operational part of the data model, the query language IQL, which uses oid's for three critical purposes: (1) to represent data-structures with sharing and cycles, (2) to manipulate sets and (3) to express any computable database query. IQL can be statically type checked, can be evaluated bottom-up and naturally generalizes most popular rule-based database languages. The model can also be extended to incorporate type inheritance, without changes to IQL. Finally, we investigate an analogous value-based data model, whose structural part is founded on regular infinite trees and whose operational part is IQL.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {159–173},
numpages = {15},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66941,
author = {Abiteboul, Serge and Kanellakis, Paris C.},
title = {Object Identity as a Query Language Primitive},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66941},
doi = {10.1145/66926.66941},
abstract = {We demonstrate the power of object identities (oid's) as a database query language primitive. We develop an object-based data model, whose structural part generalizes most of the known complex-object data models: cyclicity is allowed in both its schemas and instances. Our main contribution is the operational part of the data model, the query language IQL, which uses oid's for three critical purposes: (1) to represent data-structures with sharing and cycles, (2) to manipulate sets and (3) to express any computable database query. IQL can be statically type checked, can be evaluated bottom-up and naturally generalizes most popular rule-based database languages. The model can also be extended to incorporate type inheritance, without changes to IQL. Finally, we investigate an analogous value-based data model, whose structural part is founded on regular infinite trees and whose operational part is IQL.},
journal = {SIGMOD Rec.},
month = jun,
pages = {159–173},
numpages = {15}
}

@inproceedings{10.1145/67544.66942,
author = {Chomicki, Jan and Imieli\'{n}ski, Tomasz},
title = {Relational Specifications of Infinite Query Answers},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66942},
doi = {10.1145/67544.66942},
abstract = {We investigate here functional deductive databases: an extension of DATALOG capable of representing infinite phenomena. Rules in functional deductive databases are Horn and predicates can have arbitrary unary and limited k-ary function symbols in one fixed position. This class is known to be decidable. However, least fixpoints of functional rules may be infinite. We present here a method to finitely represent infinite least fixpoints and infinite query answers as relational specifications. Relational specifications consist of a finite set of tuples and of a finitely specified congruence relation. Our method is applicable to every domain-independent set of functional rules.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {174–183},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66942,
author = {Chomicki, Jan and Imieli\'{n}ski, Tomasz},
title = {Relational Specifications of Infinite Query Answers},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66942},
doi = {10.1145/66926.66942},
abstract = {We investigate here functional deductive databases: an extension of DATALOG capable of representing infinite phenomena. Rules in functional deductive databases are Horn and predicates can have arbitrary unary and limited k-ary function symbols in one fixed position. This class is known to be decidable. However, least fixpoints of functional rules may be infinite. We present here a method to finitely represent infinite least fixpoints and infinite query answers as relational specifications. Relational specifications consist of a finite set of tuples and of a finitely specified congruence relation. Our method is applicable to every domain-independent set of functional rules.},
journal = {SIGMOD Rec.},
month = jun,
pages = {174–183},
numpages = {10}
}

@inproceedings{10.1145/67544.66943,
author = {Sun, Xian-He and Kamel, Nabil and Ni, Lionel M.},
title = {Solving Implication Problems in Database Applications},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66943},
doi = {10.1145/67544.66943},
abstract = {Computing queries from derived relations, optimizing queries from a group of queries, and updating materialized views are important database problems and have attracted much attention. One thing common to these problems is their demand to quickly solve the implication problem — given two predicates σQ and στ, can σQ imply στ (σQ→στ)? The implication problem has been solved by converting it into a satisfiability problem. Based on a graph representation, a detailed study of the general implication problem on its own is presented in this paper. We proved that the general implication problem, in which all six comparison operators: =, ≠, &lt;, &gt;, ≤, ≥, as well as conjunctions and disjunctions are allowed, is NP-hard. In the case when “≠” operators are not allowed in σQ and disjunctions are not allowed in στ, a polynomial time algorithm is proposed to solve this restricted implication problem. The influence of the “≠” operator and disjunctions are studied. Our theoretical results show that for some special cases the polynomial complexity algorithm can solve the implication problem which allows the “≠” operator or disjunctions in the predicates. Necessary conditions for detecting when the “≠” operator and disjunctions are allowed are also given. These results are very useful in creating heuristic methods.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {185–192},
numpages = {8},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66943,
author = {Sun, Xian-He and Kamel, Nabil and Ni, Lionel M.},
title = {Solving Implication Problems in Database Applications},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66943},
doi = {10.1145/66926.66943},
abstract = {Computing queries from derived relations, optimizing queries from a group of queries, and updating materialized views are important database problems and have attracted much attention. One thing common to these problems is their demand to quickly solve the implication problem — given two predicates σQ and στ, can σQ imply στ (σQ→στ)? The implication problem has been solved by converting it into a satisfiability problem. Based on a graph representation, a detailed study of the general implication problem on its own is presented in this paper. We proved that the general implication problem, in which all six comparison operators: =, ≠, &lt;, &gt;, ≤, ≥, as well as conjunctions and disjunctions are allowed, is NP-hard. In the case when “≠” operators are not allowed in σQ and disjunctions are not allowed in στ, a polynomial time algorithm is proposed to solve this restricted implication problem. The influence of the “≠” operator and disjunctions are studied. Our theoretical results show that for some special cases the polynomial complexity algorithm can solve the implication problem which allows the “≠” operator or disjunctions in the predicates. Necessary conditions for detecting when the “≠” operator and disjunctions are allowed are also given. These results are very useful in creating heuristic methods.},
journal = {SIGMOD Rec.},
month = jun,
pages = {185–192},
numpages = {8}
}

@inproceedings{10.1145/67544.66944,
author = {Bry, Francois},
title = {Towards an Efficient Evaluation of General Queries: Quantifier and Disjunction Processing Revisited},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66944},
doi = {10.1145/67544.66944},
abstract = {Database applications often require to evaluate queries containing quantifiers or disjunctions, e.g., for handling general integrity constraints. Existing efficient methods for processing quantifiers depart from the relational model as they rely on non-algebraic procedures. Looking at quantified query evaluation from a new angle, we propose an approach to process quantifiers that makes use of relational algebra operators only. Our approach performs in two phases. The first phase normalizes the queries producing a canonical form. This form permits to improve the translation into relational algebra performed during the second phase. The improved translation relies on a new operator - the complement-join - that generalizes the set difference, on algebraic expressions of universal quantifiers that avoid the expensive division operator in many cases, and on a special processing of disjunctions by means of constrained outer-joins. Our method achieves an efficiency at least comparable with that of previous proposals, better in most cases. Furthermore, it is considerably simpler to implement as it completely relies on relational data structures and operators.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {193–204},
numpages = {12},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66944,
author = {Bry, Francois},
title = {Towards an Efficient Evaluation of General Queries: Quantifier and Disjunction Processing Revisited},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66944},
doi = {10.1145/66926.66944},
abstract = {Database applications often require to evaluate queries containing quantifiers or disjunctions, e.g., for handling general integrity constraints. Existing efficient methods for processing quantifiers depart from the relational model as they rely on non-algebraic procedures. Looking at quantified query evaluation from a new angle, we propose an approach to process quantifiers that makes use of relational algebra operators only. Our approach performs in two phases. The first phase normalizes the queries producing a canonical form. This form permits to improve the translation into relational algebra performed during the second phase. The improved translation relies on a new operator - the complement-join - that generalizes the set difference, on algebraic expressions of universal quantifiers that avoid the expensive division operator in many cases, and on a special processing of disjunctions by means of constrained outer-joins. Our method achieves an efficiency at least comparable with that of previous proposals, better in most cases. Furthermore, it is considerably simpler to implement as it completely relies on relational data structures and operators.},
journal = {SIGMOD Rec.},
month = jun,
pages = {193–204},
numpages = {12}
}

@inproceedings{10.1145/67544.66945,
author = {Ioannidis, Yannis E. and Sellis, Timos K.},
title = {Conflict Resolution of Rules Assigning Values to Virtual Attributes},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66945},
doi = {10.1145/67544.66945},
abstract = {In the majority of research work done on logic programming and deductive databases, it is assumed that the set of rules defined by the user is consistent, i.e., that no contradictory facts can be inferred by the rules. In this paper, we address the problem of resolving conflicts of rules that assign values to virtual attributes. We devise a general framework for the study of the problem, and we propose an approach that subsumes all previously suggested solutions. Moreover, it suggests several additional solutions, which very often capture the semantics of the data more accurately than the known approaches. Finally, we address the issue of how to index rules so that conflicts are resolved efficiently, i.e., only one of the applicable rules is processed at query time.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {205–214},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66945,
author = {Ioannidis, Yannis E. and Sellis, Timos K.},
title = {Conflict Resolution of Rules Assigning Values to Virtual Attributes},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66945},
doi = {10.1145/66926.66945},
abstract = {In the majority of research work done on logic programming and deductive databases, it is assumed that the set of rules defined by the user is consistent, i.e., that no contradictory facts can be inferred by the rules. In this paper, we address the problem of resolving conflicts of rules that assign values to virtual attributes. We devise a general framework for the study of the problem, and we propose an approach that subsumes all previously suggested solutions. Moreover, it suggests several additional solutions, which very often capture the semantics of the data more accurately than the known approaches. Finally, we address the issue of how to index rules so that conflicts are resolved efficiently, i.e., only one of the applicable rules is processed at query time.},
journal = {SIGMOD Rec.},
month = jun,
pages = {205–214},
numpages = {10}
}

@inproceedings{10.1145/67544.66946,
author = {McCarthy, Dennis and Dayal, Umeshwar},
title = {The Architecture of an Active Database Management System},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66946},
doi = {10.1145/67544.66946},
abstract = {The HiPAC project is investigating active, time-constrained database management. An active DBMS is one which automatically executes specified actions when specified conditions arise. HiPAC has proposed Event-Condition-Action (ECA) rules as a formalism for active database capabilities. We have also developed an execution model that specifies how these rules are processed in the context of database transactions. The additional functionality provided by ECA rules makes new demands on the design of an active DBMS. In this paper we propose an architecture for an active DBMS that supports ECA rules. This architecture provides new forms of interaction, in support of ECA rules, between application programs and the DBMS. This leads to a new paradigm for constructing database applications.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {215–224},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66946,
author = {McCarthy, Dennis and Dayal, Umeshwar},
title = {The Architecture of an Active Database Management System},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66946},
doi = {10.1145/66926.66946},
abstract = {The HiPAC project is investigating active, time-constrained database management. An active DBMS is one which automatically executes specified actions when specified conditions arise. HiPAC has proposed Event-Condition-Action (ECA) rules as a formalism for active database capabilities. We have also developed an execution model that specifies how these rules are processed in the context of database transactions. The additional functionality provided by ECA rules makes new demands on the design of an active DBMS. In this paper we propose an architecture for an active DBMS that supports ECA rules. This architecture provides new forms of interaction, in support of ECA rules, between application programs and the DBMS. This leads to a new paradigm for constructing database applications.},
journal = {SIGMOD Rec.},
month = jun,
pages = {215–224},
numpages = {10}
}

@inproceedings{10.1145/67544.66947,
author = {Cohen, D.},
title = {Compiling Complex Database Transition Triggers},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66947},
doi = {10.1145/67544.66947},
abstract = {This paper presents a language for specifying database updates, queries and rule triggers, and describes how triggers can be compiled into an efficient mechanism. The rule language allows specification of both state and transition constraints as special cases, but is more general than either. The implementation we describe compiles rules and updates independently of each other. Thus rules can be added or deleted without recompiling any update program and vice versa.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {225–234},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66947,
author = {Cohen, D.},
title = {Compiling Complex Database Transition Triggers},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66947},
doi = {10.1145/66926.66947},
abstract = {This paper presents a language for specifying database updates, queries and rule triggers, and describes how triggers can be compiled into an efficient mechanism. The rule language allows specification of both state and transition constraints as special cases, but is more general than either. The implementation we describe compiles rules and updates independently of each other. Thus rules can be added or deleted without recompiling any update program and vice versa.},
journal = {SIGMOD Rec.},
month = jun,
pages = {225–234},
numpages = {10}
}

@inproceedings{10.1145/67544.66948,
author = {Naughton, J. F. and Ramakrishnan, R. and Sagiv, Y. and Ullman, J. D.},
title = {Efficient Evaluation of Right-, Left-, and Multi-Linear Rules},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66948},
doi = {10.1145/67544.66948},
abstract = {We present an algorithm for the efficient evaluation of a useful subset of recursive queries. Like the magic sets transformation, the algorithm consists of a rewriting phase followed by semi-naive bottom-up evaluation of the resulting rules. We prove that on a wide range of recursions, this algorithm achieves a factor of Ο(n) speedup over magic sets. Intuitively, the transformations in this algorithm achieve their performance by reducing the arity of the recursive predicates in the transformed rules.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {235–242},
numpages = {8},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66948,
author = {Naughton, J. F. and Ramakrishnan, R. and Sagiv, Y. and Ullman, J. D.},
title = {Efficient Evaluation of Right-, Left-, and Multi-Linear Rules},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66948},
doi = {10.1145/66926.66948},
abstract = {We present an algorithm for the efficient evaluation of a useful subset of recursive queries. Like the magic sets transformation, the algorithm consists of a rewriting phase followed by semi-naive bottom-up evaluation of the resulting rules. We prove that on a wide range of recursions, this algorithm achieves a factor of Ο(n) speedup over magic sets. Intuitively, the transformations in this algorithm achieve their performance by reducing the arity of the recursive predicates in the transformed rules.},
journal = {SIGMOD Rec.},
month = jun,
pages = {235–242},
numpages = {8}
}

@inproceedings{10.1145/67544.66949,
author = {Larson, P.-A. and Deshpande, V.},
title = {A File Structure Supporting Traversal Recursion},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66949},
doi = {10.1145/67544.66949},
abstract = {Traversal recursion is a class of recursive queries where the evaluation of the query involves traversal of a graph or a tree. This limited type of recursion arises in many applications. In this report we investigate a simple file structure that efficiently supports traversal recursion over large, acyclic graphs. The nodes of the graph are sorted in topological order and stored in a B-tree. Hence, traversal of the graph can be done in a single scan. Nodes and edges can also be inserted, deleted, and modified efficiently.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {243–252},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66949,
author = {Larson, P.-A. and Deshpande, V.},
title = {A File Structure Supporting Traversal Recursion},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66949},
doi = {10.1145/66926.66949},
abstract = {Traversal recursion is a class of recursive queries where the evaluation of the query involves traversal of a graph or a tree. This limited type of recursion arises in many applications. In this report we investigate a simple file structure that efficiently supports traversal recursion over large, acyclic graphs. The nodes of the graph are sorted in topological order and stored in a B-tree. Hence, traversal of the graph can be done in a single scan. Nodes and edges can also be inserted, deleted, and modified efficiently.},
journal = {SIGMOD Rec.},
month = jun,
pages = {243–252},
numpages = {10}
}

@inproceedings{10.1145/67544.66950,
author = {Agrawal, R. and Borgida, A. and Jagadish, H. V.},
title = {Efficient Management of Transitive Relationships in Large Data and Knowledge Bases},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66950},
doi = {10.1145/67544.66950},
abstract = {We argue that accessing the transitive closure of relationships is an important component of both databases and knowledge representation systems in Artificial Intelligence. The demands for efficient access and management of large relationships motivate the need for explicitly storing the transitive closure in a compressed and local way, while allowing updates to the base relation to be propagated incrementally. We present a transitive closure compression technique, based on labeling spanning trees with numeric intervals, and provide both analytical and empirical evidence of its efficacy, including a proof of optimality.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {253–262},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66950,
author = {Agrawal, R. and Borgida, A. and Jagadish, H. V.},
title = {Efficient Management of Transitive Relationships in Large Data and Knowledge Bases},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66950},
doi = {10.1145/66926.66950},
abstract = {We argue that accessing the transitive closure of relationships is an important component of both databases and knowledge representation systems in Artificial Intelligence. The demands for efficient access and management of large relationships motivate the need for explicitly storing the transitive closure in a compressed and local way, while allowing updates to the base relation to be propagated incrementally. We present a transitive closure compression technique, based on labeling spanning trees with numeric intervals, and provide both analytical and empirical evidence of its efficacy, including a proof of optimality.},
journal = {SIGMOD Rec.},
month = jun,
pages = {253–262},
numpages = {10}
}

@inproceedings{10.1145/67544.66951,
author = {Gyssens, M. and Paredaens, J. and van Gucht, D.},
title = {A Grammar-Based Approach towards Unifying Hierarchical Data Models},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66951},
doi = {10.1145/67544.66951},
abstract = {A simple model for representing the hierarchical structure of information is proposed. This model, called the grammatical model, is based on trees that are generated by grammars; the grammars describe the hierarchy of the information represented by the trees. Two transformation languages, an algebra and a calculus, are presented and shown to be equally expressive.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {263–272},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66951,
author = {Gyssens, M. and Paredaens, J. and van Gucht, D.},
title = {A Grammar-Based Approach towards Unifying Hierarchical Data Models},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66951},
doi = {10.1145/66926.66951},
abstract = {A simple model for representing the hierarchical structure of information is proposed. This model, called the grammatical model, is based on trees that are generated by grammars; the grammars describe the hierarchy of the information represented by the trees. Two transformation languages, an algebra and a calculus, are presented and shown to be equally expressive.},
journal = {SIGMOD Rec.},
month = jun,
pages = {263–272},
numpages = {10}
}

@inproceedings{10.1145/67544.66952,
author = {Colby, Latha S.},
title = {A Recursive Algebra and Query Optimization for Nested Relations},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66952},
doi = {10.1145/67544.66952},
abstract = {The nested relational model provides a better way to represent complex objects than the (flat) relational model, by allowing relations to have relation-valued attributes. A recursive algebra for nested relations that allows tuples at all levels of nesting in a nested relation to be accessed and modified without any special navigational operators and without having to flatten the nested relation has been developed. In this algebra, the operators of the nested relational algebra are extended with recursive definitions so that they can be applied not only to relations but also to subrelations of a relation. In this paper, we show that queries are more efficient and succinct when expressed in the recursive algebra than in languages that require restructuring in order to access subrelations of relations. We also show that most of the query optimization techniques that have been developed for the relational algebra can be easily extended for the recursive algebra and that queries are more easily optimizable when expressed in the recursive algebra than when they are expressed in languages like the non-recursive algebra.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {273–283},
numpages = {11},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66952,
author = {Colby, Latha S.},
title = {A Recursive Algebra and Query Optimization for Nested Relations},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66952},
doi = {10.1145/66926.66952},
abstract = {The nested relational model provides a better way to represent complex objects than the (flat) relational model, by allowing relations to have relation-valued attributes. A recursive algebra for nested relations that allows tuples at all levels of nesting in a nested relation to be accessed and modified without any special navigational operators and without having to flatten the nested relation has been developed. In this algebra, the operators of the nested relational algebra are extended with recursive definitions so that they can be applied not only to relations but also to subrelations of a relation. In this paper, we show that queries are more efficient and succinct when expressed in the recursive algebra than in languages that require restructuring in order to access subrelations of relations. We also show that most of the query optimization techniques that have been developed for the relational algebra can be easily extended for the recursive algebra and that queries are more easily optimizable when expressed in the recursive algebra than when they are expressed in languages like the non-recursive algebra.},
journal = {SIGMOD Rec.},
month = jun,
pages = {273–283},
numpages = {11}
}

@inproceedings{10.1145/67544.66953,
author = {Tansel, A. U. and Garnett, L.},
title = {Nested Historical Relations},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66953},
doi = {10.1145/67544.66953},
abstract = {The paper extends nested relations for managing temporal variation of complex objects. It combines the research in temporal databases and nested relations for nontraditional database applications. The basic modelling construct is a temporal atom as an attribute value. A temporal atom consists of two components, a value and temporal set which is a set of times denoting the validity period of the value. We define algebra operations for nested historical relations. Data redundancy in nested historical relations is also discussed and criteria for well-structured nested relations are established.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {284–294},
numpages = {11},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66953,
author = {Tansel, A. U. and Garnett, L.},
title = {Nested Historical Relations},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66953},
doi = {10.1145/66926.66953},
abstract = {The paper extends nested relations for managing temporal variation of complex objects. It combines the research in temporal databases and nested relations for nontraditional database applications. The basic modelling construct is a temporal atom as an attribute value. A temporal atom consists of two components, a value and temporal set which is a set of times denoting the validity period of the value. We define algebra operations for nested historical relations. Data redundancy in nested historical relations is also discussed and criteria for well-structured nested relations are established.},
journal = {SIGMOD Rec.},
month = jun,
pages = {284–294},
numpages = {11}
}

@inproceedings{10.1145/67544.66954,
author = {Orenstein, J. A.},
title = {Redundancy in Spatial Databases},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66954},
doi = {10.1145/67544.66954},
abstract = {Spatial objects other than points and boxes can be stored in spatial indexes, but the techniques usually require the use of approximations that can be arbitrarily bad. This leads to poor performance and highly inaccurate responses to spatial queries. The situation can be improved by storing some objects in the index redundantly. Most spatial indexes permit no flexibility in adjusting the amount of redundancy. Spatial indexes based on z-order permit this flexibility. Accuracy of the query response increases with redundancy, (there is a “diminishing return” effect). Search time, as measured by disk accesses first decreases and then increases with redundancy. There is, therefore, an optimal amount of redundancy (for a given data set). The optimal use of redundancy for z-order is explored through analysis of the z-order search algorithm and through experiments.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {295–305},
numpages = {11},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66954,
author = {Orenstein, J. A.},
title = {Redundancy in Spatial Databases},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66954},
doi = {10.1145/66926.66954},
abstract = {Spatial objects other than points and boxes can be stored in spatial indexes, but the techniques usually require the use of approximations that can be arbitrarily bad. This leads to poor performance and highly inaccurate responses to spatial queries. The situation can be improved by storing some objects in the index redundantly. Most spatial indexes permit no flexibility in adjusting the amount of redundancy. Spatial indexes based on z-order permit this flexibility. Accuracy of the query response increases with redundancy, (there is a “diminishing return” effect). Search time, as measured by disk accesses first decreases and then increases with redundancy. There is, therefore, an optimal amount of redundancy (for a given data set). The optimal use of redundancy for z-order is explored through analysis of the z-order search algorithm and through experiments.},
journal = {SIGMOD Rec.},
month = jun,
pages = {295–305},
numpages = {11}
}

@inproceedings{10.1145/67544.66955,
author = {Christodoulakis, Stavros and Ford, Daniel Alexander},
title = {Retrieval Performance versus Disc Space Utilization on WORM Optical Discs},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66955},
doi = {10.1145/67544.66955},
abstract = {Steady progress in the development of optical disc technology over the past decade has brought it to the point where it is beginning to compete directly with magnetic disc technology. WORM optical discs in particular, which permanently register information on the disc surface, have significant advantages over magnetic technology for applications that are mainly archival in nature but require the ability to do frequent on-line insertions.In this paper, we propose a class of access methods that use rewritable storage for the temporary buffering of insertions to data sets stored on WORM optical discs and we examine the relationship between the retrieval performance from WORM optical discs and the utilization of disc storage space when one of these organizations is employed. We describe the performance trade off as one of fast sequential retrieval of the contents of a block versus wasted space owing to data replication. A model of a specific instance of such an organization (a buffered hash file scheme) is described that allows for the specification of retrieval performance objectives. Alternative strategies for managing data replication that allow trade offs between higher consumption rates and better average retrieval performance are also described. We then provide an expected value analysis of the amount of disc space that must be consumed on a WORM disc to meet specified performance limits. The analysis is general enough to allow easy extension to other types of buffered files systems for WORM optical discs.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {306–314},
numpages = {9},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66955,
author = {Christodoulakis, Stavros and Ford, Daniel Alexander},
title = {Retrieval Performance versus Disc Space Utilization on WORM Optical Discs},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66955},
doi = {10.1145/66926.66955},
abstract = {Steady progress in the development of optical disc technology over the past decade has brought it to the point where it is beginning to compete directly with magnetic disc technology. WORM optical discs in particular, which permanently register information on the disc surface, have significant advantages over magnetic technology for applications that are mainly archival in nature but require the ability to do frequent on-line insertions.In this paper, we propose a class of access methods that use rewritable storage for the temporary buffering of insertions to data sets stored on WORM optical discs and we examine the relationship between the retrieval performance from WORM optical discs and the utilization of disc storage space when one of these organizations is employed. We describe the performance trade off as one of fast sequential retrieval of the contents of a block versus wasted space owing to data replication. A model of a specific instance of such an organization (a buffered hash file scheme) is described that allows for the specification of retrieval performance objectives. Alternative strategies for managing data replication that allow trade offs between higher consumption rates and better average retrieval performance are also described. We then provide an expected value analysis of the amount of disc space that must be consumed on a WORM disc to meet specified performance limits. The analysis is general enough to allow easy extension to other types of buffered files systems for WORM optical discs.},
journal = {SIGMOD Rec.},
month = jun,
pages = {306–314},
numpages = {9}
}

@inproceedings{10.1145/67544.66956,
author = {Lomet, David and Salzberg, Betty},
title = {Access Methods for Multiversion Data},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66956},
doi = {10.1145/67544.66956},
abstract = {We present an access method designed to provide a single integrated index structure for a versioned timestamped database with a non-deletion policy. Historical data (superceded versions) is stored separately from current data. Our access method is called the Time-Split B-tree. It is an index structure based on Malcolm Easton's Write Once B-tree.The Write Once B-tree was developed for data stored entirely on a Write-Once Read-Many or WORM optical disk. The Time-Split B-tree differs from the Write Once B-tree in the following ways:
Current data must be stored on an erasable random-access device.Historical data may be stored on any random-access device, including WORMs, erasable optical disks, and magnetic disks. The point is to use a faster and more expensive device for the current data and a slower cheaper device for the historical data.The splitting policies have been changed to reduce redundancy in the structure—the option of pure key splits as in B+-trees and a choice of split times for time-based splits enable this performance enhancement.When data is migrated from the current to the historical database, it is consolidated and appended to the end of the historical database, allowing for high space utilization in WORM disk sectors.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {315–324},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66956,
author = {Lomet, David and Salzberg, Betty},
title = {Access Methods for Multiversion Data},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66956},
doi = {10.1145/66926.66956},
abstract = {We present an access method designed to provide a single integrated index structure for a versioned timestamped database with a non-deletion policy. Historical data (superceded versions) is stored separately from current data. Our access method is called the Time-Split B-tree. It is an index structure based on Malcolm Easton's Write Once B-tree.The Write Once B-tree was developed for data stored entirely on a Write-Once Read-Many or WORM optical disk. The Time-Split B-tree differs from the Write Once B-tree in the following ways:
Current data must be stored on an erasable random-access device.Historical data may be stored on any random-access device, including WORMs, erasable optical disks, and magnetic disks. The point is to use a faster and more expensive device for the current data and a slower cheaper device for the historical data.The splitting policies have been changed to reduce redundancy in the structure—the option of pure key splits as in B+-trees and a choice of split times for time-based splits enable this performance enhancement.When data is migrated from the current to the historical database, it is consolidated and appended to the end of the historical database, allowing for high space utilization in WORM disk sectors.},
journal = {SIGMOD Rec.},
month = jun,
pages = {315–324},
numpages = {10}
}

@inproceedings{10.1145/67544.66957,
author = {Shekita, Eugene J. and Carey, Michael J.},
title = {Performance Enhancement through Replication in an Object-Oriented DBMS},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66957},
doi = {10.1145/67544.66957},
abstract = {In this paper we describe how replicated data can be used to speedup query processing in an object-oriented database system. The general idea is to use replicated data to eliminate some of the functional joins that would otherwise be required for query processing. We refer to our technique for replicating data as field replication because it allows individual data fields to be selectively replicated. In the paper we describe how field replication can be specified at the data model level and we present storage-level mechanisms to efficiently support it. We also present an analytical cost model to give some feel for how beneficial field replication can be and the circumstances under which it breaks down. While field replication is a relatively simple notion, the analysis shows that it can provide significant performance gains in many situations.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {325–336},
numpages = {12},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66957,
author = {Shekita, Eugene J. and Carey, Michael J.},
title = {Performance Enhancement through Replication in an Object-Oriented DBMS},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66957},
doi = {10.1145/66926.66957},
abstract = {In this paper we describe how replicated data can be used to speedup query processing in an object-oriented database system. The general idea is to use replicated data to eliminate some of the functional joins that would otherwise be required for query processing. We refer to our technique for replicating data as field replication because it allows individual data fields to be selectively replicated. In the paper we describe how field replication can be specified at the data model level and we present storage-level mechanisms to efficiently support it. We also present an analytical cost model to give some feel for how beneficial field replication can be and the circumstances under which it breaks down. While field replication is a relatively simple notion, the analysis shows that it can provide significant performance gains in many situations.},
journal = {SIGMOD Rec.},
month = jun,
pages = {325–336},
numpages = {12}
}

@inproceedings{10.1145/67544.66958,
author = {Kim, Won and Bertino, Elisa and Garza, Jorge F.},
title = {Composite Objects Revisited},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66958},
doi = {10.1145/67544.66958},
abstract = {In object-oriented systems, an object may recursively reference any number of other objects. The references, however, do not capture any special relationships between objects. An important semantic relationship which may be superimposed on a reference is the IS-PART-OF relationship between a pair of objects. A set of objects related by the IS-PART-OF relationship is collectively called a composite object.An earlier paper [KIM87b] presented a model of composite objects which has been implemented in the ORION object-oriented database system at MCC. Although the composite-object feature has been found quite useful, the model suffers from a number of serious shortcomings, primarily because it overloads a number of orthogonal semantics on the references. In this paper, first we present a more general model of composite objects which does not suffer from these shortcomings. Further, [KIM87b] made an important contribution by exploring the use of composite objects as a unit for versions, physical clustering, and concurrency control. The extended model of composite objects necessitates non-trivial changes to the results of [KIM87b]. This paper describes the new results on the use of composite objects as a unit of not only versions, physical clustering and concurrency control, but also authorization.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {337–347},
numpages = {11},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66958,
author = {Kim, Won and Bertino, Elisa and Garza, Jorge F.},
title = {Composite Objects Revisited},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66958},
doi = {10.1145/66926.66958},
abstract = {In object-oriented systems, an object may recursively reference any number of other objects. The references, however, do not capture any special relationships between objects. An important semantic relationship which may be superimposed on a reference is the IS-PART-OF relationship between a pair of objects. A set of objects related by the IS-PART-OF relationship is collectively called a composite object.An earlier paper [KIM87b] presented a model of composite objects which has been implemented in the ORION object-oriented database system at MCC. Although the composite-object feature has been found quite useful, the model suffers from a number of serious shortcomings, primarily because it overloads a number of orthogonal semantics on the references. In this paper, first we present a more general model of composite objects which does not suffer from these shortcomings. Further, [KIM87b] made an important contribution by exploring the use of composite objects as a unit for versions, physical clustering, and concurrency control. The extended model of composite objects necessitates non-trivial changes to the results of [KIM87b]. This paper describes the new results on the use of composite objects as a unit of not only versions, physical clustering and concurrency control, but also authorization.},
journal = {SIGMOD Rec.},
month = jun,
pages = {337–347},
numpages = {11}
}

@inproceedings{10.1145/67544.66959,
author = {Chang, E. E. and Katz, R. H.},
title = {Exploiting Inheritance and Structure Semantics for Effective Clustering and Buffering in an Object-Oriented DBMS},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66959},
doi = {10.1145/67544.66959},
abstract = {Object-oriented databases provide new kinds of data semantics in terms of inheritance and structural relationships. This paper examines how to use these additional semantics to obtain more effective object buffering and clustering. We use the information collected from real-world object-oriented applications, the Berkeley CAD Group's OCT design tools, as the basis for a simulation model with which to investigate alternative buffering and clustering strategies. Observing from our measurements that real CAD applications exhibit high data read to write ratios, we propose a run-time clustering algorithm whose initial evaluation indicates that system response time can be improved by a factor of 200% when the read/write ratio is high. We have also found it useful to limit the amount of I/O allowed to the clustering algorithm as it examines candidate pages for clustering at run-time. Basically, there is little performance distinction between limiting reclustering to a few I/Os or many, so a low limit on I/O appears to be acceptable. We also examine, under a variety of workload assumptions, context-sensitive buffer replacement policies with alternative prefetching policies.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {348–357},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66959,
author = {Chang, E. E. and Katz, R. H.},
title = {Exploiting Inheritance and Structure Semantics for Effective Clustering and Buffering in an Object-Oriented DBMS},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66959},
doi = {10.1145/66926.66959},
abstract = {Object-oriented databases provide new kinds of data semantics in terms of inheritance and structural relationships. This paper examines how to use these additional semantics to obtain more effective object buffering and clustering. We use the information collected from real-world object-oriented applications, the Berkeley CAD Group's OCT design tools, as the basis for a simulation model with which to investigate alternative buffering and clustering strategies. Observing from our measurements that real CAD applications exhibit high data read to write ratios, we propose a run-time clustering algorithm whose initial evaluation indicates that system response time can be improved by a factor of 200% when the read/write ratio is high. We have also found it useful to limit the amount of I/O allowed to the clustering algorithm as it examines candidate pages for clustering at run-time. Basically, there is little performance distinction between limiting reclustering to a few I/Os or many, so a low limit on I/O appears to be acceptable. We also examine, under a variety of workload assumptions, context-sensitive buffer replacement policies with alternative prefetching policies.},
journal = {SIGMOD Rec.},
month = jun,
pages = {348–357},
numpages = {10}
}

@inproceedings{10.1145/67544.66960,
author = {Graefe, G. and Ward, K.},
title = {Dynamic Query Evaluation Plans},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66960},
doi = {10.1145/67544.66960},
abstract = {In most database systems, a query embedded in a program written in a conventional programming language is optimized when the program is compiled. The query optimizer must make assumptions about the values of the program variables that appear as constants in the query, the resources that can be committed to query evaluation, and the data in the database. The optimality of the resulting query evaluation plan depends on the validity of these assumptions. If a query evaluation plan is used repeatedly over an extended period of time, it is important to determine when reoptimization is necessary. Our work aims at developing criteria when reoptimization is required, how these criteria can be implemented efficiently, and how reoptimization can be avoided by using a new technique called dynamic query evaluation plans. We experimentally demonstrate the need for dynamic plans and outline modifications to the EXODUS optimizer generator required for creating dynamic query evaluation plans.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {358–366},
numpages = {9},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66960,
author = {Graefe, G. and Ward, K.},
title = {Dynamic Query Evaluation Plans},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66960},
doi = {10.1145/66926.66960},
abstract = {In most database systems, a query embedded in a program written in a conventional programming language is optimized when the program is compiled. The query optimizer must make assumptions about the values of the program variables that appear as constants in the query, the resources that can be committed to query evaluation, and the data in the database. The optimality of the resulting query evaluation plan depends on the validity of these assumptions. If a query evaluation plan is used repeatedly over an extended period of time, it is important to determine when reoptimization is necessary. Our work aims at developing criteria when reoptimization is required, how these criteria can be implemented efficiently, and how reoptimization can be avoided by using a new technique called dynamic query evaluation plans. We experimentally demonstrate the need for dynamic plans and outline modifications to the EXODUS optimizer generator required for creating dynamic query evaluation plans.},
journal = {SIGMOD Rec.},
month = jun,
pages = {358–366},
numpages = {9}
}

@inproceedings{10.1145/67544.66961,
author = {Swami, A.},
title = {Optimization of Large Join Queries: Combining Heuristics and Combinatorial Techniques},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66961},
doi = {10.1145/67544.66961},
abstract = {We investigate the use of heuristics in optimizing queries with a large number of joins. Examples of such heuristics are the augmentation and local improvement heuristics described in this paper and a heuristic proposed by Krishnamurthy et al. We also study the combination of these heuristics with two general combinatorial optimization techniques, iterative improvement and simulated annealing, that were studied in a previous paper. Several interesting combinations are experimentally compared. For completeness, we also include simple iterative improvement and simulated annealing in our experimental comparisons. We find that two combinations of the augmentation heuristic and iterative improvement perform the best under most conditions. The results are validated using two different cost models and several different synthetic benchmarks.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {367–376},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66961,
author = {Swami, A.},
title = {Optimization of Large Join Queries: Combining Heuristics and Combinatorial Techniques},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66961},
doi = {10.1145/66926.66961},
abstract = {We investigate the use of heuristics in optimizing queries with a large number of joins. Examples of such heuristics are the augmentation and local improvement heuristics described in this paper and a heuristic proposed by Krishnamurthy et al. We also study the combination of these heuristics with two general combinatorial optimization techniques, iterative improvement and simulated annealing, that were studied in a previous paper. Several interesting combinations are experimentally compared. For completeness, we also include simple iterative improvement and simulated annealing in our experimental comparisons. We find that two combinations of the augmentation heuristic and iterative improvement perform the best under most conditions. The results are validated using two different cost models and several different synthetic benchmarks.},
journal = {SIGMOD Rec.},
month = jun,
pages = {367–376},
numpages = {10}
}

@inproceedings{10.1145/67544.66962,
author = {Haas, L. M. and Freytag, J. C. and Lohman, G. M. and Pirahesh, H.},
title = {Extensible Query Processing in Starburst},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66962},
doi = {10.1145/67544.66962},
abstract = {Today's DBMSs are unable to support the increasing demands of the various applications that would like to use a DBMS. Each kind of application poses new requirements for the DBMS. The Starburst project at IBM's Almaden Research Center aims to extend relational DBMS technology to bridge this gap between applications and the DBMS. While providing a full function relational system to enable sharing across applications, Starburst will also allow (sophisticated) programmers to add many kinds of extensions to the base system's capabilities, including language extensions (e.g., new datatypes and operations), data management extensions (e.g., new access and storage methods) and internal processing extensions (e.g., new join methods and new query transformations). To support these features, the database query language processor must be very powerful and highly extensible. Starburst's language processor features a powerful query language, rule-based optimization and query rewrite, and an execution system based on an extended relational algebra. In this paper, we describe the design of Starburst's query language processor and discuss the ways in which the language processor can be extended to achieve Starburst's goals.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {377–388},
numpages = {12},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66962,
author = {Haas, L. M. and Freytag, J. C. and Lohman, G. M. and Pirahesh, H.},
title = {Extensible Query Processing in Starburst},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66962},
doi = {10.1145/66926.66962},
abstract = {Today's DBMSs are unable to support the increasing demands of the various applications that would like to use a DBMS. Each kind of application poses new requirements for the DBMS. The Starburst project at IBM's Almaden Research Center aims to extend relational DBMS technology to bridge this gap between applications and the DBMS. While providing a full function relational system to enable sharing across applications, Starburst will also allow (sophisticated) programmers to add many kinds of extensions to the base system's capabilities, including language extensions (e.g., new datatypes and operations), data management extensions (e.g., new access and storage methods) and internal processing extensions (e.g., new join methods and new query transformations). To support these features, the database query language processor must be very powerful and highly extensible. Starburst's language processor features a powerful query language, rule-based optimization and query rewrite, and an execution system based on an extended relational algebra. In this paper, we describe the design of Starburst's query language processor and discuss the ways in which the language processor can be extended to achieve Starburst's goals.},
journal = {SIGMOD Rec.},
month = jun,
pages = {377–388},
numpages = {12}
}

@inproceedings{10.1145/67544.67809,
author = {Tang, T. and Natarajan, N.},
title = {A Static Pessimistic Scheme for Handling Replicated Databases},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.67809},
doi = {10.1145/67544.67809},
abstract = {A replicated database system may partition into isolated groups in the presence of node and link failures. When the system has partitioned, a pessimistic scheme maintains availability and consistency of replicated data by ensuring that updates occur in at most one group. A pessimistic scheme is called a static scheme if these distinguished groups are determined only by the membership of different groups in the partitioned system. In this paper, we present a new static scheme that is more powerful than voting. In this scheme, the set of distinguished groups, called an acceptance set, is chosen at design time. To commit an update, a node checks if its enclosing group is a member of this acceptance set. Using an encoding scheme for groups, this check is implemented very efficiently. Another merit of the proposed scheme is that the problem of determining an optimal acceptance set is formulated as a sparse 0-1 linear programming problem. Hence, the optimization problem can be handled using the very rich class of existing techniques for solving such problems. Based on our experiments, we feel that this optimization approach is feasible for systems containing up to 10 nodes (copies).},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {389–398},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.67809,
author = {Tang, T. and Natarajan, N.},
title = {A Static Pessimistic Scheme for Handling Replicated Databases},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.67809},
doi = {10.1145/66926.67809},
abstract = {A replicated database system may partition into isolated groups in the presence of node and link failures. When the system has partitioned, a pessimistic scheme maintains availability and consistency of replicated data by ensuring that updates occur in at most one group. A pessimistic scheme is called a static scheme if these distinguished groups are determined only by the membership of different groups in the partitioned system. In this paper, we present a new static scheme that is more powerful than voting. In this scheme, the set of distinguished groups, called an acceptance set, is chosen at design time. To commit an update, a node checks if its enclosing group is a member of this acceptance set. Using an encoding scheme for groups, this check is implemented very efficiently. Another merit of the proposed scheme is that the problem of determining an optimal acceptance set is formulated as a sparse 0-1 linear programming problem. Hence, the optimization problem can be handled using the very rich class of existing techniques for solving such problems. Based on our experiments, we feel that this optimization approach is feasible for systems containing up to 10 nodes (copies).},
journal = {SIGMOD Rec.},
month = jun,
pages = {389–398},
numpages = {10}
}

@inproceedings{10.1145/67544.66963,
author = {Ellis, C. A. and Gibbs, S. J.},
title = {Concurrency Control in Groupware Systems},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66963},
doi = {10.1145/67544.66963},
abstract = {Groupware systems are computer-based systems that support two or more users engaged in a common task, and that provide an interface to a shared environment. These systems frequently require fine-granularity sharing of data and fast response times. This paper distinguishes real-time groupware systems from other multi-user systems and discusses their concurrency control requirements. An algorithm for concurrency control in real-time groupware systems is then presented. The advantages of this algorithm are its simplicity of use and its responsiveness: users can operate directly on the data without obtaining locks. The algorithm must know some semantics of the operations. However the algorithm's overall structure is independent of the semantic information, allowing the algorithm to be adapted to many situations. An example application of the algorithm to group text editing is given, along with a sketch of its proof of correctness in this particular case. We note that the behavior desired in many of these systems is non-serializable.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {399–407},
numpages = {9},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66963,
author = {Ellis, C. A. and Gibbs, S. J.},
title = {Concurrency Control in Groupware Systems},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66963},
doi = {10.1145/66926.66963},
abstract = {Groupware systems are computer-based systems that support two or more users engaged in a common task, and that provide an interface to a shared environment. These systems frequently require fine-granularity sharing of data and fast response times. This paper distinguishes real-time groupware systems from other multi-user systems and discusses their concurrency control requirements. An algorithm for concurrency control in real-time groupware systems is then presented. The advantages of this algorithm are its simplicity of use and its responsiveness: users can operate directly on the data without obtaining locks. The algorithm must know some semantics of the operations. However the algorithm's overall structure is independent of the semantic information, allowing the algorithm to be adapted to many situations. An example application of the algorithm to group text editing is given, along with a sketch of its proof of correctness in this particular case. We note that the behavior desired in many of these systems is non-serializable.},
journal = {SIGMOD Rec.},
month = jun,
pages = {399–407},
numpages = {9}
}

@inproceedings{10.1145/67544.66964,
author = {Agrawal, D. and Sengupta, S.},
title = {Modular Synchronization in Multiversion Databases: Version Control and Concurrency Control},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66964},
doi = {10.1145/67544.66964},
abstract = {In this paper we propose a version control mechanism that enhances the modularity and extensibility of multiversion concurrency control algorithms. We decouple the multiversion algorithms into two components: version control and concurrency control. This permits modular development of multiversion protocols, and simplifies the task of proving the correctness of these protocols. An interesting feature of our framework is that the execution of read-only transactions becomes completely independent of the underlying concurrency control implementation. Also, algorithms with the version control mechanism have several advantages over most other multiversion algorithms.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {408–417},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66964,
author = {Agrawal, D. and Sengupta, S.},
title = {Modular Synchronization in Multiversion Databases: Version Control and Concurrency Control},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66964},
doi = {10.1145/66926.66964},
abstract = {In this paper we propose a version control mechanism that enhances the modularity and extensibility of multiversion concurrency control algorithms. We decouple the multiversion algorithms into two components: version control and concurrency control. This permits modular development of multiversion protocols, and simplifies the task of proving the correctness of these protocols. An interesting feature of our framework is that the execution of read-only transactions becomes completely independent of the underlying concurrency control implementation. Also, algorithms with the version control mechanism have several advantages over most other multiversion algorithms.},
journal = {SIGMOD Rec.},
month = jun,
pages = {408–417},
numpages = {10}
}

@inproceedings{10.1145/67544.66965,
author = {De Troyer, O.},
title = {RIDL*: A Tool for the Computer-Assisted Engineering of Large Databases in the Presence of Integrity Constraints},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66965},
doi = {10.1145/67544.66965},
abstract = {Tools and methods that transform higher level formalisms into logical database designs become very important. Rarely if ever do these transformations take into account integrity constraints existing in the “conceptual” model. Yet these become essential if one is forced to introduce redundancies for reasons of e.g. query efficiency. We therefore adopted the Binary Relationship Model (or “NIAM”) that is rich in constraints and built a flexible tool, RIDL*, that graphically captures NIAM semantic networks, analyzes them and then transforms them into relational designs (normalized or not), under the control of a database engineer assisted by a rule base. This is made possible by a rule-driven implementation of a new, stepwise synthesis process, and its benefits are illustrated by its treatment of e.g. subtypes. RIDL* is operational at several industrial sites in Europe and the U.S. on sizeable database projects.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {418–429},
numpages = {12},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66965,
author = {De Troyer, O.},
title = {RIDL*: A Tool for the Computer-Assisted Engineering of Large Databases in the Presence of Integrity Constraints},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66965},
doi = {10.1145/66926.66965},
abstract = {Tools and methods that transform higher level formalisms into logical database designs become very important. Rarely if ever do these transformations take into account integrity constraints existing in the “conceptual” model. Yet these become essential if one is forced to introduce redundancies for reasons of e.g. query efficiency. We therefore adopted the Binary Relationship Model (or “NIAM”) that is rich in constraints and built a flexible tool, RIDL*, that graphically captures NIAM semantic networks, analyzes them and then transforms them into relational designs (normalized or not), under the control of a database engineer assisted by a rule base. This is made possible by a rule-driven implementation of a new, stepwise synthesis process, and its benefits are illustrated by its treatment of e.g. subtypes. RIDL* is operational at several industrial sites in Europe and the U.S. on sizeable database projects.},
journal = {SIGMOD Rec.},
month = jun,
pages = {418–429},
numpages = {12}
}

@inproceedings{10.1145/67544.66967,
author = {Markowitz, Victor M. and Shoshani, Arie},
title = {On the Correctness of Representing Extended Entity-Relationship Structures in the Relational Model},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66967},
doi = {10.1145/67544.66967},
abstract = {Although the relational representation of Entity-Relationship (ER) structures gained extensive coverage, scarce attention has been paid to the issue of correctness for such representations. Several mappings have been proposed for the representation of both ER and extended ER (EER) structures by relational schemas. The informal nature of most of these proposals, however, does not allow a precise evaluation of their correctness, nor a comparison of the various mappings. We propose a canonical relational representation for EER structures and prove its correctness. We claim that a relational schema represents correctly an EER structure if it has equivalent information-capacity with the corresponding canonical representation.The second problem addressed by this paper is the normalization of relational schemas that represent EER structures. We examine the conditions required by this process and show that ignoring these conditions leads to erroneous analyses and inappropriate design decisions. We show that, under these conditions, the canonical relational representation of any (unrestricted) EER structure has an (information-capacity) equivalent Boyce-Codd Normal Form schema.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {430–439},
numpages = {10},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66967,
author = {Markowitz, Victor M. and Shoshani, Arie},
title = {On the Correctness of Representing Extended Entity-Relationship Structures in the Relational Model},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66967},
doi = {10.1145/66926.66967},
abstract = {Although the relational representation of Entity-Relationship (ER) structures gained extensive coverage, scarce attention has been paid to the issue of correctness for such representations. Several mappings have been proposed for the representation of both ER and extended ER (EER) structures by relational schemas. The informal nature of most of these proposals, however, does not allow a precise evaluation of their correctness, nor a comparison of the various mappings. We propose a canonical relational representation for EER structures and prove its correctness. We claim that a relational schema represents correctly an EER structure if it has equivalent information-capacity with the corresponding canonical representation.The second problem addressed by this paper is the normalization of relational schemas that represent EER structures. We examine the conditions required by this process and show that ignoring these conditions leads to erroneous analyses and inappropriate design decisions. We show that, under these conditions, the canonical relational representation of any (unrestricted) EER structure has an (information-capacity) equivalent Boyce-Codd Normal Form schema.},
journal = {SIGMOD Rec.},
month = jun,
pages = {430–439},
numpages = {10}
}

@inproceedings{10.1145/67544.66966,
author = {Navathe, Shamkant B. and Ra, Mingyoung},
title = {Vertical Partitioning for Database Design: A Graphical Algorithm},
year = {1989},
isbn = {0897913175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67544.66966},
doi = {10.1145/67544.66966},
abstract = {Vertical partitioning is the process of subdividing the attributes of a relation or a record type, creating fragments. Previous approaches have used an iterative binary partitioning method which is based on clustering algorithms and mathematical cost functions. In this paper, however, we propose a new vertical partitioning algorithm using a graphical technique. This algorithm starts from the attribute affinity matrix by considering it as a complete graph. Then, forming a linearly connected spanning tree, it generates all meaningful fragments simultaneously by considering a cycle as a fragment. We show its computational superiority. It provides a cleaner alternative without arbitrary objective functions and provides an improvement over our previous work on vertical partitioning.},
booktitle = {Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data},
pages = {440–450},
numpages = {11},
location = {Portland, Oregon, USA},
series = {SIGMOD '89}
}

@article{10.1145/66926.66966,
author = {Navathe, Shamkant B. and Ra, Mingyoung},
title = {Vertical Partitioning for Database Design: A Graphical Algorithm},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/66926.66966},
doi = {10.1145/66926.66966},
abstract = {Vertical partitioning is the process of subdividing the attributes of a relation or a record type, creating fragments. Previous approaches have used an iterative binary partitioning method which is based on clustering algorithms and mathematical cost functions. In this paper, however, we propose a new vertical partitioning algorithm using a graphical technique. This algorithm starts from the attribute affinity matrix by considering it as a complete graph. Then, forming a linearly connected spanning tree, it generates all meaningful fragments simultaneously by considering a cycle as a fragment. We show its computational superiority. It provides a cleaner alternative without arbitrary objective functions and provides an improvement over our previous work on vertical partitioning.},
journal = {SIGMOD Rec.},
month = jun,
pages = {440–450},
numpages = {11}
}

