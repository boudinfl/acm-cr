@inproceedings{10.1145/3256800,
author = {Genev\`{e}s, Pierre},
title = {Session Details: Keynote Talk I},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256800},
doi = {10.1145/3256800},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
numpages = {1},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797070,
author = {Paoli, Jean},
title = {Documents as Data, Data as Documents: What We Learned about Semi-Structured Information for Our Open World of Cloud &amp; Devices},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797070},
doi = {10.1145/2682571.2797070},
abstract = {Many of us always believed in a unique vision unifying documents and data through semantically-rich semi-structured information. This vision is even more critical today in our open interconnected world of Clouds and Devices.The last 20 years represents a real-life worldwide experiment in this area that fueled a massive set of market applications. In this talk, we review the history and trends of a lot of what is enabling today's core interchanges on the internet: from initial research adding document user interfaces to data, to the specification of structured documents, to the generalization of document markup techniques to the wide acceptance of document databases. We will also review our share of historical acronyms such as 'Star', 'Grif', 'OpenDoc', 'WorldWideWeb/Nexus', 'Amaya', 'InfoPath' 'HTML', 'SGML', 'XML', 'JSON', 'YAML', 'Markdown', 'Schema', 'Semantics','MongoDB', 'Hadoop', 'DocumentDB' and many others.We will then turn, cautiously and humbly, to the future and try to guess: what would the world need? And what do we need to think about to make it happen?We truly believe in the potential of the open Internet. We see pieces of information (that we once called "Diamonds of the Internet"), being created, shared, re-shaped, re-routed, modified by users or tiny small devices, understood through big data and machine learning, and processed by cloud services. We see the potential of fundamentally designing open platforms connected worldwide. By bridging technologies, we create higher level abstractions and thus more complex organisms (software) that can help everyone. But at the core remains the need for semi-structured open information fundamentally unifying documents and data.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {1},
numpages = {1},
keywords = {semi-structured information, document model, openness, document interaction, document database, document representation, document processing},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/3256801,
author = {Bulterman, Dick},
title = {Session Details: Layouts Improved},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256801},
doi = {10.1145/3256801},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
numpages = {1},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797063,
author = {Hassan, Tamir and Damera Venkata, Niranjan},
title = {The Browser as a Document Composition Engine},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797063},
doi = {10.1145/2682571.2797063},
abstract = {Printing has long been a neglected aspect of the Web, and the print function of browsers, when used on documents designed for on-screen consumption, often leads to a poor result. Whereas print CSS goes some way towards optimizing the paper experience, it still does not enable full control over the page layout, which is necessary to obtain a publication-quality print result. Furthermore, its use requires web authors to invest additional resources for a feature that might only be used infrequently. This paper introduces a framework designed to alleviate these issues and improve the print experience on the Web. We describe the technologies that enable us to automatically compose and optimize the layout of a document, and generate a high quality PDF fully within the browser. This functionality can be offered to web publishers in the form of a print button, enabling content to be simultaneously delivered in screen and print formats, and ensuring a publication-quality result that adheres to the publisher's design guidelines.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {3–12},
numpages = {10},
keywords = {browser-independent rendering, printing from the web, html, css, automated publishing},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797095,
author = {Kido, Yusuke and Yokono, Hikaru and Topi\'{c}, Goran and Aizawa, Akiko},
title = {Document Layout Optimization with Automated Paraphrasing},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797095},
doi = {10.1145/2682571.2797095},
abstract = {We introduce a new concept in document layout optimization. In our approach, paraphrase-based~layout~optimization, layout issues (e.g. widows due to poor page breaking) are automatically fixed by rewording the neighboring sentences. Techniques of paraphrasing are borrowed from the field of natural language processing towards this goal, which is the first attempt in the field of document engineering. We implemented a prototype TeX pre/post-processing system that includes two simple paraphrase generators. The experiment shows that our approach is promising and effective for improving document layout.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {13–16},
numpages = {4},
keywords = {tex, paraphrase, document layout optimization, typesetting, natural language processing},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797091,
author = {Hassan, Tamir and Hunter, Andrew},
title = {Knuth-Plass Revisited: Flexible Line-Breaking for Automatic Document Layout},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797091},
doi = {10.1145/2682571.2797091},
abstract = {There is an inherent flexibility in typesetting a block of text. Traditionally, line breaks would be manually chosen at strategic points in such a way as to minimize the amount of whitespace in each line. Hyphenation would only be used as a last resort. Knuth and Plass automated this optimization procedure, which has been used in various typesetting systems and DTP applications ever since. However, an optimal solution for the line-breaking problem does not necessarily lead us to an optimal document layout on the whole. The flexibility of choosing line breaks enables us, in many cases, to adjust the height of a paragraph by changing the number of lines, without having to make adjustments to font size, leading, etc. In many cases, the word spacing remains within the usual tolerances and visual quality does not noticeably suffer. This paper presents a modification to the Knuth-Plass algorithm to return several results for a given column of text, each corresponding to a different height, and describes steps to quantify the amount of expected flexibility in a given paragraph. We conclude with a discussion on how such "sub-optimal" results can lead to a better overall document layout, particularly in the context of mobile layouts, where flexibility is of key importance.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {17–20},
numpages = {4},
keywords = {typesetting, automatic layout, microtypography},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797078,
author = {Walger, Thomas and Hersch, Roger David},
title = {Hiding Information in Multiple Level-Line Moir\'{e}s},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797078},
doi = {10.1145/2682571.2797078},
abstract = {Secure documents often comprise an information layer that is hard to reproduce. Moir\'{e} techniques for the prevention of counterfeiting rely on the superposition of an array of transparent lines or microlenses on top of a base layer containing hidden information. Level-line moir\'{e}s consist of shapes that appear to be beating upon relative translation of a revealing grating on top of a base, in which the desired information is encoded. Usually, the base only contains the information corresponding to one moir\'{e}. In order to increase the difficulty of counterfeiting, we use tessellations to incorporate two or more moir\'{e}s within the same layer. With the method we propose, the information corresponding to up to seven level-line moir\'{e}s can be embedded within a single base layer. The moir\'{e}s are recovered with a revealer printed on a transparency or with an array of cylindrical lenses. This method is general and can be extended to other fabrication technologies.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {21–24},
numpages = {4},
keywords = {level-line moir\'{e}, security printing, tiling, moir\'{e}, tessellation},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/3256802,
author = {Simske, Steven},
title = {Session Details: Knowledge Extraction},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256802},
doi = {10.1145/3256802},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
numpages = {1},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797069,
author = {Rastan, Roya and Paik, Hye-Young and Shepherd, John},
title = {TEXUS: A Task-Based Approach for Table Extraction and Understanding},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797069},
doi = {10.1145/2682571.2797069},
abstract = {In this paper, we propose a precise, comprehensive model of table processing which aims to remedy some of the problems in the discussion of table processing in the literature. The model targets application-independent, end-to-end table processing, and thus encompasses a large subset of the work in the area. The model can be used to aid the design of table processing systems (We provide an example of such a system), can be considered as a reference framework for evaluating the performance of table processing systems, and can assist in clarifying terminological differences in the table processing literature.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {25–34},
numpages = {10},
keywords = {table extraction, task-based approach, table understanding, end-to-end table processing},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797092,
author = {B\"{o}schen, Falk and Scherp, Ansgar},
title = {Multi-Oriented Text Extraction from Information Graphics},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797092},
doi = {10.1145/2682571.2797092},
abstract = {Existing research on analyzing information graphics assume to have a perfect text detection and extraction available. However, text extraction from information graphics is far from solved. To fill this gap, we propose a novel processing pipeline for multi-oriented text extraction from infographics. The pipeline applies a combination of data mining and computer vision techniques to identify text elements, cluster them into text lines, compute their orientation, and uses a state-of-the-art open source OCR engine to perform the text recognition. We evaluate our method on 121 infographics extracted from an open access corpus of scientific publications. The results show that our approach is effective and significantly outperforms a state-of-the-art baseline.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {35–38},
numpages = {4},
keywords = {ocr, multi-oriented text extraction, infographics},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797089,
author = {Lesnikova, Tatiana and David, J\'{e}r\^{o}me and Euzenat, J\'{e}r\^{o}me},
title = {Interlinking English and Chinese RDF Data Using BabelNet},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797089},
doi = {10.1145/2682571.2797089},
abstract = {Linked data technologies make it possible to publish and link structured data on the Web. Although RDF is not about text, many RDF data providers publish their data in their own language. Cross-lingual interlinking aims at discovering links between identical resources across knowledge bases in different languages. In this paper, we present a method for interlinking RDF resources described in English and Chinese using the BabelNet multilingual lexicon. Resources are represented as vectors of identifiers and then similarity between these resources is computed. The method achieves an F-measure of 88%. The results are also compared to a translation-based method.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {39–42},
numpages = {4},
keywords = {semantic web, cross-lingual link discovery, owl:sameas, cross-lingual instance linking},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797088,
author = {Mei, Jie and Kou, Xinxin and Yao, Zhimin and Rau-Chaplin, Andrew and Islam, Aminul and Moh'd, Abidalrahman and Milios, Evangelos E.},
title = {Efficient Computation of Co-Occurrence Based Word Relatedness},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797088},
doi = {10.1145/2682571.2797088},
abstract = {Measuring document relatedness using unsupervised co-occurrence based word relatedness methods is a processing-time and memory consuming task. This paper introduces the application of compact data structures for efficient computation of word relatedness based on corpus statistics. The data structure is used to efficiently lookup: (1) the corpus statistics for the Common Word Relatedness Approach, (2) the pairwise word relatedness for the Algorithm Specific Word Relatedness Approach. These two approaches significantly accelerate the processing time of word relatedness methods and reduce the space cost of storing co-occurrence statistics in memory, making text mining tasks like classification and clustering based on word relatedness practical.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {43–46},
numpages = {4},
keywords = {word relatedness, co-occurrence, document relatedness},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797085,
author = {Ray Choudhury, Sagnik and Mitra, Prasenjit and Giles, Clyde Lee},
title = {Automatic Extraction of Figures from Scholarly Documents},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797085},
doi = {10.1145/2682571.2797085},
abstract = {Scholarly papers (journal and conference papers, technical reports, etc.) usually contain multiple ``figures'' such as plots, flow charts and other images which are generated manually to symbolically represent and illustrate visually important concepts, findings and results. These figures can be analyzed for automated data extraction or semantic analysis. Surprisingly, large scale automated extraction of such figures from PDF documents has received little attention. Here we discuss the challenges of how to build a heuristic independent trainable model for such an extraction task and how to extract figures at scale. Motivated by recent developments in table extraction, we define three new evaluation metrics: figure-precision, figure-recall, and figure-F1-score. Our dataset consists of a sample of 200 PDFs, randomly collected from five million scholarly PDFs and manually tagged for 180 figure locations. Initial results from our work demonstrate an accuracy greater than 80%.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {47–50},
numpages = {4},
keywords = {document analysis, figure extraction, pdf},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/3256803,
author = {Brailsford, David F.},
title = {Session Details: Information Summarized},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256803},
doi = {10.1145/3256803},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
numpages = {1},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797061,
author = {Banerjee, Siddhartha and Mitra, Prasenjit and Sugiyama, Kazunari},
title = {Generating Abstractive Summaries from Meeting Transcripts},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797061},
doi = {10.1145/2682571.2797061},
abstract = {Summaries of meetings are very important as they convey the essential content of discussions in a concise form. Both participants and non-participants are interested in the summaries of meetings to plan for their future work. Generally, it is time consuming to read and understand the whole documents. Therefore, summaries play an important role as the readers are interested in only the important context of discussions. In this work, we address the task of meeting document summarization. Automatic summarization systems on meeting conversations developed so far have been primarily extractive, resulting in unacceptable summaries that are hard to read. The extracted utterances contain disfluencies that affect the quality of the extractive summaries. To make summaries much more readable, we propose an approach to generating abstractive summaries by fusing important content from several utterances. We first separate meeting transcripts into various topic segments, and then identify the important utterances in each segment using a supervised learning approach.The important utterances are then combined together to generate a one-sentence summary. In the text generation step, the dependency parses of the utterances in each segment are combined together to create a directed graph. The most informative and well-formed sub-graph obtained by integer linear programming (ILP) is selected to generate a one-sentence summary for each topic segment. The ILP formulation reduces disfluencies by leveraging grammatical relations that are more prominent in non-conversational style of text, and therefore generates summaries that is comparable to human-written abstractive summaries. Experimental results show that our method can generate more informative summaries than the baselines. In addition, readability assessments by human judges as well as log-likelihood estimates obtained from the dependency parser show that our generated summaries are significantly readable and well-formed.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {51–60},
numpages = {10},
keywords = {abstractive meeting summarization, integer linear programming, topic segmentation},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797083,
author = {Drzadzewski, Grzegorz and Tompa, Frank Wm.},
title = {Enhancing Exploration with a Faceted Browser through Summarization},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797083},
doi = {10.1145/2682571.2797083},
abstract = {An enhanced faceted browsing system has been developed to support users' exploration of large multi-tagged document collections. It provides summary measures of document result sets at each step of navigation through a set of representative terms and a diverse set of documents. These summaries are derived from pre-materialized views that allow for quick calculation of centroids for various result sets. The utility and efficiency of the system is demonstrated on the New York Times Annotated Corpus.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {61–64},
numpages = {4},
keywords = {document repository, faceted browser, tags, result diversification, bursty terms},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797081,
author = {Batista, Jamilson and Ferreira, Rodolfo and Tomaz, Hil\'{a}rio and Ferreira, Rafael and Dueire Lins, Rafael and Simske, Steven and Silva, Gabriel and Riss, Marcelo},
title = {A Quantitative and Qualitative Assessment of Automatic Text Summarization Systems},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797081},
doi = {10.1145/2682571.2797081},
abstract = {Text summarization is the process of automatically creating a shorter version of one or more text documents. This paper presents a qualitative and quantitative assessment of the 22 state-of-the-art extractive summarization systems using the CNN corpus, a dataset of 3,000 news articles.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {65–68},
numpages = {4},
keywords = {summarization evaluation, text summarization, survey},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797077,
author = {Ferreira, Rafael and Lins, Rafael Dueire and Cabral, Luciano and Freitas, Fred and Simske, Steven J. and Riss, Marcelo},
title = {Automatic Document Classification Using Summarization Strategies},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797077},
doi = {10.1145/2682571.2797077},
abstract = {An efficient way to automatically classify documents may be provided by automatic text summarization, the task of creating a shorter text from one or several documents. This paper presents an assessment of the 15 most widely used methods for automatic text summarization from the text classification perspective. A naive Bayes classifier was used showing that some of the methods tested are better suited for such a task.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {69–72},
numpages = {4},
keywords = {extrinsic summarization evaluation, automatic text summarization, text classification},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/3256804,
author = {Simske, Steven J.},
title = {Session Details: Keynote Talk II},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256804},
doi = {10.1145/3256804},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
numpages = {1},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797071,
author = {Kaplan, Fr\'{e}d\'{e}ric},
title = {The Venice Time Machine},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797071},
doi = {10.1145/2682571.2797071},
abstract = {The Venice Time Machine is an international scientific programme launched by the EPFL and the University Ca'Foscari of Venice with the generous support of the Fondation Lombard Odier. It aims at building a multidimensional model of Venice and its evolution covering a period of more than 1000 years. The project ambitions to reconstruct a large open access database that could be used for research and education. Thanks to a parternship with the Archivio di Stato in Venice, kilometers of archives are currently digitized, transcribed and indexed setting the base of the largest database ever created on Venetian documents. The State Archives of Venice contain a massive amount of hand-written documentation in languages evolving from medieval times to the 20th century. An estimated 80 km of shelves are filled with over a thousand years of administrative documents, from birth registrations, death certificates and tax statements, all the way to maps and urban planning designs. These documents are often very delicate and are occasionally in a fragile state of conservation. In complementary to these primary sources, the content of thousands of monographies have been indexed and made searchable.The documents digitised in the Venice Time Machine programme are intricately interweaved, telling a much richer story when they are cross-referenced. By combining this mass of information, it is possible to reconstruct large segments of the city's past: complete biographies, political dynamics, or even the appearance of buildings and entire neighborhoods. The information extracted from the primary and secondary sources are organized in a semantic graph of linked data and unfolded in space and time in an historical geographical information system. The resulting platform can serve for both research and education. About a hundred researchers and students collaborate already on this programme. A doctoral school is organised every year in Venice and several bachelor and master courses currently use the data produced in the context of the Venice Time Machine. Through all these initiatives, the Venice Time Machine explores how "big data of the past" can change research and education in historical sciences, hopefully paving the way towards a general methodology that could be applied to many other cities and archives.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {73},
numpages = {1},
keywords = {digital humanities},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/3256805,
author = {Hardy, Matthew},
title = {Session Details: Documents Made Accessible},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256805},
doi = {10.1145/3256805},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
numpages = {1},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797066,
author = {Cutter, Michael P. and Manduchi, Roberto},
title = {Towards Mobile OCR: How to Take a Good Picture of a Document Without Sight},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797066},
doi = {10.1145/2682571.2797066},
abstract = {The advent of mobile OCR (optical character recognition) applications on regular smartphones holds great promise for enabling blind people to access printed information. Unfortunately, these systems suffer from a problem: in order for OCR output to be meaningful, a well-framed image of the document needs to be taken, something that is difficult to do without sight. This contribution presents an experimental investigation of how blind people position and orient a camera phone while acquiring document images. We developed experimental software to investigate if verbal guidance aids in the acquisition of OCR-readable images without sight. We report on our participant's feedback and performance before and after assistance from our software},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {75–84},
numpages = {10},
keywords = {document processing, optical character recognition, visual impairment},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797090,
author = {Sarkis, Mira and Concolato, Cyril and Dufourd, Jean-Claude},
title = {MSoS: A Multi-Screen-Oriented Web Page Segmentation Approach},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797090},
doi = {10.1145/2682571.2797090},
abstract = {In this paper we describe a multiscreen-oriented approach for segmenting web pages. The segmentation is an automatic and hybrid visual and structural method. It aims at creating coherent blocks which have different functions determined by the multiscreen environment. It is also characterized by a dynamic adaptation to the page content.Experiments are conducted on a set of existing applications that contain multimedia elements, in particular YouTube and video player pages. Results are compared with one segmentation method from the literature and with a ground truth manually created. With a 81% precision, the MSoS is a promising method that is capable of producing good segmentation results.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {85–88},
numpages = {4},
keywords = {multiscreen, application distribution, page segmentation, web application, automatic processing},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797076,
author = {Goncu, Cagatay and Marriott, Kim},
title = {Creating EBooks with Accessible Graphics Content},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797076},
doi = {10.1145/2682571.2797076},
abstract = {We present a new model for presenting graphics in eBooks to blind readers. It is based on the GraViewer app which allows an accessible graphic embedded in an iBook to be explored on an iPad using speech and non-speech audio feedback. We also introduce a web-based tool, GraAuthor, for creating such accessible graphics and describe the workflow for including these in an iBook. Unlike previous approaches our model provides an integrated digital presentation of both text and graphics and allows the general public to create accessible graphics.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {89–92},
numpages = {4},
keywords = {ebook, authoring, accessible graphics},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797072,
author = {Hollaus, Fabian and Diem, Markus and Fiel, Stefan and Kleber, Florian and Sablatnig, Robert},
title = {Investigation of Ancient Manuscripts Based on Multispectral Imaging},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797072},
doi = {10.1145/2682571.2797072},
abstract = {This work is concerned with the digitization and analysis of historical documents. The investigation of the documents has been conducted in three successive interdisciplinary projects. The team involved in the projects consists of philologists, chemists and computer scientists specialized in the field of digital image processing. The manuscripts investigated are partially degraded since they have been infected by mold, are corrupted by background clutter or contain faded-out or even erased writings. Since these degradations impede a transcription by scholars and worsen the performance of automated document image analysis techniques, the documents have been imaged with a portable multispectral imaging system. By using this non-invasive investigation technique, the contrast of the faded out characters can be increased, compared to ordinary white light illumination. Post-processing techniques, such as dimension reduction tools, can be used to gain a further legibility increase. The resulting images are used as a basis for further document analysis methods. These methods have been especially designed for the historical documents investigated and involve Optical Character Recognition and writer identification. This paper presents an overview on selected methods that have been developed in the projects.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {93–96},
numpages = {4},
keywords = {writer identification, optical character recognition, multispectral imaging, document image analysis},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/3256806,
author = {King, Peter R.},
title = {Session Details: Scholarly Papers Analysis and Authoring},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256806},
doi = {10.1145/3256806},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
numpages = {1},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797068,
author = {Soto, Axel J. and Mohammad, Abidalrahman and Albert, Andrew and Islam, Aminul and Milios, Evangelos and Doyle, Michael and Minghim, Rosane and Ferreira de Oliveira, Maria Cristina},
title = {Similarity-Based Support for Text Reuse in Technical Writing},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797068},
doi = {10.1145/2682571.2797068},
abstract = {Technical writing in professional environments, such as user manual authoring for new products, is a task that relies heavily on reuse of content. Therefore, technical content is typically created following a strategy where modular units of text have references to each other. One of the main challenges faced by technical authors is to avoid duplicating existing content, as this adds unnecessary effort, generates undesirable inconsistencies, and dramatically increases maintenance and translation costs. However, there are few computational tools available to support this activity. This paper investigates the use of different similarity methods for the task of identification of reuse opportunities in technical writing. We evaluated our results using existing ground truth as well as feedback from technical authors. Finally, we also propose a tool that combines text similarity algorithms with interactive visualizations to aid authors in understanding differences in a collection of topics and identifying reuse opportunities.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {97–106},
numpages = {10},
keywords = {text similarity, authoring tools and systems, document analysis, visual text analytics},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797065,
author = {Di Iorio, Angelo and Giannella, Raffaele and Poggi, Francesco and Peroni, Silvio and Vitali, Fabio},
title = {Exploring Scholarly Papers Through Citations},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797065},
doi = {10.1145/2682571.2797065},
abstract = {Bibliographies are fundamental components of academic papers and both the scientific research and its evaluation are fundamentally organized around the correct examination and classification of scientific bibliographies. Currently, most digital libraries publish bibliographic information about their content for free, and many include the citations (outgoing and in some cases even incoming) to the papers they manage. Unfortunately no sophistication is spent for these lists: monolithic pieces of text where it is even difficult to tell automatically the authors, the title and publication details, and where users are provided with no mechanisms to filter and access full context of each citation. For instance, there is no way to know in which sentence a work was cited (the citation context) and why (the citation function).In this paper we introduce a novel environment for navigating, filtering and making sense of citations. The interface, called BEX, exploits data freely available in a Link Open Dataset about scholarly papers; end-user testing proved its efficacy and usability.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {107–116},
numpages = {10},
keywords = {scholarly data visualization, citations, information interfaces and presentation, semantic publishing},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797073,
author = {Banerjee, Siddhartha and Mitra, Prasenjit},
title = {Filling the Gaps: Improving Wikipedia Stubs},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797073},
doi = {10.1145/2682571.2797073},
abstract = {The availability of only a limited number of contributors on Wikipedia cannot ensure consistent growth and improvement of the online encyclopedia. With information being scattered on the web, our goal is to automate the process of generation of content for Wikipedia. In this work, we propose a technique of improving stubs on Wikipedia that do not contain comprehensive information. A classifier learns features from the existing comprehensive articles on Wikipedia and recommends content that can be added to the stubs to improve the completeness of such stubs. We conduct experiments using several classifiers - Latent Dirichlet Allocation (LDA) based model, a deep learning based architecture (Deep belief network) and TFIDF based classifier. Our experiments reveal that the LDA based model outperforms the other models (~6% F-score). Our generation approach shows that this technique is capable of generating comprehensive articles. ROUGE-2 scores of the articles generated by our system outperform the articles generated using the baseline. Content generated by our system has been appended to several stubs and successfully retained in Wikipedia.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {117–120},
numpages = {4},
keywords = {text summarization, topic modeling, wikipedia generation},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797094,
author = {Liang, Chen and Wang, Shuting and Wu, Zhaohui and Williams, Kyle and Pursel, Bart and Brautigam, Benjamin and Saul, Sherwyn and Williams, Hannah and Bowen, Kyle and Giles, C. Lee},
title = {BBookX: An Automatic Book Creation Framework},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797094},
doi = {10.1145/2682571.2797094},
abstract = {As more educational resources become available online, it is possible to acquire more up-to-date knowledge and information. We propose BBookX, a novel computer facilitated system that automatically and collaboratively builds free open online books using publicly available educational resources such as Wikipedia. BBookX has two separate components: one creates an open version of existing books by linking different book chapters to Wikipedia articles, while another with an interactive user interface supports interactive real-time book creation where users are allowed to modify a generated book from explicit feedback.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {121–124},
numpages = {4},
keywords = {personalization, automatic book creation, open educational resources},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797075,
author = {Marriott, Kim and Shi, Mingzheng and Wybrow, Michael},
title = {VEDD: A Visual Editor for Creation and Semi-Automatic Update of Derived Documents},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797075},
doi = {10.1145/2682571.2797075},
abstract = {Document content is increasingly customised to a particular audience. Such customised documents are typically built by combining content from selected logical content modules and then editing this to create the custom document. A major difficulty is how to efficiently update these derived documents when the source documents are changed. Here we describe a web-based visual editing tool for both creating and semi-automatically updating derived documents from modules in a source library.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {125–128},
numpages = {4},
keywords = {human factors, custom documents, design},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797097,
author = {Leijen, Daan},
title = {Madoko: Scholarly Documents for the Web},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797097},
doi = {10.1145/2682571.2797097},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {129–132},
numpages = {4},
keywords = {latex, markdown, madoko},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/3256807,
author = {Munson, Ethan},
title = {Session Details: Logical Structures},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256807},
doi = {10.1145/3256807},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
numpages = {1},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797060,
author = {dos Santos, Joel A.F. and Braga, Christiano and Muchaluat-Saade, D\'{e}bora C. and Roisin, C\'{e}cile and Laya\"{\i}da, Nabil},
title = {Spatio-Temporal Validation of Multimedia Documents},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797060},
doi = {10.1145/2682571.2797060},
abstract = {A multimedia document authoring system should provide analysis and validation tools that help authors find and correct mistakes before document deployment. Although very useful, multimedia validation tools are not often provided. Spatial validation of multimedia documents may be performed over the initial position of media items before presentation starts. However, such an approach does not lead to ideal results when media item placement changes over time. Some document authoring languages allow the definition of spatio-temporal relationships among media items and they can be moved or resized during runtime. Current validation approaches do not verify dynamic spatio-temporal relationships. This paper presents a novel approach for spatio-temporal validation of multimedia documents. We model the document state, extending the Simple Hypermedia Model (SHM), comprising media item positioning during the whole document presentation. Mapping between document states represent time lapse or user interaction. We also define a set of atomic formulas upon which the author's expectations related to the spatio-temporal layout can be described and analyzed.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {133–142},
numpages = {10},
keywords = {multimedia document validation, temporal validation, spatial validation},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797086,
author = {Wu, Yang and Suzuki, Nobutaka},
title = {Detecting XSLT Rules Affected by Schema Evolution},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797086},
doi = {10.1145/2682571.2797086},
abstract = {In general, schemas of XML documents are continuously updated according to changes in the real world. If a schema is updated, then XSLT stylesheets are also affected by the schema update. To maintain the consistencies of XSLT stylesheets with updated schemas, we have to detect the XSLT rules affected by schema updates. However, detecting such XSLT rules manually is a difficult and time-consuming task, since recent DTDs and XSLT stylesheets are becoming more complex and users do not always fully understand the dependencies between XSLT stylesheets and DTDs. In this paper, we consider three subclasses based on unranked tree transducer, and consider an algorithm for detecting XSLT rules affected by a DTD update for the classes.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {143–146},
numpages = {4},
keywords = {schema evolution, xml, xslt},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797062,
author = {Wang, Shuting and Liang, Chen and Wu, Zhaohui and Williams, Kyle and Pursel, Bart and Brautigam, Benjamin and Saul, Sherwyn and Williams, Hannah and Bowen, Kyle and Giles, C. Lee},
title = {Concept Hierarchy Extraction from Textbooks},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797062},
doi = {10.1145/2682571.2797062},
abstract = {Concept hierarchies have been useful tools for presenting and organizing knowledge. With the rapid growth in the number of online knowledge resources, automatic concept hierarchy extraction is increasingly attractive. Here, we focus on concept extraction from textbooks based on the knowledge in Wikipedia. Given a book, we extract important concepts in each book chapter using Wikipedia as a resource and from this construct a concept hierarchy for that book. We define local and global features that capture both the local relatedness and global coherence embedded in that textbook. In order to evaluate the proposed features and extracted concept hierarchies, we manually construct concept hierarchies for three well used textbooks by labeling important concepts for each book chapter. Experiments show that our proposed local and global features achieve better performance than using only keyphrases to construct the concept hierarchies. Moreover, we observe that incorporating global features can improve the concept ranking precision and reaffirms the global coherence in the book.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {147–156},
numpages = {10},
keywords = {web knowledge, open education, textbooks, concept hierarchy},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/3256808,
author = {King, Peter R.},
title = {Session Details: Document Understanding},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256808},
doi = {10.1145/3256808},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
numpages = {1},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797067,
author = {Widlocher, Antoine and Bechet, Nicolas and Lecarpentier, Jean-Marc and Mathet, Yann and Roger, Julia},
title = {Combining Advanced Information Retrieval and Text-Mining for Digital Humanities},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797067},
doi = {10.1145/2682571.2797067},
abstract = {Digital Humanities make more and more structured and richly annotated corpora available. Most of this data rely on well known and established standards, such as TEI, which especially enable scientists to edit and publish their work. However, one of the remaining problems is to give adequate access to this rich data, in order to produce higher-order knowledge.In this paper, we present an integrated environment combining an advanced search engine and text-mining techniques for hermeneutics in Digital Humanities. Relying on semantic web technologies, the search engine uses full text as well as complex embedding structures and offers a single interface to access rich and heterogeneous data and meta-data. Text-mining possibilities enable scholars to exhibit regularities in corpora. Results obtained on the Cartesian corpus illustrate these principles and tools.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {157–166},
numpages = {10},
keywords = {text-mining, digital humanities, information retrieval},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797059,
author = {Eskenazi, S\'{e}bastien and Gomez-Kr\"{a}mer, Petra and Ogier, Jean-Marc},
title = {The Delaunay Document Layout Descriptor},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797059},
doi = {10.1145/2682571.2797059},
abstract = {Security applications related to document authentication require an exact match between an authentic copy and the original of a document. This implies that the documents analysis algorithms that are used to compare two documents (original and copy) should provide the same output. This kind of algorithm includes the computation of layout descriptors from the segmentation result, as the layout of a document is a part of its semantic content. To this end, this paper presents a new layout descriptor that significantly improves the state of the art. The basic of this descriptor is the use of a Delaunay triangulation of the centroids of the document regions. This triangulation is seen as a graph and the adjacency matrix of the graph forms the descriptor. While most layout descriptors have a stability of 0% with regard to an exact match, our descriptor has a stability of 74% which can be brought up to 100% with the use of an appropriate matching algorithm. It also achieves 100% accuracy and retrieval in a document retrieval scheme on a database of 960 document images. Furthermore, this descriptor is extremely efficient as it performs a search in constant time with respect to the size of the document database and it reduces the size of the index of the database by a factor 400.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {167–175},
numpages = {9},
keywords = {layout descriptor, delaunay, stability, retrieval, classification, hashing},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797064,
author = {Azevedo, Roberto Gerson de Albuquerque and Lima, Guilherme F. and Soares, Luiz Fernando Gomes},
title = {An Approach to Convert NCL Applications into Stereoscopic 3D},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797064},
doi = {10.1145/2682571.2797064},
abstract = {This paper presents and discusses the internal operation of NCLSC (NCL Stereo Converter): a tool to convert a 2D interactive multimedia application annotated with depth information to a stereoscopic-multimedia application. Stereoscopic-multimedia applications are those that codify both the left-eye and right-eye views, as required by stereoscopic 3D displays. NCLSC takes as input an NCL (Nested Context Language) document and outputs an NCL stereoscopic application codified in side-by-side or top-bottom format (both common input formats for 3DTV sets). NCL is the declarative language adopted in most Latin America countries for terrestrial digital TV middleware systems and the ITU-T H.761 Recommendation for IPTV services. However, the proposed approach is not restricted to NCL and can be used by other languages. The depth annotation allows for positioning each 2D graphical component in a layered (2.5D or 2D+depth) user interface. It is used by NCLSC to compute the screen parallax (offset) between the graphical elements in the left and right views of the resulting stereoscopic application. When the resulting application is presented on stereoscopic 3D displays, such screen parallax induces retinal disparity, which creates the illusion of floating flat-2D graphical elements. NCLSC does not require any additional native middleware support to run in currently available 3D-enabled TV sets. Moreover, NCLSC can adapt, at run-time, the output application to different display sizes, viewer distances, and viewer preferences, which are usually required for a proper balance between artistic effects and user experience.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {177–186},
numpages = {10},
keywords = {ginga, stereoscopic multimedia applications, document processing, 3dtv, ncl},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797084,
author = {Vernica, Rares and Venkata, Niranjan Damera},
title = {AERO: An Extensible Framework for Adaptive Web Layout Synthesis},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797084},
doi = {10.1145/2682571.2797084},
abstract = {We present AERO, an extensible framework for adaptive web layout synthesis. The goal is to provide an underlying software architecture to allow general adaptive layout behaviors. The framework consists of a 1) a suite of templates specified in HTML/CSS, 2) A hierarchical, highly customizable scoring function specification and 3) An evaluation engine that leverages native browser rendering to rapidly render content and apply the scoring functions. Unlike current responsive layout frameworks for web (e.g., Twitter Bootstrap) that have pre-configured grid layouts that adapt in a manually pre-encoded content-independent manner, AERO allows layout to adapt automatically based on multiple content-dependent criteria like aesthetic quality, cropability of individual images, layout A/B testing results, Ad placement etc.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {187–190},
numpages = {4},
keywords = {adaptive, extensible, layout, framework, automated publishing},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797099,
author = {Silva, Gabriel and Ferreira, Rafael and Lins, Rafael Dueire and Cabral, Luciano and Oliveira, Hil\'{a}rio and Simske, Steven J. and Riss, Marcelo},
title = {Automatic Text Document Summarization Based on Machine Learning},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797099},
doi = {10.1145/2682571.2797099},
abstract = {The need for automatic generation of summaries gained importance with the unprecedented volume of information available in the Internet. Automatic systems based on extractive summarization techniques select the most significant sentences of one or more texts to generate a summary. This article makes use of Machine Learning techniques to assess the quality of the twenty most referenced strategies used in extractive summarization, integrating them in a tool. Quantitative and qualitative aspects were considered in such assessment demonstrating the validity of the proposed scheme. The experiments were performed on the CNN-corpus, possibly the largest and most suitable test corpus today for benchmarking extractive summarization strategies.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {191–194},
numpages = {4},
keywords = {extractive features, sentence scoring methods, text summarization},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797082,
author = {Denoue, Laurent and Carter, Scott and Cooper, Matthew},
title = {Searching Live Meeting Documents "Show Me the Action"},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797082},
doi = {10.1145/2682571.2797082},
abstract = {Live meeting documents require different techniques for effectively retrieving important pieces of information. During live meetings, people share web sites, edit presentation slides, and share code editors. A simple approach is to index with Optical Character Recognition (OCR) the video frames, or key-frames, being shared and let user retrieve them. Here we show that a more useful approach is to look at what actions users take inside the live document streams. Based on observations of real meetings, we focus on two important signals: text editing and mouse cursor motion. We describe the detection of text and cursor motion, their implementation in our WebRTC (Web Real-Time Communication)-based system, and how users are better able to search live documents during a meeting based on these extracted actions.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {195–198},
numpages = {4},
keywords = {ocr, live document search, indexing, screen-sharing, video conferencing, real-time search},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797087,
author = {Jansen, Jack and Frantzis, Michael and Cesar, Pablo},
title = {Multimedia Document Structure for Distributed Theatre},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797087},
doi = {10.1145/2682571.2797087},
abstract = {This paper explores the suitability of structured (and declarative) multimedia document formats for supporting a novel type of performing arts: distributed theatre. In distributed theatre, the actors are split between two (or more) locations, but together deliver a single performance mediated by the cameras, the internet, and projection technologies. Based on our efforts to make an actual distributed theatre production happen (the Tempest by Miracle Theatre), this paper reflects on our experience. Our findings are divided into two main areas: workflow and document structure. We conclude that novel types of video-mediated applications, like distributed theatre, require new manners of authoring documents. Moreover, specific extensions to existing document formats are needed in order to accommodate the new requirements imposed by such kind of applications.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {199–202},
numpages = {4},
keywords = {remote audience, video conferencing, theatre},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797079,
author = {Svendsen, Jeremy and Branzan Albu, Alexandra},
title = {Change Classification in Graphics-Intensive Digital Documents},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797079},
doi = {10.1145/2682571.2797079},
abstract = {This paper proposes an approach for the automatic detection and classification of changes occurring in images of documents with identical content, but generated with different software versions, or under different operating platforms. Our work is performed on a database of digitally-born business documents created using financial reporting tools. The proposed method involves a multi-stage process, where the end goal is to present to a human user the reports which have changed and the changes which were detected. Our main contribution is related to matching and comparing of graphical document elements. This paper focuses on detection of local, translation-based changes. Future work will explore other local changes involving size, color, and rotation.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {203–206},
numpages = {4},
keywords = {change detection, computer vision, electronic documents, document image analysis},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797098,
author = {Balinsky, Helen Y. and Mohammad, Nassir},
title = {Fine Grained Access of Interactive Personal Health Records},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797098},
doi = {10.1145/2682571.2797098},
abstract = {Electronic Personal Healthcare Records (PHRs) provide the means for individuals to hold, update and share their medical information in a digitally accessible form. However, the sensitive nature of healthcare information and the functional limitations of PHRs has resulted in their acceptance remaining relatively low. This is primarily due to fears of security and privacy in the current central authority based technologies on offer. In order to alleviate these concerns, whilst maintaining security, ease of access and distribution, we propose a PHR format that utilizes and extends a secure composite document format, Publicly Posted Composite Documents [1], originally designed for cross-organizational business workflows. The proposed PHR ensures data is always encrypted whilst traversing non-secure channels, with fine-grained access control built in to enable multiple people to have differential access to the same PHR. End-to-end encryption using Password Key Derivation Functions ensures no central authority is required to have access to plaintext data or decryption keys. This allows safe cooperation with Cloud Service Providers (CSPs) who act as the primary storage and vehicle by which PHRs can be shared. Our PHRs are designed to be partially downloaded and exported on request, and to gather PHR formatted data securely from an ecosystem of healthcare devices.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {207–210},
numpages = {4},
keywords = {personal healthcare records, security, embedded access control, publicly posted composite documents, workflows},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797093,
author = {Franze, Juliane and Marriott, Kim and Wybrow, Michael},
title = {Does a Split-View Aid Navigation Within Academic Documents?},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797093},
doi = {10.1145/2682571.2797093},
abstract = {Paper is still the dominant medium in academic reading. One reason is the ease of navigation within a paper document. We therefore investigate how to provide a more paper-like navigation within an academic document when read digitally. We present the results of a user study in which we compare the standard single-view hyperlink navigation with a split-view navigation. The split-view offers the reader a primary reading view of the document as well as a contextual view next to it. When a hyperlink is activated in the reading view the contextual view shows the referenced element. While we found no difference between user performance, the split-view was preferred by almost all users to the standard single-view navigation model.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {211–214},
numpages = {4},
keywords = {digital documents, technical reading, usability, human factors},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797096,
author = {Dumas, L\'{e}onard and Crozat, St\'{e}phane and Bachimont, Bruno and Spinelli, Sylvain},
title = {An Approach for Designing Proofreading Views in Publishing Chains},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797096},
doi = {10.1145/2682571.2797096},
abstract = {Documentary production often involves a revising process in which documents need to be proofread. This important task faces new challenges when dealing with digital documents. Indeed, three features of digital writing are problematic: (1) documents evolve very frequently and cannot be proofread each time as a whole, (2) interactions provided by hypertexts make the task less efficient and (3) document repurposing increases the views of content to proofread. As an advanced digital writing technology, XML publishing chains are a relevant framework for studying proofreading of digital documents. This paper argues the need for proofreading views, which enable the comparison of two versions of the document based on a diff algorithm. It also proposes a design approach based on case studies within Scenari publishing chains, involving the annotation of content and the validation of modifications.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {215–218},
numpages = {4},
keywords = {fragmented document, proofreading, repurposing, interactivity, xml publishing chain, diff, polymorphism},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797074,
author = {Kim, Chelhwon and Chiu, Patrick and Tang, Henry},
title = {High-Quality Capture of Documents on a Cluttered Tabletop with a 4K Video Camera},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797074},
doi = {10.1145/2682571.2797074},
abstract = {We present a novel system for detecting and capturing paper documents on a tabletop using a 4K video camera mounted overhead on pan-tilt servos. Our automated system first finds paper documents on a cluttered tabletop based on a text probability map, and then takes a sequence of high-resolution frames of the located document to reconstruct a high quality and fronto-parallel document page image. The quality of the resulting images enables OCR processing on the whole page. We performed a preliminary evaluation on a small set of 10 document pages and our proposed system achieved 98% accuracy with the open source Tesseract OCR engine.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {219–222},
numpages = {4},
keywords = {video processing, computer vision, reconstruction, tabletop system, document capture, ocr, image processing},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2797080,
author = {Lopes Filho, Alberto Nicodemus Gomes and Mello, Carlos Alexandre Barros},
title = {Segmentation of Overlapping Digits through the Emulation of a Hypothetical Ball and Physical Forces},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2797080},
doi = {10.1145/2682571.2797080},
abstract = {This paper presents an algorithm for segmenting pairs of overlapping handwritten digits. Digits can be found overlapped in text depending on writing style and organization; digits in close proximity or with elongated strokes may also overlap with their neighbors. Applications such as automated character recognition are directly affected by overlapping characters and their segmentation. The proposed approach is based on the emulation of inertia and a deformable hypothetical ball. The strokes act as a pathway for the ball to run and create the segmentation. The results of the algorithm are subject to a digit recognizer and it is shown that the method performs well and presents lower computational cost when compared to other segmentation approaches.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {223–226},
numpages = {4},
keywords = {document image processing, deformable ball, segmentation, physical forces, overlapping digits},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/3256809,
author = {Maier, Sonja},
title = {Session Details: Workshop and Tutorials},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256809},
doi = {10.1145/3256809},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
numpages = {1},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2801032,
author = {Barabucci, Gioele and Borghoff, Uwe M. and Di Iorio, Angelo and Maier, Sonja and Munson, Ethan},
title = {Document Changes: Modeling, Detection, Storage and Visualization (DChanges 2015)},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2801032},
doi = {10.1145/2682571.2801032},
abstract = {The DChanges series of workshops focuses on changes in all their aspects and applications: algorithms to detect changes, models to describe them and techniques to present them to the final users are only some of the topics we investigate. The workshop is open to researchers and practitioners from industry and academia.In this edition, we will follow up on the discussion of DChanges 2014 about algorithms and interfaces to better understand and exploit detected changes, and about standards for modeling and transmitting changes.Particular attention will also be given to the use of these techniques in digital humanities - for instance in the studies of collation, text genetics and plagiarism detection - and for real-time collaborative editing.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {227–228},
numpages = {2},
keywords = {change tracking, change detection, change analysis and interpretation, applications},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2801033,
author = {Nicholas, Charles and Brandon, Robert},
title = {Document Engineering Issues in Document Analysis},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2801033},
doi = {10.1145/2682571.2801033},
abstract = {We present an overview of the field of malware analysis with emphasis on issues related to document engineering. We will introduce the field with a discussion of the types of malware, including executable binaries, polymorphic malware, malicious PDFs, and exploit kits. We will conclude with our view of important research questions in the field.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {229–230},
numpages = {2},
keywords = {document engineering, malware analysis},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2801034,
author = {Sire, St\'{e}phane},
title = {Developing Web Applications with Document Engineering Technologies and Enjoying It!},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2801034},
doi = {10.1145/2682571.2801034},
abstract = {This tutorial proposes a practical software development method for building web applications using the XQuery and XSLT languages for manipulating semi-structured data. This method captures solutions and practices that we have applied during the last 4 years into many projects. It can be used on any XML database, as it requires only a thin layer to analyze and route incoming HTTP requests to a simple pipeline rendering the page. We will demonstrate it with a real world example developed with eXist-DB and the Oppidum lightweight XQuery framework.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {231–232},
numpages = {2},
keywords = {document engineering, xquery, web application, pipelining languages, xslt, xml databases},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

@inproceedings{10.1145/2682571.2801035,
author = {Atencia, Manuel and David, J\'{e}r\^{o}me and Genoud, Philippe},
title = {What Is This Thing Called Linked Data?},
year = {2015},
isbn = {9781450333078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682571.2801035},
doi = {10.1145/2682571.2801035},
abstract = {The Linked Data initiative has made it possible for the web to evolve from being a global information space in which only documents are linked to one in which both documents and data are linked: a web of documents and data. This tutorial aims to give an overview of the principles, models and technologies underlying Linked Data.},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
pages = {233–234},
numpages = {2},
keywords = {semantic web, sparql, owl, linked data, rdf-s, rdf},
location = {Lausanne, Switzerland},
series = {DocEng '15}
}

