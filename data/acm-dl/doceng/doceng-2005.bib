@inproceedings{10.1145/3259638,
author = {Quint, V.},
title = {Session Details: Document Structure and Content Analysis 1},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259638},
doi = {10.1145/3259638},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
numpages = {1},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096605,
author = {D\'{e}jean, Herv\'{e} and Meunier, Jean-Luc},
title = {Structuring Documents According to Their Table of Contents},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096605},
doi = {10.1145/1096601.1096605},
abstract = {In this paper, we present a method for structuring a document according to the information present in its Table of Contents. The detection of the ToC as well as the determination of the parts it refers to in the document body rely on a series of generic properties characterizing any ToC, while its hierarchization is achieved using clustering techniques. We also report on the robustness and performance of the method before discussing it, in light of related work.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {2–9},
numpages = {8},
keywords = {table of contents recognition, document structuring},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096606,
author = {R\"{o}nnau, Sebastian and Scheffczyk, Jan and Borghoff, Uwe M.},
title = {Towards XML Version Control of Office Documents},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096606},
doi = {10.1145/1096601.1096606},
abstract = {Office applications such as OpenOffice and Microsoft Office are widely used to edit the majority of today's business documents: office documents. Usually, version control systems consider office documents as binary objects, thus severely hindering collaborative work. Since XML has become a de-facto standard for office applications, we focus on versioning office documents by structured XML version control approaches. This enables state-of-the-art version control for office documents.A basic prerequisite to XML version control is a diff algorithm, which detects structural changes between XML documents. In this paper, we evaluate state-of-the-art XML diff algorithms w.r.t. their suitability to OpenOffice XML documents and the future OASIS office document standard. It turns out that, due to the specific XML office format, a careful examination of the diff algorithm characteristics is necessary. Therefore, we identify important features for XML diff approaches to handle office documents. We have implemented a first OpenOffice versioning API that can be used in version control systems as a replacement for line-based or binary diffs, which are currently used.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {10–19},
numpages = {10},
keywords = {XML diffing, version control, office applications},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096607,
author = {Behera, Ardhendu and Lalanne, Denis and Ingold, Rolf},
title = {Influence of Fusion Strategies on Feature-Based Identification of Low-Resolution Documents},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096607},
doi = {10.1145/1096601.1096607},
abstract = {The paper describes a method by which one could use the documents captured from low-resolution handheld devices to retrieve the originals of those documents from a document store. The method considers conjunctively two complementary feature sets. First, the geometrical distribution of the color in the document's 2D image plane is preferred. Secondly, the shallow layout features is considered due to the poor resolution of the captured documents. We propose in this article to fuse those two complementary feature sets in order to improve document identification performance. Finally, in order to test the influence of merging strategies on document identification performance, a synergic method is proposed and evaluated relative to a similar method in which feature sets are simply considered sequentially.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {20–22},
numpages = {3},
keywords = {document signature, shallow layout features, document retrieval, geometrical color distribution},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096608,
author = {Whiting, Mark A. and Cowley, Wendy and Cramer, Nick and Gibson, Alex and Hohimer, Ryan and Scott, Ryan and Tratz, Stephen},
title = {Enabling Massive Scale Document Transformation for the Semantic Web: The <i>Universal Parsing Agent</i>™},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096608},
doi = {10.1145/1096601.1096608},
abstract = {The Universal Parsing Agent (UPA) is a document analysis and transformation program that supports massive scale conversion of information into forms suitable for the semantic web. UPA provides reusable tools to analyze text documents; identify and extract important information elements; enhance text with semantically descriptive tags; and output the information that is needed in the format and structure that is needed.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {23–25},
numpages = {3},
keywords = {document transformation, XSLT, XML, regular expressions, parsing, natural language processing},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096609,
author = {Cheng, Isaac and Srinivasan, Savitha and Boyette, Neil},
title = {Exploiting XML Technologies for Intelligent Document Routing},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096609},
doi = {10.1145/1096601.1096609},
abstract = {Today, XML is increasingly becoming a standard for representation of semi-structured information such as documents that combines content and metadata. Typical document management applications include document representation, authoring, validation, and document routing in support of a business process. We propose a framework for intelligent document routing that exploits and extends XML technologies to automate dynamic document routing and real-time update of business routing logic. The document-routing logic is stored in a secure repository and executed by a business rules engine. During rule execution, the input parameters of each business rule are bound with the data from each inbound XML document. This document routing framework is validated in a real-world implementation with reduced development cost, accelerated rule update cycle and simplified administration efforts.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {26–28},
numpages = {3},
keywords = {worldwide, IT, MQ, turnover, outsource, organizational change, B2B, SO, training, MIS, policy, economy, ROI, DMS, downsize, logistics, J2EE, global, business transformation, SOA},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/3259639,
author = {Simske, S.},
title = {Session Details: Posters},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259639},
doi = {10.1145/3259639},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
numpages = {1},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096611,
author = {Oda, Hiromi},
title = {A System of Collecting Domain-Specific Jargons},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096611},
doi = {10.1145/1096601.1096611},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {29},
numpages = {1},
keywords = {domain specific expressions community jargons},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096612,
author = {Balinsky, Helen and Pilu, Maurizio},
title = {Emphasis for Highly Customized Documents},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096612},
doi = {10.1145/1096601.1096612},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {30},
numpages = {1},
keywords = {high customization, publishing, aesthetic measure, emphasis},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096613,
author = {Bagley, Steven R. and Brailsford, David F.},
title = {The COG Scrapbook},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096613},
doi = {10.1145/1096601.1096613},
abstract = {Creating truly dynamic documents from disparate components is not an easy process, especially for untrained end users. Existing packages such as Adobe InDesign or Quark XPress are designed for graphic-arts professionals and even these are not optimised for the process of integrating disparate components. The 'COG Scrapbook' builds on our PDF-based Component Object Graphics (COGs) technology to create a radically new 'drag and drop' software approach to creating dynamic documents for use in educational environments.The initial focus of the COG Scrapbook is to enable groups of people to collaboratively create material ranging from smaller-sized (e.g. A4) personal scrapbooks through to large (e.g. A0) posters. Some examples scenarios where the technology could be utilized are: university students producing posters for class assignments;school teachers and students jointly creating classroom wall displays;creation of a scrapbook of annotated digital photographs resulting from school field trips and museum visits.The poster illustrating the present state of this work will itself have been constructed using COG Scrapbook software.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {31},
numpages = {1},
keywords = {COGs, FormXObject, PDF, graphic objects},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/3259640,
author = {Nicholas, C.},
title = {Session Details: Making Use of Document Standards and Models},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259640},
doi = {10.1145/3259640},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
numpages = {1},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096615,
author = {Lumley, John and Gimson, Roger and Rees, Owen},
title = {A Framework for Structure, Layout &amp; Function in Documents},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096615},
doi = {10.1145/1096601.1096615},
abstract = {The Document Description Framework (DDF) is a representation for variable-data documents. It supports very high flexibility in the type and extent of variation supported, considerably beyond the 'copy-hole' or flow-based mechanisms of existing formats and tools. DDF is based on holding application data, logical data struc-ture and presentation as well as constructional 'programs' together within a single document. DDF documents can be merged with other documents, bound to variable values incrementally, combine several types of layout and styling in the same document and support final delivery to different devices and page-ready formats. The framework uses XML syntax and fragments of XSLT to describe 'programmatic construction' of a bound document. DDF is extensible, especially in the ability to add new types of layout and inter-operability between components in different formats. In this paper we describe the motivation for DDF, the major design choices and how we evaluate a DDF document with specific data values. We show through implemented examples how it can be used to construct high-complexity and variability presentations and how the framework complements and can use many existing XML-based documents formats, such as SVG and XSL-FO.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {32–41},
numpages = {10},
keywords = {XML, document construction, functional programming, SVG, XSLT},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096616,
author = {Liu, Dongxi and Hu, Zhenjiang and Takeichi, Masato},
title = {An Environment for Maintaining Computation Dependency in XML Documents},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096616},
doi = {10.1145/1096601.1096616},
abstract = {In the domain of XML authoring, there have been many tools to help users to edit XML documents. These tools make it easier to produce complex documents by using such technologies as syntax-directed or presentation-oriented editing, etc. However, when an XML document contains data with some computation dependency among them, these tools cannot free users from the burden of maintaining this dependency relationship. By computation dependency, we mean that some data are gotten by computing from other data in the same document.In this paper, we present an environment for authoring XML document, in which users can express the data dependency relationship in one document explicitly rather than implicitly in their minds. Under this environment, the dependent parts of the document are represented as expressions, which in turn can be evaluated to generate the dependent data. Therefore, users need not to compute the dependent data first and then input them manually, as required by the current authoring tools.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {42–51},
numpages = {10},
keywords = {programmable structured document, computation dependency, lazy evaluation, XML, functional programming},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096617,
author = {Genev\`{e}s, Pierre and Rose, Kristoffer},
title = {Compiling XPath for Streaming Access Policy},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096617},
doi = {10.1145/1096601.1096617},
abstract = {We show how the full XPath language can be compiled into a minimal subset suited for stream-based evaluation. Specifically, we show how XPath normalization into a core language as proposed in the current W3C ``Last Call'' draft of the XPath/XQuery Formal Semantics can be extended such that both the context state and reverse axes can be eliminated from the core XPath (and potentially XQuery) language. This allows execution of (almost) full XPath on any of the emerging streaming subsets.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {52–54},
numpages = {3},
keywords = {XPath, compilation, streaming, static rewriting},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096618,
author = {Kenji, Manaka and Hiroyuki, Sato},
title = {Static Optimization of XSLT Stylesheets: Template Instantiation Optimization and Lazy XML Parsing},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096618},
doi = {10.1145/1096601.1096618},
abstract = {The increasing popularity of XSLT brings up the requirement of more efficient performance. In this paper, we propose two optimization techniques based on template caller-callee analysis. One is the template instantiation optimization which analyzes a stylesheet and identifies the templates to be instantiated before transformation. The other is the static lazy XML parsing optimization that constructs a pruned XML tree by statically identifying the nodes that are actually referred. Furthermore, we have implemented both our optimizations on Saxon and have evaluated its performance. In these experiments, we have proved both of them to be practically useful and to improve XSLT performance.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {55–57},
numpages = {3},
keywords = {saxon, XSLT, optimization, lazy evaluation},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096619,
author = {Kuo, Y. S. and Shih, N. C. and Tseng, Lendle and Hu, Hsun-Cheng},
title = {Generating Form-Based User Interfaces for XML Vocabularies},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096619},
doi = {10.1145/1096601.1096619},
abstract = {So far, many user interfaces for XML data (documents) have been constructed from scratch for specific XML vocabularies and applications. The tool support for user interfaces for XML data is inadequate. Forms-XML is an interactive component invoked by applications for generating user interfaces for prescribed XML vocabularies automatically. Based on a given XML schema, the component generates a hierarchy of HTML forms for users to interact with and update XML data compliant with the given schema. The user interface Forms-XML generates is very simple with an abundance of guidance and hints to the user, and can be customized by user interface designers as well as developers.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {58–60},
numpages = {3},
keywords = {XML editing, user interface, XML},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096620,
author = {Macdonald, Alexander J. and Brailsford, David F. and Bagley, Steven R.},
title = {Encapsulating and Manipulating Component Object Graphics (COGs) Using SVG},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096620},
doi = {10.1145/1096601.1096620},
abstract = {Scalable Vector Graphics (SVG) has an imaging model similar to that of PostScript and PDF but the XML basis of SVG allows it to participate fully, via namespaces, in generalised XML documents.There is increasing interest in using SVG as a Page Description Language and we examine ways in which SVG document components can be encapsulated in contexts where SVG will be used as a rendering technology for conventional page printing.Our aim is to encapsulate portions of SVG content (SVG COGs) so that the COGs are mutually independent and can be moved around a page, while maintaining invariant graphic properties and with guaranteed freedom from side effects and mutual interference. Parellels are drawn between COG implementation within SVG's tree-based inheritance mechanisms and an earlier COG implementation using PDF.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {61–63},
numpages = {3},
keywords = {SVG, component object graphics, XML, parameterization, PDF},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/3259641,
author = {Vanoirbeek, C.},
title = {Session Details: Document Presentation},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259641},
doi = {10.1145/3259641},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
numpages = {1},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096622,
author = {da Silva, Ana Cristina B. and de Oliveira, Joao B. S. and Mano, Fernando T. M. and Silva, Thiago B. and Meirelles, Leonardo L. and Meneguzzi, Felipe R. and Giannetti, Fabio},
title = {Support for Arbitrary Regions in XSL-FO},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096622},
doi = {10.1145/1096601.1096622},
abstract = {This paper proposes an extension of the XSL-FO standard which allows the specification of an unlimited number of arbitrarily shaped page regions. These extensions are built on top of XSL-FO 1.1 to enable flow content to be laid out into arbitrary shapes and allowing for page layouts currently available only to desktop publishing software. Such a proposal is expected to leverage XSL-FO towards usage as an enabling technology in the generation of content intended for personalized printing.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {64–73},
numpages = {10},
keywords = {digital publishing, arbitrary shapes, LaTeX, XML, XSL-FO, typesetting, SVG},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096623,
author = {Hurst, Nathan and Marriott, Kim and Moulder, Peter},
title = {Toward Tighter Tables},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096623},
doi = {10.1145/1096601.1096623},
abstract = {Tables are provided in virtually all document formatting systems and are one of the most powerful and useful design elements in current web document standards. Unfortunately, optimal layout of tables which contain text is NP-hard for reasonable layout requirements such as minimizing table height for a given width [1]. We present two new independently-applicable techniques for table layout. The first technique is to solve a continuous approximation to the original layout problem by using a constant-area approximation of the cell content combined with a minimum width and height for the cell. The second technique starts by setting each column to its narrowest possible width and then iteratively reduces the height of the table by judiciously widening its columns. This second technique uses the actual text and line-break rules rather than the constant-area approximation used by the first technique. We also investigate two hybrid approaches both of which use iterative column widening to improve the quality of an initial solution found using a different technique. In the first hybrid approach we use the continuous approximation technique to compute the initial column widths while in the second hybrid approach a modification of the HTML table layout algorithm is used to compute the initial widths. We found that all four techniques are reasonably fast and give sig-nificantly more compact layout than that of HTML layout engines.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {74–83},
numpages = {10},
keywords = {optimisation techniques, conic programming, table layout},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096624,
author = {Kerne, Andruid and Koh, Eunyee and Sundaram, Vikram and Mistrot, J. Michael},
title = {Generative Semantic Clustering in Spatial Hypertext},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096624},
doi = {10.1145/1096601.1096624},
abstract = {This paper presents an iterative method for generative semantic clustering of related information elements in spatial hypertext documents. The goal is to automatically organize them in ways that are meaningful to the user. We consider a process in which elements are gradually added to a spatial hypertext. The method for generating meaningful layout is based on a quantitative model that measures and represents the mutual relatedness between each new element and those already in the document. The measurement is based on attributes such as metadata, term vectors, user interest expressions, and document locations. We call this model relatedness potential, because it represents how much the new element is related and thus attracted to existing elements as a vector field across the space. Using this field as a gradient potential, the new element will be placed near the most attracted elements, forming clusters of related elements. The relative magnitude of contribution of attributes to relatedness potential can be controlled through an interactive interface.Unlike prior clustering methods such as k-means and self-organizing-maps, relatedness potential works well in iterative systems, in which the collection of elements is not defined a priori. Further, users can invoke relatedness potential to re-cluster elements, as they engage in on-the-fly provisional acts of direct manipulation reorganization and latching of a few most significant elements. A preliminary study indicates that users find this method generates spatial hypertext documents that are easier to read.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {84–93},
numpages = {10},
keywords = {generative hypermedia, spatial hypertext, mixed-initiatives, document layout, collections, clustering, information triage},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/3259642,
author = {King, P. R.},
title = {Session Details: Adaptive and Variant Documents},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259642},
doi = {10.1145/3259642},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
numpages = {1},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096626,
author = {Loureiro, Gil and Azevedo, Francisco},
title = {Constrained XSL Formatting Objects for Adaptive Documents},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096626},
doi = {10.1145/1096601.1096626},
abstract = {The pagination strategy of XSL Formatting Objects (XSL:FO) is based on a "break if no fit" approach that often produces one last page with only one printable object due to a lack of space on the previous page. On a batch, high volume, personalized document production scenario, this fact can represent a high cost on extra sheets of paper with a lot of free space and a document with a poor look. In this paper, we describe a new approach to solve the pagination problem of XSL:FO documents where space use efficiency and aesthetic aspects are considered. The approach is based on constraint satisfaction using Mixed Integer Linear Programming (MILP) models. The starting point was the FO part of XSL specification, where we added a Constrained XSL:FO extension (referred to as CXSL:FO) that delivers tags used to declare constraints on size and font adjustments of target FO objects. This extension is added to our reengineered FOP formatter that builds and solves an MILP model to find the global optimal solution corresponding to a document with the minimum number of pages, each one being maximally filled. We show its effectiveness in the generation of personalized welcome letters.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {95–97},
numpages = {3},
keywords = {adaptive documents, XSL:FO, MILP, pagination, constraints},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096627,
author = {Lemlouma, Tayeb and Laya\"{\i}da, Nabil},
title = {Content Interaction and Formatting for Mobile Devices},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096627},
doi = {10.1145/1096601.1096627},
abstract = {In this paper we present an experimental content adaptation system for mobile devices. The system enables the presentation of multimedia content and considers the problem of small screen display of mobile terminals. The approach combines structural and media adaptation with the content formatting and proposes a system that handles the user interaction and the content navigation.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {98–100},
numpages = {3},
keywords = {evaluation, user interaction, content adaptation, mobile-devices, content formatting},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/3259643,
author = {Brailsford, D.},
title = {Session Details: Document Structure and Content Analysis 2},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259643},
doi = {10.1145/3259643},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
numpages = {1},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096629,
author = {Boukottaya, Aida and Vanoirbeek, Christine},
title = {Schema Matching for Transforming Structured Documents},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096629},
doi = {10.1145/1096601.1096629},
abstract = {Structured document content reuse is the problem of restructuring and translating data structured under a source schema into an instance of a target schema. A notion closely tied with structured document reuse is that of structure transformations. Schema matching is a critical strep in structured document transformations. Manual matching is expensive and error-prone. It is therefore important to develop techniques to automate the matching process and thus the transformation process. In this paper, we contributed in both understanding the matching problem in the context of structured document transformations and developing matching methods those output serves as the basis for the automatic generation of transformation scripts.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {101–110},
numpages = {10},
keywords = {document structure transformations, schema matching},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096630,
author = {Leydier, Yann and LeBourgeois, Frank and Emptoz, Hubert},
title = {Textual Indexation of Ancient Documents},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096630},
doi = {10.1145/1096601.1096630},
abstract = {In the past years many levels of indexation have been developped to allow a fast retrieval of digitized documents. Among all the ways of indexing a document, textual indexation allows the finest querries on a the documents' content. Usually, the plain text transcription of a digitized document is obtained by applying an OCR (Optical Character Recognition) software on it. What if the OCR fails? Indeed OCR systems are inefficient on low-quality printed documents, and are unsuited to the processing of ancient fonts. Furthermore, OCR is not applicable to manuscript text recognition. In this paper we introduce two alternative methods of accessing to text trough the image: the Computer Assisted Transcription and the Word Spotting.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {111–117},
numpages = {7},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096631,
author = {\'{A}vila, Bruno Ten\'{o}rio and Lins, Rafael Dueire},
title = {A Fast Orientation and Skew Detection Algorithm for Monochromatic Document Images},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096631},
doi = {10.1145/1096601.1096631},
abstract = {Very often in the digitization process, documents are either not placed with the correct orientation or are rotated of small angles in relation to the original image axis. These factors make more difficult the visualization of images by human users, increase the complexity of any sort of automatic image recognition, degrade the performance of OCR tools, increase the space needed for image storage, etc. This paper presents a fast algorithm for orientation and skew detection for complex monochromatic document images, which is capable of detecting any document rotation at a high precision.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {118–126},
numpages = {9},
keywords = {orientation and skew detection, monochromatic document image},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096632,
author = {Simske, Steven J. and Li, Dalong and Aronoff, Jason S.},
title = {A Statistical Method for Binary Classification of Images},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096632},
doi = {10.1145/1096601.1096632},
abstract = {The classification of documents with sparse text, and video analysis, relies on accurate image classification. We herein present a method for binary classification that accommodates any number of individual classifiers. Each individual classifier is defined by the critical point between its two means, and its relative weighting is inversely proportional to its expected error rate. Using 10 simple image analysis metrics, we distinguish a set of "natural" and "city" scenes, providing a "semantically meaningful" classification. The optimal combination of 5 of these 10 classifiers provides 85.8% accuracy on a small (120 image) feasibility corpus. When this feasibility corpus is then split into half training and half testing images, the mean accuracy of the optimum set of classifiers was 81.7%. Accuracy as high as 90% was obtained for the test set when training percentage was increased. These results demonstrate that an accurate classifier can be constructed from a large pool of simple classifiers through the use of the statistical ("Normal") classification method described herein.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {127–129},
numpages = {3},
keywords = {image classification, classifier, combined classifiers, binary classification, normal},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096633,
author = {\'{A}vila, Bruno Ten\'{o}rio and Lins, Rafael Dueire and Oliveira, Lamberto},
title = {A New Rotation Algorithm for Monochromatic Images},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096633},
doi = {10.1145/1096601.1096633},
abstract = {The classical rotation algorithm applied to monochromatic images introduces white holes in black areas, making edges uneven and disconnecting neighboring elements. Several algorithms in the literature address only the white hole problem. This paper proposes a new algorithm that solves those three problems, producing better quality images.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {130–132},
numpages = {3},
keywords = {monochromatic image rotation, skew correction},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/3259644,
author = {Gormish, M.},
title = {Session Details: Panel Session},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259644},
doi = {10.1145/3259644},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
numpages = {1},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096635,
author = {Gormish, Michael},
title = {Interaction between Paper and Electronic Documents},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096635},
doi = {10.1145/1096601.1096635},
abstract = {Documents today almost always exist in two forms: paper and electronic. Many documents, especially legacy documents start as paper, but are then scanned and recognized. Other documents are started electronically but then printed for easy reading, annotation, or distribution. Some documents are scanned, operated on electronically, then printed. Often machine readable information, e.g. barcodes or RFID tags are added to paper documents to allow association with the electronic document, or with "meta-data" in some database. Sometimes the ability to go back and forth between paper and electronic forms, round-tripping, is important, other times the two forms are fundamentally different.While the end of paper in the offices has been long predicted, actual volume of printed materials continues to rise. Electronic documents have, in fact, greatly increased the use of paper. This panel addresses making paper more useful in an electronic document world, and making electronic databases deal with paper. There are obvious challenges including scanning paper documents and printing electronic ones, but there are additional opportunities including using paper to summarize and access multimedia documents, and using paper to control electronic actions.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {133},
numpages = {1},
keywords = {classification, scanning, paper manifestation, enterprise document, document analysis, segmentation, machine identifiers, document databases, printing, model},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/3259645,
author = {Vion-Dury, J.-Y.},
title = {Session Details: Document Authoring, Markup and Manipulation 1},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259645},
doi = {10.1145/3259645},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
numpages = {1},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096637,
author = {Haralambous, Yannis and Bella, G\'{a}bor},
title = {Injecting Information into Atomic Units of Text},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096637},
doi = {10.1145/1096601.1096637},
abstract = {This paper presents a new approach to text processing, based on textemes. These are atomic text units generalising the concepts of character and glyph by merging them in a common data structure, together with an arbitrary number of user-defined properties. In the first part, we give a survey of the notions of character and glyph and their relation with Natural Language Processing models, some visual text representation issues and strategies adopted by file formats (SVG, PDF, DVI) and software (Uniscribe, Pango). In the second part we show applications of textemes in various text processing issues: ligatures, variant glyphs and other OpenType-related properties, hyphenation, color and other presentation attributes, Arabic form and morphology, CJK spacing, metadata, etc. Finally we describe how the Omega typesetting system implements texteme processing as an example of a generalised approach to input character stream parsing, internal representation of text, and modular typographic transformations. In the data flow from input to output, whether in memory or through serializations in auxiliary data files, textemes progressively accumulate information that is used by Omega's paragraph builder engine and included in the output DVI file. We show how this additional information increases efficiency of conversions to other file formats such as PDF or SVG. We conclude this paper by presenting interesting potential applications of texteme methods in document engineering.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {134–142},
numpages = {9},
keywords = {PDF, omega, texteme, SVG, character, OpenType, multilingual typesetting, Unicode, glyph},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096638,
author = {Tannier, Xavier and Girardot, Jean-Jacques and Mathieu, Mihaela},
title = {Classifying XML Tags through "Reading Contexts"},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096638},
doi = {10.1145/1096601.1096638},
abstract = {Some tags used in XML documents create arbitrary breaks in the natural flow of the text. This may constitute an impediment to the application of some methods of document engineering. This article introduces the concept of ``reading contexts'', and gives clues to handle it theorically and in practice. This work should notably allow to recognize emphasis tags in a text, to define a new concept of term proximity in structured documents, to improve indexing techniques, and also to open up the way to advanced linguistic analyses of XML corpora.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {143–145},
numpages = {3},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/3259646,
author = {Wiley, A.},
title = {Session Details: Document Searching, Document Annotation, and Document Metadata},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259646},
doi = {10.1145/3259646},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
numpages = {1},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096640,
author = {Beaudoux, Olivier},
title = {XML Active Transformation (EXAcT): Transforming Documents within Interactive Systems},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096640},
doi = {10.1145/1096601.1096640},
abstract = {Stylesheets and batch transformations are the most widely used techniques to transform "abstract" documents into target presentation documents. Despite the recent introduction of incremental transformations, several important features required by interactive systems are yet to be addressed, such as multiple sources (e.g. preferences and resources), multiple targets (e.g. multiple views), source-to-target linking (e.g. interacting with the source via the tar-get), and bidirectional linking (e.g. interacting directly with the target). This paper proposes the use of XML Active Transformations (eXAcT) in order to fulfil these requirements. The eXAcT specification is based on the definition of two new DOM node types, active fragment and anchor, and on a transformation process in-spired from XSLT. Our jaXAT implementation toolkit allows the active transformation of any DOM document into (but not limited to) SVG presentations.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {146–148},
numpages = {3},
keywords = {authoring tools, active transformations, SVG, XML, GUI},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096641,
author = {Huang, Chia-Hsin and Chuang, Tyng-Ruey and Lee, Hahn-Ming},
title = {Prefiltering Techniques for Efficient XML Document Processing},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096641},
doi = {10.1145/1096601.1096641},
abstract = {Document Object Model (DOM) and Simple API for XML (SAX) are the two major programming models for XML document processing. Each, however, has its own efficiency limitation. DOM assumes an in-core representation of XML documents which can be problematic for large documents. SAX needs to scan over the document in a linear manner in order to locate the interesting fragments. Previously, we have used tree-to-table mapping and indexing techniques to help answer structural queries to large, or large collections of, XML documents. In this paper, we generalize the previous techniques into a prefiltering framework where repeated access to large XML documents can be efficiently carried out within the existing DOM and SAX models. The prefiltering framework essentially uses a tiny search engine to locate useful fragments in the target XML documents by approximately executing the user's queries. Those fragments are gathered into a candidate-set XML document, and is returned to the user's DOM- or SAX-based applications for further processing. This results in a practical and efficient model of XML processing, especially when the XML documents are large and infrequently updated, but are frequently being queried.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {149–158},
numpages = {10},
keywords = {two-phased XML processing model, DOM, prefiltering, SAX, structural query},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096642,
author = {Beaudoux, Olivier},
title = {Event Points: Annotating XML Documents for Remote Sharing},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096642},
doi = {10.1145/1096601.1096642},
abstract = {Collaboration is heavily based on sharing documents. However, most groupware toolkits do not directly support document sharing, but rather focus on supporting mechanisms such as remote concurrent access to shared objects. We propose the notion of event point as a single and unified concept for defining sharing capabilities of XML documents and introduce four types of event points for real-time groupware: replication, copy, echo, and synchronization. These event points support such collaborative features as real-time sharing, synchronization, telepointing, localization, and echo. The paper presents the concept of event point, its implementation in the DoPIdom toolkit, and some sample uses in our Sovigo drawing tool.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {159–161},
numpages = {3},
keywords = {XML documents, CSCW toolkit, real-time groupware},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096643,
author = {Vilares, Jes\'{u}s and G\'{o}mez-Rodr\'{\i}guez, Carlos and Alonso, Miguel A.},
title = {Managing Syntactic Variation in Text Retrieval},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096643},
doi = {10.1145/1096601.1096643},
abstract = {Information Retrieval systems are limited by the linguistic variation of language. The use of Natural Language Processing techniques to manage this problem has been studied for a long time, but mainly focusing on English. In this paper we deal with European languages, taking Spanish as a case in point. Two different sources of syntactic information, queries and documents, are studied in order to increase the performance of Information Retrieval systems.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {162–164},
numpages = {3},
keywords = {natural language processing, shallow parsing, information retrieval},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096644,
author = {Choumane, Ali and Blanchon, Herv\'{e} and Roisin, C\'{e}cile},
title = {Integrating Translation Services within a Structured Editor},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096644},
doi = {10.1145/1096601.1096644},
abstract = {Fully automatic machine translation cannot produce high quality translation; Dialog-Based Machine Translation (DB-MT) is the only way to provide authors with a means of translating documents in languages they have not mastered, or do not even know. With such environment, the author must help the system to "understand" the document by means of an interactive disambiguation step. In this paper we study the consequences of integrating the DBMT services within a structured document editor (Amaya). The source document (named edited document) needs a companion document enriched with different data produced during the interactive translation process (question trees, answers of the author, translations). The edited document also needs to be enriched (annotated) in order to enable access to the question trees. The enriched edited document and the companion document have to be synchronized in case the edited document is further updated.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {165–167},
numpages = {3},
keywords = {interactive disambiguation, editing of structured documents, self-explaining document, XML document, DBMT},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/3259647,
author = {Phelps, T.},
title = {Session Details: Document Authoring, Markup and Manipulation 2},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259647},
doi = {10.1145/3259647},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
numpages = {1},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096646,
author = {Quint, Vincent and Vatton, Ir\'{e}ne},
title = {Towards Active Web Clients},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096646},
doi = {10.1145/1096601.1096646},
abstract = {Recent developments of document technologies have strongly impacted the evolution of Web clients over the last fifteen years, but all Web clients have not taken the same advantage of this advance. In particular, mainstream tools have put the emphasis on accessing existing documents to the detriment of a more cooperative usage of the Web. However, in the early days, Web users were able to go beyond browsing and to get more actively involved. This paper presents the main features needed to make Web clients more active and creative tools, by taking advantage of the latest advances of document technology. These features are implemented in Amaya, a user agent that supports several languages from the XML family and integrates seamlessly such complementary functionalities as browsing, editing, publishing, and annotating.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {168–176},
numpages = {9},
keywords = {authoring, XML documents, style languages, compound documents, web user agent},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096647,
author = {Thomas, Peter L. and Brailsford, David F.},
title = {Enhancing Composite Digital Documents Using XML-Based Standoff Markup},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096647},
doi = {10.1145/1096601.1096647},
abstract = {Document representations can rapidly become unwieldy if they try to encapsulate all possible document properties, ranging from abstract structure to detailed rendering and layout.We present a composite document approach wherein an XML-based document representation is linked via a 'shadow tree' of bi-directional pointers to a PDF representation of the same document. Using a two-window viewer any material selected in the PDF can be related back to the corresponding material in the XML, and vice versa. In this way the treatment of specialist material such as mathematics, music or chemistry (e.g. via 'read aloud' or 'play aloud') can be activated via standard tools working within the XML representation, rather than requiring that application-specific structures be embedded in the PDF itself.The problems of textual recognition and tree pattern matching between the two representations are discussed in detail.Comparisons are drawn between our use of a shadow tree of pointers to map between document representations and the use of a code-replacement shadow tree in technologies such as XBL.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {177–186},
numpages = {10},
keywords = {XML, PDF, standoff markup, XBL, MusicXML, composite documents, MathML},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096648,
author = {Norrie, Moira C. and Palinginis, Alexios and Signer, Beat},
title = {Content Publishing Framework for Interactive Paper Documents},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096648},
doi = {10.1145/1096601.1096648},
abstract = {Paper persists as an important medium for documents and this has motivated the development of new technologies for interactive paper that enable actions on paper to be linked to digital actions. A major issue that remains is how to integrate these technologies into the document life cycle and, specifically, how to facilitate the authoring of links between printed documents and digital documents and services. We describe how we have extended a general web publishing framework to support the production of interactive paper documents, thereby integrating paper as a new web channel in a platform for multi-channel access.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {187–196},
numpages = {10},
keywords = {publishing framework, interactive paper},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/3259648,
author = {Harrington, S. J.},
title = {Session Details: Techniques for Document Management and Document Engineering},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259648},
doi = {10.1145/3259648},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
numpages = {1},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096650,
author = {Yacoub, Sherif and Burns, John and Faraboschi, Paolo and Ortega, Daniel and Peiro, Jose Abad and Saxena, Vinay},
title = {Document Digitization Lifecycle for Complex Magazine Collection},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096650},
doi = {10.1145/1096601.1096650},
abstract = {The conversion of large collections of documents from paper to digital formats that are suitable for electronic archival is a complex multi-phase process. The creation of good quality images from paper documents is just one phase. To extract relevant information that they contain, with an accuracy that fits the purpose of target applications, an automated document analysis system and a manual verification/review process are needed. The automated system needs to perform a variety of analysis and recognition tasks in order to reach an accuracy level that minimizes the manual correction effort downstream.This paper describes the complete process and the associated technologies, tools, and systems needed for the conversion of a large collection of complex documents and deployment for online web access to its information rich content. We used this process to recapture 80 years of Time magazines. The historical collection is scanned, automatically processed by advanced document analysis components to extract articles, manually verified for accuracy, and converted in a form suitable for web access. We discuss the major phases of the conversion lifecycle and the technology developed and tools used for each phase. We also discuss results in terms of recognition accuracy.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {197–206},
numpages = {10},
keywords = {preservation of historical content, document engineering, document analysis and understanding, document digitization},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096651,
author = {Yamanaka, Masakazu and Niimura, Kenji and Kamada, Tomio},
title = {A Programming Environment for Demand-Driven Processing of Network XML Data and Its Performance Evaluation},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096651},
doi = {10.1145/1096601.1096651},
abstract = {This paper proposes a programming environment for Java that
processes network XML data in a demand-driven manner to return
quick initial responses. Our system provides a data binding tool
and a tree operation package, and the programmer can easily handle
network XML data as tree-based operations using these facilities.
For efficiency, demand-driven data binding allows the application
to start the processing of a network XML document before the
arrival of the whole data, and our tree operators are also designed
to start the calculation using the initially accessible part of the
input data. Our system uses multithread technology for
implementation with optimization techniques to reduce runtime
overheads. It can return initial responses quickly, and often
shortens the total execution time due to the effects of latency
hiding and the reduction of memory usage. Compared with an ordinary
tree-based approach, our system shows a highly improved response
and a 1-28% reduction of total execution time on the benchmark
programs. It only needs 1-4% runtime overheads against the
event-driven programs.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {207–216},
numpages = {10},
keywords = {multi-threading, demand-driven, XML, data binding},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096652,
author = {Gruhl, Daniel and Meredith, Daniel and Pieper, Jan},
title = {A Case Study on Alternate Representations of Data Structures in XML},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096652},
doi = {10.1145/1096601.1096652},
abstract = {XML provides a universal and portable format for document and data exchange. While the syntax and specification of XML makes documents both human readable and machine parsable, it is often at the expense of efficiency when representing simple data structures.We investigate the ``costs'' associated with XML serialization from several resource perspectives: storage, transport, processing and human readability. These experiments are done within the context of a large text-centric service oriented architecture -- IBM's WebFountain project.We find that for several applications, human readable formats outperform binary equivalents, especially in the area of data size, and that the costs of processing encoded binary data often exceeds that of processing terse human readable formats.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {217–219},
numpages = {3},
keywords = {XML, compression, data structures, serialization, WebFountain},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096653,
author = {Boyette, Neil and Krishna, Vikas and Srinivasan, Savitha},
title = {Eclipse Modeling Framework for Document Management},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096653},
doi = {10.1145/1096601.1096653},
abstract = {The lifecycle of document management applications typically comprises a set of loosely coupled subsystems that provide capture, index, search, workflow, fulfillment and archival features. However, there exists no standard model for composing these elements together to instantiate a complete application. Therefore, every application invariably incorporates custom application code to provide the linkages between each of these loosely coupled subsystems. This paper proposes a model-based approach to instantiating document management applications. An Eclipse Modeling Framework (EMF) based model is used to formalize the variable elements in the document management applications. The modeling tool supports the instantiation of an EMF model for every new application and supports the generation of runtime artifacts - this includes code, XML configurations, scripts and business logic. This approach to creating new instances of document management applications with a formal EMF model has been validated with a real-world document management application.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {220–222},
numpages = {3},
keywords = {logistics, modeling, eclipse, framework, business transformation, DMS, documents, EMF, IT, process, ROI},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/3259649,
author = {Carr, L.},
title = {Session Details: Demonstrations},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259649},
doi = {10.1145/3259649},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
numpages = {1},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096655,
author = {Saxena, Vinay and Yacoub, Sherif},
title = {GroundTruth Tools &amp; Technology: Applications in Real World},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096655},
doi = {10.1145/1096601.1096655},
abstract = {The process of creating digital archive from paper based document is gaining popularity. Automated systems/frameworks for document analysis techniques have been developed, but still lack in achieving the required accuracy goals in terms of text, article identification etc. Rendering problems, such as missing graphical components, wrong reading ordering in multi columned journals/magazine, missing indentation and broken text lines, hyphenation issues, are basically due to poor layout information extracted from the scanned document during the OCR process. Also lacking are the tools to take the output of these processes and be able to create highly accurate content with associated metadata from the original. The term "Ground Truth" in the current context is used to refer to the process (automatic and manual collectively) by which we ensure that the end result of the process are highly accurate and complete rich text content (articles, papers, etc) generated from the original scanned version of content.We present to the audience PerfectDoc - A suite of tools for manual GroundTruthing. The suite consist of tools to create highly accurate GroundTruth, GT editors and tools to take this data and deliver output suitable for web based viewing.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {223–224},
numpages = {2},
keywords = {document digitization, document engineering, document analysis and understanding, preservation of historical content},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096656,
author = {Jacquin, Thierry and Fambon, Olivier and Chidlovskii, Boris},
title = {A Web-Based Document Harmonization and Annotation Chain: From PDF to RDF},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096656},
doi = {10.1145/1096601.1096656},
abstract = {We propose a demonstration of a Web-based document harmonization and annotation chain developed within the VIKEF integrated project. The chain integrates a combination of Web Services in order to access, harmonize and semantically annotate remote document collections. Annotations are then mapped onto RDF descriptions that serve as a basis for building semantic-enabled services to support community processes.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {225–226},
numpages = {2},
keywords = {document annotation, PDF, web services, RDF},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096657,
author = {Lumley, John and Gimson, Roger and Rees, Owen},
title = {A Demonstration of the Document Description Framework},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096657},
doi = {10.1145/1096601.1096657},
abstract = {The Document Description Framework (DDF) [1] is a representation for variable-data documents, designed to support very high flexibility in the type and extent of variation, considerably beyond 'copy-hole' or flow-based mechanisms of existing formats and tools. This demonstration shows how i) DDF documents can be evaluated and merged to construct complex multi-stage documents, ii) the layout capabilities can be extended flexibly and iii) how they may be created and edited within a GUI-based environment.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {227–228},
numpages = {2},
keywords = {SVG, document construction, XSLT, XML, functional programming},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096658,
author = {Miles-Board, Timothy and Woukeu, Arouna and Carr, Leslie and Wills, Gary and Hall, Wendy},
title = {Bringing the Semantic Web to the Office Desktop},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096658},
doi = {10.1145/1096601.1096658},
abstract = {Many Semantic Web applications address the needs of human readers of the Web (e.g. searching, annotating), but these technologies can also address the needs of human writers of the Web. The WiCK project has explored the application of knowledge bases and services to the Office desktop, in order to assist document production, culminating in the WiCKOffice environment. This aim of this demonstration is to showcase the most recent offshoot of the WiCKOffice development, WiCKLite: a lightweight component for connecting knowledge services to document templates in order to deliver targeted assistance to end users.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {229–230},
numpages = {2},
keywords = {knowledge writing, smart tags, semantic web},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096659,
author = {Xing, Guangming and Malla, Chaitanya R. and Ernest, Andrew},
title = {XIS: An XML Document Integration System},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096659},
doi = {10.1145/1096601.1096659},
abstract = {We describe XIS, an XML document integration system. The system is based on an algorithm that computes the top-down edit distance between an XML document and a schema. The complexity of the algorithm is t x s x log s, where t is the size of the document and s is the size of the schema.The system includes a GUI that allows the user to visualize the operations performed on the XML document. Synthesized and real data-sets will be used to show the efficiency and efficacy of the system.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {231–232},
numpages = {2},
keywords = {tree grammar, XML, document integration},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096660,
author = {Bagley, Steven R. and Brailsford, David F.},
title = {The COG Scrapbook},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096660},
doi = {10.1145/1096601.1096660},
abstract = {The COG Scrapbook technology is an attempt to by the authors to convert their COG technology into a usable suite of software for typical end-users, rather than Document Engineering specialists.This demonstration illustrates the four major components of this software suite, the COG Manipulator; COG Encapsulator; COG Extractor; and COG Creator. These four components provide the user with the tools required to manipulate COG PDF documents within the Adobe Acrobat environment.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {233–234},
numpages = {2},
keywords = {COGs, PDF, graphic objects, FormXObject},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096661,
author = {Mahnke, Achim},
title = {The MMiSS Repository},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096661},
doi = {10.1145/1096601.1096661},
abstract = {The mmiss repository is a system for storing and versioning structured documents in a multi-author environment. In contrast to general purpose versioning systems like CVS, versioning and merging is based on the logical structure of documents. New functionalities like the support for developing ontologies along with documents and providing variants as a means for adaptation are introduced. They aim at the development of higher level functions on documents, like change management and consistency checking.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {235–236},
numpages = {2},
keywords = {variants, version control, ontology, repository, consistency, document management},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096662,
author = {Lanfranchi, Vitaveska and Ciravegna, Fabio and Moore, Phil and Petrelli, Daniela},
title = {Document Editing and Browsing in AKTiveDoc},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096662},
doi = {10.1145/1096601.1096662},
abstract = {In this demo paper, we present AKTiveDoc, a tool for supporting sharing and reuse of knowledge in document production (e.g. writing) and use (e.g. reading).},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {237–238},
numpages = {2},
keywords = {interfaces, free-text annotation, ontology-driven annotation, knowledge suggestion, semi-automatic annotation, semantic web},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

@inproceedings{10.1145/1096601.1096663,
author = {Lins, Rafael Dueire and \'{A}vila, Bruno Ten\'{o}rio},
title = {BigBatch: A Toolbox for Monochromatic Documents},
year = {2005},
isbn = {1595932402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1096601.1096663},
doi = {10.1145/1096601.1096663},
abstract = {BigBatch is a tool designed to automatically process thousands of monochromatic images of documents generated by production line scanners. It removes noisy borders, checks and corrects orientation, calculates and compensates the skew angle, crops the image standardizing document sizes, and finally compresses it according to user defined file format. BigBatch encompasses the best and recently developed algorithms for such kind of document images. BigBatch may work either in standalone or operator assisted modes. Besides that, BigBatch in standalone mode is able to process in clusters of workstations.},
booktitle = {Proceedings of the 2005 ACM Symposium on Document Engineering},
pages = {239–240},
numpages = {2},
keywords = {skew detection, document processing, border removal, monochromatic images, image processing, orientation},
location = {Bristol, United Kingdom},
series = {DocEng '05}
}

