@inproceedings{10.1145/3253505,
author = {Antonacopoulos, Apostolos},
title = {Session Details: Keynote Address},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253505},
doi = {10.1145/3253505},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
numpages = {1},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860561,
author = {Conteh, Aly Kaloa},
title = {Exploring the World's Knowledge in the Digital Age},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860561},
doi = {10.1145/1860559.1860561},
abstract = {The advent of the digital age has brought about a dramatic change in the techniques for the dissemination of, and access to, historical documents. For today's researcher, the internet is a primary source of the tools and information that supports their research. The British Library (BL) provides world class information services to the academic, business, research and scientific communities and offers unparalleled access to the world's largest and most comprehensive research collection. The British Library's collections include 150 million items from every era of written human history beginning with Chinese oracle bones dating from 300 BC, right up to the latest e-journals. For almost 20 years the British Library has been engaged in transforming physical collection items into digital form enabling access to that content over the World Wide Web.In this talk, I describe the depth and range of the collections that are held at the British Library. I will outline the digital conversion process we undertake, including our current solutions for digital formats, metadata standards, providing access to the content and preserving the digital outputs in perpetuity. Finally, as the British Library undertakes projects that will digitise millions of pages of historical text based items, I will look at the challenges we face, such as storage requirements and enhancing resource discovery, and how are we addressing those challenges.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {1–2},
numpages = {2},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/3253506,
author = {Munson, Ethan},
title = {Session Details: Systems},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253506},
doi = {10.1145/3253506},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
numpages = {1},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860563,
author = {Sojka, Petr and Hatlapatka, Radim},
title = {Document Engineering for a Digital Library: PDF Recompression Using JBIG2 and Other Optimizations of PDF Documents},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860563},
doi = {10.1145/1860559.1860563},
abstract = {This paper describes several innovative document transformations and tools that have been developed in the process of building the Digital Mathematical Library DML-CZ http://dml.cz. The main result presented in this paper is our PDF re-compression tools developed using a jbig2enc library. Together with other programs, especially pdfsizeopt.py by P\'{e}ter Szab\'{o}, we have managed to decrease PDF storage size and transmission needs be 62%: using both programs we reduced the size of the original PDFs to 38%.This paper briefly describes other approaches and tools developed while creating the digital library. The batch digital signature stamper, the document similarity metrics which uses four different methods, a [meta]data validation process and some math OCR tools represent some of the main byproducts of this project. These ways of document engineering, together with Google Scholar indexing optimizations have led to the success of serving digitized and born-digital scientific math documents to the public in DML=CZ, and will be employed also in the project of The European Digital Mathematics Library, EuDML.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {3–12},
numpages = {10},
keywords = {categorization, layout), authoring tools and systems, classification, structure, digital mathematical library, character recognition, document presentation (typography, digitisation workflow, layout and content analysis, formatting, representations/standards},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860564,
author = {Lecarpentier, Jean-Marc and Bazin, Cyril and Le Crosnier, Herv\'{e}},
title = {Multilingual Composite Document Management Framework for the Internet: An FRBR Approach},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860564},
doi = {10.1145/1860559.1860564},
abstract = {Most Web Content is nowadays published with Content Management Systems (CMS). As outlined in this paper, existing tools lack some functionalities to create and manage multilingual composite documents efficiently. In another domain, the International Federation of Library Associations and Institutions (IFLA) published the Functional Requirements for Bibliographic Records (FRBR) to lay the foundation for cataloguing documents and their various versions, translations and formats, setting the focus on the intellectual work.Using the FRBR concepts as guidelines, we introduce a tree-based model to describe relations between a digital document's various versions, translations and formats. Content negotiation and relationships between documents at the highest level of the tree allow composite documents to be rendered according to a user's preferences (e.g. language, user agent...). The proposed model has been implemented and validated within the Sydonie framework, a research and industrial project. Sydonie implements our model in a CMS-like tool to imagine new ways to create, edit and publish multilingual composite documents.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {13–16},
numpages = {4},
keywords = {document management system, composite documents, multilingual documents},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/3253507,
author = {King, Peter},
title = {Session Details: Authoring},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253507},
doi = {10.1145/3253507},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
numpages = {1},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860566,
author = {Fag\'{a}, Roberto and Motti, Vivian Genaro and Cattelan, Renan Gon\c{c}alves and Teixeira, Cesar Augusto Camillo and Pimentel, Maria da Gra\c{c}a Campos},
title = {A Social Approach to Authoring Media Annotations},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860566},
doi = {10.1145/1860559.1860566},
abstract = {End-user generated content is responsible for the success of several collaborative applications, as it can be noted in the context of the web. The collaborative use of some of these applications is made possible, in many cases, by the availability of annotation features which allow users to include commentaries on each other's content. In this paper we first discuss the opportunity of defining vocabularies that allow third-party applications to integrate annotations to end-user generated documents, and present a proposal for such a vocabulary. We then illustrate the usefulness of our proposal by detailing a tool which allows users to add multimedia annotations to end-user generated video content.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {17–26},
numpages = {10},
keywords = {open vocabulary, multimodal, watch-and-comment, video, annotation, youtube, collaboration},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860567,
author = {Laiola Guimar\~{a}es, Rodrigo and Cesar, Pablo and Bulterman, Dick C.A.},
title = {Creating and Sharing Personalized Time-Based Annotations of Videos on the Web},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860567},
doi = {10.1145/1860559.1860567},
abstract = {This paper introduces a multimedia document model that can structure community comments about media. In particular, we describe a set of temporal transformations for multimedia documents that allow end-users to create and share personalized timed-text comments on third party videos. The benefit over current approaches lays in the usage of a rich captioning format that is not embedded into a specific video encoding format. Using as example a Web-based video annotation tool, this paper describes the possibility of merging video clips from different video providers into a logical unit to be captioned, and tailoring the annotations to specific friends or family members. In addition, the described transformations allow for selective viewing and navigation through temporal links, based on end-users' comments. We also report on a predictive timing model for synchronizing unstructured comments with specific events within a video(s). The contributions described in this paper bring significant implications to be considered in the analysis of rich media social networking sites and the design of next generation video annotation tools.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {27–36},
numpages = {10},
keywords = {smiltext, temporal hyperlinks, video annotation tools, timed end-user comments, document transformations},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860568,
author = {Vega-Oliveros, Didier Augusto and Martins, Diogo Santana and Pimentel, Maria da Gra\c{c}a Campos},
title = {"This Conversation Will Be Recorded": Automatically Generating Interactive Documents from Captured Media},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860568},
doi = {10.1145/1860559.1860568},
abstract = {Synchronous communication tools allow remote users to collaborate by exchanging text, audio, images or video messages in synchronous sessions. In some scenarios, it is paramount that collaborative synchronous sessions be recorded for later review. In particular in the case of web conferencing tools, the approach usually adopted for recording a meeting is to generate a linear video with the content of the exchanged media. Such approach limits the review of a meeting to users watching a video using traditional timeline-based video controls. In this work we advocate that interactive multimedia documents can be generated automatically as a result of capturing a synchronous session. We outline our approach presenting a case of study involving remote communication, and detail the generation of a multimedia document by means of operators focusing on the interaction among the collaborating users.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {37–40},
numpages = {4},
keywords = {automatic authoring, interactive video},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/3253508,
author = {Brailsford, David},
title = {Session Details: Tools},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253508},
doi = {10.1145/3253508},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
numpages = {1},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860570,
author = {Simske, Steven J. and Sturgill, Margaret and Adams, Guy and Everest, Paul},
title = {Document Imaging Security and Forensics Ecosystem Considerations},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860570},
doi = {10.1145/1860559.1860570},
abstract = {Much of the focus in document security tends to be on the deterrent -- the physical (printed, manufactured) item placed on a document, often used for routing in addition to security purposes. Hybrid (multiple) deterrents are not always reliably read by a single imaging device, and so a single device generally cannot simultaneously provide overall document security. We herein show how a relatively simple deterrent can be used in combination with multiple imaging devices to provide document security. In this paper, we show how these devices can be used to classify the printing technology used, a subject of importance for counterfeiter identification as well as printer quality control. Forensic-level imaging is also useful in preventing repudiation and forging, while mobile and/or simple scanning can be used to prevent tampering -- propitiously in addition to providing useful, non-security related, capabilities such as document routing (track and trace) and workflow association.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {41–50},
numpages = {10},
keywords = {3D bar codes, forensics, high-resolution imaging, color tiles, document fraud, security},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860571,
author = {Tseng, Lendle and Kuo, Yue-Sun and Lee, Hsiu-Hui and Chen, Chuen-Liang},
title = {XUIB: XML to User Interface Binding},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860571},
doi = {10.1145/1860559.1860571},
abstract = {Separated from GUI builders, existing GUI building tools for XML are complex systems that duplicate many functions of GUI builders. They are specialized in building GUIs for XML data only. This puts unnecessary burden on developers since an application usually has to handle both XML and non-XML data. In this paper, we propose a solution that separates the XML-to-GUI bindings from the construction of the GUIs for XML, and concentrates on the XML-to-GUI bindings only. Cooperating with a GUI builder, the proposed system can support the construction of the GUIs for XML as GUI building tools for XML can. Furthermore, the proposed mechanism is neutral to GUI builders and toolkits. As a result, multiple GUI builders and toolkits can be supported by the proposed solution with moderate effort. Our current implementation supports two types of GUI platforms: Java/Swing and Web/Html.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {51–60},
numpages = {10},
keywords = {XML, user interface, GUI, GUI builders, XML authoring},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860572,
author = {Quint, Vincent and Roisin, C\'{e}cile and Sire, St\'{e}phane and Vanoirbeek, Christine},
title = {From Templates to Schemas: Bridging the Gap between Free Editing and Safe Data Processing},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860572},
doi = {10.1145/1860559.1860572},
abstract = {In this paper we present tools that provide an easy way to edit XML content directly on the web, with the usual benefit of valid XML content. These tools make it possible to create content targeted for lightweight web applications. Our approach uses (1) the XTiger template language, (2) the AXEL Javascript library for authoring structured XML content and (3) XSLT transformations for generating XML schemas against which the XML content can be validated. Template-driven editing allows any web user to easily enter content while schemas make sure applications can safely process this content.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {61–64},
numpages = {4},
keywords = {document language, web editing, XML, document authoring},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860573,
author = {Bagley, Steven R.},
title = {Lessons from the Dragon: Compiling PDF to Machine Code},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860573},
doi = {10.1145/1860559.1860573},
abstract = {Page Description Languages, such as PDF or PostScript, describe the page as a series of graphical operators, which are then imaged to draw the page content. An interpreter executes these operators one-by-one every time the page is rendered into a viewable form. Typically, this interpreter takes the form of a tokenizer that splits the page description into the separate operators. Various subroutines are then called depending on which tokens are encountered. This process is analogous to instruction execution at the heart of a CPU: the CPU fetches machine code instructions from memory and dispatches them to the various parts of the chip as necessary.In this paper, we show that it is possible to compile a page description directly into machine code, bypassing the need to interpret the page description. This can bring a speed increase in PDF rendering -- particularly important on low-power devices -- and could also help increase document accessibility},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {65–68},
numpages = {4},
keywords = {PDF, page description languages, interpretation, compilation, document format},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/3253509,
author = {Balinsky, Helen},
title = {Session Details: E-Books},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253509},
doi = {10.1145/3253509},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
numpages = {1},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860575,
author = {Battle, Steven A. and Bernius, Matthew},
title = {Transquotation in EBooks},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860575},
doi = {10.1145/1860559.1860575},
abstract = {This paper describes the use of transquotation in eBooks to support collaborative publishing. Users are able to prepare content on a wiki and assemble this into a publishable eBook. The eBook content should remain connected to its origin so that as the wiki content changes, the eBook may be revised accordingly. The problems raised in creating a transquoting eBook editing environment include transforming wiki content into a suitable presentation, mapping selections back to plain-text, and mapping existing selections into the presentation space for review.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {69–72},
numpages = {4},
keywords = {transquotation, eBooks, transclusion},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860576,
author = {Marinai, Simone and Marino, Emanuele and Soda, Giovanni},
title = {Table of Contents Recognition for Converting PDF Documents in E-Book Formats},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860576},
doi = {10.1145/1860559.1860576},
abstract = {We describe one tool for Table of Content (ToC) identification and recognition from PDF books. This task is part of ongoing research on the development of tools for the semi-automatic conversion of PDF documents in the Epub format that can be read on several E-book devices. Among various sub-tasks, the ToC extraction and recognition is particularly useful for an easy navigation of book contents.The proposed tool first identifies the ToC pages. The bounding boxes of ToC titles in the book body are subsequently found in order to add suitable links in the Epub ToC. The proposed approach is tolerant to discrepancies between the ToC text and the corresponding titles. We evaluated the tool on several open access books edited by University Presses that are partner of the OAPEN EcontentPlus project},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {73–76},
numpages = {4},
keywords = {e-book conversion, table of content, PDF},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/3253510,
author = {Bulterman, Dick},
title = {Session Details: Editing},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253510},
doi = {10.1145/3253510},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
numpages = {1},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860578,
author = {Thao, Cheng and Munson, Ethan V.},
title = {Using Versioned Tree Data Structure, Change Detection and Node Identity for Three-Way XML Merging},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860578},
doi = {10.1145/1860559.1860578},
abstract = {XML has become the standard document representation for many popular tools in various domains. When multiple authors collaborate to produce a document, they must be able to work in parallel and periodically merge their efforts into a single work. While there exist a small number of three-way XML merging tools, their performance could be improved in several areas and they lack any form of user interface for resolving conflicts.In this paper, we present an implementation of a three-way XML merge algorithm that is faster, uses less memory and is more precise than existing tools. It uses a specialized versioning tree data structure that supports node identity and change detection. The algorithm applies the traditional three-way merge found in GNU diff3 to the children of changed nodes. The editing operations it supports are addition, deletion, update, and move. A graphical interface for visualizing and resolving conflicts is also provided. An evaluation experiment was conducted comparing the proposed algorithm with three other tools on randomly generated XML data.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {77–86},
numpages = {10},
keywords = {three-way merge, XML, document trees, versioning system},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860579,
author = {Jansen, Jack and Cesar, Pablo and Bulterman, Dick C.A.},
title = {A Model for Editing Operations on Active Temporal Multimedia Documents},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860579},
doi = {10.1145/1860559.1860579},
abstract = {Inclusion of content with temporal behavior in a structured document leads to such a document gaining temporal semantics. If we then allow changes to the document during its presentation, this brings with it a number of fundamental issues that are related to those temporal semantics. In this paper we study modifications of active multimedia documents and the implications of those modifications for temporal consistency. Such modifications are becoming increasingly important as multimedia documents move from being primarily a standalone presentation format to being a building block in a larger application.We present a categorization of modification operations, where each category has distinct consistency and implementation implications for the temporal semantics. We validate the model by applying it to the SMIL language, categorizing all possible editing operations. Finally, we apply the model to the design of a teleconferencing application, where multimedia composition is only a small component of the whole application, and needs to be reactive to the rest of the system.The primary contribution of this paper is the development of a temporal editing model and a general analysis which we feel can help application designers to structure their applications such that the temporal impact of document modification can be minimized.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {87–96},
numpages = {10},
keywords = {application design, multimedia, dynamic transformations, declarative languages},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860580,
author = {Autexier, Serge and M\"{u}ller, Normen},
title = {Semantics-Based Change Impact Analysis for Heterogeneous Collections of Documents},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860580},
doi = {10.1145/1860559.1860580},
abstract = {An overwhelming amount of documents is produced and changed every day in most areas of our everyday life, such as, for instance, business, education, research or administration. The documents are seldom isolated artifacts but are related to other documents. Therefore changing one document possibly requires adaptations to other documents.Although dedicated tools may provide some assistance when changing documents, they often ignore other documents or documents of a different type. To resolve that discontinuity, we present a framework that embraces existing document types and supports the declarative specification of semantic annotation and propagation rules inside and across documents of different types, and on which basis we define change impact analysis for heterogeneous collections of documents.The framework is implemented in the tool GMoC which can be used to semantically annotate collections of documents and to analyze the impacts of changes made in different documents of a collection.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {97–106},
numpages = {10},
keywords = {graph rewriting, document collections, change impact analysis, document management, semantics},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860581,
author = {Beaudoux, Olivier and Blouin, Arnaud},
title = {Linking Data and Presentations: From Mapping to Active Transformations},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860581},
doi = {10.1145/1860559.1860581},
abstract = {Modern GUI toolkits, and especially RIA ones, propose the concept of binding to dynamically link domain data and their presentations. Bindings are very simple to use for predefined graphical components. However, they remain dependent on the GUI platform, are not as expressive as transformation languages, and require specific coding when designing new graphical components. A solution to such issues is to use active transformations: an active transformation is a transformation that dynamically links source data to target data. Active transformations are however complex to write and/or to process. In this paper, we propose the use of the AcT framework that consists of: a platform-independent mapping language that masks the complexity of active transformations; a graphical mapping editor; and an implementation on the .NET platform.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {107–110},
numpages = {4},
keywords = {mapping, model driven engineering, active transformation},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860582,
author = {Chao, Hui and Tretter, Daniel R. and Zhang, Xuemei and Atkins, C. Brian},
title = {Blocked Recursive Image Composition with Exclusion Zones},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860582},
doi = {10.1145/1860559.1860582},
abstract = {Photo collages are a popular and powerful storytelling mechanism. They are often enhanced with background artwork that sets the theme for the story. However, layout algorithms for photo collage creation typically do not take this artwork into account, which can result in collages where photos overlay important artwork elements. To address this, we extend our previous Blocked Recursive Image Composition (BRIC) method to allow any number of photos to be automatically arranged around preexisting exclusion zones on a canvas (exBRIC). We first generate candidate binary splitting trees to partition the canvas into regions that accommodate both photos and exclusion zones. We use a Cassowary constraint solver to ensure that the desired exclusion zones are not covered by photos. Finally, photo areas, exclusion zones and layout symmetry are evaluated to select the best candidate. This method provides flexible, dynamic and integrated photo layout with background artwork.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {111–114},
numpages = {4},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/3253511,
author = {Marinai, Simone},
title = {Session Details: Document Systems},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253511},
doi = {10.1145/3253511},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
numpages = {1},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860584,
author = {Balinsky, Helen Y. and Simske, Steven J.},
title = {Differential Access for Publicly-Posted Composite Documents with Multiple Workflow Participants},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860584},
doi = {10.1145/1860559.1860584},
abstract = {A novel mechanism for providing and enforcing differential access control for publicly-posted composite documents is proposed. The concept of a document is rapidly changing: individual file-based, traditional formats can no longer accommodate the required mixture of differently formatted parts: individual images, video/audio clips, PowerPoint presentations, html-pages, Word documents, Excel spreadsheets, pdf files, etc. Multi-part composite documents are created and managed in complex workflows, with participants including external consultants, partners and customers distributed across the globe, with many no longer contained within one monolithic secure environment. Distributed over non-secure channels, these documents carry different types of sensitive information: examples include (a) an enterprise pricing strategy for new products, (b) employees' personal records, (c) government intelligence, and (d) individual medical records. A central server solution is often hard or impossible to create and maintain for ad-hoc workflows. Thus, the documents are often circulated between workflow participants over traditional, low security e-mails, placed on shared drives, or exchanged using CD/DVD or USB. The situation is more complicated when multiple workflow participants need to contribute to various parts of such a document with different access levels: for example, full editing rights, read-only, reading of some parts only, etc., for different users. We propose a full scale differential access control approach, enabling public posting of composite documents, to address these concerns.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {115–124},
numpages = {10},
keywords = {composite document, document security, policy, access control},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860585,
author = {Truran, Mark and Georg, Gersende and Cavazza, Marc and Zhou, Dong},
title = {Assessing the Readability of Clinical Documents in a Document Engineering Environment},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860585},
doi = {10.1145/1860559.1860585},
abstract = {Previous work has established that specific linguistic markers present in specialised medical documents (clinical guidelines) can be used to support their automatic structuring within a document engineering environment. This technique is commonly used by the French Health Authority (la Haute Autorite de Sante) during elaboration of clinical guidelines to improve the quality of the final document. In this paper, we explore the readability of clinical guidelines. We discuss a structural measure of document readability that exploits the ratio between these linguistic markers (deontic structures) and the remainder of the text. We describe an experiment in which a corpus of 10 French clinical guidelines is scored for structural readability. We correlate these scores with measures of textual cohesion (computed using latent semantic analysis) and the results of a readability survey performed by a panel of domain experts. Our results suggest an association between the density of deontic structures in a clinical guideline and its overall readability. This implies that certain generic readability measures can henceforth be utilised in our document engineering environment.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {125–134},
numpages = {10},
keywords = {latent semantic analysis, readability, cohesion, medical document processing, LSA},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860586,
author = {Ollis, James A. and Brailsford, David F. and Bagley, Steven R.},
title = {Optimized Reprocessing of Documents Using Stored Processor State},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860586},
doi = {10.1145/1860559.1860586},
abstract = {Variable Data Printing (VDP) allows customised versions of material such as advertising flyers to be readily produced. However, VDP is often extremely demanding of computing resources because, even when much of the material stays invariant from one document instance to the next, it is often simpler to re-evaluate the page completely rather than identifying just the portions that vary.In this paper we explore, in an XML/XSLT/SVG workflow and in an editing context, the reduction of the processing burden that can be realised by selectively reprocessing only the variant parts of the document. We introduce a method of partial re-evaluation that relies on re-engineering an existing XSLT parser to handle, at each XML tree node, both the storage and restoration of state for the underlying document processing framework. Quantitative results are presented for the magnitude of the speed-ups that can be achieved.We also consider how changes made through an appearance-based interactive editing scheme for VDP documents can be automatically reflected in the document view via optimised XSLT re-evaluation of sub-trees that are affected either by the changed script or by altered data.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {135–138},
numpages = {4},
keywords = {document authoring, XSLT, partial re-evaluation, document editing, VDP, SVG, variable data documents},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860587,
author = {Simske, Steven J. and Balinsky, Helen},
title = {APEX: Automated Policy Enforcement EXchange},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860587},
doi = {10.1145/1860559.1860587},
abstract = {The changing nature of document workflows, document privacy and document security merit a new approach to the enforcement of policy. We propose the use of automated means for enforcing policy, which provides advantages for compliance and auditing, adaptability to changes in policy, and compatibility with a cloud-based exchange. We describe the Automated Policy Enforcement eXchange (APEX) software system, which consists of: (1) a policy editor, (2) a policy server, (3) a local daemon on every PC/laptop to maintain local secure up-to-date storage and policy, and (4) local (policy-enforcing) wrappers to capture document-handling user actions such as document export, e-mail, print, edit and save. During the performance of relevant incremental change, or other user-elicited action, on a composite document, the document and its metadata are scanned for salient policy eliciting terms (PETs). The document is then partitioned based on relevant policies and the security policy for each part is determined. If the document contains no PETs, then the user-initiated actions are allowed; otherwise, alternative actions are suggested, including: (a) encryption, (b) redirecting to a secure printer and requiring authorization (e.g. PIN) for printing, and (c) disallowing printing until specific sensitive data is removed.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {139–142},
numpages = {4},
keywords = {document systems, document system components, text analysis, policy server, policy editor, policy, security},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/3253512,
author = {Lumley, John},
title = {Session Details: Analysis},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253512},
doi = {10.1145/3253512},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
numpages = {1},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860589,
author = {Cutter, Michael Patrick and Beusekom, Joost van and Shafait, Faisal and Breuel, Thomas Michael},
title = {Unsupervised Font Reconstruction Based on Token Co-Occurrence},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860589},
doi = {10.1145/1860559.1860589},
abstract = {High quality conversions of scanned documents into PDF usually either rely on full OCR or token compression. This paper describes an approach intermediate between those two: it is based on token clustering, but additionally groups tokens into candidate fonts. Our approach has the potential of yielding OCR-like PDFs when the inputs are high quality and degrading to token based compression when the font analysis fails, while preserving full visual fidelity. Our approach is based on an unsupervised algorithm for grouping tokens into candidate fonts. The algorithm constructs a graph based on token proximity and derives token groups by partitioning this graph. In initial experiments on scanned 300 dpi pages containing multiple fonts, this technique reconstructs candidate fonts with 100% accuracy.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {143–150},
numpages = {8},
keywords = {token co-occurrence graph partitioning, candidate fonts, font reconstruction, token compression},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860590,
author = {Spengler, Alex and Gallinari, Patrick},
title = {Document Structure Meets Page Layout: Loopy Random Fields for Web News Content Extraction},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860590},
doi = {10.1145/1860559.1860590},
abstract = {Web content extraction is concerned with the automatic identification of semantically interesting web page regions. To generalize to pages from unknown sites, it is crucial to exploit not only the local characteristics of a particular web page region, but also the rich interdependencies that exist between the regions and their latent semantics. We therefore propose a loopy conditional random field which combines semantic intra-page dependencies derived from both document structure and page layout, uses a realistic set of local and relational features and is efficiently learnt in the tree-based reparameterization framework. The results of our empirical analysis on a corpus of real-world news web pages from 177 distinct sites with multiple annotations on DOM node level demonstrate that our combination of document structure and layout-driven interdependencies leads to a significant error reduction on the semantically interesting regions of a web page.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {151–160},
numpages = {10},
keywords = {web content extraction, loopy conditional random fields, news600 data set, tree-based reparameterization},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860591,
author = {Slimane, Fouad and Kanoun, Slim and Alimi, Adel M. and Hennebert, Jean and Ingold, Rolf},
title = {Comparison of Global and Cascading Recognition Systems Applied to Multi-Font Arabic Text},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860591},
doi = {10.1145/1860559.1860591},
abstract = {A known difficulty of Arabic text recognition is in the large variability of printed representation from one font to the other. In this paper, we present a comparative study between two strategies for the recognition of multi-font Arabic text. The first strategy is to use a global recognition system working independently on all the fonts. The second strategy is to use a so-called cascade built from a font identification system followed by font-dependent systems. In order to reach a fair comparison, the feature extraction and the modeling algorithms based on HMMs are kept as similar as possible between both approaches. The evaluation is carried out on the large and publicly available APTI (Arabic Printed Text Image) database with 10 different fonts. The results are showing a clear advantage of performance for the cascading approach. However, the cascading system is more costly in terms of cpu and memory.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {161–164},
numpages = {4},
keywords = {APTI, font recognition, text recognition, GMM, HMM},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860592,
author = {Lim, Suk Hwan and Zheng, Liwei and Jin, Jianming and Hou, Huiman and Fan, Jian and Liu, Jerry},
title = {Automatic Selection of Print-Worthy Content for Enhanced Web Page Printing Experience},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860592},
doi = {10.1145/1860559.1860592},
abstract = {The user experience of printing web pages has not been very good. Web pages typically contain contents that are not print-worthy or informative such as side bars, footers, headers, advertisements, and auxiliary information for further browsing. Since the inclusion of such contents degrades the web printing experience, we have developed a tool that first selects the main part of the web page automatically and then allows users to make adjustments. In this paper, we describe the algorithm for selecting the main content automatically during the first pass. The web page is first segmented into several coherent areas or blocks using our web page segmentation method that clusters content based on the affinity values between basic elements. The relative importance values for the segmented blocks are computed using various features and the main content is extracted based on the constraint of one DOM (Document Object Model) sub-tree and high important scores. We evaluated our algorithm on 65 web pages and computed the accuracy based on area of overlap between the ground truth and the extracted result of the algorithm.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {165–168},
numpages = {4},
keywords = {web page layout analysis, segmentation, web page printing, block importance},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/3253513,
author = {Simske, Steven},
title = {Session Details: Creation/Printing},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253513},
doi = {10.1145/3253513},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
numpages = {1},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860594,
author = {Bilauca, Mihai and Healy, Patrick},
title = {A New Model for Automated Table Layout},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860594},
doi = {10.1145/1860559.1860594},
abstract = {In this paper we consider the table layout problem. We present a combinatorial optimization modeling method for the table layout optimization problem, the problem of minimizing a table's height subject to it fitting on a given page (width). We present two models of the problem and report on their evaluation.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {169–176},
numpages = {8},
keywords = {constrained optimization, table layout},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860595,
author = {Giannetti, Fabio and Dispoto, Gary and Lins, Rafael Dueire and Silva, Gabriel de Fran\c{c}a Pereira e and Cabeda, Alexis},
title = {PDF Profiling for B&amp;W versus Color Pages Cost Estimation for Efficient on-Demand Book Printing},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860595},
doi = {10.1145/1860559.1860595},
abstract = {Today, the way books, magazines and newspapers are published is undergoing a democratic revolution. Digital Presses have enabled the on-demand model, which provides individuals with the opportunity to produce and publish their own books with very low upfront cost. With these new markets, opportunities, and challenges have arisen. In a traditional environment, black-and-white and color pages were printed using different presses. Later on, the book was assembled combining the pages accordingly. In a digital workflow all the pages are printed with the same press, although the page cost varies significantly between color and b/w pages. Having an accurate printing cost profiler for pdf-files is fundamental for the print-on-demand business, as jobs often have a mix of color and b/w pages. To meet the expectations of some of HP customers in the large Print Service Providers (PSPs) business, a profiler was developed which yielded a reasonable cost estimate. The industrial use of such a tool showed some discrepancies between estimated and printer log, however. The new profiler presented herein provides a more accurate account of pdf jobs to be printed. Tested on 79 "real world" pdf jobs, totaling 7,088 pages, the new profiler made only one page misclassification, while the previous one yielded 54 classification errors.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {177–180},
numpages = {4},
keywords = {digital presses, PDF profiling, printing costs},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860596,
author = {Hassan, Tamir and Hu, Changyuan and Hersch, Roger D.},
title = {Next Generation Typeface Representations: Revisiting Parametric Fonts},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860596},
doi = {10.1145/1860559.1860596},
abstract = {Outline font technology has long been established as the standard way to represent typefaces, allowing characters to be represented independently of print size and resolution. Although outline font technologies are mature and produce results of sufficient quality for professional printing applications, they are inherently inflexible, which presents limitations in a number of document engineering applications. In the 1990s, the topic of finding a successor to outline fonts was a hot topic of research. Unfortunately, none of the methods developed at the time were successful in replacing outline font technology and this field of research has since then declined sharply in popularity.In this paper, we revisit a parametric font format developed between 1995 and 2001 by Hu and Hersch, where characters are built up from connected shape components. We extend this representation and use it to synthesize several characters from the Frutiger typeface and alter their weights by setting the relevant parameters. These settings are automatically propagated to the other characters of the font family.To conclude, we provide a discussion on next-generation font technologies in the light of today's Web-centric technologies and suggest applications that could greatly benefit from the use of flexible, parametric font representations.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {181–184},
numpages = {4},
keywords = {font representation, parameterized fonts, re-typesetting, font synthesis, parametric fonts, digital typography},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/3253514,
author = {Gormish, Michael},
title = {Session Details: Document Engineering I: Posters},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253514},
doi = {10.1145/3253514},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
numpages = {1},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860598,
author = {Skaf-Molli, Hala and Canals, G\'{e}r\^{o}me and Molli, Pascal},
title = {DSMW: A Distributed Infrastructure for the Cooperative Edition of Semantic Wiki Documents},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860598},
doi = {10.1145/1860559.1860598},
abstract = {DSMW is a distributed semantic wiki that offers new collaboration modes to semantic wiki users and supports dataflow-oriented processes.DSMW is an extension to Semantic Mediawiki (SMW), it allows to create a network of SMW servers that share common semantic wiki pages. DSMW users can create communication channels between servers and use a publish-subscribe approach to manage the change propagation. DSMW synchronizes concurrent updates of shared semantic pages to ensure their consistency.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {185–186},
numpages = {2},
keywords = {distribution, semantic wiki, replication},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860599,
author = {Sorio, Enrico and Bartoli, Alberto and Davanzo, Giorgio and Medvet, Eric},
title = {Open World Classification of Printed Invoices},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860599},
doi = {10.1145/1860559.1860599},
abstract = {A key step in the understanding of printed documents is their classification based on the nature of information they contain and their layout. In this work we consider a dynamic scenario in which document classes are not known a priori and new classes can appear at any time. This open world setting is both realistic and highly challenging. We use an SVM-based classifier based only on image-level features and use a nearest-neighbor approach for detecting new classes. We assess our proposal on a real-world dataset composed of 562 invoices belonging to 68 different classes. These documents were digitalized after being handled by a corporate environment, thus they are quite noisy---e.g., big stamps and handwritten signatures at unfortunate positions and alike. The experimental results are highly promising.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {187–190},
numpages = {4},
keywords = {machine learning, document image classification, nearest-neighbor, SVM},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860600,
author = {Vion-Dury, Jean-Yves},
title = {Diffing, Patching and Merging XML Documents: Toward a Generic Calculus of Editing Deltas.},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860600},
doi = {10.1145/1860559.1860600},
abstract = {This work addresses what we believe to be a central issue in the field of XML diff and merge computation: the mathematical modeling of the so-called "editing deltas" and the study of their formal abstract properties. We expect at least three outputs from this theoretical work: a common basis to compare performances of the various algorithms through a structural normalization of deltas, a universal and flexible patch application model and a clearer separation of patch and merge engine performance from delta generation performance. Moreover, this work could inspire technical approaches to combine heterogeneous engines thank to sound delta transformations. This short paper reports current results, discusses key points and outlines some perspectives.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {191–194},
numpages = {4},
keywords = {version control, tree-to-tree correction, XML, tree edit distance, tree transformation},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860601,
author = {Yang, Shengwen and Jin, Jianming and Parag, Joshi and Liu, Sam},
title = {Contextual Advertising for Web Article Printing},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860601},
doi = {10.1145/1860559.1860601},
abstract = {Advertisements provide the necessary revenue model supporting the Web ecosystem and its rapid growth. Targeted or contextual ad insertion plays an important role in optimizing the financial return of this model. Nearly all the current ad payment strategies such as "pay-per-impression" and "pay-per-click" on web pages are geared for electronic viewing purposes. Little attention, however, is focused on deriving additional ad revenues when the content is repurposed for alternative mean of presentation, e.g. being printed. Although more and more content is moving to the Web, there are still many occasions where printed output of web content or RSS feeds is desirable, such as maps and articles; thus printed ad insertion can potentially be lucrative.In this paper, we describe a cloud-based printing service that enables automatic contextual ad insertion, with respect to the main web page content, when a printout of the page is requested. To encourage service utilization, it would provide higher quality printouts than what is possible from current browser print drivers, which generally produce poor outputs - ill formatted pages with lots of unwanted information, e.g. navigation icons. At this juncture we will limit the scope to only article-related web pages although the concept can be extended to arbitrary web pages. The key components of this system include (1) automatic extraction of article from web pages, (2) the ad service network for ad matching and delivering, and (3) joint content and ad printout creation.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {195–198},
numpages = {4},
keywords = {contextual advertisement, web printing},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860602,
author = {Bilauca, Mihai and Healy, Patrick},
title = {Table Layout Performance of Document Authoring Tools},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860602},
doi = {10.1145/1860559.1860602},
abstract = {In this paper we survey table creation in several popular document authoring programs and identify usability bottlenecks and inconsistencies between several of them. We discuss the user experience when drawing tables and we draw attention to the fact that authoring tables is still difficult and can be a frustrating and error prone exercise and that the development of high-quality table tools should be further pursued.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {199–202},
numpages = {4},
keywords = {table layout},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860603,
author = {Penad\'{e}s, Ma Carmen and Can\'{o}s, Jos\'{e} H. and Borges, Marcos R.S. and Llavador, Manuel},
title = {Document Product Lines: Variability-Driven Document Generation},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860603},
doi = {10.1145/1860559.1860603},
abstract = {In this paper, we propose a process model, which we call Document Product Lines, for the intensive generation of documents with variable content. Unlike current approaches, we identify the variability sources at the requirements level, including an explicit representation and management of these sources. The process model provides a methodological guidance to the (semi)automated generation of customized editors following the principles, techniques, and available technologies of Software Product Line Engineering. We illustrate our proposal with its application to the intensive generation of Emergency Plans.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {203–206},
numpages = {4},
keywords = {emergency management, emergency plans, document generation, software product lines, variability management},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860604,
author = {Namane, Abderrahmane and Soubari, El Houssine and Meyrueis, Patrick},
title = {Degraded Dot Matrix Character Recognition Using CSM-Based Feature Extraction},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860604},
doi = {10.1145/1860559.1860604},
abstract = {This paper presents an OCR method for degraded character recognition applied to a reference number (RN) of 15 printed characters of an invoice document produced by dot-matrix printer. First, the paper deals with the problem of the reference number localization and extraction, in which the characters tops or bottoms are or not touched with a printed reference line of the electrical bill. In case of touched RN, the extracted characters are severely degraded leading to missing parts in the characters tops or bottoms. Secondly, a combined recognition method based on the complementary similarity measure (CSM) method and MLP-based classifier is used. The CSM is used to accept or reject an incoming character. In case of acceptation, the CSM acts as a feature extractor and produces a feature vector of ten component features. The MLP is then trained using these feature vectors. The use of the CSM as a feature extractor tends to make the MLP very powerful and very well suited for rejection. Experimental results on electrical bills show the ability of the model to yield relevant and robust recognition on severely degraded printed characters.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {207–210},
numpages = {4},
keywords = {multiple classification, character recognition, OCR, dot matrix, feature extraction},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860605,
author = {Chiu, Patrick and Chen, Francine and Denoue, Laurent},
title = {Picture Detection in Document Page Images},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860605},
doi = {10.1145/1860559.1860605},
abstract = {We present a method for picture detection in document page images, which can come from scanned or camera images, or rendered from electronic file formats. Our method uses OCR to separate out the text and applies the Normalized Cuts algorithm to cluster the non-text pixels into picture regions. A refinement step uses the captions found in the OCR text to deduce how many pictures are in a picture region, thereby correcting for under- and over-segmentation. A performance evaluation scheme is applied which takes into account the detection quality and fragmentation quality. We benchmark our method against the ABBYY application on page images from conference papers.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {211–214},
numpages = {4},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860606,
author = {Stoppe, Jannis and Gottfried, Bj\"{o}rn},
title = {Down to the Bone: Simplifying Skeletons},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860606},
doi = {10.1145/1860559.1860606},
abstract = {This paper is about off-line handwritten text comparison of historic documents. The long-term motivation is the support of palaeographic research, in particular to back up decisions as to whether two handwritings can be ascribed to the same author. In this paper, a first fundamental step is presented for extracting relevant structures from handwritten texts. Such structures are represented by skeletons, due to their resemblance to original writing movements.Core result is an approach to the simplification of skeleton structures. While skeletons represent constitutive structures for a wide variety of subsequent algorithms, simplification algorithms usually focus on pruning branches off the skeleton instead of simplifying the skeleton as a whole. By contrast, our approach reduces the amount of elements in a skeleton based on a global error level, reducing the skeleton's complexity while keeping its structure as close to the original exemplar as possible. The results are much easier to analyse while relevant information is maintained.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {215–218},
numpages = {4},
keywords = {image processing, skeletons, text, simplification, shapes},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860607,
author = {Ramos-Terrades, Oriol and Toselli, Alejandro H. and Serrano, Nicolas and Romero, Ver\'{o}nica and Vidal, Enrique and Juan, Alfons},
title = {Interactive Layout Analysis and Transcription Systems for Historic Handwritten Documents},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860607},
doi = {10.1145/1860559.1860607},
abstract = {The amount of digitized legacy documents has been rising dramatically over the last years due mainly to the increasing number of on-line digital libraries publishing this kind of documents, waiting to be classified and finally transcribed into a textual electronic format (such as ASCII or PDF). Nevertheless, most of the available fully-automatic applications addressing this task are far from being perfect and heavy and inefficient human intervention is often required to check and correct the results of such systems. In contrast, multimodal interactive-predictive approaches may allow the users to participate in the process helping the system to improve the overall performance. With this in mind, two sets of recent advances are introduced in this work: a novel interactive method for text block detection and two multimodal interactive handwritten text transcription systems which use active learning and interactive-predictive technologies in the recognition process.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {219–222},
numpages = {4},
keywords = {interactive predictive processing, partial supervision, handwriting recognition, interactive layout analysis},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860608,
author = {Piotrowski, Michael},
title = {Document Conversion for Cultural Heritage Texts: FrameMaker to HTML Revisited},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860608},
doi = {10.1145/1860559.1860608},
abstract = {Many large-scale digitization projects are currently under way that intend to preserve the cultural heritage contained in paper documents (in particular books) and make it available on the Web. Typically OCR is used to produce searchable electronic texts from books. For newer books, approximately from the late 1980s onwards,digital text may already exist in the form of typesetting data. For applications that require a higher level of accuracy than OCR can deliver, the conversion of typesetting data can thus be an alternative to manual keying. In this paper, we describe a tool for converting typesetting data in FrameMaker format to XHTML+CSS developed for a collection of source editions of medieval and early modern documents. Even though the books of the Collection are typeset in good quality and in modern typefaces, OCR is unusable,since the text is in various historical forms of German, French,Italian, Rhaeto-Romanic, and Latin. The conversion of typesetting data produces fully reliable text free from OCR errors and thus also provides a basis for the construction of language resources for the processing of historical texts.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {223–226},
numpages = {4},
keywords = {framemaker, XHTML, cultural heritage data, document format conversion, CSS},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860609,
author = {Meyer-Lerbs, Lothar and Schuldt, Arne and Gottfried, Bj\"{o}rn},
title = {Glyph Extraction from Historic Document Images},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860609},
doi = {10.1145/1860559.1860609},
abstract = {This paper is about the reproduction of ancient texts with vectorised fonts. While for OCR only recognition rates count, a reproduction process does not necessarily require the recognition of characters. Our system aims at extracting all characters from printed historic documents without the employment of knowledge of language, font, or writing system. It searches for the best prototypes and creates a document-specific font from these glyphs. To reach this goal, many common OCR preprocessing steps are no longer adequate. We describe the necessary changes of our system that deals particularly with documents typeset in Fraktur. On the one hand, algorithms are described that extract glyphs accurately for the purpose of precise reproduction. On the other hand, classification results of extracted Fraktur glyphs are presented for different shape descriptors.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {227–230},
numpages = {4},
keywords = {image enhancement, glyph classification, glyph extraction, document-specific font, glyph shape},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860610,
author = {Jiao, Limei and Lim, Suk Hwan and Bhatti, Nina and Xiong, Yuhong and Liu, Jerry},
title = {Style and Branding Elements Extraction from Businessweb Sites},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860610},
doi = {10.1145/1860559.1860610},
abstract = {We describe a method to extract style and branding elements from multiple web pages in a given site for content repurposing. Style and branding elements convey the values of the site owners effectively and connect with the target prospects. They are manifested through logos, graphical elements, background color, font styles, font colors and other illustrations. Our method automatically extracts color and image elements appearing frequently and prominently on multiple pages throughout the site. We rely on a DOM tree matching method to obtain the frequency of re-occurring elements and use relative sizes and positions of elements to determine the type of elements. Note that approximate locations of these elements provide an added clue to the content repurposing engine as to where to place the elements in the repurposed document. The obtained results show that the proposed method can efficiently extract style and branding elements with high accuracy.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {231–234},
numpages = {4},
keywords = {high frequent elements extraction, tree matching, style and branding extraction},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/3253515,
author = {Ingold, Rolf},
title = {Session Details: Document Engineering II: Posters},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253515},
doi = {10.1145/3253515},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
numpages = {1},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860612,
author = {Denoue, Laurent and Adcock, John and Carter, Scott and Chui, Patrick and Chen, Francine},
title = {FormCracker: Interactive Web-Based Form Filling},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860612},
doi = {10.1145/1860559.1860612},
abstract = {Filling out document forms distributed by email or hosted on the Web is still problematic and usually requires a printer and scanner. Users commonly download and print forms, fill them out by hand, scan and email them. Even if the document is form-enabled (PDFs with FDF information), to read the file users still have to launch a separate application which may not be available, especially on mobile devices.FormCracker simplifies this process by providing an interactive, fully web-based document viewer that lets users complete forms online. Document pages are rendered as images and presented in a simple HTML-based viewer. When a user clicks in a form-field, FormCracker identifies the correct form-field type using lightweight image processing and heuristics based on nearby text. Users can then seamlessly enter data in form-fields such as text boxes, check boxes, radio buttons, multiple text lines, and multiple single-box characters. FormCracker also provides useful auto-complete features based on the field type, for example a date picker, a drop-down menu for city names, state lists, and an auto-complete text box for first and last names. Finally, FormCracker allows users to save and print the completed document.In summary, with FormCracker a user can efficiently complete and reuse any electronic form},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {235–238},
numpages = {4},
keywords = {form filling, image processing, interactive, document processing},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860613,
author = {Guo, Jingzhi and Ho, Ming Sang},
title = {Semantics-Enriched Document Exchange},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860613},
doi = {10.1145/1860559.1860613},
abstract = {In e-business development, semantics-oriented document exchange is becoming important, because it can support cross-domain user connection, business transaction and collaboration. To provide this support, this paper proposes a DOC Mechanism to exchange semantically interoperable business documents between heterogeneous enterprise information systems. This mechanism is designed on a layered-sign network, which enables any exchanged e-business document to be independently interpretable without losing semantic consistency.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {239–242},
numpages = {4},
keywords = {sign, electronic business, document exchange, XML product map, semantics, document engineering, representation, concept},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860614,
author = {Cahier, Jean-Pierre and Ma, Xiaoyue and Zaher, L'H\'{e}di},
title = {Document and Item-Based Modeling: A Hybrid Method for a Socio-Semantic Web},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860614},
doi = {10.1145/1860559.1860614},
abstract = {The paper discusses the challenges of categorising documents and "items of the world" to promote knowledge sharing in large communities of interest. We present the DOCMA method (Document and Item-based Model for Action) dedicated to end-users who have minimal or no knowledge of information science. Community members can elicit structure and indexed business items stemming from their query including projects, actors, products, places of interest, and geo-situated objects. This hybrid method was applied in a collaborative Web portal in the field of sustainability for the past two years.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {243–246},
numpages = {4},
keywords = {folksonomy, socio-semantic web, web2.0, document, method},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860615,
author = {Dubuc, Julien and Bergler, Sabine},
title = {Structure-Aware Topic Clustering in Social Media},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860615},
doi = {10.1145/1860559.1860615},
abstract = {The rapid evolution and growth of social media software has enabled hundreds of millions to interact within on-line communities on a global scale. While they enable communication through a common set of metaphors, such as discussion threads and quoting text in replies, this software uses a variety of diverging ways of representing discussion. Since the meaning of a conversation is defined not only by the content of a piece of text, but also by the relationships between pieces of text, part of the meaning of the discussion is obscured from automated processing.Search engines, which act as gateways to outsiders into the social text in a community, are reduced to giving an incomplete picture. This paper proposes a model for representing both the content and the structure of social text in a consistent way, enabling automated processing of the structure of the discussion along with its text content.It also describes a method for indexing text that uses this structural information to provide meaningful contexts for paragraphs of interest. It then describes a method for clustering text content into topic groups, using this indexing method, and also using the social structure to make informed decisions about which pieces of text to compare meaningfully.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {247–250},
numpages = {4},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860616,
author = {Lumley, John},
title = {Pre-Evaluation of Invariant Layout in Functional Variable-Data Documents},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860616},
doi = {10.1145/1860559.1860616},
abstract = {Layout of content in variable data documents can be computationally expensive. When very large numbers of almost similar copies of a document are required, automated pre-evaluation of invariant sections may increase efficiency of final document generation. If the layout model is functional and combinatorial in nature (such as in the Document Description Framework), there are some generalised conservative techniques to do this that involve very modest changes to implementations, independent of details of the actual layouts. This paper describes these techniques and how they might be used with other similar document layout models.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {251–254},
numpages = {4},
keywords = {functional programming, document construction, SVG, XSLT},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860617,
author = {Hassan, Tamir},
title = {Towards a Common Evaluation Strategy for Table Structure Recognition Algorithms},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860617},
doi = {10.1145/1860559.1860617},
abstract = {A number of methods for evaluating table structure recognition systems have been proposed in the literature, which have been used successfully for automatic and manual optimization of their respective algorithms. Unfortunately, the lack of standard, ground-truthed datasets coupled with the ambiguous nature of how humans interpret tabular data has made it difficult to compare the obtained results between different systems developed by different research groups.With reference to these approaches, we describe our experiences in comparing our algorithm for table detection and structure recognition to another recently published system using a freely available dataset of 75 PDF documents. Based on examples from this dataset, we define several classes of errors and propose how they can be treated consistently to eliminate ambiguities and ensure the repeatability of the results and their comparability between different systems from different research groups.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {255–258},
numpages = {4},
keywords = {table detection, ground truth, recall, evaluation, table structure recognition, table recognition, precision},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860618,
author = {Karol, Sven and Heinzerling, Martin and Heidenreich, Florian and A\ss{}mann, Uwe},
title = {Using Feature Models for Creating Families of Documents},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860618},
doi = {10.1145/1860559.1860618},
abstract = {Variants in a family of office documents are usually created by ad-hoc copy and paste actions from a set of base documents. As a result, the set of variants is decoupled from the original documents and is difficult to manage. In this paper we present a novel approach that uses concepts from Feature Oriented Domain Analysis (FODA) to specify document families to generate variants. As a proof of concept, we implemented the Document Feature Mapper tool, which is based on our previous experience in Software Product Line Engineering (SPLE) with FODA. In our tool, variant spaces are precisely specified using feature models and mappings relating features to slices in the document family. Gives a selection of features satisfying the feature model's constraints a variant can be derived. To show the applicability of our approach and tool, we conducted two case studies with documents in the Open Document Format (ODF).},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {259–262},
numpages = {4},
keywords = {variants, feature models, document families, XML, ODF},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860619,
author = {Riva, Aline Duarte and Seki, Alexandre Kazuo and de Oliveira, Jo\~{a}o Batista Souza and Manssour, Isabel Harb and Piccoli, Ricardo Farias},
title = {Two New Aesthetic Measures for Item Alignment},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860619},
doi = {10.1145/1860559.1860619},
abstract = {This paper introduces two methods for measuring the alignment of items on a page with respect to its left/right margins. The methods are based on the path followed by the eyes as they follow the items from top to bottom of the page.Examples are presented and both methods are analyzed with respect to the axioms presented in [2], that describe how good alignment measure is supposed to behave.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {263–266},
numpages = {4},
keywords = {document aesthetics, page alignment, page layout},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860620,
author = {Nunes, S\'{e}rgio and Ribeiro, Cristina and David, Gabriel},
title = {Term Frequency Dynamics in Collaborative Articles},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860620},
doi = {10.1145/1860559.1860620},
abstract = {Documents on the World Wide Web are dynamic entities. Mainstream information retrieval systems and techniques are primarily focused on the latest version a document, generally ignoring its evolution over time. In this work, we study the term frequency dynamics in web documents over their lifespan. We use the Wikipedia as a document collection because it is a broad and public resource and, more important, because it provides access to the complete revision history of each document. We investigate the progression of similarity values over two projection variables, namely revision order and revision date. Based on this investigation we find that term frequency in encyclopedic documents - i.e. comprehensive and focused on a single topic - exhibits a rapid and steady progression towards the document's current version. The content in early versions quickly becomes very similar to the present version of the document.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {267–270},
numpages = {4},
keywords = {wikipedia, term frequency, document dynamics},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860621,
author = {M\"{u}ller, Arthur and R\"{o}nnau, Sebastian and Borghoff, Uwe M.},
title = {A File-Type Sensitive, Auto-Versioning File System},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860621},
doi = {10.1145/1860559.1860621},
abstract = {Auto-versioning file systems offer a simple and reliable interface to document change control. The implicit versioning of documents at each write access catches the whole evolution of a document, thus supporting regulatory compliance rules. Most existing file systems work on low abstraction levels and track the document evolution on their binary representation. Higher-level differencing tools allow for a far more meaningful change-tracking, though.In this paper, we present an auto-versioning file system that is able to handle files depending on their file type. This way, a suitable differencing tool can be assigned to each file type. Our approach supports regulatory compliant storage as well as the archiving of documents},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {271–274},
numpages = {4},
keywords = {version control, auto-versioning, document management, file system, regulatory compliance},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860622,
author = {Baechler, Micheal and Ingold, Rolf},
title = {Medieval Manuscript Layout Model},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860622},
doi = {10.1145/1860559.1860622},
abstract = {Medieval manuscript layouts are quite complex. Additionally to their main text flow, which can spread over one or several columns, such manuscripts contain also other textual elements such as insertions, annotations, and corrections. They are often richly decorated with ornaments, illustrations, and drop capitals making their layout even more complex. In this paper we propose a generic layout model to represent their physical structure.To achieve this goal we propose to use four layers in order to distinguish between the different graphical elements. In this paper we show how this model is used to represent automatic segmentation results and how it allows a quantitative measure of their accuracy.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {275–278},
numpages = {4},
keywords = {segmentation, annotation, layout model, layout, medieval, manuscript, medieval manuscript},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860623,
author = {Beaudoux, Olivier and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using Model Driven Engineering Technologies for Building Authoring Applications},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860623},
doi = {10.1145/1860559.1860623},
abstract = {Building authoring applications is a tedious and complex task that requires a high programming effort. Document technologies, especially XML based ones, can help in reducing such an effort by providing common bases for manipulating documents. Still, the overall task consists mainly of writing the application's source code. Model Driven Engineering (MDE) focuses on generating the source code from an exhaustive model of the application. In this paper, we illustrate that MDE technologies can be used to automate the development of authoring application components, but fail in generating the code of graphical components. We present our framework, called Malai, that aims to solve this issue.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {279–282},
numpages = {4},
keywords = {Malan, Malai, authoring applications, MDE},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

@inproceedings{10.1145/1860559.1860624,
author = {Balinsky, Alexander A. and Balinsky, Helen Y. and Simske, Steven J.},
title = {On Helmholtz's Principle for Documents Processing},
year = {2010},
isbn = {9781450302319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1860559.1860624},
doi = {10.1145/1860559.1860624},
abstract = {Keyword extraction is a fundamental problem in text data mining and document processing. A large number of document processing applications directly depend on the quality and speed of keyword extraction algorithms. In this article, a novel approach to rapid change detection in data stream.and documents is developed. It is based on ideas from image processing and especially on the Helmholtz Principle from the Gestalt Theory of human perception. Applied to the problem of keywords extraction, it delivers fast and effective tools to identify meaningful keywords using parameter-free methods. We also define a level of meaningfulness of the keywords which can be used to modify the set of keywords depending on application needs.},
booktitle = {Proceedings of the 10th ACM Symposium on Document Engineering},
pages = {283–286},
numpages = {4},
keywords = {rapid change detection, helmholtz principle, meaningful words, gestalt, keyword extraction},
location = {Manchester, United Kingdom},
series = {DocEng '10}
}

