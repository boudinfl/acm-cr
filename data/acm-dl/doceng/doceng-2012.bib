@inproceedings{10.1145/3259280,
author = {King, Peter},
title = {Session Details: Keynote Address},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259280},
doi = {10.1145/3259280},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
numpages = {1},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361356,
author = {Bachimont, Bruno},
title = {Document and Archive: Editing the Past},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361356},
doi = {10.1145/2361354.2361356},
abstract = {Document engineering has a difficult task: to propose tools and methods to manipulate contents and make sense of them. This task is still harder when dealing with archive, insofar as document engineering has not only to provide tools for expressing sense but above all tools and methods to keep contents accessible in their integrity and intelligible according to their meaning. However, these objectives may be contradictory: access implies to transform contents to make them accessible through networks, tools and devices. Intelligibility may imply to adapt contents to the current state of knowledge and capacity of understanding. But, by doing that, can we still speak of authenticity, integrity, or even the identity of documents? Document engineering has provided powerful means to express meaning and to turn an intention into a semiotic expression. Document repurposing has become a usual way for exploiting libraries, archives, etc. By enabling to reuse a specific part of a given content, repurposing techniques allow to entirely renegotiate the meaning of this part by changing its context, its interactivity, in short the way people can consider this piece of content and interpret it. Put in this way, there could be an antinomy between archiving and document engineering. However, transforming document, editing content is an efficient way to keep them alive and compelling for people. Preserving contents does not consist in simply storing them but in actively transforming them to adapt them technically and keep them intelligible. Editing the past is then a new challenge, merging a content deontology with a document technology. This challenge implies to redefine some classical notions as authenticity and highlight the needs for new concepts and methods. Especially in a digital world, documents are permanently reconfigured by technical tools that produce variants, similar contents calling into question the usual definition the identity of documents. Editing the past calls for a new critics of variants.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {1–2},
numpages = {2},
keywords = {document engineering, digital preservation, archive},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/3259281,
author = {Brailsford, David},
title = {Session Details: Layout and Presentation Generation},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259281},
doi = {10.1145/3259281},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
numpages = {1},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361358,
author = {Damera-Venkata, Niranjan and Bento, Jos\'{e}},
title = {Ad Insertion in Automatically Composed Documents},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361358},
doi = {10.1145/2361354.2361358},
abstract = {We consider the problem of automatically inserting advertisements (ads) into machine composed documents. We explicitly analyze the fundamental tradeoff between expected revenue due to ad insertion and the quality of the corresponding composed documents. We show that the optimal tradeoff a publisher can expect may be expressed as an efficient-frontier in the revenue-quality space. We develop algorithms to compose documents that lie on this optimal tradeoff frontier. These algorithms can automatically choose distributions of ad sizes and ad placement locations to optimize revenue for a given quality or optimize quality for given revenue. Such automation allows a market maker to accept highly personalized content from publishers who have no design or ad inventory management capability and distribute formatted documents to end users with aesthetic ad placement. The ad density/coverage may be controlled by the publisher or the end user on a per document basis by simply sliding along the tradeoff frontier. Business models where ad sales precede (ad-pull) or follow (ad-push) document composition are analyzed from a document engineering perspective.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {3–12},
numpages = {10},
keywords = {layout synthesis, advertisement insertion, document composition, automated publishing},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361359,
author = {Gange, Graeme and Marriott, Kim and Stuckey, Peter},
title = {Optimal Guillotine Layout},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361359},
doi = {10.1145/2361354.2361359},
abstract = {Guillotine-based page layout is a method for document layout commonly used by newspapers and magazines, where each region of the page either contains a single article, or is recursively split either vertically or horizontally. Suprisingly there appears to be little research into algorithms for automatic guillotine-based document layout. In this paper we give efficient algorithms to find optimal solutions to guillotine layout problems of two forms. Fixed-cut layout is where the structure of the guillotining is given and we only have to determine the best configuration for each individual article to give the optimal total configuration. Free layout is where we also have to search for the optimal structure. We give bottom-up and top-down dynamic programming algorithms to solve these problems, and propose a novel interaction model for documents on electronic media. Experiments show that our algorithms are effective for realistic layout problems.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {13–22},
numpages = {10},
keywords = {guillotine-based document layout, constrained optimization, typography, dynamic programming},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361360,
author = {Acebal, C\'{e}sar and Bos, Bert and Rodr\'{\i}guez, Mar\'{\i}a and Cueva, Juan Manuel},
title = {ALMcss: A Javascript Implementation of the CSS Template Layout Module},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361360},
doi = {10.1145/2361354.2361360},
abstract = {Traditionally, web standards in general and Cascading Style Sheets (CSS) in particular take a long time from when they are defined by the W3C until they are implemented by browser vendors. This has been a limitation not only for authors, who had to wait even years before they were able to use certain CSS properties in their web pages, but also for the creators of the specification itself, who were not able to test their proposals in practice.In this paper we present ALMcss, a JavaScript prototype that implements the CSS Template Layout Module, a proposal for an addition to CSS to make it a more capable layout language. It has been developed inside the W3C CSS Working Group by two of the authors of this paper. We present the rationale of the module and an introduction to its syntax, before discussing the design of our prototype.ALMcss has served us as a proof of concept that the Template Layout Module is not only feasible, but it can be in fact implemented in current web browsers using just JavaScript and the Document Object Model (DOM). In addition, ALMcss allows web designers to start to use today the new layout capabilities of CSS that the module provides, even before it becomes an official W3C specification.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {23–32},
numpages = {10},
keywords = {CSS3, javascript, CSS, web standards, layout, browsers, W3C},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361361,
author = {Moulder, Peter and Marriott, Kim},
title = {Learning How to Trade off Aesthetic Criteria in Layout},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361361},
doi = {10.1145/2361354.2361361},
abstract = {Typesetting software is often faced with conflicting aesthetic goals. For example, choosing where to break lines in text might involve aiming to minimize hyphenation, variation in word spacing, and consecutive lines starting with the same word. Typically, automatic layout is modelled as an optimization problem in which the goal is to minimize a complex objective function that combines various penalty functions each of which corresponds to a particular bad feature. Determining how to combine these penalty functions is difficult and very time consuming, becoming harder each time we add another penalty. Here we present a machine-learning approach to do this, and test it in the context of line-breaking. Our approach repeatedly queries the expert typographer as to which one of a pair of layouts is better, and accordingly refines the estimate of how best to weight the penalties in a linear combination. It chooses layout pair queries by a heuristic to maximize the amount that can be learnt from them so as to reduce the number of combinations that must be considered by the typographer.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {33–36},
numpages = {4},
keywords = {typography, line-breaking, progressive articulation of preference, multi-objective optimization},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/3259282,
author = {Marinai, Simone},
title = {Session Details: Document Analysis},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259282},
doi = {10.1145/3259282},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
numpages = {1},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361363,
author = {Jayabal, Yogalakshmi and Ramanathan, Chandrashekar and Sheth, Mehul Jayprakash},
title = {Challenges in Generating Bookmarks from TOC Entries in E-Books},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361363},
doi = {10.1145/2361354.2361363},
abstract = {ABSTRACT The task of extracting document structures from a digital e-book is difficult and is an active area of research. On the other hand, many e-books already have a table of contents (TOC) at the beginning of the document. This may lead us to believe that adding bookmarks into digital document (e-book) based on the existing TOC would be trivial. In this paper, we highlight the challenges involved in this task of automatically adding bookmarks to an existing e-book based on the TOC that exists within the document. If we are able to reliably identify the specific locations of each TOC entry within the document, the algorithms can be easily extended to identify document structures within e-books that have TOC. We describe a tool we have built called Booky that tries to add automatic PDF bookmarks to existing PDF based e-books as they have TOC as part of the document content. The tool addresses most of the challenges that have been identified while still leaving a few tricky scenarios still open.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {37–40},
numpages = {4},
keywords = {table of contents, bookmarks, PDF, structure analysis, TOC},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361364,
author = {Truran, Mark and Georg, Gersende and Cavazza, Marc and Zhou, Dong},
title = {A Section Title Authoring Tool for Clinical Guidelines},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361364},
doi = {10.1145/2361354.2361364},
abstract = {Professional users of medical information often report difficulties when attempting to locate specific information in lengthy documents. Sometimes these difficulties can be attributed to poorly specified section titles which fail to advertise relevant content. In this paper we describe preliminary work on a software plug-in for a document engineering environment that will assist authors when they formulate section-level headings. We describe two different algorithms which can be used to generate section titles. We compare the performance of these algorithms and correlate our experimental results with an evaluation of title quality performed by domain experts.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {41–44},
numpages = {4},
keywords = {title, section, quality, content, clinical guideline},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361365,
author = {G\"{o}bel, Max and Hassan, Tamir and Oro, Ermelinda and Orsi, Giorgio},
title = {A Methodology for Evaluating Algorithms for Table Understanding in PDF Documents},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361365},
doi = {10.1145/2361354.2361365},
abstract = {This paper presents a methodology for the evaluation of table understanding algorithms for PDF documents. The evaluation takes into account three major tasks: table detection, table structure recognition and functional analysis. We provide a general and flexible output model for each task along with corresponding evaluation metrics and methods. We also present a methodology for collecting and ground-truthing PDF documents based on consensus-reaching principles and provide a publicly available ground-truthed dataset.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {45–48},
numpages = {4},
keywords = {ground-truth dataset, table processing, performance evaluation, document analysis, document understanding, metrics},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/3259283,
author = {Concolato, Cyril},
title = {Session Details: Multimedia and Hypermedia},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259283},
doi = {10.1145/3259283},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
numpages = {1},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361367,
author = {Meixner, Britta and Kosch, Harald},
title = {Interactive Non-Linear Video: Definition and XML Structure},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361367},
doi = {10.1145/2361354.2361367},
abstract = {A literature review on the term "interactive video" and "interactive non-linear video" revealed different levels of interaction in varying definitions. We give a formal definition of the term "interactive non-linear video" to clarify the elements and possible relations between elements contained in such videos. Furthermore, we introduce a new event-based XML format consisting of four required and two optional elements to describe this form of video. A scene graph consisting of scenes with triggers for annotations builds the core of the format. Formal definition and XML format are both illustrated by a real world example.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {49–58},
numpages = {10},
keywords = {video annotations, non-linear video, multimedia document, interactive video},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361368,
author = {Jansen, Jack and Cesar, Pablo and Guimaraes, Rodrigo Laiola and Bulterman, Dick C.A.},
title = {Just-in-Time Personalized Video Presentations},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361368},
doi = {10.1145/2361354.2361368},
abstract = {Using high-quality video cameras on mobile devices, it is relatively easy to capture a significant volume of video content for community events such as local concerts or sporting events. A more difficult problem is selecting and sequencing individual media fragments that meet the personal interests of a viewer of such content. In this paper, we consider an infrastructure that supports the just-in-time delivery of personalized content. Based on user profiles and interests, tailored video mash-ups can be created at view-time and then further tailored to user interests via simple end-user interaction. Unlike other mash-up research, our system focuses on client-side compilation based on personal (rather than aggregate) interests. This paper concentrates on a discussion of language and infrastructure issues required to support just-in-time video composition and delivery. Using a high school concert as an example, we provide a set of requirements for dynamic content delivery. We then provide an architecture and infrastructure that meets these requirements. We conclude with a technical and user analysis of the just-in-time personalized video approach.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {59–68},
numpages = {10},
keywords = {video mashups, seamless playback, late binding of media},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361369,
author = {Soares Neto, Carlos S. and Pinto, Hedvan F. and Soares, Luiz Fernando G.},
title = {TAL Processor for Hypermedia Applications},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361369},
doi = {10.1145/2361354.2361369},
abstract = {TAL (Template Authoring Language) is a specification language for hypermedia document templates. Templates describe application families with structural and semantic similarities. In TAL, templates not only define design patterns that applications must follow, but also constraints on the use of these patterns. A template must be processed together with a padding document giving rise to a new document in some specification language, called target language. TAL supports the description of templates independently of the languages used to specify target and padding documents. Usually a specific processor is required for each target language and for each padding document used. This paper concerns TAL processors. However, we should note that the proposal can be easily extended to any other solution used to define templates. Any pattern language and any language used to define constraints could be used instead of TAL. The TAL processor architecture is general and it is discussed when presenting the processor framework. As an instantiation example, an implementation of a TAL Processor targeting NCL (the declarative language of Ginga DTV middleware) is examined, and also another one targeting HTML-based middleware. The use of wizards for defining padding documents is also discussed in the examples of the proposed architecture instantiation.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {69–78},
numpages = {10},
keywords = {NCL, IDTV, TAL, nested context language, Ginga, digital TV applications},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361370,
author = {Aubert, Olivier and Pri\'{e}, Yannick and Schmitt, Daniel},
title = {Advene as a Tailorable Hypervideo Authoring Tool: A Case Study},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361370},
doi = {10.1145/2361354.2361370},
abstract = {Audiovisual documents provide a great primary material for analysis in multiple domains, such as sociology or interaction studies. Video annotation tools offer new ways of analysing these documents, beyond the conventional transcription. However, these tools are often dedicated to specific domains, putting constraints on the data model or interfaces that may not be convenient for alternative uses. Moreover, most tools serve as exploratory and analysis instruments only, not proposing export formats suitable for publication. We describe in this paper a usage of the Advene software, a versatile video annotation tool that can be tailored for various kinds of analyses: users can define their own analysis structure and visualizations, and share their analyses either as structured annotations with visualization templates, or published on the Web as hypervideo documents. We explain how users can customize the software through the definition of their own data structures and visualizations. We illustrate this adaptability through an actual usage for interview analysis.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {79–82},
numpages = {4},
keywords = {annotation, active reading, hypervideo, mediafragments, video analysis},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/3259284,
author = {Schmitz, Patrick},
title = {Session Details: Keynote Address},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259284},
doi = {10.1145/3259284},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
numpages = {1},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361372,
author = {Delprat, Thierry},
title = {Content and Document Based Approach for Digital Productivity Applications},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361372},
doi = {10.1145/2361354.2361372},
abstract = {In today's world most of the data produced and consumed by employees is content. In this talk we will present our approach to create and deploy content and document based applications to improve business processes and user experience.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {83–84},
numpages = {2},
keywords = {enterprise content management, ECM},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/3259285,
author = {Hardy, Matthew},
title = {Session Details: XML and Related Tools},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259285},
doi = {10.1145/3259285},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
numpages = {1},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361374,
author = {Di Iorio, Angelo and Peroni, Silvio and Poggi, Francesco and Vitali, Fabio},
title = {A First Approach to the Automatic Recognition of Structural Patterns in XML Documents},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361374},
doi = {10.1145/2361354.2361374},
abstract = {XML is among the preferred formats for storing the structure of documents such as scientific articles, manuals, documentation, literary works, etc. Sometimes publishers adopt established and well-known vocabularies such as DocBook and TEI, other times they create partially or entirely new ones that better deal with the particular requirements of their documents. The (explicit and implicit) requirements of use in these vocabularies often follow well-established patterns, creating meta-structures (the block, the container, the inline element, etc.) that persist across vocabularies and authors and that describe a truer and more general conceptualization of the documents' building blocks. Addressing such meta-structures not only gives a better insight of what documents really are composed of, but provides abstract and more general mechanisms to work on documents regardless of the availability of specific schemas, tools and presentation stylesheets. In this paper we introduce a schemaindependent theory based on eleven structural patterns. We provide a definition of such patterns and how they synthesize characteristics emerging from real markup documents. Additionally, we propose an algorithm that allows us to identify the pattern of each element in a set of homogeneous markup documents.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {85–94},
numpages = {10},
keywords = {XML, descriptive markup, document visualisation, pattern recognition, structural patterns},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361375,
author = {Junedi, Muhammad and Genev\`{e}s, Pierre and Laya\"{\i}da, Nabil},
title = {XML Query-Update Independence Analysis Revisited},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361375},
doi = {10.1145/2361354.2361375},
abstract = {XML transformations can be resource-costly in particular when applied to very large XML documents and document sets. Those transformations usually involve lots of XPath queries and may not need to be entirely re-executed following an update of the input document. In this context, a given query is said to be independent of a given update if, for any XML document, the results of the query are not affected by the update. We revisit Benedikt and Cheney's framework for query-update independence analysis and show that performance can be drastically enhanced, contradicting their initial claims. The essence of our approach and results resides in the use of an appropriate logic, to which queries and updates are both succinctly translated. Compared to previous approaches, ours is more expressive from a theoretical point of view, equally accurate, and more efficient in practice. We illustrate this through practical experiments and comparative figures.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {95–98},
numpages = {4},
keywords = {update, query, XML, independence},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361376,
author = {Chuang, Tyng-Ruey and Wu, Hui-Yin},
title = {Structure-Conforming XML Document Transformation Based on Graph Homomorphism},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361376},
doi = {10.1145/2361354.2361376},
abstract = {We propose a principled method to specify XML document transformation so that the outcome of a transformation can be ensured to conform to certain structural constraints as required by the target XML document type. We view XML document types as graphs, and model transformations as relations between the two graphs. Starting from this abstraction, we use and extend graph homomorphism as a formalism for the specifications of transformations between XML document types. A specification can then be checked to ensure whether results from the transformation will always be structure-conforming.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {99–102},
numpages = {4},
keywords = {XML, document transformation, graph homomorphism},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361377,
author = {Oliveira, Raquel and Genev\`{e}s, Pierre and Laya\"{\i}da, Nabil},
title = {Toward Automated Schema-Directed Code Revision},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361377},
doi = {10.1145/2361354.2361377},
abstract = {Updating XQuery programs in accordance with a change of the input XML schema is known to be a time-consuming and error-prone task. We propose an automatic method aimed at helping developers realign the XQuery program with the new schema. First, we introduce a taxonomy of possible problems induced by a schema change. This allows to differentiate problems according to their severity levels, e.g. errors that require code revision, and semantic changes that should be brought to the developer's attention. Second, we provide the necessary algorithms to detect such problems using a solver that checks satisfiability of XPath expressions.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {103–106},
numpages = {4},
keywords = {schema evolution, schemas, XML, XQuery},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/3259286,
author = {Likforman-Sulem, Laurence},
title = {Session Details: OCR and Visual Analysis},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259286},
doi = {10.1145/3259286},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
numpages = {1},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361379,
author = {Chen, Zhanghui and Zhou, Baoyao},
title = {Effective Radical Segmentation of Offline Handwritten Chinese Characters towards Constructing Personal Handwritten Fonts},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361379},
doi = {10.1145/2361354.2361379},
abstract = {Effective radical segmentation of handwritten Chinese characters can greatly facilitate the subsequent character processing tasks, such as Chinese handwriting recognition/identification and the generation of Chinese handwritten fonts. In this paper, a popular snake model is enhanced by considering the guided image force and optimized by Genetic Algorithm, such that it achieves a significant improvement in terms of both accuracy and efficiency when applied to segment the radicals in handwritten Chinese characters. The proposed radical segmentation approach consists of three stages: constructing guide information, Genetic Algorithm optimization and post-embellishment. Testing results show that the proposed approach can effectively decompose radicals with overlaps and connections from handwritten Chinese characters with various layout structures. The segmentation accuracy reaches 94.91% for complicated samples with overlapped and connected radicals and the segmentation speed is 0.05 second per character. For demonstrating the advantages of the approach, radicals extracted from the user input samples are reused to construct personal Chinese handwritten font library. Experiments show that the constructed characters well maintain the handwriting style of the user and have good enough performance. In this way, the user only needs to write a small number of samples for obtaining his/her own handwritten font library. This method greatly reduces the cost of existing solutions and makes it much easier for people to use computers to write letters/e-mails, diaries/blogs, even magazines/books in their own handwriting.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {107–116},
numpages = {10},
keywords = {snake model, personal handwritten fonts, radical segmentation, offline handwritten chinese character recognition, genetic algorithm},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361380,
author = {Law, Marc Teva and Thome, Nicolas and Gan\c{c}arski, St\'{e}phane and Cord, Matthieu},
title = {Structural and Visual Comparisons for Web Page Archiving},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361380},
doi = {10.1145/2361354.2361380},
abstract = {In this paper, we propose a Web page archiving system that combines state-of-the-art comparison methods based on the source codes of Web pages, with computer vision techniques. To detect whether successive versions of a Web page are similar or not, our system is based on: (1) a combination of structural and visual comparison methods embedded in a statistical discriminative model, (2) a visual similarity measure designed for Web pages that improves change detection, (3) a supervised feature selection method adapted to Web archiving. We train a Support Vector Machine model with vectors of similarity scores between successive versions of pages. The trained model then determines whether two versions, defined by their vector of similarity scores, are similar or not. Experiments on real archives validate our approach.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {117–120},
numpages = {4},
keywords = {change detection algorithms, web archiving, digital preservation, support vector machines, pattern recognition},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361381,
author = {Janssen, Bill and Saund, Eric and Bier, Eric and Wall, Patricia and Sprague, Mary Ann},
title = {Receipts2Go: The Big World of Small Documents},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361381},
doi = {10.1145/2361354.2361381},
abstract = {The Receipts2Go system is about the world of one-page documents: cash register receipts, book covers, cereal boxes, price tags, train tickets, fire extinguisher tags. In that world, we're exploring techniques for extracting accurate information from documents for which we have no layout descriptions -- indeed no initial idea of what the document's genre is -- using photos taken with cell phone cameras by users who aren't skilled document capture technicians. This paper outlines the system and reports on some initial results, including the algorithms we've found useful for cleaning up those document images, and the techniques used to extract and organize relevant information from thousands of similar-but-different page layouts.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {121–124},
numpages = {4},
keywords = {receipt analysis, geometric information analysis, image normalization},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361382,
author = {Marinai, Simone and Quiriconi, Stefano},
title = {Displaying Chemical Structural Formulae in EPub Format},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361382},
doi = {10.1145/2361354.2361382},
abstract = {We describe one tool designed to enhance the visualization of chemical structural formulae in E-book readers. When dealing with small formulae, to avoid the pixelation effect with zoomed images, the formula is converted to a vectoral representation and then enlarged. On the opposite, large formulae are split in sub-images by cutting the image in suitable locations attempting to reduce the parts of the formula that are broken. In both cases the formulae are embedded in one ePub document that allows users to browse the chemical structure on most reading devices.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {125–128},
numpages = {4},
keywords = {vectorization, e-book conversion, ePub, SVG, chemical structural formula},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361383,
author = {Palfray, Thomas and Hebert, David and Nicolas, St\'{e}phane and Tranouez, Pierrick and Paquet, Thierry},
title = {Logical Segmentation for Article Extraction in Digitized Old Newspapers},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361383},
doi = {10.1145/2361354.2361383},
abstract = {Newspapers are documents made of news item and informative articles. They are not meant to be read iteratively: the reader can pick his items in any order he fancies. Ignoring this structural property, most digitized newspaper archives only offer access by issue or at best by page to their content. We have built a digitization workflow that automatically extracts newspaper articles from images, which allows indexing and retrieval of information at the article level. Our back-end system extracts the logical structure of the page to produce the informative units: the articles. Each image is labelled at the pixel level, through a machine learning based method, then the page logical structure is constructed up from there by the detection of structuring entities such as horizontal and vertical separators, titles and text lines. This logical structure is stored in a METS wrapper associated to the ALTO file produced by the system including the OCRed text. Our front-end system provides a web high definition visualisation of images, textual indexing and retrieval facilities, searching and reading at the article level. Articles transcriptions can be collaboratively corrected, which as a consequence allows for better indexing. We are currently testing our system on the archives of the Journal de Rouen, one of France eldest local newspaper. These 250 years of publication amount to 300 000 pages of very variable image quality and layout complexity. Test year 1808 can be consulted at plair.univ-rouen.fr.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {129–132},
numpages = {4},
keywords = {conditional random field, page layout analysis, document image labelling, structural analysis, articles extraction in newspapers, information extraction from document images, logical structure},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361384,
author = {Kim, Seongchan and Han, Keejun and Kim, Soon Young and Liu, Ying},
title = {Scientific Table Type Classification in Digital Library},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361384},
doi = {10.1145/2361354.2361384},
abstract = {Tables are ubiquitous in digital libraries and on the Web, utilized to satisfy various types of data delivery and document formatting goals. For example, tables are widely used to present experimental results or statistical data in a condensed fashion in scientific documents. Identifying and organizing tables of different types is an absolutely necessary task for better table understanding, and data sharing and reusing. This paper has a three-fold contribution: 1) We propose Introduction, Methods, Results, and Discussion (IMRAD)-based table functional classification for scientific documents; 2) A fine-grained table taxonomy is introduced based on an extensive observation and investigation of tables in digital libraries; and 3) We investigate table characteristics and classify tables automatically based on the defined taxonomy. The preliminary experimental results show that our table taxonomy with salient features can significantly improve scientific table classification performance.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {133–136},
numpages = {4},
keywords = {scientific tables, classification, fine-grained, IMRAD, taxonomy},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361385,
author = {Gabdulkhakova, Aysylu and Hassan, Tamir},
title = {Document Understanding of Graphical Content in Natively Digital PDF Documents},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361385},
doi = {10.1145/2361354.2361385},
abstract = {This paper presents an object-based method for analysing the content drawn by graphical operators in natively digital PDF documents. We propose that graphical content in a document can be classified either as structural or non-structural and present an output model for our analysis result. Heuristic techniques are used to group the instructions into regions and determine their logical role in the document's structure. Experimental results demonstrate the effectiveness of the algorithm.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {137–140},
numpages = {4},
keywords = {non-structural, logical structure, PDF, natively digital, document understanding, document analysis, structural, PDF operator},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/3259287,
author = {Marriott, Kim},
title = {Session Details: Demonstrations and Posters},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259287},
doi = {10.1145/3259287},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
numpages = {1},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361387,
author = {Pruitt, Steve and Wiley, Anthony},
title = {HP Relate: A Customer Communication System for the SMB Market},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361387},
doi = {10.1145/2361354.2361387},
abstract = {Enterprise businesses rely on variable data publishing solutions to produce customer communications, such as letters, statements, and financial reports, which are tailored to individual recipients. Until now, however, such customer communications systems were out of the reach of the small and medium business (SMB) market for several reasons. In order to produce enterprise-quality documents, businesses needed employees with advanced skills in document design and automated document composition. In addition, customized documents typically require scripted business logic and complicated data integration. To achieve this level of document composition and delivery would require the SMB user to have access to IT systems and staffing that would be prohibitively expensive. HP Relate is an innovative document design system that delivers enterprise-quality documents for a next-generation customer communication system for the SMB market. HP Relate features easy-to-use document design tools that require no more than self-assisted training. Document business logic and data integration is accessible to SMB users through common office tools, such as dragging and dropping and spreadsheets. Instead of requiring software installed on the user's system, HP Relate is provisioned on a cloud-based platform using a software as a service (SaaS) subscription-based model. In addition, the HP Relate platform enables SMBs to deliver documents in the format of a customer's choosing, including traditional print forms, web-based deployment, and mobile devices.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {141–144},
numpages = {4},
keywords = {interactive},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361388,
author = {Crozat, St\'{e}phane},
title = {Structured and Fragmented Content in Collaborative XML Publishing Chains},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361388},
doi = {10.1145/2361354.2361388},
abstract = {In this paper, we present the main results of the C2M project through one of its operational deliverable: the Scenari4 collaborative editing and publishing system for XML content. The purpose of the C2M project was to design a system able to manage structured and fragmented contents - as XML publishing chains do - while providing collaborative possibilities - as Enterprise Content Management systems (ECM) do. The main issue is related to transclusion relationships which are massively used in XML publishing chains, in order to support repurposing without copying. This approach is not compatible with the classical way ECMs manage content, especially in terms of propagation of modifications, rights or transactions management. We propose two complementary solutions to manage two different levels of collaboration. The workspace is designed as a highly dynamic place able to deal with live fragments, linked together in a network, that can be easily updated at any time by any user. The library is a more static and more classical way to manage content, dedicated to folder-documents, which are XML frozen versions of sub-networks extracted from workspaces. While workspaces are dedicated to content elaboration and maintenance, libraries are places to store, to read, or to exchange stable documents. Scenari4 is released under FLOSS license and has been being used in several experimental and commercial contexts since the beginning of 2012.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {145–148},
numpages = {4},
keywords = {structured document, transclusion, fragmented document, repurposing, ECM, XML publishing chain},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361389,
author = {Mancilla, Blanca and Beck, Jarryd P. and Plaice, John},
title = {Typesetting Multiple Interacting Streams},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361389},
doi = {10.1145/2361354.2361389},
abstract = {We present a new means for specifying multiple interacting streams, as is needed for documents with multiple systems of notes, side-by-side translations, and critical editions. Each stream is treated as a sequence of components, and anchors are used in the concrete syntax to define reference points used by other streams. When these streams are loaded into memory, the anchors simply become iterators in a container. We present a set of algorithms for the typesetting of multiple streams of text, each with multiple streams of floats and footnotes.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {149–152},
numpages = {4},
keywords = {critical editions, parallel editions, markup and markdown mechanisms, typesetting},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361390,
author = {Lecarpentier, Jean-Marc and Buard, Pierre-Yves and Le Crosnier, Herv\'{e} and Brixtel, Romain},
title = {An Inheritance Model for Documents in Web Applications with Sydonie},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361390},
doi = {10.1145/2361354.2361390},
abstract = {Each web site has to manage documents tailored for its specific needs. When building applications with a specific document model, web developers must make a choice: build from scratch or use existing tools with the need to accomodate the model. We propose an inheritance model for documents, implemented in the Sydonie open source web development framework. It offers a flexible environment to create classes of documents. Sydonie's document model uses entity nodes inspired by the Functional Requirements for Bibliographics Records (FRBR). Document content and metadata are modeled using a set of relations between entity nodes and attribute objects. Classes of documents or attribute types can be defined through a declarative XML file. Our inheritance model provides the possibility to define them at the framework level, application profile level or application level. This demonstration explains the document definition process and inheritance model implemented in the framework and gives several examples of its advantages.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {153–156},
numpages = {4},
keywords = {composite documents, document model, web development framework, document management system},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361391,
author = {Marchese, Francis T. and Shergill, Maninder Pal Kaur},
title = {500 Year Documentation},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361391},
doi = {10.1145/2361354.2361391},
abstract = {Museum visitors today can regularly view 500 year old art by Renaissance masters. Will visitors to museums 500 years in the future be able to see the work of digital artists from the early 21st century? This paper considers the real problem of conserving interactive digital artwork for museum installation in the far distant future by exploring the requirements for creating documentation that will support an artwork's adaptation to future technology. In effect, this documentation must survive as long as the artwork itself -- effectively, in perpetuity. A proposal is made for the use of software engineering methodologies as solutions for designing this documentation.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {157–160},
numpages = {4},
keywords = {conservation, digital art, requirements engineering},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/3259288,
author = {Nicholas, Charles},
title = {Session Details: Search and Sensemaking},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259288},
doi = {10.1145/3259288},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
numpages = {1},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361393,
author = {Hu, Yeming and Milios, Evangelos E. and Blustein, James and Liu, Shali},
title = {Personalized Document Clustering with Dual Supervision},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361393},
doi = {10.1145/2361354.2361393},
abstract = {The potential for semi-supervised techniques to produce personalized clusters has not been explored. This is due to the fact that semi-supervised clustering algorithms used to be evaluated using oracles based on underlying class labels. Although using oracles allows clustering algorithms to be evaluated quickly and without labor intensive labeling, it has the key disadvantage that oracles always give the same answer for an assignment of a document or a feature. However, different human users might give different assignments of the same document and/or feature because of different but equally valid points of view. In this paper, we conduct a user study in which we ask participants (users) to group the same document collection into clusters according to their own understanding, which are then used to evaluate semi-supervised clustering algorithms for user personalization. Through our user study, we observe that different users have their own personalized organizations of the same collection and a user's organization changes over time. Therefore, we propose that document clustering algorithms should be able to incorporate user input and produce personalized clusters based on the user input. We also confirm that semi-supervised algorithms with noisy user input can still produce better organizations matching user's expectation (personalization) than traditional unsupervised ones. Finally, we demonstrate that labeling keywords for clusters at the same time as labeling documents can improve clustering performance further compared to labeling only documents with respect to user personalization.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {161–170},
numpages = {10},
keywords = {document supervision, user supervision, feature supervision, personalization, user interface},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361394,
author = {Widl\"{o}cher, Antoine and Mathet, Yann},
title = {The Glozz Platform: A Corpus Annotation and Mining Tool},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361394},
doi = {10.1145/2361354.2361394},
abstract = {Corpus linguistics and Natural Language Processing make it necessary to produce and share reference annotations to which linguistic and computational models can be compared. Creating such resources requires a formal framework supporting description of heterogeneous linguistic objects and structures, appropriate representation formats, and adequate manual annotation tools, making it possible to locate, identify and describe linguistic phenomena in textual documents. The Glozz platform addresses all these needs, and provides a highly versatile corpus annotation tool with advanced visualization, querying and evaluation possibilities.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {171–180},
numpages = {10},
keywords = {natural language processing, corpus linguistics, annotation formats and tools},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361395,
author = {Geel, Matthias and Church, Timothy and Norrie, Moira C.},
title = {Sift: An End-User Tool for Gathering Web Content on the Go},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361395},
doi = {10.1145/2361354.2361395},
abstract = {Although web sites have started to embed semantic metadata within their documents, it remains a challenge for non-technical end-users to exploit that markup to extract and store information of interest. To address this challenge, we show how tools can be developed that allow users to identify extractable information while browsing and then control how that information should be extracted and stored in a personal library. The proposed approach is based on an extensible framework capable of using different kinds of markup to aid the extraction process and a unique fusion of several well-established techniques from areas such as the semantic web, data warehousing, web scraping and web feeds. We present the Sift tool which is a proof-of-concept implementation of the approach.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {181–190},
numpages = {10},
keywords = {information gathering, information extraction, web content aggregation},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361396,
author = {Peroni, Silvio and Shotton, David and Vitali, Fabio},
title = {Faceted Documents: Describing Document Characteristics Using Semantic Lenses},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361396},
doi = {10.1145/2361354.2361396},
abstract = {The semantic enhancement of a traditional scientific paper is not a straightforward operation, since it involves many different aspects or facets. In this paper we propose eight different semantic lenses through which these facets may be viewed, and describe and exemplify the ontologies by which these lenses may be implemented.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {191–194},
numpages = {4},
keywords = {document semantics, semantic web, semantic publishing},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/3259289,
author = {Schmitz, Patrick},
title = {Session Details: Digital Humanities},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259289},
doi = {10.1145/3259289},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
numpages = {1},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361398,
author = {Mahlow, Cerstin and Gr\"{u}n, Christian and Holupirek, Alexander and Scholl, Marc H.},
title = {A Framework for Retrieval and Annotation in Digital Humanities Using XQuery Full Text and Update in BaseX},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361398},
doi = {10.1145/2361354.2361398},
abstract = {A key difference between traditional humanities research and the emerging field of digital humanities is that the latter aims to complement qualitative methods with quantitative data. In linguistics, this means the use of large corpora of text, which are usually annotated automatically using natural language processing tools. However, these tools do not exist for historical texts, so scholars have to work with unannotated data. We have developed a system for systematic, iterative exploration and annotation of historical text corpora, which relies on an XML database (BaseX) and in particular on the Full Text and Update facilities of XQuery.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {195–204},
numpages = {10},
keywords = {phraseology, XML, XQuery full text, corpus linguistics, database, TEI},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361399,
author = {Tranouez, Pierrick and Nicolas, St\'{e}phane and Dovgalecs, Vladislavs and Burnett, Alexandre and Heutte, Laurent and Liang, Yiqing and Guest, Richard and Fairhurst, Michael},
title = {DocExplore: Overcoming Cultural and Physical Barriers to Access Ancient Documents},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361399},
doi = {10.1145/2361354.2361399},
abstract = {In this paper, we describe DocExplore, an integrated software suite centered on the handling of digitized documents with an emphasis on ancient manuscripts. This software suite allows the augmentation and exploration of ancient documents of cultural interest. Specialists can add textual and multimedia data and metadata to digitized documents through a graphical interface that does not require technical knowledge. They are helped in this endeavor by sophisticated document analysis tools that allows for instance to spot words or patterns in images of documents. The suite is intended to ease considerably the process of bringing locked away historical materials to the attention of the general public by covering all the steps from managing a digital collection to creating interactive presentations suited for cultural exhibitions. Its genesis and sustained development reside in a collaboration of archivists, historians and computer scientists, the latter being not only in charge of the development of the software, but also of creating and incorporating novel pattern recognition for document analysis techniques.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {205–208},
numpages = {4},
keywords = {virtual book, authoring system, document indexing, document image analysis, cultural heritage, historical documents, manuscripts, word spotting},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361400,
author = {Kim, Young-Min and Bellot, Patrice and Tavernier, Jade and Faath, Elodie and Dacos, Marin},
title = {Evaluation of BILBO Reference Parsing in Digital Humanities via a Comparison of Different Tools},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361400},
doi = {10.1145/2361354.2361400},
abstract = {Automatic bibliographic reference annotation involves the tokenization and identification of reference fields. Recent methods use machine learning techniques such as Conditional Random Fields to tackle this problem. On the other hand, the state of the art methods always learn and evaluate their systems with a well structured data having simple format such as bibliography at the end of scientific articles. And that is a reason why the parsing of new reference different from a regular format does not work well. In our previous work, we have established a standard for the tokenization and feature selection with a less formulaic data such as notes. In this paper, we evaluate our system BILBO with other popular online reference parsing tools on a new data from totally different source. BILBO is constructed with our own corpora extracted and annotated from real world data, digital humanities articles of Revues.org site (90% in French) of OpenEdition. The robustness of BILBO system allows a language independent tagging result. We expect that this first attempt of evaluation will motivate the development of other efficient techniques for the scattered and less formulaic bibliographic references.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {209–212},
numpages = {4},
keywords = {automatic annotation, bibliographic reference, comparison, BILBO},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361401,
author = {Worch, Jan-Hendrik and Lawo, Mathias and Gottfried, Bj\"{o}rn},
title = {Glyph Spotting for Mediaeval Handwritings by Template Matching},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361401},
doi = {10.1145/2361354.2361401},
abstract = {This paper reports on the analysis of different approaches in order to search for glyphs within handwritten mediaeval documents. As layout analysis methods are difficult to apply to the documents at hand, template matching methods are employed. A number of different shape descriptions are used to filter out false positives, since the application of correlation coefficients alone results in too many matches. The overall goal consists in the interactive support of an editor who is transcribing a given handwriting. For this purpose, the automatic spotting of glyphs enables the editor to compare glyphs within different contexts.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {213–216},
numpages = {4},
keywords = {mediaeval handwriting, glyph spotting, shape descriptions, correlation coefficient, transcription assistance},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/3259290,
author = {Wiley, Anthony},
title = {Session Details: Architecture and Document Management},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259290},
doi = {10.1145/3259290},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
numpages = {1},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361403,
author = {Soares, Luiz Fernando G. and Soares Neto, Carlos S. and Sousa, Jos\'{e} Geraldo},
title = {Architecture for Hypermedia Dynamic Applications with Content and Behavior Constraints},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361403},
doi = {10.1145/2361354.2361403},
abstract = {This paper deals with the generation of dynamic hypermedia applications whose content and behavior their authors may not be able to predict a priori, but which must conform to a strict set of explicitly defined constraints. In the paper, we show that it is possible to establish an architecture configuration to be followed by this special kind of dynamic applications. In the proposed architecture, templates are responsible for specifying the design patterns and the constraints to be followed. Some alternatives for distributing (from the client side to the server side) the components that comprise the architecture are discussed, and one of them is used to exemplify an instantiation of the architecture. In the instantiation, TAL (Template Authoring Language) is used to define templates. In TAL, templates are open-compositions, that is, especial set of patterns for compositions, whose content must obey some explicitly defined constraints. The paper also shows how the architecture instantiation could be used to build dynamic digital TV applications.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {217–226},
numpages = {10},
keywords = {templates, dynamic DTV applications, TAL, NCL},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361404,
author = {Wong, Raymond K. and Shi, Fengming and Lam, Nicole},
title = {Full-Text Search on Multi-Byte Encoded Documents},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361404},
doi = {10.1145/2361354.2361404},
abstract = {The Burrows Wheeler transform (BWT) has become popular in text compression, full-text search, XML representation, and DNA sequence matching. It is very efficient to perform a full-text search on BWT encoded text using backward search. This paper aims to study different approaches for applying BWT on multi-byte encoded (e.g. UTF-16) text documents. While previous work has studied BWT on word-based models, and BWT can be applied directly on multi-byte encodings (by treating the document as single-byte coded), there has been no extensive study on how to utilize BWT on multi-byte encoded documents for efficient full-text search. Therefore, in this paper, we propose several ways to efficiently backward search multi-byte text documents. We demonstrate our findings using Chinese text documents. Our experiment results show that our extensions to the standard BWT method offer faster search performance and use less runtime memory.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {227–236},
numpages = {10},
keywords = {multi-byte encodings, full-text search, Burrows Wheeler transform},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361405,
author = {Penad\'{e}s, Mª Carmen and G\'{o}mez, Abel and Can\'{o}s, Jos\'{e} H.},
title = {Deriving Document Workflows from Feature Models},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361405},
doi = {10.1145/2361354.2361405},
abstract = {Despite the increasing interest in the Document Engineering community, a formal definition of document workflow is still to come. Often, the term refers to an abstract process consisting in a set of tasks to contribute to some document contents, and some techniques are being developed to support parts of these tasks rather than how to generate the process itself. In most proposals, these tasks are implicit in the business processes running in an organization, lacking an explicit document workflow model that could be analysed and enacted as a coherent unit. In this paper, we propose a document-centric approach to document workflow generation. We have extended the feature-based document meta-model of the Document Product Lines approach with an organiza-tional metamodel. For a given configuration of the feature model, we assign tasks to different members of the organization to con-tribute to the document contents. Moreover, the relationships between features define an ordering of the tasks, which may be refined to produce a specification of the document workflow model automatically. The generation of customized software manuals is used to illustrate the proposal.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {237–240},
numpages = {4},
keywords = {variable data printing, document generation, organizational model, document product lines, document workflow},
location = {Paris, France},
series = {DocEng '12}
}

@inproceedings{10.1145/2361354.2361406,
author = {Mancilla, Blanca and Plaice, John},
title = {Charactles: More than Characters},
year = {2012},
isbn = {9781450311168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361354.2361406},
doi = {10.1145/2361354.2361406},
abstract = {In this paper, we propose a general notion of character which encompasses two concepts: points within a character set, such as Unicode, as well as arbitrary tuples defining structured objects. We call these general characters "charactles". Using this model, text can be defined to be a linear sequence of charactles, not requiring the use of hierarchical structures to encodethe text. As a result, all sorts of processing, such as searching and typesetting, are potentially simplified.},
booktitle = {Proceedings of the 2012 ACM Symposium on Document Engineering},
pages = {241–244},
numpages = {4},
keywords = {character representation, multidimensional text},
location = {Paris, France},
series = {DocEng '12}
}

