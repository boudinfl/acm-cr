@inproceedings{10.1145/3254119,
author = {Schimmler, Sonja},
title = {Session Details: Session 0: Workshops and Tutorials},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254119},
doi = {10.1145/3254119},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
numpages = {1},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967173,
author = {G\"{o}bel, Max and Hassan, Tamir and Oro, Ermelinda and Orsi, Giorgio and Rastan, Roya},
title = {Table Modelling, Extraction and Processing},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967173},
doi = {10.1145/2960811.2967173},
abstract = {This tutorial is targeted at academics and practitioners, both within and outside of the Document Engineering community, who are confronted with table processing tasks such as information extraction and conversion, or have an interest in the topic, and wish to deepen their understanding of the state-of-the-art in this field.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {1–2},
numpages = {2},
keywords = {table understanding, evaluation, semantic interpretation, ground truth, functional analysis, table location, table detection, table structure recognition, competition, table interpretation},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967174,
author = {Nicholas, Charles and Brandon, Robert},
title = {Document Engineering Issues in Malware Analysis},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967174},
doi = {10.1145/2960811.2967174},
abstract = {We present an overview of the field of malware analysis with emphasis on issues related to document engineering. We will introduce the field with a discussion of the types of malware, including executable binaries, malicious PDFs, polymorphic malware, ransomware, and exploit kits. We will conclude with our view of important research questions in the field. This is an updated version of last year's tutorial, with more information about web-based malware and malware targeting the Android market.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {3},
numpages = {1},
keywords = {malware analysis, document engineering},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967169,
author = {Barabucci, Gioele and Borghoff, Uwe M. and Di Iorio, Angelo and Schimmler, Sonja and Munson, Ethan},
title = {Document Changes: Modeling, Detection, Storage and Visualization (DChanges 2016)},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967169},
doi = {10.1145/2960811.2967169},
abstract = {The DChanges series of workshops focuses on changes in all their aspects and applications: algorithms to detect changes, models to describe them and techniques to present them to the final users are only some of the topics we investigate. The workshop is open to researchers and practitioners from industry and academia. This year, we would like to focus on the application of change-tracking and diff algorithms to documents that are collaboratively edited via the web. We will also follow up on the discussion of DChanges 2015 about algorithms and interfaces to better understand and exploit detected changes, and about standards for modeling and transmitting changes. Particular attention will also be given to the use of diff, change tracking, and versioning techniques in the field of digital humanities.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {5–6},
numpages = {2},
keywords = {change analysis and interpretation, change detection, change tracking, applications},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967170,
author = {Piotrowski, Michael},
title = {Future Publishing Formats},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967170},
doi = {10.1145/2960811.2967170},
abstract = {The familiar PDF-based scholarly publishing workflow-which emulates even earlier paper-based workflows-has been surprisingly resistent to change. However, it is becoming increasingly clear that it no longer meets the requirements of a quickly evolving scholarly, technical, and political environment, which includes the trend towards open access publishing, reproducible research, mobile devices, linked open data, and many other developments. This workshop approaches scholarly publishing from a document engineering perspective and focuses on the question of document formats for submission, review, publication, and archival of scholarly publications. We will discuss the current state of scholarly publishing from a document engineering point of view, with the explicit goal of identifying potential alternatives to the current workflow.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {7–8},
numpages = {2},
keywords = {document formats, publishing workflow, markup languages, scholarly publishing},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/3254120,
author = {Sablatnig, Robert},
title = {Session Details: Session 2: Keynote I},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254120},
doi = {10.1145/3254120},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
numpages = {1},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967146,
author = {M\"{u}hlberger, G\"{u}nter},
title = {Research Infrastructures, or How Document Engineering,Cultural Heritage, and Digital Humanities Can Go Together},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967146},
doi = {10.1145/2960811.2967146},
abstract = {Research Infrastructures (RIs) are one of the key concepts in Horizon 2020, the European Commission's Research programme. A budget of EUR 2.7bn is available for projects under the RI programme. The talk will describe some of the main characteristics of RIs and introduce the H2020 Recognition and Enrichment of Archival Documents (READ) project which is dedicated to setting up a highly specialized service platform and making available some of the state-of-the-art technology in pattern recognition and document engineering, namely Handwritten Text Recognition, Automatic Writer Identification, and Keyword Spotting. Archives and libraries, as well as humanities scholars and the general public will be enabled to use the service platform which will improve access to cultural heritage, advance research in humanities and encourage a broad audience to investigate their personal family history.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {9},
numpages = {1},
keywords = {document analysis, research infrastructures, archival documents},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/3254121,
author = {Bagley, Steven R.},
title = {Session Details: Session 3: Layouts and Publishing},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254121},
doi = {10.1145/3254121},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
numpages = {1},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2960820,
author = {Mittelbach, Frank},
title = {A General Framework for Globally Optimized Pagination},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2960820},
doi = {10.1145/2960811.2960820},
abstract = {Pagination problems deal with questions around transforming a source text stream into a formatted document by dividing it up into individual columns and pages, including adding auxiliary elements that have some relationship to the source stream data but may allow a certain amount of variation in placement (such as figures or footnotes).Traditionally the pagination problem has been approached by separating it into one of micro-typography (e.g., breaking text into paragraphs, also known as h&amp;j) and one of macro-typography (e.g., taking a galley of already formatted paragraphs and breaking them into columns and pages) without much interaction between the two.While early solutions for both problem spaces used simple greedy algorithms, Knuth and Plass introduced in the '80s a global-fit algorithm for line breaking that optimizes the breaks across the whole paragraph [1]. This algorithm was implemented in TeX'82 [2] and has since kept its crown as the best available solution for this space. However, for macro-typography there has been no (successful) attempt to provide globally optimized page layout: all systems to date (including TeX) use greedy algorithms for pagination. Various problems in this area have been researched (e.g., [3,4,5,6]) and the literature documents some prototype development. But none of these prototypes have been made widely available to the research community or ever made it into a generally usable and publicly available system.This paper presents a framework for a global-fit algorithm for page breaking based on the ideas of Knuth/Plass. It is implemented in such a way that it is directly usable without additional executables with any modern TeX installation. It therefore can serve as a test bed for future experiments and extensions in this space. At the same time a cleaned-up version of the current prototype has the potential to become a production tool for the huge number of TeX users world-wide.The paper also discusses two already implemented extensions that increase the flexibility of the pagination process: the ability to automatically consider existing flexibility in paragraph length (by considering paragraph variations with different numbers of lines [7]) and the concept of running the columns on a double spread a line long or short. It concludes with a discussion of the overall approach, its inherent limitations and directions for future research.[1] D. E. Knuth and M. F. Plass. Breaking Paragraphs into Lines. Software-Practice and Experience, 11(11):1119-1184, Nov. 1981. [2] D. E. Knuth. TeX: The Program, volume B of Computers and Typesetting. Addison-Wesley, Reading, MA, USA, 1986. [3] A. Br\"{u}ggemann-Klein, R. Klein, and S. Wohlfeil. Computer science in perspective. Chapter On the Pagination of Complex Documents, pages 49-68. Springer-Verlag New York, Inc., New York, NY, USA, 2003. [4] C. Jacobs, W. Li, and D. H. Salesin. Adaptive document layout via manifold content. In Second International Workshop on Web Document Analysis (wda2003), Liverpool, UK, 2003, 2003. [5] A. Holkner. Global multiple objective line breaking. Master's thesis, School of Computer Science and Information Technology, RMIT University, Melbourne, Victoria, Australia, 2006. [6] P. Ciancarini, A. Di Iorio, L. Furini, and F. Vitali. High-quality pagination for publishing. Software-Practice and Experience, 42(6):733-751, June 2012. [7] T. Hassan and A. Hunter. Knuth-Plass revisited: Flexible line-breaking for automatic document layout. In Proceedings of the 2015 ACM Symposium on Document Engineering, DocEng '15, pages 17-20, New York, NY, USA, 2015.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {11–20},
numpages = {10},
keywords = {adaptive layout, typesetting, page breaking, global optimization, pagination, macro-typography, automatic layout},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2960821,
author = {Sch\"{o}lgens, David and M\"{u}ller, Sven and Bauer, Christine and Tilly, Roman and Schoder, Detlef},
title = {Aesthetic Measures for Document Layouts: Operationalization and Analysis in the Context of Marketing Brochures},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2960821},
doi = {10.1145/2960811.2960821},
abstract = {Designing layouts that are perceived as pleasant by the viewer is no easy task: it requires a wide variety of skills, including a sense for aesthetics. When numerous documents with different content need to be created, one of the bottlenecks is to manually create appealing layouts for each document. Thus, automation for aesthetic layout creation is becoming increasingly important. Prerequisite for this automation are algorithms to measure aesthetics. While the literature proposes basic theoretical fundamentals and mathematical formulas as aesthetic measures, researchers have not operationalized these measures yet. This paper presents the challenges associated with and the lessons learned from operationalizing 36 aesthetics measures derived from the literature for the context of marketing brochures. We measured the aesthetics of 744 brochure pages from 10 major retailers and found very strong and highly significant correlations between at least 11 of the aesthetic measures, which represent five latent aesthetic concepts. Still, most of the measures were found to be independent in our sample, and they cover a wide range of different aesthetic concepts. Nevertheless, our results suggest that retailers optimize some of these measures more than others. In terms of the aesthetic measures, retailers seem to design brochure pages in the same way regardless of which category products on this page belong to or if it is the first, last, an odd, or an even page. We propose to consider the quality values of aesthetic measures derived from our analysis of the measured brochures as target values for automated document layout creation for aesthetic marketing brochures.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {21–30},
numpages = {10},
keywords = {layout arrangement, marketing brochures, aesthetics, document layout, aesthetic measures},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967155,
author = {Liu, Lei and Vernica, Rares and Hassan, Tamir and Damera Venkata, Niranjan and Lei, Yang and Fan, Jian and Liu, Jerry and Simske, Steven J. and Wu, Shanchan},
title = {METIS: A Multi-Faceted Hybrid Book Learning Platform},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967155},
doi = {10.1145/2960811.2967155},
abstract = {Today, students are offered a wide variety of alternatives to printed material for the consumption of educational content. Previous research suggests that, while digital content has its advantages, printed content still offers benefits that cannot be matched by digital media. This paper introduces the Meaningful Education and Training Information System (METIS), a multi-faceted hybrid book learning platform. The goal of the system is to provide an easy digital-to-print-to-digital content creation and reading service. METIS incorporates technology for layout, personalization, co-creation and assessment. These facilitate and, in many cases, significantly simplify common teacher/student tasks. Our system has been demonstrated at several international education events, partner engagements, and pilots with local universities and high schools. We present the system and discuss how it enables hybrid learning.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {31–34},
numpages = {4},
keywords = {printing, automated publishing, education, hybrid learning},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/3254122,
author = {Munson, Ethan},
title = {Session Details: Session 5: XML and Data Modelling},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254122},
doi = {10.1145/3254122},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
numpages = {1},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2960818,
author = {Lagos, Nikolaos and Vion-Dury, Jean-Yves},
title = {Digital Preservation Based on Contextualized Dependencies},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2960818},
doi = {10.1145/2960811.2960818},
abstract = {Most of existing efforts in digital preservation have focused on extending the life of documents beyond their period of creation, without taking into account intentions and assumptions made. However, in a continuously evolving setting, knowledge about the context of documents is nearly mandatory for their continuous understanding, use, care, and sustainable governance. In this work we propose a method that considers the preservation of a number of interdependent digital entities, including documents, in conformance with context related information. A change that influences one of these objects can be propagated to the rest of the objects via analysis of their represented dependencies. We propose to represent dependencies not only as simple links but as complex, semantically rich, constructs that encompass context-related information. We illustrate how this method can aid in fine-grained contextually-aware change propagation and impact analysis with a case study.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {35–44},
numpages = {10},
keywords = {context, ontology, dependency, data governance, linked resource model, digital preservation},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2960816,
author = {Barrellon, Vincent and Portier, Pierre-Edouard and Calabretto, Sylvie and Ferret, Olivier},
title = {Schema-Aware Extended Annotation Graphs},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2960816},
doi = {10.1145/2960811.2960816},
abstract = {Multistructured (M-S) documents were introduced as an answer to the need of ever more expressive data models for scholarly annotation, as experienced in the frame of Digital Humanities. Many proposals go beyond XML, that is the gold standard for annotation, and allow the expression of multilevel, concurrent annotation. However, most of them lack support for algorithmic tasks like validation and querying, despite those being central in most of their application contexts.In this paper, we focus on two aspects of annotation: data model expressiveness and validation. We introduce extended Annotation Graphs (eAG), a highly expressive graph-based data model, fit for the enrichment of multimedia resources. Regarding validation of M-S documents, we identify algorithmic complexity as a limiting factor. We advocate that this limitation may be bypassed provided validation can be checked by construction, that is by constraining the shape of data during its very manufacture. So far as we know, no existing validation mechanism for graph-structured data meets this goal. We define here such a mechanism, based on the simulation relation, somehow following a track initiated in Dataguides. We prove that thanks to this mechanism, the validity of M-S data regarding a given schema can be guaranteed without any algorithmic check.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {45–54},
numpages = {10},
keywords = {graphs., schemas, validation, multistructured data model},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967167,
author = {Moreno, Marcio Ferreira and Brandao, Rafael and Cerqueira, Renato},
title = {NCM 3.1: A Conceptual Model for Hyperknowledge Document Engineering},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967167},
doi = {10.1145/2960811.2967167},
abstract = {Most of multimedia documents available today are agnostic to data semantics and their specification language offer little to ease authoring and mechanisms to their players so they can retrieve and present meaningful content to improve user experience. In this paper, we present the main entities of the version 3.1 of the Nested Context Model (NCM), which concentrate efforts at integrating support for enriched knowledge description to the model. This extension enables the specification of relationships between knowledge descriptions in the traditional hypermedia way, composing what we call hyperknowledge in this paper. NCM previous version (NCM 3.0) is a conceptual model for hypermedia document engineering. NCL (Nested Context Language), which is part of international standards and ITU recommendations, is an XML application language that was engineered according to NCM 3.0 definitions. The extensions discussed in this paper contribute not only for advances in the NCL specifications, but mainly as a conceptual model for hyperknowledge document engineering.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {55–58},
numpages = {4},
keywords = {knowledge modeling, hypermedia, hyperknowledge, multimedia documents, document engineering, conceptual model, NCM},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/3254123,
author = {King, Peter R.},
title = {Session Details: Session 7: Text Analysis I: Similarity},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254123},
doi = {10.1145/3254123},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
numpages = {1},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2960817,
author = {Ehsan, Nava and Tompa, Frank Wm. and Shakery, Azadeh},
title = {Using a Dictionary and N-Gram Alignment to Improve Fine-Grained Cross-Language Plagiarism Detection},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2960817},
doi = {10.1145/2960811.2960817},
abstract = {The Web offers fast and easy access to a wide range of documents in various languages, and translation and editing tools provide the means to create derivative documents fairly easily. This leads to the need to develop effective tools for detecting cross-language plagiarism. Given a suspicious document, cross-language plagiarism detection comprises two main subtasks: retrieving documents that are candidate sources for that document and analyzing those candidates one by one to determine their similarity to the suspicious document. In this paper we focus on the second subtask and introduce a novel approach for assessing cross-language similarity between texts for detecting plagiarized cases. Our proposed approach has two main steps: a vector-based retrieval framework that focuses on high recall, followed by a more precise similarity analysis based on dynamic text alignment. Experiments show that our method outperforms the methods of the best results in PAN-2012 and PAN-2014 in terms of plagdet score. We also show that aligning n-gram units, instead of aligning complete sentences, improves the accuracy of detecting plagiarism.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {59–68},
numpages = {10},
keywords = {n-grams, bilingual dictionaries, cross-language plagiarism detection, text alignment},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2960813,
author = {Wang, Xiangru and Nourashrafeddin, Seyednaser and Milios, Evangelos},
title = {Relaxing Orthogonality Assumption in Conceptual Text Document Similarity},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2960813},
doi = {10.1145/2960811.2960813},
abstract = {By reflecting the degree of proximity or remoteness of documents, similarity measure plays the key role in text analytics. Traditional measures, e.g. cosine similarity, assume that documents are represented in an orthogonal space formed by words as dimensions.Words are considered independent from each other and document similarity is computed based on lexical overlap. This assumption is also made in the bag of concepts representation of documents while the space is formed by concepts. This paper proposes new semantic similarity measures without relying on the orthogonality assumption. By employing Wikipedia as an external resource, we introduce five similarity measures using concept-concept relatedness. Experimental results on real text datasets reveal that eliminating the orthogonality assumption improves the quality of text clustering algorithms.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {69–78},
numpages = {10},
keywords = {semantic similarity, text clustering, wikipedia, concept relatedness},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967157,
author = {Knight, Ian A. and Brailsford, David F.},
title = {Enhancing the Searchability of Page-Image PDF Documents Using an Aligned Hidden Layer from a Truth Text},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967157},
doi = {10.1145/2960811.2967157},
abstract = {The search accuracy achieved in a PDF image-plus-hidden-text (PDF-IT) document depends upon the accuracy of the optical character recognition (OCR) process that produced the searchable hidden text layer. In many cases recognising words in a blurred area of a PDF page image may exceed the capabilities of an OCR engine.This paper describes a project to replace an inadequate hidden textual layer of a PDF-IT file with a more accurate hidden layer produced from a 'truth text'. The alignment of the truth text with the image is guided by using OCR-provided page-image co-ordinates, for those glyphs that are correctly recognised, as a set of fixed location points between which other truth-text words can be inserted and aligned with blurred glyphs in the image. Results are presented to show the much enhanced searchability of this new file when compared to that of the original file, which had an OCR-produced hidden layer with no truth-text enhancement.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {79–82},
numpages = {4},
keywords = {searchability, OCR, tesseract, PDF, truth text},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/3254124,
author = {Hassan, Tamir},
title = {Session Details: Session 8: Keynote II},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254124},
doi = {10.1145/3254124},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
numpages = {1},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967145,
author = {Bil'ak, Peter},
title = {Design is Not What You Think It Is},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967145},
doi = {10.1145/2960811.2967145},
abstract = {In this talk, Peter Bil'ak will examine the ways that current publishing practices are rooted in the 19th century, and how in order to move forward, we may have to go back to the roots and reconnect with readers. He will also talk about his recent project, Works That Work magazine, which set out to rethink publishing paradigms, starting with its financing, distribution and production. Works That Work aims to discuss design outside of the traditional design discourse, and Peter Bil'ak will argue for widening the understanding of the design discipline.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {83},
numpages = {1},
keywords = {design disciplines, publishing paradigms, publishing practices},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/3254125,
author = {Piotrowski, Michael},
title = {Session Details: Session 9: Text Analysis II: Classification},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254125},
doi = {10.1145/3254125},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
numpages = {1},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2960819,
author = {Trani, Salvatore and Ceccarelli, Diego and Lucchese, Claudio and Orlando, Salvatore and Perego, Raffaele},
title = {SEL: A Unified Algorithm for Entity Linking and Saliency Detection},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2960819},
doi = {10.1145/2960811.2960819},
abstract = {The Entity Linking task consists in automatically identifying and linking the entities mentioned in a text to their URIs in a given Knowledge Base, e.g., Wikipedia. Entity Linking has a large im- pact in several text analysis and information retrieval related tasks. This task is very challenging due to natural language ambiguity. However, not all the entities mentioned in a document have the same relevance and utility in understanding the topics being dis- cussed. Thus, the related problem of identifying the most relevant entities present in a document, also known as Salient Entities, is attracting increasing interest. In this paper we propose SEL, a novel supervised two-step algo- rithm comprehensively addressing both entity linking and saliency detection. The first step is based on a classifier aimed at identi- fying a set of candidate entities that are likely to be mentioned in the document, thus maximizing the precision of the method with- out hindering its recall. The second step is still based on machine learning, and aims at choosing from the previous set the entities that actually occur in the document. Indeed, we tested two dif- ferent versions of the second step, one aimed at solving only the entity linking task, and the other that, besides detecting linked en- tities, also scores them according to their saliency. Experiments conducted on two different datasets show that the proposed algo- rithm outperforms state-of-the-art competitors, and is able to detect salient entities with high accuracy.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {85–94},
numpages = {10},
keywords = {machine learning, entity linking, salient entities},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967153,
author = {Oevermann, Jan and Ziegler, Wolfgang},
title = {Automated Intrinsic Text Classification for Component Content Management Applications in Technical Communication},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967153},
doi = {10.1145/2960811.2967153},
abstract = {Classification models are used in component content management to identify content components for retrieval, reuse and distribution. Intrinsic metadata, such as the assigned information class, play an important role in these tasks. With the increasing demand for efficient classification of content components, the sector of technical documentation needs mechanisms that allow for an automation of such tasks. Vector space model based approaches can lead to sufficient results, while maintaining good performance, but they must be adapted to the peculiarities that characterize modular technical documents.In this paper we will present domain specific differences, as well as characteristics, that are special to the field of technical documentation and derive methods to adapt widespread classification and retrieval techniques for these tasks. We verify our approach with data provided from companies in the sector of manufacturing and mechanical engineering and use it for supervised learning and automated classification.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {95–98},
numpages = {4},
keywords = {vector space model, machine learning, content management, text classification, technical documentation},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967150,
author = {Kubek, Mario M. and Unger, Herwig},
title = {Centroid Terms as Text Representatives},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967150},
doi = {10.1145/2960811.2967150},
abstract = {Algorithms to topically cluster and classify texts rely on information about their semantic distances and similarities. Standard methods based on the bag-of-words model to determine this information return only rough estimations regarding the relatedness of texts. Moreover, they are per se unable to find generalising terms or abstractions describing the textual contents. A new method to determine centroid terms in texts and to evaluate their similarity using those representing terms will be introduced. In first experiments, its results and advantages will be discussed.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {99–102},
numpages = {4},
keywords = {document similarity, text processing, co-occurrence graph, centroid term},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967161,
author = {Chotnithi, Phanucheep and Takasu, Atsuhiro},
title = {Frequent Multi-Byte Character Subtring Extraction Using a Succinct Data Structure},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967161},
doi = {10.1145/2960811.2967161},
abstract = {Frequent string mining is widely used in text processing to extract text features. Most researchers have focused on text using single-byte characters. Consequently, their applications have problems when applied to text represented with multibyte characters such as Japanese and Chinese text. The main drawback is huge memory us-age for treating multibyte character strings. To solve this problem,we use wavelet tree-based compressed suffix arrays instead of the normal suffix array to reduce the memory usage, and a novel technique that utilizes the rank operation to improve runtime efficiency.Our experimental evaluation shows that the proposed method reduces the processing time by 45% compared with a method usingonly compressed suffix arrays. The proposed method also reduces the memory usage by 75%.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {103–106},
numpages = {4},
keywords = {suffix array, frequent string mining, multibyte, wavelet tree, longest common prefix},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967156,
author = {Cabral, Luciano and Neto, Manoel and Borges, Artur and Lins, Rafael and Lima, Rinaldo and Mello, Rafael and Riss, Marcelo and Simske, Steven J.},
title = {Mobile Summarizer and News Summary Navigator: Two Multilingual News Article Summarization Tools for Mobile Devices},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967156},
doi = {10.1145/2960811.2967156},
abstract = {Mobile devices such as smart phones and tablets are omnipresent in modern societies. Such devices allow browsing the Internet. This paper briefly describes two tools for news article summarization in mobile devices that attempts to automatically collect and sieve the most important information of news article in WebPages.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {107–110},
numpages = {4},
keywords = {mobile applications, text summarization, multilingual summarization},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967168,
author = {Leijen, Daan},
title = {Rendering Mathematics for the Web Using Madoko},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967168},
doi = {10.1145/2960811.2967168},
abstract = {Madoko [6-8] is a novel authoring system for writing complex documents. It is especially well suited for complex academic or industrial documents, like scientific articles, reference manuals, or math-heavy presentations. One particular important aspect of Madoko is to write a document in high-level Markdown [5] with a focus on semantic content. From this document specification we can generate both high-quality PDF output (via LATEX) but also generate highquality HTML that can re-scale and re-flow dynamically. Styling is done through standard CSS attributes and can be done orthogonal to the content.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {111–114},
numpages = {4},
keywords = {madoko, markdown, latex, rendering, mathematics},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967162,
author = {Rastan, Roya and Paik, Hye-Young and Shepherd, John},
title = {A PDF Wrapper for Table Processing},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967162},
doi = {10.1145/2960811.2967162},
abstract = {We propose a PDF document wrapper system that is specifically targeted at table processing applications. We (i) review the PDF specifications and identify particular challenges from the table processing point of view, (ii) specify a table-oriented document model containing the required atomic elements for table extraction and understanding applications. Our evaluation showed that the wrapper was able to detect important features such as page columns, bullets and numbering in all measures, recording over 90% accuracy, leading to better table locating and segmenting.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {115–118},
numpages = {4},
keywords = {document model, PDF wrappers, table processing},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967152,
author = {Shigarov, Alexey and Mikhailov, Andrey and Altaev, Andrey},
title = {Configurable Table Structure Recognition in Untagged PDF Documents},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967152},
doi = {10.1145/2960811.2967152},
abstract = {Today, PDF is one of the most popular document formats in the web. Many PDF documents are not images, but remain untagged. They have no tags for identifying the logical reading order, paragraphs, figures, and tables. One of the challenges with these documents is how to extract tables from them. The paper discusses a new system for table structure recognition in untagged PDF documents. It is formulated as a set of configurable parameters and ad-hoc heuristics for recovering table cells. We consider two different configurations for the system and demonstrate experimental results based on the existing competition dataset for both of them.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {119–122},
numpages = {4},
keywords = {pdf accessibility, untagged pdf documents, table structure recognition, table extraction, pdf document analysis},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967147,
author = {Gradl, Tobias and Henrich, Andreas},
title = {Extending Data Models by Declaratively Specifying Contextual Knowledge},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967147},
doi = {10.1145/2960811.2967147},
abstract = {The research data landscape of the arts and humanities is characterized by a high degree of heterogeneity. To improve interoperability, recent initiatives and research infrastructures are encouraging the use of standards and best practices. However, custom data models are often considered necessary to exactly reflect the requirements of a particular collection or research project. To address the needs of scholars in the arts and humanities for a composition of research data irrespective of the degree of structuredness and standardization, we propose a concept on the basis of formal languages, which facilitates declarative data modeling by respective domain experts. By identifying and defining grammatical patterns and deriving transformation functions, the structure of data is generated or extended in accordance with the particular context and needs of the domain.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {123–126},
numpages = {4},
keywords = {digital humanities, dariah, language applications, descriptive data modeling},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967148,
author = {Calefati, Alessandro and Gallo, Ignazio and Zamberletti, Alessandro and Noce, Lucia},
title = {Using Convolutional Neural Networks for Content Extraction from Online Flyers},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967148},
doi = {10.1145/2960811.2967148},
abstract = {The rise of online shopping has hurt physical retailers, which struggle to persuade customers to buy products in physical stores rather than online. Marketing flyers are a great mean to increase the visibility of physical retailers, but the unstructured offers appearing in those documents cannot be easily compared with similar online deals, making it hard for a customer to understand whether it is more convenient to order a product online or to buy it from the physical shop. In this work we tackle this problem, introducing a content extraction algorithm that automatically extracts structured data from flyers. Unlike competing approaches that mainly focus on textual content or simply analyze font type, color and text positioning, we propose a new approach that uses Convolutional Neural Networks to classify words extracted from flyers typically used in marketing materials to attract the attention of readers towards specific deals. We obtained good results and a high language and genre independence.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {127–130},
numpages = {4},
keywords = {content extraction, portable document format, marketing flyers, convolutional neural network},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967151,
author = {Swoboda, Tobias and Hemmje, Matthias and Dascalu, Mihai and Trausan-Matu, Stefan},
title = {Combining Taxonomies Using Word2vec},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967151},
doi = {10.1145/2960811.2967151},
abstract = {Taxonomies have gained a broad usage in a variety of fields due to their extensibility, as well as their use for classification and knowledge organization. Of particular interest is the digital document management domain in which their hierarchical structure can be effectively employed in order to organize documents into content-specific categories. Common or standard taxonomies (e.g., the ACM Computing Classification System) contain concepts that are too general for conceptualizing specific knowledge domains. In this paper we introduce a novel automated approach that combines sub-trees from general taxonomies with specialized seed taxonomies by using specific Natural Language Processing techniques. We provide an extensible and generalizable model for combining taxonomies in the practical context of two very large European research projects. Because the manual combination of taxonomies by domain experts is a highly time consuming task, our model measures the semantic relatedness between concept labels in CBOW or skip-gram Word2vec vector spaces. A preliminary quantitative evaluation of the resulting taxonomies is performed after applying a greedy algorithm with incremental thresholds used for matching and combining topic labels.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {131–134},
numpages = {4},
keywords = {automated semantic integration, word2vec, taxonomy integration, ontology alignment},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967163,
author = {Tanijiri, Junki and Ohta, Manabu and Takasu, Atsuhiro and Adachi, Jun},
title = {Important Word Organization for Support of Browsing Scholarly Papers Using Author Keywords},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967163},
doi = {10.1145/2960811.2967163},
abstract = {When new researchers read scholarly papers, they often encounter unfamiliar technical terms, which may require considerable time to investigate. We have been developing a user interface to support the browsing of scholarly papers, which can provide useful links to information about such technical terms. The interface displays "important terms" extracted from a paper on top of the image of the paper. In this study, we organize the important terms extracted from papers by using author keywords. We first identify the important terms and then associate them with author keywords by using a method based on the word2vec model. Experiments showed that our method improved the classification accuracy of important terms compared with a simple baseline method. It associated each author keyword with about 2.5 relevant important terms.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {135–138},
numpages = {4},
keywords = {word2vec, scholarly paper, author keyword, TF-IDF, browsing interface, browsing support},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967164,
author = {Li, Baoli},
title = {Selecting Features with Class Based and Importance Weighted Document Frequency in Text Classification},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967164},
doi = {10.1145/2960811.2967164},
abstract = {Document Frequency (DF), which counts how many documents a feature appears in, is reported by Yang and Pedersen [1] to be quite effective for feature selection in text classification. Features with the same DF value are likely to have different appearance distribution over categories, and demonstrate quite different discriminative powers for classification. However, the original DF metric is class independent and does not consider features' distribution over classes. On the other hand, different features play different roles in delivering the content of a document. The chosen features are expected to be the important ones, which carry the main information of a document collection. However, the traditional DF metric considers features equally important. To overcome simultaneously the above two problems of the original document frequency metric, we propose a class based and importance weighted document frequency measure. Preliminary experiments on two text classification datasets do validate the effectiveness of the proposed metric.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {139–142},
numpages = {4},
keywords = {text categorization, document frequency, text classification, feature filtering, feature selection},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967165,
author = {Sfikas, Giorgos and Louloudis, Georgios and Stamatopoulos, Nikolaos and Gatos, Basilis},
title = {Bayesian Mixture Models on Connected Components for Newspaper Article Segmentation},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967165},
doi = {10.1145/2960811.2967165},
abstract = {In this paper we propose a new method for automated segmentation of scanned newspaper pages into articles. Article regions are produced as a result of merging sub-article level content and title regions. We use a Bayesian Gaussian mixture model to model page Connected Component information and cluster input into sub-article components. The Bayesian model is conditioned on a prior distribution over region features, aiding classification into titles and content. Using a Dirichlet prior we are able to automatically estimate correctly the number of title and article regions. The method is tested on a dataset of digitized historical newspapers, where visual experimental results are very promising.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {143–146},
numpages = {4},
keywords = {newspaper layout analysis, article tracking, page segmentation, document understanding, greek historical documents, connected component analysis, finite mixture models, bayesian inference},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/3254126,
author = {Bulterman, Dick},
title = {Session Details: Session 13: Text Analysis III: Summarization},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254126},
doi = {10.1145/3254126},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
numpages = {1},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967158,
author = {Ferreira, Rodolfo and Ferreira, Rafael and Lins, Rafael Dueire and Oliveira, Hil\'{a}rio and Riss, Marcelo and Simske, Steven J.},
title = {Appling Link Target Identification and Content Extraction to Improve Web News Summarization},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967158},
doi = {10.1145/2960811.2967158},
abstract = {The existing automatic text summarization systems whenever applied to web-pages of news articles show poor performance as the text is encapsulated within a HTML page. This paper takes advantage of the link identification and content extraction techniques. The results show the validity of such a strategy.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {197–200},
numpages = {4},
keywords = {link identification, summarization, content extraction},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967159,
author = {Batista, Jamilson and Lins, Rafael Dueire and Lima, Rinaldo and Simske, Steven J. and Riss, Marcelo},
title = {Towards Cohesive Extractive Summarization through Anaphoric Expression Resolution},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967159},
doi = {10.1145/2960811.2967159},
abstract = {This paper presents a new method for improving the cohesiveness of summaries generated by extractive summarization systems. The solution presented attempts to improve the legibility and cohesion of the generated summaries through coreference resolution. It is based on a post-processing step that binds dangling coreference to the most important entity in a given coreference chain. The proposed solution was evaluated on the CNN corpus of 3,000 news articles, using four state-of-the-art summarization systems and seventeen techniques for sentence scoring proposed in the literature. The experimental results may be considered encouraging, as the final summaries reached better ROUGE scores, besides being more cohesive.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {201–204},
numpages = {4},
keywords = {anaphoric expressions, coreference resolution, cohesive summarization},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967160,
author = {Oliveira, Hil\'{a}rio and Lima, Rinaldo and Lins, Rafael Dueire and Freitas, Fred and Riss, Marcelo and Simske, Steven J.},
title = {Assessing Concept Weighting in Integer Linear Programming Based Single-Document Summarization},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967160},
doi = {10.1145/2960811.2967160},
abstract = {Some of the recent state-of-the-art systems for Automatic Text Summarization rely on the concept-based approach using Integer Linear Programming (ILP), mainly for multi-document summarization. A study on the suitability of such an approach to single-document summarization is still missing, however. This work presents an assessment of several methods of concept weighing for a concept-based ILP approach on the single-document summarization scenario. The unigram and bigram representations for concepts are also investigated. The experimental results obtained on the DUC 2001-2002 and the CNN corpora show that bigrams are more suitable than unigrams for the representation of concepts. Among the concept scoring methods investigated, the sentence position method presented the best performance on all evaluation corpora.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {205–208},
numpages = {4},
keywords = {concept-based ilp approaches, single-document summarization, automatic text summarization},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/3254127,
author = {Brailsford, David F.},
title = {Session Details: Session 15: Applications and Security},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254127},
doi = {10.1145/3254127},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
numpages = {1},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2960812,
author = {Laiola Guimar\~{a}es, Rodrigo and Avegliano, Priscilla and Villa Real, Lucas C.},
title = {A Lightweight and Efficient Mechanism for Fixing the Synchronization of Misaligned Subtitle Documents},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2960812},
doi = {10.1145/2960811.2960812},
abstract = {Online subtitle databases allow users to easily find subtitle documents in multiple languages for thousands of films and TV series episodes. However, getting the subtitle document that gives satisfactory synchronization on the first attempt is like hitting the jackpot. The truth is that this process often involves a lot of trial- and-error because multiple versions of subtitle documents have distinct synchronization references, given that they are targeted at variations of the same audiovisual content. Building on our previous efforts to address this problem, in this paper we formalize and validate a two-phase subtitle synchronization framework. The benefit over current approaches lays in the usage of audio fingerprint annotations generated from the base audio signal as second-level synchronization anchors. This way, we allow the media player to dynamically fix during playback the most common cases of subtitle synchronization misalignment that compromise users' watching experience. Results from our evaluation process indicate that our framework has minimal impact on existing subtitle documents and formats as well as on the playback performance.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {175–184},
numpages = {10},
keywords = {SRT, subtitles, audio fingerprinting, synchronization},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967154,
author = {Denoue, Laurent and Carter, Scott and Cooper, Matthew},
title = {DocuGram: Turning Screen Recordings into Documents},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967154},
doi = {10.1145/2960811.2967154},
abstract = {In this paper we describe DocuGram, a novel tool to capture and share documents originating from any application. As users scroll through pages of their document inside the native application (Word, Google Docs, web browser), the system captures and analyses in real-time the rendered video frames and reconstitutes the original document pages into an easy to view HTML-based representation. In addition to detecting and regenerating the document pages, a DocuGram also includes the interactions users had over them, e.g. mouse motions and voice comments. A DocuGram allows users to flexibly share enhanced documents across applications.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {185–188},
numpages = {4},
keywords = {video processing, image processing, document capture, interactive documents},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967149,
author = {Ben Charrada, Eya and Mussato, Stefan},
title = {An Exploratory Study on Managing and Searching for Documents in Software Engineering Environments},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967149},
doi = {10.1145/2960811.2967149},
abstract = {A large number of documents are usually produced in the software industry. In this work, we conduct a qualitative study to explore the main practices and challenges related to managing these documents. The results of this study are based on interviews with 13 practitioners from nine companies. The main findings of the study are: (1) much data is stored in e-mails and in meeting protocols, (2) practitioners like wikis, (3) when searching for documents, practitioners would rather browse the structure than use the search function and (4) searching for documents is still a challenge due to the low effectiveness of search functions and the scattering of documents over several locations and tools.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {189–192},
numpages = {4},
keywords = {software engineering, qualitative study, document management, industrial study},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2967166,
author = {Sturgill, Margaret and Simske, Steven J.},
title = {Mass Serialization Method for Document Encryption Policy Enforcement},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2967166},
doi = {10.1145/2960811.2967166},
abstract = {Analytics obtained during the creation of a database of mass serialized codes can also be used to help enforcement of encryption policy on documents. In this paper, we introduce a set of metrics which complement traditional NIST cryptography methods -- 4 mass serialization and one entropy metric -- which in combination can allow a discrimination between encrypted vs. zipped files. We describe the use of these methods to identify a broad range of non-randomness in number sets, and apply them to a more mundane problem-that of automatic assessment of the encryption state of a corpora of documents.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {193–196},
numpages = {4},
keywords = {classification, mass serialization, encryption policy enforcement},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/3254128,
author = {Simske, Steven J.},
title = {Session Details: Session 16: Visual Document Analysis},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254128},
doi = {10.1145/3254128},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
numpages = {1},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2960822,
author = {Jana, Prerana and Majumdar, Anubhab and Mandal, Sekhar and Chanda, Bhabatosh},
title = {Generation of Search-Able PDF of the Chemical Equations Segmented from Document Images},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2960822},
doi = {10.1145/2960811.2960822},
abstract = {PDF format of scanned document images is not searchable. OCR tries to remedy this adversity by converting document images into editable and searchable data, but it has its own limitations in presence of equations - both mathematical and chemical. OCR system for mathematical equation is already a major research area and has provided successful result. However, chemical equation segmentation has been a less ventured road. In this paper, we present a novel method for automated generation of searchable PDF format of segmented chemical equations from scanned document images by performing chemical symbol recognition and auto-correction of OCR output. We use existing OCR system, pattern recognition technique, contextual data analysis and a standard LaTeX package to generate the chemical equation in searchable PDF format. The effectiveness of the proposed method is verified through exhaustive testing on 234 document images.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {147–156},
numpages = {10},
keywords = {morphological operation, chemical equations, mathematical symbols},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2960815,
author = {Granell, Emilio and Mart\'{\i}nez-Hinarejos, Carlos-D.},
title = {A Multimodal Crowdsourcing Framework for Transcribing Historical Handwritten Documents},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2960815},
doi = {10.1145/2960811.2960815},
abstract = {Transcription of handwritten historical documents is one of the main topics in document analysis systems, due to cultural reasons. State-of-the-art handwritten text recognition systems allow to speed up the transcription task. Currently, this automatic transcription is far from perfect, and human expert revision is required in order to obtain the actual transcription. In this context, crowdsourcing emerged as a powerful tool for massive transcription at a relatively low cost, since the supervision effort of professional transcribers may be dramatically reduced. However, current transcription crowdsourcing platforms are mainly limited to the use of non-mobile devices, since the use of keyboards in mobile devices is not friendly enough for most users. This work presents the alternative of using speech dictation of handwritten text lines as transcription source in a crowdsourcing platform. The experiments explore how an initial handwritten text recognition hypothesis can be improved by using the contribution of speech recognition from several speakers, providing as a final result a better hypothesis to be amended by a professional transcriber with less effort.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {157–163},
numpages = {7},
keywords = {speech recognition, crowdsourcing framework, multimodal combination, historical handwritten transcription},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2960811.2960814,
author = {Noce, Lucia and Gallo, Ignazio and Zamberletti, Alessandro and Calefati, Alessandro},
title = {Embedded Textual Content for Document Image Classification with Convolutional Neural Networks},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2960814},
doi = {10.1145/2960811.2960814},
abstract = {In this paper we introduce a novel document image classification method based on combined visual and textual information. The proposed algorithm's pipeline is inspired to the ones of other recent state-of-the-art methods which perform document image classification using Convolutional Neural Networks. The main addition of our work is the introduction of a preprocessing step embedding additional textual information into the processed document images. To do so we combine Optical Character Recognition and Natural Language Processing algorithms to extract and manipulate relevant text concepts from document images. Such textual information is then visually embedded within each document image to improve the classification results of a Convolutional Neural Network. Our experiments prove that the overall document classification accuracy of a Convolutional Neural Network trained using these text-augmented document images is considerably higher than the one achieved by a similar model trained solely on classic document images, especially when different classes of documents share similar visual characteristics.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {165–173},
numpages = {9},
keywords = {natural language processing, document image classification, convolutional neural network},
location = {Vienna, Austria},
series = {DocEng '16}
}

