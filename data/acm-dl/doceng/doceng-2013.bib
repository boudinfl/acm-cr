@inproceedings{10.1145/3253542,
author = {Marinai, Simone},
title = {Session Details: Keynote Address},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253542},
doi = {10.1145/3253542},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
numpages = {1},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494291,
author = {Esposito, Floriana},
title = {Symbolic Machine Learning Methods for Historical Document Processing},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494291},
doi = {10.1145/2494266.2494291},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {1–2},
numpages = {2},
keywords = {incremental learning, concept learning methods, semantic processing, inductive logic programming},
location = {Florence, Italy},
series = {DocEng '13}
}

@dataset{10.1145/review-2494266.2494291_R49339,
author = {Hodgson, Jonathan P. E.},
title = {Review ID:R49339 for DOI: 10.1145/2494266.2494291},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-2494266.2494291_R49339}
}

@inproceedings{10.1145/3253543,
author = {Vitali, Fabio},
title = {Session Details: Digital Humanities},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253543},
doi = {10.1145/3253543},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
numpages = {1},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494275,
author = {Bagley, Steven R. and Brailsford, David F. and Kernighan, Brian W.},
title = {Revisiting a Summer Vacation: Digital Restoration and Typesetter Forensics},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494275},
doi = {10.1145/2494266.2494275},
abstract = {In 1979 the Computing Science Research Center ('Center 127') at Bell Laboratories bought a Linotron 202 typesetter from the Mergenthaler company. This was a 'third generation' digital machine that used a CRT to image characters onto photographic paper. The intent was to use existing Linotype fonts and also to develop new ones to exploit the 202's line-drawing capabilities.Use of the 202 was hindered by Mergenthaler's refusal to reveal the inner structure and encoding mechanisms of the font files. The particular 202 was further dogged by extreme hardware and software unreliability.A memorandum describing the experience was written in early 1980 but was deemed to be too "sensitive" to release. The original troff input for the memorandum exists and now, more than 30 years later, the memorandum can be released. However, the only available record of its visual appearance was a poor-quality scanned photocopy of the original printed version.This paper details our efforts in rebuilding a faithful retypeset replica of the original memorandum, given that the Linotron 202 disappeared long ago, and that this episode at Bell Labs occurred 5 years before the dawn of PostScript (and later PDF) as de facto standards for digital document preservation.The paper concludes with some lessons for digital archiving policy drawn from this rebuilding exercise.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {3–12},
numpages = {10},
keywords = {chess fonts, troff, archiving, reverse engineering, linotron 202, digital restoration, postscript fonts},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494288,
author = {Agosti, Maristella and Conlan, Owen and Ferro, Nicola and Hampson, Cormac and Munnelly, Gary},
title = {Interacting with Digital Cultural Heritage Collections via Annotations: The CULTURA Approach},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494288},
doi = {10.1145/2494266.2494288},
abstract = {This paper introduces the main characteristics of the digital cultural collections that constitute the use cases presently in use in the CULTURA environment. A section on related work follows giving an account on efforts on the management of digital annotations that are pertinent and that have been considered. Afterwards the innovative annotation features of the CULTURA portal for digital humanities are described; those features are aimed at improving the interaction of non-specialist users and general public with digital cultural heritage content. The annotation functions consist of two modules: the FAST annotation service as back-end and the CAT Web front-end integrated in the CULTURA portal. The annotation features have been, and are being, tested with different types of users and useful feedback is being collated, with the overall aim of generalising the approach to diverse document collections and not only the area of cultural heritage.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {13–22},
numpages = {10},
keywords = {adaptive environment, annotation, digital libraries and archives, digital library system, cultural heritage collections, digital cultural heritage collections, digital humanities, hypertext},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494304,
author = {Torabi, Katayoun and Durgan, Jessica and Tarpley, Bryan},
title = {Early Modern OCR Project (EMOP) at Texas A&amp;M University: Using Aletheia to Train Tesseract},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494304},
doi = {10.1145/2494266.2494304},
abstract = {Great effort is being made to collect and preserve historic manuscripts from the early modern and eighteenth-century periods; unfortunately, searching the Early English Books Online (EEBO) and Eighteenth Century Collections Online (ECCO) collections can be extremely difficult for researchers because current Optical Character Recognition (OCR) engines struggle to read and recognize various historic fonts, especially in manuscripts of declining quality. To address this problem, the Early Modern OCR Project (eMOP) at the Initiative for the Digital Humanities, Media, and Culture (IDHMC) at Texas A&amp;M University seeks to train OCR engines to read historic documents more effectively in order to make the entirety of these collections accessible to searching. The first step in this project involves using Aletheia Desktop Tool, developed by PRImA Research Lab at the University of Salford, to use documents from the EEBO and ECCO collections to create training sets to aid OCR engines, such as Google's Tesseract, in recognizing the special characters such as ligatures, italics, and blackletter found within early modern fonts. In the year that the Aletheia team has been working to create these font training libraries, we have overcome several problems, including learning how to select, extract, and deliver the data that best suits Tesseract training requirements. This work with Aletheia is part of a larger scholarly project that endeavors to not only make the EEBO and ECCO collections more accessible for data mining purposes for researchers, but also seeks to make available to the public the methodologies, workflow, and digital tools developed during the eMOP project to aid libraries, museums, and scholars in other fields in their efforts to preserve and study our combined cultural history.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {23–26},
numpages = {4},
keywords = {sql, xml, c#, xslt},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/3253544,
author = {Lumley, John},
title = {Session Details: Dealing with Multiple Versions},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253544},
doi = {10.1145/3253544},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
numpages = {1},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494277,
author = {Ba, M. Lamine and Abdessalem, Talel and Senellart, Pierre},
title = {Uncertain Version Control in Open Collaborative Editing of Tree-Structured Documents},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494277},
doi = {10.1145/2494266.2494277},
abstract = {In order to ease content enrichment, exchange, and sharing, web-scale collaborative platforms such as Wikipedia or Google Docs enable unbounded interactions between a large number of contributors, without prior knowledge of their level of expertise and reliability. Version control is then essential for keeping track of the evolution of the shared content and its provenance. In such environments, uncertainty is ubiquitous due to the unreliability of the sources, the incompleteness and imprecision of the contributions, the possibility of malicious editing and vandalism acts, etc. To handle this uncertainty, we use a probabilistic XML model as a basic component of our version control framework. Each version of a shared document is represented by an XML tree and the whole document, together with its different versions, is modeled as a probabilistic XML document. Uncertainty is evaluated using the probabilistic model and the reliability measure associated to each source, each contributor, or each editing event, resulting in an uncertainty measure on each version and each part of the document. We show that standard version control operations can be implemented directly as operations on the probabilistic XML model; efficiency with respect to deterministic version control systems is demonstrated on real-world datasets.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {27–36},
numpages = {10},
keywords = {version control, xml, collaborative work, uncertain data},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494278,
author = {N\'{e}delec, Brice and Molli, Pascal and Mostefaoui, Achour and Desmontils, Emmanuel},
title = {LSEQ: An Adaptive Structure for Sequences in Distributed Collaborative Editing},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494278},
doi = {10.1145/2494266.2494278},
abstract = {Distributed collaborative editing systems allow users to work distributed in time, space and across organizations. Trending distributed collaborative editors such as Google Docs, Etherpad or Git have grown in popularity over the years. A new kind of distributed editors based on a family of distributed data structure replicated on several sites called Conflict-free Replicated Data Type (CRDT for short) appeared recently. This paper considers a CRDT that represents a distributed sequence of basic elements that can be lines, words or characters (sequence CRDT). The possible operations on this sequence are the insertion and the deletion of elements. Compared to the state of the art, this approach is more decentralized and better scales in terms of the number of participants. However, its space complexity is linear with respect to the total number of inserts and the insertion points in the document. This makes the overall performance of such editors dependent on the editing behaviour of users. This paper proposes and models LSEQ, an adaptive allocation strategy for a sequence CRDT. LSEQ achieves in the average a sub-linear spatial-complexity whatever is the editing behaviour. A series of experiments validates LSEQ showing that it outperforms existing approaches.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {37–46},
numpages = {10},
keywords = {distributed documents, distributed collaborative editing, real-time editing, document authoring tools and systems, conflict-free replicated data types},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494284,
author = {Barabucci, Gioele},
title = {Introduction to the Universal Delta Model},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494284},
doi = {10.1145/2494266.2494284},
abstract = {There are currently no shared formalization of the output of diff algorithms, the so called deltas. From a theoretical point of view, without such a formalization it is difficult to compare the output of different algorithms. In more practical terms, the lack of a shared formalization makes it hard to create tools that support more than one diff algorithm.This paper introduces the universal delta model: a formal definition of changes (the pieces of information that records that something has changed), operations (the definitions of the kind of change that happened) and deltas (coherent summaries of what has changed between two documents). The fundamental mechanism that makes the changes as defined in the universal delta model a very expressive tool, is the use of encapsulation relations between changes: changes are not only simple records of what has changed, they can also be combined into more complex changes to express the fact that the algorithm has detected more nuanced kinds of changes. The universal delta model has been applied successfully in various projects that served as an evaluation for the model. In addition to the model itself, this paper briefly describes one of these projects: the measurement of objective qualities of deltas as produced by various diff algorithms.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {47–56},
numpages = {10},
keywords = {diff, edit script, versioning, patch, delta model},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494269,
author = {Pandey, Meenu and Munson, Ethan V.},
title = {Version Aware LibreOffice Documents},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494269},
doi = {10.1145/2494266.2494269},
abstract = {Version control systems provide a methodology for maintaining changes to a document over its lifetime and provide better management and control of evolving document collections, such as source code for large software systems. However, no version control system supports similar functionalities for office documents.Version Aware XML documents integrate full versioning functionality into an XML document type, using XML namespaces to avoid document type errors. Version aware XML documents contain a preamble with versions stored in reverse delta format, plus unique ID attributes attached to the nodes of the documents. They support the full branching and merging functionalities familiar to software engineers, in contrast to the constrained versioning models typical of Office applications.LibreOffice is an open source office document suite which is widely used for document creation. Each document is represented in the Open Office Document Format, which is a collection of XML files. The current project is an endeavor to show the practicality of the version aware XML documents approach by modifying the LibreOffice document suite to support version awareness. We are modifying LibreOffice to accept and preserve both the preamble and the IDs of the version aware framework. Initially, other functionality will be provided by wrapper applications and independent tools, but full integration into the LibreOffice user interface is envisioned.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {57–60},
numpages = {4},
keywords = {version aware, user collaboration, xml},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/3253545,
author = {Landoni, Monica},
title = {Session Details: Search &amp; Sense Making},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253545},
doi = {10.1145/3253545},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
numpages = {1},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494279,
author = {Nourashrafeddin, Seyednaser and Milios, Evangelos and Arnold, Dirk},
title = {Interactive Text Document Clustering Using Feature Labeling},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494279},
doi = {10.1145/2494266.2494279},
abstract = {We propose an interactive text document method, which is based on term labeling. The algorithm asks the user to cluster the top keyterms associated with document clusters iteratively. The keyterm clusters are used to guide the clustering method. Rather than using standard clustering algorithms, we propose a new text clusterer using term clusters. Terms that exist in a document corpus are clustered. Using a greedy approach, the term clusters are distilled in order to remove non-discriminative general terms. We then present a heuristic approach to extract seed documents associated with each distilled term cluster. These seeds are finally used to cluster all documents. We compared our interactive term labeling to a baseline interactive term selection algorithm on some real standard text datasets. The experiments show that with a comparable amount of user effort, our term labeling is more effective than the baseline term selection method.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {61–70},
numpages = {10},
keywords = {term clouds, active feature supervision, term clustering},
location = {Florence, Italy},
series = {DocEng '13}
}

@dataset{10.1145/review-2494266.2494279_R49561,
author = {Can, Fazli},
title = {Review ID:R49561 for DOI: 10.1145/2494266.2494279},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-2494266.2494279_R49561}
}

@inproceedings{10.1145/2494266.2494280,
author = {Srivastava, Ajitesh and Soto, Axel J. and Milios, Evangelos},
title = {A Graph-Based Topic Extraction Method Enabling Simple Interactive Customization},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494280},
doi = {10.1145/2494266.2494280},
abstract = {It is often desirable to identify the concepts that are present in a corpus. A popular way to deal with this objective is to discover clusters of words or topics, for which many algorithms exist in the literature. Yet most of these methods lack the interpretability that would enable interaction with a user not familiar with their inner workings. The paper proposes a graph-based topic extraction algorithm, which can also be viewed as a soft-clustering of words present in a given corpus. Each topic, in the form of a set of words, represents an underlying concept in the corpus. The method allows easy interpretation of the clustering process, and hence enables the scope of user involvement at various steps. For a quantitative evaluation of the topics extracted, we use them as features to get a compact representation of documents for classification tasks. We compare the classification accuracy achieved by a reduced feature set obtained with our method versus other topic extraction techniques, namely Latent Dirichlet Allocation and Non-negative Matrix Factorization. While the results from all the three algorithms are comparable, the speed and easy interpretability of our algorithm makes it more appropriate to be used interactively by lay users.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {71–80},
numpages = {10},
keywords = {topic extraction, soft clustering, visual text mining},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494282,
author = {Wu, Zhaohui and Das, Sujatha and Li, Zhenhui and Mitra, Prasenjit and Giles, C. Lee},
title = {Searching Online Book Documents and Analyzing Book Citations},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494282},
doi = {10.1145/2494266.2494282},
abstract = {Academic search engines and digital libraries provide convenient online search and access facilities for scientific publications. However, most existing systems do not include books in their collections although several books are freely available online. Academic books are different from papers in terms of their length, contents and structure. We argue that accounting for academic books is important in understanding and assessing scientific impact. We introduce an open-book search engine that extracts and indexes metadata, contents, and bibliography from online PDF book documents. To the best of our knowledge, no previous work gives a systematical study on building a search engine for books.We propose a hybrid approach for extracting title and authors from a book that combines results from CiteSeer, a rule based extractor, and a SVM based extractor, leveraging web knowledge. For "table of contents" recognition, we propose rules based on multiple regularities based on numbering and ordering. In addition, we study bibliography extraction and citation parsing for a large dataset of books. Finally, we use the multiple fields available in books to rank books in response to search queries. Our system can effectively extract metadata and contents from large collections of online books and provides efficient book search and retrieval facilities.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {81–90},
numpages = {10},
keywords = {book structure extraction, book citation analysis, book search},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494312,
author = {Williams, Kyle and Giles, C. Lee},
title = {Near Duplicate Detection in an Academic Digital Library},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494312},
doi = {10.1145/2494266.2494312},
abstract = {The detection and potential removal of duplicates is desirable for a number of reasons, such as to reduce the need for unnecessary storage and computation, and to provide users with uncluttered search results. This paper describes an investigation into the application of scalable simhash and shingle state of the art duplicate detection algorithms for detecting near duplicate documents in the CiteSeerX digital library. We empirically explored the duplicate detection methods and evaluated their performance and application to academic documents and identified good parameters for the algorithms. We also analyzed the types of near duplicates identified by each algorithm. The highest F-scores achieved were 0.91 and 0.99 for the simhash and shingle-based methods respectively. The shingle-based method also identified a larger variety of duplicate types than the simhash-based method.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {91–94},
numpages = {4},
keywords = {simhash, near duplicate detection, shingles},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/3253546,
author = {Munson, Ethan},
title = {Session Details: Architecture &amp; Processes},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253546},
doi = {10.1145/3253546},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
numpages = {1},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494305,
author = {Kaczorek, Jerzy and Wiszniewski, Bogdan},
title = {Augmenting Digital Documents with Negotiation Capability},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494305},
doi = {10.1145/2494266.2494305},
abstract = {Active digital documents are not only capable of performing various operations using their internal functionality and external services, accessible in the environment in which they operate, but can also migrate on their own over a network of mobile devices that provide dynamically changing execution contexts. They may imply conflicts between preferences of the active document and the device the former wishes to execute on. In the paper we propose a solution for solving such conflicts with automatic negotiations, allowing documents and devices to find contracts satisfying both sides. It is based on a simple bargaining model reinforced with machine learning mechanisms to classify string sequences representing negotiation histories.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {95–98},
numpages = {4},
keywords = {mobile computing, automatic bargaining, active document},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494309,
author = {Sadallah, Madjid and Encelle, Beno\^{\i}t and Mared, Azze-Eddine and Prie, Yannick},
title = {A Framework for Usage-Based Document Reengineering},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494309},
doi = {10.1145/2494266.2494309},
abstract = {This ongoing work investigates usage-based document reengineering as a means to support authors in modifying their documents. Document usages (i.e. usage feedbacks) cover readers' explicit annotations and their reading traces. We first describe a conceptual framework with various levels of assistance for document reengineering: indications on reading, problem detection, reconception suggestions and automatic reconception propositions, taking our example in e-learning document management. We then present a technical framework for usage-based document reengineering and its associated models for documents, annotations and traces representation.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {99–102},
numpages = {4},
keywords = {traces, reading usages, digital reading, annotations, document reconception, document reengineering},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/3253547,
author = {Nicholas, Charles},
title = {Session Details: Document Recognition &amp; Analysis I},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253547},
doi = {10.1145/3253547},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
numpages = {1},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494299,
author = {Duthil, Benjamin and Coustaty, Mickael and Courboulay, Vincent and Ogier, Jean-Marc},
title = {Visual Saliency and Terminology Extraction for Document Annotation},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494299},
doi = {10.1145/2494266.2494299},
abstract = {The document digitization process becomes a crucial economical issue in our society. Then, it becomes necessary to be able to organize this huge amount of documents. The work proposed in this paper tends to propose a new method to automatically classify document using a saliency-based segmentation process on one hand, and a terminology extraction and annotation on the other hand. The saliency-based segmentation is used to extract salient regions and by the way logo, while the terminology approach is used to annotate them and to automatically classify the document. The approach does not require human expertise, and use Google Images as a knowledge database. The results obtained on a real database of 1766 documents show the relevance of the approach.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {103–106},
numpages = {4},
keywords = {visual saliency, terminology extraction, document annotation},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494301,
author = {Neves, Renata Freire de Paiva and Zanchettin, Cleber and Mello, Carlos Alexandre Barros},
title = {An Adaptive Thresholding Algorithm Based on Edge Detection and Morphological Operations for Document Images},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494301},
doi = {10.1145/2494266.2494301},
abstract = {This paper presents a new algorithm to threshold document images. The proposed algorithm deal with complex background images, illumination and aspect variants, back-to-front interference, variation of brightness and different positioned shadows. The algorithm have two phases. The first one uses edge detection and morphological operations to identify the text on the image. The second phase uses the positions of the text to define the threshold value in an adaptive process. Our approach presents promising results in images with complex background released from the Document Image Binarization Contest (DIBCO) when compared with other literature and competition thresholding algorithms.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {107–110},
numpages = {4},
keywords = {handwritten document, thresholding, segmentation, binarization},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494302,
author = {Esser, Daniel and Muthmann, Klemens and Schuster, Daniel},
title = {Information Extraction Efficiency of Business Documents Captured with Smartphones and Tablets},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494302},
doi = {10.1145/2494266.2494302},
abstract = {Businesses and large organizations currently prefer scanners to incorporate paper documents into their electronic document archives. While cameras integrated into mobile devices such as smartphones and tablets are commonly available, it is still unclear how using a mobile device for document capture influences document content recognition. This is especially important for information extraction carried out on documents captured in a mobile scenario. Therefore this paper presents a set of experiments to compare automatic index data extraction from business documents in a static and in a mobile case. The paper shows which decline in extraction one can expect, explains the reasons and gives a short overview over possible solutions.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {111–114},
numpages = {4},
keywords = {evaluation, information extraction, mobile tagging},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494303,
author = {Carel, Elodie and Courboulay, Vincent and Burie, Jean-Christophe and Ogier, Jean-Marc},
title = {Dominant Color Segmentation of Administrative Document Images by Hierarchical Clustering},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494303},
doi = {10.1145/2494266.2494303},
abstract = {This paper addresses the problem of color documents images segmentation in an industrial context. Automated Document Recognition (ADR) systems highly reduce time and resource costs of companies by managing their huge amount of administrative documents, and by optimizing their workflow. Most of the time, a binarization is performed due to their historical industrial process. Therefore, colorimetric information can improve the process. In this paper, we propose a hierarchical clustering based approach to extract dominant color masks of documents. Indeed, our dataset comprises different kind of scanned administrative document images such as invoices, forms, letters, and so on. We do not know a priori the number of dominant colors on our documents. These masks will further feed the inputs to an OCR in order to bring extra-information about the colorimetric context. This approach requires neither user interaction nor setting steps. Experiments on several types of documents show the relevance of the proposed approach},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {115–118},
numpages = {4},
keywords = {dominant colors, document analysis, clustering},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494307,
author = {Satkhozhina, Aziza and Ahmadullin, Ildus and Allebach, Jan P.},
title = {Optical Font Recognition Using Conditional Random Field},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494307},
doi = {10.1145/2494266.2494307},
abstract = {Automated publishing systems require large databases containing document page layout templates. Most of these layout templates are created manually. A lower cost alternative is to extract document page layouts from existing documents. In order to extract the layout from a scanned document image, it is necessary to perform Optical Font Recognition (OFR) since the font is an important element in layout design. In this paper, we use the Conditional Random Field (CRF) model to perform OFR. First, we extract typographical features of the text. Then, we train the probabilistic model using a log-linear parameterization of CRF. The advantage of using CRF is that it does not assume that the typographical features are independent of each other. We demonstrate the effectiveness of this approach on a set of 616 fonts.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {119–122},
numpages = {4},
keywords = {document design, page layout, conditional random fields, optical font recognition},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494315,
author = {Alvaro, Francisco and Zanibbi, Richard},
title = {A Shape-Based Layout Descriptor for Classifying Spatial Relationships in Handwritten Math},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494315},
doi = {10.1145/2494266.2494315},
abstract = {We consider the difficult problem of classifying spatial relationships between symbols and subexpressions in handwritten mathematical expressions. We first improve existing geometric features based on bounding boxes and center points, normalizing them using the distance between the centers of the two symbols or subexpressions in question. We then propose a novel feature set for layout classification, using polar histograms computed over points in handwritten strokes. A series of experiments are presented in which a Support Vector Machine is used with these new features to classify spatial relationships of five types in the MathBrush corpus (horizontal, superscript, subscript, below, and inside (e.g. in a square root)). The normalized geometric features provide an improvement over previously published results, while the shape-based features provide a natural representation with results comparable to those for the geometric features. Combining the features produced a very small improvement in accuracy.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {123–126},
numpages = {4},
keywords = {shape descriptors, math recognition, spatial relationship classification},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494318,
author = {Faigenbaum, Shira and Shaus, Arie and Sober, Barak and Turkel, Eli and Piasetzky, Eli},
title = {Evaluating Glyph Binarizations Based on Their Properties},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494318},
doi = {10.1145/2494266.2494318},
abstract = {Document binary images, created by different algorithms, are commonly evaluated based on a pre-existing ground truth. Previous research found several pitfalls in this methodology and suggested various approaches addressing the issue. This article proposes an alternative binarization quality evaluation solution for binarized glyphs, circumventing the ground truth. Our method relies on intrinsic properties of binarized glyphs. The features used for quality assessment are stroke width consistency, presence of small connected components (stains), edge noise, and the average edge curvature. Linear and tree-based combinations of these features are also considered. The new methodology is tested and shown to be nearly as sound as human experts' judgments.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {127–130},
numpages = {4},
keywords = {ground truth, glyph, quality measure, evaluation, binarization},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/3253548,
author = {Bagley, Steven},
title = {Session Details: Document Layout &amp; Presentation Generation I},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253548},
doi = {10.1145/3253548},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
numpages = {1},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494274,
author = {Lumley, John W.},
title = {Functional, Extensible, Svg-Based Variable Documents},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494274},
doi = {10.1145/2494266.2494274},
abstract = {Architectures for documents that vary in response to binding to data, or user interaction, are usually based on limited layout semantics, such as text flows, and simple data variability, such as replacing reserved constructs. By using a generalised XML graphical representation (SVG), decorated with an extensible set of layout intent declarations, and with embedded fragments of XSLT decorated with program retention directives, it is possible to produce self-contained documents that are both highly flexible and extensible and can adapt their presentation to multiple stages of data binding, as well as user interaction. The essentials of the architecture are presented with examples and details of the necessary implementation and support tools, most of which are written in declarative, functional XSLT. Recent developments in XSLT technologies make it possible to consider such documents operating within unmodified browsers - techniques are discussed.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {131–140},
numpages = {10},
keywords = {document construction, xslt, svg, functional programming},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494276,
author = {Ahmadullin, Ildus and Damera-Venkata, Niranjan},
title = {Hierarchical Probabilistic Model for News Composition},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494276},
doi = {10.1145/2494266.2494276},
abstract = {We present a method for the automated composition of personalized newspapers. Traditional newsprint composition is a laborious and expensive manual process. We develop a two level hierarchical page layout model that models aesthetic design choices using local (within article region) and global (page level) prior probability distributions. Given content to be composed, our model can infer the best way to divide a page into layout regions and simultaneously optimize content fit within these regions. We automate decisions on how to paginate articles, flow article text across pages, crop images, adjust whitespace etc. for the best overall newspaper compositions. We also show how content editing which is a very important task in the traditional news workflow can be incorporated in a semi-automated manner within our framework. Our model is a generalization of our prior work on probabilistic modeling of single-flow layouts to enable multiple article flows on a page, while still allowing one or more articles that may break on a page and continue on subsequent pages.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {141–150},
numpages = {10},
keywords = {newsaper layout, probabilistic document model, automated publishing},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494285,
author = {Piccoli, Ricardo and Oliveira, Jo\~{a}o Batista},
title = {Balancing Font Sizes for Flexibility in Automated Document Layout},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494285},
doi = {10.1145/2494266.2494285},
abstract = {This paper presents an improved approach for automatically laying out content onto a document page, where the number and size of the items are unknown in advance. Our solution leverages earlier results from Oliveira (2008) wherein layouts are modeled by a guillotine partitioning of the page. The benefit of such method is its efficiency and ability to place as many items on a page as desired. In our model, items have flexible representations and texts may freely change their font sizes to fit a particular area of the page. As a consequence, the optimization goal is to find a layout that produces the least noticeable difference between font sizes, in order to obtain the most aesthetically pleasing layout. Finding the best areas for text requires knowledge of how typesetting engines actually render text for a particular setting. As such, we also model the behavior of the TeX typesetting engine when computing the height to be occupied by a text block as a function of the font size, text length and line width. An analytical approximation for text placement is then presented, refined by using curve fitting over TeX-generated data. As a practical result, the resulting layouts for a newspaper generation application are also presented. Finally, we discuss these results and directions for further research.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {151–160},
numpages = {10},
keywords = {tex, automatic document layout, guillotine layout, typography, personalized documents},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/3253549,
author = {Simske, Steven},
title = {Session Details: Document Recognition &amp; Analysis II},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253549},
doi = {10.1145/3253549},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
numpages = {1},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494281,
author = {Thanh-Ha, Do and Tabbone, Salvatore and Ramos Terrades, Oriol},
title = {Document Noise Removal Using Sparse Representations over Learned Dictionary},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494281},
doi = {10.1145/2494266.2494281},
abstract = {In this paper, we propose an algorithm for denoising document images using sparse representations. Following a training set, this algorithm is able to learn the main document characteristics and also, the kind of noise included into the documents. In this perspective, we propose to model the noise energy based on the normalized cross-correlation between pairs of noisy and non-noisy documents. Experimental results on several datasets demonstrate the robustness of our method compared with the state-of-the-art.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {161–168},
numpages = {8},
keywords = {noise suppression, sparse representation, learned dictionary, normalized cross correlation, k-svd},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494300,
author = {Vilares, David and Alonso, Miguel \'{A}ngel and G\'{o}mez-Rodr\'{\i}guez, Carlos},
title = {Supervised Polarity Classification of Spanish Tweets Based on Linguistic Knowledge},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494300},
doi = {10.1145/2494266.2494300},
abstract = {We describe a system that classifies the polarity of Spanish tweets. We adopt a hybrid approach, which combines machine learning and linguistic knowledge acquired by means of NLP. We use part-of-speech tags, syntactic dependencies and semantic knowledge as features for a supervised classifier. Lexical particularities of the language used in Twitter are taken into account in a pre-processing step. Experimental results improve over those of pure machine learning approaches and confirm the practical utility of the proposal.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {169–172},
numpages = {4},
keywords = {document analysis, twitter, linguistic analysis, sentiment analysis, opinion mining, machine learning},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494272,
author = {Ferilli, Stefano and Esposito, Floriana and Redavid, Domenico},
title = {Hi-Fi HTML Rendering of Multi-Format Documents in DoMinUS},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494272},
doi = {10.1145/2494266.2494272},
abstract = {Digital Libraries collect, organize and provide to end users large quantities of selected documents. While these documents come in a variety of formats, it is desirable that they are delivered to final users in a uniform way. Web formats are a suitable choice for this purpose. While Web documents are very flexible as to layout presentation, that is determined at runtime by the interpreter, documents coming from a library should preserve their original layout when displayed to final users. Using raster images would not allow the user to access the actual content of the document's components (text and images). This paper presents a technique to render in an HTML file the original layout of a document, preserving the peculiarity of its components (text, images, formulas, tables, algorithms). It builds on the DoMInUS framework, that can process documents in several source formats.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {173–176},
numpages = {4},
keywords = {document representation, layout analysis, document rendering},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494271,
author = {Constantin, Alexandru and Pettifer, Steve and Voronkov, Andrei},
title = {PDFX: Fully-Automated PDF-to-XML Conversion of Scientific Literature},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494271},
doi = {10.1145/2494266.2494271},
abstract = {PDFX is a rule-based system designed to reconstruct the logical structure of scholarly articles in PDF form, regardless of their formatting style. The system's output is an XML document that describes the input article's logical structure in terms of title, sections, tables, references, etc. and also links it to geometrical typesetting markers in the original PDF, such as paragraph and column breaks. The key aspect of the presented approach is that the rule set used relies on relative parameters derived from font and layout specifics of each article, rather than on a template-matching paradigm. The system thus obviates the need for domain- or layout-specific tuning or prior training, exploiting only typographical conventions inherent in scientific literature. Evaluated against a significantly varied corpus of articles from nearly 2000 different journals, PDFX gives a 77.45 F1 measure for top-level heading identification and 74.03 for extracting individual bibliographic items. The service is freely available for use at http://pdfx.cs.man.ac.uk/.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {177–180},
numpages = {4},
keywords = {logical structure recovery, pdf conversion, pdfx, document structure analysis},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494319,
author = {Di Iorio, Angelo and Peroni, Silvio and Poggi, Francesco and Vitali, Fabio and Shotton, David},
title = {Recognising Document Components in XML-Based Academic Articles},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494319},
doi = {10.1145/2494266.2494319},
abstract = {Recognising textual structures (paragraphs, sections, etc.) provides abstract and more general mechanisms for describing documents independent of the particular semantics of specific markup schemas, tools and presentation stylesheets. In this paper we propose an algorithm that allows us to identify the structural role of each element in a set of homogeneous scientific articles stored as XML files.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {181–184},
numpages = {4},
keywords = {xml, doco, document components},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/3253550,
author = {Ferilli, Stefano},
title = {Session Details: Metadata &amp; Annotation},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253550},
doi = {10.1145/3253550},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
numpages = {1},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494306,
author = {Harinek, Jozef and \v{S}imko, Mari\'{a}n},
title = {Improving Term Extraction by Utilizing User Annotations},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494306},
doi = {10.1145/2494266.2494306},
abstract = {Automated acquisition of relevant domain terms from educational documents available in social educational systems can benefit from processing a growing number of user-created annotations assigned to the content. Annotations provide us potentially useful information about documents and can improve the results of base Automatic Term Recognition (ATR) algorithms. We propose a method for relevant domain terms extraction based on user-created annotations processing. We consider three basic annotation types: tags, comments and highlights. The final term weight is computed by combining relevant domain terms weights obtained from the individual annotation types and those obtained from the text. The method was evaluated using data from Principles of Software Engineering course in adaptive educational system ALEF and showed that enhancements based on annotation processing yield significant improvement of results.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {185–188},
numpages = {4},
keywords = {learning, metadata creation, document analysis, lightweight semantics, social annotation, relevant domain term extraction, atr},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494308,
author = {Vion-Dury, Jean-Yves},
title = {Using RDFS/OWL to Ease Semantic Integration of Structured Documents},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494308},
doi = {10.1145/2494266.2494308},
abstract = {This paper defines i/ an RDFS/OWL schema to capture the syntactic structure of marked-up documents and ii/ the (reversible) transposition of any XML/SGML/HTML document into a set of conformant RDF triples that convey the relevant tree information, be it meta-information (structure of the tree, attributes, XML comments...) or basic information (textual content).The translation we propose reuses predefined semantics of RDFS and OWL W3C standards, thus making tree manipulation and transformations homogeneous to common RDF semantic models; once translated, operations on XML/SGML/HTML trees can be much easily integrated into Semantic Web applications (and this applies particularly well to emerging HTML notational systems such as RDFa or micro-formats).Where this makes sense for application areas, specific document schemas can be totally or partially translated into supplemental RDFS/OWL constraints manageable by inference engines complying with the W3C standards.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {189–192},
numpages = {4},
keywords = {document models, rdfs, owl, xml representation},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494321,
author = {Rahtz, Sebastian and Burnard, Lou},
title = {Reviewing the TEI ODD System},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494321},
doi = {10.1145/2494266.2494321},
abstract = {For many years the Text Encoding Initiative (TEI) has maintained a specialised high-level XML vocabulary in the 'literate programming' paradigm to define its influential Guidelines, from which schemas or DTDs in other schema languages are derived. This paper reviews the development of this vocabulary, known as ODD (for 'One Document Does it all'). We discuss some problems with the language, and propose solutions to make it more complete and extensible.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {193–196},
numpages = {4},
keywords = {tei odd},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494268,
author = {Rinaldi, Fabio},
title = {Assisted Editing in the Biomedical Domain: Motivation and Challenges.},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494268},
doi = {10.1145/2494266.2494268},
abstract = {One of the characteristics of biomedical scientific literature is the high ambiguity of the domain-specific terminology which can be used to describe technical concepts and specific objects of the domain. This is partly due to the very broad scope of the domain of interest and partly to inherent properties of the terminology itself. There aresimply very large numbers of genes, proteins, organs, cell lines, cellular phenomena, experimental methods, and so on. For example, UniProt, the most authoritative protein database, currently contains more than 33 million entries. Clearly, the names which are typically used to refer to proteins are polysemic and might refer to hundreds of different entries in a reference database.Such a large and extensive terminology necessarily makes it difficult to derive from the literature a simplified representation of the entities and relationships described in the articles, despite considerable efforts by the text mining community. In this paper we propose to complement such efforts with editing tools that can assist the authors in efficiently adding to their publications a minimal semantic annotation so that much of the ambiguity is avoided.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {197–200},
numpages = {4},
keywords = {text mining, biomedical literature processing},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494270,
author = {\v{S}imko, Mari\'{a}n and Franta, Martin and Habd\'{a}k, Martin and Vrablecov\'{a}, Petra},
title = {Managing Content, Metadata and User-Created Annotations in Web-Based Applications},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494270},
doi = {10.1145/2494266.2494270},
abstract = {We introduce a tool aimed to facilitate the management of content, metadata and social annotations assigned to documents in semantic web-based applications. The COME2T (COllaboration- and MEtadata-oriented COntent Management EnvironmenT) allows easy administration of lightweight semantics for the provided content and user-created annotations, which are often created as a result of implicit collaboration between users of a web-based application. We present the tool's most important features and briefly describe a pilot application of the tool when used to manage content for the adaptive learning portal ALEF.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {201–204},
numpages = {4},
keywords = {social annotation, semantic web, lightweight semantics, content management, adaptation, user-created content},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/3253551,
author = {Bulterman, Dick},
title = {Session Details: Multimedia I},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253551},
doi = {10.1145/3253551},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
numpages = {1},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494283,
author = {Azevedo, Roberto Gerson de Albuquerque and Santos, Rodrigo Costa Mesquita and Ara\'{u}jo, Eduardo Cruz and Soares, Luiz Fernando Gomes and Soares Neto, Carlos de Salles},
title = {Multimedia Authoring Based on Templates and Semi-Automatic Generated Wizards},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494283},
doi = {10.1145/2494266.2494283},
abstract = {Templates have been used to engage non-expert multimedia authors as content producers. In template-based authoring, templates with most of the relevant application logic and application constraints are developed by experts, who must also specify the template semantics, report which are the required gaps to be filled in, and how to do so. Filling template's gaps is the single task left to inexperienced users to produce the final applications. To do that, they usually must understand the padding instructions reported by template authors and learn some specific padding language. An alternative is using specific GUI components created specifically to each new developed template. This paper proposes a semi-automatic generation of GUI Wizards to guide end(-user) authors to create multimedia applications. The wizard can be tuned to improve the communication between the template author and the template end user, and also if the template specification is not complete. Many successful trial cases show that the generated wizards are usually simple enough to be used by non-experts. The contributions coming from this paper is not constrained to any specific template language or final-application format. Nevertheless, aiming at testing the proposal it was instantiated to work with TAL (Template Authoring Language) whose template processors can generate applications in different target languages.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {205–214},
numpages = {10},
keywords = {multimedia, wizards, template-based authoring, authoring, tal},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494313,
author = {Denoue, Laurent and Carter, Scott and Cooper, Matthew},
title = {Content-Based Copy and Paste from Video Documents},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494313},
doi = {10.1145/2494266.2494313},
abstract = {Unlike text, copying and pasting parts of video documents is challenging. Yet, the abundance of video documents now available including how-to tutorials requires simpler tools that allow users to easily copy and paste fragments of video materials into new documents. We describe new direct video manipulation techniques enabling users to quickly copy and paste content from video documents into a user's own multimedia document. While the video plays, users interact with the video canvas to select text regions, scrollable regions, slide sequences built up across many frames, or semantically meaningful regions such as dialog boxes. Instead of relying on the timeline to accurately select sub-parts of the video document, users navigate using familiar selection techniques such as mouse-wheel to scroll back and forward over a video shot in which the content scrolls, double-clicks over rectangular regions to select them, or clicks and drags over textual regions of the video canvas to select them. We describe the video processing techniques that run in real-time in modern web browsers using HTML5 and JavaScript; and show how they help users quickly copy and paste video fragments into new documents, allowing them to efficiently reuse video documents for authoring or note-taking.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {215–218},
numpages = {4},
keywords = {document authoring tools, real-time video document processing, user interaction for content reuse, video document structure and analysis},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494267,
author = {Cunha, Bruna C.R. and Machado Neto, Olib\'{a}rio J. and Pimentel, Maria da Gra\c{c}a},
title = {MoViA: A Mobile Video Annotation Tool},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494267},
doi = {10.1145/2494266.2494267},
abstract = {The user interaction with mobile devices has dramatically improved over the last years. Increasingly we rely on smartphones and tablets for a wider range of tasks. Modern mobile devices enable users to access, manage and transmit multiple types of media in an easy, convenient and portable way. In this context, the playback of videos on mobile devices becomes a usual activity. Many works regarding video annotations have been made, but few are concerned with the mobile scenario. The ability to add annotations and to share them with others is a content enriching process which can improve activities from educational to entertainment purposes. In this paper, we present an intuitive tool that allows users to perform temporal video annotations on mobile devices. Using conventional tablets and smartphones equipped with the Android operating system, text, audio and digital ink annotations can be made on any video. It is possible to share text annotations with other users and play multiple annotations at the same time. The several display sizes and the possibility to switch between portrait and landscape mode have also been considered.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {219–222},
numpages = {4},
keywords = {mobile devices, user interfaces, video annotation, authoring},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/3253552,
author = {King, Peter},
title = {Session Details: Posters &amp; Demonstrations},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253552},
doi = {10.1145/3253552},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
numpages = {1},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494292,
author = {Wells, Christopher Alan and Jirak, Joel and Pruitt, Steve and Wiley, Anthony J.},
title = {Enterprise Document System Cloud Deployment},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494292},
doi = {10.1145/2494266.2494292},
abstract = {The software used by enterprise businesses for creating variable-data customer documents must be highly reliable, and vendors are increasingly distributing such software via the cloud as an online service. This means that vendors now assume responsibility for the IT resources hosting and supporting the software as well as the customer documents and data. Vendors also assume responsibility for pushing updates to all customers simultaneously. To support the test and release of new versions, software vendors must deploy and configure the software at an unprecedented rate.To reduce the time spent deploying and configuring software in the cloud, and to minimize the chance for human error, we present StackLauncher. By making it possible to automatically configure and launch software "stacks" with push-button simplicity, StackLauncher is a valuable addition to the software development lifecycle for cloud deployment of enterprise document software.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {223–224},
numpages = {2},
keywords = {tool, document systems, cloud-deployment, enterprise},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494293,
author = {Nguyen, Nhu Van and Ogier, Jean-Marc and Charneau, Franck},
title = {Bag of Subjects: Lecture Videos Multimodal Indexing},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494293},
doi = {10.1145/2494266.2494293},
abstract = {In this paper, we address multimodal indexing and retrieval for videos of lectures or seminars. This paper proposes a combination of technologies respectively issuing from image document analysis and text mining. Based on visual information and textual information extracted from slide images, we investigate a Bag of mixed Words (visual words and textual words) model to represent lecture slide's contents. Lecture videos are indexed and retrieved by using extended Bag of Words model. In this model, it is assumed that a video may contain multiple subjects; and this model discovers the visual representation of these subjects automatically and indexes the video accordingly. We discuss the mixed text/image query and proposed indexing approach for retrieval lecture videos and report a quantitative evaluation on lecture videos of our Lab.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {225–226},
numpages = {2},
keywords = {lecture videos, multimodal indexing, bag of words},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494294,
author = {S\'{a}nchez, Joan Andreu and M\"{u}hlberger, G\"{u}nter and Gatos, Basilis and Schofield, Philip and Depuydt, Katrien and Davis, Richard M. and Vidal, Enrique and de Does, Jesse},
title = {TranScriptorium: A European Project on Handwritten Text Recognition},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494294},
doi = {10.1145/2494266.2494294},
abstract = {The tranScriptorium project aims to develop innovative, efficient and cost-effective solutions for annotating handwritten historical documents using modern, holistic Handwritten Text Recognition (HTR) technology. Three actions are planned in tranScriptorium: i) improve basic image preprocessing and holistic HTR techniques; ii) develop novel indexing and keyword searching approaches; and iii) capitalize on new, user-friendly interactive-predictive HTR approaches for computer-assisted operation.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {227–228},
numpages = {2},
keywords = {hanwritten text recognition, interactive trancription},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494295,
author = {Matos da Silva, Luiz Augusto and Laerte N. da Silva, Luiz and Mattoso, Marta and Braganholo, Vanessa},
title = {On the Performance of the Position() XPath Function},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494295},
doi = {10.1145/2494266.2494295},
abstract = {In very large XML documents or collections, the query response times are not always satisfactory. To overcome this limitation, parallel processing can be applied. Data can be replicated in several processors and queries can be partitioned to run over different virtual data partitions on each processor, on an approach called virtual partitioning. PartiX-VP is a simple XML virtual partitioning approach that generates virtual data partitions by dividing the cardinality of the partitioning attribute by the number of allocated processors, resulting in intervals of equal size for each processor. In this approach, the XML query is rewritten and selection predicates are added to define the virtual partitions. These selection predicates use the position() XPath function that addresses a set of elements on a given position in the document. In this paper, we present an experimental evaluation of the position() XPath function in five XML native DBMS. We have identified differences in the processing time of the position() XPath function in large collections of XML documents. This may lead to load unbalancing in simple virtual partitioning approaches, thus this analysis opens space for improvements in virtual partitioning.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {229–230},
numpages = {2},
keywords = {scale and performance, xml query processing.},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494296,
author = {Marcacini, Ricardo Marcondes and Rezende, Solange Oliveira},
title = {Incremental Hierarchical Text Clustering with Privileged Information},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494296},
doi = {10.1145/2494266.2494296},
abstract = {In many text clustering tasks, there is some valuable knowledge about the problem domain, in addition to the original textual data involved in the clustering process. Traditional text clustering methods are unable to incorporate such additional (privileged) information into data clustering. Recently, a new paradigm called LUPI - Learning Using Privileged Information - was proposed by Vapnik to incorporate privileged information in classification tasks. In this paper, we extend the LUPI paradigm to deal with text clustering tasks. In particular, we show that the LUPI paradigm is potentially promising for incremental hierarchical text clustering, being very useful for organizing large textual databases. In our method, the privileged information about the text documents is applied to refine an initial clustering model by means of consensus clustering. The initial model is used for incremental clustering of the remaining text documents. We carried out an experimental evaluation on two benchmark text collections and the results showed that our method significantly improves the clustering accuracy when compared to a traditional hierarchical clustering method.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {231–232},
numpages = {2},
keywords = {privileged information, incremental hierarchical clustering},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494297,
author = {Yeloglu, Ozge and Milios, Evangelos and ZIncir-Heywood, A. Nur},
title = {Beyond Term Clusters: Assigning Wikipedia Concepts to Scientific Documents},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494297},
doi = {10.1145/2494266.2494297},
abstract = {We propose a model for assigning Wikipedia Concepts as scientific category labels to scientific documents where their terms are first grouped together using the well-known topic modelling method, Latent Dirichlet Allocation (LDA) and then assigned to Wikipedia Concepts by wikification. We wikify the terms of the topic model of a document to extract related concepts from Wikipedia. We experiment on two different datasets: the abstracts of the documents from the ACM Digital Library and the full papers of the UvT Collection. The ACM dataset includes Computer Science publications whereas UvT includes scientific publications from a range of topics. Domain specific taxonomies are used for evaluation. Results show that our approach is able to assign Wikipedia Concepts to the scientific publications in an automated manner, removing any need for human supervision.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {233–234},
numpages = {2},
keywords = {topic labelling, scientific corpora, categorizing term clusters},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494298,
author = {Yuan, Dayu and Mitra, Prasenjit},
title = {Cross Language Indexing and Retrieval of the Cypriot Digital Antiquities Repository},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494298},
doi = {10.1145/2494266.2494298},
abstract = {We design and implement a cross-language retrieval system for the Cypriot Digital Antiquities Repository (cyDAR). Users can query either by English and Ancient Greek to search for documents written in Ancient Greek. Because of the lack of dictionary and parallel corpus, we use translation machine to translate the documents. We index both the original Ancient Greek text and translated English text to facilitated multi-language search.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {235–236},
numpages = {2},
keywords = {cross language information system, machine translation},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/3253553,
author = {Damera-Venkata, Niranjan},
title = {Session Details: Document Layout &amp; Presentation Generation II},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253553},
doi = {10.1145/3253553},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
numpages = {1},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494310,
author = {Pinkney, Alexander J. and Bagley, Steven R. and Brailsford, David F.},
title = {No Need to Justify Your Choice: Pre-Compiling Line Breaks to Improve EBook Readability},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494310},
doi = {10.1145/2494266.2494310},
abstract = {Implementations of eBooks have existed in one form or another for at least the past 20 years, but it is only in the past 5 years that dedicated eBook hardware has become a mass-market item.New screen technologies, such as e-paper, provide a reading experience similar to those of physical books, and even backlit LCD and OLED displays are beginning to have high enough pixel densities to render text crisply at small point sizes. Despite this, the major element of the physical book that has not yet made the transition to the eBook is high-quality typesetting.The great advantage of eBooks is that the presentation of the page can adapt, at rendering time, to the physical screen size and to the reading preferences of the user. Until now, simple first-fit line-breaking algorithms have had to be used in order to give acceptable rendering speed whilst conserving battery life.This paper describes a system for producing well-typeset, scalable document layouts for eBook readers, without the computational overhead normally associated with better-quality typesetting. We precompute many of the complex parts of the typesetting process, and perform the majority of the 'heavy lifting' at document compile-time, rather than at rendering time. Support is provided for floats (such as figures in an academic paper, or illustrations in a novel), for arbitrary screen sizes, and also for arbitrary point-size changes within the text.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {237–240},
numpages = {4},
keywords = {typesetting, ebooks, document layout},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494311,
author = {Marinai, Simone},
title = {Reflowing and Annotating Scientific Papers on EBook Readers},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494311},
doi = {10.1145/2494266.2494311},
abstract = {Working with scientific and technical papers on small screen devices, such as tablets and eBook readers, is difficult since these works are often typeset in multiple columns with a relatively small font size.On tablets, pan and zoom operations allow users to visualize the text in the desired size, however, tracing the text in multiple columns can be uneasy and not appropriate for studying and working with the scientific works. Moreover, these operations are slow on most e-ink eBook readers that have limited computation resources. Document reflow is in this case one option, but it is difficult to provide a satisfactory visualization of scientific and technical papers.In this paper, we describe one off-line tool for scientific document reflow that adopts document image processing techniques to generate one modified version of the original PDF organized as a single column text that can be easily visualized on eBook readers. Moreover, the tool allows the user to make free-form annotations on the modified paper using the tools of the eBook reader. These annotations are faithfully reproduced in the original two-column document.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {241–244},
numpages = {4},
keywords = {annotations, epub, ebook, svg},
location = {Florence, Italy},
series = {DocEng '13}
}

@dataset{10.1145/review-2494266.2494311_R49187,
author = {Sugiyama, Kazunari},
title = {Review ID:R49187 for DOI: 10.1145/2494266.2494311},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-2494266.2494311_R49187}
}

@inproceedings{10.1145/2494266.2494316,
author = {Truran, Mark and Siddle, Jonathan and Georg, Gersende and Cavazza, Marc},
title = {Automatic Generation of Limited-Depth Hyper-Documents from Clinical Guidelines},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494316},
doi = {10.1145/2494266.2494316},
abstract = {Research suggests that browsing clinical guidelines in a linear format is difficult for users. One national producer of clinical guidelines (HAS, the French National Authority for Health) has recently developed a new document format designed to improve accessibility. It is a limited-depth hypertext structurally constrained so that all information lies within two clicks of a central index ('reco2clics'). In the following paper, we introduce an authoring tool which converts full-length clinical guidelines to the 'reco2clics' format. Alongside routine editorial operations, this tool supports dynamic document restructuring, a complex operation using text segmentation algorithms and deontic analysis.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {245–248},
numpages = {4},
keywords = {hypertext, clinical guidelines, document structure},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494317,
author = {Bilauca, Mihai and Healy, Patrick},
title = {Splitting Wide Tables Optimally},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494317},
doi = {10.1145/2494266.2494317},
abstract = {In this paper we discuss the problems that occur when splitting wide tables across multiple pages. We focus our attention on finding solutions that minimize the impact on the meaning of data when the objective is to reorder the columns such that the number of pages used is minimal. Reordering of columns in a table raises a number of complex optimization problems that we will study in this paper: minimizing page count and at the same time the number of column positions changes or the number of column groups split across pages. We show that by using integer programming solutions the number of pages used when splitting wide tables can be reduced by up to 25% and it can be achieved in short computational time.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {249–252},
numpages = {4},
keywords = {constrained optimization, wide tables, tabular layout, table layout},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/3253554,
author = {Roisin, C\'{e}cile},
title = {Session Details: Multimedia II},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253554},
doi = {10.1145/3253554},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
numpages = {1},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494273,
author = {Silva, Esdras Caleb Oliveira and dos Santos, Joel A. F. and Muchaluat-Saade, D\'{e}bora C.},
title = {NCL4WEB: Translating NCL Applications to HTML5 Web Pages},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494273},
doi = {10.1145/2494266.2494273},
abstract = {Testing Digital TV applications is not a simple task. DTV applications either need to be transmitted by a TV broadcaster or someone with an equipment capable of generating a DTV signal with the application embedded. Alternatively, an interactive TV application developer may use a virtual execution environment, like a virtual set-top box installed in a computer, which implements the digital TV middleware standard. In both cases, the application usually does not reach a large number of final users, and developers may not be motivated to continue working with digital TV interactive content. On the other hand, HTML5 support for multimedia content will certainly attract multimedia authors to web development. Considering this scenario, this work proposes an alternative way of presenting a digital TV application developed in NCL for the Ginga declarative middleware, translating it into HTML5 web pages, so it can be presented using a common web browser. The translation tool is called NCL4WEB. Like HTML, NCL is XML-based, so NCL4WEB is based on XSLT stylesheets. It transforms NCL elements into HTML5 elements and a set of JavaScript functions that implement synchronization relationships among media objects, including user interaction. Using NCL4WEB, NCL developers are able to publish their interactive TV applications on the web. It is transparent for final users to access HTML5 or NCL content using a web browser.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {253–262},
numpages = {10},
keywords = {multimedia synchronization, iptv, html5, ncl, xslt, ncl4web},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494287,
author = {Viel, Caio Cesar and Melo, Erick Lazaro and Pimentel, Maria da Gra\c{c}a Campos and Teixeira, Cesar Augusto Camillo},
title = {Go beyond Boundaries of ITV Applications},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494287},
doi = {10.1145/2494266.2494287},
abstract = {The development of multimedia applications that require the manipulation and the synchronization of multiple media and the handling of different types of user interactions usually requires specialized knowledge in imperative languages. Declarative languages have been proposed in order to make this task easier, especially when applications are restricted to certain classes, as it is the case of Interactive TV applications in which user interactions are restricted to a few simple models. However, those simple models may be too simple when documents are reused in other platforms: for instance, when watching a video most web users expect an interactive timeline to be available --- which is not the case in interactive TV videos. This paper presents a component-based approach to the enrichment of declarative languages for multimedia so that desirable user-media interactions are made possible at the same time that the original ease of authoring is maintained. We detail the components and present a corresponding proof-of-concept prototype. We also discuss design decisions associated with the development of the components, which should be useful in further extensions.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {263–272},
numpages = {10},
keywords = {annotation, time-based navigation, user query, component-based development, spatial composition},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494314,
author = {Jansen, Jack and Cesar, Pablo and Bulterman, Dick},
title = {Multimedia Document Synchronization in a Distributed Social Context},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494314},
doi = {10.1145/2494266.2494314},
abstract = {Watching digital content together and commenting on it is becoming a social habit between friends and family members living apart. It is also becoming an important value-added activity for business video conferencing. In both cases, the video sharing experience can easily be spoiled if synchronization problems arise, since the context of the conversation will not be consistent across locations. In the past, research has treated the distributed synchronization problem as a technical one, mainly focusing on timestamps, frame accuracy, and protocol-dependent control messages. That approach is based on a content agnostic approach which we feel does not adequately address the higher-level constraints of individual conversations.In this paper we postulate that the technical issues are just part of the problem, so solutions need to take into account the media being shared and the social setting. Therefore, we propose a framework that allows researchers to experiment with different synchronization policies tailored to specific settings. The framework allows the evaluation of these policies through user testing, for finding the most appropriate policies and strategies.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {273–276},
numpages = {4},
keywords = {multimedia applications, user testing, synchronization, video conferencing},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494320,
author = {H\"{o}ver, Kai Michael and M\"{u}hlh\"{a}user, Max},
title = {Interchanging and Preserving Presentation Recordings},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494320},
doi = {10.1145/2494266.2494320},
abstract = {The importance of presentation recordings is steadily increasing. This trend is indicated for example by the growing MOOCs market. Many systems for the production of such recordings exist. However, produced recordings are not exchangeable between systems due to different representation formats. In this paper, we present an ontology for the conceptual description of presentation recordings and describe the transformation process between different systems. Furthermore, we explain how this ontology can be used to preserve presentation recordings as ebooks.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {277–280},
numpages = {4},
keywords = {ontology, presentation recordings, interoperability, ebooks},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494322,
author = {Barabucci, Gioele and Borghoff, Uwe M. and Di Iorio, Angelo and Maier, Sonja},
title = {Document Changes: Modeling; Detection; Storing and Visualization (DChanges)},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494322},
doi = {10.1145/2494266.2494322},
abstract = {Many people have approached the problem of investigating the evolution of documents and data from different perspectives, e.g. by tracking changes, versioning and diffing. The goal of this workshop is to share ideas, common issues and principles, and to foster research collaboration on these topics.},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {281–282},
numpages = {2},
keywords = {change analysis and interpretation, merging changes, change detection, applications, change tracking},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494323,
author = {Tomasi, Francesca and Vitali, Fabio},
title = {Collaborative Annotations in Shared Environments: Metadata, Vocabularies and Techniques in the Digital Humanities (DH-CASE 2013)},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494323},
doi = {10.1145/2494266.2494323},
abstract = {We present here the workshop DH-CASE 2013, aimed at investigating the state of art in the field of collaboration in text annotation, by exploring methods, tools and techniques used in the domain of the Digital Humanities (DH).},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {283–284},
numpages = {2},
keywords = {ontologies, annotation, metadata, environment, digital humanities, vocabularies, collaboration},
location = {Florence, Italy},
series = {DocEng '13}
}

@inproceedings{10.1145/2494266.2494324,
author = {Wybrow, Michael},
title = {Reimagining Digital Publishing for Technical Documents},
year = {2013},
isbn = {9781450317894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494266.2494324},
doi = {10.1145/2494266.2494324},
abstract = {This workshop asks how we might reimagine digital publishing for technical documents and proposes to investigate new adaptive approaches to document reading with flexible navigation and where contextual information---figures, references, definitions, etc---might be displayed dynamically at the point they are referred to. The workshop ultimately seeks to answer the question of what needs to happen for reading and annotation of technical documents on digital devices to become more comfortable and productive than on paper?},
booktitle = {Proceedings of the 2013 ACM Symposium on Document Engineering},
pages = {285–286},
numpages = {2},
keywords = {technical documents, digital reading environments, e-books},
location = {Florence, Italy},
series = {DocEng '13}
}

