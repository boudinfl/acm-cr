@inproceedings{10.1145/1030397.1030399,
author = {Lindholm, Tancred},
title = {A Three-Way Merge for XML Documents},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030399},
doi = {10.1145/1030397.1030399},
abstract = {Three-way merging is a technique that may be employed for reintegrating changes to a document in cases where multiple independently modified copies have been made. While tools for three-way merge of ASCII text files exist in the form of the ubiquitous diff and patch tools these are of limited applicability to XML documents.We present a method for three-way merging of XML which is targeted at merging XML formats that model human-authored documents as ordered trees (e.g. rich text formats structured text drawings etc.). To this end we investigate a number of use cases on XML merging (collaborative editing propagating changes across document variants) from which we derive a set of high-level merge rules. Our merge is based on these rules.We propose that our merge is easy to both understand and implement yet sufficiently expressive to handle several important cases of merging on document structure that are beyond the capabilities of traditional text-based tools. In order to justify these claims we applied our merging method to the merging tasks contained in the use cases. The overall performance of the merge was found to be satisfactory.The key contributions of this work are: a set of merge rules derived from use cases on XML merging a compact and versatile XML merge in accordance with these rules and a classification of conflicts in the context of that merge.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {1–10},
numpages = {10},
keywords = {XML, collaborative editing, three-way merge, structured text, conflict},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/3244324,
author = {Vion-Dury, J.-Y.},
title = {Session Details: Document Querying and Transformation},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244324},
doi = {10.1145/3244324},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
numpages = {1},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030400,
author = {Huang, Chia-Hsin and Chuang, Tyng-Ruey and Lee, Hahn-Ming},
title = {Fast Structural Query with Application to Chinese Treebank Sentence Retrieval},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030400},
doi = {10.1145/1030397.1030400},
abstract = {In natural language processing a huge amount of structured data is constantly used for the extraction and presentation of grammatical structures in sentences. For example the Chinese Treebank corpus developed at the Institute of Information Science Academia Sinica Taiwan is a semantically annotated corpus that has been used to help parse and study Chinese sentences. In this setting users usually use structured tree patterns instead of keywords to query the corpus.In this paper we present an online prototype system that provides exploratory search ability. The system implements two flexible and efficient structural query methods and employs a user-friendly web-based interface. Although the system adopts the XML format to present the corpora and search results it does not use conventional XML query languages. As searching the Chinese Treebank corpora is structural in nature and often deals with structural similarities conventional XML query languages such as XPath and XQuery are inflexible and inefficient. We propose and implement a query algorithm called Parent-Child Relationship Filter <i>(PCRF)</i> which provides flexible and efficient structural search. <i>PCRF</i> is sufficiently flexible to provide several similarity-matching options such as wildcard unordered sibling sub-trees ancestor-descendant matching and their combinations. In addition <i>PCRF</i> supports stream-based matching to help users query their XML documents online. We also present three accelerating rules that achieve a 1.5- to 8-fold performance improvement in query time. Our experiment results show that our method archive a 10- to 1000-fold performance improvement compared to the usual text-based XPath query method.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {11–20},
numpages = {10},
keywords = {XML, structural query, treebank},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030401,
author = {Zhang, Hui and Tompa, Frank Wm.},
title = {Querying XML Documents by Dynamic Shredding},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030401},
doi = {10.1145/1030397.1030401},
abstract = {With the wide adoption of XML as a standard data representation and exchange format querying XML documents becomes increasingly important. However relational database systems constitute a much more mature technology than what is available for native storage of XML. To bridge the gap one way to manage XML data is to use a commercial relational database system. In this approach users typically first ``shred'' their documents by isolating what they predict to be meaningful fragments then store the individual fragments according to some relational schema and later translate each XML query (e.g. expressed in W3C's XQuery) to SQL queries expressed against the shredded documents.In this paper we propose an alternative approach that builds on relational database technology but shreds XML documents dynamically. This avoids many of the problems in maintaining document order and reassembling compound data from its fragments. We then present an algorithm to translate a significant subset of XQuery into an extended relational algebra that includes operators defined for the structured text datatype. This algorithm can be used as the basis of a sound translation from XQuery to SQL and the starting point for query optimization which is required for XML to be supported by relational database technology.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {21–30},
numpages = {10},
keywords = {XQuery, XML, text ADT, relational algebra, dynamic shredding},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030402,
author = {Gan\c{c}arski, Alda Lopes and Henriques, Pedro Rangel},
title = {Presenting the Results of Relevance-Oriented Search over XML Documents},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030402},
doi = {10.1145/1030397.1030402},
abstract = {In this paper we discuss how to present the result of searching elements of any type from XML documents relevant to some information need (<i>relevance-oriented search</i>). As the resulting elements can contain each other we show an intuitive way of organizing the resulting list of elements in several ranked lists at different levels such that each element is presented only one time. Depending on the size of such ranked lists its presentation is given by a <i>structure tree</i> for small lists or by a <i>sequence of pointers</i> for large lists. In both cases the textual content of the implied elements is given. We also analyse the size of ranked lists in a real collection of XML documents.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {31–33},
numpages = {3},
keywords = {user interface for XML retrieval},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030403,
author = {Rose, Kristoffer H.},
title = {The XML World View},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030403},
doi = {10.1145/1030397.1030403},
abstract = {XML is unique in its very broad acceptance throughout both the document engineering and data processing community. This creates a unique opportunity for unifying the traditionally separate worlds and ask questions such as "What are the data relations in my document?" and "How can I read a textual version of my data?" all within the single framework provided by XML. In this talk I'll speculate on how one could view the whole world as a single XML document from which both relational "table" and textual "report" queries are possible.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {34},
numpages = {1},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/3244325,
author = {Pimentel, M.},
title = {Session Details: Document Adaptation},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244325},
doi = {10.1145/3244325},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
numpages = {1},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030405,
author = {Zhang, Li and Bieber, Michael and Millard, David and Oria, Vincent},
title = {Supporting Virtual Documents in Just-in-Time Hypermedia Systems},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030405},
doi = {10.1145/1030397.1030405},
abstract = {Many analytical or computational applications especially legacy systems create documents and display screens in response to user queries "dynamically" or in "real time". These "virtual documents" do not exist in advance and thus hypermedia features must be generated "just in time" - automatically and dynamically. Additionally the hypermedia features may have to cause target documents to be generated or re-generated. This paper focuses on the specific challenges faced in hypermedia support for virtual documents of dynamic hypermedia functionality dynamic regeneration and dynamic anchor re-identification and re-location. It presents a prototype called JHE (Just-in-time Hypermedia Engine) to support just-in-time hypermedia across third party applications with dynamic content and discusses issues prompted by this research.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {35–44},
numpages = {10},
keywords = {re-location, integration architecture, virtual documents, just-in-time hypermedia, dynamic hypermedia functionality, dynamic regeneration, re-identification},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030406,
author = {de Andrade, Andrea R. and Munson, Ethan V. and Pimentel, Mariada G.},
title = {A Document-Based Approach to the Generation of Web Applications},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030406},
doi = {10.1145/1030397.1030406},
abstract = {wVIEW is an automated system for generating Web applications that relies extensively on document representations and transformations. wVIEW adopts the widely accepted hypermedia design principle that content navigation and presentation are separate concerns. Each of these aspects of the design process is controlled by separate declarative specifications. Only the first specification the content structure specification which is described using UML must be provided. However the wVIEW user is free to add extensions and customizations to both the data and navigation models in order to make the final application suit specific needs. This paper describes the wVIEW approach and the current prototype which focuses on the data and navigation modelling aspects. The paper discusses experiences in using XSLT as the primary development tool and shows examples how the enhancements planned to XSLT address some limitations of the application generation process.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {45–47},
numpages = {3},
keywords = {XML, design, web applications, cocoon, XSLT},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030407,
author = {Andric, Mirjana and Hall, Wendy and Carr, Leslie},
title = {Assisting Artifact Retrieval in Software Engineering Projects},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030407},
doi = {10.1145/1030397.1030407},
abstract = {The research presented in this paper focuses on the issue of how a recommender system can support the task of searching documents and artifacts constructed in a software development project. The "A LA" (Associative Linking of Attributes) system represents a recommender facility built on top of a document management system. The facility provides assistance to finding items by utilising hypertextually connected metadata. In order to determine metadata relationships "A LA" employs techniques of content analysis together with exploiting user-generated metadata and usage logs. An evaluation study that compares querying using a full text search approach with the "A LA" method for finding relevant documents was conducted.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {48–50},
numpages = {3},
keywords = {recommender systems, links, metadata, zigzag},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030408,
author = {Nnadi, Nkechi and Bieber, Michael},
title = {Lightweight Integration of Documents and Services},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030408},
doi = {10.1145/1030397.1030408},
abstract = {This research's primary contribution is providing a relatively straightforward sustainable infrastructure for integrating documents and services. Users see a totally integrated environment. The integration infrastructure generates supplemental link anchors. Selecting one generates a list of relevant links automatically through the use of relationship rules.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {51–53},
numpages = {3},
keywords = {service integration, relationship rules, metainformation, automatic link generation},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030409,
author = {Blustein, James and Noor, Mona},
title = {Personal Glossaries on the WWW: An Exploratory Study},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030409},
doi = {10.1145/1030397.1030409},
abstract = {We examine basic issues of glossary tools as part of a suite of annotational tools to help users make meaning from documents from unfamiliar realms of discourse. We specifically evaluated the performance of glossary tools for reading medical information about common diseases by users with no formal medical education.We developed both automatic and an editable glossary tools. Both of them extracted definitions from the text of articles. Only the editable glossary tool allowed users to add delete and change entries.Both tools were evaluated to find out how useful they were to users reading technical articles online. The analytical results showed that user performance improved without increasing total reading time. The glossary tools were effective and pleasing to users at no decrease in efficiency. This experiment points the way for longer-term studies with adaptable tools particularly to help users unfamiliar with technical documents. We also discuss the r\^{o}le of glossaries as part of a suite of annotational tools to help users make personal (and therefore meaningful) hypertextual document collections.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {54–56},
numpages = {3},
keywords = {user interfaces, evaluation experiment, hyperlinked glossaries, annotation support},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/3244326,
author = {Munson, E.},
title = {Session Details: Time, Media, Interaction},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244326},
doi = {10.1145/3244326},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
numpages = {1},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030411,
author = {King, Peter and Schmitz, Patrick and Thompson, Simon},
title = {Behavioral Reactivity and Real Time Programming in XML: <i>Functional Programming Meets SMIL Animation</i>},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030411},
doi = {10.1145/1030397.1030411},
abstract = {XML and its associated languages are emerging as powerful authoring tools for multimedia and hypermedia web content. Furthermore intelligent presentation generation engines have begun to appear as have models and platforms for adaptive presentations. However XML-based models are limited by their lack of expressiveness in presentation and animation. As a result authors of dynamic adaptive web content must often use considerable amounts of script or code. The use of such script or code has two serious drawbacks. First such code undermines the declarative description possible in the original presentation language and second the scripting/coding approach does not readily lend itself to authoring by non programmers. In this paper we describe a set of XML language extensions inspired by features from the functional programming world which are designed to widen the class of reactive systems which could be described in languages such as SMIL. The described features extend the power of declarative modeling for the web by allowing the introduction of web media items which may dynamically react to continuously varying inputs both in a continuous way and by triggering discrete user-defined events. The two extensions described herein are discussed in the context of SMIL Animation and SVG but could be applied to many XML-based languages.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {57–66},
numpages = {10},
keywords = {animation, time, continuous, behaviors, events, declarative, expressions, DOM, SMIL, functional programming, modeling, SVG, XML},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030412,
author = {Watanabe, Yasuhiko and Sono, Kazuya and Yokomizo, Kazuya and Okada, Yoshihiro},
title = {A Question Answer System Using Mails Posted to a Mailing List},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030412},
doi = {10.1145/1030397.1030412},
abstract = {The most serious difficulty in developing a QA system is knowledge. In this paper we first discuss three problems of developing a knowledge base by which a QA system answers how type questions. Then we propose a method of developing a knowledge base by using mails posted to a mailing list. Next we describe a QA system which can answer how type questions based on the knowledge base. Our system finds question mails which are similar to user's question and shows the answers to the user. The similarity between user's question and a question mail is calculated by matching of user's question and a significant sentence in the question mail. Finally we show that mails posted to a mailing list can be used as a knowledge base by which a QA system answers how type questions.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {67–73},
numpages = {7},
keywords = {mailing list, sentence extraction, question answer system},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@dataset{10.1145/review-1030397.1030412_R38578,
author = {Schmitz, Hans-Christian},
title = {Review ID:R38578 for DOI: 10.1145/1030397.1030412},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1030397.1030412_R38578}
}

@inproceedings{10.1145/1030397.1030413,
author = {Camacho-Guerrero, Jos\'{e} Antonio and Macedo, Alessandra Alaniz and da Gra\c{c}a Campos Pimentel, Maria},
title = {A Look at Some Issues during Textual Linking of Homogeneous Web Repositories},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030413},
doi = {10.1145/1030397.1030413},
abstract = {Interacting with services that create links automatically via Web users are able to identify relationships among documents stored in different repositories. The fact that automatic linking services do not use queries performed by a human user has impact in the use of information retrieval techniques for the identification of relationships. Information retrieval techniques can lead to the identification of relationships that should not have been generated (generating non-relevant links) at the same time that fail to identify all relevant relationships (poor recall). Towards improving the quality of the relationships identified we have investigated some design issues considered during the automatic linking of textual repositories. The investigations have used a collection of documents from online Brazilian Newspapers and the Cystic Fibrosis Collection. The results of the investigations have defined procedures infrastructures and consequently the requirements for a configurable linking service made also available as a contribution of this work.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {74–83},
numpages = {10},
keywords = {information retrieval, web, semantic structures, homogeneous repositories, linking},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030414,
author = {Goularte, Rudinei and Cattelan, Renan G. and Camacho-Guerrero, Jos\'{e} A. and In\'{a}cio, Valter R. and da Gra\c{c}a C. Pimentel, Maria},
title = {Interactive Multimedia Annotations: Enriching and Extending Content},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030414},
doi = {10.1145/1030397.1030414},
abstract = {This paper discusses an approach to the problem of annotating multimedia content. Our approach provides annotation as metadata for indexing retrieval and semantic processing as well as content enrichment. We use an underlying model for structured multimedia descriptions and annotations allowing the establishment of spatial temporal and linking relationships. We discuss aspects related with documents and annotations used to guide the design of an application that allows annotations to be made with pen-based interaction with Tablet PCs. As a result a video stream can be annotated during the capture. The annotation can be further edited extended or played back synchronously.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {84–86},
numpages = {3},
keywords = {annotation, multimodal interfaces, MPEG-7},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030415,
author = {Troncy, Rapha\"{e}l and Carrive, Jean},
title = {A Reduced yet Extensible Audio-Visual Description Language},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030415},
doi = {10.1145/1030397.1030415},
abstract = {Enabling an intelligent access to multimedia data requires a powerful description language. In this paper we demonstrate why the MPEG-7 standard fails to fulfill this task. We introduce then our proposition: an audio-visual specific description language modular reduced but designed to be extensible. This language is centered on the notions of <i>descriptor</i> and <i>structure</i> with a well-defined semantics. A descriptor can be a low-level feature automatically extracted from the signal or a higher semantic concept that will be used to annotate the video documents. The descriptors can be combined into structures according to defined models that provide description patterns.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {87–89},
numpages = {3},
keywords = {descriptor, structure, knowledge representation, semantics, audio-visual description language, semantic web, MPEG-7},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/3244327,
author = {Quint, V.},
title = {Session Details: Document Creation I},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244327},
doi = {10.1145/3244327},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
numpages = {1},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030417,
author = {Carr, Leslie and Miles-Board, Timothy and Woukeu, Arouna and Wills, Gary and Hall, Wendy},
title = {The Case for Explicit Knowledge in Documents},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030417},
doi = {10.1145/1030397.1030417},
abstract = {The Web is full of documents which must be interpreted by human readers and by software agents (search engines recommender systems clustering processes <i>etc.</i>). Although Web standards have addressed format obfuscation by using XML schemas and stylesheets to specify unambiguous structure and presentation semantics interpretation is still hampered by the fundamental ambiguity of information in <b>PCDATA</b> text. Even the most easily distinguishable kinds of knowledge such as article citations and proper nouns (referring to people organisations projects products technical concepts) have to be identified by fallible post-hoc extraction processes. The WiCK project has investigated the writing process in a Semantic Web environment where knowledge services exist and actively assist the author. In this paper we discuss the need to make knowledge an explicit part of the document representation and the advantages and disadvantages of this step.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {90–98},
numpages = {9},
keywords = {document structure, semantic web, knowledge writing},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030418,
author = {Hardy, Matthew R. B. and Brailsford, David F. and Thomas, Peter L.},
title = {Creating Structured PDF Files Using XML Templates},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030418},
doi = {10.1145/1030397.1030418},
abstract = {This paper describes a tool for recombining the logical structure from an XML document with the typeset appearance of the corresponding PDF document. The tool uses the XML representation as a template for the insertion of the logical structure into the existing PDF document thereby creating a Structured/Tagged PDF. The addition of logical structure adds value to the PDF in three ways: the accessibility is improved (PDF screen readers for visually impaired users perform better) media options are enhanced (the ability to reflow PDF documents using structure as a guide makes PDF viable for use on hand-held devices) and the re-usability of the PDF documents benefits greatly from the presence of an XML-like structure tree to guide the process of text retrieval in reading order (e.g. when interfacing to XML applications and databases).},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {99–108},
numpages = {10},
keywords = {logical structure insertion, XML, PDF},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030419,
author = {Harrington, Steven J. and Naveda, J. Fernando and Jones, Rhys Price and Roetling, Paul and Thakkar, Nishant},
title = {Aesthetic Measures for Automated Document Layout},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030419},
doi = {10.1145/1030397.1030419},
abstract = {A measure of aesthetics that has been used in automated layout is described. The approach combines heuristic measures of attributes that degrade the aesthetic quality. The combination is nonlinear so that one bad aesthetic feature can harm the overall score. Example heuristic measures are described for the features of alignment regularity separation balance white-space fraction white-space free flow proportion uniformity and page security.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {109–111},
numpages = {3},
keywords = {document, layout, aesthetics},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030420,
author = {Yeh, Ching-Long and Chen, Yi-Chun},
title = {Creation of Topic Map by Identifying Topic Chain in Chinese},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030420},
doi = {10.1145/1030397.1030420},
abstract = {XML Topic maps enable multiple concurrent views of sets of information objects and can be used to different applications. For example thesaurus-like interfaces to corpora navigational tools for cross-references or citation systems information filtering or delivering depending on user profiles etc. However to enrich the information of a topic map or to connect with some document's URI is very labor-intensive and time-consuming. To solve this problem we propose an approach based on natural language processing techniques to identify and extract useful information in raw Chinese text. Unlike most traditional approaches to parsing sentences based on the integration of complex linguistic information and domain knowledge we work on the output of a part-of-speech tagger and use shallow parsing instead of complex parsing to identify the topics of sentences. The key elements of the centering model of local discourse coherence are employed to extract structures of discourse segments. We use the local discourse structure to solve the problem of zero anaphora in Chinese and then identify the topic which is the most salient element in a sentence. After we obtain all the topics of a document we may assign this document into a topic node of the topic map and add the information of the document into the topic element simultaneously.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {112–114},
numpages = {3},
keywords = {centering model, zero anaphora resolution, topic identification, topic maps, shallow parsing},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/3244328,
author = {Brailsford, D.},
title = {Session Details: Document Creation II},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244328},
doi = {10.1145/3244328},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
numpages = {1},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030422,
author = {Quint, Vincent and Vatton, Ir\^{o}ne},
title = {Techniques for Authoring Complex XML Documents},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030422},
doi = {10.1145/1030397.1030422},
abstract = {This paper reviews the main innovations of XML and considers their impact on the editing techniques for structured documents. Namespaces open the way to compound documents; well-formedness brings more freedom in the editing task; CSS allows style to be associated easily with structured documents. In addition to these innovative features the wide deployment of XML introduces structured documents in many new applications including applications where text is not the dominant content type. In languages such as SVG or SMIL for instance XML is used to represent vector graphics or multimedia presentations.This is a challenging situation for authoring tools. Traditional methods for editing structured documents are not sufficient to address the new requirements. New techniques must be developed or adapted to allow more users to efficiently create advanced XML documents. These techniques include multiple views semantic-driven editing direct manipulation concurrent manipulation of style and structure and integrated multi-language editing. They have been implemented and experimented in the Amaya editor and in some other tools.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {115–123},
numpages = {9},
keywords = {structured editing, CSS, authoring tools, direct manipulation, style languages, XML, compound documents},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030423,
author = {Francisco-Revilla, Luis and Shipman, Frank},
title = {Instructional Information in Adaptive Spatial Hypertext},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030423},
doi = {10.1145/1030397.1030423},
abstract = {Spatial hypertext is an effective medium for the delivery of help and instructional information on the Web. Spatial hypertext's intrinsic features allow documents to visually reflect the inherent structure of the information space and represent implicit relationships between information objects. This work presents a study of the effectiveness of spatial hypertext as medium for delivery of instructional information. Results were gathered based on direct observation of the people reading a spatial hypertext document which was used as informational support for a complex task. Two versions of the spatial hypertext document were used: a non-adaptive and an adaptive. The document was adapted based upon the inferred relevance of information to the user's knowledge and task requirements. The study produced insights on emergent reading strategies such as informed link traversals and the use of collections as bookmarks. Observations and evaluation of how people interacted with both document versions showed that the spatial layout and the use of collections as a way to encapsulate information allowed people to read browse and navigate very large information spaces while maintaining a clear understanding the structure of the information. Finally several differences between the adaptive and non-adaptive versions were identified showing that adaptation alters not only the display of information but the way that people read spatial hypertext document.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {124–133},
numpages = {10},
keywords = {spatial hypertext, information delivery, adaptation},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030424,
author = {Bagley, Steven R. and Brailsford, David F.},
title = {Page Composition Using PPML as a Link-Editing Script},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030424},
doi = {10.1145/1030397.1030424},
abstract = {The advantages of a COG (Component Object Graphic) approach to the composition of PDF pages have been set out in a previous paper [1]. However if pages are to be composed in this way then the individual graphic objects must have known bounding boxes and must be correctly placed on the page in a process that resembles the link editing of a multi-module computer program. Ideally the linker should be able to utilize all declared resource information attached to each COG.We have investigated the use of an XML application called Personalized Print Markup Language (PPML) to control the link editing process for PDF COGs. Our experiments though successful have shown up the shortcomings of PPML's resource handling capabilities which are currently active at the document and page levels but which cannot be elegantly applied to individual graphic objects at a sub-page level. Proposals are put forward for modifications to PPML that would make easier any COG-based approach to page composition.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {134–136},
numpages = {3},
keywords = {graphic objects, PDF, form Xobjects, PPML, link editing},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/3244329,
author = {King, P.},
title = {Session Details: Document Management},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244329},
doi = {10.1145/3244329},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
numpages = {1},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030426,
author = {Scheffczyk, Jan and R\"{o}dig, Peter and Borghoff, Uwe M. and Schmitz, Lothar},
title = {Managing Inconsistent Repositories via Prioritized Repairs},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030426},
doi = {10.1145/1030397.1030426},
abstract = {Whenever a group of authors collaboratively edits interrelated documents semantic consistency is a major goal. Current document management systems (DMS) lack adequate consistency management facilities. We propose liberal use of formal consistency rules which permits inconsistencies. In this paper we focus on deriving repairs for inconsistencies. Our major contributions are: (1) deriving (common) repairs for multiple rules (2) resolving conflicts between repairs (3)prioritizing repairs and (4) support for partial inconsistency resolution which resolves the most troubling inconsistencies and leaves less important inconsistencies for a later handling. The novel aspect of our approach is that we derive repairs from DAGs (directed acyclic graphs) and not from documents directly. That way the repository is locked during DAG generation only which is performed incrementally.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {137–146},
numpages = {10},
keywords = {document management, repair, consistency maintenance},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030427,
author = {Antonacopoulos, A. and Karatzas, D. and Krawczyk, H. and Wiszniewski, B.},
title = {The Lifecycle of a Digital Historical Document: Structure and Content},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030427},
doi = {10.1145/1030397.1030427},
abstract = {This paper describes the lifecycle of a digital historical document, from template-based structure definition through to content extraction from the scanned pages and its final reconstitution as an electronic document (combining content and semantic information) along with the tools that have been created to realise each stage in the lifecycle. The whole approach is described in the context of different types of typewritten documents relating to prisoners in World-War II concentration camps and is the result of a multinational collaboration under the MEMORIAL project funded (€1.5M) by the European Union (www.memorial-project.info). Extensive tests with historians/archivists and evaluation of the content extraction results indicate the superior performance of the whole semantics-driven approach both over manual transcription and over the semi-automated application of off-the-shelf OCR and the use of a conventional (text and layout) document format.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {147–154},
numpages = {8},
keywords = {digital libraries, historical documents, document analysis, text enhancement, document architecture, document engineering},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@dataset{10.1145/review-1030397.1030427_R38843,
author = {Jaromczyk, Jerzy W.},
title = {Review ID:R38843 for DOI: 10.1145/1030397.1030427},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1030397.1030427_R38843}
}

@inproceedings{10.1145/1030397.1030428,
author = {AbuSafiya, Majed and Mazumdar, Subhasish},
title = {Accommodating Paper in Document Databases},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030428},
doi = {10.1145/1030397.1030428},
abstract = {Although the paperless office has been imminent for decades, documents in paper form continue to be used extensively in almost all organizations. Present-day information systems are designed on the premise that any paper document in use will be either converted into electronic form or merely printed from electronic file(s) accessible to the system. Yet, paper is the medium of choice in many situations, mainly owing to its portability and usability, and the medium of necessity in others, especially where external communication or the traditional notion of authenticity are involved. Humans who find unique attractive features in both paper and electronic forms of documents, must survive this tension between the de-jure banishment of paper and its de-facto prevalence. In this paper, we propose to make paper documents first-class citizens by including them in the model underlying the information system. Specifically, we extend the schema of a document database with the notion of paper documents, physical locations, and the organizational hierarchy. This leads to an overall enhancement of document integrity and the ability to answer queries such as "where are the customer complaint letters we have received today?" and "which documents are in this filing cabinet?". Recent technological advances such as sensors have made the implementation of such a model very realistic.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {155–162},
numpages = {8},
keywords = {document management, enterprise document model, paper documents, RFID, document databases, paper manifestation},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030429,
author = {Meneguzzi, Felipe Rech and Meirelles, Leonardo Luceiro and Martins Mano, Fernando Tarl\'{a} and de Souza Oliveira, Joao Batista and Benso da Silva, Ana Cristina},
title = {Strategies for Document Optimization in Digital Publishing},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030429},
doi = {10.1145/1030397.1030429},
abstract = {Recent advances in digital press technology have enabled the creation of high-quality personalized documents, with the potential of generating an entire batch of one-of-a-kind documents. Even though digital presses are capable of printing such document sets as fast as they would print regular press jobs, raster image processing might possibly be performed for every different page in the job. Such process demands a large computational effort and it is therefore interesting to gather repeated images that are used throughout all documents and rasterize them as few times as possible. Moreover, performing such process separately from document production in the publishing workflow allows optimization to be performed prior to final printing, thus allowing it to take press hardware specifics into account, and reducing the time taken for it to produce the final output. This paper describes techniques to perform this task using PPML as the document description language, as well as the main issues concerning this kind of document optimization. Several gathering policies are described along with explanatory examples. We also provide and discuss experimental data supporting the use of such strategie.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {163–170},
numpages = {8},
keywords = {variable data printing, personalized printing, variable information documents, digital press, PPML},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/3244330,
author = {Antonacopoulos, A.},
title = {Session Details: Document Analysis},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244330},
doi = {10.1145/3244330},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
numpages = {1},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030431,
author = {Simske, Steven J. and Baggs, Scott C.},
title = {Digital Capture for Automated Scanner Workflows},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030431},
doi = {10.1145/1030397.1030431},
abstract = {The use of scanners and other capture devices to incorporate film- and paper-based materials into digital workflows is an important part of "digital convergence", or the bringing of paper-based and electronic documents together into the same electronic workflows. The diversity of captured information-from text and mixed-type documents to photos, negatives, slides and transparencies-requires a combination of document analysis techniques to perform, automatically, the segmentation, classification and workflow assignment of the scanned images. We herein present technologies that provide fast (&lt; 1.0 sec) and reliable (&gt; 95% job accuracy) capture solutions for all of these input content types. These solutions offer near real-time capture that provides automated workflow capabilities to a repertoire of scanning hardware: scanners, all-in-one devices, copiers and multifunctional printers. The techniques used to categorize the documents, perform zoning analysis on the documents, and then perform closed loop quality assurance on the documents are presented.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {171–177},
numpages = {7},
keywords = {user interface, scanning, photos, zoning, segmentation, classification, slides, negatives},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030432,
author = {Behera, Ardhendu and Lalanne, Denis and Ingold, Rolf},
title = {Visual Signature Based Identification of Low-Resolution Document Images},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030432},
doi = {10.1145/1030397.1030432},
abstract = {In this paper, we present (a) a method for identifying documents captured from low-resolution devices such as web-cams, digital cameras or mobile phones and (b) a technique for extracting their textual content without performing OCR. The first method associates a hierarchically structured visual signature to the low-resolution document image and further matches it with the visual signatures of the original high-resolution document images, stored in PDF form in a repository. The matching algorithm follows the signature hierarchy, which speeds-up the search by guiding it towards fruitful solution spaces. In a second step, the content of the original PDF document is extracted, structured, and matched with its corresponding high-resolution visual signature. Finally, the matched content is attached to the low-resolution document image's visual signature, which greatly enriches the document's content and indexing. We present in this article both these identification and extraction methods and evaluate them on various documents, resolutions and lighting conditions, using different capture devices.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {178–187},
numpages = {10},
keywords = {document-based meeting retrieval, documents' content extraction, document visual signature, low-resolution document image identification},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030433,
author = {Silva, Heron V. O. and Rodrigues, Rog\'{e}rio F. and Soares, Luiz Fernando G. and Muchaluat Saade, D\'{e}bora C.},
title = {NCL 2.0: Integrating New Concepts to XML Modular Languages},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030433},
doi = {10.1145/1030397.1030433},
abstract = {This paper presents the main new features of Nested Context Language (NCL) version 2.0. NCL 2.0 is a modular and declarative hypermedia language, whose modules can be combined to other languages, such as SMIL, to provide new facilities. Among the NCL 2.0 new features, we can highlight the support for handling hypermedia relations as first-class entities, through the definition of hypermedia connectors, and the possibility of specifying any semantics for a hypermedia composition, using the concept of composition templates. Another important goal of this paper is to describe a framework to facilitate the development of NCL parsing and processing tools. Based on this framework, the paper comments several implemented compilers, which allow, for instance, the conversion of NCL documents into SMIL specifications.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {188–197},
numpages = {10},
keywords = {XConnector, NCL, SMIL, framework for parsing and processing XML, hypermedia connector, XTemplate, composition template},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030434,
author = {Ulges, Adrian and Lampert, Christoph H. and Breuel, Thomas},
title = {Document Capture Using Stereo Vision},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030434},
doi = {10.1145/1030397.1030434},
abstract = {Capturing images of documents using handheld digital cameras has a variety of applications in academia, research, knowledge management, retail, and office settings. The ultimate goal of such systems is to achieve image quality comparable to that currently achieved with flatbed scanners even for curved, warped, or curled pages. This can be achieved by high-accuracy 3D modeling of the page surface, followed by a "flattening" of the surface. A number of previous systems have either assumed only perspective distortions, or used techniques like structured lighting, shading, or side-imaging for obtaining 3D shape. This paper describes a system for handheld camera-based document capture using general purpose stereo vision methods followed by a new document dewarping technique. Examples of shape modeling and dewarping of book images is shown.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {198–200},
numpages = {3},
keywords = {dewarping, camera based document capture, stereo vision},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/3244331,
author = {Dymeetman, M.},
title = {Session Details: Theory and Models I},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244331},
doi = {10.1145/3244331},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
numpages = {1},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030436,
author = {Chuang, Tyng-Ruey and Lin, Jan-Li},
title = {On Modular Transformation of Structural Content},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030436},
doi = {10.1145/1030397.1030436},
abstract = {We show that an XML DTD (Document Type Definition) can be viewed as the fixed point of a parametric content model. Based on the parametric content model, we develop a model of modular transformation of XML documents. A fold operator is used to capture a class of functions that consume valid XML document trees in a bottom-up matter. Similarly, an unfold operator is used to generate valid XML document trees in a top-down fashion. We then show that DTD-aware XML document transformation, which consumes a document of one DTD and generates a document of another DTD, can be thought as both a fold operation and an unfold operation.This leads us to model certain DTD-aware document transformations by mappings from the source content models to the target content models. From these mappings, we derive DTD-aware XML document transformational programs. Benefits of such derived programs include automatic validation of the target documents (no invalid document will be generated) and modular property in the composition of these programs (intermediate results from successive transformations can be eliminated).},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {201–210},
numpages = {10},
keywords = {ML, XML, document transformation and validation, modules, bird-meertens formalism, functional programming},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030437,
author = {Genev\`{e}s, Pierre and Vion-Dury, Jean-Yves},
title = {Logic-Based XPath Optimization},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030437},
doi = {10.1145/1030397.1030437},
abstract = {XPath [5] was introduced by the W3C as a standard language for specifying node selection, matching conditions, and for computing values from an XML document. XPath is now used in many XML standards such as XSLT [4] and the forthcoming XQuery [10] database access language. Since efficient XML content querying is crucial for the performance of almost all XML processing architectures, a growing need for studying high performance XPath-based querying has emerged. Our approach aims at optimizing XPath performance through static analysis and syntactic transformation of XPath expressions.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {211–219},
numpages = {9},
keywords = {optimization, axiomatization, containment, XML, XPath, query, efficiency},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/3244332,
author = {Chuang, T.-R.},
title = {Session Details: Theory and Medels II},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244332},
doi = {10.1145/3244332},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
numpages = {1},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030439,
author = {Chidlovskii, Boris and Fuselier, J\'{e}r\^{o}me},
title = {Supervised Learning for the Legacy Document Conversion},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030439},
doi = {10.1145/1030397.1030439},
abstract = {We consider the problem of document conversion from the rendering-oriented HTML markup into a semantic-oriented XML annotation defined by user-specific DTDs or XML Schema descriptions. We represent both source and target documents as rooted ordered trees so the conversion can be achieved by applying a set of tree transformations. We apply the supervised learning framework to the conversion task according to which the tree transformations are learned from a set of training examples. %Because of the complexity of tree-to-tree transformations, We develop a two-step approach to the conversion problem, that first labels leaves in the source trees and then recomposes target trees from the leaf labels. We present two solutions based of the leaf classification with the target terminals and paths. Moreover, we develop three methods for the leaf classification. All methods and solutions have been tested on two real collections.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {220–228},
numpages = {9},
keywords = {legacy document conversion, XML markup, machine learning},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030440,
author = {Dymetman, Marc},
title = {Chart-Parsing Techniques and the Prediction of Valid Editing Moves in Structured Document Authoring},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030440},
doi = {10.1145/1030397.1030440},
abstract = {We present an approach to controlled document authoring that significantly extends the functionality of existing methods by allowing bottom-up and top-down specifications to be freely mixed. A finite-state automaton is used to represent the partial, evolving, description of the document during authoring. Using a generalization of chart-parsing techniques to FSAs rather than fixed input strings, we show how the authoring system is able to automatically detect the consequences of the choices already made by the author so as to only propose for the next authoring steps choices which may provably lead to a globally valid document.We start by considering the case of authoring purely textual documents controlled by a context-free grammar, then show a generalization of this approach to structured documents controlled by a specification whose formal expressive power is at least that of Regular Hedge Grammars (closely related to RELAX NG Schemas) and therefore greater than that of DTDs.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {229–238},
numpages = {10},
keywords = {XML, parsing, document authoring tools and systems, computational linguistics},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

@inproceedings{10.1145/1030397.1030441,
author = {Kilpel\"{a}inen, Pekka and Tuhkanen, Rauno},
title = {Towards Efficient Implementation of XML Schema Content Models},
year = {2004},
isbn = {1581139381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1030397.1030441},
doi = {10.1145/1030397.1030441},
abstract = {XML Schema uses an extension of traditional regular expressions for describing allowed contents of document elements. Iteration is described through numeric attributes <b>minOccurs</b> and <b>maxOccurs</b> attached to content-describing elements such as <b>sequence</b>, <b>choice</b>, and <b>element</b>. These numeric occurrence indicators are a challenge to standard automata-based solutions. Straightforward solutions require space that is exponential with respect to the length of the expressions.We describe a strategy to implement unambiguous content model expressions as <i>counter automata</i>, which are of linear size only.},
booktitle = {Proceedings of the 2004 ACM Symposium on Document Engineering},
pages = {239–241},
numpages = {3},
keywords = {XML schema, automaton, regular expression},
location = {Milwaukee, Wisconsin, USA},
series = {DocEng '04}
}

