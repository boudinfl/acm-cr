@inproceedings{10.1145/3247986,
author = {Lee, Doheon},
title = {Session Details: Keynote Address},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247986},
doi = {10.1145/3247986},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
numpages = {1},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/2064696.2064698,
author = {Rebholz-Schuhmann, Dietrich},
title = {Semantic Interoperability between Literature and Data Resources: From Genes to Diseases},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064696.2064698},
doi = {10.1145/2064696.2064698},
abstract = {The analysis of gene-disease associations requires the integration of different data resources, e.g. the functional annotations of genes and proteins with expression studies. The scientific literature provides additional data for the interpretation of the phenotypic expression on the molecular and on the anatomical level [3]. This leads into the need to transform the literature into a representation which is semantically interoperable with the other biomedical data resources.The literature research team at the European Bioinformatics Institute has established solutions for the standardisation of the scientific literature and the integration into other data resources. The presentation will give an overview, will explain how ontological resources contribute to semantic interoperability and will show how relevant information can be extracted from the scientific literature.},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
pages = {1–2},
numpages = {2},
keywords = {phenotypes, Gene-disease associations, semantic web},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/3247987,
author = {Kang, Jaewoo},
title = {Session Details: Bio-Data Mining},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247987},
doi = {10.1145/3247987},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
numpages = {1},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/2064696.2064700,
author = {Strauch, Martin and Galizia, C. Giovanni},
title = {Fast PCA for Processing Calcium-Imaging Data from the Brain of Drosophila Melanogaster},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064696.2064700},
doi = {10.1145/2064696.2064700},
abstract = {The calcium-imaging technique allows us to record movies of brain activity in the antennal lobe of the fruitfly Drosophila melanogaster, a brain compartment where information about odors is processed. For signal processing that scales up with the growing data sizes in imaging, we have developed an approximate Principal Component Analysis (PCA) for fast dimensionality reduction. The approach relies on selecting a set of relevant pixels from the movies based on a priori knowledge about the nature of the data, ensuring a high-quality approximation. Once in PCA space, we can efficiently perform source separation, e.g to detect biological signals in the movies and to remove artifacts.},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
pages = {3–10},
numpages = {8},
keywords = {monte carlo algorithm, biological image mining, approximate pca, source separation},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/2064696.2064701,
author = {Anwar, Muhammad Naveed and Oakes, Michael Philip},
title = {Data Mining of Audiology Patient Records: Factors Influencing the Choice of Hearing Aid Type},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064696.2064701},
doi = {10.1145/2064696.2064701},
abstract = {In this paper we describe our analysis of a database of over 180,000 patient records, collected from over 23,000 patients, by the hearing aid clinic at James Cook University Hospital in Middlesbrough, UK. These records consist of audiograms (graphs of the faintest sounds audible to the patient at six different pitches), categorical data (such as age, gender, diagnosis and hearing aid type) and brief free text notes made by the technicians. We mine this data to determine which factors contribute to the decision to fit a BTE (worn behind the ear) hearing aid as opposed to an ITE (worn in the ear) hearing aid. From PCA (principal component analysis) we determined four main audiogram types, and we relate these to the type of hearing aid chosen. We combine the effects of age, gender, diagnosis, masker, mould and individual audiogram frequencies into a single model by means of logistic regression. We also discovered some significant keywords in the free text fields by using the chi-squared (χ2) test, which can also be used in the model. The final model can act a decision support tool to help decide whether an individual patient should be offered a BTE or an ITE hearing aid.},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
pages = {11–18},
numpages = {8},
keywords = {chi-squared, principal component analysis, hearing aids, logistic regression},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/2064696.2064702,
author = {Choi, Jaehoon and Kim, Donghyeon and Kim, Seongsoon and Lee, Sunwon and Lee, Kyubum and Kang, Jaewoo},
title = {BOSS: A Biomedical Object Search System},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064696.2064702},
doi = {10.1145/2064696.2064702},
abstract = {It has never been easy to search relevant information from ever increasing corpus of academic literatures. Large volume of research exists concerning this problem. The previous solutions are put on either ends of spectrum: general-purpose search and domain-specific "deep" search systems. The general-purpose search systems such as PubMed offer flexible query interface, but churn out a list of matching documents that users have to digest in order to find the answers to their queries. On the other hand, the "deep" search systems such as PPI Finder and iHOP return the precompiled results in a structured way. Their results, however, are often found only within some predefined contexts.In order to address this problem, we introduce a new search engine, BOSS, for search on biomedical objects. Unlike the conventional search systems, BOSS indexes segments, rather than documents. A segment refers to a minimal semantic unit such as phrase, clause or sentence that is semantically coherent in the given context (e.g., biomedical objects or their relations). For a user query, BOSS finds all matching segments, identifies the objects appearing in the segments, and aggregates the segments for each object. Finally, it turns up for the user the ranked list of the objects along with their matching segments.BOSS fills the gap between either ends of the spectrum by allowing users to pose context-free queries and by returning a structured set of results. Furthermore, BOSS exhibits the characteristic of good scalability, just as with conventional document search engines, because as it is designed to use a standard document-indexing model with minimal modifications. Considering the features, BOSS is believed to notch up the technological level of traditional solutions for search on biomedical information. BOSS is accessible at http://boss.korea.ac.kr.},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
pages = {19–26},
numpages = {8},
keywords = {information retrieval, biomedical object search engine, biomedical text mining},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/3247988,
author = {Martinez, David},
title = {Session Details: Bio-Text Mining},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247988},
doi = {10.1145/3247988},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
numpages = {1},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/2064696.2064704,
author = {Lee, Sejoon and Choi, Jaejoon and Park, KyungHyun and Song, Min and Lee, Doheon},
title = {Inferring Hidden Relationships from Biological Literature with Multi-Level Context Terms},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064696.2064704},
doi = {10.1145/2064696.2064704},
abstract = {The Swanson's ABC model is powerful to infer hidden relationships buried in biological literatures. However, the model is inadequate to infer the relations with context information. In addition, the model generates very large amount of candidates from biological text, and it is the semi-automatic, labor intensive technique requiring human expert's input. In this paper, we propose a novel interaction inference technique that incorporates context term vectors into the ABC model to discover meaningful, hidden relationships. Our hypothesis is that the context-based relation extraction between AB interactions and BC interactions is more effective and efficient than the original ABC model without considering the context information. We evaluated our hypothesis with the datasets of the "Alzheimer's disease" related 77,711 PubMed abstracts. As golden standards, PharmGKB and CTD databases were used. The results indicate that context-based interaction extraction achieved better precision than the basic ABC model approach. The literature analysis also shows that interactions inferred by the context-based approach are more meaningful than interactions by the basic ABC model.},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
pages = {27–34},
numpages = {8},
keywords = {inference of new relations, context terms, text mining},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/2064696.2064705,
author = {Meurs, Marie-Jean and Murphy, Caitlin and Morgenstern, Ingo and Naderi, Nona and Butler, Greg and Powlowski, Justin and Tsang, Adrian and Witte, Ren\'{e}},
title = {Semantic Text Mining for Lignocellulose Research},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064696.2064705},
doi = {10.1145/2064696.2064705},
abstract = {Semantic technologies, including natural language processing (NLP), ontologies, semantic web services and web-based collaboration tools, promise to support users in dealing with complex data, thereby facilitating knowledge-intensive tasks. An ongoing challenge is to select the appropriate technologies and combine them in a coherent system that brings measurable improvements to the users. We present our ongoing development of a semantic infrastructure in support of genomics-based lignocellulose research. Part of this effort is the automated curation of knowledge from information on enzymes from fungi that is available in the literature and genome resources. Fungi naturally break down lignocellulose, hence the identification and characterization of the enzymes that they use in lignocellulose hydrolysis is an important part in research and development of biomass-derived products and fuels. Working close to the biology researchers who manually curate the existing literature, we developed ontological NLP pipelines integrated in a Web-based interface to help them in two main tasks: mining the literature for relevant information, and at the same time providing rich and semantically linked information.},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
pages = {35–42},
numpages = {8},
keywords = {semantic computing, text mining, lignocellulose research},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/2064696.2064706,
author = {Korkontzelos, Ioannis and Mu, Tingting and Restificar, Angelo and Ananiadou, Sophia},
title = {Text Mining for Efficient Search and Assisted Creation of Clinical Trials},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064696.2064706},
doi = {10.1145/2064696.2064706},
abstract = {Clinical trials are mandatory protocols describing medical research on humans and among the most valuable sources of medical practice evidence. Searching for trials relevant to some query is laborious due to the immense number of existing protocols. Apart from search, writing new trials includes composing detailed eligibility criteria, which might be time-consuming, especially for new researchers. In this paper we present ASCOT, an efficient search application customised for clinical trials. ASCOT uses text mining and data mining methods to enrich clinical trials with metadata, that in turn serve as effective tools to narrow down search. In addition, ASCOT integrates a component for recommending eligibility criteria based on a set of selected protocols.},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
pages = {43–50},
numpages = {8},
keywords = {clinical trials, facet, eligibility criteria, clustering, terms, cluster labels, term extraction},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/2064696.2064707,
author = {MacKinlay, Andrew and Martinez, David and Baldwin, Timothy},
title = {A Parser-Based Approach to Detecting Modification of Biomedical Events},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064696.2064707},
doi = {10.1145/2064696.2064707},
abstract = {This work describes a system for identifying event mentions in bio-molecular text that are either speculative (e.g. analysis of IkappaBalpha phosphorylation, where it is not specified whether phosphorylation did or did not occur) or negated (e.g. inhibition of IkappaBalpha phosphorylation, where phosphorylation did not occur). Our system combines a simple bag-of-words approach with two grammar-based approaches, namely the English Resource Grammar and the RASP parser. We interpret the output of the respective parsers via MRS semantics, and feed them into a machine learner. Our results indicate that grammar-based techniques can enhance the accuracy of methods for detecting event modification.},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
pages = {51–58},
numpages = {8},
keywords = {parsing, information extraction, machine learning, biomedical text},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/3247989,
author = {Oakes, Michael},
title = {Session Details: Short Paper},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247989},
doi = {10.1145/3247989},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
numpages = {1},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/2064696.2064709,
author = {Stevenson, Mark},
title = {Disambiguation of Medline Abstracts Using Topic Models},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064696.2064709},
doi = {10.1145/2064696.2064709},
abstract = {Topic models are an established technique for generating information about the subjects discussed in collections of documents. Latent Dirichlet Allocation (LDA) is a widely applied topic model. The topic models generated by LDA consist of sets of terms associated with each topic and these are used to provide context for a Word Sense Disambiguation (WSD) system. It is found that using this context leads to a statistically significant improvement in the performance of a graph-based WSD system when applied to a standard evaluation resource.},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
pages = {59–62},
numpages = {4},
keywords = {personalized pagerank, nlp, natural language processing, lda, wsd, latent dirichlet allocation, word sense disambiguation, topic models},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/2064696.2064710,
author = {Bong, Seong-Yong and Hwang, Kyu-Baek},
title = {Keyphrase Extraction in Biomedical Publications Using Mesh and Intraphrase Word Co-Occurrence Information},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064696.2064710},
doi = {10.1145/2064696.2064710},
abstract = {Document keyphrases are used for various tasks such as indexing, clustering, and summarization. To documents without keyphrases, an automatic extraction method can be applied. In this paper, we propose an enhanced method of extracting keyphrases from biomedical papers, using MeSH (Medical Subject Headings) and intraphrase word co-occurrence information. MeSH terms assigned to biomedical papers can serve, not only as important features for keyphrase extraction, but also for expansion of keyphrase candidates. Intraphrase word co-occurrence information can be exploited for re-ranking keyphrase candidates. Through an experimental evaluation on 1,799 articles from three academic journals in the biomedical literature, we show that the candidate expansion and re-ranking steps of our approach are highly effective for improving the performance of keyphrase extraction.},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
pages = {63–66},
numpages = {4},
keywords = {keyphrase extraction, mesh, intraphrase word co-occurrence, machine learning},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/2064696.2064711,
author = {Lee, Jinsoo and Kasperovics, Romans and Han, Wook-Shin and Cho, Hune},
title = {On Supporting Efficient Updates of Regular Expression Indexes in RDF Databases},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064696.2064711},
doi = {10.1145/2064696.2064711},
abstract = {The Resource Description Framework (RDF) is widely used for sharing biomedical resources, such as the online protein database UniProt or gene database GeneOntology. SPARQL is the native query language for RDF databases and it features regular expressions in queries for which the exact values are either irrelevant or unknown. A recent paper by Lee et al. presented an efficient indexing support for such queries adopting multigram indexes for regular expressions. In this paper we contribute to their work by addressing index updates. As a result, we identify a major performance problem of straightforward implementations and design a new algorithm that utilizes unique properties of multigram indexes. Our contributions can be summarized as follows: 1) we propose an efficient update algorithm for regular expression indexes in RDF databases; 2) we build a prototype system for the proposed framework in C++; 3) we conduct extensive experiments to demonstrate the properties of our algorithm. The experiments show that our algorithm outperforms the straightforward implementations by an order of magnitude.},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
pages = {67–70},
numpages = {4},
keywords = {rdf, gene ontology, regular expression, update, sparql},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

@inproceedings{10.1145/2064696.2064712,
author = {Yu, Hasun and Lee, Doheon},
title = {Functional Analysis of Human Whole Brain Regions Based on Gene Expression},
year = {2011},
isbn = {9781450309608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064696.2064712},
doi = {10.1145/2064696.2064712},
abstract = {We studied the relationship between gene expression patterns of whole human brain, classical anatomical structures of human brain and functions of human brain. We used microarray data from Allen Human Brain Atlas (AHBA) which provides genome-wide expression profiles in two human brains. We analyzed gene expression patterns in human whole brain regions using k-means clustering and correlation analysis. Our result shows that the gene expression patterns in two human brains are highly correlated with the anatomical structure of human brain. In addition, we found region specific genes which are significantly over-expressed genes in one brain region and analyzed those genes using the DAVID functional annotation tool. We found the case that several region specific genes are indeed associated to the function of a particular brain region. Our results help understand connection among the gene expression patterns, anatomical structures and functions of human brain.},
booktitle = {Proceedings of the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics},
pages = {71–74},
numpages = {4},
keywords = {human brain, brain functions, gene expression, brain structures},
location = {Glasgow, Scotland, UK},
series = {DTMBIO '11}
}

