@inproceedings{10.1145/3098279.3098546,
author = {Duente, Tim and Pfeiffer, Max and Rohs, Michael},
title = {Zap++: A 20-Channel Electrical Muscle Stimulation System for Fine-Grained Wearable Force Feedback},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098546},
doi = {10.1145/3098279.3098546},
abstract = {Electrical muscle stimulation (EMS) has been used successfully in HCI to generate force feedback and simple movements both in stationary and mobile settings. However, many natural limb movements require the coordinated actuation of multiple muscles. Off-the-shelf EMS devices are typically limited in their ability to generate fine-grained movements, because they only have a low number of channels and do not provide full control over the EMS parameters. More capable medical devices are not designed for mobile use or still have a lower number of channels and less control than is desirable for HCI research. In this paper we present the concept and a prototype of a 20-channel mobile EMS system that offers full control over the EMS parameters. We discuss the requirements of wearable multi-electrode EMS systems and present the design and technical evaluation of our prototype. We further outline several application scenarios and discuss safety and certification issues.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {1},
numpages = {13},
keywords = {wearable, mobile haptic output, mobile, electrode grid, wearable force feedback, electrical muscle stimulation},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098564,
author = {Ghosh, Surjya and Ganguly, Niloy and Mitra, Bivas and De, Pradipta},
title = {TapSense: Combining Self-Report Patterns and Typing Characteristics for Smartphone Based Emotion Detection},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098564},
doi = {10.1145/3098279.3098564},
abstract = {Typing based communication applications on smartphones, like WhatsApp, can induce emotional exchanges. The effects of an emotion in one session of communication can persist across sessions. In this work, we attempt automatic emotion detection by jointly modeling the typing characteristics, and the persistence of emotion. Typing characteristics, like speed, number of mistakes, special characters used, are inferred from typing sessions. Self reports recording emotion states after typing sessions capture persistence of emotion. We use this data to train a personalized machine learning model for multi-state emotion classification. We implemented an Android based smartphone application, called TapSense, that records typing related metadata, and uses a carefully designed Experience Sampling Method (ESM) to collect emotion self reports. We are able to classify four emotion states - happy, sad, stressed, and relaxed, with an average accuracy (AUCROC) of 84% for a group of 22 participants who installed and used TapSense for 3 weeks.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {2},
numpages = {12},
keywords = {SMOTE, emotion detection, Markov chain, experience sampling method, Smartphone typing, emotion persistence},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098562,
author = {St\"{u}tz, Thomas and Domhardt, Michael and Emsenhuber, Gerlinde and Huber, Daniela and Tiefengrabner, Martin and Matis, Nicholas and Ginzinger, Simon},
title = {An Interactive 3D Health App with Multimodal Information Representation for Frozen Shoulder},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098562},
doi = {10.1145/3098279.3098562},
abstract = {Patients with Frozen Shoulder suffer from a decreased mobility and pain. Exercise-based physiotherapy is a common treatment and patients mostly perform the exercises at home. Correct exercise performance and compliance are the main issues in home-based therapy of Frozen Shoulder patients. To support patients diagnosed with Frozen Shoulder, a multimodal 3D smartphone app was designed, developed and evaluated. Additional to ten potential users, one physician, five physiotherapists, three computer scientists, two 3D artists and one HCI specialist were involved in the co-creation process. The app was evaluated by five patients during a three-week pilot study, which showed the feasibility of our approach. Exercise correctness, usage of multimodal instructions and user satisfaction were analyzed. Exercise correctness was nearly perfect and the interactive 3D animation was used for exercise instructions. Satisfaction using the app was rated very high according to SUS score. The results confirm that the co-creation process led to an effective, highly satisfactory and actually used system.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {3},
numpages = {11},
keywords = {user study, co-creation, patients, mHealth, multimodal information representation, evaluation, eHealth},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098533,
author = {Ortega, Micha\"{e}l and Maisonnasse, J\'{e}r\^{o}me and Nigay, Laurence},
title = {EXHI-Bit: A Mechanical Structure for Prototyping EXpandable Handheld Interfaces},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098533},
doi = {10.1145/3098279.3098533},
abstract = {We present EXHI-bit, a mechanical structure for prototyping unique shape-changing interfaces that can be easily built in a fabrication laboratory. EXHI-bit surfaces consist of inter-weaving units that slide in two dimensions. This assembly enables the creation of unique expandable handheld surfaces with continuous transitions while maintaining the surface flat, rigid, and non-porous. EXHI-bit surfaces can be combined to create 2D and 3D multi-surface objects. In this paper, we demonstrate the versatility and generality of EXHI-bit with user-deformed and self-actuated 1D, 2D, and 3D prototypes employed in an architectural urban planning scenario. We also present vision on the use of expandable tablets in our everyday life from 10 users after having interacted with an EXHI-bit tablet.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {4},
numpages = {11},
keywords = {tangible user interfaces, handheld device, expansion, organic user interface, shape-changing interface},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098528,
author = {Rekik, Yosra and Vezzoli, Eric and Grisoni, Laurent},
title = {Understanding Users' Perception of Simultaneous Tactile Textures},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098528},
doi = {10.1145/3098279.3098528},
abstract = {We study users' perception of simultaneous tactile textures in ultrasonic devices. We investigate how relevant is providing the user with different complementary and simultaneous textures with respect to the different fingers that can be used to touch the surface. We show through a controlled experiment that users are able to distinguish the number of different textures independently of using fingers from one or two hands. However, our findings indicate that users are not able to differentiate between two different textures, that is to correctly identify each of them, when using fingers from the same hand. Based on our findings, we are then able to outline three relevant guidelines to assist multi-finger tactile feedback ergonomic and devices design.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {5},
numpages = {6},
keywords = {experiment, tactile feedback, two-handed, one-handed, perception simultaneous textures, finger, ultrasonic device},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098535,
author = {Svangren, Michael K. and Skov, Mikael B. and Kjeldskov, Jesper},
title = {The Connected Car: An Empirical Study of Electric Cars as Mobile Digital Devices},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098535},
doi = {10.1145/3098279.3098535},
abstract = {The amount of interactive digital technology in cars is increasing rapidly, and many new cars are shipped with connectivity. As a result, a new platform has emerged that holds potentials to facilitate many new and different interactions, both inside and outside the car. Within the area of HCI for cars, the focus has predominantly been on interactions with in-vehicle systems and applications of technology that is enabled through connectivity. However, we still lack in-depth empirical studies that provide details of the connected car, its use, opinions towards it, and how it integrates into people's everyday lives. We report from a qualitative study of 13 households with connected electric cars. We present our findings in 3 themes of interaction through connectivity, updating and upgrading car software, and security and privacy. We further discuss our findings in 3 themes that might inform and inspire further mobile HCI research with the connected car.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {6},
numpages = {12},
keywords = {connected car, connectivity, electric car, mobile devices},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098529,
author = {Wenig, Nina and Wenig, Dirk and Ernst, Steffen and Malaka, Rainer and Hecht, Brent and Sch\"{o}ning, Johannes},
title = {Pharos: Improving Navigation Instructions on Smartwatches by Including Global Landmarks},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098529},
doi = {10.1145/3098279.3098529},
abstract = {Landmark-based navigation systems have proven benefits relative to traditional turn-by-turn systems that use street names and distances. However, one obstacle to the implementation of landmark-based navigation systems is the complex challenge of selecting salient local landmarks at each decision point for each user. In this paper, we present Pharos, a novel system that extends turn-by-turn navigation instructions using a single global landmark (e.g. the Eiffel Tower, the Burj Khalifa, municipal TV towers) rather than multiple, hard-to-select local landmarks. We first show that our approach is feasible in a large number of cities around the world through the use of computer vision to select global landmarks. We then present the results of a study demonstrating that by including global landmarks in navigation instructions, users navigate more confidently and build a more accurate mental map of the navigated area than using turn-by-turn instructions.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {7},
numpages = {13},
keywords = {landmark-based navigation, computer vision, global landmarks, pedestrian navigation, smartwatches},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098563,
author = {Zeng, Limin and Simros, Markus and Weber, Gerhard},
title = {Camera-Based Mobile Electronic Travel Aids Support for Cognitive Mapping of Unknown Spaces},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098563},
doi = {10.1145/3098279.3098563},
abstract = {Blind and visually impaired people often use an "object-to-object" strategy to explore unknown spaces through physical contact via their canes or bodies. Camera-based mobile electronic travel aids (ETAs) not only offer a larger work range than a white cane while detecting obstacles, but also enable users to recognise objects of interest without physical contact. In this paper, we conducted a case study with seven visually impaired participants, to investigate how they use a depth-sensing camera-based ETA for exploring unknown spaces, and how they reconstruct cognitive mappings of surrounding objects. We found that camera-based mobile ETAs would assist visually impaired users to explore the surrounding environment effectively and without physical contact. Furthermore, our original study indicates ETA users change their strategies for exploring the surrounding environment from using an "object-to-object" approach to an "observation-point to observation-point" strategy. Participants improved their cognitive mappings of the surrounding environment compared to white cane users.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {8},
numpages = {10},
keywords = {electronic travel aid, visually impaired, cognitive mapping, verbal exploration, obstacle detection, quantitative assessment},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098545,
author = {Matthies, Denys J. C. and Roumen, Thijs and Kuijper, Arjan and Urban, Bodo},
title = {CapSoles: Who is Walking on What Kind of Floor?},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098545},
doi = {10.1145/3098279.3098545},
abstract = {Foot interfaces, such as pressure-sensitive insoles, still yield unused potential such as for implicit interaction. In this paper, we introduce CapSoles, enabling smart insoles to implicitly identify who is walking on what kind of floor. Our insole prototype relies on capacitive sensing and is able to sense plantar pressure distribution underneath the foot, plus a capacitive ground coupling effect. By using machine-learning algorithms, we evaluated the identification of 13 users, while walking, with a confidence of ∼95% after a recognition delay of ∼1s. Once the user's gait is known, again we can discover irregularities in gait plus a varying ground coupling. While both effects in combination are usually unique for several ground surfaces, we demonstrate to distinguish six kinds of floors, which are sand, lawn, paving stone, carpet, linoleum, and tartan with an average accuracy of ∼82%. Moreover, we demonstrate the unique effects of wet and electrostatically charged surfaces.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {9},
numpages = {14},
keywords = {ground surface detection, capacitive sensing, implicit input, data mining, machine learning, floor detection, shoe interface, smart insole, user identification, wearable computing, foot interaction},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098549,
author = {Arif, Ahmed Sabbir and Kim, Sunjun and Lee, Geehyuk},
title = {Usability of Different Types of Commercial Selfie Sticks},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098549},
doi = {10.1145/3098279.3098549},
abstract = {This paper reviews and categorizes the most common types of selfie sticks available in the market and discusses their design and usability. Through a theoretical analysis, it demonstrates how most commercial selfie sticks ignore important human factors, including ergonomics. It, then, presents results of an online survey, where selfie stick users (N=105), predominantly from the Republic of Korea, rated the usability of their selfie sticks. Results of the survey provided an insight into users' selfie stick usage behavior and suggested that most commercial selfie sticks are unergonomic, causing the users short-term fatigues that could "in theory" turn into chronic over the time with extensive use. Finally, based on the survey results, the paper makes recommendations for future design.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {10},
numpages = {8},
keywords = {smartphone, mobile device, evaluation, selfie stick, design, self-photography, ergonomics, usability, camera},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098526,
author = {Pielot, Martin and Rello, Luz},
title = {Productive, Anxious, Lonely: 24 Hours without Push Notifications},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098526},
doi = {10.1145/3098279.3098526},
abstract = {We report from the Do Not Disturb Challenge where 30 volunteers disabled notification alerts for 24 hours across all devices. The effect of the absence of notifications on the participants was isolated through an experimental study design: we compared self-reported feedback from the day without notifications against a baseline day. The evidence indicates that notifications have locked us in a dilemma: without notifications, participants felt less distracted and more productive. But, they also felt no longer able to be as responsive as expected, which made some participants anxious. And, they felt less connected with one's social group. In contrast to previous reports, about two third of the participants expressed the intention to change how they manage notifications. Two years later, half of the participants are still following through with their plans.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {11},
numpages = {11},
keywords = {mobile devices, notifications, deprivation study},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098532,
author = {Visuri, Aku and van Berkel, Niels and Luo, Chu and Goncalves, Jorge and Ferreira, Denzil and Kostakos, Vassilis},
title = {Predicting Interruptibility for Manual Data Collection: A Cluster-Based User Model},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098532},
doi = {10.1145/3098279.3098532},
abstract = {Previous work suggests that Quantified-Self applications can retain long-term usage with motivational methods. These methods often require intermittent attention requests with manual data input. This may cause unnecessary burden to the user, leading to annoyance, frustration and possible application abandonment. We designed a novel method that uses on-screen alert dialogs to transform recurrent smartphone usage sessions into moments of data contributions and evaluate how accurately machine learning can reduce unintended interruptions. We collected sensor data from 48 participants during a 4-week long deployment and analysed how personal device usage can be considered in scheduling data inputs. We show that up to 81.7% of user interactions with the alert dialogs can be accurately predicted using user clusters, and up to 75.5% of unintended interruptions can be prevented and rescheduled. Our approach can be leveraged by applications that require self-reports on a frequent basis and may provide a better longitudinal QS experience.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {12},
numpages = {14},
keywords = {interruptibility, smartphones, quantified-self, self-reports},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098569,
author = {Kushlev, Kostadin and Cardoso, Bruno and Pielot, Martin},
title = {Too Tense for Candy Crush: Affect Influences User Engagement with Proactively Suggested Content},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098569},
doi = {10.1145/3098279.3098569},
abstract = {Push notifications are increasingly being used to engage users with app content. In the present research, we propose that users' current affect (i.e., how they are feeling) should be a critical---yet underexplored---factor in user engagement. Participants (N = 337) downloaded a custom-developed app that delivered notifications. After attending to a notification (N = 32,704), participants reported how they felt and chose whether to engage with further content; they could choose between mentally demanding or simple/diverting tasks. When feeling good, people were less likely to engage with mentally demanding tasks. When feeling calm, people were more likely to engage with diverting tasks. When feeling energetic, people were less likely to respond to distractions altogether. These findings provide a tantalizing first glimpse into how affect predicts the kind of content users choose to engage with, paving the way for the use of affect in the design of notification systems.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {13},
numpages = {6},
keywords = {mobile computing, push notifications, attention management, affect, pervasive computing, user engagement, proactive recommendations},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098555,
author = {Pearson, Jennifer and Robinson, Simon and Jones, Matt and Coutrix, C\'{e}line},
title = {Evaluating Deformable Devices with Emergent Users},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098555},
doi = {10.1145/3098279.3098555},
abstract = {This research forms part of a wider body of work focused around involving emergent users---those just beginning to get access to mobile devices---in the development and refinement of far-future technologies. In this paper we present an evaluation of a new type of deformable slider with emergent users, designed to investigate whether shape-changing interfaces provide any benefit over touchscreens for this type of user. Our trials, which took place in two contexts and three disparate regions, revealed that while there was a clear correlation between performance and technology exposure, emergent users had similar ability with both touchscreen and deformable controls.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {14},
numpages = {7},
keywords = {sensels, emergent users, tangibility, deformable devices},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098552,
author = {Bilal, Anas and Rextin, Aimal and Kakakhel, Ahmad and Nasim, Mehwish},
title = {Roman-Txt: Forms and Functions of Roman Urdu Texting},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098552},
doi = {10.1145/3098279.3098552},
abstract = {In this paper, we present a user study conducted on students of a local university in Pakistan and collected a corpus of Roman Urdu text messages. We were interested in forms and functions of Roman Urdu text messages. To this end, we collected a mobile phone usage dataset. The data consists of 116 users and 346, 455 text messages. Roman Urdu text, is the most widely adopted style of writing text messages in Pakistan. Our user study leads to interesting results, for instance, we were able to quantitatively show that a number of words are written using more than one spelling; most participants of our study were not comfortable in English and hence they write their text messages in Roman Urdu; and the choice of language adopted by the participants sometimes varies according to who the message is being sent. Moreover we found that many young students send text messages(SMS) of intimate nature.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {15},
numpages = {9},
keywords = {forms and functions, texting adaptations, roman urdu text messages, text message analysis},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098554,
author = {Conradi, Jessica},
title = {Influence of Letter Size on Word Reading Performance during Walking},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098554},
doi = {10.1145/3098279.3098554},
abstract = {Mobile devices like smartphones facilitate parallel activities of IT-interaction and other tasks. Especially smartphone usage during walking can often be observed. This can lead to distracted walking which frequently results in serious consequences for the user and others. As people will probably go on using mobile devices in this way, e.g., for navigation, we have to provide interaction interfaces which are adapted to the special needs of secondary usage while walking. One way to adapt apps to walking is to reduce time for content perception. A substantial part of content is given in written language, and therefore, reading is of high importance. Based on this, we pursued the reduction of reading time on a mobile device during walking.We conducted a study which provides information about the minimum acceptable letter sizes for reading single words while walking. We carried out an experiment in which we combined short presentation times of commonly used words with different letter sizes and analyzed the outcome by means of psychometric functions. We administered these functions for the independent variables presentation time, walking speed vs. standing and length of words (number of letters) and analyzed the outcome statistically in respect to the minimal visual angle. We found highly significant influences of all three conditions on the legibility of the words. We recommend the adaptation of applications for walking according to our findings.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {16},
numpages = {9},
keywords = {words, letter size, distracted walking, mobile devices, reading, interface design},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098566,
author = {Kraft, Jonas F. and Hurtienne, J\"{o}rn},
title = {Transition Animations Support Orientation in Mobile Interfaces without Increased User Effort},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098566},
doi = {10.1145/3098279.3098566},
abstract = {Some animations in mobile user interfaces aim at supporting user orientation by facilitating users to build a mental model of the UI's structure. Possible drawbacks are that animations are time-consuming and that complex and distracting animations may increase users' mental workload. These effects of orientation animations are investigated in an empirical study. Participants either used an animated or a non-animated version of a mobile movie recommender app. The results imply that animations can support users in building more accurate mental models of the app's structure and enhance gesture-based interaction. No additional costs in terms of time or mental workload were incurred when using the animations. Thus, lightweight orientation animations can have large potential benefits at little cost.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {17},
numpages = {6},
keywords = {mental workload, orientation, animation, usability, user experience, mental model},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098559,
author = {Othman, Mohammad and Amaral, Telmo and McNaney, R\'{o}is\'{\i}n and Smeddinck, Jan D. and Vines, John and Olivier, Patrick},
title = {CrowdEyes: Crowdsourcing for Robust Real-World Mobile Eye Tracking},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098559},
doi = {10.1145/3098279.3098559},
abstract = {Current eye tracking technologies have a number of drawbacks when it comes to practical use in real-world settings. Common challenges, such as high levels of daylight, eyewear (e.g. spectacles or contact lenses) and eye make-up, give rise to noise that undermines their utility as a standard component for mobile computing, design, and evaluation. To work around these challenges, we introduce CrowdEyes, a mobile eye tracking solution that utilizes crowdsourcing for increased tracking accuracy and robustness. We present a pupil detection task design for crowd workers together with a study that demonstrates the high-level accuracy of crowdsourced pupil detection in comparison to state-of-the-art pupil detection algorithms. We further demonstrate the utility of our crowdsourced analysis pipeline in a fixation tagging task. In this paper, we validate the accuracy and robustness of harnessing the crowd as both an alternative and complement to automated pupil detection algorithms, and explore the associated costs and quality of our crowdsourcing approach.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {18},
numpages = {13},
keywords = {crowdsourcing, pupil detection, mobile computing, crowd quality control, wearable computing, eye tracking},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098544,
author = {Navas Medrano, Samuel and Pfeiffer, Max and Kray, Christian},
title = {Enabling Remote Deictic Communication with Mobile Devices: An Elicitation Study},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098544},
doi = {10.1145/3098279.3098544},
abstract = {Mobile systems provide many means to relay information to a distant partner, but remote communication is still limited compared to face-to-face interaction. Deictic communication and pointing, in particular, are challenging when two parties communicate across distances. In this paper, we investigate how people envision remote pointing would work when using mobile devices. We report on an elicitation study where we asked participants to perform a series of remote pointing tasks. Our results provide initial insights into user behaviors and specific issues in this context. We discovered that most people follow one of two basic patterns, that their individual pointing behavior is very consistent and that the shape and location of the target object have little influence on the pointing gesture used. From our results, we derived a set of design guidelines for future user interfaces for remote pointing. Our contributions can benefit designers and researchers of such interfaces.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {19},
numpages = {13},
keywords = {participatory design, mobile interaction, gestures},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098540,
author = {Lottridge, Danielle and Bentley, Frank and Wheeler, Matt and Lee, Jason and Cheung, Janet and Ong, Katherine and Rowley, Cristy},
title = {Third-Wave Livestreaming: Teens' Long Form Selfie},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098540},
doi = {10.1145/3098279.3098540},
abstract = {Mobile livestreaming is now well into its third wave. From early systems such as Bambuser and Qik, to more popular apps Meerkat and Periscope, to today's integrated social streaming features in Facebook and Instagram, both technology and usage have changed dramatically. In this latest phase of livestreaming, cameras turn inward to focus on the streamer, instead of outwards on the surroundings. Teens are increasingly using these platforms to entertain friends, meet new people, and connect with others on shared interests. We studied teens' livestreaming behaviors and motivations on these new platforms through a survey completed by 2,247 American livestreamers and interviews with 20 teens, highlighting changing practices, teens' differences from the broader population, and implications for designing new livestreaming services.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {20},
numpages = {12},
keywords = {video, teens, mobile, livestreaming},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098531,
author = {Pinder, Charlie and Vermeulen, Jo and Cowan, Benjamin R. and Beale, Russell and Hendley, Robert J.},
title = {Exploring the Feasibility of Subliminal Priming on Smartphones},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098531},
doi = {10.1145/3098279.3098531},
abstract = {Subliminal priming has the potential to influence people's attitudes and behaviour, making them prefer certain choices over others. Yet little research has explored its feasibility on smartphones, even though the global popularity and increasing use of smartphones has spurred interest in mobile behaviour change interventions. This paper addresses technical, ethical and design issues in delivering mobile subliminal priming. We present three explorations of the technique: a technical feasibility study, and two participant studies. A pilot study (n=34) explored subliminal goal priming in-the-wild over 1 week, while a semi-controlled study (n=101) explored the immediate effect of subliminal priming on 3 different types of stimuli. We found that although subliminal priming is technically possible on smartphones, there is limited evidence of impact on changes in how much stimuli are preferred by users, with inconsistent effects across stimuli types. We discuss the implications of our results and directions for future research.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {21},
numpages = {15},
keywords = {mobile, mere exposure effect, smartphones, subliminal priming, nonconscious behaviour change technology},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098548,
author = {Chan, Liwei and Minamizawa, Kouta},
title = {FrontFace: Facilitating Communication between HMD Users and Outsiders Using Front-Facing-Screen HMDs},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098548},
doi = {10.1145/3098279.3098548},
abstract = {A head-mounted display (HMD) immerses users in a virtual world, but separates them from outsiders in the real world. We present FrontFace, which is a novel HMD that combines an eye-tracker with a front-facing screen, to lower the communication barrier between HMD users and outsiders. The front-facing screen reveals user attention (e.g., the users eye motions) and user presence in the virtual or real world by displaying the scene in the virtual world or a skin background respectively, enabling eye-contact interactions between the HMD user and the outsiders. FrontFace has the following benefits. Firstly, it communicates the presence of the HMD user to outsiders; secondly, it reveals the player's visual attention by introducing the HMD users originally occluded eye motions, enabling outsiders to make sense of the HMD user's reaction in the virtual world or the real world. Three interactive techniques for the outsiders to initiate communication to HMD users are proposed: they are tap-trigger, hand-gesture trigger, and voice-trigger interactions. A small focus group provided feedback.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {22},
numpages = {5},
keywords = {spectator experience, virtual reality},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098560,
author = {Dunlop, Mark D. and Roper, Marc and Imperatore, Gennaro},
title = {Text Entry Tap Accuracy and Exploration of Tilt Controlled Layered Interaction on Smartwatches},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098560},
doi = {10.1145/3098279.3098560},
abstract = {Design of text entry on small screen devices, e.g. smartwatches, faces two related challenges: trading off a reasonably sized keyboard area against space to display the entered text and the concern over "fat fingers". This paper investigates tap accuracy and revisits layered interfaces to explore a novel layered text entry method. A two part user study identifies preferred typing and reading tilt angles and then investigates variants of a tilting layered keyboard against a standard layout. We show good typing speed (29 wpm) and very high accuracy on the standard layout - contradicting fears of fat-fingers limiting watch text-entry. User feedback is positive towards tilting interaction and we identify ∼14° tilt as a comfortable typing angle. However, layering resulted in slightly slower and more erroneous entry. The paper contributes new data on tilt angles and key offsets for smartwatch text entry and supporting evidence for the suitability of QWERTY on smartwatches.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {23},
numpages = {11},
keywords = {fat finger, tap accuracy, layered interfaces, text-entry, user experimentation, smartwatch interaction},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098542,
author = {Kerber, Frederic and Kiefer, Tobias and L\"{o}chtefeld, Markus and Kr\"{u}ger, Antonio},
title = {Investigating Current Techniques for Opposite-Hand Smartwatch Interaction},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098542},
doi = {10.1145/3098279.3098542},
abstract = {The small display size of smartwatches creates a challenge for touch input, which is still the interaction technique of choice. Researchers and producers have started to investigate alternative interaction techniques. Apple and Samsung, for example, introduced digital versions of classic watch components such as the digital crown and the rotatable bezel. However, it remains an open question how well these components behave in terms of user interaction. Based on a self-built smartwatch prototype, we compare current interaction paradigms (touch input, rotatable bezel and digital crown) for one-dimensional tasks, i.e. scrolling in a list, two-dimensional tasks, i.e. navigation on a digital map, and a complex navigation/zoom task. To check for ecological validity of our results, we conducted an additional study focusing on interaction with currently available off-the-shelf devices using our considered interaction paradigms. Following our results, we present guidelines on which interaction techniques to use for the respective tasks.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {24},
numpages = {12},
keywords = {smartwatches, opposite-side interaction, input techniques},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098541,
author = {Yeo, Hui-Shyong and Lee, Juyoung and Bianchi, Andrea and Harris-Birtill, David and Quigley, Aaron},
title = {SpeCam: Sensing Surface Color and Material with the Front-Facing Camera of a Mobile Device},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098541},
doi = {10.1145/3098279.3098541},
abstract = {SpeCam is a lightweight surface color and material sensing approach for mobile devices which only uses the front-facing camera and the display as a multi-spectral light source. We leverage the natural use of mobile devices (placing it face-down) to detect the material underneath and therefore infer the location or placement of the device. SpeCam can then be used to support discreet micro-interactions to avoid the numerous distractions that users daily face with today's mobile devices. Our two-parts study shows that SpeCam can i) recognize colors in the HSB space with 10 degrees apart near the 3 dominant colors and 4 degrees otherwise and ii) 30 types of surface materials with 99% accuracy. These findings are further supported by a spectroscopy study. Finally, we suggest a series of applications based on simple mobile micro-interactions suitable for using the phone when placed face-down.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {25},
numpages = {9},
keywords = {surface detection, discreet interaction, material detection},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098561,
author = {Delamare, William and Han, Teng and Irani, Pourang},
title = {Designing a Gaze Gesture Guiding System},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098561},
doi = {10.1145/3098279.3098561},
abstract = {We propose the concept of a guiding system specifically designed for semaphoric gaze gestures, i.e. gestures defining a vocabulary to trigger commands via the gaze modality. Our design exploration considers fundamental gaze gesture phases: Exploration, Guidance, and Return. A first experiment reveals that Guidance with dynamic elements moving along 2D paths is efficient and resistant to visual complexity. A second experiment reveals that a Rapid Serial Visual Presentation of command names during Exploration allows for more than 30% faster command retrievals than a standard visual search. To resume the task where the guide was triggered, labels moving from the outward extremity of 2D paths toward the guide center leads to efficient and accurate origin retrieval during the Return phase. We evaluate our resulting Gaze Gesture Guiding system, G3, for interacting with distant objects in an office environment using a head-mounted display. Users report positively on their experience with both semaphoric gaze gestures and G3.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {26},
numpages = {13},
keywords = {RSVP, guide, design, gesture, guidance, gaze, eye input},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098538,
author = {Kikuchi, Takashi and Sugiura, Yuta and Masai, Katsutoshi and Sugimoto, Maki and Thomas, Bruce H.},
title = {EarTouch: Turning the Ear into an Input Surface},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098538},
doi = {10.1145/3098279.3098538},
abstract = {In this paper, we propose EarTouch, a new sensing technology for ear-based input for controlling applications by slightly pulling the ear and detecting the deformation by an enhanced earphone device. It is envisioned that EarTouch will enable control of applications such as music players, navigation systems, and calendars as an "eyes-free" interface. As for the operation of EarTouch, the shape deformation of the ear is measured by optical sensors. Deformation of the skin caused by touching the ear with the fingers is recognized by attaching optical sensors to the earphone and measuring the distance from the earphone to the skin inside the ear. EarTouch supports recognition of multiple gestures by applying a support vector machine (SVM). EarTouch was validated through a set of user studies.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {27},
numpages = {6},
keywords = {skin deformation, photo reflective sensor, earphone},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098565,
author = {Dingler, Tilman and Weber, Dominik and Pielot, Martin and Cooper, Jennifer and Chang, Chung-Cheng and Henze, Niels},
title = {Language Learning On-the-Go: Opportune Moments and Design of Mobile Microlearning Sessions},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098565},
doi = {10.1145/3098279.3098565},
abstract = {Learning a foreign language is a daunting and time-consuming task. People often lack the time or motivation to sit down and engage with learning content on a regular basis. We present an investigation of microlearning sessions on mobile phones, in which we focus on session triggers, presentation methods, and user context. Therefore, we built an Android app that prompts users to review foreign language vocabulary directly through notifications or through app usage across the day. We present results from a controlled and an in-the-wild study, in which we explore engagement and user context. In-app sessions lasted longer, but notifications added a significant number of "quick" learning sessions. 37.6% of sessions were completed in transit, hence learning-on-the-go was well received. Neither the use of boredom as trigger nor the presentation (flashcard and multiple-choice) had a significant effect. We conclude with implications for the design of mobile microlearning applications with context-awareness.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {28},
numpages = {12},
keywords = {microlearning, mobile device, attention, prediction, language learning, microinteractions},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098536,
author = {Marichal, Sebasti\'{a}n and Rosales, Anadrea and Perilli, Fernando Gonzalez and Pires, Ana Cristina and Bakala, Ewelina and Sansone, Gustavo and Blat, Josep},
title = {CETA: Designing Mixed-Reality Tangible Interaction to Enhance Mathematical Learning},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098536},
doi = {10.1145/3098279.3098536},
abstract = {The benefits of applying technology to education have been often questioned. Learning through digital devices might imply reducing the children's physical interaction with the real world, when cognitive theories hold that such interaction is essential to develop abstract concepts in Mathematics or Physics. However, conflicting reports suggest that tangible interaction does not always improve engagement or learning. A central question is how cognitive theories can be successfully applied to the design of interactive systems in order to achieve enhanced learning experiences. In this paper we discuss the interaction design of a mixed-reality system for mathematics learning for school-aged children. Our design approach combines inspiration from previous frameworks with a user-centered design process with early prototype evaluations. As a result of this process we have created a mixed-reality environment for low-cost tablets and an augmented version of the Cuisenaire rods, a milestone of the manipulatives for mathematics learning.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {29},
numpages = {13},
keywords = {technology enhanced learning, embodied interaction, mixed-reality, interaction design},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098525,
author = {Putjorn, Pruet and Siriaraya, Panote and Ang, Chee Siang and Deravi, Farzin},
title = {Designing a Ubiquitous Sensor-Based Platform to Facilitate Learning for Young Children in Thailand},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098525},
doi = {10.1145/3098279.3098525},
abstract = {Education plays an important role in helping developing nations reduce poverty and improving quality of life. Ubiquitous and mobile technologies could greatly enhance education in such regions by providing augmented access to learning. This paper presents a three-year iterative study where a ubiquitous sensor based learning platform was designed, developed and tested to support science learning among primary school students in underprivileged Northern Thailand. The platform is built upon the school's existing mobile devices and was expanded to include sensor-based technology. Throughout the iterative design process, observations, interviews and group discussions were carried out with stakeholders. This lead to key reflections and design concepts such as the value of injecting anthropomorphic qualities into the learning device and providing personally and culturally relevant learning experiences through technology. Overall, the results outlined in this paper help contribute to knowledge regarding the design, development and implementation of ubiquitous sensor-based technology to support learning.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {30},
numpages = {13},
keywords = {wireless sensing, mobile learning, mobile devices, primary school students, ubiquitous learning environment},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098568,
author = {Wong, Stephanie and Yang, Lillian and Riecke, Bernhard and Cramer, Emily and Neustaedter, Carman},
title = {Assessing the Usability of Smartwatches for Academic Cheating during Exams},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098568},
doi = {10.1145/3098279.3098568},
abstract = {Smartwatches are growing in usage, yet they come with the additional challenge of regulating their usage during the taking of academic tests. However, it is unclear how effective they are at actually allowing students to cheat. We conducted an experiment that examines the use of smartwatches for cheating on Multiple-Choice Questions (MCQ) and Short Answers (SA) with either Pictures/Text shown on the watch to aid students. Our results indicate that smartwatches are neither efficient nor have a high usability rating for cheating. However, students are able to score higher on Multiple-Choice Questions compared to Short Answers. We use the cheating paradigm as an example to understand the perceived usability and appropriation of smartwatches in an academic setting. We provide suggestions that help to deter cheating in an academic setting. Our study contributes to the research on academic integrity and the growing demand of wearable technologies.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {31},
numpages = {11},
keywords = {usability metric for user experience (UMUX), cheating, usability, wearables, smartwatches, academic integrity},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098543,
author = {Bertel, Sven and Dressel, Thomas and Kohlberg, Tom and von Jan, Vanessa},
title = {Spatial Knowledge Acquired from Pedestrian Urban Navigation Systems},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098543},
doi = {10.1145/3098279.3098543},
abstract = {We investigated the spatial knowledge that users of pedestrian navigation support acquire about the navigated area. In particular, we compare two conditions: A spatially richer condition, which provides continual access to information about route directions and surroundings via a local map at closest zoom level. And a spatially sparser condition, in which route directions are given via a tactile display and only as decision points come up. In a field study, 28 participants navigated on foot through a previously unfamiliar urban area. Data on resulting spatial knowledge, gaze distribution on environmental features, performance, individual spatial abilities, and user experience were collected and analysed. We were specifically interested in the route and survey knowledge that participants had acquired. The results point to advantages for acquiring route knowledge through using the sparser, tactile display condition and for acquiring survey knowledge through using the richer map condition. We conclude with discussing ramifications for the design and use of different types of pedestrian navigation support systems for different task scenarios.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {32},
numpages = {6},
keywords = {navigation, tactile interface, mobile device, spatial knowledge, wayfinding, user study, gaze data},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098537,
author = {Mayer, Sven and Gad, Perihan and Wolf, Katrin and Woundefinedniak, Pawe\l{} W. and Henze, Niels},
title = {Understanding the Ergonomic Constraints in Designing for Touch Surfaces},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098537},
doi = {10.1145/3098279.3098537},
abstract = {While most current interactive surfaces use only the position of the finger on the surface as the input source, previous work suggests using the finger orientation for enriching the input space. Thus, an understanding of the physiological restrictions of the hand is required to build effective interactive techniques that use finger orientation. We conducted a study to derive the ergonomic constraints for using finger orientation as an effective input source. In a controlled experiment, we systematically manipulated finger pitch and yaw while performing a touch action. Participants were asked to rate the feasibility of the touch action. We found that finger pitch and yaw do significantly affect perceived feasibility and 21.1% of the touch actions were perceived as impossible to perform. Our results show that the finger yaw input space can be divided into the comfort and non-comfort zones. We further present design considerations for future interfaces using finger orientation.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {33},
numpages = {9},
keywords = {surface, ergonomic zone, touch, pitch, yaw, ergonomics, mobile, non-comfort zone, finger orientation},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098556,
author = {Epp, Carrie Demmans and Munteanu, Cosmin and Axtell, Benett and Ravinthiran, Keerthika and Aly, Yomna and Mansimov, Elman},
title = {Finger Tracking: Facilitating Non-Commercial Content Production for Mobile e-Reading Applications},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098556},
doi = {10.1145/3098279.3098556},
abstract = {Limited literacy and visual impairment reduce the ability of many to read on their own. Current e-reader solutions rely on either unnatural synthetic voices or professionally produced audio e-books. Neither provide the same enjoyment as having a family member read to a user, especially when the user requires assistive reading (following printed text while listening to it being read). Unfortunately, the support for non-commercial production of such e-books is limited and requires significant effort. We evaluate a novel, assistive mobile interaction technique that facilitates the recording of audio e-books and their synchronization with the read text. We show that a technique based on a finger tracking metaphor provides optimal support with respect to reading speed. These human-in-the-loop, adaptive techniques can now be used to reduce the content-creation burden that is associated with supporting those who cannot read on their own.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {34},
numpages = {15},
keywords = {visual impairment, mobile e-readers, reading, literacy, assistive technology, education},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098530,
author = {Di Geronimo, Linda and Bertarini, Marica and Badertscher, Julia and Husmann, Maria and Norrie, Moira C.},
title = {Exploiting Mid-Air Gestures to Share Data among Devices},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098530},
doi = {10.1145/3098279.3098530},
abstract = {The number of smart devices that people own or share with family and friends has increased dramatically. As a result, users often want to copy data among devices such as smart-phones, tablets and desktop computers. While various chat and cloud services support the sharing of data, they require users to interrupt their workflow to copy resources. We present MyoShare, a system that allows content to be shared among devices using mid-air gestures that can be used at any time, independent of the current task and location of devices. We report on an elicitation study where participants designed a set of gestures for sharing content. In a second user study, we compared mid-air gestures with alternative interaction modes using keyboard or touch Shortcuts, Speech, and Menu Selection. We discuss the results of the study in terms of both the strengths and weaknesses of mid-air gestures, along with suggestions for future work.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {35},
numpages = {11},
keywords = {mobile, cross-device, elicitation study, mid-air gestures},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098553,
author = {Kerber, Frederic and Puhl, Michael and Kr\"{u}ger, Antonio},
title = {User-Independent Real-Time Hand Gesture Recognition Based on Surface Electromyography},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098553},
doi = {10.1145/3098279.3098553},
abstract = {In this paper, we present a novel real-time hand gesture recognition system based on surface electromyography. We employ a user-independent approach based on a support vector machine utilizing ten features extracted from the raw electromyographic data obtained from the Myo armband by Thalmic Labs. Through an improved synchronization approach, we simplified the application process of the sensing armband. We report the results of a user study with 14 participants using an extended set consisting of 40 gestures. Considering the set of five hand gestures currently supported off-the-shelf by the Myo armband, we outperform their approach with an overall accuracy of 95% compared to 68% with the original algorithm on the same dataset.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {36},
numpages = {7},
keywords = {electromyography, gestural input, hand gestures},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098527,
author = {Kirkham, Reuben and Ebassa, Romeo and Montague, Kyle and Morrissey, Kellie and Vlachokyriakos, Vasilis and Weise, Sebastian and Olivier, Patrick},
title = {WheelieMap: An Exploratory System for Qualitative Reports of Inaccessibility in the Built Environment},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098527},
doi = {10.1145/3098279.3098527},
abstract = {The built environment remains a persistent accessibility challenge for people with mobility impairments. Whilst platforms to report these inaccessible locations exist, the underlying documentation processes are verbose, time-consuming and fail to effectively communicate the barrier at hand. We propose WheelieMap, a platform which uses the motion of manual wheelchair users to support the identification and documentation of potentially problematic locations. WheelieMap captures and segments device video footage and GPS as evidence of the problematic space, which can then be shared with both other people with disabilities and the relevant authorities. We document the use of the WheelieMap prototype by both manual wheelchair users and planning experts through semi-structured interviews. The qualitative findings revealed this approach to be the most viable route for documenting inaccessibility, compared to the existing alternatives. We also offer guidance on how to design and develop similar community driven reporting and annotation systems in the accessibility setting.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {38},
numpages = {12},
keywords = {mapping, disability, town planning, accessibility},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098557,
author = {Wang, Xiying and Fussell, Susan R.},
title = {EnergyHome: Leveraging Housemate Dynamics to Motivate Energy Conservation},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098557},
doi = {10.1145/3098279.3098557},
abstract = {We designed, implemented, and evaluated an iOS mobile application called EnergyHome to investigate how social interaction among housemates is related to their engagement in energy-saving practices. EnergyHome enables housemates to track personal energy-saving activities and to collaborate on saving energy together. Fourteen pairs of housemates used EnergyHome for a week. Afterwards, we interviewed each participant about their use of the app. All interviews were transcribed and analyzed using an iterative comparative approach. We identified two types of social dynamics in the way housemates used the app. In complementary dynamics, one housemate took the initiative to set energy-saving challenges and the other(s) followed. In symmetrical dynamics, housemates collectively set energy-saving challenges but no one tried to exert control. Based on these findings, we discuss design strategies to motivate energy conservation that take into account different social dynamics among housemates.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {39},
numpages = {12},
keywords = {housemates, mobile design, household energy conservation, user experience, social dynamics},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098551,
author = {Schiavo, Gianluca and Leonardi, Chiara and Pasolli, Mattia and Sarti, Silvia and Zancanaro, Massimo},
title = {Weigh It and Share It! Crowdsourcing for pro-Environmental Data Collection},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098551},
doi = {10.1145/3098279.3098551},
abstract = {Crowdsourcing for addressing environmental challenges is a promising area in HCI research. This work investigates participation in a crowdsourcing initiative that combined a social-purpose activity with the interest of a company to crowdsource a labour-intensive task. The initiative was based on the deployment of a mobile application for pro-environmental data collection, namely collecting data about weight and type of product packaging. We report the results of a 9-month study conducted within a living lab that involved 96 customers of a large retail store. The analysis of usage logs and patterns of behaviour show different user categories: constant, sprinter and casual users. A survey was conducted to compare those categories as far as demographics, personality traits and usability metrics are concerned. Ten follow-up interviews further investigated motivations behind different usage patterns. The results provide insights on different types of contributors, reporting evidence on what motivated committed and less committed participants.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {40},
numpages = {12},
keywords = {crowdsourcing, living lab, motivation, sustainability},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098547,
author = {Alak\"{a}rpp\"{a}, Ismo and Jaakkola, Elisa and V\"{a}yrynen, Jani and H\"{a}kkil\"{a}, Jonna},
title = {Using Nature Elements in Mobile AR for Education with Children},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098547},
doi = {10.1145/3098279.3098547},
abstract = {We present a concept, prototype and in-the-wild evaluation of a mobile augmented reality (AR) application in which physical items from nature are used as AR markers. By blending the physical and digital, AR technology has the potential to create an enhanced learning experience compared to paper-based solutions and conventional mobile applications. Our prototype, an application running on a tablet computer, uses natural markers such as leaves and pinecones in a game-like nature quiz. The system was evaluated using interviews with and observations of 6- to 12-year-old children (n=11) who played the game as well as focus group discussions with play club counsellors (n=4) and primary school teachers (n=7). Our salient findings suggest that the concept has sound potential in its mixture of physical activity and educational elements in an outdoor context. In particular, teachers found the use of natural objects to be an appealing approach and a factor contributing to the learning experience.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {41},
numpages = {13},
keywords = {mobile augmented reality, field studies, visual markers, education, children},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098558,
author = {Patern\`{o}, Fabio and Schiavone, Antonio Giovanni and Conti, Antonio},
title = {Customizable Automatic Detection of Bad Usability Smells in Mobile Accessed Web Applications},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098558},
doi = {10.1145/3098279.3098558},
abstract = {Remote usability evaluation enables the possibility of analysing users' behaviour in their daily settings. We present a method and an associated tool able to identify potential usability issues through the analysis of client-side logs of mobile Web interactions. Such log analysis is based on the identification of specific usability smells. We describe an example set of bad usability smells, and how they are detected. The tool also allows evaluators to add new usability smells not included in the original set. We also report on the tool use in analysing the usability of a real, widely used application accessed by forty people through their smartphones whenever and wherever they wanted.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {42},
numpages = {11},
keywords = {remote usability evaluation, usability bad smells, web mobile application log analysis},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098539,
author = {Cowan, Benjamin R. and Pantidi, Nadia and Coyle, David and Morrissey, Kellie and Clarke, Peter and Al-Shehri, Sara and Earley, David and Bandeira, Natasha},
title = {"What Can i Help You with?": Infrequent Users' Experiences of Intelligent Personal Assistants},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098539},
doi = {10.1145/3098279.3098539},
abstract = {Intelligent Personal Assistants (IPAs) are widely available on devices such as smartphones. However, most people do not use them regularly. Previous research has studied the experiences of frequent IPA users. Using qualitative methods we explore the experience of infrequent users: people who have tried IPAs, but choose not to use them regularly. Unsurprisingly infrequent users share some of the experiences of frequent users, e.g. frustration at limitations on fully hands-free interaction. Significant points of contrast and previously unidentified concerns also emerge. Cultural norms and social embarrassment take on added significance for infrequent users. Humanness of IPAs sparked comparisons with human assistants, juxtaposing their limitations. Most importantly, significant concerns emerged around privacy, monetization, data permanency and transparency. Drawing on these findings we discuss key challenges, including: designing for interruptability; reconsideration of the human metaphor; issues of trust and data ownership. Addressing these challenges may lead to more widespread IPA use.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {43},
numpages = {12},
keywords = {user experience, speech interfaces, trust, intelligent personal assistants, privacy},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098534,
author = {Robinson, Simon and Pearson, Jennifer and Jones, Matt and Joshi, Anirudha and Ahire, Shashank},
title = {Better Together: Disaggregating Mobile Services for Emergent Users},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098534},
doi = {10.1145/3098279.3098534},
abstract = {Mainstream mobile interactions are focused around individual devices, with any collaboration happening via `the cloud'. We carried out design workshops with emergent users, revealing opportunities for novel collocated collaborative interactions. In this paper we present Better Together - a framework for disaggregating services, splitting interaction elements over separate mobiles. This distribution supports both sharing of resources (such as screen real-estate, or mobile data); and, scaffolding of inclusive interaction in mixed groups (e.g., in terms of literacy or prior technology exposure). We developed two prototypes to explore the concept, trialling the first---collocated group-based shopping list making---with emergent users in South Africa and India. We deployed the second probe, which splits YouTube into its constituent parts across separate mobiles, in a longitudinal study with users in Kenya, South Africa and India. We describe the concept and design process, and report on the design's suitability for emergent users based on our results.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {44},
numpages = {13},
keywords = {emergent users, distributed interaction, device sharing, disaggregation, mobile phones},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3098550,
author = {Hundlani, Kalpana and Chiasson, Sonia and Hamid, Larry},
title = {No Passwords Needed: The Iterative Design of a Parent-Child Authentication Mechanism},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098550},
doi = {10.1145/3098279.3098550},
abstract = {Even though the vast majority of children are online, our exploration of the user authentication literature and available tools revealed few alternatives specifically for authenticating children. We create an authentication mechanism that reduces the password burden for children and adds customizable parental oversight to increase security. With Bluink, our industry partner, we iteratively designed and user tested three parent-child prototypes, with each iteration addressing issues raised in the previous iteration. Our final design is a parent-child authentication mechanism based on OpenID and FIDO U2F which allows children to log in to websites without requiring a password and enables parents using their mobile device to remotely determine whether a login request should be granted.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {45},
numpages = {11},
keywords = {authentication, parents, children, user study, mobile device},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3125434,
author = {Dinic, Radomir and Domhardt, Michael and Ginzinger, Simon and St\"{u}tz, Thomas},
title = {EatAR Tango: Portion Estimation on Mobile Devices with a Depth Sensor},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3125434},
doi = {10.1145/3098279.3125434},
abstract = {The accurate assessment of nutrition information is a challenging task, but crucial for people with certain diseases, such as diabetes. An important part of the assessment of nutrition information is portion estimation, i.e. volume estimation. Given the volume and the food type, the nutrition information can be computed on the basis of the food type specific nutrition density. Recently mobile devices with depth sensors have been made available for the public (Google's project tango platform). In this work, an app for mobile devices with a depth sensor is presented which assists users in portion estimation. Furthermore, we present the design of a user study for the app and preliminary results.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {46},
numpages = {7},
keywords = {augmented reality, portion estimation, mobile},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3125435,
author = {Marichal, Sebasti\'{a}n and Rosales, Andrea and Sansone, Gustavo and Pires, Ana Cristina and Bakala, Ewelina and Perilli, Fernando Gonzalez and Blat, Josep},
title = {CETA: Open, Affordable and Portable Mixed-Reality Environment for Low-Cost Tablets},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3125435},
doi = {10.1145/3098279.3125435},
abstract = {Mixed-reality environments allow to combine tangible interaction with digital feedback, empowering interaction designers to take benefits from both real and virtual worlds. This interaction paradigm is also being applied in classrooms for learning purposes. However, most of the times the devices supporting mixed-reality interaction are neither portable nor affordable, which could be a limitation in the learning context. In this paper we propose CETA, a mixed-reality environment using low-cost Android tablets which tackles portability and costs issues. In addition, CETA is open-source, reproducible and extensible.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {47},
numpages = {7},
keywords = {tangible interaction, open-hardware, open-source, mixed-reality},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3125436,
author = {Di Geronimo, Linda and Bertarini, Marica and Badertscher, Julia and Husmann, Maria and Norrie, Moira C.},
title = {MyoShare: Sharing Data among Devices via Mid-Air Gestures},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3125436},
doi = {10.1145/3098279.3125436},
abstract = {With the growth of mobile devices, the necessity for a more ubiquitous network among the smartphones, tablets and computers of users increases. Sending and receiving content among personal devices should be an intuitive rather than a distracting task to be performed. With MyoShare, we propose a system that exploits the use of mid-air gestures, captured via the Myo wearable armband, to share web data among devices without requiring the user to open additional windows or copy-paste content into emails or chat applications. Users can select content from any web page and send it to another device by simply waving their hands. During the demo session, participants of the conference will be able to try out and see our system in action.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {48},
numpages = {3},
keywords = {mid-air gestures, cross-device, wearable},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3125437,
author = {Tai, Katsunori and Kono, Yasuyuki},
title = {Walking Motion Recognition System by Estimating Position and Pose of Leg Mounted Camera Device Using Visual SLAM},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3125437},
doi = {10.1145/3098279.3125437},
abstract = {This paper presents the method for recognizing the walking motion of the user with estimating position and pose of leg mounted camera devices by employing Visual SLAM. Approximate displacement of the position and orientation of the sensor terminal can be estimated using output values of the acceleration sensor, the gyro sensor, and the geomagnetic sensor. It is also considered that the position and orientation of the terminal can be estimated by the image captured by the camera mounted on each terminal. The system records surrounding scenery of the user with each camera device attached to each part of the user's leg, and estimates the 3D position and orientation of each camera device by using Visual SLAM. Finally, we develop the walking training support system that does not require a special environment.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {49},
numpages = {6},
keywords = {sensor, rehabilitation, camera, localization, SLAM, Android},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3125438,
author = {Axtell, Benett and Munteanu, Cosmin},
title = {Using Frame of Mind: Documenting Reminiscence through Unstructured Digital Picture Interaction},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3125438},
doi = {10.1145/3098279.3125438},
abstract = {Mobile technologies have made family photo collections extremely portable. People can now carry all their pictures with them wherever they go and show them to others in any setting with smartphones or tablets. However, current options for portable photo viewing are not intended for in-person sharing and reminiscence. Frame of Mind presents a new way to interact with digital pictures on a touch screen that encourages storytelling through its free-flowing interaction using the metaphor of looking at pictures on a table top. This allows family reminiscence to be lightweight, portable, and more accessible by supporting photo viewing on tablets that can have access to complete picture collections. So Frame of Mind moves towards digital tools that support our current photo viewing and sharing activities.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {50},
numpages = {4},
keywords = {older adults, reminiscence, user experience design, contextual inquiry, digital storytelling},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3125439,
author = {Gruenefeld, Uwe and Stratmann, Tim Claudius and Heuten, Wilko and Boll, Susanne},
title = {PeriMR: A Prototyping Tool for Head-Mounted Peripheral Light Displays in Mixed Reality},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3125439},
doi = {10.1145/3098279.3125439},
abstract = {Nowadays, Mixed and Virtual Reality devices suffer from a field of view that is too small compared to human visual perception. Although a larger field of view is useful (e.g., conveying peripheral information or improving situation awareness), technical limitations prevent the extension of the field-of-view. A way to overcome these limitations is to extend the field-of-view with peripheral light displays. However, there are no tools to support the design of peripheral light displays for Mixed or Virtual Reality devices. Therefore, we present our prototyping tool PeriMR that allows researchers to develop new peripheral head-mounted light displays for Mixed and Virtual Reality.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {51},
numpages = {6},
keywords = {augmented reality, head-mounted, prototyping, virtual reality, mixed reality, peripheral light displays},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3125440,
author = {Granell, Emilio and Leiva, Luis A.},
title = {βTap: Back-of-Device Tap Input with Built-in Sensors},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3125440},
doi = {10.1145/3098279.3125440},
abstract = {We present βTap, a Back-of-device (BoD) tap detection software for mobile devices that uses commodity sensors, without the need to instrument the device. Although just basic interactions are supported (namely, single and double taps), βTap is highly accurate and performance-friendly, since it uses a low-cost yet highly discriminative set of features. Our software is publicly available at the Google Play Store, so that others can build upon our work.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {52},
numpages = {6},
keywords = {sensors, tap-based input, software, feature selection, machine learning, BoD interaction},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3125441,
author = {Jost, Jana and Kirks, Thomas and Maettig, Benedikt},
title = {Study on Manual Palletization of Inhomogeneous Boxes with the Help of Different Interfaces to Assess Specific Factors of Ergonomic Impact},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3125441},
doi = {10.1145/3098279.3125441},
abstract = {The pursued project concerns the general topic of optimizing the packaging of pallets. To be more precise, it analyzes the execution of the work task of palletizing from the point of view of the user. Thus, the project seeks to provide a guidance for an ergonomic task execution for the worker as well as to ensure an economical pallet-packaging pattern concerning the work task of packing a pallet.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {53},
numpages = {6},
keywords = {head-mounted displays, augmented reality, mobile 3D, natural interaction},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3125442,
author = {Zeng, Limin and Weber, Gerhard and Simros, Markus and Conradie, Peter and Saldien, Jelle and Ravyse, Ilse and van Erp, Jan and Mioch, Tina},
title = {Range-IT: Detection and Multimodal Presentation of Indoor Objects for Visually Impaired People},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3125442},
doi = {10.1145/3098279.3125442},
abstract = {In the paper we present our Range-IT prototype, which is a 3D depth camera based electronic travel aid (ETA) to assist visually impaired people in finding out detailed information of surrounding objects. In addition to detecting indoor obstacles and identifying several objects of interest (e.g., walls, open doors and stairs) up to 7 meters, the Range-IT system employs a multimodal audio-vibrotactile user interface to present this spatial information.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {54},
numpages = {6},
keywords = {cognitive maps, visual impairments, vibrotactile, multimodal, spatial information, electronic travel aids},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3125443,
author = {Almeida, Alexandre and Alves, Ana},
title = {Activity Recognition for Movement-Based Interaction in Mobile Games},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3125443},
doi = {10.1145/3098279.3125443},
abstract = {Although smartphones include a set of sensors that enable innovative interactions, current mobile game interaction is mostly touch-based. Some games also include tilt movement based on the accelerometer sensor. However, sensors like accelerometers and gyroscopes can be used to recognize, in real time, full body motions. Exploring this can lead to innovative and immersive experiences while promoting physical activity. We present a proof of concept 3D endless running game called ActivRunner which implements an activity recognition system that predicts, in real time, 4 activities: standing, move left, move right, squat and jump. The goal is to replace the traditional touch interaction with a more natural movement-based one, showing the potential of this kind of interaction to create innovative and immersive mobile experiences while promoting physical activity.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {55},
numpages = {8},
keywords = {movement-based interaction, gyroscope, smartphone activity recognition, accelerometer, mobile exergames, machine learning, endless running mobile games},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3125444,
author = {Wiehr, Frederik and Kosmalla, Felix and Daiber, Florian and Kr\"{u}ger, Antonio},
title = {FootStriker: An EMS-Based Assistance System for Real-Time Running Style Correction},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3125444},
doi = {10.1145/3098279.3125444},
abstract = {Today, ambitioned amateur athletes often do not have access to professional coaching but still invest great effort in becoming faster runners. Apart from a pure increase in the quantitative training load, a change of the running technique, e.g. transitioning from heel striking to fore- or midfoot running, can be highly effective and usually prevents knee-related injuries.With this demo, we highlight factors to consider when determining EMS actuation phases for real-time running style correction in an outdoor scenario. During actuation the wearable applies electrical muscle stimulation (EMS) in the flight phase of a stride after having detected a heel-strike with force sensing resistors (FSR) in a sensor insole. To complement the original FootStriker lab prototype, we address the applicability in the field of the aforementioned real-time running style correction system.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {56},
numpages = {6},
keywords = {wearable, real-time feedback, real-time assistance, motor learning, sports training, online feedback, in-situ feedback, running, motor skills, electrical muscle stimulation},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119840,
author = {Blazevski, Boban and Haslwanter, Jean D. Hallewell},
title = {User-Centered Development of a System to Support Assembly Line Worker},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119840},
doi = {10.1145/3098279.3119840},
abstract = {This industrial perspective contribution describes the development of a prototype of a mobile worker assistance system to be used in the assembly line for motors that is changing to single piece flow. A large variety of user-centered methods were chosen to cover all phases of the development. Users were included in the analysis and evaluation phases. The results show that the methods are suitable for this type of development. Extended tests with users indicate that the prototype was very acceptable, especially for people with less experience on the assembly line. Since the system was tested in a real environment for five days with a number of users, the company now feels confident investing in the development of the system.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {57},
numpages = {7},
keywords = {worker assistance system, industrie 4.0, user-centered design, single piece flow},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119841,
author = {Bhavnani, Yash and Rodden, Kerry and Guarnotta, Laura Cuozzo and Lynn, Margaret T. and Chizari, Sara and Granka, Laura},
title = {Understanding Mobile Phone Activities via Retrospective Review of Visualizations of Usage Data},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119841},
doi = {10.1145/3098279.3119841},
abstract = {It can be very challenging to get an accurate understanding of mobile phone usage because of the difficulty of observing phone activity in a natural setting. We describe a retrospective methodology where participants review visualizations of their logged activity in an interview setting, and our lessons learned in applying this methodology in a study of user goals and journeys on mobile devices across apps.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {58},
numpages = {10},
keywords = {visualization, ethno-mining, mobile phone usage, retrospective cued recall},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119911,
author = {Mirza, Iram and Tabak, Joshua},
title = {Designing for Delight},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119911},
doi = {10.1145/3098279.3119911},
abstract = {Delightful user experiences (UX) create memorable products and generate brand love. Mobile technology allows for UX theatric affordances such as animations, transitions and gestures that can craft macro and micro feature delight. Mobile experiences crafted by companies such as Google, Facebook, KakaoTalk and Uber integrate fun and surprises into their products in an attempt to develop an emotional connection with their users. This talk unpacks what constitutes UX delight, the psychology behind emotional design, how to create a culture that supports building not just usable and simple products but magical ones, and methods to measure the impact of delight on user success and brand perception.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {59},
numpages = {3},
keywords = {designing for emotion, user delight, memorable user experiences, brand perception},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119912,
author = {Sturm, Christian and Aly, Maha and von Schmidt, Birka and Flatten, Tessa},
title = {Entrepreneurial &amp; UX Mindsets: Two Perspectives - One Objective},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119912},
doi = {10.1145/3098279.3119912},
abstract = {This paper argues that integrating entrepreneurial thinking into the field of User Experience (UX) increases the efficacy of user-centered design approaches in industrial settings. The argument unfolds in three steps: (1) the element of "problem solving" is identified as being the core and the common denominator of both user- centered design and entrepreneurial thinking, (2) the connection between both approaches is shown by focusing on processes as well as mental models, and (3) it is explained how the extension of UX by entrepreneurial elements leads to an increased impact and area of influence in industrial settings.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {60},
numpages = {11},
keywords = {UX thinking, user entrepreneurs, mindset, business, problem solving, UX, multidisciplinary, entrepreneurial thinking, user-centered design},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119913,
author = {Ghosh, Sanjay},
title = {What Users Want in Their Mobile Phones? Localization for Low Socio-Economic Emerging Market},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119913},
doi = {10.1145/3098279.3119913},
abstract = {This paper describes our experience in customizing the software design of an existing smartphone product for a localized market. This was targeted as a low-cost smartphone for the masses in the rapidly emerging market of India, and the sub-continent region. The earlier commercial version of the product was already designed with enough user research and R&amp;D efforts for few other markets. The challenge was to improve the existing features to make the next product version more appealing to the users. Due to the pressing business demand for short release time, we adopted the 'Localization' strategy based on user-centered design process, wherein user research was conducted alongside the design activities. This paper shares our experiences and learnings from the challenges we encountered. We share several instances of how user stories were inferred, converted into localization concepts and finally refined into commercially viable solutions.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {61},
numpages = {10},
keywords = {socio-economic behavior, ICT in social development, interaction design for developing regions, product localization},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119915,
author = {Mayer, Sven and Le, Huy Viet and Henze, Niels},
title = {Machine Learning for Intelligent Mobile User Interfaces Using TensorFlow},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119915},
doi = {10.1145/3098279.3119915},
abstract = {One key feature of TensorFlow includes the possibility to compile the trained model to run efficiently on mobile phones. This enables a wide range of opportunities for researchers and developers. In this tutorial, we teach attendees two basic steps to run neural networks on a mobile phone: Firstly, we will teach how to develop neural network architectures and train them in TensorFlow. Secondly, we show the process to run the trained models on a mobile phone.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {62},
numpages = {5},
keywords = {machine learning, regression, mobile device, tensorflow, supervised learning, classification},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119916,
author = {Pfleging, Bastian and Kun, Andrew L. and Broy, Nora},
title = {The Car as an Environment for Mobile Devices},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119916},
doi = {10.1145/3098279.3119916},
abstract = {The objective of this tutorial is to provide MobileHCI newcomers to the domain of automotive user interfaces (AutomotiveUI) with an introduction and overview of the field. The tutorial will introduce the specifics and challenges of in-vehicle user interfaces that set this field apart from others. With a clear focus on the integration of mobile devices into the car, we will provide an overview of the specific requirements of AutomotiveUI, discuss the design of such interfaces, also with regard to standards and guidelines. We further outline how to evaluate interfaces in the car, discuss the challenges with upcoming automated driving and present trends and challenges in this domain.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {63},
numpages = {5},
keywords = {manual and automated driving, mobile user interfaces, automotive user interfaces, in-car user interfaces, mobile device integration, car driving},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119917,
author = {Su, Denise and Torkildson, Megan K. and Sales, Heidi},
title = {Speed Dating, Love Letters, and Couples Interviews: How to Get the Spark Back in User Research Methods},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119917},
doi = {10.1145/3098279.3119917},
abstract = {Many researchers find themselves in a methodological rut and end up using the same tried-and-true user research methods, such as usability studies or interviews. Though these methods have their merits, there are times when asking questions directly may not suffice or researchers simply have trouble getting to the insights that are needed. This two-hour tutorial will focus on teaching creative methods that can spark new conversation or illuminate different insights. We will focus on three methods: speed-dating, love letters, and couple interviews. These methods are particularly effective for researchers and practitioners who study personal topics such as communication messaging apps and websites. The tutorial will provide a useful toolkit of creative methods and best practices.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {64},
numpages = {5},
keywords = {design thinking, creative methods, user experience research, speed dating, love letter, user experience, co-design},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119918,
author = {Daiber, Florian and Kosmalla, Felix},
title = {Tutorial on Wearable Computing in Sports},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119918},
doi = {10.1145/3098279.3119918},
abstract = {Wearable sports technology such as fitness trackers, smart watches or heart rate sensors has become ubiquitous in our everyday lives. This technology enables even recreational athletes to keep track of their workouts in a comprehensive manner. Besides the general assumption that this technology improves motivation to exercise more often, it also enables the athlete to get a better understanding of her current fitness level. However, current technology is mainly focussed on (quantifiable) performance indicators such as mileage, pace, cadence, watts, heart rate, etc.In this tutorial we aim to introduce wearable sports technologies that provide real-time support to athletes while exercising. Topics of interest range from engineering problems to research methods as they apply in the context of mobile and ubiquitous sports technologies.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {65},
numpages = {4},
keywords = {real-time assistance, motor skills, wearables, wearable devices, motor learning, sports tracking, in-situ feedback, real-time feedback},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119919,
author = {Munteanu, Cosmin and Penn, Gerald},
title = {Speech and Hands-Free Interaction: Myths, Challenges, and Opportunities},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119919},
doi = {10.1145/3098279.3119919},
abstract = {HCI research has for long been dedicated to better and more naturally facilitating information transfer between humans and machines. Unfortunately, humans' most natural form of communication, speech, is also one of the most difficult modalities to be understood by machines - despite, and perhaps, because it is the highest-bandwidth communication channel we possess. While significant research efforts, from engineering, to linguistic, and to cognitive sciences, have been spent on improving machines' ability to understand speech, the MobileHCI community (and the HCI field at large) has been relatively timid in embracing this modality as a central focus of research. This can be attributed in part to the unexpected variations in error rates when processing speech, in contrast with often-unfounded claims of success from industry, but also to the intrinsic difficulty of designing and especially evaluating speech and natural language interfaces. As such, the development of interactive speech-based systems is mostly driven by engineering efforts to improve such systems with respect to largely arbitrary performance metrics. Such developments have often been void of any user-centered design principles or consideration for usability or usefulness.The goal of this course is to inform the MobileHCI community of the current state of speech and natural language research, to dispel some of the myths surrounding speech-based interaction, as well as to provide an opportunity for researchers and practitioners to learn more about how speech recognition and speech synthesis work, what are their limitations, and how they could be used to enhance current interaction paradigms. Through this, we hope that HCI researchers and practitioners will learn how to combine recent advances in speech processing with user-centred principles in designing more usable and useful speech-based interactive systems.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {66},
numpages = {4},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119920,
author = {Duente, Tim and Schneegass, Stefan and Pfeiffer, Max},
title = {EMS in HCI: Challenges and Opportunities in Actuating Human Bodies},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119920},
doi = {10.1145/3098279.3119920},
abstract = {Electrical Muscle Stimulation (EMS) recently received considerable attention in the HCI community. By applying small signals to the user's body, different types of movement can be generated. These movements allow designers to create more meaningful and embodied haptic feedback compared to vibrotactile feedback. This advantage also comes with further technical and practical challenges which need to be tackled. These challenges include a fine grained calibration procedure and a close contact to the user's body at specific on-body locations. This tutorial gives an overview about current research projects, challenges, and opportunities to use EMS for providing rich embodied feedback followed by a hands on experience. The main goal of this tutorial is that participants get a basic understanding of how EMS works and how systems that are using EMS can be developed and evaluated.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {67},
numpages = {4},
keywords = {haptic feedback, electrical muscle stimulation, tutorial},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119921,
author = {Weber, Dominik},
title = {Towards Smart Notification Management in Multi-Device Environments},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119921},
doi = {10.1145/3098279.3119921},
abstract = {Notifications are a core feature of current smart devices and are used to proactively communicate with users. However, notifications are known to cause disruptive effects. With the increasing number of smart devices, these negative effects are multiplying. With mobile devices, like smartphones and smartwatches, being always connected and always with the users, it is necessary to find a balance between notifying users while respecting their attention. In this work, we provide an overview of our research on smart notification management in multi-device environments. We present our research questions, research conducted so far, and planned efforts. The expected outcomes are guidelines and models to manage notifications in multi-device environments.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {68},
numpages = {2},
keywords = {multi-device, interruptions, notifications, mobile devices},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119922,
author = {Svangren, Michael Kvist},
title = {Understanding and Designing for Emerging Digital Eco-Systems: The Cases of Private and Shared Cars},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119922},
doi = {10.1145/3098279.3119922},
abstract = {The amount of interactive digital technology in cars is increasing rapidly. The result is a new platform that holds the potential to facilitate many different systems, interactions, and users both in and outside the car. Many systems and services from car manufacturers already exist, and several is suggested in mobile HCI research. However, we still lack research that investigates how these digital artifacts interconnect and are used. Adding an alternative view to the existing HCI research, I investigate how private and shared cars have become digital artifacts in digital eco-systems, and how we can use this knowledge to design future mobile systems and services.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {69},
numpages = {4},
keywords = {digital eco-systems, digital artifacts, cars},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119923,
author = {Steer, Cameron},
title = {Designing Mobile Deformable Controls for Creation of Digital Art},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119923},
doi = {10.1145/3098279.3119923},
abstract = {My research explores the design and development of deformable controls and shape displays for mobile devices. My significant work so far has been set in the context of tools for creating digital art. I have been studying how we might design and develop deformable interfaces that support interaction with digital art applications. Along with how to bring elements of realistic painting to the digital experience through physical mobile controls. Part of this is designing and prototyping physical hardware interfaces and then evaluating them using HCI research methods in user studies and interviews. This extended abstract outlines my research aims, progress so far, and my future work and direction.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {70},
numpages = {4},
keywords = {shape-changing, deformable devices, tangibility, digital painting},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119924,
author = {B\^{a}ce, Mihai},
title = {Augmenting Human Interaction Capabilities with Proximity, Natural Gestures, and Eye Gaze},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119924},
doi = {10.1145/3098279.3119924},
abstract = {Nowadays, humans are surrounded by many complex computer systems. When people interact among each other, they use multiple modalities including voice, body posture, hand gestures, facial expressions, or eye gaze. Currently, computers can only understand a small subset of these modalities, but such cues can be captured by an increasing number of wearable devices. This research aims to improve traditional human-human and human-machine interaction by augmenting humans with wearable technology and developing novel user interfaces.More specifically, (i) we investigate and develop systems that enable a group of people in close proximity to interact using in-air hand gestures and facilitate effortless information sharing. Additionally, we focus on (ii) eye gaze which can further enrich the interaction between humans and cyber-physical systems.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {71},
numpages = {3},
keywords = {HCI, deep learning, collaboration, wearable technology, eye gaze, proximity detection, CPS, augmented human},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119925,
author = {Duval, Jared},
title = {A Mobile Game System for Improving the Speech Therapy Experience},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119925},
doi = {10.1145/3098279.3119925},
abstract = {A lack of intrinsic motivation to practice speech is attributed to tedious and repetitive speech curriculums, but mobile games have been widely recognized as a valid motivator for jaded individuals. SpokeIt is an interactive storybook style speech therapy game that intends to turn practicing speech into a motivating and productive experience for individuals with speech impairments as well as provide speech therapists an important diagnostic tool. In this paper, I discuss the novel intellectual contributions SpokeIt can provide such as an offline critical conversational speech recognition system, and the application of therapy curriculums to mobile platforms, I present conducted research, and consider exciting future work and research directions.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {72},
numpages = {3},
keywords = {mobile applications, speech processing, speech therapy, human computer interaction, games},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119926,
author = {Smith, Sean-Ryan},
title = {Mobile Context-Aware Cognitive Testing System},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119926},
doi = {10.1145/3098279.3119926},
abstract = {Traditional cognitive testing for older adults can be inaccessible, expensive, and time consuming. The development of computerized cognitive tests (CCTs) has made strides to alleviate such issues with traditional cognitive testing. Self-administered CCTs allow for individuals to test rapidly and conveniently on various devices. However, such tests may not factor in relevant contextual information pertinent to the testing situation (e.g., is the user in a proper environment or context to test?). This dissertation aims to develop a mobile, context-aware cognitive testing system (CACTS) capable of tracking and analyzing contextual information during CCTs. By utilizing mobile device sensors and user input, the proposed context-aware system will capture ambient and behavioral data during testing to compliment user performance results. This research will help provide insight into the contextual factors that are relevant to the user's testing efficacy and performance in CCTs.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {73},
numpages = {4},
keywords = {assistive technology, older persons, context-aware, mobile sensors, cognitive testing},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119927,
author = {Karyda, Maria},
title = {Crafting Collocated Interactions: Exploring Physical Representations of Personal Data},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119927},
doi = {10.1145/3098279.3119927},
abstract = {This PhD project explores a third wave of research on Mobile Collocated Interactions, which focuses on craft. Strongly inspired by the field of Data Physicalization it aims to explore how would people physically share (physiological) personal data in collocated activities. In achieving that it investigates potential relationships between personal data and meaningful personal objects for individuals. Future steps involve prototyping towards crafting collocated interactions with personal data.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {74},
numpages = {4},
keywords = {data physicalization, collocated interactions, user experience design, data-gifts, personal data},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119839,
author = {Yeo, Hui-Shyong and Laput, Gierad and Gillian, Nicholas and Quigley, Aaron},
title = {Workshop on Object Recognition for Input and Mobile Interaction},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119839},
doi = {10.1145/3098279.3119839},
abstract = {Today we can see an increasing number of object recognition systems of very different sizes, portability, embedability and form factors which are starting to become part of the ubiquitous, tangible, mobile and wearable computing ecosystems that we might make use of in our daily lives. These systems rely on a variety of technologies including computer vision, radar, acoustic sensing, tagging and smart objects.Such systems open up a wide-range of new forms of touchless and mobile interaction. With systems deployed in mobile products then using everyday objects that can be found in the office or home, we can realise new applications and novel types of interaction. Object based interactions might revolutionise how people interact with a computer. System could be used in conjunction with a mobile phone, for example it could be trained to open a recipe app when you hold a phone to your stomach, or change its settings when operating with a gloved hand.Although the last few years have seen an increasing amount of research in this area, knowledge about this subject remains under explored, fragmented, and cuts across a set of related but heterogeneous issues. This workshop brings together researchers and practitioners interested in the challenges posed by Object Recognition for Input and Mobile Interaction.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {75},
numpages = {5},
keywords = {radar, context-aware interaction, object recognition, material classification, ubiquitous computing, interaction techniques and mobile computing},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119838,
author = {Jenson, Scott},
title = {The UX of IoT: Unpacking the Internet of Things},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119838},
doi = {10.1145/3098279.3119838},
abstract = {When discussing the Internet of Things (IoT), product concepts usually involve overly complex systems with baroque-like setup and confusing behaviors. This workshop will step a bit back from the hype and create a richer, more nuanced way of talking about the IoT.The workshop will start with a structure to the UX of IoT, creating a UX taxonomy and then challenge participants to "think small". Special focus will be put on the Physical Web, a lightweight technology that lets any place or device wirelessly broadcast a URL, unlocking very simple and lightweight interactions. Participants will be provoked to think: how can we reduce an IoT concept to the bare minimum? Can we focus on user needs and not be carried away by the technology to create something lightweight and simple?Workshop participants are expected to come prepared with one or two IoT scenarios they would like to work on; then, through a series of exercises, refine one of these down into a much simpler, user-focused design.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {76},
numpages = {2},
keywords = {physical web, internet of things, UX},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119836,
author = {H\"{a}kkil\"{a}, Jonna and Colley, Ashley and Cheverst, Keith and Robinson, Simon and Sch\"{o}ning, Johannes and Bidwell, Nicola J. and Kosmalla, Felix},
title = {NatureCHI 2017: The 2nd Workshop on Unobtrusive User Experiences with Technology in Nature},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119836},
doi = {10.1145/3098279.3119836},
abstract = {Being in nature is typically regarded to be calming, relaxing and purifying. When in nature, people often seek to be mobile through physical activity such as hiking. But also, nature provides an opportunity for meditative, mindful or inspiring experiences remote from urban everyday life. Mobile Technologies such as sports tracking technologies, electronic tourist guides, mobile phone integrated cameras and omnipresent social media access, have potential to both enhance and disrupt a user's interaction with and experience of nature. This MobileHCI workshop follows on from the first successful NatureCHI workshop by focusing on the challenges associated with the design of mobile technologies that support unobtrusive interaction in nature.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {77},
numpages = {4},
keywords = {nature, outdoors, unobtrusive design, mobile computing},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3119837,
author = {Meschtscherjakov, Alexander and Tscheligi, Manfred and Fr\"{o}hlich, Peter and McCall, Rod and Riener, Andreas and Palanque, Philippe},
title = {Mobile Interaction with and in Autonomous Vehicles},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3119837},
doi = {10.1145/3098279.3119837},
abstract = {The rise of autonomous and fully autonomous vehicles requires a closer examination of how people will interact with them. In this workshop, we especially address two areas: (1) What can we learn from mobile HCI for the interaction design between human drivers and the autonomous vehicle? (2) What opportunities for mobile HCI arise due to new freedoms for drivers, when they also take on the role of passengers in autonomous vehicles? The workshop will seek to further develop the challenges involved and/or to suggest early solutions through the use of a highly participative format.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {78},
numpages = {6},
keywords = {autonomous vehicles, mobile interaction, trust, experience},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122122,
author = {Braun, Michael and Broy, Nora and Pfleging, Bastian and Alt, Florian},
title = {A Design Space for Conversational In-Vehicle Information Systems},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122122},
doi = {10.1145/3098279.3122122},
abstract = {In this paper we chart a design space for conversational in-vehicle information systems (IVIS). Our work is motivated by the proliferation of speech interfaces in our everyday life, which have already found their way into consumer electronics and will most likely become pervasive in future cars.Our design space is based on expert interviews as well as a comprehensive literature review. We present five core dimensions - assistant, position, dialog design, system capabilities, and driver state - and show in an initial study how these dimensions affect the design of a prototypical IVIS.Design spaces have paved the way for much of the work done in HCI including but not limited to areas such as input and pointing devices, smart phones, displays, and automotive UIs. In a similar way, we expect our design space to aid practitioners in designing future IVIS but also researchers as they explore this young area of research.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {79},
numpages = {8},
keywords = {speech interaction, natural language interfaces, automotive user interfaces, design space},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122123,
author = {Koelle, Marion and Heuten, Wilko and Boll, Susanne},
title = {Are You Hiding It? Usage Habits of Lifelogging Camera Wearers},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122123},
doi = {10.1145/3098279.3122123},
abstract = {Though still a rare sight, body-worn lifelogging cameras such as Mofily's YoCam or the Narrative Clip have become increasingly popular amongst tech-savvy audiences. In this paper, we investigate whether users of those devices prefer to wear them openly or in a concealed, less obtrusive manner. We discuss the camouflage of lifelogging cameras based on results from an online study (N=117), including the why (not) and how as well as qualitative insights on how social contexts influence usage habits. The results of our study provide empirical evidence that deliberate concealment can be perceived unethical, and that moderate noticeability is favoured. We furthermore found contrary effects of lifelogging cameras in interpersonal relationships, including self-censorship by the user, avoidance behaviour by her/his peers and conversation starting character of the device itself. We conclude by highlighting design challenges concerning ubiquitous, body-worn cameras.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {80},
numpages = {8},
keywords = {device usage, social aspects, lifelogging},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122124,
author = {Gruenefeld, Uwe and Ali, Abdallah El and Heuten, Wilko and Boll, Susanne},
title = {Visualizing Out-of-View Objects in Head-Mounted Augmented Reality},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122124},
doi = {10.1145/3098279.3122124},
abstract = {Various off-screen visualization techniques that point to off-screen objects have been developed for small screen devices. A similar problem arises with head-mounted Augmented Reality (AR) with respect to the human field-of-view, where objects may be out of view. Being able to detect so-called out-of-view objects is useful for certain scenarios (e.g., situation monitoring during ship docking). To augment existing AR with this capability, we adapted and tested well-known 2D off-screen object visualization techniques (Arrow, Halo, Wedge) for head-mounted AR. We found that Halo resulted in the lowest error for direction estimation while Wedge was subjectively perceived as best. We discuss future directions of how to best visualize out-of-view objects in head-mounted AR.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {81},
numpages = {7},
keywords = {head-mounted, peripheral awareness, visualization techniques, augmented reality, off-screen, out-of-view objects},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122125,
author = {Mayer, Sven and Mayer, Michael and Henze, Niels},
title = {Feasibility Analysis of Detecting the Finger Orientation with Depth Cameras},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122125},
doi = {10.1145/3098279.3122125},
abstract = {Over the last decade, a body of research investigated enriching touch actions by using finger orientation as an additional input. Beyond new interaction techniques, we envision new user interface elements to make use of the additional input information. We define the fingers orientation by the pitch, roll, and yaw on the touch surface. Determining the finger orientation is not possible using current state-of-the-art devices. As a first step, we built a system that can determine the finger orientation. We developed a working prototype with a depth camera mounted on a tablet. We conducted a study with 12 participants to record ground truth data for the index, middle, ring and little finger to evaluate the accuracy of our prototype using the PointPose [3] algorithm to estimate the pitch and yaw of the finger. By applying 2D linear correction models, we further show a reduction of RMSE by 45.4% for pitch and 21.83% for yaw.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {82},
numpages = {8},
keywords = {pitch, yaw, depth camera, mobile devices, modeling, touch, finger orientation},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122126,
author = {Montuwy, Ang\'{e}lique and Cahour, B\'{e}atrice and Dommes, Aur\'{e}lie},
title = {Visual, Auditory and Haptic Navigation Feedbacks among Older Pedestrians},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122126},
doi = {10.1145/3098279.3122126},
abstract = {As worldwide population is aging and concentrating in cities, designing a pedestrian navigation aid adapted to elderly specificities and needs could help in preserving their autonomy, as their cognitive and perceptive abilities may decline with age. This study was aimed at comparing visual, auditory and haptic guidance feedbacks among older pedestrians, taking into account their navigation performance and user experience. A virtual environment was used to measure time to destination and rate of correct responses. Post-activity interviews were used to access participants' perceptions, feelings and hesitations in a qualitative way. Results showed great performance and user experience for visual guidance, whereas performance and user experience were poor for haptic guidance.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {83},
numpages = {8},
keywords = {virtual environment, sensory modalities, user experience, pedestrians, aging, navigation},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122127,
author = {Blumenstein, Kerstin and Niederer, Christina and Wagner, Markus and Pfersmann, Wilhelm and Seidl, Markus and Aigner, Wolfgang},
title = {Visualizing Spatial and Time-Oriented Data in a Second Screen Application},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122127},
doi = {10.1145/3098279.3122127},
abstract = {Mobile devices are more and more used in parallel, esp. in the field of TV viewing as second screen devices. Such scenarios aim to enhance the viewers' user experience while watching TV. We designed and implemented a second screen prototype intended to be used in parallel to watching a TV documentary. It allows to interactively explore a combination of spatial and time-oriented data to extend and enrich the TV content. We evaluated our proto-type in a twofold approach, consisting of expert reviews and user evaluation. We identified different interaction habits in a second screen scenario and present its benefits in relation to documentaries.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {84},
numpages = {8},
keywords = {information visualization, mobile, second screen, TV},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122128,
author = {Lingamaneni, Ragavendra and Kubitza, Thomas and Scheible, J\"{u}rgen},
title = {DroneCAST: Towards a Programming Toolkit for Airborne Multimedia Display Applications},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122128},
doi = {10.1145/3098279.3122128},
abstract = {In recent years, new type of public displays have captured the interest of researchers-displays with the ability to move freely in three-dimensional space. We refer to such display systems as Airborne Multimedia Display (AMD) systems. In this paper, we provide a comprehensive analysis of requirements for developing interactive AMD applications based on extensive literature survey of the related work and from our own experience of flying AMD systems. We then outline the design and implementation of DroneCAST: a programming toolkit for developing AMD applications with remote delivery and control mechanism for multimedia content with IoT middleware as the core part of the system. Finally, we build a sample AMD application with the toolkit to further illustrate the applicability of the system.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {85},
numpages = {8},
keywords = {interaction, drones, toolkit, requirements analysis, airborne displays},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122129,
author = {Yang, Nan and van Hout, Gerbrand and Feijs, Loe and Chen, Wei and Hu, Jun},
title = {Eliciting Values through Wearable Expression in Weight Loss},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122129},
doi = {10.1145/3098279.3122129},
abstract = {This paper presents the work-in-progress prototype of i-Ribbon---a wearable device designed to elicit values in the context of weight loss. Starting with an Obesity Awareness Ribbon, we introduced the i-Ribbon concept. For prototyping, we built a system that could extract user's personal health-related data through a mobile application and sent it to a wearable device. The functional prototype enabled us to implement different interaction strategies to elicit corresponding values. Base on the reflection on design and prototyping process, possibilities of future research were identified.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {86},
numpages = {6},
keywords = {values elicitation, weigh loss, wearable, social interaction, values, social expression},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122130,
author = {Kriglstein, Simone and Brandm\"{u}ller, Mario and Pohl, Margit and Bauer, Christine},
title = {A Location-Based Educational Game for Understanding the Traveling Salesman Problem: A Case Study},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122130},
doi = {10.1145/3098279.3122130},
abstract = {Due to the high potential of digital media to support learning processes and outcomes, educational games have gained wide acceptance over the years. The combination of mobile devices with location-based technologies offers new options and possibilities for the development of educational games in consideration of learners' environment with the positive side effect to promote learner's physical activities. This paper introduces a mobile educational game for promoting a better understanding of concepts related to route problems and route optimization on the basis of real world examples in a playful manner. The game combines problem-solving tasks with a quiz to teach concepts related to the Traveling Salesman Problem (TSP) by using the Global Positioning System (GPS) technology.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {87},
numpages = {8},
keywords = {game-based learning, location-based game, traveling salesman problem, global positioning system},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122131,
author = {Wei\ss{}ker, Tim and Genc, Erdan and Berst, Andreas and Schreiber, Frederik David and Echtler, Florian},
title = {ShakeCast: Using Handshake Detection for Automated, Setup-Free Exchange of Contact Data},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122131},
doi = {10.1145/3098279.3122131},
abstract = {We present ShakeCast, a system for automatic peer-to-peer exchange of contact information between two persons who just shook hands. The accelerometer in a smartwatch is used to detect the physical handshake and implicitly triggers a setup-free information transfer between the users' personal smartphones using Bluetooth LE broadcasts. An abstract representation of the handshake motion data is used to disambiguate between multiple simultaneous transmissions and to prevent accidental data leakage.To evaluate our system, we collected individual wrist acceleration data from 130 handshakes, performed by varying combinations of 20 volunteers. We present a systematic analysis of possible data features which can be used for disambiguation, and we validate our approach using the most salient features. Our analysis shows an expected match rate between corresponding handshakes of 92.3%.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {88},
numpages = {8},
keywords = {data exchange, mobile, Bluetooth, contact data, peer-to-peer, handshake, P2P, smartwatch, smartphone, contacts, accelerometer},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122132,
author = {Kettner, Romina and Bader, Patrick and Kosch, Thomas and Schneegass, Stefan and Schmidt, Albrecht},
title = {Towards Pressure-Based Feedback for Non-Stressful Tactile Notifications},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122132},
doi = {10.1145/3098279.3122132},
abstract = {Smartphones, wearables, and other mobile devices often use tactile feedback for notifying users. This feedback type proved to be beneficial since it does not occupy the visual or auditory channel. However, it still can be distracting in other situations such as when users are already stressed. To investigate tactile feedback patterns which do not increase the user's stress level, we developed two wrist-worn prototypes capable of providing tactile feedback (i.e., vibrotactile and pressure-based feedback). Further, we conducted a user-study with 14 participants comparing both feedback types. The results suggest that vibrotactile feedback increases the user's stress level more, compared to pressure-based feedback particularly applied when the user currently has a low stress level. Consequently, we present implications for designing notifications for mobile and wearable devices.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {89},
numpages = {8},
keywords = {notifications, tactile feedback, mobile computing, affective computing},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122133,
author = {Stigberg, Susanne Koch},
title = {Simplifying the Making of Probes, Prototypes and Toolkits in Mobile Interaction Research Using Tasker},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122133},
doi = {10.1145/3098279.3122133},
abstract = {This paper presents a technique to support the making of mobile interaction interfaces for controlling the smartphone. We often use smartphones while moving, resulting in non-optimal or even unsafe interactions. Better mobile interactions need to be created with locomotion in mind and experienced in practice. But making and testing new interaction interfaces is time-consuming. It often involves the making of an input device; establishing a connection between device and smartphone; and implementing an application on the phone for testing interactions with the input device. This paper reports from three ongoing projects on how a commercial available automation tool called Tasker can be used for coupling phone functionalities to new input devices, eliminating the need for implementing a complete phone application, and enabling flexible, reusable, and easy making of interaction interfaces for smartphones.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {90},
numpages = {8},
keywords = {making, probe, toolkit, co-design, mobile interactions, prototype},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122134,
author = {G\"{u}ldenpfennig, Florian and Ganh\"{o}r, Roman and Fitzpatrick, Geraldine},
title = {How to Look at Two-Sided Photos? Exploring Novel Perspectives on Digital Images},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122134},
doi = {10.1145/3098279.3122134},
abstract = {In recent years, HCI has been concerned with supporting memory processes through digital mementos. Still, relatively little work has been done on the design of interactive and mobile image viewers to support "reconstructing" meaningful memories. To explore novel perspectives on digital images, we provided eight participants with five different viewing tools and invited them to look at both conventional photos and at "duographs" (duography is a recent and experimental type of "two-sided" photography for capturing inspiring and meaningful pictures with mobile phones). Drawing on interviews and surveys, we found that participants were divided into two user groups during the interviews in the lab: "pragmatics" and "romantics". While the first group appreciated features supporting efficiency, the latter was interested in playful and explorative affordances. In this paper, we present our study and discuss preliminary findings.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {91},
numpages = {8},
keywords = {duography, image viewers, digital photography},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122135,
author = {Spiel, Katta and Werner, Katharina and H\"{o}dl, Oliver and Ehrenstrasser, Lisa and Fitzpatrick, Geraldine},
title = {Creating Community Fountains by (Re-)Designing the Digital Layer of Way-Finding Pillars},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122135},
doi = {10.1145/3098279.3122135},
abstract = {Way-finding pillars for tourists aid them in navigating an unknown area. The pillars show nearby points of interest, offer information about public transport and provide a scale for the neighbourhood. Through a series of studies with tourists and locals, we establish their different needs. In this space, we developed Mappy, a mobile application which augments and enhances way-finding pillars with an added digital layer. Mappy opens up opportunities for reappropriation of, and engagement with, the pillars. Seeing the pillars beyond their initial use case by involving a diverse range of people let us develop the digital layer and subsequently overall meaning of way-finding pillars further: as "community fountains" they engage locals and tourists alike and can provoke encounters between them.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {92},
numpages = {8},
keywords = {smart city, digital layers, reappropriation, augmented reality, way-finding, urban spaces},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122136,
author = {Rigby, Jacob M. and Brumby, Duncan P. and Gould, Sandy J. J. and Cox, Anna L.},
title = {Film, Interrupted: Investigating How Mobile Device Notifications Affect Immersion during Movies},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122136},
doi = {10.1145/3098279.3122136},
abstract = {Mobile devices are increasingly used while watching video, both as a secondary device and for dedicated viewing. However, devices frequently issue notifications that can interrupt viewing. This study investigated the effect of interruptions from notifications on viewer immersion. Participants watched 10 minutes of a movie without notifications, and 10 minutes while receiving message notifications. There were two participant groups: one watched video on a 30-inch monitor with messages sent to a separate smartphone; while another watched on a smartphone while also receiving messages on it. Viewer immersion was assessed after each condition via questionnaire. We also considered message response times. Results showed that immersion scores were lower when the video was interrupted with notifications, regardless of viewing device. Message response times were fastest when using the phone as a secondary device. Our results suggest that device-driven interruptions should be minimised for an immersive film experience.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {93},
numpages = {8},
keywords = {dual screening, film, interruptions, media multitasking, task switching, television, notifications, video},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122137,
author = {Chopra, Simran and Chivukula, Shruthi},
title = {My Phone Assistant Should Know I Am an Indian: Influencing Factors for Adoption of Assistive Agents},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122137},
doi = {10.1145/3098279.3122137},
abstract = {Intelligent assistants are an emerging form of interface in digital devices. They are disrupting this space through conversations which have socio-cultural dimensions. However, what is not known are people's perceptions and expectations from these assistants. In this paper, we studied the Indian context through guided interviews and situational observations. The user-agent interaction begins with communication of intent. We found that Indians take a lot of time to articulate intent and also prefer conversing through type. Personalization and context are other two critical components, where latter gives deeper meaning to user intent and is culture specific. E.g. in India; security, shared public spaces and infrastructure availability are of higher importance as compared to the west. The need for user-profile building is identified as a key factor in adoption of assistive agents in India. We discuss influencing factors that will help build these user-profiles for Indian scenarios.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {94},
numpages = {8},
keywords = {intelligent agent, human-agent interactions, indian scenarios, context aware, conversational user interface, assistive user interface},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122138,
author = {D\"{o}belt, Susen and Schrammel, Johann and Tscheligi, Manfred},
title = {Which Cloak Dresses You Best? Comparing Location Cloaking Methods for Mobile Users},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122138},
doi = {10.1145/3098279.3122138},
abstract = {Location cloaking methods enable the protection of private location data. Different temporal and spatial approaches to cloak a specific user location (e.g., k-anonymity) have been suggested. Besides the research focusing on functionality, little work has been done on how cloaking methods should be presented to the user. In practice common location referencing services force the user to either accept or deny exact positioning. Therefore, users are not enabled to regulate private location information on a granular level. To improve the usage of location cloaking methods and foster location privacy protection, we conducted a user study (N = 24) comparing different visualized cloaking methods. The results of our lab study revealed a preference for visualizations using already known and well understood real world entities. Thus, the usage of simple and real world concepts can contribute to the application of cloaking methods and subsequently to location privacy protection.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {95},
numpages = {8},
keywords = {location obfuscation, location cloaking, mobile location sharing, location privacy, user study},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122139,
author = {Sylla, Cristina Maria and Arif, Ahmed Sabbir and Segura, Elena M\'{a}rquez and Brooks, Eva Irene},
title = {Paper Ladder: A Rating Scale to Collect Children's Opinion in User Studies},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122139},
doi = {10.1145/3098279.3122139},
abstract = {We present Paper Ladder, a low-cost, light-weight, and paper-based variant of the Sticky Ladder rating scale to collect children's product preferences in user studies. This paper version revamps the original, effective but arguably impractical evaluation method by making it more accessible to researchers. In this paper, we show its potential with preliminary results from a pilot study, where 45 grade-2 children used a Paper Ladder to rate their preference for different types of correctness feedback in a math app. Results suggest that Paper Ladder could be an effective and valuable instrument for conducting user studies with young children.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {96},
numpages = {8},
keywords = {survey methods, children, user studies, evaluation},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122140,
author = {Dionisio, Mara and Paulino, Teresa and Suri, Trisha and Autzen, Nicolas and Sch\"{o}ning, Johannes},
title = {"In Search of Light": Enhancing Touristic Recommender Services with Local Weather Data},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122140},
doi = {10.1145/3098279.3122140},
abstract = {Many destinations' economies strongly rely on tourism. Therefore, it is crucial to meet tourists' expectations, so they will return to the destination. The geographical formation of certain touristic islands often leads to local climates where it can be rainy and windy on one side of the island, whereas the other part is sunny. In this paper, we present a novel use for a network of sensors, LightBeam, a mobile location-based application aiming to improve the tourists' experience. The application focuses on providing real-time guidance for tourists seeking sunlight to maximize their holiday experience by suggesting the closest points of interest (POIs) to the user with the "best sunlight". To achieve this, we implemented and installed a network of geospatial sensors. The data from the sensor network is combined with the current location of the users to provide recommendations. We report on the initial design and prototype of LightBeam.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {97},
numpages = {8},
keywords = {user experience, tourism, weather application, mobile application},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122141,
author = {Hajinejad, Nassrin and Gr\"{u}ter, Barbara and Roque, Licinio},
title = {Prototyping Sonic Interaction for Walking},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122141},
doi = {10.1145/3098279.3122141},
abstract = {Sounds play a substantial role in the experience of movement activities such as walking. Drawing on the movement inducing effects of sound, sonic interaction opens up numerous possibilities to modify the walker's movements and experience. We argue that designing sonic interaction for movement activities demands an experiential awareness of the interplay of sound, body movement and use situation, and, propose a prototyping method to understand possibilities and challenges related to the design of mobile sonic interaction. In this paper, we present a rapid prototyping system that enables non-expert users to design sonic interaction for walking and to experience their design in the real-world context. We discuss the way this prototyping system allows designers to experience how their design ideas unfold in mobile use and affect the walking.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {98},
numpages = {8},
keywords = {mobile context, walking phrases, sonic interaction design, rapid prototyping system},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122142,
author = {Awwad, Aiman M. Ayyal and Schindler, Christian and Luhana, Kirshan Kumar and Ali, Zulfiqar and Spieler, Bernadette},
title = {Improving Pocket Paint Usability via Material Design Compliance and Internationalization &amp; Localization Support on Application Level},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122142},
doi = {10.1145/3098279.3122142},
abstract = {This paper discusses the implementation of Google's Material Design guidelines, internationalization, and localization for mobile applications in the case of Pocket Paint, an Android painting application. The intended goal of this redesign is to broaden the user base by improving overall usability and supporting right-to-left written languages such as Arabic. The main challenges of the redesign are the intricacies to thoroughly support both right-to-left and left-to-right scripts, e.g., the positioning, translation, mirroring of text and graphical elements, the 'when' and 'when not' to mirror. Related to the Material Design guideline compliance we carried out a user experience test with six users (age 13) of our target group. All participants rated the redesigned application being simpler, more appealing and concise in comparison to the previous version.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {99},
numpages = {8},
keywords = {mobile HCI, BiDi, I18n, material design, mobile app, bidirectional, GUI redesign, L10n, internationalization, pocket paint, localization},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122143,
author = {Le, Huy Viet and Mayer, Sven and Bader, Patrick and Henze, Niels},
title = {A Smartphone Prototype for Touch Interaction on the Whole Device Surface},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122143},
doi = {10.1145/3098279.3122143},
abstract = {Previous research proposed a wide range of interaction methods and use cases based on the previously unused back side and edge of a smartphone. Common approaches to implementing Back-of-Device (BoD) interaction include attaching two smartphones back to back and building a prototype completely from scratch. Changes in the device's form factor can influence hand grip and input performance as shown in previous work. Further, the lack of an established operating system and SDK requires more effort to implement novel interaction methods. In this work, we present a smartphone prototype that runs Android and has a form factor nearly identical to an off-the-shelf smartphone. It further provides capacitive images of the hand holding the device for use cases such as grip-pattern recognition. We describe technical details and share source files so that others can re-build our prototype. We evaluated the prototype with 8 participants to demonstrate the data that can be retrieved for an exemplary grip classification.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {100},
numpages = {8},
keywords = {full-touch phone, capacitive, prototype, mobile},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122144,
author = {Grover, Shruti and Johnson, Simon},
title = {Balance Trees: A New Visual Representation for Body Balance},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122144},
doi = {10.1145/3098279.3122144},
abstract = {This case study describes the results from a user-centric research project developing new visual representations of body balance to accompany a sway measurement mobile application. The study consisted of a short initial interview with participants, followed by 30 days of home usage of our Balance iOS application by the participants and concluded with a 90 minute interview at the participant's homes. In the second interview, real participant data visualized as spark-lines was presented back to the users to initiate discussion about their experiences, followed by a card sorting activity to rank various visualization options. As a result, a composite visual representation which focuses on longer term patterns and communicates four body balance attributes: sway score, symmetry, sway distribution and variability was developed to meet our users' needs.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {101},
numpages = {7},
keywords = {health data, visualizations, user-centric},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122145,
author = {Andone, Ionut and Blaszkiewicz, Konrad and B\"{o}hmer, Matthias and Markowetz, Alexander},
title = {Impact of Location-Based Games on Phone Usage and Movement: A Case Study on Pok\'{e}Mon GO},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122145},
doi = {10.1145/3098279.3122145},
abstract = {Pok\'{e}mon GO was a short lived mobile location-based gaming phenomenon. After its launch in July 2016, it quickly reached 500 million installs, but afterwards interest faded. As part of a large scale "in the wild" mobile phone study we have recorded phone usage and location measurements between June and September 2016. We investigate who were the people who installed and played Pok\'{e}mon GO and what effects it had on their behaviour. We chose as a middle point the start of playing the game, and selected users that had activity for at least two weeks before and two weeks after it. In this work we present our findings on a sample of 2, 861 users. We compare demographic characteristics and Big Five personality traits of these users with 7, 904 non-playing users from the same time period. The general daily phone usage of players increased on average by 27 minutes, which represents 16% per day. In terms of large scale movement patterns, these did not change, with regard to diameter and total path length per day.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {102},
numpages = {8},
keywords = {phone usage, location-based gaming, movement, Pok\'{e}mon GO},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122146,
author = {Cates, Sonya and Barron, Daniel and Ruddiman, Patrick},
title = {MobiLearn Go: Mobile Microlearning as an Active, Location-Aware Game},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122146},
doi = {10.1145/3098279.3122146},
abstract = {Mobile technologies hold great potential to make studying both more effective and more enjoyable. In this work we present a mobile, microlearning application. Our system is designed with two goals: be flexible enough to support learning in any subject and encourage frequent short study sessions in a variety of contexts. We discuss the use of our application to assess the feasibility of microlearning for non-language learning and the relationship between the physical location of study sessions and information retention.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {103},
numpages = {7},
keywords = {microlearning, mobile learning},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122147,
author = {Greis, Miriam and Dingler, Tilman and Schmidt, Albrecht and Schmandt, Chris},
title = {Leveraging User-Made Predictions to Help Understand Personal Behavior Patterns},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122147},
doi = {10.1145/3098279.3122147},
abstract = {People use more and more applications and devices that quantify daily behavior such as the step count or phone usage. Purely presenting the collected data does not necessarily support users in understanding their behavior. In recent research, concepts such as learning by reflection are proposed to foster behavior change based on personal data. In this paper, we introduce user-made predictions to help users understand personal behavior patterns. Therefore, we developed an Android application that tracks users' screen-on and unlock patterns on their phone. The application asks users to predict their daily behavior based on their former usage data. In a user study with 12 participants, we showed the feasibility of leveraging user-made predictions in a quantified self approach. By trying to improve their predictions over the course of the study, participants automatically discovered new insights into personal behavior patterns.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {104},
numpages = {8},
keywords = {quantified self, user-made predictions, personal behavior patterns, reflection},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122148,
author = {Giannopoulos, Ioannis and Komninos, Andreas and Garofalakis, John},
title = {Interacting with Large Maps Using HMDs in VR Settings},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122148},
doi = {10.1145/3098279.3122148},
abstract = {Location based services are a common application scenario in mobile and ubiquitous computing use. A typical issue with cartographic applications in this domain is the limited size of the displayed map, which makes interaction and visualization a difficult problem to solve. With the increasing popularity of head mounted displays for VR and AR systems, an opportunity is presented for map-based applications to overcome the limitation of the small display size, as the user's information visualization space can extend to his entire surroundings. In this paper we present a preliminary investigation into how interaction with such very large display maps can take place, using a virtual reality headset as the sole input and interaction method.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {105},
numpages = {9},
keywords = {virtual reality, digital maps, augmented reality, map-based applications, interaction},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122149,
author = {Yeom, GwangRae and Lee, Garam and Jeong, Dayoung and Rhee, Jeonghoon and Cho, Jundong},
title = {Fam-On: Family Shared Time Tracker to Improve Their Emotional Bond},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122149},
doi = {10.1145/3098279.3122149},
abstract = {As the number of dual-income household's increases, time spent by parents with children has been decreasing. To solve this issue, we have designed our system, Fam-On Platform. This platform is intended that it has increased the quantity of time by making parents recognized the lack of parenting time through time measurement. On the other hands, showing the amount of parenting time, it tries to decrease the imbalance of parenting time between fathers and mother as well. In this process, we manufactured physical wearable watch devices to promote children's interest and provide direct feedback to the family. Also, we tried to improve family bond by family-sharing time, providing contents to share among family members through application including gamification elements. We have completed a prototype of our platform and conducted pilot test targeting one child. We could gain the results we intended and find issues we need to improve further.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {106},
numpages = {8},
keywords = {affective communication, wearable device, family IoT, application, gamification},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122150,
author = {Henze, Niels and Mayer, Sven and Le, Huy Viet and Schwind, Valentin},
title = {Improving Software-Reduced Touchscreen Latency},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122150},
doi = {10.1145/3098279.3122150},
abstract = {The latency of current mobile devices' touchscreens is around 100ms and has widely been explored. Latency down to 2ms is noticeable, and latency as low as 25ms reduces users' performance. Previous work reduced touch latency by extrapolating a finger's movement using an ensemble of shallow neural networks and showed that predicting 33ms into the future increases users' performance. Unfortunately, this prediction has a high error. Predicting beyond 33ms did not increase participants' performance, and the error affected the subjective assessment. We use more recent machine learning techniques to reduce the prediction error. We train LSTM networks and multilayer perceptrons using a large data set and regularization. We show that linear extrapolation causes an 116.7% higher error and the previously proposed ensembles of shallow networks cause a 26.7% higher error compared to the LSTM networks. The trained models, the data used for testing, and the source code is available on GitHub.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {107},
numpages = {8},
keywords = {latency, lag, touch input, multilayer perceptron, machine learning, touchscreen, LSTM, prediction},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/3098279.3122151,
author = {Lee, Garam and Quero, Luis Cavazos and Yang, Jing and Jung, Hyunhee and Son, Jooyoung and Cho, Jundong},
title = {Slate Master: A Tangible Braille Slate Tutor for Mobile Devices},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122151},
doi = {10.1145/3098279.3122151},
abstract = {The development of the information technologies has benefited education through the integration of technologies and didactic tools to ease and enhance the education experience in and outside of the classroom. After a survey in several welfare centers, it was revealed that the visually impaired community has yet to benefit from the technology integration. In this work, we introduce Slate Master, a mobile device didactic tool for the visually impaired designed to ease learning how to use the Braille slate. The Braille slate is a tool used by the visually impaired to manually write Braille encoded text on paper. Slate Master is composed by a Braille tutor mobile application and a custom input interface that mimics the use of the Braille slate. We also present the insights obtained from a preliminary study performed with six Braille education experts that led to the design and development of Slate Master.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {108},
numpages = {6},
keywords = {assistive technology, slate and stylus, Braille, accessibility, visually impaired},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

