@inproceedings{10.1145/2957265.2957270,
author = {McNally, Jennifer and Bentley, Frank and Peesapati, S. Tejaswi and Ponnada, Soujanya},
title = {Exploring Best Practices for Card Interactions through a Three-Method Triangulation},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2957270},
doi = {10.1145/2957265.2957270},
abstract = {We describe a series of studies using three methods to understand usage of a cards-based mobile queryless search experience. An existing product was chosen as stimuli to allow for quick execution to learn from participants who have had experience over an extended period of time, as opposed to only studying first time use. Findings from these studies provide implications that can be used to inform products aimed at satisfying user needs for personal information while on the go. Focus is placed on the ability for queryless search results to display actionable information as well as provide direct actions without additional clicks.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {541–550},
numpages = {10},
keywords = {crowdsourcing, cards, mixed-methods, experimentation, triangulation, anticipatory search, diary studies, human factors, Google now, design},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2957271,
author = {Kim, Hyun K. and Kim, Changwon and Lim, Eunyoung and Kim, Hyunjin},
title = {How to Develop Accessibility UX Design Guideline in Samsung},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2957271},
doi = {10.1145/2957265.2957271},
abstract = {Accessibility is the major social responsibility of the Information Technology (IT) companies. New accessibility technology can make IT devices more accessible to diverse users, thus it can reduce barriers to the use of IT devices. The objective of this study is to inform the procedures to develop accessibility UX guidelines in Samsung. In 2014, the comprehensive literature survey was conducted including academic research papers, accessibility laws, and international standard documents. In 2015, a user test was conducted to clarify and specify the guidelines. In 2016, lawyers reviewed the guidelines to increase the reliability of them. The proposed procedure is helpful to develop new accessibility UX guidelines.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {551–556},
numpages = {6},
keywords = {user interface, IT devices, accessibility guideline},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2957273,
author = {Ali-Hasan, Noor and Garb, Rachel and Pereira, Mindy},
title = {Designing Android Marshmallow Volume Controls: A User Experience Case Study},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2957273},
doi = {10.1145/2957265.2957273},
abstract = {When a user presses physical hardware volume keys on a smartphone, volume controls appears on the screen. Traditionally, these volume controls are represented by a single slider. Android's Lollipop release introduced new functionality into the volume controls: a way to temporarily silence interruptions from notifications and phone calls. After the launch of Android Lollipop, we discovered some usability issues with the more robust volume controls interface. We decided to address these issues in Android's next release, Marshmallow. In this case study, we describe the trade-offs we faced in designing Android Marshmallow volume controls and how our interdisciplinary user experience team designed a longitudinal study that helped us evaluate two designs we were considering. We also describe the impact of our research approach and how we arrived at a final design.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {557–563},
numpages = {7},
keywords = {smartphones, volume controls, Android Marshmallow, user experience, mobile user experience, usability, longitudinal research},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2957274,
author = {Aranda, Julie and Ali-Hasan, Noor and Baig, Safia},
title = {I'm Just Trying to Survive: An Ethnographic Look at Mobile Notifications and Attention Management},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2957274},
doi = {10.1145/2957265.2957274},
abstract = {There is a prevailing sentiment in popular culture that we have become too attached to our phones. Smartphone notifications play a critical role in drawing people's attention to their phones. As user experience researchers on the Android team at Google, we used an ethnographic approach to understand how people experience smartphone notifications. We conducted an ethnographic study of smartphone users in New York City, while engaging members of our product team (including product managers, engineers, and designers) in the data collection and analysis. In this case study, we describe our research methods, what we learned about notifications' role in people's lives, and discuss the impact that our research has had on various product teams at Google.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {564–574},
numpages = {11},
keywords = {attention management, notifications, user experience, smartphones, ethnographic research, mobile user experience},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2957272,
author = {Mashhadi, Afra and Mathur, Akhil and Van Den Broeck, Marc and Vanderhulst, Geert and Godon, Marc and Kawsar, Fahim},
title = {A Case Study on Capturing and Visualising Face-to-Face Interactions in the Workplace},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2957272},
doi = {10.1145/2957265.2957272},
abstract = {Face-to-face interactions have proven to accelerate team and larger organisation success. Many past research has explored the benefits of quantifying face-to-face interactions for informed workplace management, with little attention being paid to how this information is perceived by the employees. In this paper, we offer a reflection on the automated feedback of personal interactions in a workplace through a longitudinal study of capturing, modelling and visualisation of face-to-face interactions of 47 employees for 4 months in an industrial research lab in Europe. We conducted semi-structured interviews with 20 employees to understand their perception and experience with the system. Our findings suggest that the short-term feedback on personal face-to-face interactions was not perceived as an effective external cue to promote self-reflection by most, and that employees desire long-term feedback annotated with actionable attributes.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {575–584},
numpages = {10},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2957275,
author = {Kim, Kahyun Sophie and Mansour, Anna-Marie and Lundell, Jay W.},
title = {Lessons Learned from Designing a Displayless Consumer Wearable Tech},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2957275},
doi = {10.1145/2957265.2957275},
abstract = {This paper discusses challenges and lessons learned from designing and launching consumer-facing wearable technologies that Intel developed with other companies. The lessons are drawn from the perspective of a user experience team working in a traditionally hardware company.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {585–590},
numpages = {6},
keywords = {display-less devices, agile UX, wearable devices},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961824,
author = {Gai, Wei and Yang, Chenglei and Bian, Yulong and Dong, Mingda and Liu, Juan and Dong, Yifan and Niu, Chengjie and Lin, Cheng and Meng, Xiangxu and Shen, Chia},
title = {UbiMaze: A New Genre of Virtual Reality Game Based on Mobile Devices},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961824},
doi = {10.1145/2957265.2961824},
abstract = {Most of current virtual/augmented reality games focus on players' immersive experience. The creative process of game design is usually not accessible to the player. In this paper, we present a new game genre that combines player initiated game design with game play. This new genre enables users to design games in a physical space and then play in a rendered virtual space. To this aim we illustrate our conceptual design of a virtual reality game, called UbiMaze, which promotes player participation, and provides a rich, interactive and engaging experience.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {591–593},
numpages = {3},
keywords = {virtual reality, natural interaction, mobile 3D, head-mounted displays},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961825,
author = {Yeo, Hui-Shyong and Lee, Juyoung and Bianchi, Andrea and Quigley, Aaron},
title = {WatchMI: Applications of Watch Movement Input on Unmodified Smartwatches},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961825},
doi = {10.1145/2957265.2961825},
abstract = {In this demo, we show that it is possible to enhance touch interaction on unmodified smartwatch to support continuous pressure touch, twist and pan gestures, by only analyzing the real-time data of Inertial Measurement Unit (IMU). Our evaluation results show that the three proposed input interfaces are accurate, noise-resistant, easy to use and can be deployed to a variety of smartwatches. We then showcase the potential of this work with seven example applications. During the demo session, users can try the prototype.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {594–598},
numpages = {5},
keywords = {rich touch, smart watch, small screen, wearable devices},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961827,
author = {Hsiu, Min-Chieh and Huang, Da-Yuan and Chen, Chi An and Lin, Yu-Chih and Hung, Yi-ping and Yang, De-Nian and Chen, Mike},
title = {ForceBoard: Using Force as Input Technique on Size-Limited Soft Keyboard},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961827},
doi = {10.1145/2957265.2961827},
abstract = {Various typing methods of qwerty-based keyboards on smartwatches have been proposed in recent years. However, since each key can only occupy limited input space and our fingers are too big, recent solutions are mainly two-step typing methods. Users have to navigate the desired key on an enlarged keyboard, and then select the target. The two-step process is distant from our physical keyboard experiences, and requires users to frequently change the keyboard layouts. The aim of this paper is to propose a single-step typing technique that allows users to key in a character with a single touch. We introduce ForceBoard, which combines two adjacent keys into one region and uses force as selecting mechanism. By using that, it not only provides more precise selection, but also allows users to type texts without changing the visual contents of keyboard. We conducted a study comparing the performance of ForceBoard with other two state-of-the-art two-step methods, ZoomBoard and SplitBoard. Our results showed that ForceBoard outperformed ZoomBoard significantly with 30.52% on average, and was slightly better than SplitBoard. Furthermore, ForceBoard also received higher preferences on text speed and satisfaction.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {599–604},
numpages = {6},
keywords = {soft keyboard, text entry, ForceBoard, smartwatch},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961829,
author = {Araki, Tomohiro and Komuro, Takashi},
title = {On-Mouse Projector: Extended Workspace Using a Mouse with a Projector},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961829},
doi = {10.1145/2957265.2961829},
abstract = {In this paper, we propose On-mouse projector, an interface that combines a mouse and a mobile projector. This system satisfies both portability and large information space and enables stable operation. The mobile projector is placed on the mouse and projects images on a surface in front of the mouse. The projected image presents a part of large information space and users can change the area to see by moving the mouse. The system is assumed to be used on a flat surface such as a desk and users can stably perform the same operation as the ordinary mouse operation. We created a prototype in which a projector is fixed above a mouse using acrylic plates. This prototype works in a standalone configuration by using a stick PC and realizes various operations such as object selection, object moving and image zooming.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {605–610},
numpages = {6},
keywords = {peephole interaction, information space, mouse input, mobile projector},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961830,
author = {Lucero, Andr\'{e}s and Porcheron, Martin and Fischer, Joel E.},
title = {Collaborative Use of Mobile Devices to Curate Sources of Inspiration},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961830},
doi = {10.1145/2957265.2961830},
abstract = {We demonstrate a prototype mobile application designed to support individually collecting personal sources of inspiration on mobile phones, and then the sharing and curating of these collected materials in a face-to-face situation.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {611–616},
numpages = {6},
keywords = {mood boards, smartphones, ideation, interaction design, collocated interaction, handheld devices},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961832,
author = {Roinesalo, Paula and Rantakari, Juho and Virtanen, Lasse and H\"{a}kkil\"{a}, Jonna},
title = {Clothes Integrated Visual Markers as Self-Expression Tool},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961832},
doi = {10.1145/2957265.2961832},
abstract = {In this demo, we present a concept where garment-integrated visual markers are used for self-expression. We present a wearable design, where clothing design style integrates with the visual design of AR markers, which are read with a mobile phone or tablet. The garment functions as a platform for self-expression, and the demo illustrates how both the AR content and the placement of the markers can play a role in the self-expression.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {617–620},
numpages = {4},
keywords = {visual markers, outfit centric design, self-expression tools, wearable computing, clothing design, mobile AR},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961826,
author = {Li, Ming and Scharf, Kaspar Maximilian and Kobbelt, Leif},
title = {MobileVideoTiles: Video Display on Multiple Mobile Devices},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961826},
doi = {10.1145/2957265.2961826},
abstract = {Modern mobile phones can capture and process high quality videos, which makes them a very popular tool to create and watch video content. However when watching a video together with a group, it is not convenient to watch on one mobile display due to its small form factor. One idea is to combine multiple mobile displays together to create a larger interactive surface for sharing visual content. However so far a practical framework supporting synchronous video playback on multiple mobile displays is still missing. We present the design of "MobileVideoTiles", a mobile application that enables users to watch local or online videos on a big virtual screen composed of multiple mobile displays. We focus on improving video quality and usability of the tiled virtual screen. The major technical contributions include: mobile peer-to-peer video streaming, playback synchronization, and accessibility of video resources. The prototype application has got several thousand downloads since release and received very positive feedback from users.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {621–626},
numpages = {6},
keywords = {video streaming, displays, tiled, synchronization, user interface, multiple mobile},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961828,
author = {Aiyoshizawa, Toshiaki and Koarai, Naoto and Komuro, Takashi},
title = {PZBoard: A Prediction-Based Zooming Interface for Supporting Text Entry on a Mobile Device},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961828},
doi = {10.1145/2957265.2961828},
abstract = {In this paper, we propose Predictive Zooming Keyboard (PZBoard) which predicts a target position from the finger movement above the touch-screen of a mobile device and that enlarges a part of the keyboard around the predicted position. We use a hover function of the mobile device to obtain the finger position above the touch-screen. While a finger is detected, a part of the keyboard around the finger position are enlarged. When the user moves his/her finger fast to enter a distant key, the target position is predicted and the center of enlargement moves to the predicted position. Using prediction, the system can start drawing the screen before the finger reaches the target position, which reduces the system's latency and quickens the user's response. The proposed interface does not force the user to perform additional operation for enlarging keys, and enables stable selection of both near and distant keys by fixing or changing the center of enlargement based on the velocity of the finger.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {627–632},
numpages = {6},
keywords = {zooming interface, touch-screen, hover input, text entry},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961833,
author = {Mart\'{\i}n-Albo, Daniel and Leiva, Luis A.},
title = {G3: Bootstrapping Stroke Gestures Design with Synthetic Samples and Built-in Recognizers},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961833},
doi = {10.1145/2957265.2961833},
abstract = {Stroke gestures are becoming increasingly important with the ongoing success of touchscreen-capable devices. However, training a high-quality gesture recognizer requires providing a large number of examples to enable good performance on unseen, future data. Furthermore, recruiting participants, data collection and labeling, etc. necessary for achieving this goal are usually time-consuming and expensive. In response to this need, we introduce G3, a mobile-first web application for bootstrapping unistroke, multistroke, or multitouch gestures. The user only has to provide a gesture example once, and G3 will create a kinematic model of that gesture. Then, by introducing local and global perturbations to the model parameters, G3 will generate any number of synthetic human-like samples. In addition, the user can get a gesture recognizer together with the synthesized data. As such, the outcome of G3 can be directly incorporated into production-ready applications.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {633–637},
numpages = {5},
keywords = {user interfaces, strokes, multistrokes, multitouch, gesture recognition, kinematics, marks, bootstrapping, symbols, gesture synthesis, rapid prototyping, unistrokes},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961834,
author = {Filho, Jackson Feij\'{o} and Prata, Wilson and Oliveira, Juan},
title = {Affective-Ready, Contextual and Automated Usability Test for Mobile Software},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961834},
doi = {10.1145/2957265.2961834},
abstract = {This work proposes the use of system to perform affective-ready, contextual and automated usability tests for mobile software. Our proposal augments the traditional methods of software usability evaluation by monitoring users' location, weather conditions, moving/stationary status, data connection availability and spontaneous facial expressions automatically. This aims to identify the moment of negative and positive events. Identifying those situations and systematically associating them to the context of interaction, assisted software creators to overcome design flaws and enhancing interfaces' strengths.The validation of our approach include post-test questionnaires with test subjects. The results indicate that the automated user-context logging can be a substantial supplement to mobile software usability tests.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {638–644},
numpages = {7},
keywords = {usability, affective computing, mobile software},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961831,
author = {Tibrewal, Richa and Singh, Ankita and Bhattacharyya, Malay},
title = {MSTROKE: A Crowd-Powered Mobility towards Stroke Recognition},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961831},
doi = {10.1145/2957265.2961831},
abstract = {We demonstrate a crowd-powered model for the early diagnosis of stroke using a mobile device. The simple approach consists of monitoring the subject's health in three simple steps including the smile test for facial weakness, raising hands test for arm weakness and speech test for slurring of speech. Our demonstrated system shows a performance accuracy of 87.5% over a total number of 40 test cases.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {645–650},
numpages = {6},
keywords = {crowdsourcing, human computer interaction, medical diagnosis, sensors},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961835,
author = {Tsai, Hsin-Ruey and Huang, Da-Yuan and Hsieh, Chen-Hsin and Huang, Lee-Ting and Hung, Yi-Ping},
title = {MovingScreen: Selecting Hard-to-Reach Targets with Automatic Comfort Zone Calibration on Mobile Devices},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961835},
doi = {10.1145/2957265.2961835},
abstract = {Smartphone screen size becomes larger nowadays. However, it causes users hard to reach a target with the thumb when using the smartphone in one hand. We propose an interaction method MovingScreen to solve the hard-to-reach problem by making the smartphone screen view movable. Users move the screen view to make the target into their comfort zone, an area users comfortably performing touch input with the thumb. They then select a target easily. Using the proposed triggering gesture bezel-scroll, MovingScreen automatically calibrates the comfort zone. Bezel-scroll detects the comfort zone and provides different screen moving ratios for users in different poses to hold the smartphone. Users are even allowed to adjust screen moving speed by altering bezel-scroll length. We evaluate performance of MovingScreen and other methods in a user study. The results show that MovingScreen has similar selection time (1030.58ms) but lower error rate (4.57%) to the other methods.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {651–658},
numpages = {8},
keywords = {one-handed interaction, touch-screen, hard-to-reach problem, mobile devices, thumb-based interaction},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961842,
author = {Holzer, Adrian and Vozniuk, Andrii and Bendahan, Samuel and Gillet, Denis},
title = {Rule of Thumb: Effect of Social Button Icons on Interaction},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961842},
doi = {10.1145/2957265.2961842},
abstract = {Social buttons are now widespread in social media apps. They are used to assign weight to user content and trigger user engagement. They come in different shapes (e.g., thumb in Facebook, arrows in Reddit or StackOverflow, plus one in Google+) but very little is known about the influence of the shape on user behaviour. This paper, addresses this issue by presenting results of a controlled randomized experiment with 173 users. The results suggest that thumbs up / thumbs down icons are significantly more engaging than the plus one / minus one icons. At the same time the result shows that type of the icon used has no significant influence on the direction of the vote.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {659–666},
numpages = {8},
keywords = {interaction design, HCI, social media, social buttons},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961856,
author = {Sajjad, Umaira Uzma and Shahid, Suleman},
title = {Baby+: A Mobile Application to Support Pregnant Women in Pakistan},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961856},
doi = {10.1145/2957265.2961856},
abstract = {This paper investigates the use of an application, Baby+, designed to support pregnant women in Pakistan. Baby+ is a localized mobile application, which helps pregnant women in keeping track of their pregnancy and gives them more control over it by providing them with relevant information. The application was designed after investigating the needs of pregnant women in the Pakistani context. In our early evaluation, women appreciated the design and functionality of Baby+ and valued it as an assistive tool, which has a potential of aiding the pregnancy health management in the region.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {667–674},
numpages = {8},
keywords = {mothers, Pakistan, mHealth, ICT4D, pregnancy, HCI4D, healthcare, mobile},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961857,
author = {Lim, Hyunchul and An, Gwangseok and Cho, Yoonkyong and Lee, Kyogu and Suh, Bongwon},
title = {WhichHand: Automatic Recognition of a Smartphone's Position in the Hand Using a Smartwatch},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961857},
doi = {10.1145/2957265.2961857},
abstract = {As mobile users often operate their devices with one enhancing one-handed interaction. In this paper, we present WhichHand, a system that 1) automatically detects which hand is holding a mobile phone and then 2) enhances user interfaces by adapting layouts to left-or right-handed use. For WhichHand, we utilize orientation sensors from a smartphone and a smartwatch. The relationship of sensor data between two mobile devices plays an important role in our recognition system. We evaluated WhichHand in a controlled study with 14 participants and conducted a user study with 10 participants to receive feedback. The accuracy of over 97% and early feedback on WhichHand provide useful insights on the design for one-handed interaction.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {675–681},
numpages = {7},
keywords = {sensors, smart phone, smart watch, adaptive user interface, machine learning, one-handed interaction},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961849,
author = {Kuflik, Tsvi and Variat, Yuri and Dim, Eyal and Mumblat, Yevgeni},
title = {Info-Bead Group Modeling in a Mobile Scenario},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961849},
doi = {10.1145/2957265.2961849},
abstract = {The mobile scenario is an extremely challenging one when it comes to providing personalized, context aware services to mobile users. Users may dynamically and continuously enter and leave smart environments that may offer them relevant services. However, the environments may not know anything about the users and hence, providing personalized, context aware services becomes a challenge: users need to be identified, queried for their preferences and monitored before a service can be provided. The lack of standard, easy to use personalization infrastructure worsens the problem -- every service provider needs to build a proprietary, add-hoc user modeling component from scratch, thus to invest considerable effort in the task. This work builds on top of previous work on Info-Beads user modeling. Following past research, it suggests an Info-Beads approach for mobile user modeling for monitoring users and enabling standardization in building user models, reusing both components and data. The specific contribution is to allow monitoring mobile users, reasoning on their data and creating individual and group models from it. We demonstrate the ideas in the area of media content recommendations for groups and individual mobile users in smart environments, as a possible case study.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {682–689},
numpages = {8},
keywords = {group modeling, Info-Bead, Info-Bead group modeling, mobile user modeling, Info-Bead user modeling, user modeling},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961837,
author = {Pinder, Charlie and Vermeulen, Jo and Wicaksono, Adhi and Beale, Russell and Hendley, Robert J.},
title = {If This, Then Habit: Exploring Context-Aware Implementation Intentions on Smartphones},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961837},
doi = {10.1145/2957265.2961837},
abstract = {Implementation intentions, 'if-then' plans where 'if's are contextual cues and 'then's are specific goal-related behaviours, hold much promise as an effective behaviour change technique to support habit formation. Nevertheless, they have been underused in digital behaviour change interventions. To address this gap, we outline a novel design of an implementation intention intervention that exploits the context-aware functionality of smartphones to extend the scope of these goal constructs. The results of a probe study and qualitative data from an elicitation survey are presented, from which we derive a set of key design recommendations and pointers for future research.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {690–697},
numpages = {8},
keywords = {nonconscious behaviour change technology, implementation intentions, context-aware smartphones},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961840,
author = {Eardley, Rachel and Gill, Steve and Roudaut, Anne and Thompson, Stephen and Hare, Joanna},
title = {Investigating How the Hand Interacts with Different Mobile Phones},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961840},
doi = {10.1145/2957265.2961840},
abstract = {In this paper we investigate the physical interaction between the hand and three types of mobile device interaction: touchscreen, physical keyboard and stylus. Through a controlled study using video observational analysis, we observed firstly, how the participants gripped the three devices and how these grips were device dependent. Secondly we looked closely at these grips to uncover how participants performed what we call micro-movements to facilitate a greater range of interaction, e.g. reaching across the keyboard. The results extend current knowledge by comparing three handheld device input methods and observing the movements, which the hand makes in five grips. The paper concludes by describing the development of a conceptual design, proposed as a provocation for the opening of dialogue on how we conceive hand usage and how it might be optimized when designed for mobile devices.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {698–705},
numpages = {8},
keywords = {product design, grasp, design, mobile device, interaction design, hand, interaction},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961841,
author = {Ren, Xipei and Visser, Vincent and Lu, Yuan and Brankaert, Rens and Offermans, Serge and Nagtzaam, Hugo},
title = {FLOW Pillow: Exploring Sitting Experience towards Active Ageing},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961841},
doi = {10.1145/2957265.2961841},
abstract = {In this paper, we present an exploratory study of the prototype of FLOW, a portable exercise platform that aims at aiding sedentary behavior among elderly people by offering them a more active sitting experience in their daily routines. We designed FLOW based on three iterations, revolving technology implementation, interaction design, and user experience design. With our prototype, we conducted a pilot study with elderly people (n=5) in a care center in the Netherlands. We evaluated user experiences and interactions with the FLOW pillow through observation, interview, and questionnaire techniques from Flow Theory. By analyzing the obtained data from the pilot study, we discussed the opportunities of FLOW in motivating active sitting behaviors for elderly and implications for future works.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {706–713},
numpages = {8},
keywords = {user experience, active ageing, interactive music, gerotechnology, sitting exercise},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961843,
author = {Rigby, Jacob M. and Brumby, Duncan P. and Cox, Anna L. and Gould, Sandy J. J.},
title = {Watching Movies on Netflix: Investigating the Effect of Screen Size on Viewer Immersion},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961843},
doi = {10.1145/2957265.2961843},
abstract = {Film and television content is moving out of the living room and onto mobile devices - viewers are now watching when and where it suits them, on devices of differing sizes. This freedom is convenient, but could lead to differing experiences across devices. Larger screens are often believed to be favourable, e.g. to watch films or sporting events. This is partially supported in the literature, which shows that larger screens lead to greater presence and more intense physiological responses. However, a more broadly-defined measure of experience, such as that of immersion from computer games research, has not been studied. In this study, 19 participants watched content on three different screens and reported their immersion level via questionnaire. Results showed that the 4.5-inch phone screen elicited lower immersion scores when compared to the 13-inch laptop and 30-inch monitor, but there was no difference when comparing the two larger screens. This suggests that very small screens lead to reduced immersion, but after a certain size the effect is less pronounced.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {714–721},
numpages = {8},
keywords = {mobile devices, experience, immersion, screen size, on-demand video, film, television},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961845,
author = {Faklaris, Cori and Cafaro, Francesco and Hook, Sara Anne and Blevins, Asa and O'Haver, Matt and Singhal, Neha},
title = {Legal and Ethical Implications of Mobile Live-Streaming Video Apps},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961845},
doi = {10.1145/2957265.2961845},
abstract = {The introduction of mobile apps such as Meerkat, Periscope, and Facebook Live has sparked enthusiasm for live-streaming video. This study explores the legal and ethical implications of mobile live-streaming video apps through a review of public-policy considerations and the computing literature as well as analyses of a mix of quantitative and qualitative user data. We identify lines of research inquiry for five policy challenges and two areas of the literature in which the impact of these apps is so far unaddressed. The detailed data gathered from these inquiries will significantly contribute to the design and development of tools, signals or affordances to address the concerns that our study identifies. We hope our work will help shape the fields of ubiquitous computing and collaborative and social computing, jurisprudence, public policy and applied ethics in the future.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {722–729},
numpages = {8},
keywords = {mobile live-streaming video, privacy, security, surveillance, informatics, intellectual property},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961864,
author = {Salazar, Carlos A. F. and Henze, Niels and Wolf, Katrin},
title = {Ergonomics of Thumb-Based Pointing While Holding Tablets},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961864},
doi = {10.1145/2957265.2961864},
abstract = {Interaction in mobile computing mainly relies on selecting targets by touch. A large body of work showed the effect of target size and target distance on selection time. Recent work on hand-held devices suggests that size and distance are not the only factors that affect selection time. In this paper, we investigate target selection performance of the thumb when interacting with grasping hands (see Figure 1). In the first study, we show that the relative direction of the target has a significant effect on selection time. In the second study we show that the direction of movement also has a significant effect. The results extend our knowledge about pointing on hand-held devices and can be used to improve transfer functions of mobile GUIs.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {730–737},
numpages = {8},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961866,
author = {Shahid, Suleman and ter Voort, Jip and Somers, Maarten and Mansour, Inti},
title = {Skeuomorphic, Flat or Material Design: Requirements for Designing Mobile Planning Applications for Students with Autism Spectrum Disorder},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961866},
doi = {10.1145/2957265.2961866},
abstract = {This study explores the user interface design requirements for developing a mobile planning application for students with autism spectrum disorder (ASD). We developed a mobile agenda application to support students in planning their activities. To test students' preference for a particular style, we designed three versions of the app, based on three different design styles (flat design, material design, and skeuomorphic design). Results show that the app was perceived as useful, likeable and user-friendly. Although, no significant difference was found between three designs, the material design was largely preferred over other two designs.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {738–745},
numpages = {8},
keywords = {skeuomorphic, flat design, mobile, autism, material},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961839,
author = {Korzetz, Mandy and K\"{u}hn, Romina and Heisig, Peter and Schlegel, Thomas},
title = {Natural Collocated Interactions for Merging Results with Mobile Devices},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961839},
doi = {10.1145/2957265.2961839},
abstract = {In collocated collaborative work, group members typically create, edit and share content to perform specific tasks. Working digitally, e.g. with mobile devices, facilitates the further processing. However, direct communication plays a decisive role. Since some tasks have to be split to be worked on, individual results have to be discussed and merged for a final solution. We provide intuitive multi-device interactions to support group members in merging multiple partial solutions to one integrated solution seamlessly. Each mobile device shows one solution that can be used completely or in parts. Our proposed interactions include complementing one part of a solution with further parts. Furthermore, content can be added or replaced by "pouring" content between several mobile devices. We implemented an application prototype to show the feasibility of our interactions. They enable an integration of mobile devices in collaborative work without replacing direct communication and support users in fulfilling their given tasks.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {746–752},
numpages = {7},
keywords = {multi-device, user experience, merging content, interaction design, collaboration},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961854,
author = {Ghosh, Sarthak and Shah, Pratik and Navarro, Lorina and Chen, Xiaowei},
title = {MusiSkate: Enhancing the Skateboarding Experience through Musical Feedback},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961854},
doi = {10.1145/2957265.2961854},
abstract = {In this paper, we investigate the potential of using musical feedback to enhance the skateboarding experience and to encourage skaters to gain more skills. We adhere to the UCD (User-Centered Design) process to discover opportunities for technological contributions in skating, followed by proposing MusiSkate as a solution based on user needs and contexts. Our findings suggest that MusiSkate has the potential to enhance the satisfaction of skating. Furthermore, it conforms to the guidelines for designing skateboarding applications as set forth by existing literature. Finally, we suggest future explorations for using audio feedback with skateboarding based on the results of our pilot study.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {753–759},
numpages = {7},
keywords = {user-centered design, skateboarding, audio feedback},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961858,
author = {Lee, Joon Young and Hong, Nahi and Kim, Soomin and Oh, Jonghwan and Lee, Joonhwan},
title = {Smiley Face: Why We Use Emoticon Stickers in Mobile Messaging},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961858},
doi = {10.1145/2957265.2961858},
abstract = {As mobile instant messaging has become a major means of communication with the widespread use of smartphones, emoticons, symbols that are meant to indicate particular emotions in instant messages, have also developed into various forms. The primary purpose of this study is to classify the usage patterns of emoticons focusing on a particular variant known as "stickers" to observe individual and social characteristics of emoticon use and reinterpret the meaning of emoticons in instant messages. A qualitative approach with an in-depth semi-structured interview was used to uncover the motive in using emoticon stickers. The study suggests that besides using emoticon stickers for expressing emotions, users may have other motives: strategic and functional purposes.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {760–766},
numpages = {7},
keywords = {emoticons, mobile messages, usage pattern, emoticon sticker, mobile usage},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961862,
author = {Jain, Minal and Seshagiri, Sarita and Chopra, Simran},
title = {How Do I Communicate My Emotions on SNS and IMs?},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961862},
doi = {10.1145/2957265.2961862},
abstract = {With influx of new devices and applications over the past few years, computer mediated communication has developed as an alternate to face-to-face communication. Through our study we attempt to understand how individuals communicate in mediated settings in emotion-laden situations. We explore their preferred medium of communication in different situations along with analysis of the content of communication. Spatial arrays, lexical surrogates, vocal spellings and grammatical markers were used as strategies by individuals for communicating non-verbal cues in text based communication. We also look at how messages sent on IMs or posts on social networks are interpreted by readers in terms of the emotional state of the sender. We found that while valence of the sender gets easily and accurately communicated, arousal is misinterpreted in most situations. In this paper, we present findings from our study which can be valuable for technology companies looking to better the current communication experience across different media.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {767–774},
numpages = {8},
keywords = {social networks, emotion, communication, IM},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961863,
author = {Aly, Yomna and Munteanu, Cosmin and Raimondo, Stefania and Wu, Alan Yusheng and Wei, Molly},
title = {Spin-Lock Gesture Authentication for Mobile Devices},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961863},
doi = {10.1145/2957265.2961863},
abstract = {The security of authentication mechanisms on touchscreen mobile devices is often at risk due to various usability barriers. The most common authentication step mobile users go through is the lock screen that blocks access to the entire device. This is subject to a usability-vs-security tension: users want quick and easy access to their phones while ensuring it is protected from unauthorized access. Since ease-of-use is a critical attribute for any new authentication mechanism, we propose an authentication system inspired by an intuitive real-life paradigm, the single dial combination (spin) lock, and evaluate its usability and user acceptance. We show that a Spin-lock authentication is perceived by users as more enjoyable than other lock methods. We use this to discuss implications for the design of mobile authentication mechanisms and suggest situations for which real-life inspired locking systems may be preferable.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {775–782},
numpages = {8},
keywords = {smart phone, dial combination lock, mobile lock screen},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961865,
author = {Wolf, Katrin and Mayer, Sven and Meyer, Stephan},
title = {Microgesture Detection for Remote Interaction with Mobile Devices},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961865},
doi = {10.1145/2957265.2961865},
abstract = {The rise of smart rings enables for ubiquitous control of computers that are wearable or mobile. We developed a ring interface using a 9 DOF IMU for detecting microgestures that can be executed while performing another task that involve hands, e.g. riding a bicycle. For the gesture classification we implemented 4 classifiers that run on the Android operating system without the need of clutch events. In a user study, we compared the success of 4 classifiers in a cycling scenario. We found that Random Forest (RF) works better for microgesture detection on Android than Dynamic Time Warping (DTW), K-Nearest-Neighbor (KNN), and than a Threshold (TH)-based approach as it has the best detection rate while it runs in real-time on Android. This work shell encourages other researchers to develop further mobile applications for using remote microgesture control in encumbered contexts.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {783–790},
numpages = {8},
keywords = {encumbered contexts, ergonomics, gesture, microgesture, bio-mechanic},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961859,
author = {Tsai, Hsin-Ruey and Wu, Cheng-Yuan and Huang, Lee-Ting and Hung, Yi-Ping},
title = {ThumbRing: Private Interactions Using One-Handed Thumb Motion Input on Finger Segments},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961859},
doi = {10.1145/2957265.2961859},
abstract = {We propose an input device, ThumbRing, for items selection on head-mounted displays (HMDs) or smart glasses. ThumbRing is a ring with an inertial measurement unit (IMU) worn on the thumb to track the motion. By arranging an item to a finger segment, users touch and slide finger segments to select the items. To resist shake in mobile conditions such as walking, another IMU is attached to the back of the hand to compute relative angles between the hand and the thumb. Sliding and touching the segments with the thumb in the hand provide privacy, subtlety, natural haptic feedback and similar input area to smartphones. A pilot study is performed to obtain users' preference finger segments. We evaluate the performance of ThumbRing in different conditions and commitment approaches in a user study. The results show that accuracy are 92.3% and 89.7% in the sitting and walking conditions, respectively.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {791–798},
numpages = {8},
keywords = {subtle, finger ring, mobile, private input},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961847,
author = {Krainz, Elmar and Lind, Viktoria and Moser, Werner and Dornhofer, Markus},
title = {Accessible Way Finding on Mobile Devices for Different User Groups},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961847},
doi = {10.1145/2957265.2961847},
abstract = {Modern technology provides various apps for navigation and routing, but these are not always accessible for anyone. People with special needs have different requirements for the suggested walking path and also other demands to the user interface.In this work we combine an accessible routing service with an accessible smartphone app, to support people with varying impairments. In a field study with 12 participants usability and accessibility issues of the concept were identified.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {799–806},
numpages = {8},
keywords = {elderly people, accessibility, blind, visually impaired, routing, assisting technology, cognitive impairment, way finding},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961848,
author = {Siebra, Clauirton and Gouveia, Tatiana and Macedo, Jefte and Correia, Walter and Penha, Marcelo and Anjos, Marcelo and Florentin, Fabiana and Silva, Fabio Q. B. and Santos, Andre L. M.},
title = {Observation Based Analysis on the Use of Mobile Applications for Visually Impaired Users},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961848},
doi = {10.1145/2957265.2961848},
abstract = {The current efforts to specify an usability guideline for accessible mobile applications are sparse and they are still far to present a concrete pattern. Our previous work carried out a broad survey to consolidate the findings of these efforts in a unique list with 36 requirements, 13 of them focused on vision impairments. In this paper we show the results of an observation-based analysis involving visually impaired volunteers, whose aim was to complement this review and confirm if the lack of these requirements in fact affects the use of mobile applications.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {807–814},
numpages = {8},
keywords = {mobile devices, user interfaces, accessibility},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961851,
author = {Shibata, Hirohito and Ichino, Junko and Hashiyama, Tomonori and Tano, Shun'ichi},
title = {A Rhythmical Tap Approach for Sending Data across Devices},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961851},
doi = {10.1145/2957265.2961851},
abstract = {This paper proposes a new user interface technique to specify sending data across digital devices. In this approach, users specify what to send from what device to what device by tapping them rhythmically. This technique is easy to operate, low implementation cost, applicable to a wide range of devices, and scalable by adding numerous rhythmical tap sequences. We confirmed the feasibility of this approach through preliminary experiments.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {815–822},
numpages = {8},
keywords = {cross-device interaction, ad-hoc network connections, rhythmical taps},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961852,
author = {Oppermann, Leif and Shekow, Marius and Bicer, Deniz},
title = {Mobile Cross-Media Visualisations Made from Building Information Modelling Data},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961852},
doi = {10.1145/2957265.2961852},
abstract = {The advent of Building Information Modelling (BIM) provides geometry data that can be easily used for visualisations. We present six demonstrators made from the same data using similar workflows. They cover different categories of mobile devices, ranging from head-mounted displays to smartphones and tablets with inside-out positional tracking. They showcase cross-media visualisations depending on the device capabilities, which vary from a sophisticated car-based AR-setup, over wired and wireless VR, to see-through AR on smart glasses, and video-based AR on tablets.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {823–830},
numpages = {8},
keywords = {architecture, visualisations, mixed reality, mobile, construction, augmented reality, building information modelling, 3D, virtual reality, CAD data},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961853,
author = {Ross, Rachael and Zhou, Wenjin},
title = {Lessons Learned: Designing a Mobile Application for Teaching Computer Science Concepts to Middle School Girls},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961853},
doi = {10.1145/2957265.2961853},
abstract = {While the technology field is growing very fast and is important to our day-to-day lives, very few women consider this field as one they would like to pursue. While the reasons for this are primarily cultural, early exposure to these fields is also very important. In this small study, we tested a customized mobile application (App) we designed to teach middle school girls about computer science concepts. We used surveys to determine which content presentation methods were more effective for learning and to find out their perception of the App. We also tracked participants' movements in the App. We found that among our 7 participants, interactive presentation methods seemed to be better for teaching than heavy text-based methods. We also found out that no participant took the same path through the App. We hope the results from this preliminary study will help add to the knowledge of this demographic in both teaching and design work.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {831–838},
numpages = {8},
keywords = {Android, computer science education, menu layout, mobile development},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961855,
author = {Pohl, Henning and Krefeld, Bastian and Rohs, Michael},
title = {Multi-Level Interaction with an LED-Matrix Edge Display},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961855},
doi = {10.1145/2957265.2961855},
abstract = {Interaction with mobile devices currently requires close engagement with them. For example, users need to pick them up and unlock them, just to check whether the last notification was for an urgent message. But such close engagement is not always desirable, e.g., when working on a project with the phone just laying around on the table. Instead, we explore around-device interactions to bring up and control notifications. As users get closer to the device, more information is revealed and additional input options become available. This allows users to control how much they want to engage with the device. For feedback, we use a custom LED-matrix display prototype on the edge of the device. This allows for coarse, but bright, notifications in the periphery of attention, but scales up to allow for slightly higher resolution feedback as well.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {839–845},
numpages = {7},
keywords = {proxemics, edge display, dot-matrix display, auxiliary display, casual interaction, LEDs},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961836,
author = {Runge, Nina and Hellmeier, Marius and Wenig, Dirk and Malaka, Rainer},
title = {Tag Your Emotions: A Novel Mobile User Interface for Annotating Images with Emotions},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961836},
doi = {10.1145/2957265.2961836},
abstract = {People tend to collect more and more data, this is especially true for images on mobile devices. Tagging images is a good way to sort such collections. While automatic tagging systems are often focused on the content, such as objects or persons in the image, manual annotations are very important to describe the context of an image. Often especially emotions are important, e.g., when a person reflects a situation, shows images from a very personal collection to others, or when using images to illustrate presentations. Unfortunately, manual annotation is often very boring and users are not very motivated to do so. While there are many approaches to motivate people to annotate data in a conventional way, none of them has focused on emotions. In this poster abstract, we present EmoWheel; an innovative interface to annotate images with emotional tags. We conducted a user study with 18 participants. Results show that the EmoWheel can enhance the motivation to annotate images.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {846–853},
numpages = {8},
keywords = {image tagging, user interfaces, apps, emotions, mobile devices, plutchik},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961861,
author = {Park, Jaehyun},
title = {Classifying Weight Training Workouts with Deep Convolutional Neural Networks: A Precedent Study},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961861},
doi = {10.1145/2957265.2961861},
abstract = {In recent years, deep learning algorithms have been widely used in both academic research and practical applications. This study uses a deep convolutional neural network to analyze and predict physical movements. We evaluated the effectiveness of our proposed network by recruiting a professional fitness trainer and let the trainer wear a smart watch equipped with an accelerometer capable of assessing physical movement. The results confirmed the ability of the network to correctly predict the bench press, dips, squat, deadlift, and military press with an accuracy rate of 92.8%. This preliminary study has several limitations such as a low sample size and the lack of a specified network layer. In subsequent studies we plan to address these limitations by extending our investigation to include the analysis of diverse movements.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {854–858},
numpages = {5},
keywords = {smart watch, deep learning, weight training, movement analysis},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961844,
author = {Tigwell, Garreth W. and Flatla, David R.},
title = {Oh That's What You Meant! Reducing Emoji Misunderstanding},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961844},
doi = {10.1145/2957265.2961844},
abstract = {Emoji provide a way to express nonverbal conversational cues in computer-mediated communication. However, people need to share the same understanding of what each emoji symbolises, otherwise communication can breakdown. We surveyed 436 people about their use of emoji and ran an interactive study using a two-dimensional emotion space to investigate (1) the variation in people's interpretation of emoji and (2) their interpretation of corresponding Android and iOS emoji. Our results show variations between people's ratings within and across platforms. We outline our solution to reduce misunderstandings that arise from different interpretations of emoji.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {859–866},
numpages = {8},
keywords = {emotion, emoji, computer-mediated communication},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961850,
author = {Kwon, Soyoung and Lee, Kun-Pyo},
title = {What Makes Readers Laugh? Value of Sensing Laughter for Humor Webtoon},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961850},
doi = {10.1145/2957265.2961850},
abstract = {Webtoon is a popular content in South Korea that has more fun techniques by using both IT and cartoon elements. However, the rating system for webtoon is still unsatisfying which have limitations on comprehending users' unconscious behavior. In this paper, we explore the value of using users' laughter reaction data for humor webtoons. Users' laughter reaction data and the rating scores were extracted simultaneously in user observation. As a result, the laughter reaction significantly correlates with the manual rating score. Also, we elicited each participants' flow of laughter which enabled to understand their laughter behavior and scenes that were attractive. With those data, ideation was conducted to generate ideas on how laughter reaction data can be used in new ways for humor webtoons. Thus, we proposed the potential values that suggest viable solutions of capturing laughter reactions for humor webtoons.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {867–874},
numpages = {8},
keywords = {content rating, reaction sensing, smile and laughter, user study, web cartoon},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961838,
author = {Pinder, Charlie and Beale, Russell and Hendley, Robert J.},
title = {Zephyr: Exploring Digital Behaviour Change Interventions to Treat Hoarding},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961838},
doi = {10.1145/2957265.2961838},
abstract = {Hoarding disorder is a complex condition that has attracted little research attention, despite adversely affecting 2-5% of the population. We review the options and difficulties in the treatment of hoarding disorder using technology. We present a novel intervention design, delivered on tablets, that combines a Cognitive Bias Modification game with goal tracking functionality. We outline two experiments in progress: a lab study to measure the impact of our game on a non-hoarding population, and a probe study to determine the suitability of the intervention for participants with hoarding disorder.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {875–882},
numpages = {8},
keywords = {hoarding, goal setting, nonconscious behaviour change using technology, cognitive bias modification},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961846,
author = {Wang, Wesley and Singh, Karan Pratap and Chu, Yan Ting Mandy and Huber, Annick},
title = {Educating Bicycle Safety and Fostering Empathy for Cyclists with an Affordable and Game-Based VR App},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961846},
doi = {10.1145/2957265.2961846},
abstract = {In recent years, there has been a rise in the use of virtual reality (VR) both in specialized fields and commercial settings. Modern applications of VR include games, films, education, arts, and healthcare, etc. Today, VR applications exist beyond expensive research labs; they are being employed to solve real world problems. To explore a new practical application of VR, we designed and prototyped a work-in-progress VR mobile app of common biking incidents in the form of a choose-your-own-adventure game. Our goal is to teach people about bicycle safety in cities, and to foster empathy within the driving community towards cyclists.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {883–890},
numpages = {8},
keywords = {instructional design, empathy, human computer interaction, accessibility, transportation, education, virtual reality},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2961860,
author = {Tsai, Hsin-Ruey and Hsiu, Min-Chieh and Hsiao, Jui-Chun and Huang, Lee-Ting and Chen, Mike and Hung, Yi-Ping},
title = {TouchRing: Subtle and Always-Available Input Using a Multi-Touch Ring},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2961860},
doi = {10.1145/2957265.2961860},
abstract = {We propose a finger-worn touch device TouchRing to provide subtle and multi-touch input. TouchRing leverages printed electrodes and the capacitive sensing technique to detect touch input. It allows users to perform multi-touch gestures in one hand to increase input modality. TouchRing worn on the index finger allows multi-touch using the thumb and middle finger. Ten multi-touch gestures are designed in this paper. We also propose touch detection and gesture recognition approaches in TouchRing. Gesture Recognition accuracy is evaluated in the user study. Applications for TouchRing are also proposed to make controlling smart glasses more convenient.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {891–898},
numpages = {8},
keywords = {multi-touch, wearable device, gesture recognition, touch input, always-available, capacitive touch, printed electronics, subtle},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2963116,
author = {Lander, Christian},
title = {Methods for Calibration Free and Multi-User Eye Tracking},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2963116},
doi = {10.1145/2957265.2963116},
abstract = {Human beings sense and perceive most of the world through their eyes. The point of gaze clearly reflects our visual attention indicating our interests. Hence gaze can be used as a powerful tool in different research areas (e.g., marketing, psychology). The progress made over the years in eye tracking enables the creation of gaze-based interactive interfaces. However, these interfaces lack of generic usability outside a controlled environment in a spontaneous pervasive way. The main objective of this research is to investigate eye-tracking technologies by means of calibration. Since calibration is user, location, orientation and target dependent, it prevents from Multi-User interaction and gaze estimation on multiple various objects (e.g., multiple screens of different sizes). Tackling these issues, new mobile as well as remote interfaces are explored and new design spaces are opened.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {899–900},
numpages = {2},
keywords = {calibration, eye tracking, multi-user, gaze},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2963113,
author = {Le, Huy Viet},
title = {Modeling Human Behavior during Touchscreen Interaction in Mobile Situations},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2963113},
doi = {10.1145/2957265.2963113},
abstract = {As mobile devices are becoming more ubiquitous, it is common for users to interact with them in many different situations. In our research, we focus on modeling human behavior during touchscreen interaction in mobile situations. Resulting models do not only increase our understanding of human behavior but also predict or infer intended user interaction. This enables us to design interfaces suited for one-handed interaction and to derive new interaction possibilities. In our work, we will collect data in lab studies as well as on a large scale to create models with high external validity.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {901–902},
numpages = {2},
keywords = {touchscreen, mobile device, research in the large, human behavior},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2963112,
author = {Fitzpatrick, Colin},
title = {Distance, Time, and Friends: System-Generated Cues and Impression Formation in Mediated Spaces},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2963112},
doi = {10.1145/2957265.2963112},
abstract = {More people are turning to apps for connecting with others nearby for a range of relational goals (i.e. dates, sex). These apps themselves constrain profiles in certain ways, while also supplementing them with additional system-generated cues. The proposed experiments are designed to investigate three such popular cues (distance, time, and number of friends) and how they affect the impression formation process in this context of varied relational goals.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {903–904},
numpages = {2},
keywords = {impression formation, system-generated cues, attraction, location-based social apps},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2963111,
author = {Silina, Yulia},
title = {Designing Social Wearables for Mediation of Intimate Relationships},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2963111},
doi = {10.1145/2957265.2963111},
abstract = {There are many benefits to mediating intimate relationships through technology, and an increasing number of ways of doing so. Among these, there is a growing interest in social wearables. But most of these devices are either bespoke one-off items or generalized and lack consideration for cultural context and needs of varied user groups. Overall, our understanding of the design criteria for these artifacts and potential implications of their newly-afforded multifaceted interactions is lagging far behind. My research aims to extend this knowledge by adopting multidisciplinary perspective and developing design guidelines with a focus on meaningful use of social wearables over time.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {905–906},
numpages = {2},
keywords = {relationship, computational jewelry, computer-mediated communication, wearables, H2H, experience design},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2963110,
author = {Yeo, Hui-Shyong},
title = {Single-Handed Interaction for Mobile and Wearable Computing},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2963110},
doi = {10.1145/2957265.2963110},
abstract = {Screen sizes on devices are becoming smaller, to allow them to fit in more places (e.g., wrists, sports bands and small music players). At the same time, screen sizes can be seen to become larger to accommodate new experiences (e.g., phablets, tablets, eReaders). Each of these trends can make devices difficult to use with only one hand (e.g., fat-finger or reachability). However, there are many occasions when the user's other hand is occupied (encumbered) or not available. The aim of this research is to explore, create and study novel interaction techniques that support effective single-hand usage on mobile and wearable devices.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {907–908},
numpages = {2},
keywords = {encumbered, single-handed interaction, wearable, mobile},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2963109,
author = {Sankar, Aditya},
title = {In-Situ Semantic 3D Modeling},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2963109},
doi = {10.1145/2957265.2963109},
abstract = {Semantic 3D models of indoor scenes enable compelling interior design applications such as remodeling, refurnishing and rearrangement of furniture. However, creating these models is still a challenging task. Most existing approaches are designed to work ex-situ or out of context, and rely on the modeler's memory, photographs or measurements from the scene.We propose a novel in-situ, mobile capture system that leverages quick and easy semantic input from the user and offloads tedious reconstruction and modeling tasks to the computer. In this way, our system combines the advantages of automatic and manual CAD based methods to significantly reduce modeling time and effort. Our approach runs on commodity mobile devices and can potentially scale to a much larger audience of casual mobile phone users.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {909–910},
numpages = {2},
keywords = {sketching, mobile augmented reality, interactive 3D modeling, human factors, design, algorithms},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2963115,
author = {Zhang, Emma Yann},
title = {A Multimodal Networked Kissing Machine for Mobile Phones},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2963115},
doi = {10.1145/2957265.2963115},
abstract = {Internet communication has revolutionised human communication, which is rapidly migrating from the physical world to the digital world. However, digital communication is often criticised for encouraging social isolation, and diminishing our abilities to empathise and form emotional bonds. Communication technologies still focus on transmitting visual and audio information, thus missing the emotional exchange during face-to-face interaction expressed through physical contact and non-verbal cues.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {911–913},
numpages = {3},
keywords = {multimodal interface, force control, haptics, mediated kissing, affective communication},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965025,
author = {Weber, Dominik and Shirazi, Alireza Sahami and Gehring, Sven and Henze, Niels and Poppinga, Benjamin and Pielot, Martin and Okoshi, Tadashi},
title = {Smarttention, Please! 2nd Workshop on Intelligent Attention Management on Mobile Devices},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965025},
doi = {10.1145/2957265.2965025},
abstract = {Today, many users of mobile devices are continuously confronted with a huge variety of information: notifications from Facebook, new application updates, won badges, or reminders. This leads to an information overload, which makes it hard to stay focused. This workshop will investigate approaches towards smart attention management systems. We will discuss the fundamental challenges of smart notifications and the design of proactive notification mechanisms. We invite submissions that focus on the understanding of users and their current, mobile information handling. We further appreciate contributions that propose design concepts for the interaction with smart attention management systems. The expected workshop outcome is a summary of emerging challenges in the design and development of smart attention management systems as well as approaches to address them.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {914–917},
numpages = {4},
keywords = {smart systems, multimodal interaction, notifications, attention management},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962657,
author = {Kerber, Frederic and Hirtz, Christoph and Gehring, Sven and L\"{o}chtefeld, Markus and Kr\"{u}ger, Antonio},
title = {Managing Smartwatch Notifications through Filtering and Ambient Illumination},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962657},
doi = {10.1145/2957265.2962657},
abstract = {The ongoing development of smart, wearable devices opens up a new range of possibilities with respect to human-computer interaction. Recent research has confirmed that smartwatches are primarily used to visualize notifications. However, the limited screen size is at odds with the ever-growing amount of information. Often, explicit interaction is needed to get an overview on the currently available information. We provide an aggregation/filtering approach as well as several displaying concepts based on a self-built, power-efficient smartwatch prototype with twelve full-color LEDs around a low-resolution display. In a user study with twelve participants, we evaluated our concepts, and we conclude with guidelines that could easily be applied to today's smartwatches to provide more expressive notification systems.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {918–923},
numpages = {6},
keywords = {notifications, notification filtering, smartwatch, LED},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962658,
author = {Schr\"{o}der, Svenja and Hirschl, Jakob and Reichl, Peter},
title = {CoConUT: Context Collection for Non-Stationary User Testing},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962658},
doi = {10.1145/2957265.2962658},
abstract = {CoConUT is an Android app for collecting the mobile context as well as the frequency of interactions during mobile field studies (for example usability studies) using sensor data on the test device. For evaluation purposes the recorded user trial sessions can be visually explored. This facilitates an assessment of the user's attention patterns and enables the detection of limited cognitive resources caused by distracting contextual factors. The app was tested in a preliminary study for technical feasibility and is planned to be extended in the near future.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {924–929},
numpages = {6},
keywords = {app, attention, field studies, context, human factors, mobile HCI, mobile context, usability},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962659,
author = {G\"{o}bel, Fabian and Giannopoulos, Ioannis and Raubal, Martin},
title = {The Importance of Visual Attention for Adaptive Interfaces},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962659},
doi = {10.1145/2957265.2962659},
abstract = {Efficient user interfaces help their users to accomplish their tasks by adapting to their current needs. The processes involved before and during interface adaptation are complex and crucial for the success and acceptance of a user interface. In this work we identify these processes and propose a framework that demonstrates the benefits that can be gained by utilizing the user's visual attention in the context of adaptive cartographic maps.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {930–935},
numpages = {6},
keywords = {gaze-based interaction, interface adaptation, visual attention},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962660,
author = {Weber, Dominik and Voit, Alexandra and Le, Huy Viet and Henze, Niels},
title = {Notification Dashboard: Enabling Reflection on Mobile Notifications},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962660},
doi = {10.1145/2957265.2962660},
abstract = {Notifications are a key feature on current smartphones. Apps gain the attention of the users to inform them about new messages, upcoming appointments or system updates. Previous studies investigated how many notifications users receive and how users interact with those notifications. Related work explored means to manage incoming notifications. In this work, we present the Notification Dashboard to enable users to reflect on their received notifications and to identify unwanted interruptions. We conducted a user study, in which we logged participants' smartphone notifications for one month. Afterwards, we visualized the log files using the dashboard and interviewed the participants about their impressions. The results show that participants underestimated the amount of notifications and were positive about using the dashboard to reflect on their received notifications.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {936–941},
numpages = {6},
keywords = {dashboard, mobile notifications, visualization},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962661,
author = {Voit, Alexandra and Machulla, Tonja and Weber, Dominik and Schwind, Valentin and Schneegass, Stefan and Henze, Niels},
title = {Exploring Notifications in Smart Home Environments},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962661},
doi = {10.1145/2957265.2962661},
abstract = {Notifications are a core mechanism of current smart devices. They inform about a variety of events including messages, social network comments, and application updates. While users appreciate the awareness that notifications provide, notifications cause distraction, higher cognitive load, and task interruptions. With the increasing importance of smart environments, the number of sensors that could trigger notifications will increase dramatically. A flower with a moisture sensor, for example, could create a notification whenever the flower needs water. We assume that current notification mechanisms will not scale with the increasing number of notifications. We therefore explore notification mechanisms for smart homes. Notifications are shown on smartphones, on displays in the environment, next to the sending objects, or on the user's body. In an online survey, we compare the four locations in four scenarios. While different aspects influence the perceived suitability of each notification location, the smartphone generally is rated the best.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {942–947},
numpages = {6},
keywords = {wearable display, notifications, reminder, online study, peripheral display, ambient display, smart home},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962662,
author = {Lee, Jemin and Kwon, Jinse and Kim, Hyungshin},
title = {Reducing Distraction of Smartwatch Users with Deep Learning},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962662},
doi = {10.1145/2957265.2962662},
abstract = {Smartwatches are overloaded with various notifications from smartphones. Users are largely distracted, while they may benefit from these relayed notification. To reduce smartwatch user's distraction, we propose an intelligent notification delivery system that relays only important notifications to the smartwatch. We claim that important notifications should be handled within a certain time and they are involved in launching mobile applications. To build model, we collect 6491 notifications and sensor data from three users. A mobile application has been developed to unobtrusively monitor relevant data Then, we implemented a binary classifier which identifies important notifications using deep learning and 8 features are extracted from sensor data. Our classifier shows that an important notification can be predicted with 61% - 90% and 51% - 99% of precision and recall.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {948–953},
numpages = {6},
keywords = {notification, smartwatch, deep learning},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962663,
author = {Schneegass, Stefan and Rzayev, Rufat},
title = {Embodied Notifications: Implicit Notifications through Electrical Muscle Stimulation},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962663},
doi = {10.1145/2957265.2962663},
abstract = {Notifications are an important function of mobile devices. They inform users about important events such as incoming messages or upcoming events. Prior work, however, showed that notifications can be disruptive which will become worse with the increasing number of notifications. Since notifications are currently not prioritized, disruption may lead to disregard of many important notifications. We propose embodied notification, a novel way of gaining the attention of the user. In contrast to regular ways of notifying the user such as presenting visual, auditory, or vibro-tactile cues, embodied notification use the body of the user as feedback channel. Thus, embodied notifications provide benefit to the user due to their embodiment and implicit nature. This novel type of notification can help gaining the attention of the user.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {954–959},
numpages = {6},
keywords = {notifications, electrical muscle stimulation},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965001,
author = {Ardissono, Liliana and Gena, Cristina and Kuflik, Tsvi},
title = {Mobile Access to Cultural Heritage: Mobile-CH 2016},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965001},
doi = {10.1145/2957265.2965001},
abstract = {The Mobile-CH workshop, held in conjunction with the MOBILE HCI Conference, will be the meeting point between cultural heritage research and personalization -- using any kind of technology, especially mobile and ubiquitous ones, to enhance the personal experience in cultural heritage sites. The workshop is aimed at bringing together researchers and practitioners who are working on various aspects of CH and are interested in exploring the potential of state of the art technology to enhance the CH visit experience. The result of the workshop is a multidisciplinary research agenda that will inform future research directions and hopefully, forge some research collaborations.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {960–963},
numpages = {4},
keywords = {cultural heritage, personalization, mobile user interfaces, tangible user interfaces, mobile guides, pervasive systems},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962643,
author = {Papangelis, Konstantinos and Chamberlain, Alan and Liang, Hai-Ning},
title = {New Directions for Preserving Intangible Cultural Heritage through the Use of Mobile Technologies},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962643},
doi = {10.1145/2957265.2962643},
abstract = {While many cultural heritage projects currently exist, few explore the full potential of mobile technologies as a mechanism to explore intangible heritage as a way to preserve culture. This paper outlines three distinct areas necessary for the design, development and application of mobile technologies within this domain. We represent these as: a) The documentation of traditions within their unique context, as articulated by the represented community---co-curated; b) The translation of traditions and their modes of expression into emerging technology-based designs; c) Co-design and ethnography as approaches to build meaningful mobile experiences.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {964–967},
numpages = {4},
keywords = {mobile technologies, indigenous, design, cultural preservation, intangible cultural heritage},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962644,
author = {Shrestha, Sujan and Chakraborty, Joyram and Mohamed, Mona A.},
title = {A Comparative Pilot Study of Historical Artifacts in a CAVE Automatic Virtual Reality Environment versus Paper-Based Artifacts},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962644},
doi = {10.1145/2957265.2962644},
abstract = {The purpose of this research is to synthesize and transform real world physical environments (PE) into a CAVE automatic virtual reality system (CAVE) by using three-dimensional (3D) models of cultural and historical artifacts. 3D models are often used in many applications including visualizations and digital preservation. Virtual reality is used to improve perception and sensation and to better understand products and environments for studying human factors and behaviors. As a pilot study, we developed and prototyped a customizable 3D physical environment using historical data and archives into an interactive CAVE virtual reality (VR) system. We then conducted a study of user preferences using pretest and post-test questionnaires of the CAVE versus paper-based artifacts.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {968–977},
numpages = {10},
keywords = {3D visualization, CAVE automatic virtual reality, human computer interaction, user-centered design, human factors, 3D modeling},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962645,
author = {Wecker, Alan J. and Kuflik, Tsvi and Stock, Oliviero},
title = {Dynamic Personalization Based on Mobile Behavior: From Personality to Personalization: A Blueprint},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962645},
doi = {10.1145/2957265.2962645},
abstract = {Systems often try to give advice to users. Personalization and the use of personality in the use of recommendation systems is a very topical. Examining the Cultural Heritage Domain, we propose a framework how we can monitor visitor behavior on the go, something that is mildly volatile, to determine personality traits, something that is more stable. This knowledge can be then used along with context to give tailored advice. Methods of monitoring visitor behavior, converting that to traits and that to personality types are described. Different dimensions of how to give tailored advice based on personality are described.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {978–983},
numpages = {6},
keywords = {museum visitor types, lifelong cultural heritage, personalization, personality},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962647,
author = {Mokatren, Moayad and Kuflik, Tsvi and Shimshoni, Ilan},
title = {A Novel Image Based Positioning Technique Using Mobile Eye Tracker for a Museum Visit},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962647},
doi = {10.1145/2957265.2962647},
abstract = {Eye tracking can be an easy way for identifying users' focus of attention and interests. This promise triggered large and continues research and technology development efforts with remarkable results. In this paper we aim at developing a novel technique for location awareness, interest detection and focus of attention using computer vision techniques and mobile eye-tracking technology. Our focus will be on museum visit and optimizing the positioning procedure by exploiting the visit style to choose the appropriate algorithm.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {984–991},
numpages = {8},
keywords = {context aware service, smart environment, personalized information, mobile guide, mobile eye tracking},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962648,
author = {Ardissono, Liliana and Lucenteforte, Maurizio and Mauro, Noemi and Savoca, Adriano and Voghera, Angioletta and La Riccia, Luigi},
title = {Exploration of Cultural Heritage Information via Textual Search Queries},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962648},
doi = {10.1145/2957265.2962648},
abstract = {Searching information in a Geographical Information System (GIS) usually imposes that users explore precompiled category catalogs and select the types of information they are looking for. Unfortunately, that approach is challenging because it forces people to adhere to a conceptualization of the information space that might be different from their own. In order to address this issue, we propose to support textual search as the basic interaction model, exploiting linguistic information, together with category exploration, for query interpretation and expansion. This paper describes our model and its adoption in the OnToMap Participatory GIS.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {992–1001},
numpages = {10},
keywords = {GIS, community maps, text-based information search},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962649,
author = {Luca, Dierna Giovanni and Alberto, Mach\`{\i}},
title = {From Proximity to Accurate Indoor Localization for Context Awareness in Mobile Museum Guides},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962649},
doi = {10.1145/2957265.2962649},
abstract = {Proximity localization has become a common support in mobile-based personal CH fruition mobile assistants, notifying the visitor information about points of interest near to him but can hardly used to recognize its presence inside areas defining a semantic context in museums or expositions. In this paper we present an accurate indoor localization methodology relying on Bluetooth Low Energy technology that can be used to gently suggest to the user evidence of contextually coherent areas of interest around him. A localization accuracy as low as two meter has been measured and one meters limit evaluated during experimentation of a mobile guide in the Archeological Museum of Camarina, in province of Ragusa (Italy).},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1002–1009},
numpages = {8},
keywords = {indoor positioning, context-aware information presentation, mobile museum guides},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962650,
author = {Ardito, Carmelo and Costabile, Maria Francesca and Desolda, Giuseppe and Matera, Maristella},
title = {Supporting Professional Guides to Create Personalized Visit Experiences},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962650},
doi = {10.1145/2957265.2962650},
abstract = {Information and communication technologies have a great potential to enhance personal experience in cultural heritage sites. Our research in the Cultural Heritage (CH) aims to foster a wider appreciation of archaeology by offering tools able to engage the general public and to increase awareness of the importance of CH. In this paper we discuss how a generic mashup platform can be used to support the work of professional guides of CH sites, in order to support them creating personalized and engaging visit experiences.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1010–1015},
numpages = {6},
keywords = {cultural heritage, mashups, web composition environments},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2963117,
author = {Georgiadi, Natalia and Kokkoli-Papadopoulou, Eleni and Kordatos, George and Partheniadis, Konstantinos and Sparakis, Manos and Koutsabasis, Panayiotis and Vosinakis, Spyros and Zissis, Dimitris and Stavrakis, Modestos},
title = {A Pervasive Role-Playing Game for Introducing Elementary School Students to Archaeology},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2963117},
doi = {10.1145/2957265.2963117},
abstract = {This paper presents ongoing work on the design and prototyping of a pervasive, role-playing game for elementary school students. The game takes place in a designated space presented as an excavation site, in which students become acquainted with a number of principal roles and tasks taking place in archaeological fieldwork. The educational goals are to introduce students to fundamental archaeology concepts and to inform them about the historical background of a specific site and the discovered artifacts. The game apparatus consists of a mobile application (android), a number of small wireless sensors (beacons), tangible models of the antiquities and simplified prop tools of the archaeological equipment (3D printed). The paper outlines the main design concepts, technologies used and gameplay and reports on a preliminary evaluation.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1016–1020},
numpages = {5},
keywords = {mobile, archaeology, interaction design for children, wireless sensors, cultural heritage, serious games, pervasive games, beacons, role-playing games},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2963118,
author = {Anagnostakis, Giorgos and Antoniou, Michalis and Kardamitsi, Elena and Sachinidis, Thodoris and Koutsabasis, Panayiotis and Stavrakis, Modestos and Vosinakis, Spyros and Zissis, Dimitris},
title = {Accessible Museum Collections for the Visually Impaired: Combining Tactile Exploration, Audio Descriptions and Mobile Gestures},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2963118},
doi = {10.1145/2957265.2963118},
abstract = {This paper describes an affordable approach and prototype system that can enhance the accessibility of museum exhibits to visually impaired users. The approach supports the navigation in exhibition halls and the tactual exploration of exhibit replicas using touch-sensitive audio descriptions and touch gestures on a mobile device. The required technology includes 3D printed exhibits, attached touch sensors, Arduino boards, and a respective mobile app. A preliminary usability evaluation with ten users (blind, visually impaired and blindfolded) revealed a positive user experience with satisfactory and similar performance.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1021–1025},
numpages = {5},
keywords = {blind users, tactile exploration, museum, visually impaired users, exhibit, audio guide, Arduino, mobile device, gesture control},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965016,
author = {Toivanen, Miika and Puolam\"{a}ki, Kai and Lukander, Kristian and H\"{a}kkinen, Jukka and Radun, Jenni},
title = {Inferring User Action with Mobile Gaze Tracking},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965016},
doi = {10.1145/2957265.2965016},
abstract = {Gaze tracking in psychological, cognitive, and user interaction studies has recently evolved toward mobile solutions, as they enable direct assessing of users' visual attention in natural environments, and augmented and virtual reality (AR/VR) applications. Productive approaches in analyzing and predicting user actions with gaze data require a multidisciplinary approach with experts in cognitive and behavioral sciences, machine vision, and machine learning. This workshop brings together a cross-domain group of individuals to (i) discuss and contribute to the problem of using mobile gaze tracking for inferring user action, (ii) advance the sharing of data and analysis algorithms as well as device solutions, and (iii) increase understanding of behavioral aspects of gaze-action sequences in natural environments and AR/VR applications.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1026–1028},
numpages = {3},
keywords = {machine learning, gaze tracking algorithms, virtual reality, natural environments, behavioral analysis, mobile gaze tracking, augmented reality, action inference},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965012,
author = {Chanijani, Seyyed Saleh Mozaffari and Klein, Pascal and Al-Naser, Mohammad and Bukhari, Syed Saqib and Kuhn, Jochen and Dengel, Andreas},
title = {A Study on Representational Competence in Physics Using Mobile Eye Tracking Systems},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965012},
doi = {10.1145/2957265.2965012},
abstract = {In this paper, we have conducted an eye tracking experiment by employing an inexpensive, lightweight, and portable eye tracker paired with a tablet. Students were instructed to solve the physics problems by presenting them three coherent representations about a phenomenon: Vectorial representations, data tables and diagrams. The effectiveness of each representation was assessed for three levels of student expertise (experts, intermediates and novices) using eye-tracking gaze data. The results show that students of different skill level (a) prefer different representations for problem-solving, (b) switch between representations with different frequencies, and (c) can be distinguished by the density of representation use. The obtained results confirm earlier findings of physics education research quantitatively which were initially obtained by student interviews and observational studies.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1029–1032},
numpages = {4},
keywords = {physics, education, mobile remote eye tracker, representational competence},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965013,
author = {Al-Naser, Mohammad and Lanzer, Peter and Dengel, Andreas and Bukhari, Syed Saqib and Chanijani, Seyyed Saleh Mozaffari},
title = {Knowledge Transfer from Experts to Novices in Minimally Invasive Catheter-Mediated (MIC) Interventions, Eye-Tracking Study},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965013},
doi = {10.1145/2957265.2965013},
abstract = {Minimally invasive catheter-mediated (MIC) interventions represent a key approach to treat patients with a wide range of cardiovascular diseases; the operators' performance rely on his or her ability to read the dynamic (cine, fluoroscopy) and static x-rays images rapidly, and accurately. Here, we demonstrate the feasibility of expertise transfer employing a low cost eye tracking system for experts gaze visualization in real-life (MIC) interventional scenario. As the video quality from head-mounted eye tracker is not sufficient for data analysis, due to head-movement, dark shades, blurring, etc., therefore we have developed an automatic method for mapping the recorded gaze from the eye-tracker video to high quality x-ray video, allowing for tracking of the complete visual perception of individual operators throughout the life performance of individual interventions based on high resolution image recordings. The high quality gaze video from an expert doctors provide an important educational resource to teach novices how to read the dynamic x-ray images.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1033–1036},
numpages = {4},
keywords = {eye-tracking, detection, minimally invasive cardiovascular interventions, cognitive learning, gaze mapping},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965014,
author = {Kit, Dmitry and Sullivan, Brian},
title = {Classifying Mobile Eye Tracking Data with Hidden Markov Models},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965014},
doi = {10.1145/2957265.2965014},
abstract = {Naturalistic eye movement behavior has been measured in a variety of scenarios [15] and eye movement patterns appear indicative of task demands [16]. However, systematic task classification of eye movement data is a relatively recent development [1,3,7]. Additionally, prior work has focused on classification of eye movements while viewing 2D screen based imagery. In the current study, eye movements from eight participants were recorded with a mobile eye tracker. Participants performed five everyday tasks: Making a sandwich, transcribing a document, walking in an office and a city street, and playing catch with a flying disc [14]. Using only saccadic direction and amplitude time series data, we trained a hidden Markov model for each task and classified unlabeled data by calculating the probability that each model could generate the observed sequence. We present accuracy and time to recognize results, demonstrating better than chance performance.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1037–1040},
numpages = {4},
keywords = {task classification, machine learning, mobile eye tracking, natural tasks},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965015,
author = {Rothkopf, Constantin A.},
title = {Minimal Sequential Gaze Models for Inferring Walkers' Tasks},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965015},
doi = {10.1145/2957265.2965015},
abstract = {Eye movements in extended sequential behavior are known to reflect task demands much more than low-level feature saliency. However, the more naturalistic the task is the more difficult it becomes to establish what cognitive processes a particular task elicits moment by moment. Here we ask the question, which sequential model is required to capture gaze sequences so that the ongoing task can be inferred reliably. Specifically, we consider eye movements of human subjects navigating a walkway while avoiding obstacles and approaching targets in a virtual environment. We show that Hidden-Markov Models, which have been used extensively in modeling human sequential behavior, can be augmented with few state variables describing the egocentric position of subjects relative to objects in the environment to dramatically increase successful classification of the ongoing task and to generate gaze sequences, that are very close to those observed in human subjects.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1041–1044},
numpages = {4},
keywords = {inferring human actions, hidden Markov models, sequential gaze models},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2964195,
author = {Chamberlain, Alan and B\o{}dker, Mads and Hazzard, Adrian and Benford, Steve},
title = {Audio in <i>Place</i>: Media, Mobility &amp; HCI - Creating Meaning in <i>Space</i>},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2964195},
doi = {10.1145/2957265.2964195},
abstract = {Audio-based content, location and mobile technologies can offer a multitude of interactional possibilities when combined in innovative and creative ways. It is important not to underestimate impact of the interplay between location, place and sound. Even if intangible and ephemeral, sounds impact upon the way in which we experience the built as well as the natural world. As technology offer us the opportunity to augment and access the world, mobile technologies offer us the opportunity to interact while moving though the world. They are technologies that can mediate, provide and locate experience in the world. Vision, and to some extent the tactile senses have been dominant modalities discussed in experiential terms within HCI. This workshop suggests that there is a need to better understand how sound can be used for shaping and augmenting the experiential qualities of places through mobile computing.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1045–1048},
numpages = {4},
keywords = {sound, semantic, audio, mobile, software, design, music, place, sonic, evaluation, HCI, location},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2964198,
author = {Brosda, Constantin and Bartsch, Silke and Oppermann, Leif and Schaal, Steffen},
title = {On the Use of Audio in the Educational Location Based Game Platform MILE},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2964198},
doi = {10.1145/2957265.2964198},
abstract = {An easy-to-use tool is presented that supports educators to create geogames for smartphones with media. The research questions are addressed whether educators use audios to support learning, and if they do, how do they use them? First results of the actual usage indicate that the potential of audio has not been fully exploited in all cases.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1049–1054},
numpages = {6},
keywords = {location based game, education, cognitive load, audio, nutrition},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2964197,
author = {Cramer, Henriette},
title = {Local Sounds &amp; Singing along in Cars},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2964197},
doi = {10.1145/2957265.2964197},
abstract = {While audio in itself may appear place-less, audio content is in many cases implicitly or explicitly tied to location. Focusing on primarily music content, this position paper outlines a number of dimensions of the interplay of audio and place, and the ways in which locality can play a role in people's engagement with music. An example of different locality levels is provided using the example of streaming content in the car as a personal space traversing from A to B.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1055–1058},
numpages = {4},
keywords = {singing-along-in-cars, location, audio, music},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2964199,
author = {De Roure, David and Willcox, Pip},
title = {Numbers in Places: Creative Interventions in Musical Space &amp; Time},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2964199},
doi = {10.1145/2957265.2964199},
abstract = {Ada Lovelace noted that Charles Babbage's Analytical Engine "might compose elaborate and scientific pieces of music of any degree of complexity or extent". The Numbers into Notes project, in its first phase, explored how this might have occurred nearly two centuries ago on the giant steam-powered machine using the mathematics of the time. Now we are asking what Lovelace might do today, with a microcontroller instead of the analytical engine. In our experiment, multiple devices are programmed to generate music, with creative interventions by humans to compose and influence the experience in locative sound.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1059–1063},
numpages = {5},
keywords = {analytical engine, algorithmic composition, Ada Lovelace},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2964196,
author = {McGookin, David},
title = {Towards Ubiquitous Location-Based Audio: Challenges and Future Directions},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2964196},
doi = {10.1145/2957265.2964196},
abstract = {Through the discussion of one completed and one on-going study, we outline how location-based audio interaction must move beyond the constraints of solely manually curated content, to consider how the influx of location-based social and cultural media databases can be used to provide more ubiquitous interaction about places. We outline our experience of these through a completed study, identifying challenges and opportunities for research, before discussing our current work on a cultural heritage app to support some of these challenges.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1064–1068},
numpages = {5},
keywords = {location-based media, cultural heritage, Twitter},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965002,
author = {Tzovaras, Dimitrios and Valtolina, Stefano and Abdelnour-Nocera, Jose and Votis, Konstantinos and Barricelli, Barbara Rita and Moustakas, Konstantinos and Kikidis, Dimitrios},
title = {Workshop on Mobile Healthcare for the Self-Management of Chronic Diseases and the Empowerment of Patients},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965002},
doi = {10.1145/2957265.2965002},
abstract = {The wave of digital health is continuously growing and promises to transform the experience of patients, redefining their role as empowered actors of the healthcare processes rather than passive receivers of medical help. Mobile technologies are a fundamental component of this transformation since they have provided a platform for the development of novel solutions, allowing a gradual shift of healthcare closer to the patients' daily living and away from the traditional clinical environment. Chronic diseases are in the center of these developments as they require the continuous and active involvement of not only healthcare professionals but also patients both of who can be empowered through the use of specialized mobile applications and the analysis of data from modern miniaturized and wearable sensing devices. Furthermore, the communication channels introduced by mobile technologies can significantly increase the efficiency of the healthcare system and facilitate the communication between patients and healthcare professionals. The current workshop invites researchers from the fields of Information Technologies and Medical Sciences as well as healthcare professionals and technology developers to demonstrate and discuss innovative approaches related to the utilization of mobile Human Computer Interaction approaches in the modern healthcare environment.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1069–1072},
numpages = {4},
keywords = {patient empowerment, electronic health records, personalized user interfaces, visual analytics, chronic diseases, medical privacy protection, disease self-management, mobile health, mobile technologies, patient decision support, patient education and training},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965003,
author = {Kultsova, Marina and Romanenko, Roman and Zhukova, Irina and Usov, Andrey and Penskoy, Nikita and Potapova, Tatiana},
title = {Assistive Mobile Application for Support of Mobility and Communication of People with IDD},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965003},
doi = {10.1145/2957265.2965003},
abstract = {This paper describes the mobile application 'Travel and Communication Assistant' which supports the mobility and communication of people with Intellectual and Development Disabilities (IDD). This application provides the possibility to people with IDD to independently perform a known route (for example a route from home to the day care center, from home to the baker's, etc.) under the remote supervision of their caregivers and to communicate with them using text, voice and pictogram messages.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1073–1076},
numpages = {4},
keywords = {mobile applications, intellectual and developmental disabilities, adaptive user interface, special needs assessment, assistive technologies},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965004,
author = {Fadhil, Ahmed and Matteotti, Cristina and Armellin, Giampaolo and Villafiorita, Adolfo and Betti, Dario},
title = {C<span class="smallcaps SmallerCapital">oach</span>M<span class="smallcaps SmallerCapital">e</span>: A Platform for Promoting Healthy Lifestyle},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965004},
doi = {10.1145/2957265.2965004},
abstract = {Promoting health and wellness reflects a holistic approach to maintain the overall wellbeing of the nation. This paper discusses challenges in dietary adherence and reviews approaches in promoting healthy diet, physical activity, and healthy lifestyle presented in the literature. After discussing persisting challenges, we propose our future approach and contribution to overcome these challenges.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1077–1080},
numpages = {4},
keywords = {tailored approach, persuasive technology, healthy diet, coaching, health&amp;wellbeing},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965005,
author = {Ghiani, Giuseppe and Manca, Marco and Patern\`{o}, Fabio and Santoro, Carmen},
title = {End-User Personalization of Context-Dependent Applications in AAL Scenarios},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965005},
doi = {10.1145/2957265.2965005},
abstract = {The design and development of flexible applications able to match the many possible user needs and provide high quality user experience is still a major issue. In ambient-assisted living scenarios there is the need of giving adequate support to elderly so that they can independently live at home. Thus, providing personalized assistance is particularly critical because ageing people often have different ranges of individual needs, requirements and disabilities. In this position paper we introduce a solution based on an End-User Development environment that allows patients and caregivers to tailor the context-dependent behaviour of their Web applications in order to facilitate patients' life. This is done through the specification of trigger-action rules to support application customization.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1081–1084},
numpages = {4},
keywords = {end-user development, personalization, AAL},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965006,
author = {Wang, Yunlong and Duan, Le and Butscher, Simon and Mueller, Jens and Reiterer, Harald},
title = {Fingerprints: Detecting Meaningful Moments for Mobile Health Intervention},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965006},
doi = {10.1145/2957265.2965006},
abstract = {Personalized and contextual interventions are promising techniques for mobile persuasive technologies in mobile health. In this paper, we propose the "fingerprints" technique to analyze the users' daily behavior patterns to find the meaningful moments to better support mobile persuasive technologies, especially mobile health interventions. We assume that for many persons, their behaviors have patterns and can be detected through the sensor data from smartphones. We develop a three-step interactive machine learning workflow to describe the concept and approach of the "fingerprints" technique. By this we aim to implement a practical and light-weight mobile intervention system without burdening the users with manual logging. In our feasibility study, we show results that provide first insights into the design of the "fingerprints" technique.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1085–1088},
numpages = {4},
keywords = {interactive machine learning, mobile intervention, mobile persuasive technologies},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965007,
author = {Pernencar, Cl\'{a}udia and Rom\~{a}o, Teresa},
title = {Mobile Apps for IBD Self: Management Using Wearable Devices and Sensors},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965007},
doi = {10.1145/2957265.2965007},
abstract = {Smart technology, like wearable sensors or biochips, presents a vast capacity for monitoring vital signs, assess patients' behaviour and context, and simultaneously provide feedback with a significant effect in diagnosis, treatment, and control of diseases. Many chronic disease, in particular Inflammatory Bowel Disease (IBD), patients need to monitor their behaviour and register their disease history (e.g. symptoms, medication intake), as well as collect their physiological data, in order to control the disease, find correlations between their behaviour and the disease progress and help doctors to adjust treatment and promote patients behaviour changes. We have been working in the use of m-health applications by chronic disease patients to facilitate self-management of their diseases and increase their autonomy. We are now studying the use of wearable devices and biochips to automatically collect patients' data and empower them in managing their own health conditions.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1089–1092},
numpages = {4},
keywords = {wearable devices, chronic diseases, health self-management, m-Health, monitoring},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965008,
author = {Murnane, Elizabeth L. and Matthews, Mark and Gay, Geri},
title = {Opportunities for Technology in the Self-Management of Mental Health},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965008},
doi = {10.1145/2957265.2965008},
abstract = {Mental health is becoming an increasingly pressing healthcare issue on a worldwide level. Chronic mental health conditions such as bipolar disorder are some of the most challenging illnesses to treat and are associated with considerable negative consequences, both in terms of societal costs as well as individual patients' quality of life. Mobile and wearable devices, with their rising ownership levels and sensing capabilities, have the potential to enable more personalized and broadly deployable forms of condition monitoring, symptom detection, and timely intervention. In this workshop paper, we overview our research into the lived experiences and self-management practices of individuals with bipolar disorder, the resultant implications for designing technology-based solutions, and the steps we have taken towards development of such assessment and intervention oriented tools. Importantly, we surface tensions between the opportunities of technology and the potential risks associated with their usage in the context of mental health.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1093–1096},
numpages = {4},
keywords = {mobile sensing, intervention, self-assessment, mHealth, mental health, bipolar disorder},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965009,
author = {Barricelli, Barbara Rita and Valtolina, Stefano and Abdelnour-Nocera, Jose},
title = {Sociotechnical Design of MHealth Applications for Chronic Diseases},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965009},
doi = {10.1145/2957265.2965009},
abstract = {This workshop paper aims at briefly presenting the authors' previous experience in the field of sociotechnical design of mHealth applications and at illustrating the opportunity in joining forces of multidisciplinary researchers, domain experts, and practitioners for improving the field.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1097–1100},
numpages = {4},
keywords = {mHealth, personal health records, mobile devices, sociotechnical design, patient engagement, chronic diseases, patient empowerment},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965010,
author = {Zechmann, Beatrix and Bobeth, Jan and Tscheligi, Manfred},
title = {Towards Successful Self-Management and Empowerment for COPD Patients},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965010},
doi = {10.1145/2957265.2965010},
abstract = {People suffering from Chronic Obstructive Pulmonary Disease (COPD) have to deal with the disease throughout their lifetime and there is no getting around without a successful self-management process. However, a poor adherence to COPD treatment is common in COPD patients and responsible for increased hospitalizations, mortality, reduced quality of life and loss of productivity. This paper envisions a novel, technology assisted solution and describes the steps needed to cover the full process of self-management. Thereby, it highlights general requirements of a technology assisted self-management device for chronic diseases and depicts a solution especially for people living with COPD.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1101–1104},
numpages = {4},
keywords = {COPD, empowerment, adherence, treatment, lifestyle change, awareness, self-management, chronic obstructive pulmonary disease},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965011,
author = {Baretta, Dario and Sartori, Fabio and Greco, Andrea and Melen, Riccardo and Stella, Fabio and Bollini, Letizia and D'addario, Marco and Steca, Patrizia},
title = {Wearable Devices and AI Techniques Integration to Promote Physical Activity},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965011},
doi = {10.1145/2957265.2965011},
abstract = {Physical activity (PA) is considered one of the most important factors for the prevention and management of non-communicable diseases (NCDs). Mobile technologies offer several opportunities for supporting PA, especially if combined with psychological aspects, model-based reasoning systems and personalized human computer interaction. This still on-going research aims at developing a scalable framework that targets PA promotion among both clinical and non-clinical population, exploiting Bayesian Networks and Expert Systems to characterize and predict qualitative variables like self-efficacy. The expected outcomes are the collection and management of real-time behavioral and psychological data to define a personalized strategy for increasing PA.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1105–1108},
numpages = {4},
keywords = {bayesian networks, expert systems, wearable devices, behavior change techniques, self-efficacy},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2970370,
author = {Iakovakis, Dimitrios and Hadjileontiadis, Leontios},
title = {Standing Hypotension Prediction Based on Smartwatch Heart Rate Variability Data: A Novel Approach},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2970370},
doi = {10.1145/2957265.2970370},
abstract = {The number of wearable and smart devices which are connecting every day in the Internet of Things (IoT) is continuously growing. We have a great opportunity though to improve the quality of life (QoL) standards by adding medical value to these devices. Especially, by exploiting IoT technology, we have the potential to create useful tools which utilize the sensors to provide biometric data. This novel study aims to use a smartwatch, independent from other hardware, to predict the Blood Pressure (BP) drop caused by postural changes. In cases that the drop is due to orthostatic hypotension (OH) can cause dizziness or even faint factors, which increase the risk of fall in the elderly but, as well as, in younger groups of people. A mathematical prediction model is proposed here which can reduce the risk of fall due to OH by sensing heart rate variability (data and drops in systolic BP after standing in a healthy group of 10 subjects. The experimental results justify the efficiency of the model, as it can perform correct prediction in 86.7% of the cases, and are encouraging enough for extending the proposed approach to pathological cases, such as patients with Parkinson's disease, involving large scale experiments.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1109–1112},
numpages = {4},
keywords = {heart rate variability, smartwatch, regression, blood pressure drop},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2970371,
author = {Lopes, Arminda Guerra},
title = {Technologies to Support Psychologists and Patients Interactions},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2970371},
doi = {10.1145/2957265.2970371},
abstract = {Technology becomes part of our daily life. The new technological solutions turned more sophisticated to surpass communication barriers. The online therapeutic work presents new possibilities and challenges for the professionals. We found in literature several computerized psychological treatment applications and also text-based sentiment analysis studies. However, we did not find an application that makes text analysis based on emotional clues and conversely serves as a management tool for psychologists. This paper reflects the increasing role of technology in health care delivery and the potential benefits that this could bring either to psychologists or to patients. We present a technological tool, which was developed following the human work interaction design approach to support interactions between patients and clinicians in psychological treatment. We are convinced that we will find some significant problems with sentiments analysis and with identification of positive and negative language. However, the proposed solution seems to fit the interviewees demand in a primary phase.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1113–1116},
numpages = {4},
keywords = {interaction design, therapeutic writing, psychologist, text analytics, work analysis, analysis, computerized psychological treatment, patient},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962651,
author = {Lucero, Andr\'{e}s and Quigley, Aaron and Rekimoto, Jun and Roudaut, Anne and Porcheron, Martin and Serrano, Marcos},
title = {Interaction Techniques for Mobile Collocation},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962651},
doi = {10.1145/2957265.2962651},
abstract = {Research on mobile collocated interactions has been exploring situations where collocated users engage in collaborative activities using their personal mobile devices (e.g., smartphones and tablets), thus going from personal/individual toward shared/multiuser experiences and interactions. The proliferation of ever-smaller computers that can be worn on our wrists (e.g., Apple Watch) and other parts of the body (e.g., Google Glass), have expanded the possibilities and increased the complexity of interaction in what we term "mobile collocated" situations. The focus of this workshop is to bring together a community of researchers, designers and practitioners to explore novel interaction techniques for mobile collocated interactions.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1117–1120},
numpages = {4},
keywords = {collocated, multi-user, handheld devices, collaboration, multi-device},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962652,
author = {Kono, Yasuyuki and Tanaka, Yuki},
title = {A Wearable Handwriting System for Time-Warping Collocation},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962652},
doi = {10.1145/2957265.2962652},
abstract = {This paper presents our new wearable handwriting input system that tracks user's index finger in the air and the finger loci are stored to the user location as handwriting and the handwritings can be shared by users as virtual scribbles and/or messages in the air. Our proposed method detects the finger locus to the location by descending the user's head motion from finger tracking reading. Their head posture is calculated in semi real time by integrating the image processing from the head-worn camera images into acceleration/gyro sensor readings equipped with the head-mounted display.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1121–1124},
numpages = {4},
keywords = {wearable interface, memory augmentation, handwriting, finger tracking},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962653,
author = {Shema, Alain and Huang, Yun},
title = {Indoor Collocation: Exploring the Ultralocal Context},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962653},
doi = {10.1145/2957265.2962653},
abstract = {Prior research has investigated how to improve awareness of collocation at neighborhood scales or citywide through the use of smartphone apps. The availability of indoor maps and more accurate indoor navigation technologies motivate us to investigate the concept of collocation in the context of indoor settings. In this paper, we introduce the notion of ultralocality, which involves people and various kinds of resources collocated in an indoor environment. We present our interview study and initial results that help us understand how people perceive collocation in an ultralocal enviornment. We also introduce a mobile application that helps people explore an ultralocal environment in the college campus buildings.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1125–1128},
numpages = {4},
keywords = {ultralocal, indoor maps, mobile collocation, hyperlocal},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962654,
author = {Jokela, Tero and Rezaei, Parisa Pour and V\"{a}\"{a}n\"{a}nen, Kaisa},
title = {Using Elicitation Studies to Generate Collocated Interaction Methods},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962654},
doi = {10.1145/2957265.2962654},
abstract = {Elicitation studies allow collecting interaction methods directly from end-users by presenting the users with the end effect of an operation and then asking them to perform the action that caused it. Applying elicitation studies in the domain of collocated interaction might enable designing more intuitive and natural group interaction methods. However, in the past elicitation studies have primarily been conducted with individual users -- they have rarely been applied to groups. In this paper, we report our initial experiences in using the elicitation study methodology to generate interaction methods for groups of collocated users with wearable devices.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1129–1133},
numpages = {5},
keywords = {elicitation study, wearable devices, guessability study, collocated interaction, multi-device user interfaces},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962655,
author = {Worgan, Paul and Knibbe, Jarrod and Fraser, Mike and Plasencia, Diego Martinez},
title = {Mobile Energy Sharing Futures},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962655},
doi = {10.1145/2957265.2962655},
abstract = {We foresee a future where energy in our mobile devices can be shared and redistributed to suit our current task needs. Many of us are beginning to carry multiple mobile devices and we seek to re-evaluate the traditional view of a mobile device as only accepting energy. In our vision, we can leverage the energy stored in our devices to wirelessly distribute energy between our friends, family, colleagues and strangers devices.In this paper we explore the opportunities and interactions presented by such spontaneous energy transfer interactions and present some envisaged collaborative energy sharing futures.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1134–1137},
numpages = {4},
keywords = {energy sharing, inductive power transfer, collaborative energy transfer},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2962656,
author = {Serna, Audrey and Tong, Lili and Tabard, Aur\'{e}lien and Pageaud, Simon and George, S\'{e}bastien},
title = {F-Formations and Collaboration Dynamics Study for Designing Mobile Collocation},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2962656},
doi = {10.1145/2957265.2962656},
abstract = {Mobile devices offer great opportunities in the field of collaborative learning for providing digital information while still supporting social interactions between group members. We designed and tested an orienteering mobile learning game to better understand how device use shaped collaboration in highly mobile conditions. The study involved four groups of three students all equipped with tablets. We focused our analysis on the relationship between participants' arrangements (F-formations), their device usage and coordination mechanisms (i.e. awareness, regulation, information sharing, and discussion). Our results emphasize the importance of considering the transitions between arrangements. From these observations we derive recommendations for the design of relevant interactions techniques for mobile collaborative activities.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1138–1141},
numpages = {4},
keywords = {coordination, ubiquitous computing, mobile learning, F-formation, collaboration, collaboration dynamics, group regulation},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965024,
author = {H\"{a}kkil\"{a}, Jonna and Juhlin, Oskar and Boll, Susanne and Colley, Ashley},
title = {The Role and Impact of Aesthetics in Designing Mobile Devices},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965024},
doi = {10.1145/2957265.2965024},
abstract = {This workshop addresses a topic, which has been relatively (and surprisingly) little considered among mobile HCI research -- aesthetics in design. Whereas research in the area is under represented, aesthetics is one of the key parameters in product design, and an important part of user experience. By understanding the role and impact of aesthetics, we can better understand user behavior and preferences, create better user interfaces, and improve our design processes. As mobile HCI is expanding in mass markets to new areas and form factors such as bracelets, glasses and smart clothing, the possibilities for designers are growing. In this workshop, we consider, e.g., user research, design research, prototypes and case studies related to aesthetics in designing mobile devices and interactions, and draw a research agenda for the future.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1142–1145},
numpages = {4},
keywords = {wearable computing, mobile devices, user experience, aesthetics, design, fashion},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965017,
author = {Rantakari, Juho and Virtanen, Lasse and Yliniva, Jenni-Liisa},
title = {Exploring Digital-Fabricated Natural Materials and Patterns for Mobile Devices},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965017},
doi = {10.1145/2957265.2965017},
abstract = {In this workshop paper we present our exploration into using different natural materials and digital-fabricated patterns for mobile devices. We present different methods of altering natural materials through digital fabrication that alter their shape, flexibility and other characteristics. Using these methods, we believe that natural materials could be used more in mobile devices which could create more engaging user experiences.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1146–1149},
numpages = {4},
keywords = {mobile, natural material, design, digital fabrication, aesthetic, pattern},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965021,
author = {Wenig, Dirk and Sch\"{o}ning, Johannes},
title = {The Aesthetics of StripeMaps: Being Small and Beautiful},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965021},
doi = {10.1145/2957265.2965021},
abstract = {Maps are a very powerful form of information visualization. Beside their main purpose to communicate often very complex relationships between elements of some space, "good" maps are often perceived as aesthetically pleasing and beautiful. Maps are collected for those reasons; since centuries and ages they are "beautiful" --over all trends. Cartography, the study and practice of making maps, has long tradition. Even today, most of the maps, e.g., road maps and public you-are-here maps, are still crafted by professional cartographers. However, maps do require space to be displayed while mobile and wearable devices are getting smaller and smaller. This presents new challenges in cartography, as designing aesthetic user interfaces is an important aspect in the area of HCI. With StripeMaps we present a technique to bring maps to very small displays without destroying their utility and beauty. In this workshop paper we discuss our design considerations in depth and open up the discussion for alternative approaches.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1150–1154},
numpages = {5},
keywords = {cartography, mobile maps, smartwatches, stripe maps},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965020,
author = {Pakanen, Minna and Lappalainen, Tuomas and Colley, Ashley and H\"{a}kkil\"{a}, Jonna},
title = {User Perspective for Interactive Handbag Design},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965020},
doi = {10.1145/2957265.2965020},
abstract = {In this paper, we address handbags as interactive mobile devices. As handbags are typical accessories carried practically by half of the adult population when on the move, they offer an interesting and widely distributed platform for mobile computing. As handbags are visible items representing the user's style, and form part of the user's overall outfit, aesthetics form an important part of their design. In this paper, we report ideas gathered from 20 participants that participated to a user study on the topic.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1155–1158},
numpages = {4},
keywords = {aesthetics, user studies, wearable computing, handbags},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965023,
author = {Wang, Jinyi and Juhlin, Oskar and Blomgren, Erika and B\r{a}gander, Linnea and K\"{a}go, Evelin and Meier, Florian and Takahashi, Mariko and Thornquist, Clemens},
title = {Design Space of the New Materials for Fashionable Wearables},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965023},
doi = {10.1145/2957265.2965023},
abstract = {This paper presents a design workshop that explores the future of fashionable wearable technology focusing on aesthetics. The results of the workshop include four fashion design concepts and the implications emerged from the discussions on each concept during the workshop. These implications open up new design space of technologies and materials that account for aesthetics beyond traditional fabric, i.e. transparency, scale, irregularity, movement, contextual expressions and fashion intelligence.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1159–1162},
numpages = {4},
keywords = {aesthetics, wearable technology, fashion, design},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965022,
author = {Kaul, Oliver Beren and Rohs, Michael},
title = {Wearable Head-Mounted 3D Tactile Display Application Scenarios},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965022},
doi = {10.1145/2957265.2965022},
abstract = {Current generation virtual reality (VR) and augmented reality (AR) head-mounted displays (HMDs) usually include no or only a single vibration motor for haptic feedback and do not use it for guidance. In a previous work, we presented HapticHead, a potentially mobile system utilizing vibration motors distributed in three concentric ellipses around the head to give intuitive haptic guidance hints and to increase immersion for VR and AR applications. The purpose of this paper is to explore potential application scenarios and aesthetic possibilities of the proposed concept in order to create an active discussion amongst workshop participants.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1163–1167},
numpages = {5},
keywords = {assistive technology, immersion, virtual reality, vibrotactile, haptic feedback, guidance, augmented reality},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965018,
author = {Hannula, Petri and Harjuniemi, Emmi and Napari, Emma},
title = {IoT Owl: Soft Tangible User Interface for Detecting the Presence of People},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965018},
doi = {10.1145/2957265.2965018},
abstract = {In this paper, we present the IoT Owl, which is the result of exploration of the use of textile materials as part of a visualization system. After hectic workshop days, our development team had several ideas to be prototyped. The IoT Owl is a prototype of a system that, via lighting and movement, displays the presence information from a sensor at a remote location, based on scan of available Bluetooth devices. It can be completely operated via Internet. The purpose of the IoT Owl is to help the user to avoid a rush hour in a chosen public space and meet the right people at the right time for example during lunch hours. The IoT Owl can calculate the crowd amount but it can also recognize familiar devices. We present a concept design for the device and create an initial prototype with features and technology in it.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1168–1172},
numpages = {5},
keywords = {bluetooth, IoT, connected environments, E-textile},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2957265.2965019,
author = {H\"{a}kkil\"{a}, Jonna and Virtanen, Lasse},
title = {Aesthetic Physical Items for Visualizing Personal Sleep Data},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2965019},
doi = {10.1145/2957265.2965019},
abstract = {In the area of wellness and health, people are currently logging and monitoring an increasing amount of information of their everyday lives. The visualization of the logged data is currently typically presented in a mobile phone app. Here, we present our ongoing research on physical visualizations of sleep data, monitored with a wearable sensor. Our aim is to create tangible artifacts where the data has been integrated to the design in an aesthetic way, and hence provide information appliances that people can reflect upon.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1173–1177},
numpages = {5},
keywords = {wearable computing, wellness, aesthetics, data visualization, physical visualization, sleep},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3254084,
author = {Rohs, Michael},
title = {Session Details: Wrist and Hand Interaction I},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254084},
doi = {10.1145/3254084},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
numpages = {1},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935362,
author = {Hsiu, Min-Chieh and Wang, Chiuan and Huang, Da-Yuan and Lin, Jhe-Wei and Lin, Yu-Chih and Yang, De-Nian and Hung, Yi-ping and Chen, Mike},
title = {Nail+: Sensing Fingernail Deformation to Detect Finger Force Touch Interactions on Rigid Surfaces},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935362},
doi = {10.1145/2935334.2935362},
abstract = {Force sensing has been widely used for bringing the touch from binary to multiple states, creating new abilities on surface interactions. However, prior proposed force sensing techniques mainly focus on enabling force-applied gestures on certain devices. This paper presents Nail+, a technique using fingernail deformation to enable force touch sensing interactions on everyday rigid surfaces. Our prototype, 3x3 0.2mm strain sensor array mounted on a fingernail, was implemented and conducted with a 12-participant study for evaluating the feasibility of this sensing approach. Result showed that the accuracy for sensing normal and force-applied tapping and swiping can achieve 84.67% on average. We finally proposed two example applications using Nail+ prototype for controlling the interfaces of head-mounted display (HMD) devices and remote screens.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {1–6},
numpages = {6},
keywords = {fingernail, strain gauges, nail deformation, natural user interface (NUI), force sensing},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935351,
author = {Pohl, Henning and Medrek, Justyna and Rohs, Michael},
title = {ScatterWatch: Subtle Notifications via Indirect Illumination Scattered in the Skin},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935351},
doi = {10.1145/2935334.2935351},
abstract = {With the increasing popularity of smartwatches over the last years, there has been a substantial interest in novel input methods for such small devices. However, feedback modalities for smartwatches have not seen the same level of interest. This is surprising, as one of the primary function of smartwatches is their use for notifications. It is the interrupting nature of current notifications on smartwatches that has also drawn some of the more critical responses to them. Here, we present a subtle notification mechanism for smartwatches that uses light scattering in a wearer's skin as a feedback modality. This does not disrupt the wearer in the same way as vibration feedback and also connects more naturally with the user's body.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {7–16},
numpages = {10},
keywords = {wearables, indirect illumination, notifications, in the wild},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935345,
author = {Guo, Anhong and Paek, Tim},
title = {Exploring Tilt for No-Touch, Wrist-Only Interactions on Smartwatches},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935345},
doi = {10.1145/2935334.2935345},
abstract = {Because smartwatches are worn on the wrist, they do not require users to hold the device, leaving at least one hand free to engage in other activities. Unfortunately, this benefit is thwarted by the typical interaction model of smartwatches; for interactions beyond glancing at information or using speech, users must utilize their other hand to manipulate a touchscreen and/or hardware buttons. In order to enable no-touch, wrist-only smartwatch interactions so that users can, for example, hold a cup of coffee while controlling their device, we explore two tilt-based interaction techniques for menu selection and navigation: AnglePoint, which directly maps the position of a virtual pointer to the tilt angle of the smartwatch, and ObjectPoint, which objectifies the underlying virtual pointer as an object imbued with a physics model. In a user study, we found that participants were able to perform menu selection and continuous selection of menu items as well as navigation through a menu hierarchy more quickly and accurately with ObjectPoint, even though previous research on tilt for other mobile devices suggested that AnglePoint would be more effective. We provide an explanation of our results and discuss the implications for more "hands-free" smartwatch interactions.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {17–28},
numpages = {12},
keywords = {interaction techniques, IMU sensors, tilt-based interaction, smartwatch, wearable devices, mobile sensing},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935358,
author = {Hsieh, Meng-Ju and Liang, Rong-Hao and Chen, Bing-Yu},
title = {NailTactors: Eyes-Free Spatial Output Using a Nail-Mounted Tactor Array},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935358},
doi = {10.1145/2935334.2935358},
abstract = {This paper investigates the feasibility of using a nail-mounted array of tactors, NailTactors, as an eyes-free output device. By rim-attached eccentric-rotating-mass (ERM) vibrators to artificial nails, miniature high-resolution tactile displays were realized as an eyes-free output device. To understand how to deliver rich signals to users for valid signal perception, three user studies were conducted. The results suggest that users can not only recognized absolute and relative directional cues, but also recognized numerical characters in EdgeWrite format with an overall 89% recognition rate. Experiments also identified the optimal placement of ERM actuators for maximizing information transfer.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {29–34},
numpages = {6},
keywords = {always-available output, nail-mounted device, tactor array},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3254085,
author = {Quigley, Aaron},
title = {Session Details: Text Entry},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254085},
doi = {10.1145/3254085},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
numpages = {1},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935366,
author = {Dalvi, Girish and Ahire, Shashank and Emmadi, Nagraj and Joshi, Manjiri and Joshi, Anirudha and Ghosh, Sanjay and Ghone, Prasad and Parmar, Narendra},
title = {Does Prediction Really Help in Marathi Text Input? Empirical Analysis of a Longitudinal Study},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935366},
doi = {10.1145/2935334.2935366},
abstract = {As part of an ongoing standardization effort, we were asked to evaluate Marathi text input mechanisms on smartphones. We undertook a between-subject longitudinal evaluation of four existing keyboards with 153 novice users who participated for 31 sessions each, spread over 3--4 weeks. In this paper, we present the empirical results of the performance of these keyboards and discuss them with respect to their designs. We found that keyboards with logical layouts performed marginally better than keyboards with partially frequency-based layouts. Results also showed that users performed poorly on keyboards that have word prediction features in comparison with keyboards that don't have prediction features while typing Marathi. We speculate that this difference in performance is related to a "cognitive toll" that the users pay to use word prediction. We identify several directions for future research.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {35–46},
numpages = {12},
keywords = {Indian languages, text input, text input evaluation, text prediction},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935376,
author = {Rodrigues, Andr\'{e} and Nicolau, Hugo and Montague, Kyle and Carri\c{c}o, Lu\'{\i}s and Guerreiro, Tiago},
title = {Effect of Target Size on Non-Visual Text-Entry},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935376},
doi = {10.1145/2935334.2935376},
abstract = {Touch-enabled devices have a growing variety of screen sizes; however, there is little knowledge on the effect of key size on non-visual text-entry performance. We conducted a user study with 12 blind participants to investigate how non-visual input performance varies with four QWERTY keyboard sizes (ranging from 15mm to 2.5mm). This paper presents an analysis of typing performance and touch behaviors discussing its implications for future research. Our findings show that there is an upper limit to the benefits of larger target sizes between 10mm and 15mm. Input speed decreases from 4.5 to 2.4 words per minute (WPM) for targets sizes below 10mm. The smallest size was deemed unusable by participants even though performance was in par with previous work.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {47–52},
numpages = {6},
keywords = {text-entry, key size, touchscreen, performance, blind},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935360,
author = {Mottelson, Aske and Larsen, Christoffer and Lyderik, Mikkel and Strohmeier, Paul and Knibbe, Jarrod},
title = {Invisiboard: Maximizing Display and Input Space with a Full Screen Text Entry Method for Smartwatches},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935360},
doi = {10.1145/2935334.2935360},
abstract = {The small displays of smartwatches make text entry difficult and time consuming. While text entry rates can be increased, this continues to occur at the expense of available screen display space. Soft keyboards can easily use half the display space of tiny-screened devices. To combat this problem, we present Invisiboard: an invisible text entry method using the entire display for both text entry and display simultaneously. Invisiboard combines a numberpad-like layout with swipe gestures. This maximizes input target size, provides a familiar layout, and maximizes display space. Through this, Invisiboard achieves entry rates comparable or even faster than an existing research baseline. A user study with 12 participants writing 3264 words revealed an entry rate of 10.6 Words Per Minute (WPM) after 30 minutes, 7% faster than ZoomBoard. Furthermore, with nominal training, some participants demonstrated entry rates of over 30 WPM.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {53–59},
numpages = {7},
keywords = {text entry, smartwatch, wearables, swipe},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935336,
author = {Shao, Yuan-Fu and Chang-Ogimoto, Masatoshi and Pointner, Reinhard and Lin, Yu-Chih and Wu, Chen-Ting and Chen, Mike},
title = {SwipeKey: A Swipe-Based Keyboard Design for Smartwatches},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935336},
doi = {10.1145/2935334.2935336},
abstract = {The rise of smartwatches calls for efficient, convenient and suitable text input methods for these small computers. The minuscule size of these screens brings many challenges on how to interact with these devices. Keyboard design requires optimization for these small screens to provide a good user experience and fast text entry method on these devices. We introduce SwipeKey, a text input method that uses swipe directions to allow multiple inputs per button and thus allows for an increase in the effective size of input buttons. We have conducted thorough experiments optimizing SwipeKey to create a fast, low-error, and easy to learn soft keyboard for smartwatches. These benefits result from having a keyboard that emphasizes the use of swipe motions. Our user study results show that with a specific combination of swipe directions and corresponding button size, SwipeKey users achieved a speed of 11 in words per minute (WPM), a 53% improvement from baseline (7.2 in WPM) and dramatically decreased character error rate (CER) from the baseline of 10% down to 3.4%.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {60–71},
numpages = {12},
keywords = {keyboard design, input, SwipeKey, text entry, smartwatch},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3254086,
author = {Patern\`{o}, Fabio},
title = {Session Details: Supporting Visual Impairment},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254086},
doi = {10.1145/3254086},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
numpages = {1},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935386,
author = {Corbett, Eric and Weber, Astrid},
title = {What Can I Say? Addressing User Experience Challenges of a Mobile Voice User Interface for Accessibility},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935386},
doi = {10.1145/2935334.2935386},
abstract = {Voice interactions on mobile phones are most often used to augment or supplement touch based interactions for users' convenience. However, for people with limited hand dexterity caused by various forms of motor-impairments voice interactions can have a significant impact and in some cases even enable independent interaction with a mobile device for the first time. For these users, a Mobile Voice User Interface (M-VUI), which allows for completely hands-free, voice only interaction would provide a high level of accessibility and independence. Implementing such a system requires research to address long standing usability challenges introduced by voice interactions that negatively affect user experience due to difficulty learning and discovering voice commands.In this paper we address these concerns reporting on research conducted to improve the visibility and learnability of voice commands of a M-VUI application being developed on the Android platform. Our research confirmed long standing challenges with voice interactions while exploring several methods for improving the onboarding and learning experience. Based on our findings we offer a set of implications for the design of M-VUIs.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {72–82},
numpages = {11},
keywords = {accessibility, universal voice control, voice user interfaces},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935367,
author = {Grussenmeyer, William and Garcia, Jesel and Jiang, Fang},
title = {Feasibility of Using Haptic Directions through Maps with a Tablet and Smart Watch for People Who Are Blind and Visually Impaired},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935367},
doi = {10.1145/2935334.2935367},
abstract = {In order to navigate through the world, people who are blind and visually impaired typically use maps through either textual directions or tactile printouts. However, visual maps on a touchscreen are not accessible to this population. Two prototypes were designed to test users' ability to trace graphical lines and directions through maps on a touchscreen using haptic feedback from an Android smart watch and tablet. With the first prototype, we show that blind and visually impaired users had lower threshold than sighted users for determining the distance between two lines on a touchscreen, suggesting their enhanced ability to form representations of spatial distance from tactile vibrational cues. With the second prototype, we show that it is feasible for blind and visually impaired users to follow directions through graphical maps based on vibrational cues. We believe these results show that our prototypes have the potential to be effective in real-world applications.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {83–89},
numpages = {7},
keywords = {haptic feedback, touchscreen, blind, accessibility, visually impaired, wearable computing},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935361,
author = {Ahmetovic, Dragan and Gleason, Cole and Ruan, Chengxiong and Kitani, Kris and Takagi, Hironobu and Asakawa, Chieko},
title = {NavCog: A Navigational Cognitive Assistant for the Blind},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935361},
doi = {10.1145/2935334.2935361},
abstract = {Turn-by-turn navigation is a useful paradigm for assisting people with visual impairments during mobility as it reduces the cognitive load of having to simultaneously sense, localize and plan. To realize such a system, it is necessary to be able to automatically localize the user with sufficient accuracy, provide timely and efficient instructions and have the ability to easily deploy the system to new spaces.We propose a smartphone-based system that provides turn-by-turn navigation assistance based on accurate real-time localization over large spaces. In addition to basic navigation capabilities, our system also informs the user about nearby points-of-interest (POI) and accessibility issues (e.g., stairs ahead). After deploying the system on a university campus across several indoor and outdoor areas, we evaluated it with six blind subjects and showed that our system is capable of guiding visually impaired users in complex and unfamiliar environments.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {90–99},
numpages = {10},
keywords = {navigation assistance, localization, visual impairments, assistive technologies, real world accessibility, bluetooth low-energy beacons, turn-by-turn navigation},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935342,
author = {Bardot, Sandra and Serrano, Marcos and Jouffrais, Christophe},
title = {From Tactile to Virtual: Using a Smartwatch to Improve Spatial Map Exploration for Visually Impaired Users},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935342},
doi = {10.1145/2935334.2935342},
abstract = {Tactile raised-line maps are paper maps widely used by visually impaired people. We designed a mobile technique, based on hand tracking and a smartwatch, in order to leverage pervasive access to virtual maps. We use the smartwatch to render localized text-to-speech and vibratory feedback during hand exploration, but also to provide filtering functions activated by swipe gestures. We conducted a first study to compare the usability of a raised-line map with three virtual maps (plain, with filter, with filter and grid). The results show that virtual maps are usable, and that adding a filter, or a filter and a grid, significantly speeds up data exploration and selection. The results of a following case study showed that visually impaired users were able to achieve a complex task with the device, i.e. finding spatial correlations between two sets of data.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {100–111},
numpages = {12},
keywords = {map exploration, wearable devices, geospatial data, visually impaired users},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3254087,
author = {Baille, Lynne},
title = {Session Details: Health and Elderly},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254087},
doi = {10.1145/3254087},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
numpages = {1},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935338,
author = {Micallef, Nicholas and Baillie, Lynne and Uzor, Stephen},
title = {Time to Exercise! An Aide-Memoire Stroke App for Post-Stroke Arm Rehabilitation},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935338},
doi = {10.1145/2935334.2935338},
abstract = {A majority of Stroke survivors have an arm impairment (up to 80%), which persists over the long term (&gt;12 months). Physiotherapy experts believe that a rehabilitation Aide-Memoire could help these patients [25]. Hence, we designed, with the input of physiotherapists, Stroke experts and former Stroke patients, the Aide-Memoire Stroke (AIMS) App to help them remember to exercise more frequently. We evaluated its use in a controlled field evaluation on a smartphone, tablet and smartwatch. Since one of the main features of the app is to remind Stroke survivors to exercise we also investigated reminder modalities (i.e., visual, vibrate, audio, speech). One key finding is that Stroke survivors opted for a combination of modalities to remind them to conduct their exercises. Also, Stroke survivors seem to prefer smartphones compared to other mobile devices due to their ease of use, usability, familiarity and being easier to handle with one arm.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {112–123},
numpages = {12},
keywords = {reminder modalities, exercise app, wearables, user design, mobile devices, user studies, stroke},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935356,
author = {Hakobyan, Lilit and Lumsden, Jo and Shaw, Rachel and O'Sullivan, Dympna},
title = {A Longitudinal Evaluation of the Acceptability and Impact of a Diet Diary App for Older Adults with Age-Related Macular Degeneration},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935356},
doi = {10.1145/2935334.2935356},
abstract = {Ongoing advances in technology are increasing the scope for enhancing and supporting older adults' daily living. The digital divide between older and younger adults raises concerns, however, about the suitability of technological solutions for older adults, especially for those with impairments. Taking older adults with Age-Related Macular Degeneration (AMD) as a case study, we used user-centred and participatory design approaches to develop an assistive mobile app for self-monitoring their intake of food [12,13]. In this paper we report on findings of a longitudinal field evaluation of our app that was conducted to investigate how it was received and adopted by older adults with AMD and its impact on their lives. Demonstrating the benefit of applying inclusive design methods for technology for older adults, our findings reveal how the use of the app raises participants' awareness and facilitates self-monitoring of diet, encourages positive (diet) behaviour change, and encourages learning.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {124–134},
numpages = {11},
keywords = {health behaviour change, older adults, diet diary, assistive technology, mobile apps, age-related macular degeneration (AMD), user-centred design (UCD)},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935377,
author = {Geurts, Eva and Haesen, Mieke and Dendale, Paul and Luyten, Kris and Coninx, Karin},
title = {Back on Bike: The BoB Mobile Cycling App for Secondary Prevention in Cardiac Patients},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935377},
doi = {10.1145/2935334.2935377},
abstract = {Persons that suffered from a cardiac disease are often recommended to integrate a sufficient level of physical exercise in their daily life. Initially, cardiac rehabilitation takes place in a closely monitored setting in a hospital or a rehabilitation center. Sustaining the effort once the patient has left the ambulatory, supervised environment is a challenge, and drop-out rates are high. Emerging approaches such as telemonitoring and telerehabilitation have been proven to show the potential to support the cardiac patient in adhering to the advised physical exercise. However, most telerehabilitation solutions only support a limited range of physical exercise, such as step-counting during walking. We propose BoB (Back on Bike), a mobile application that guides cardiac patients while cycling. Design choices are explained according to three pillars: ease of use, reduce fear, and direct and indirect motivation. In this paper, we report the results from a field study with cardiac patients.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {135–146},
numpages = {12},
keywords = {user experience, persuasion, remote monitoring, mobile application, cardiac rehabilitation, prevention},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935380,
author = {Kim, Sunyoung and Gajos, Krzysztof Z. and Muller, Michael and Grosz, Barbara J.},
title = {Acceptance of Mobile Technology by Older Adults: A Preliminary Study},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935380},
doi = {10.1145/2935334.2935380},
abstract = {Mobile technologies offer the potential for enhanced healthcare, especially by supporting self-management of chronic care. For these technologies to impact chronic care, they need to work for older adults, because the majority of people with chronic conditions are older. A major challenge remains: integrating the appropriate use of such technologies into the lives of older adults. We investigated how older adults would accept mobile technologies by interviewing two groups of older adults (technology adopters and non-adopters who aged 60+) about their experiences and perspectives to mobile technologies. Our preliminary results indicate that there is an additional phase, the intention to learn, and three relating factors, self-efficacy, conversion readiness, and peer support, that significantly influence the acceptance of mobile technologies among the participants, but are not represented in the existing models. With these findings, we propose a tentative theoretical model that extends the existing theories to explain the ways in which our participants came to accept mobile technologies. Future work should investigate the validity of the proposed model by testing our findings against younger people.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {147–157},
numpages = {11},
keywords = {aging, digital health, healthcare technology, mobile technology adoption, older adults},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3254088,
author = {Lucero, Andres},
title = {Session Details: Input Techniques},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254088},
doi = {10.1145/3254088},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
numpages = {1},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935389,
author = {Ashbrook, Daniel and Tejada, Carlos and Mehta, Dhwanit and Jiminez, Anthony and Muralitharam, Goudam and Gajendra, Sangeeta and Tallents, Ross},
title = {Bitey: An Exploration of Tooth Click Gestures for Hands-Free User Interface Control},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935389},
doi = {10.1145/2935334.2935389},
abstract = {We present Bitey, a subtle, wearable device for enabling input via tooth clicks. Based on a bone-conduction microphone worn just above the ears, Bitey recognizes the click sounds from up to five different pairs of teeth, allowing fully hands-free interface control. We explore the space of tooth input and show that Bitey allows for a high degree of accuracy in distinguishing between different tooth clicks, with up to 94% accuracy under laboratory conditions for five different tooth pairs. Finally, we illustrate Bitey's potential through two demonstration applications: a list navigation and selection interface and a keyboard input method.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {158–169},
numpages = {12},
keywords = {subtle interfaces, gestures, wearable computing, bio-acoustics, audio interfaces, tooth input},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935339,
author = {Su, Qingkun and Au, Oscar Kin-Chung and Xu, Pengfei and Fu, Hongbo and Tai, Chiew-Lan},
title = {2D-Dragger: Unified Touch-Based Target Acquisition with Constant Effective Width},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935339},
doi = {10.1145/2935334.2935339},
abstract = {In this work we introduce 2D-Dragger, a unified touch-based target acquisition technique that enables easy access to small targets in dense regions or distant targets on screens of various sizes. The effective width of a target is constant with our tool, allowing a fixed scale of finger movement for capturing a new target. Our tool is thus insensitive to the distribution and size of the selectable targets, and consistently works well for screens of different sizes, from mobile to wall-sized screens. Our user studies show that overall 2D-Dragger performs the best compared to the state-of-the-art techniques for selecting both near and distant targets of various sizes in different densities.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {170–179},
numpages = {10},
keywords = {target acquisition, touch input, effective width, accessibility},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935341,
author = {Saidi, Houssem and Serrano, Marcos and Dubois, Emmanuel},
title = {Investigating the Effects of Splitting Detailed Views in Overview+Detail Interfaces},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935341},
doi = {10.1145/2935334.2935341},
abstract = {While several techniques offer more than one detailed view in Overview+Detail (O+D) interfaces, the optimal number of detailed views has not been investigated. But the answer is not trivial: using a single detailed view offers a larger display size but only allows a sequential exploration of the overview; using several detailed views reduces the size of each view but allows a parallel exploration of the overview. In this paper we investigate the benefits of splitting the detailed view in O+D interfaces for working with very large graphs. We implemented an O+D interface where the overview is displayed on a large screen while 1, 2 or 4 split views are displayed on a tactile tablet. We experimentally evaluated the effect of the number of split views according to the number of nodes to connect. Using 4 split views is better than 1 and 2 for working on more than 2 nodes.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {180–184},
numpages = {5},
keywords = {interaction techniques, multi-surface, multi-view, graph, overview and detail, multi-device},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935371,
author = {Corsten, Christian and Link, Andreas and Karrer, Thorsten and Borchers, Jan},
title = {Understanding Back-to-Front Pinching for Eyes-Free Mobile Touch Input},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935371},
doi = {10.1145/2935334.2935371},
abstract = {Using a smartphone touchscreen to control apps mirrored to a distant display is hard, since the user cannot see where she is touching while looking at the distant screen. Tactile landmarks at the back of the phone can mitigate this problem, especially in landscape mode [3]: By moving a finger across these landmarks, the user can haptically estimate the finger position in proportion to the touchscreen. Upon pinching the thumb resting above the touchscreen towards that finger at the back, the finger position is transferred to the front and registered as a touch. However, despite proprioception, this technique leads to a shift between back and front position, denoted as pinch error. We investigated this error using different target locations, device thicknesses, and tilt angles to derive target sizes that can be acquired at a 96% success rate.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {185–189},
numpages = {5},
keywords = {eyes-free, pinch error, back-of-device, proprioception},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3254089,
author = {Serrano, Marcos},
title = {Session Details: Wearables},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254089},
doi = {10.1145/3254089},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
numpages = {1},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935344,
author = {Lyons, Kent},
title = {Visual Parameters Impacting Reaction Times on Smartwatches},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935344},
doi = {10.1145/2935334.2935344},
abstract = {As a new generation of smartwatches enters the market, one common use is for displaying information such as notifications. While some content might warrant immediately interrupting a user, there is also information that might be important to display yet less urgent. It would be useful to show this content on the watch but not immediately draw the user's attention away from their primary task. In this paper, we investigate how fast three visual parameters draw a user's attention. In particular, we present data from a smartwatch user study where we examine the size, frequency, and color of a visual prompt and the associated impact on reaction time. We find statistically significant differences for size and frequency where smaller and slower result in the less immediate reactions. We also present reaction time distributions that a designer can use to tailor expected notification response times to match their content.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {190–194},
numpages = {5},
keywords = {smartwatch, reaction time, user study, notification},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935340,
author = {Hernandez, Javier and McDuff, Daniel and Infante, Christian and Maes, Pattie and Quigley, Karen and Picard, Rosalind},
title = {Wearable ESM: Differences in the Experience Sampling Method across Wearable Devices},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935340},
doi = {10.1145/2935334.2935340},
abstract = {The Experience Sampling Method is widely used for collecting self-report responses from people in natural settings. While most traditional approaches rely on using a phone to trigger prompts and record information, wearable devices now offer new opportunities that may improve this method. This research quantitatively and qualitatively studies the experience sampling process on head-worn and wrist-worn wearable devices, and compares them to the traditional "smartphone in the pocket." To enable this work, we designed and implemented a custom application to provide similar prompts across the three types of devices and evaluated it with 15 individuals for five days (75 days total), in the context of real-life stress measurement. We found significant differences in response times across devices, and captured tradeoffs in interaction types, screen size, and device familiarity that can affect both users' experience and the reports made by users.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {195–205},
numpages = {11},
keywords = {smartwatch, smart-eyewear, ecological momentary assessment, wearable devices, experience sampling method, smartphone, Google Glass},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935346,
author = {Jokela, Tero and Rezaei, Parisa Pour and V\"{a}\"{a}n\"{a}nen, Kaisa},
title = {Natural Group Binding and Cross-Display Object Movement Methods for Wearable Devices},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935346},
doi = {10.1145/2935334.2935346},
abstract = {As wearable devices become more popular, situations where there are multiple persons present with such devices will become commonplace. In these situations, wearable devices could support collaborative tasks and experiences between co-located persons through multi-user applications. We present an elicitation study that gathers from end users interaction methods for wearable devices for two common tasks in co-located interaction: group binding and cross-display object movement. We report a total of 154 methods collected from 30 participants. We categorize the methods based on the metaphor and modality of interaction, and discuss the strengths and weaknesses of each category based on qualitative and quantitative feedback given by the participants.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {206–216},
numpages = {11},
keywords = {smartglasses, elicitation study, smartwatches, cross-display object movement, pairing, multi-device user interfaces, device ecosystem binding, co-located interaction, group association, wearable devices, guessability study},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935364,
author = {Vatavu, Radu-Daniel and Mossel, Annette and Sch\"{o}nauer, Christian},
title = {Digital Vibrons: Understanding Users' Perceptions of Interacting with Invisible, Zero-Weight Matter},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935364},
doi = {10.1145/2935334.2935364},
abstract = {We investigate in this work users' perceptions of interacting with invisible, zero-weight digital matter for smart mobile scenarios. To this end, we introduce the concept of a digital vibron as vibrational manifestation of a digital object located outside its container device. We exemplify gesture-based interactions for digital vibrons and show how thinking about interactions in terms of digital vibrons can lead to new interactive experiences in the physical-digital space. We present the results of a user study that showed high scores of users' perceived experience, usability, and desirability, and we discuss users' preferences for vibration patterns to inform the design of vibrotactile feedback for digital vibrons. We hope that this work will inspire researchers and practitioners to further explore and develop digital vibrons to design localized vibrotactile feedback for digital objects outside their smart devices toward new interactive experiences in the physical-digital space.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {217–226},
numpages = {10},
keywords = {digital matter, gestures, vibrotactile, user study, elicitation study, smart device, vibrons, feedback, evaluation, touch},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3254090,
author = {Ashbrook, Daniel},
title = {Session Details: Tools},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254090},
doi = {10.1145/3254090},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
numpages = {1},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935391,
author = {Sadana, Ramik and Li, Yang},
title = {Gesture Morpher: Video-Based Retargeting of Multi-Touch Interactions},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935391},
doi = {10.1145/2935334.2935391},
abstract = {We present Gesture Morpher, a tool for prototyping and testing multi-touch interactions based on video recordings of target application behaviors, e.g., a sequence of screenshots recorded by a screen capture tool. Gesture Morpher extracts continuous behaviors from video recordings, such as transformations of UI content, and suggests a set of multi-touch interactions that are suitable for achieving these behaviors. Designers can easily test different interactions on a touch device with visual response that is automatically synthesized from the video recording, all without any programming. We discuss a range of multi-touch interaction scenarios Gesture Morpher supports, our method for extracting continuous interaction behaviors from video recordings, and techniques for associating touch-input with the output effect extracted from the videos.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {227–232},
numpages = {6},
keywords = {video analysis, multi-touch, simulation, rapid prototyping},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935337,
author = {Sankar, Aditya and Seitz, Steven M.},
title = {In Situ CAD Capture},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935337},
doi = {10.1145/2935334.2935337},
abstract = {We present an interactive system to capture CAD-like 3D models of indoor scenes, on a mobile device. To overcome sensory and computational limitations of the mobile platform, we employ an in situ, semi-automated approach and harness the user's high-level knowledge of the scene to assist the reconstruction and modeling algorithms. The modeling proceeds in two stages: (1) The user captures the 3D shape and dimensions of the room. (2) The user then uses voice commands and an augmented reality sketching interface to insert objects of interest, such as furniture, artwork, doors and windows. Our system recognizes the sketches and add a corresponding 3D model into the scene at the appropriate location. The key contributions of this work are the design of a multi-modal user interface to effectively capture the user's semantic understanding of the scene and the underlying algorithms that process the input to produce useful reconstructions.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {233–243},
numpages = {11},
keywords = {design, interactive 3D modeling, human factors, mobile augmented reality, algorithms, sketching},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935349,
author = {Das, Sauvik and Wiese, Jason and Hong, Jason I.},
title = {Epistenet: Facilitating Programmatic Access &amp; Processing of Semantically Related Mobile Personal Data},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935349},
doi = {10.1145/2935334.2935349},
abstract = {Effective use of personal data is a core utility of modern smartphones. On Android, several challenges make developing compelling personal data applications difficult. First, personal data is stored in isolated silos. Thus, relationships between data from different providers are missing, data must be queried by source of origin rather than meaning and the persistence of different types of data differ greatly. Second, interfaces to these data are inconsistent and complex. In turn, developers are forced to interleave SQL with Java boilerplate, resulting in error-prone code that does not generalize. Our solution is Epistenet: a toolkit that (1) unifies the storage and treatment of mobile personal data; (2) preserves relationships between disparate data; (3) allows for expressive queries based on the meaning of data rather than its source of origin (e.g., one can query for all communications with John while at the park); and, (4) provides a simple, native query interface to facilitate development.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {244–253},
numpages = {10},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935374,
author = {Franjcic, Zlatko and Woundefinedniak, Pawe\l{} W. and Kasparavi\v{c}i\={u}te, Gabriele and Fjeld, Morten},
title = {WAVI: Improving Motion Capture Calibration Using Haptic and Visual Feedback},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935374},
doi = {10.1145/2935334.2935374},
abstract = {Motion tracking systems are gaining popularity and have a number of applications in research, entertainment, and arts. These systems must be calibrated before use. This process requires extensive user effort to determine a 3D coordinate system with acceptable accuracy. Usually, this is achieved by rapidly manipulating a calibration device (e.g. a calibration wand) in a volume for a set amount of time. While this is a complex spatial input task, improving the user experience of calibration inspired little research. This paper presents the design, implementation, and evaluation of WAVI --- a prototype device mounted on a calibration wand to jointly provide visual and tactile feedback during the calibration process. We conducted a user study that showed that the device significantly increases calibration quality without increasing user effort. Based on our experiences with WAVI, we present new insights for improving motion tracking calibration and complex spatial input.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {254–265},
numpages = {12},
keywords = {calibration, motion tracking, prototyping, spatial input},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3254091,
author = {Kr\"{u}ger, Antonio},
title = {Session Details: Wayfinding},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254091},
doi = {10.1145/3254091},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
numpages = {1},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935352,
author = {El Ali, Abdallah and Bachour, Khaled and Heuten, Wilko and Boll, Susanne},
title = {Technology Literacy in Poor Infrastructure Environments: Characterizing Wayfinding Strategies in Lebanon},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935352},
doi = {10.1145/2935334.2935352},
abstract = {While HCI for development (HCI4D) research has typically focused on technological practices of poor and low-literate communities, little research has addressed how technology literate individuals living in a poor infrastructure environment use technology. Our work fills this gap by focusing on Lebanon, a country with longstanding political instability, and the wayfinding issues there stemming from missing street signs and names, a poor road infrastructure, and a non-standardized addressing system. We examine the relationship between technology literate individuals' navigation and direction giving strategies and their usage of current digital navigation aids. Drawing on an interview study (N=12) and a web survey (N=85), our findings show that while these individuals rely on mapping services and WhatsApp's share location feature to aid wayfinding, many technical and cultural problems persist that are currently resolved through social querying. We discuss our results in light of problems that any map user encounters in poor infrastructure environments.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {266–277},
numpages = {12},
keywords = {strategies, Lebanon, addressing, navigation, ICT4D, HCI4D, mobile, mapping services, wayfinding, giving directions},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935365,
author = {Heller, Florian and Jevanesan, Jayan and Dietrich, Pascal and Borchers, Jan},
title = {Where Are We? Evaluating the Current Rendering Fidelity of Mobile Audio Augmented Reality Systems},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935365},
doi = {10.1145/2935334.2935365},
abstract = {Mobile audio augmented reality systems (MAARS) simulate virtual audio sources in a physical space via headphones. While 20 years ago, these required expensive sensing and rendering equipment, the necessary technology has become widely available. Smartphones have become capable to run high-fidelity spatial audio rendering algorithms, and modern sensors can provide rich data to the rendering process. Combined, these constitute an inexpensive, powerful platform for audio augmented reality.We evaluated the practical limitations of currently available off-the-shelf hardware using a voice sample in a lab experiment. State of the art motion sensors provide multiple degrees of freedom, including pitch and roll angles instead of yaw only. Since our rendering algorithm is also capable of including this richer sensor data in terms of source elevation, we also measured its impact on sound localization. Results show that mobile audio augmented reality systems achieve the same horizontal resolution as stationary systems. We found that including pitch and roll angles did not significantly improve the users' localization performance.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {278–282},
numpages = {5},
keywords = {spatial audio, virtual audio spaces, audio augmented reality, mobile devices, navigation},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935359,
author = {Matviienko, Andrii and L\"{o}cken, Andreas and El Ali, Abdallah and Heuten, Wilko and Boll, Susanne},
title = {NaviLight: Investigating Ambient Light Displays for Turn-by-Turn Navigation in Cars},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935359},
doi = {10.1145/2935334.2935359},
abstract = {Car navigation systems typically combine multiple output modalities; for example, GPS-based navigation aids show a real-time map, or feature spoken prompts indicating upcoming maneuvers. However, the drawback of graphical navigation displays is that drivers have to explicitly glance at them, which can distract from a situation on the road. To decrease driver distraction while driving with a navigation system, we explore the use of ambient light as a navigation aid in the car, in order to shift navigation aids to the periphery of human attention. We investigated this by conducting studies in a driving simulator, where we found that drivers spent significantly less time glancing at the ambient light navigation aid than on a GUI navigation display. Moreover, ambient light-based navigation was perceived to be easy to use and understand, and preferred over traditional GUI navigation displays. We discuss the implications of these outcomes on automotive personal navigation devices.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {283–294},
numpages = {12},
keywords = {navigation, peripheral visualization, ambient displays, light-based navigation},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935369,
author = {Fitzpatrick, Colin and Birnholtz, Jeremy and Gergle, Darren},
title = {People, Places, and Perceptions: Effects of Location Check-in Awareness on Impressions of Strangers},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935369},
doi = {10.1145/2935334.2935369},
abstract = {Social media platforms and mobile applications increasingly include geographic features and services. While previous research has looked into how people perceive, interpret, and act on information available about a person, the spatial self, an individual's display of mobility through space for identity performance, is underexplored, especially in encounters with strangers. Strangers themselves offer a unique potential for exploring relational contexts and how those may relate to interpreting and reacting to the spatial self. We ran a 3 (map: personal, social, and task) x 3 (relationship: date, friend, coworker) x 2 (gender of participant: female, male) laboratory experiment with a mixed model design to see if and how the spatial self affects interest in future interaction. We find that maps, relationship, and gender all affect the ways in which people interpret and act on expressing interest in an individual. We discuss theoretical and design implications of how spatial selves affect this process.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {295–305},
numpages = {11},
keywords = {check-ins, relationships, impression formation, location, spatial self, logistic regression, experiment},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3254092,
author = {Santoro, Carmen},
title = {Session Details: Games &amp; Learning},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254092},
doi = {10.1145/3254092},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
numpages = {1},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935335,
author = {Feuchtner, Tiare and Walter, Robert and M\"{u}ller, J\"{o}rg},
title = {Interruption and Pausing of Public Display Games},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935335},
doi = {10.1145/2935334.2935335},
abstract = {We present a quantitative and qualitative analysis of interruptions of interaction with a public display game, and explore the use of a manual pause mode in this scenario. In previous public display installations we observed users frequently interrupting their interaction. To explore ways of supporting such behavior, we implemented a gesture controlled multiuser game with four pausing techniques. We evaluated them in a field study analyzing 704 users and found that our pausing techniques were eagerly explored, but rarely used with the intention to pause the game. Our study shows that interactions with public displays are considerably intermissive, and that users mostly interrupt interaction to socialize and mainly approach public displays in groups. We conclude that, as a typical characteristic of public display interaction, interruptions deserve consideration. However, manual pause modes are not well suited for games on public displays. Instead, interruptions should be implicitly supported by the application design.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {306–317},
numpages = {12},
keywords = {interruption, gesture interaction, games, user studies, public displays, pause mode},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935343,
author = {Tong, Lili and Serna, Audrey and Pageaud, Simon and George, S\'{e}bastien and Tabard, Aur\'{e}lien},
title = {It's Not How You Stand, It's How You Move: F-Formations and Collaboration Dynamics in a Mobile Learning Game},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935343},
doi = {10.1145/2935334.2935343},
abstract = {Mobile devices offer great opportunities in the field of collaborative learning. They are especially interesting in their ability to provide digital information while still supporting social interactions between group members, which are essential elements of coordinated and shared activities. However, in truly mobile conditions, e.g. outdoors, the high variability of groups spatial configurations can potentially modify coordination mechanisms. We designed and tested an orienteering mobile learning game to better understand how device use shaped collaboration in highly mobile conditions. The study involved four groups of three students all equipped with tablets. We focused our analysis on the relationship between participants' arrangements (F-formations), their device usage and coordination mechanisms (i.e. awareness, regulation, information sharing, and discussion). Our results emphasize the importance of considering the transitions between arrangements more than F-formations per se. We discuss the implications of these findings for the design and analysis of mobile collaborative activities.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {318–329},
numpages = {12},
keywords = {mobile learning, ubiquitous computing, F-formation, coordination, collaboration dynamics, group regulation, collaboration},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935368,
author = {Oppermann, Leif and Blum, Lisa and Shekow, Marius},
title = {Playing on AREEF: Evaluation of an Underwater Augmented Reality Game for Kids},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935368},
doi = {10.1145/2935334.2935368},
abstract = {This paper reports on a study of AREEF, a multi-player Underwater Augmented Reality (UWAR) experience for swimming pools. Using off-the-shelf components combined with a custom made waterproof case and an innovative game concept, AREEF puts computer game technology to use for recreational and educational purposes in and under water. After an experience overview, we present evidence gained from a user-centred design-process including a pilot study with 3 kids and a final evaluation with 36 kids. Our discussion covers technical findings regarding marker placement, tracking, and device handling, as well as design related issues like virtual object placement and the need for extremely obvious user interaction and feedback when staging a mobile underwater experience.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {330–340},
numpages = {11},
keywords = {augmented reality, underwater, virtual environments, mobile, exertion, games},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935354,
author = {Sarangapani, Vidya and Kharrufa, Ahmed and Balaam, Madeline and Leat, David and Wright, Pete},
title = {Virtual.Cultural.Collaboration: Mobile Phones, Video Technology, and Cross-Cultural Learning},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935354},
doi = {10.1145/2935334.2935354},
abstract = {Cross-cultural learning has gained increased interest and importance within school curricula in recent years. Schools are using technology to accumulate resources for cross-cultural learning, which has predominantly been pre-prepared videos, documentaries, photos and textual information available online. In this paper we describe the engagement with video technology on mobile smartphones by three migrant families who were tasked with developing cross-cultural resources over the course of six weeks. The resources developed were then used as a learning resource in a classroom and feedback was taken from the teacher. Our study has established that mobile phones particularly smartphones are an accessible, evocative and affordable avenue to aid in the development of cross-cultural resources alongside building stronger parental engagement in schools. The study contributes an expansion of knowledge in research areas that seek to use video technology on mobile phones to build cross-cultural resources for learning and strengthen home-school and school-home communication.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {341–352},
numpages = {12},
keywords = {mediator, community collaboration, video technology, cross-cultural, learning, education},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3254093,
author = {Church, Karen},
title = {Session Details: Participation},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254093},
doi = {10.1145/3254093},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
numpages = {1},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935379,
author = {Fechner, Thore and Schlarmann, Dominik and Kray, Christian},
title = {Facilitating Citizen Engagement in Situ: Assessing the Impact of pro-Active Geofenced Notifications},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935379},
doi = {10.1145/2935334.2935379},
abstract = {A key problem in the area of citizen engagement is to make people aware of opportunities to participate and to motivate them to take action. We propose an approach that uses geofences and proactive notifications on mobile devices to raise citizen awareness of engagement opportunities in situ and to trigger the exploration of these opportunities. Notifications are automatically triggered in the near vicinity of engagement opportunities based on space, time, and user preferences. We conducted two user studies to investigate our approach. A field-based study revealed specific usage patterns and motivational aspects of the situated discovery of engagement opportunities. A lab-based comparison study investigated the pragmatic and hedonistic qualities of our application. Results indicate that users prefer to be informed in situ even when they do not necessarily interact with notifications straight away.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {353–364},
numpages = {12},
keywords = {location-based service, notification, citizen engagement, geofencing},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935387,
author = {Huang, Yun and Zimmerman, John and Tomasic, Anthony and Steinfeld, Aaron},
title = {Combining Contribution Interactions to Increase Coverage in Mobile Participatory Sensing Systems},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935387},
doi = {10.1145/2935334.2935387},
abstract = {Participatory sensing systems use people and their smartphones as a sensing infrastructure, and getting people to make contributions remains a critical challenge. Little work details how system designers should combine different interactions to increase coverage of service location. Tiramisu, a participatory sensing system, invites transit riders to crowdsource real-time arrival information by sharing location traces when they commute. We extended this system with a new feature that allows riders at stops to "spot" buses passing by. To better understand the impact of this new feature, we conducted an observational log analysis, examining changes in coverage and user behavior before and after the new feature. Following the addition of the spotting feature, participants' contributions increased coverage (the number of trips with real-time data) by 98%, and they used the app more than twice as much. The addition of the spotting feature was also followed by a significant increase of trace contributions.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {365–376},
numpages = {12},
keywords = {mobile crowdsourcing, coverage of service location, user contribution, participatory sensing},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935390,
author = {Tomlinson, Brianna J. and Schuett, Jonathan H. and Shortridge, Woodbury and Chandran, Jehoshaph and Walker, Bruce N.},
title = {Talkin' about the Weather: Incorporating TalkBack Functionality and Sonifications for Accessible App Design},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935390},
doi = {10.1145/2935334.2935390},
abstract = {As ubiquitous as weather is in our daily lives, individuals with vision impairments endure poorly designed user experiences when attempting to check the weather on their mobile devices. This is primarily caused by a mismatch between the visually based information layout on screen and the order in which a screen reader, such as TalkBack or VoiceOver, presents the information to users with visual impairments. Additionally, any image or icon included on the screen presents no information to the user if they are not able to see it. Therefore we created the Accessible Weather App to run on Android and integrate with the TalkBack accessibility feature that is already available on the operating system. We also included a set of auditory weather icons which use sound, rather than visuals, to convey current weather conditions to users in a fast and pleasant way. This paper discusses the process for determining what features the users' would want and require, as well as our methodology for evaluating the beta version of our app.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {377–386},
numpages = {10},
keywords = {blind, visually impaired, talkback, assistive technology, VoiceOver, app, sonification, weather, accessibility},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935378,
author = {Paay, Jeni and Kjeldskov, Jesper and Skov, Mikael B. and Nielsen, Per M. and Pearce, Jon},
title = {Discovering Activities in Your City Using Transitory Search},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935378},
doi = {10.1145/2935334.2935378},
abstract = {Discovering activities in the city around you can be difficult with traditional search engines unless you know what you are looking for. Searching for inspiration on things to do requires a more open-ended and explorative approach. We introduce transitory search as a dynamic way of uncovering information about activities in the city around you that allows the user to start from a vague idea of what they are interested in, and iteratively modify their search using slider continuums to discover best-fit results. We present the design of a smartphone app exemplifying the idea of transitory search and give results from a lab evaluation and a 4-week field deployment involving 15 people in two different cities. Our findings indicate that transitory search on a mobile device both supports discovering activities in the city and more interestingly helps users reflect on and shape their preferences in situ. We also found that ambiguous slider continuums work well as people happily form and refine individual interpretations of them.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {387–393},
numpages = {7},
keywords = {explorative search, sliders, event finding, transitory search},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3254094,
author = {Leiva, Luis},
title = {Session Details: Wrist and Hand Interaction II},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254094},
doi = {10.1145/3254094},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
numpages = {1},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935375,
author = {Yeo, Hui-Shyong and Lee, Juyoung and Bianchi, Andrea and Quigley, Aaron},
title = {WatchMI: Pressure Touch, Twist and Pan Gesture Input on Unmodified Smartwatches},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935375},
doi = {10.1145/2935334.2935375},
abstract = {The screen size of a smartwatch provides limited space to enable expressive multi-touch input, resulting in a markedly difficult and limited experience. We present WatchMI: Watch Movement Input that enhances touch interaction on a smartwatch to support continuous pressure touch, twist, pan gestures and their combinations. Our novel approach relies on software that analyzes, in real-time, the data from a built-in Inertial Measurement Unit (IMU) in order to determine with great accuracy and different levels of granularity the actions performed by the user, without requiring additional hardware or modification of the watch. We report the results of an evaluation with the system, and demonstrate that the three proposed input interfaces are accurate, noise-resistant, easy to use and can be deployed on a variety of smartwatches. We then showcase the potential of this work with seven different applications including, map navigation, an alarm clock, a music player, pan gesture recognition, text entry, file explorer and controlling remote devices or a game character.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {394–399},
numpages = {6},
keywords = {rich touch, small screen, wearable devices, smart watch},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935373,
author = {Wenig, Dirk and Steenbergen, Alexander and Sch\"{o}ning, Johannes and Hecht, Brent and Malaka, Rainer},
title = {ScrollingHome: Bringing Image-Based Indoor Navigation to Smartwatches},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935373},
doi = {10.1145/2935334.2935373},
abstract = {Providing pedestrian navigation instructions on small screens is a challenging task due to limited screen space. As image-based approaches for navigation have been successfully proven to outperform map-based navigation on mobile devices, we propose to bring image-based navigation to smartwatches. We contribute a straightforward pipeline to easily create image-based indoor navigation instructions that allow users to freely navigate in indoor environments without any localization infrastructure and with minimal user input on the smartwatch. In a user study, we show that our approach outperforms the current state-of-the art application in terms of task completion time, perceived task load and perceived usability. In addition, we did not find an indication that there is a need to provide explicit directional instructions for image-based navigation on small screens.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {400–406},
numpages = {7},
keywords = {pedestrian navigation, mobile maps, stripe maps, smartwatches, cartography},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935355,
author = {Carter, Scott and Marlow, Jennifer and Komori, Aki and M\"{a}kel\"{a}, Ville},
title = {Bringing Mobile into Meetings: Enhancing Distributed Meeting Participation on Smartwatches and Mobile Phones},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935355},
doi = {10.1145/2935334.2935355},
abstract = {Most teleconferencing tools treat users in distributed meetings monolithically: all participants are meant to be interconnected in more-or-less the same manner. In practice, people connect to meetings in different contexts, sometimes sitting in front of a laptop or tablet giving their full attention, but at other times mobile and concurrently involved in other tasks or as a liminal participant in a larger group meeting. In this paper, we present the design and evaluation of two applications, MixMeetWear and MixMeetMate, to help users in non-standard contexts flexibly participate in meetings.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {407–417},
numpages = {11},
keywords = {teleconferencing, mobile, wearable},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935348,
author = {Pfeiffer, Max and Duente, Tim and Rohs, Michael},
title = {Let Your Body Move: A Prototyping Toolkit for Wearable Force Feedback with Electrical Muscle Stimulation},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935348},
doi = {10.1145/2935334.2935348},
abstract = {Electrical muscle stimulation (EMS) is a promising wearable haptic output technology as it can be miniaturized considerably and delivers a wide range of haptic output. However, prototyping EMS applications is challenging. It requires detailed knowledge and skills about hardware, software, and physiological characteristics. To simplify prototyping with EMS in mobile and wearable situations we present the Let Your Body Move toolkit. It consists of (1) a hardware control module with Bluetooth communication that uses off-the-shelf EMS devices as signal generators, (2) a simple communications protocol to connect mobile devices, and (3) a set of control applications as starting points for EMS prototyping. We describe EMS-specific parameters, electrode placements on the skin, and user calibration. The toolkit was evaluated in a workshop with 10 researchers in haptics. The results show that the toolkit allows to quickly generate non-trivial prototypes. The hardware schematics and software components are available as open source software.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {418–427},
numpages = {10},
keywords = {prototyping, EMS, wearable, toolkit, mobile, force feedback, haptic feedback, electrical muscle stimulation},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3254095,
author = {V\"{a}\"{a}n\"{a}nen, Kaisa},
title = {Session Details: Improving Mobile Interaction},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254095},
doi = {10.1145/3254095},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
numpages = {1},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935372,
author = {Akhadov, Sabir and Lancelle, Marcel and Bazin, Jean-Charles and Gross, Markus},
title = {Motion Based Remote Camera Control with Mobile Devices},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935372},
doi = {10.1145/2935334.2935372},
abstract = {With current digital cameras and smartphones, taking photos and videos has never been easier. However, it is still difficult to take a photo of a brief action at the right time. In addition, editing captured videos, such as modifying the playback speed of some parts of a video, remains a time consuming task.In this work we investigate how the motion sensors embedded in mobile devices, such as smartphones, can facilitate camera control. In particular, we show two families of applications: automatic camera trigger control for jump photos and automatic playback speed control (video speed ramping) for action videos. Our approach uses joint devices: a remote camera takes a photo or a video of the scene and it is controlled by the motion sensor of a mobile device, either during or after recording. This allows casual users to achieve visually appealing effects with little effort, even for self portraits.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {428–433},
numpages = {6},
keywords = {motion sensor, remote control, camera trigger, jump photo, video speed ramping},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935381,
author = {Henze, Niels and Funk, Markus and Shirazi, Alireza Sahami},
title = {Software-Reduced Touchscreen Latency},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935381},
doi = {10.1145/2935334.2935381},
abstract = {Devices with touchscreens have an inherent latency. When a user's finger drags an object across the screen the object follows with a latency of around 100ms for current devices. Previous work showed that latencies down to 25ms reduce users' performance and that even 10ms latency is noticeable. In this paper we demonstrate an approach that reduces latency using a predictive model. Extrapolating the finger's movement we predict where the finger will be in the next moment. Comparing different prediction approaches we show for three different tasks that prediction using neural networks is more precise than linear and polynomial extrapolation. Furthermore, we show through a Fitts' Law dragging experiment that reducing touch latency can significantly increases users' performance. As the approach is software-based it can easily be integrated into existing mobile applications and systems.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {434–441},
numpages = {8},
keywords = {lag, latency, touchscreen, prediction, touch input},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935384,
author = {Colley, Ashley and Van Vlaenderen, Wouter and Sch\"{o}ning, Johannes and H\"{a}kkil\"{a}, Jonna},
title = {Changing the Camera-to-Screen Angle to Improve AR Browser Usage},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935384},
doi = {10.1145/2935334.2935384},
abstract = {Mobile devices are currently the most commonly used platform to experience Augmented Reality (AR). Nevertheless, they typically provide a less than ideal ergonomic experience, requiring the user to operate them with arms raised. In this paper we evaluate how to improve the ergonomics of AR experiences by modifying the angle between the mobile device's camera and its display. Whereas current mobile device cameras point out vertically from the back cover, we modify the camera angle to be 0, 45 and 90 degrees. In addition, we also investigate the use of the smartwatch as an AR browser form factor. Key findings are, that whilst the current approximately see-through configuration provides the fastest task completion times, a camera offset angle of 45° provides reduced task load and was preferred by users. When comparing different form factors and screen sizes, the smartwatch format was found to be unsuitable for AR browsing use.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {442–452},
numpages = {11},
keywords = {mobile devices, smartwatches, augmented reality browsers, magic lens interaction, augmented reality},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935388,
author = {Wiese, Oliver and Roth, Volker},
title = {See You next Time: A Model for Modern Shoulder Surfers},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935388},
doi = {10.1145/2935334.2935388},
abstract = {Friends, family and colleagues at work may repeatedly observe how their peers unlock their smartphones. These "insiders" may combine multiple partial observations to form a hypothesis of a target's secret. This changing landscape requires that we update the methods used to assess the security of unlocking mechanisms against human shoulder surfing attacks. In our paper, we introduce a methodology to study shoulder surfing risks in the insider threat model. Our methodology dissects the authentication process into minimal observations by humans. Further processing is based on simulations. The outcome is an estimate of the number of observations needed to break a mechanism. The flexibility of this approach benefits the design of new mechanisms. We demonstrate the application of our methodology by performing an analysis of the SwiPIN scheme published at CHI 2015. Our results indicate that SwiPIN can be defeated reliably by a majority of the population with as few as 6 to 11 observations.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {453–464},
numpages = {12},
keywords = {authentication, mobile devices, insider threats, shoulder surfing},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3254096,
author = {Inkpen, Korin},
title = {Session Details: Observing and Logging},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254096},
doi = {10.1145/3254096},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
numpages = {1},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935383,
author = {Murnane, Elizabeth L. and Abdullah, Saeed and Matthews, Mark and Kay, Matthew and Kientz, Julie A. and Choudhury, Tanzeem and Gay, Geri and Cosley, Dan},
title = {Mobile Manifestations of Alertness: Connecting Biological Rhythms with Patterns of Smartphone App Use},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935383},
doi = {10.1145/2935334.2935383},
abstract = {Our body clock causes considerable variations in our behavioral, mental, and physical processes, including alertness, throughout the day. While much research has studied technology usage patterns, the potential impact of underlying biological processes on these patterns is under-explored. Using data from 20 participants over 40 days, this paper presents the first study to connect patterns of mobile application usage with these contributing biological factors. Among other results, we find that usage patterns vary for individuals with different body clock types, that usage correlates with rhythms of alertness, that app use features such as duration and switching can distinguish periods of low and high alertness, and that app use reflects sleep interruptions as well as sleep duration. We conclude by discussing how our findings inform the design of biologically-friendly technology that can better support personal rhythms of performance.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {465–477},
numpages = {13},
keywords = {alertness, mobile app use, circadian rhythms, sleep},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935357,
author = {Suzuki, Kenji and Okabe, Kazumasa and Sakamoto, Ryuuki and Sakamoto, Daisuke},
title = {Fix and Slide: Caret Navigation with Movable Background},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935357},
doi = {10.1145/2935334.2935357},
abstract = {We present a concept of using a movable background to navigate a caret on small mobile devices. The standard approach to selecting text on mobile devices is to directly touch the location on the text that a user wants to select. This is problematic because the user's finger hides the area to select. Our concept is to use a movable background to navigate the caret. Users place a caret by tapping on the screen and then move the background by touching and dragging. In this method, the caret is fixed on the screen and the user drags the background text to navigate the caret. We compared our technique with the iPhone's default UI and found that even though participants were using our technique for the first time, average task completion time was not different or even faster than Default UI in the case of the small font size and got a significantly higher usability score than Default UI.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {478–482},
numpages = {5},
keywords = {caret, text selection, mobile device, movable background},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935350,
author = {Nasim, Mehwish and Rextin, Aimal and Khan, Numair and Malik, Muhammad Muddassir},
title = {Understanding Call Logs of Smartphone Users for Making Future Calls},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935350},
doi = {10.1145/2935334.2935350},
abstract = {In this measurement study, we analyze whether mobile phone users exhibit temporal regularity in their mobile communication. To this end, we collected a mobile phone usage dataset from a developing country -- Pakistan. The data consists of 783 users and 229, 450 communication events. We found a number of interesting patterns both at the aggregate level and at dyadic level in the data. Some interesting results include: the number of calls to different alters consistently follow the rank-size rule; a communication event between an ego-alter(user-contact) pair greatly increases the chances of another communication event; certain ego-alter pairs tend to communicate more over weekends; ego-alter pairs exhibit autocorrelation in various time quantum. Identifying such idiosyncrasies in the ego-alter communication can help improve the calling experience of smartphone users by automatically (smartly) sorting the call log without any manual intervention.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {483–490},
numpages = {8},
keywords = {time series, mobile phones, temporal patterns, call logs},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935363,
author = {Rosales, Andrea and Fern\'{a}ndez-Ard\`{e}vol, Mireia},
title = {Smartphones, Apps and Older People's Interests: From a Generational Perspective},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935363},
doi = {10.1145/2935334.2935363},
abstract = {It is well-documented that ICT are designed mostly with young users in mind. In addition, most studies about smartphone use do not include older people or even consider age differences. Consequently, little is known about how to design smartphone apps taking older people's interests into account. We have used a mixed-method approach with an intergenerational perspective to approach this topic. First, we track the smartphone activities of 238 panelists. Second, we conduct an online survey (382 respondents). Third, we document the experiences of a group of older people in a smartphone learning club. We have found specific media consumption and communication patterns among older individuals: for example, at home they are more prone to jumping between devices for ergonomic reasons, thus, cross-device interactions are key for this group. We discuss the relevance of intergenerational studies in counterbalancing the spread of age stereotypes and identifying alternative adoption trends.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {491–503},
numpages = {13},
keywords = {use, cross-device, older people, tracking, ageism, mixed-methods, log data, smartphone, age stereotypes},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3254097,
author = {Munteanu, Cosmin},
title = {Session Details: Sociability},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254097},
doi = {10.1145/3254097},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
numpages = {1},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935370,
author = {Cramer, Henriette and de Juan, Paloma and Tetreault, Joel},
title = {Sender-Intended Functions of Emojis in US Messaging},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935370},
doi = {10.1145/2935334.2935370},
abstract = {Emojis are an extremely common occurrence in mobile communications, but their meaning is open to interpretation. We investigate motivations for their usage in mobile messaging in the US. This study asked 228 participants for the last time that they used one or more emojis in a conversational message, and collected that message, along with a description of the emojis' intended meaning and function. We discuss functional distinctions between: adding additional emotional or situational meaning, adjusting tone, making a message more engaging to the recipient, conversation management, and relationship maintenance. We discuss lexical placement within messages, as well as social practices. We show that the social and linguistic function of emojis are complex and varied, and that supporting emojis can facilitate important conversational functions.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {504–509},
numpages = {6},
keywords = {emojis, messaging, mobile},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935382,
author = {Pohl, Henning and Stanke, Dennis and Rohs, Michael},
title = {EmojiZoom: Emoji Entry via Large Overview Maps 😄🔍},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935382},
doi = {10.1145/2935334.2935382},
abstract = {Current soft keyboards for emoji entry all present emoji in the same way: in long lists, spread over several categories. While categories limit the number of emoji in each individual list, the overall number is still so large, that emoji entry is a challenging task. The task takes particularly long if users pick the wrong category when searching for an emoji. Instead, we propose a new zooming keyboard for emoji entry. Here, users can see all emoji at once, aiding in building spatial memory where related emoji are to be found. We compare our zooming emoji keyboard against the Google keyboard and find that our keyboard allows for 18% faster emoji entry, reducing the required time for one emoji from 15.6 s to 12.7 s. A preliminary longitudinal evaluation with three participants showed that emoji entry time over the duration of the study improved at up to 60 % to a final average of 7.5 s.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {510–517},
numpages = {8},
keywords = {text entry, emoji, interaction technique, mobile input, soft keyboard, spatial memory, zooming user interfaces},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935347,
author = {Schulze, Florian and Groh, Georg},
title = {Conversational Context Helps Improve Mobile Notification Management},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935347},
doi = {10.1145/2935334.2935347},
abstract = {We explore if and how identifying the character of face-to-face conversations can help manage notifications on smartphones so that they become less disruptive. We show that the social dimensions depth/importance and formality/goal orientation of a conversation are strong indicators of receptiveness. Furthermore, we find that there are types of conversation, e.g. small talk, in which individuals are even more receptive to notifications than in situations without any verbal social interaction at all. This refutes the assumption currently found in the literature that the occurrence of a conversation is a strong predictor of unavailability. We demonstrate a system that tracks conversations in which the user is engaged and that analyzes speech in terms of embedded affective and social cues. Eventually, we find that information of either kind, derived from audio, improves the accuracy of personal notification preference models substantially.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {518–528},
numpages = {11},
keywords = {conversation-awareness, interruptibility, character of conversation, mobile notification management},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/2935334.2935385,
author = {Jarusriboonchai, Pradthana and Olsson, Thomas and Lyckvi, Sus Lundgren and V\"{a}\"{a}n\"{a}nen, Kaisa},
title = {Let's Take Photos Together: Exploring Asymmetrical Interaction Abilities on Mobile Camera Phones},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935385},
doi = {10.1145/2935334.2935385},
abstract = {Mobile phones have become common tools for photography. Despite the fact that photos are social artifacts, mobile phones afford the act of photo taking only as an individual activity. Photo taking that involves more than one photographer has been envisioned to create positive outcomes and experiences. We implemented this vision with mobile camera phones, exploring how this would influence photo taking practices and experiences. We conducted a user study where altogether 22 participants (11 pairs) were using a novel mobile photography method based on asymmetrical interaction abilities, comparing that with two traditional methods. We present the collaborative practices emerged in different photography methods and report user experience findings particularly with regard to enforced collaboration in mobile photo taking. The results highlight benefits and positive experiences in collaborative photo taking. We discuss lessons learned and point out design implications that come into play when designing for mobile collocated collaboration.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {529–540},
numpages = {12},
keywords = {design research, user experience, collocated interaction, digital photography, photo taking, user study, asymmetry, collaboration},
location = {Florence, Italy},
series = {MobileHCI '16}
}

