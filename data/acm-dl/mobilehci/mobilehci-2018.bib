@inproceedings{10.1145/3236112.3236113,
author = {Poguntke, Romina and Tasci, Cagri and Korhonen, Olli and Alt, Florian and Schneegass, Stefan},
title = {AVotar: Exploring Personalized Avatars for Mobile Interaction with Public Displays},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236113},
doi = {10.1145/3236112.3236113},
abstract = {Engaging users with public displays has been a major challenge in public display research. Interactive displays often suffer from being ignored by potential users. Research showed that user representations are a valid way to partially address this challenge, e.g., by attracting attention, conveying interactivity, and serving as entry points to gestures and touch interaction. We believe that user representations, particularly personalized avatars, could further increase the attractiveness of public displays, if carefully designed. In this work, we provide first insights on how such avatars can be designed and which properties are important for users. In particular, we present AVotar, a voting application for mobiles that lets users design avatars being utilized to represent them. In an user study we found that users appreciate high degrees of freedom in customization and focus on expressive facial features. Finally, we discuss the findings yielding useful implications for designers of future public display applications employing avatars.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1–8},
numpages = {8},
keywords = {avatars, public displays, engagement, personalization, user representation},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236114,
author = {Poguntke, Romina and Kiss, Francisco and Kaplan, Ayhan and Schmidt, Albrecht and Schneegass, Stefan},
title = {RainSense: Exploring the Concept of a Sense for Weather Awareness},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236114},
doi = {10.1145/3236112.3236114},
abstract = {The amplification of human senses has been in the focus of contemporary research for the past decades. Apart from the replication of human organs, the functionality of the human body has been enhanced. While many approaches aim to augment existing sensory channels, our research purpose is to explore the creation of a new sense, namely a sense for weather awareness. For this, we present our concept which is based on the presentation of thermal stimuli. Hence, we initially explored the perception and suitability of thermal feedback stimuli to communicate weather information, and particularly precipitation in an experiment comprising 16 participants. From the qualitative and quantitative results we derive important findings helping us to advance the realization of our concept in future research involving a field study to further evaluate the creation of a sense for weather awareness.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {9–15},
numpages = {7},
keywords = {thermal feedback, sense enhancement, wearable computing, augmented senses},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236115,
author = {Gruenefeld, Uwe and Stratmann, Tim Claudius and Pr\"{a}del, Lars and Heuten, Wilko},
title = {MonoculAR: A Radial Light Display to Point towards out-of-View Objects on Augmented Reality Devices},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236115},
doi = {10.1145/3236112.3236115},
abstract = {Present head-mounted displays (HMDs) for Augmented Reality (AR) devices have narrow fields-of-view (FOV). The narrow FOV further decreases the already limited human visual range and worsens the problem of objects going out of view. Therefore, we explore the utility of augmenting head-mounted AR devices with MonoculAR, a peripheral light display comprised of twelve radially positioned light cues, to point towards out-of-view objects. In this work, we present two implementations of MonoculAR: (1) On-screen virtual light cues and (2) Off-screen LEDs. In a controlled user study we compare both approaches and evaluate search time performance for locating out-of-view objects in AR on the Microsoft Hololens. Key results show that participants find out-of-view objects faster when the light cues are presented on the screen. Furthermore, we provide implications for building peripheral HMDs.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {16–22},
numpages = {7},
keywords = {out-of-view, head-mounted displays, augmented reality, small field-of-view, peripheral display},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236116,
author = {Ungurean, Ovidiu-Ciprian and Vatavu, Radu-Daniel and Leiva, Luis A. and Mart\'{\i}n-Albo, Daniel},
title = {Predicting Stroke Gesture Input Performance for Users with Motor Impairments},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236116},
doi = {10.1145/3236112.3236116},
abstract = {The performance of users with motor impairments with stroke gesture input on touchscreens has been little examined so far, despite the wide prevalence of mobile devices and the benefits they bring to increase users' quality of life. In this work, we present the first empirical results on this subject matter from 915 gestures collected from 10 participants with motor impairments (spastic tetraplegia and tetraparesis) and 10 participants without known impairments. We report that different motor abilities lead to different performance in terms of gesture production time. We also show that the production times of gestures articulated by users with motor impairments can be accurately predicted with an absolute error of just 150 ms and a relative error of only 3.7% with respect to actual times (user-independent tests), a result that will enable designers to estimate human performance a priori when prototyping gesture UIs for users with motor impairments.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {23–30},
numpages = {8},
keywords = {stroke gestures, production time, touchscreens, touch gestures, motor impairments, gesture performance},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236117,
author = {Mozgovoy, Maxim and Pyshkin, Evgeny},
title = {Mobile Farm for Software Testing},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236117},
doi = {10.1145/3236112.3236117},
abstract = {We introduce an approach to user interface testing with a particular focus on non-native GUI based mobile applications. We particularly address the domain of entertainment and education software including mobile games. We describe a prototype system based on inexpensive components and open source software, intended to support product development cycle for companies on lean budget. On the base of a prototype system discussed in this paper we expect to develop a distributed infrastructure that would allow users to use facilities of users' own computers and connected devices as a part of a common testing framework. The approach presented in this work is also suitable for wider range of mobile applications with a high variety of human-computer interaction mechanisms.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {31–38},
numpages = {8},
keywords = {automated testing framework, time-consuming test suites, mobile application, non-native GUI, smoke testing},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236118,
author = {Kiss, Francisco and Poguntke, Romina and Schmidt, Albrecht and Woundefinedniak, Pawe\l{} W.},
title = {S5: Selective Sensing of Single Sound Sources},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236118},
doi = {10.1145/3236112.3236118},
abstract = {The sense of hearing provides humans with information about their surroundings and is the primary means of communication, yet it is limited in its ability to focus on particular stimuli. To provide this ability, we designed and built S5, a mobile proof-of-concept prototype that allows Selective Sensing of Single Sound Sources. Our design consists of a head-mounted directional microphone attached to a smart-phone, which acts as controller, filter and amplifier. Users hear the selective signal through headphones and activate the device by touching their ear. To evaluate this sensory augmentation, we conducted a study with 16 participants that showed the system was appealing and perceived as useful. Based on our findings, we conclude that the proposed augmentation is feasible and we provide insights for further development of the concept.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {39–46},
numpages = {8},
keywords = {interfaces, augmented hearing},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236119,
author = {Prandi, Catia and Nisi, Valentina and Villaflor, RJ and Liao, Stephanie and Best, Bria and Gavina, Victor and Nunes, Nuno},
title = {On Designing a Way-Finding System to Assist Users with Respiratory Ailments and Track Symptoms},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236119},
doi = {10.1145/3236112.3236119},
abstract = {Asthma is a complex disease caused by genetic and environmental factors, affecting 235 million people worldwide. The effects of chronic respiratory diseases, such as asthma and respiratory ailments, can be particularly aggressive in islands, due to the optimal climate conditions and the rich nature and flora. Conceived as a case study on Madeira island, we applied user-driven innovation strategy to inform the design of a mobile app able to assist the users in tracking asthma symptoms through an intuitive interface and a smart analog button, providing real-time personalized recommendations. The app is the result of a workshop conducted with 28 17-year-old students from Madeira. The workshop goal was to involve users in thinking about how to exploit data about air quality and weather conditions collected by a pervasive low-cost infrastructure spreads across the island.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {47–54},
numpages = {8},
keywords = {user-driven innovation, crowdsourcing, asthma, personalized paths, pervasive sensing infrastructure},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236120,
author = {von Jan, Vanessa and Bertel, Sven and Hornecker, Eva},
title = {Information Push and Pull in Tactile Pedestrian Navigation Support},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236120},
doi = {10.1145/3236112.3236120},
abstract = {For pedestrian navigation support, we report on how the feeling of being in control about receiving updates impacts navigation efficiency and user experience. In an exploratory field study, 24 participants navigated to previously unknown targets using a wristband which conveyed tactile information about targets' bearing. Information was either pulled by the user at times of her choosing via a simple arm gesture, or was pushed by the armband at a regular, preset interval. While the push mode resulted in higher efficiency, more users preferred actively pulling information, possibly as this afforded feeling more in control. Interestingly, mode preference was independent of individual navigation ability. Results suggest that properties of the specific navigation context should be used to determine whether an interface offers push or pull modes for navigation support.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {55–62},
numpages = {8},
keywords = {urban navigation, mobile interface, tactile interface, user control},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236121,
author = {Patel, Dilisha and Blandford, Ann and Stephenson, Judith},
title = {So You're Planning a Baby? A Review of Preconception Care Apps},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236121},
doi = {10.1145/3236112.3236121},
abstract = {Previous research shows couples favour online information sources when seeking support for preconception health and pregnancy planning, with mobile applications (apps) becoming increasingly popular. This study aimed to establish what smartphone apps currently exist to support couples when preparing for pregnancy. A functionality review was conducted to explore app content and an analysis of user reviews was undertaken to investigate user views towards existing apps. 25 apps were analysed, which provided information on diet, weight, alcohol, smoking and caffeine amongst others. Positive reviews mainly referred to the helpfulness of the app. Negative comments focused on the over simplification of information. Overall, user comments showed a positive response towards existing preconception care apps, but users reported concerns towards information accuracy and reliability. Further work will be undertaken to evaluate whether existing apps engage users to improve their preconception health care and whether these apps fulfils user requirements.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {63–70},
numpages = {8},
keywords = {user reviews, mhealth, apps, preconception care},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236122,
author = {K\"{u}hn, Romina and Schlegel, Thomas},
title = {Mixed-Focus Collaboration Activities for Designing Mobile Interactions},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236122},
doi = {10.1145/3236112.3236122},
abstract = {Collaboration is essential in both the working and the learning context. Although there is a wide range of collaboration tools and technology, they mostly focus on one specific application and thus are seldom reusable for other domains. To provide reusable solutions, it seems to be reasonable to develop not for one explicit application but for recurring activities that collaborators perform. We identified typical collaborative activities in mixed-focus collaboration, e.g., creating content or presenting results, which we derived from literature survey, observation and existing tools. Collaboration activities aim to classify solutions and enable designers and developers to reuse them for other scenarios. We illustrate this approach with mobile interaction techniques for two collaboration activities. The presented interactions address collaborative comparing and sharing and are applicable for various scenarios. This approach extends other collaboration processes to focus on recurring activities and improve reusability of tools.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {71–78},
numpages = {8},
keywords = {mixed-focus collaboration, mobile device-based interactions, collaboration activities},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236123,
author = {Micallef, Nicholas and Misra, Gaurav},
title = {Towards Designing a Mobile App That Creates Avatars for Privacy Protection},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236123},
doi = {10.1145/3236112.3236123},
abstract = {Online forms often require users to provide a lot of personal information when registering for services, which puts their privacy at risk. While recent legislation has focused on how personal data is handled by organizations, the recent Cambridge Analytica revelations expose the limitations of relying on organizations to adhere to legislation or even their own privacy policies. In this research, we tackle this problem by taking the first steps towards providing users with more control over their personal data when registering for services. We employ a user-centered approach to design a privacy protection app, which, through the use of avatars, would provide users with greater control and flexibility over the personal information they disclose during online registrations. This paper reports on a set of design findings and observations extracted from a series of design workshops conducted to identify the design elements users would prefer in this novel privacy protection app.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {79–86},
numpages = {8},
keywords = {user-centered design, usable privacy, privacy protection, avatars},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236124,
author = {Ejaz, Hira and Hussain, Syed Ali and Raza, Agha Ali},
title = {The Case for IVR-Based Citizen Journalism in Pakistan},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236124},
doi = {10.1145/3236112.3236124},
abstract = {Freedom of expression is a fundamental human right. Unfortunately, this right gets denied to the majority of people because they cannot read and write. This is because most modern means of communication rely on textual interfaces that are not inclusive to less educated and visually impaired people. However, simple and feature mobile phones are becoming widely available to such populations. In this paper, we present Mehfil, an IVR based citizen journalism platform that was deployed in Pakistan for 41 days. It received 789 calls from 535 users (2.4% of them blind) from all provinces of Pakistan. Mehfil provides a platform to its users to report their local area problems by recording their grievances on a range of social issues including unemployment, personal safety, health, education, corruption and rights of disabled (especially visually impaired). This paper reveals a demand for mobile phone-based citizen journalism and grievance reporting platforms among low-literate people in Pakistan.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {87–94},
numpages = {8},
keywords = {IVR, low literate, speech interfaces, citizen journalism, social concerns, visually impaired},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236125,
author = {Forlastro, Gianni and Gena, Cristina and Chiesa, Ilaria and Cietto, Valerio},
title = {IoT for the Circular Economy: The Case of a Mobile Set for Video-Makers},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236125},
doi = {10.1145/3236112.3236125},
abstract = {This paper describes I-Set, a mobile set for video-makers, able to transform traditional video equipment into smart objects, and making possible to control the equipment and the shooting through a mobile application. I-Set has been designed according to circular economy principles, giving users the possibility of installing the modules on existing equipment, and of printing its part using recyclable materials. I-Set follows a user centered design approach, involving final users in all the project phases.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {95–102},
numpages = {8},
keywords = {smart objects, IoT, circular economy},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236126,
author = {Railean, Alexandr and Reinhardt, Delphine},
title = {Let There Be LITE: Design and Evaluation of a Label for IoT Transparency Enhancement},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236126},
doi = {10.1145/3236112.3236126},
abstract = {We present a "privacy facts" label, which aims at helping non-experts understand how an Internet of Things (IoT) device collects and handles data. We describe our design methodology, and detail the results of our user study involving 31 participants, assessing the efficacy of the label. The results suggest that the label was perceived positively by the participants, and is a promising solution to help users in making informed decisions.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {103–110},
numpages = {8},
keywords = {privacy, internet of things, IoT, usability, label},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236127,
author = {Schwind, Valentin and Reinhardt, Jens and Rzayev, Rufat and Henze, Niels and Wolf, Katrin},
title = {Virtual Reality on the Go? A Study on Social Acceptance of VR Glasses},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236127},
doi = {10.1145/3236112.3236127},
abstract = {Virtual reality (VR) glasses enable to be present in an environment while the own physical body is located in another place. Recent mobile VR glasses enable users to be present in any environment they want at any time and physical place. Still, mobile VR glasses are rarely used. One explanation is that it is not considered socially acceptable to immerse in another environment in certain situations. We conducted an online experiment that investigates the social acceptance of VR glasses in six different contexts. Our results confirm that social acceptability depends on the situation. In the bed, in the metro, or in a train, mobile VR glasses seem to be acceptable. However, while being surrounded by other people where a social interaction between people is expected, such as in a living room or a public cafe, the acceptance of mobile VR glasses is significantly reduced. If one or two persons wear glasses seems to have a negligible effect. We conclude that social acceptability of VR glasses depends on the situation and is lower when the user is supposed to interact with surrounding people.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {111–118},
numpages = {8},
keywords = {social acceptance, virtual reality glasses, virtual reality},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236128,
author = {Baldauf, Matthias and B\"{o}sch, Raffael and Frei, Christian and Hautle, Fabian and Jenny, Marc},
title = {Exploring Requirements and Opportunities of Conversational User Interfaces for the Cognitively Impaired},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236128},
doi = {10.1145/3236112.3236128},
abstract = {Interacting with traditional user interfaces can be challenging for people with cognitive impairments. Speech-based conversational interfaces and virtual assistants such as Amazon's Alexa and Apple's Siri might provide great potential for this user group. Yet, knowledge about how cognitively impaired perceive such conversational interfaces and about the special requirements and opportunities is scarce. To gain first insights, we conducted a group interview with participants with mild to moderate cognitive impairments. They expressed their high expectations and imagined mobile conversational assistants for controlling vending machines, for example. Yet, they emphasized that smart assistants must not replace but rather complement personal contact with humans.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {119–126},
numpages = {8},
keywords = {cognitive impairment, voice interface, conversational interface, virtual assistant},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236130,
author = {Pyae, Aung and Joelsson, Tapani N.},
title = {Investigating the Usability and User Experiences of Voice User Interface: A Case of Google Home Smart Speaker},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236130},
doi = {10.1145/3236112.3236130},
abstract = {Recently, commercial Voice User Interfaces (VUIs) have been introduced to the market (e.g. Amazon Echo and Google Home). Although they have drawn much attention from users, little is known about their usability, user experiences, and usefulness. In this study, we conducted a web-based survey to investigate usability, user experiences, and usefulness of the Google Home smart speaker. A total of 114 users, who are active in a social-media based interest group, participated in the study. The findings show that the Google Home is usable and user-friendly for users, and shows the potential for international users. Based on the users' feedback, we identified the challenges encountered by the participants. The findings from this study can be insightful for researchers and developers to take into account for future research in VUI.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {127–131},
numpages = {5},
keywords = {user experience, usability, voice user interfaces},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236131,
author = {Abeer, Ifti Azad and Saha, Anik and Sinha, Anik and Ahmed, Nova},
title = {Reinforcement Based Learning through Communicative Android App for Autism Personnel},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236131},
doi = {10.1145/3236112.3236131},
abstract = {Autism is a rapidly growing problem all over the world. But in the developing countries like Bangladesh, the technological support for Autism is not adequate. The acutest problems children with autism faces are the communication problem. An Android app was developed to meet these needs of the children with autism. Study on the user feedbacks of the survey conducted about the review of the app suggested a very important integration to the app. The process of learning through the app can be made much more efficient by involving the parents of the autism personnel. The emotional attachment towards parents is a great fulcrum to teach autism personnel. This paper gives an overview of the development of the Android App and the integrated part of reinforcement based learning. To let the parents, aid the learning process, we have kept options for them to customize the app contents.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {132–138},
numpages = {7},
keywords = {android app, bengali language, autism, communicative app},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236132,
author = {Exposito, Marc and Hernandez, Javier and Picard, Rosalind W.},
title = {Affective Keys: Towards Unobtrusive Stress Sensing of Smartphone Users},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236132},
doi = {10.1145/3236112.3236132},
abstract = {This work explores the use of pressure-sensing to capture cues of the stress of smartphone users while typing. In a controlled laboratory study, 11 participants were asked to write about a recent stressful and relaxing experience in counterbalanced order. Preliminary results show a significant positive correlation between the increase in typing pressure and self-reported stress across the two conditions (r=0.75, p=0.0081). In addition, we observed a significant negative correlation between the typing pressure baseline and the self-reported stress (r=-0.74, p=0.0093). These findings can help inform the development of less invasive methods for stress measurement.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {139–145},
numpages = {7},
keywords = {stress measurement, pressure-sensitive keyboard, affective computing, 3D touch},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236133,
author = {Ku, Pin-Sung and Wu, Te-Yen and Bastias, Ericka Andrea Valladares and Chen, Mike Y.},
title = {Wink It: Investigating Wink-Based Interactions for Smartphones},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236133},
doi = {10.1145/3236112.3236133},
abstract = {Commodity mobile devices have front-facing cameras that can be used to precisely track facial expressions, such as winking, which can provide an additional input modality that co-exists with touchscreen input. We evaluate and compare three types of wink-based interactions: single wink, double wink, and long wink, in three mobile usage scenarios: sitting, walking, and lying down. Results show that single wink has similar error rate as touch input, and is preferred over touch input for targets in corner regions on a smartphone.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {146–150},
numpages = {5},
keywords = {eye expressions, blink interactions, eye muscle movement},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236134,
author = {Hafiz, Pegah and Maharjan, Raju and Kumar, Devender},
title = {Usability of a Mood Assessment Smartphone Prototype Based on Humor Appreciation},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236134},
doi = {10.1145/3236112.3236134},
abstract = {Humor appreciation is one of the determinants of individual's mood and can be assessed through jokes. We have developed a functional prototype called Humoris which asks users to select the funniest punchline and register their affective response to the jokes. Based on users' responses, the application predicts and displays their short-term mood using emoticons. Our smartphone prototype is evaluated using the 'think-aloud' method with 9 participants. Usability of Humoris was examined by System Usability Scale questionnaire which gave an average score of 79.44 (SD=8.08). Based on our findings, participants liked the application interface as well as the mood prediction but some of them found some jokes difficult to understand.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {151–157},
numpages = {7},
keywords = {humor, mood assessment, smartphone prototype, usability, mobile, affective response},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236135,
author = {Luo, Elle and Vega, Katia},
title = {Scentery: A Calming Multisensory Environment by Mixing Virtual Reality, Sound, and Scent},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236135},
doi = {10.1145/3236112.3236135},
abstract = {Scentery proposes a novel approach to create calming multisensory environments by displaying visualizations, reproducing audios and activating olfactory sensations. By the use of recent literature, we introduce an initial Emotive Design Taxonomy that intersects emotions, and colors, sounds and scents. Scentery's users switch between different multisensory scenarios that promote calm sensation. The first VR scenario immerses the user into the scenery of lavender field, which bursts into a carnival of purple, a lavender scent and ambient instrumental sound. The other scenario is the scenery of raining forest, a ylang-ylang scent and nature sound. Scentery was developed with Unity 3D for creating the 3D scenarios, Unity Remote for the camera control and viewer's perspectives, and a microcontroller for triggering the scents in the vaporizer.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {158–165},
numpages = {8},
keywords = {virtual reality, olfactory, stress, auditory, multisensory},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236136,
author = {Ganapathi, Priya and Sorathia, Keyur},
title = {Investigating Controller Less Input Methods for Smartphone Based Virtual Reality Platforms},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236136},
doi = {10.1145/3236112.3236136},
abstract = {With the advent of the low-cost headsets, Virtual Reality (VR) applications have gained enormous attention and popularity. Object selection is the primary task in VR interfaces. Although numerous studies have been done for the selection of objects, there is limited understanding of how the input methods affect the object selection in sparse, dense and occluded dense Virtual Environment (VE). In this paper, we investigate the commonly used input methods (capacitive touch and dwell gaze) for a smartphone-based VR head-mounted device. We evaluate the task completion time, error rates, ease of use, comfort and user preference correlated to the proposed VEs. The results indicated that capacitive touch was a preferred method of selection in dense and occluded dense VEs. The users preferred dwell gaze in the sparsely populated VE.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {166–173},
numpages = {8},
keywords = {input methods, capacitive touch, dwell gaze, virtual environment, object selection, virtual reality},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236137,
author = {Park, Hyanghee and Eun, Jinsu and Lee, Joonhwan},
title = {Why Do Smartphone Users Hesitate to Delete Unused Apps?},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236137},
doi = {10.1145/3236112.3236137},
abstract = {The process of installing and removing smartphone apps is simple, but choosing which apps to install or remove requires users' time and effort because users need to categorize and think about which apps they will still use and not use based on their memory and behavior. Although there have been several studies on app recommendation, there is relatively a sparse understanding on what motivates users to take an action to delete apps which they no longer use. In this study, we aim to investigate (1) whether users feel necessary to remove smartphone apps. If so, (2) why, when users make a decision to delete installed apps, and which types of apps are deleted. Also, we suggest (3) five categories of burdens that smartphone users may feel when they attempt to delete their installed apps. Finally, this paper suggests how to automate and develop a system of smartphone app removal.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {174–181},
numpages = {8},
keywords = {app deletion, app organization, app usage, behavioral pattern, smartphone, usage pattern},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236138,
author = {Motahar, Tamanna and Fatema, Taskin and Das, Rajdeep},
title = {Bornomala AR-Bengali Learning Experience Using Augmented Reality},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236138},
doi = {10.1145/3236112.3236138},
abstract = {Bornomala AR is an Augmented Reality Application for Android devices which provides a better and easy way to learn Bengali alphabets. This application is developed targeting the children of age 3 to 5 of Bangladesh, to make them more familiar with their mother language. We present a qualitative research where we have measured the learning efficiency with this application of 20 kids from two preschools in Dhaka, Bangladesh. It has been found that, without the application, the average learning efficiency is 41.67% per week whereas with this application the learning efficiency becomes 58.33% per week with 17% improvement.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {182–188},
numpages = {7},
keywords = {developing country, education, augmented reality, bangla language, android application},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236139,
author = {Diamond, Lisa and Schrammel, Johann and Fr\"{o}hlich, Peter and Regal, Georg and Tscheligi, Manfred},
title = {Privacy in the Smart Grid: End-User Concerns and Requirements},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236139},
doi = {10.1145/3236112.3236139},
abstract = {Mobile interfaces will be central in connecting end-users to the smart grid and enabling their active participation. Services and features supporting this participation do, however, rely on high-frequency collection and transmission of energy usage data by smart meters which is privacy-sensitive. The successful communication of privacy to end-users via consumer interfaces will therefore be crucial to ensure smart meter acceptance and consequently enable participation. Current understanding of user privacy concerns in this context is not very differentiated, and user privacy requirements have received little attention. A preliminary user questionnaire study was conducted to gain a more detailed understanding of the differing perceptions of various privacy risks and the relative importance of different privacy-ensuring measures. The results underline the significance of open communication, restraint in data collection and usage, user control, transparency, communication of security measures, and a good customer relationship.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {189–196},
numpages = {8},
keywords = {smart grid, privacy, user requirements},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236140,
author = {Fabiano, Federico and Sanchez-Francisco, Monica and D\'{\i}az, Paloma and Aedo, Ignacio},
title = {Evaluating a Pervasive Game for Urban Awareness},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236140},
doi = {10.1145/3236112.3236140},
abstract = {The combination of augmented reality and pervasive games in urban environments can be exploited to enable situated and informal learning through ludic and engaging activities. In this research, we explore how to use such technologies to improve citizens' awareness about their urban environment by means of an AR pervasive game to learn about a specific urban space. Since the game is pervasively played in a physical space, many usability issues need to be assessed before evaluating whether it can engage citizens whilst promoting some sense of urban awareness. In this paper, we introduce a usability study of the application and a preliminary set of factors to be analysed to assess user engagement.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {197–204},
numpages = {8},
keywords = {user engagement, urban environment, playable cities, augmented reality, pervasive games, usability},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236141,
author = {Rodrigues, Andr\'{e} and Camacho, Leonardo and Nicolau, Hugo and Montague, Kyle and Guerreiro, Tiago},
title = {Aidme: Interactive Non-Visual Smartphone Tutorials},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236141},
doi = {10.1145/3236112.3236141},
abstract = {The constant barrage of updates and novel applications to explore creates a ceaseless cycle of new layouts and interaction methods that we must adapt to. One way to address these challenges is through in-context interactive tutorials. Most applications provide onboarding tutorials using visual metaphors to guide the user through the core features available. However, these tutorials are limited in their scope and are often inaccessible to blind people. In this paper, we present AidMe, a system-wide authoring and playthrough of non-visual interactive tutorials. Tutorials are created via user demonstration and narration. Using AidMe, in a user study with 11 blind participants we identified issues with instruction delivery and user guidance providing insights into the development of accessible interactive non-visual tutorials.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {205–212},
numpages = {8},
keywords = {smartphone, accessibility, assistance, tutorials, blind},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236142,
author = {Kattenbeck, Markus and Kilian, Melanie A. and Ferstl, Matthias and Alt, Florian and Ludwig, Bernd},
title = {Airbot: Using a Work Flow Model for Proactive Assistance in Public Spaces},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236142},
doi = {10.1145/3236112.3236142},
abstract = {We investigate how a task-sensitive personal assistant on smartphones can support users in public space. Therefore, we designed, implemented, and evaluated airbot, a mobile chatbot providing air travelers with proactive information during flight relevant tasks. We tested the application on passengers at a major airport (N = 101). The results of our evaluation study suggest, firstly, that airbot's utility is acknowledged by its users and, secondly, that its use affects the perception of passengers' airport service experience, both positively and negatively.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {213–220},
numpages = {8},
keywords = {assistance system, human-computer interaction, cooperative problem solving, mobile information needs},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236143,
author = {Komninos, Andreas and Dunlop, Mark and Katsaris, Kyriakos and Garofalakis, John},
title = {A Glimpse of Mobile Text Entry Errors and Corrective Behaviour in the Wild},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236143},
doi = {10.1145/3236112.3236143},
abstract = {Research in mobile text entry has long focused on speed and input errors during lab studies. However, little is known about how input errors emerge in real-world situations or how users deal with these. We present findings from an in-the-wild study of everyday text entry and discuss their implications for future studies.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {221–228},
numpages = {8},
keywords = {smartphones, field study, text entry, error behaviour},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236144,
author = {Salai, Ana-Maria and Baillie, Lynne},
title = {Exploring the Key Design Functions of an Overactive Bladder Application},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236144},
doi = {10.1145/3236112.3236144},
abstract = {Even though Overactive Bladder is a treatable condition (75% of cases can be cured or ameliorated), it remains mostly undertreated due to embarrassment, lack of knowledge or misperceptions. In this paper, we investigate what are the key features of a mobile phone application that could help people with Overactive Bladder symptoms adhere to a self-managed rehabilitation program. We also investigate what methods are appropriate to use with end users with an embarrassing condition. Our results show that it is important to include all stakeholders (health professionals and end users) in the iterative design process as contradictions were found between and within the stakeholders.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {229–236},
numpages = {8},
keywords = {rehabilitation, assistive technology, overactive bladder, design interviews, mobile health},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236145,
author = {Magnusson, Charlotte and Rassmus-Gr\"{o}hn, Kirsten and Rydeman, Bitte and Caltenco, H\'{e}ctor},
title = {Walk after Stroke: Initial Development of a Step Counting Game for Stroke Survivors},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236145},
doi = {10.1145/3236112.3236145},
abstract = {This paper presents work done within the EU project STARR. Within the framework of the project technologies to empower and support stroke survivors are developed. We report on the iterative development of an outdoor activity game for stroke survivors, and discuss design choices, experiences from the initial testing and outline potential future developments.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {237–244},
numpages = {8},
keywords = {stroke, mobile game, co-design, activity game},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236146,
author = {Panizzi, Emanuele and Calvitti, Daniele},
title = {A Framework to Enhance the User Experience of Car Mobile Applications},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236146},
doi = {10.1145/3236112.3236146},
abstract = {We describe the rationale, the design and development of an iOS framework to enhance the end user experience of car-related apps. The car framework fuses the smartphone sensors' raw data to detect user behaviour and car events, and provides relevant and timely information that an app can use to relief the user from manually inputting data and to foster implicit interaction. Thus, app developers can focus on user needs and UX rather than on code complexity. We show how detected events map common needs of car-related apps.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {245–252},
numpages = {8},
keywords = {app, sensors, smartphone, framework, parking, car},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236147,
author = {Polgar, Sylvia and Mychajliw, Christian and Wortha, Franz and Ninaus, Manuel and Holz, Heiko},
title = {Towards the Development of a Tablet-Based Screening for Cognitive Symptoms of Neuropsychiatric Disorders},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236147},
doi = {10.1145/3236112.3236147},
abstract = {Tablet-based screenings have been shown to enhance diagnostics of symptoms of neuropsychiatric disorders such as Alzheimer's or Parkinsons by providing more diagnostic data through digital media in addition to conventional paper-and-pencil tests. However, user acceptance/experience with older patients have not been systematically researched in this context. We present the design and evaluation of a tablet-based prototype of a neuropsychiatric screening of cognitive symptoms. After developing two layouts, one identical to the original pencil-and-paper test (TI), one optimized for tablets (TO), the tablet-based versions were compared with the original version in a user test (n = 20). Results showed user acceptance/experience to be positive for all versions and TO favored over the other versions. Test-retest reliability was maintained in the tablet-based versions and differences in digitizer features between healthy and cognitively impaired participants were explored.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {253–260},
numpages = {8},
keywords = {neuropsychiatric diseases, tablet-based screening, mhealth},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236148,
author = {Hesenius, Marc and B\"{o}rsting, Ingo and Meyer, Ole and Gruhn, Volker},
title = {Don't Panic! Guiding Pedestrians in Autonomous Traffic with Augmented Reality},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236148},
doi = {10.1145/3236112.3236148},
abstract = {Recent advances allow for driverless cars that do not rely on necessary support systems for humans such as traffic lights. However, humans will still walk the streets in the future, thus we must think about integrating pedestrians into a system of completely autonomous vehicles. We developed several prototypes for mobile UIs based on three navigational concepts showing how future navigational systems for pedestrians within automated traffic systems may display information via augmented reality. In a small qualitative study, we gathered first feedback to determine future working directions. We found that our participants liked information about autonomous vehicles, their intents, and safe zones, but were concerned with information overload.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {261–268},
numpages = {8},
keywords = {navigation, guidance, pedestrians, autonomous traffic, augmented reality},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236149,
author = {Murad, Christine and Munteanu, Cosmin and Clark, Leigh and Cowan, Benjamin R.},
title = {Design Guidelines for Hands-Free Speech Interaction},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236149},
doi = {10.1145/3236112.3236149},
abstract = {As research on speech interfaces continues to grow in the field of HCI, there is a need to develop design guidelines that help solve usability and learnability issues that exist in hands-free speech interfaces. While several sets of established guidelines for GUIs exist, an equivalent set of principles for speech interfaces does not exist. This is critical as speech interfaces are so widely used in a mobile context, which in itself evolved with respect to design guidelines as the field matured. We explore design guidelines for GUIs and analyze how these are applicable to speech interfaces. For this we identified 21 papers that reflect on the challenges of designing (predominantly mobile) voice interfaces. We present an investigation of how GUI design principles apply to such hands-free interfaces. We discuss how this can serve as the foundation for a taxonomy of design guidelines for hands-free speech interfaces.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {269–276},
numpages = {8},
keywords = {conversational interfaces, design guidelines, speech interaction, voice user interfaces},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236150,
author = {Constantinides, Argyris and Fidas, Christos and Belk, Marios and Samaras, George},
title = {On Sociocultural-Centered Graphical Passwords: An Initial Framework},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236150},
doi = {10.1145/3236112.3236150},
abstract = {Graphical user authentication schemes typically require users to draw a secret on a background image or select images on a grid. Although it is known that various image-related attributes affect security and memorability of generated passwords, current state-of-the-art approaches deliver image-content either randomly or based on the end-users' selections. Motivated by sociocultural theories which underpin that the meaning of an image varies across different people depending on their sociocultural background and experiences, in this paper we elaborate on a multi-layer image-content delivery approach which is supported by an initial framework that targets to deliver background images tailored to the unique sociocultural experiences of users. By doing so, we aim to trigger the users' sociocultural episodic memories, and ultimately help the creation of more secure and memorable passwords. Initial experimental results related to the value of this approach are also presented.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {277–284},
numpages = {8},
keywords = {memorability, user authentication, episodic memory, security, sociocultural context, graphical passwords},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236151,
author = {von Niman, Bruno and B\"{o}cker, Martin and Boveda, Angel},
title = {Develop a User-Centered, Accessible and Harmonized Terminology for Mobile ICT - Freely Available in Five Languages - with Us!},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236151},
doi = {10.1145/3236112.3236151},
abstract = {Users, unfamiliar with the terminology, technical meaning or intended functionality of mobile ICT may be reluctant to use them and miss out on potential benefits. This also prevents from exploiting the true potential of ICT and hinders the uptake and use of services, including those of societal relevance. We present an alternative focusing on improving the overall user experience and accessibility through the provision of recommendations for a harmonized terminology, covering basic, commonly used ICT features in English, French, German, Italian and Spanish. Technical Committee Human Factors (TC HF) of the European Telecommunication Standards Institute (ETSI) has initiated this ongoing work, to develop a freely available to be published in August 2019.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {285–290},
numpages = {6},
keywords = {user experience, user-centered, accessibility, harmonization, terminology, usability},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236154,
author = {Nishiyama, Yuuki and Dey, Anind K. and Ferreira, Denzil and Yonezawa, Takuro and Nakazawa, Jin},
title = {Senbay: A Platform for Instantly Capturing, Integrating, and Restreaming of Synchronized Multiple Sensor-Data Stream},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236154},
doi = {10.1145/3236112.3236154},
abstract = {The spread of smartphones allows us to freely capture video and diverse hardware sensors' data (e.g., accel erometer, gyroscope). While recording such data is relatively simple, it is often challenging to share and restream this data to other people and applications. Such capability is very valuable for a range of applications such as a context-aware prototyping/developing platform, an integrated data recording and analysis tool, and a sensor-data based video editing system. To enable such complex operations, we propose Senbay, a platform for instant sensing, integrating, and restreaming multiple-sensor data streams. The platform embeds collected sensor data into a video frame using an animated two-dimensional barcode via real-time video processing. The video-embedded sensor data, dubbed Senbay Video, can be easily restreamed to other people and reused by data rich, context-aware applications.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {291},
numpages = {1},
keywords = {sensor-data embedded video, extensible platform, multiple sensor-data streams, data restreaming},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236155,
author = {Opoku Asare, Kennedy and Leikanger, Tore and Schuss, Christian and Klakegg, Simon and Visuri, Aku and Ferreira, Denzil},
title = {S3: Environmental Fingerprinting with a Credit Card-Sized NFC Powered Sensor Board},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236155},
doi = {10.1145/3236112.3236155},
abstract = {People have become more aware about their environment and pay more attention to conditions, e.g., air quality, and UV light exposure. Conventional technologies for reading environmental conditions are expensive, bulky, situated, and do not meet people's need for a mobile and portable tool for environmental fingerprinting on demand. We present a mobile-enabled client-server system for personalized environmental fingerprinting and crowdsourced environmental fingerprint datasets using a smartphone and a portable credit card-sized NFC powered sensor board.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {298–305},
numpages = {8},
keywords = {portable NFC-powered sensors, environment sensing with smartphones, environmental fingerprinting},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236156,
author = {Rauschenberger, Maria and Rello, Luz and Baeza-Yates, Ricardo},
title = {A Tablet Game to Target Dyslexia Screening in Pre-Readers},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236156},
doi = {10.1145/3236112.3236156},
abstract = {Using serious games to screen dyslexia has been a successful approach for English, German and Spanish. In a pilot study with a desktop game, we addressed pre-readers screening, that is, younger children who have not acquired reading or writing skills. Based on our results, we have redesigned the game content and new interactions with visual and musical cues. Hence, here we present a tablet game, DGames, which has the potential to predict dyslexia in pre-readers. This could contribute to around 10% of the population that is affected by dyslexia, as children will gain more time to learn to cope with the challenges of learning how to read and write.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {306–312},
numpages = {7},
keywords = {screening, serious games, computer-based assessment, universal screening, gamification, language-independent, pre-readers, detection, visual cues, dyslexia, musical cues},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236157,
author = {Marichal, Sebasti\'{a}n and Rosales, Andrea and Sansone, Gustavo and Pires, Ana Cristina and Bakala, Ewelina and Perilli, Fernando Gonzalez and Fleischer, Bruno and Blat, Josep},
title = {LETSmath},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236157},
doi = {10.1145/3236112.3236157},
abstract = {Visual information can be decoded very fast, letting us perceive and process a large amount of data in parallel. There is a lot of knowledge organized as guidelines and recommendations for GUI design. However, for blind people that perceive the world through auditory and haptic channels, GUIs might not fit their needs. In this paper we present a prototype of LETS Math (Learning Environment for Tangible Smart Mathematics), a tangible system for mathematics learning for blind children. LETS Math consists of tangibles blocks with tactile and auditory feedback, a working space, and a tablet-mediated audio game.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {313–320},
numpages = {8},
keywords = {technology enhanced learning, interaction design, tangible interaction, visual impairments, embodied interaction},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236158,
author = {Denman, Pete and Lewis, Erica and Prasad, Sai and Healey, Jennifer and Syed, Haroon and Nachman, Lama},
title = {Affsens: A Mobile Platform for Capturing Affect in Context},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236158},
doi = {10.1145/3236112.3236158},
abstract = {Capturing natural emotions as they occur in the wild is a known challenge. Participant compliance is often low and often the most important events are not captured. We present a new application design for capturing emotional assessments in the wild, using phone based sensors to determine events of interest and to generate timely in situ prompts. We present a flexible design that allows both quick assessments (less than 30s) and in depth journaling. The application is designed to be beautiful, engaging and informative while capturing rich data to inform insights and algorithms.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {321–326},
numpages = {6},
keywords = {circumplex, smart watch, GUI, mobile, application, affect, diary, audio, heart rate, design, sensor, survey, assessment, phone, emotion},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236159,
author = {Leiva, Luis A.},
title = {Responsive Snippets: Adaptive Skim-Reading for Mobile Devices},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236159},
doi = {10.1145/3236112.3236159},
abstract = {Responsive Snippets is the accompanying software of my recent work on responsive text summarization, an approach to web design aimed at allowing desktop web pages to be read in response to the size of the device a user is browsing with. Responsive Snippets allow designers to create HTML summaries quickly and effortlessly, by simply using CSS selectors and media queries. Responsive Snippets can be especially useful for news and blog sites, as a means to quickly allow users get a glimpse of the main body content, although they can be applied to any kind of HTML elements having text. The software is publicly available, so that others can build upon this work.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {327–331},
numpages = {5},
keywords = {graph algorithms, responsive web design, media queries, skimming, text summarization},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236160,
author = {Barhoush, Yazan and Mustonen, Miikka and Ferreira, Denzil and Georgiev, Georgi V and Nguyen, Daniel and Pouke, Matti},
title = {The Gravity of Thought: Exploring Positively Surprising Interactions},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236160},
doi = {10.1145/3236112.3236160},
abstract = {We discuss the design and initial evaluation of Gravity of Thought Waterfall, which is designed to be a surprising and creative prototype. Users can control the visual "gravity" effect of the waterfall with an EEG headset. The potentially surprising interactions with the prototype are evaluated through a set of questionnaires and a survey. Results show that the designed prototype is perceived to be surprising and creative; almost all participants were positively surprised when interacting with it.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {333–338},
numpages = {6},
keywords = {unexpectedness, digital fabrication, creativity, EEG, surprising interactions, surprise},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236161,
author = {Guti\'{e}rrez, Francisco and Verbert, Katrien and Htun, Nyi Nyi},
title = {PHARA: An Augmented Reality Grocery Store Assistant},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236161},
doi = {10.1145/3236112.3236161},
abstract = {Staying healthy is one of the most important things in life, and our daily decisions determine how healthy or unhealthy we are. We present PHARA, an augmented reality (AR) mobile assistant that supports decision-making for food products at grocery stores. Using a user-centered design approach we investigated the possibilities of AR technology in presenting food product information. Then, following an iterative design process, we implemented a mobile AR application to support users with typical decision-making tasks that take place at grocery stores. In this paper, detailed explanations of the working prototype of PHARA and its use case are presented.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {339–345},
numpages = {7},
keywords = {mobile, grocery assistant, decision making, recommender systems, augmented reality},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236162,
author = {Fedosov, Anton and Bexheti, Agon and Ermolaev, Egor and Langheinrich, Marc},
title = {Sharing Physical Objects Using Smart Contracts},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236162},
doi = {10.1145/3236112.3236162},
abstract = {Networked digital sharing economy services enable the effective and efficient sharing of vehicles, housing, and everyday objects. However, contemporary online sharing platforms face several challenges related to the establishment of trust among peers, as well difficulties to deal with the growing number of intermediaries (e.g., payment, insurance) needed to ensure an adequate service delivery. We designed and developed "Just Share It" (JSI), an interactive system that enables the sharing of personal physical possessions (e.g., power tools, toys, sports gear) by directly connecting lenders and borrowers, as peers, through mobile technology. The JSI system utilizes a blockchain ledger and smart contracting technologies to improve peer trust and limit the number of required intermediaries, respectively. In this submission, we briefly review emergent challenges in this space, describe the JSI prototype system and its trust model, and reflect on future architectural opportunities for an eventual "in the wild" deployment.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {346–352},
numpages = {7},
keywords = {blockchain, trust, sharing economy services, disintermediation, smartphone app, smart contract},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236163,
author = {Le, Huy Viet and Kosch, Thomas and Mayer, Sven and Henze, Niels},
title = {Demonstrating Palm Touch: The Palm as an Additional Input Modality on Commodity Smartphones},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236163},
doi = {10.1145/3236112.3236163},
abstract = {Touchscreens are the most successful input method for smartphones. Despite their flexibility, touch input is limited to the location of taps and gestures. We present Palm Touch, an additional input modality that differentiates between touches of fingers and the palm. Touching the display with the palm can be a natural gesture since moving the thumb towards the device's top edge implicitly places the palm on the touchscreen. We developed a model that differentiates between finger and palm touch with an accuracy of 99.53% in realistic scenarios. In this demonstration, we exhibit different use cases for Palm Touch, including the use as a shortcut and for improving reachability. In a previous evaluation, we showed that participants perceive the input modality as intuitive and natural to perform. Moreover, they appreciate Palm Touch as an easy and fast solution to address the reachability issue during one-handed smartphone interaction compared to thumb stretching or grip changes.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {353–358},
numpages = {6},
keywords = {machine learning, palm, smartphone, capacitive image},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236164,
author = {de Santos, Alberto and Oliver, Nuria},
title = {Musical Data: A Novel Approach to Create Music from Mobile Data},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236164},
doi = {10.1145/3236112.3236164},
abstract = {Data visualisation is one of the most common mechanisms to explore data. It is therefore no surprise that there are today a broad array of techniques and tools available to visually explore data. However, data may be also perceived through other sensory channels, such as touch, taste or sound. In this paper we propose Musical Data, a novel interactive demo that transforms mobile usage data into music. In same way as there is a visual language to interpret data visualisations, we can draw from the musical language to interpret the music generated from the data. Musical-Data offers two key advantages: first, it enables visually impaired individuals to make sense of complex data; second, Musical Data -used by itself or combined with data visualisations- opens new possibilities in terms of customer understanding and human computer interaction, as musical patterns may provide a novel perspective for understanding the behavior of mobile users.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {359–364},
numpages = {6},
keywords = {music, data visualisation, behavioural analytics},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236165,
author = {Vaquero-Melchor, Diego and Garc\'{\i}a-Hospital, Jorge and Bernardos, Ana M. and Besada, Juan A. and Casar, Jos\'{e} R.},
title = {Holo-Mis: A Mixed Reality Based Drone Mission Definition System},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236165},
doi = {10.1145/3236112.3236165},
abstract = {Drone mission definition for inspection purposes may require the operator or pilot to manage a lot of specific data. This process is time consuming, and validation prior to the flight is highly recommended to be able to fulfil the mission with the minimum risks and the maximum efficiency. This paper explores the use of wearable Mixed Reality as a tool to enhance interaction with information when planning drone-based missions. A Hololens-adapted set of automated functions to define complex missions through geometrical shapes is provided, together with update and information layer visualization. The use of an MR wearable enables the operator to easily configure, view and manage the mission over the virtual terrain.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {365–370},
numpages = {6},
keywords = {drones, interaction, augmented reality, mixed reality},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236170,
author = {Kaltenhauser, Annika and Schacht, Isabelle},
title = {Quiri: Chronic Pain Assessment for Patients and Physicians},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236170},
doi = {10.1145/3236112.3236170},
abstract = {We introduce Quiri, a holistic design concept for pain assessment and documentation. In this paper, we present the design studies of two mobile interfaces - a tablet app especially designed for children and adolescents and a smartwatch app for adults with chronic pain. This case study aims to firstly gain a better understanding of the preferences and needs of pain patients and physicians. Secondly, it describes the design process we took to arrive at the final design of Quiri. Quiri was developed based on the IEC 62366 usability standards for medical devices. The results of the design studies form an interdependent system which is complemented by a web user interface for physicians for the evaluation of the data collected in the mobile applications. Initial qualitative feedback from physicians indicates that Quiri is useful and appropriate for supporting pain therapy.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {371–378},
numpages = {8},
keywords = {pain assessment, user-centered design, mobile, user experience, usability standards, medical, telemedicine, pain therapy, chronic pain},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236171,
author = {Aylett, Matthew P. and Braude, David A.},
title = {Designing Speech Interaction for the Sony Xperia Ear and Oakley Radar Pace Smartglasses},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236171},
doi = {10.1145/3236112.3236171},
abstract = {Speech synthesis is a key enabling technology for wearable technology. We discuss the design challenges in customising speech synthesis for the Sony Xperia Ear, and the Oakley Radar Pace smartglasses. In order to support speech interaction designers working on novel interactive eye-free mobile devices, specific functionality is required including: flexibility in terms of performance, memory footprint, disk requirements, server or local configurations, methods for personification and branding, architectures for fast reactive interfaces, and customisation for content, genres and speech styles. We describe implementations of this required functionality and how this functionality can be made available to engineers and designers working on 3rd party devices and the impact they can have on user experience. To conclude we discuss why some customers are reluctant to depend on speech services from well known providers such as Google and Amazon and consider the barrier to entry for custom built personal digital advisors.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {379–384},
numpages = {6},
keywords = {speech synthesis, mobile, audio interaction},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236172,
author = {Krome, Sven and Beaurepaire, Jerome and Grani, Francesco and Liu, Annie and Bosdelekidis, Vasileios},
title = {Design-Led Exploration of Indoor Parking: An Industry Perspective},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236172},
doi = {10.1145/3236112.3236172},
abstract = {For many people, parking in large indoor venues can be a challenging and demanding task. Advanced positioning technologies and accurate indoor maps offer technical solutions for developing an indoor navigation system that helps drivers with the challenges of indoor parking. However, in order to develop a meaningful indoor navigation system, we first need to understand the experiential context of indoor parking. To do so, we investigated the indoor parking experience through explorative studies and design-led explorations. In this paper, we present the design process and the prototyping activities involved to gain an understanding of the indoor parking context. Furthermore, we reflect on how design-led research has helped us to iteratively reframe the research question and we present some of the relevant findings to inspire future academic or industrial research on this topic.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {385–394},
numpages = {10},
keywords = {navigation systems, indoor maps, design-led research, user experience, indoor parking},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236174,
author = {Liu, Chang-Fu and Chiang, Pei-Ying},
title = {Smart Glasses Based Intelligent Trainer for Factory New Recruits},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236174},
doi = {10.1145/3236112.3236174},
abstract = {We propose an intelligent factory training system based on smart AR glasses, with the support of spatial cognition and positioning technology to achieve a simple and intuitive self-learning interface. We have designed four training scenarios including the inspection of the equipment components, the troubleshooting of the ill functioning instrument, the factory route guidance, and the factory area information display and communication. Users can have the training and guide of the factory without additional manpower assistance. Finally, we sum up the user experience and the result showed our users are mostly positive in terms of their feedback. It is highly accepted by the user for the use of the AR intelligent trainer.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {395–399},
numpages = {5},
keywords = {human-computer interaction, augmented reality, industry 4.0, head-mounted display},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236175,
author = {Diamond, Lisa and Busch, Marc and Jilch, Valentin and Tscheligi, Manfred},
title = {Using Technology Acceptance Models for Product Development: Case Study of a Smart Payment Card},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236175},
doi = {10.1145/3236112.3236175},
abstract = {Since its development, the technology acceptance model (TAM) has been adapted for a multitude of different technologies and has proven very useful in a research context. These TAM adaptations are, however, less appropriate in product development since they do not contribute much to guide design and branding. We have revised the original TAM with the specific aim of application during new product development (NPD) and applied this model in a study on the acceptance of smart payment cards. The results provide helpful insights into the relevance of different potential benefits, suggesting that usefulness perception is most impacted by increased convenience, improved transaction overview and usage fun. Further, the model suggests that a good fit with who we are, rather than who we wish to be or feel we ought to be, is of special importance for usage intention. The application of NPDTAM in product development can be recommended.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {400–409},
numpages = {10},
keywords = {product development, technology acceptance, TAM, smart payment cards},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236176,
author = {Chi, Pei-Yu (Peggy) and Batra, Anurag and Hsu, Maxwell},
title = {Mobile Crowdsourcing in the Wild: Challenges from a Global Community},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236176},
doi = {10.1145/3236112.3236176},
abstract = {Recent research has been devoted to designing mobile applications that encourage users to complete microtasks in everyday context, known as "mobile crowdsourcing". In this case study, we present our ongoing effort of a publicly-available mobile application, Crowdsource, that has over 540,000 global users from 200 countries or regions. Over 15 million sessions have been performed since the initial launch in August 2016. To better support users' motivations and goals, we conducted a survey with our active users and validate their feedback with a set of usability studies. Our findings suggest design considerations for crowdsourcing microtasks with mobile users at the global scale.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {410–415},
numpages = {6},
keywords = {mobile crowdsourcing, microtasking, diversity, mobile apps},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236177,
author = {Torres, Deborah Z.},
title = {User Practices for Smartphone Control of TV},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236177},
doi = {10.1145/3236112.3236177},
abstract = {Mobile apps allow TV viewers to use their smartphone to control and interact with their TV. Our study set out to understand real-world practices of smartphone interaction with TV and uncover reasons people use smartphones instead of their remote controls. An online survey was conducted to determine which activities viewers perform when they use their smartphone to interact with TV, how they perform these activities, and why they choose to use their smartphone over their remote. Results reveal that the most popular activities are considered easier to complete on a smartphone than with a remote. Our findings outline these activities and provide a discussion on how the experience of smartphone control of TV can be improved. These discussion points are relevant as design teams decide how much to invest in mobile apps versus remote controls and companies evaluate if the smartphone can replace the remote control.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {416–424},
numpages = {9},
keywords = {TV, remote control, smartphone, TV soft remote, mobile phone applications},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236178,
author = {Bailey, Gavin},
title = {Augmenting the Reading Experience},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236178},
doi = {10.1145/3236112.3236178},
abstract = {My Research explores methods to enhance the reading experience of both digital and physical methods. Up to now in my PhD journey I have designed and developed prototypes using paper as input for digital reading devices. I have been exploring methods to augment the reading experience by adding tangible controls to digital reading devices and methods of communicating with printed books. This paper briefly outlines my research goals, research to date and future work.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {425–427},
numpages = {3},
keywords = {printed book, reading, e-book, interactive paper},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236179,
author = {Becker, Vincent},
title = {Augmented Humans Interacting with an Augmented World},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236179},
doi = {10.1145/3236112.3236179},
abstract = {Our research explores a seamless interaction with smart devices, which we divide into three stages: (i) device recognition, (ii) user input recognition, and (iii) inferring the appropriate action to be carried out on the device (cf. Figure 1). We leverage wearable computers and combine them into one interaction system. This makes interactions ubiquitous in two ways: While smart devices are becoming increasingly ubiquitous, our wearable system will be ubiquitously available to the user, making it possible to interact everywhere, any time.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {428–429},
numpages = {2},
keywords = {HCI, smart devices, wearable computing},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236180,
author = {Monastero, Beatrice},
title = {Augmenting Daily Spaces and Objects for Opportunistic Social Interaction},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236180},
doi = {10.1145/3236112.3236180},
abstract = {The objective of my research is to investigate how daily objects can be augmented with embedded technologies to support public opportunistic sociality (i.e. social curiosity, awareness, f2f interaction). I investigate how inhabitants of public spaces discover and use the augmented objects and the effects on daily sociality. I explore how different modes of engagement impact on interactions and how these vary in different locations. I address my research questions by deploying technological prototypes in month-lasting studies in the wild. I rely on both quantitative and qualitative methods to analyse how heterogeneous and differently recurrent users discover make sense and appropriate technologies over time in relation to situated sociality.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {430–432},
numpages = {3},
keywords = {sociality, augmented spaces, embedded technology},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236181,
author = {Li, Hong},
title = {Connecting across the Miles: Mediating Emotional Communication for Distant Loved Ones through Unconventional Artifacts},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236181},
doi = {10.1145/3236112.3236181},
abstract = {Advances in technologies have enabled a variety of convenient channels for couples in long-distance relationships to communicate and interact over vast distances. However, the hyper-connectivity can be a double-edged sword. Prior research has pointed out the focus of mainstream communication technologies is the transmission of explicit information, which neglects the mediation of emotional communication that is needed in intimate relationships [1]. As a result, there is a gap between understanding the users' needs in research and designing technologies for them in practice. My research has been dedicated to applying design thinking to investigate how intensive technologies can be redesigned and humanized to create a subtle and poetic cue of the presence of a distant loved one.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {434–436},
numpages = {3},
keywords = {user studies, user-centered design, long-distance relationships, emotional communication},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236182,
author = {Zuidhof, Niek and Allouch, Somaya Ben and Peters, Oscar and Verbeek, Peter-Paul},
title = {Appropriation of Wearable Augmented Reality},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236182},
doi = {10.1145/3236112.3236182},
abstract = {Wearables, like augmented reality glasses, are more and more commercially available but uptake has been slow and concerns on social and ethical implications are raised. Current adoption theories can provide insights into the adoption or rejection process, but very few studies are conducted in this field to address the social and ethical implications of wearables. This paper provides results about the inclusion of different perspectives, namely diffusion and adoption theories, mutual shaping perspectives and philosophy of technology to study the social interactions and ethical implications of wearables as well. By following this path we would like to develop a model of appropriation in the future to get a better understanding of the acceptance and interactions of emerging technologies such as wearable augmented reality.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {437–439},
numpages = {3},
keywords = {head-mounted display, augmented reality, wearables, adoption, mediation, appropriation},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236184,
author = {Murmann, Patrick},
title = {Usable Transparency for Enhancing Privacy in Mobile Health Apps},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236184},
doi = {10.1145/3236112.3236184},
abstract = {We report on our research on usable transparency in the context of mobile health (mhealth) tracking. Usable transparency refers to the usability of transparency-enhancing tools (TETs), which seek to aid users of online data services in improving their privacy. Focusing on fitness tracking scenarios, our research addresses the conceptual and technical demands of such tools in terms of usability.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {440–442},
numpages = {3},
keywords = {data privacy, notification, GDPR, usability, mobile health, transparency},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236185,
author = {Epp, Felix A.},
title = {Augmented Self-Presentation: Supporting Collocated Social Interaction},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236185},
doi = {10.1145/3236112.3236185},
abstract = {As mobile technology is embedded in people's collocated social interactions, it has impacts on self-presentation. With a perspective of self-presentation as a performance, we might gain insights into people's everyday practices with augmented personal information. This research looks into how people could make use of personal information dynamically integrated into their context, appearance and actions. With a focus on practice, this research combines field studies and interventions with personal wearable prototypes to reveal insights for design.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {443–445},
numpages = {3},
keywords = {augmented information, collocated social interactions, self-presentation, wearables},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3236112.3236186,
author = {Muralidhar, Srihari Hulikal},
title = {Opportunities and Challenges for Creating a Digital Payments Ecosystem: A Case of Auto-Rickshaw and Cabdrivers in India},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236186},
doi = {10.1145/3236112.3236186},
abstract = {The aim of this doctoral study is to generate knowledge on how the digital payments ecosystem in India can be (re)designed to benefit the poor. This is not to say that that digital money offers the best solution to drivers' financial problems. Rather, choosing to use cash for financial transactions is different from an inevitable dependence on cash. This study, therefore, situates digital payments within the broader ecosystem of financial practices and 'acceptance network' of digital (card or mobile-based) money. It shows how different parts of the ecosystem are shaped by practices of the poor and need to be integrated for digital payments to be usable and beneficial for the poor in substantive terms.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {446–448},
numpages = {3},
keywords = {digitization, financial practices, ecosystem, platform economy, mobile application, ethnography},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229439,
author = {Steil, Julian and M\"{u}ller, Philipp and Sugano, Yusuke and Bulling, Andreas},
title = {Forecasting User Attention during Everyday Mobile Interactions Using Device-Integrated and Wearable Sensors},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229439},
doi = {10.1145/3229434.3229439},
abstract = {Visual attention is highly fragmented during mobile interactions, but the erratic nature of attention shifts currently limits attentive user interfaces to adapting after the fact, i.e. after shifts have already happened. We instead study attention forecasting - the challenging task of predicting users' gaze behaviour (overt visual attention) in the near future. We present a novel long-term dataset of everyday mobile phone interactions, continuously recorded from 20 participants engaged in common activities on a university campus over 4.5 hours each (more than 90 hours in total). We propose a proof-of-concept method that uses device-integrated sensors and body-worn cameras to encode rich information on device usage and users' visual scene. We demonstrate that our method can forecast bidirectional attention shifts and predict whether the primary attentional focus is on the handheld mobile device. We study the impact of different feature sets on performance and discuss the significant potential but also remaining challenges of forecasting user attention during mobile interactions.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {1},
numpages = {13},
keywords = {handheld mobile device, egocentric vision, attentive user interfaces, attention shifts, mobile eye tracking},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229436,
author = {Weber, Dominik and Voit, Alexandra and Auda, Jonas and Schneegass, Stefan and Henze, Niels},
title = {Snooze! Investigating the User-Defined Deferral of Mobile Notifications},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229436},
doi = {10.1145/3229434.3229436},
abstract = {Notifications on mobile devices are a prominent source of interruptions. Previous work suggests using opportune moments to deliver notifications to reduce negative effects. In this paper, we instead explore the manual deferral of notifications. We developed an Android app that allows users to "snooze" mobile notifications for a user-defined amount of time or to a user-defined point in time. Using this app, we conducted a year-long in-the-wild study with 295 active users. To complement the findings, we recruited 16 further participants who used the app for one week and subsequently interviewed them. In both studies, snoozing was mainly used to defer notifications related to people and events. The reasons for deferral were manifold, from not being able to attend notifications immediately to not wanting to. Daily routines played an important role in the deferral of notifications. Most notifications were deferred to the same day or next morning, and a deferral of more than two days was an exception. Based on our findings, we derive design implications that can inform the design of future smart notification systems.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {2},
numpages = {13},
keywords = {deferral, mobile notifications, interruptions, notification management, in-situ, in-the-wild, mobile HCI},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229445,
author = {Pielot, Martin and Vradi, Amalia and Park, Souneil},
title = {Dismissed! A Detailed Exploration of How Mobile Phone Users Handle Push Notifications},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229445},
doi = {10.1145/3229434.3229445},
abstract = {We analyzed 794,525 notifications from 278 mobile phone users and how they were handled. Our study advances prior analyses in two ways: first, we systematically split notifications into five categories, including a novel separation of messages into individual- and group messages. Second, we conduct a comprehensive analysis of the behaviors involved in attending the notifications. Our participants received a median number of 56 notifications per day, which does not indicate that the number of notifications has increased over the past years. We further show that messaging apps create most of the notifications, and that other types of notifications rarely lead to a conversion (rates between ca. 15 and 25%). A surprisingly large fraction of notifications is received while the phone is unlocked or the corresponding app is in foreground, hinting at possibility to optimize for this scenario. Finally, we show that the main difference in handling notifications is how long users leave them unattended if they will ultimately not consume them.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {3},
numpages = {11},
keywords = {notifications, attendance, mobile devices, interruptibility},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229463,
author = {Okeke, Fabian and Sobolev, Michael and Dell, Nicola and Estrin, Deborah},
title = {Good Vibrations: Can a Digital Nudge Reduce Digital Overload?},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229463},
doi = {10.1145/3229434.3229463},
abstract = {Digital overuse on mobile devices is a growing problem in everyday life. This paper describes a generalizable mobile intervention that combines nudge theory and negative reinforcement to create a subtle, repeating phone vibration that nudges a user to reduce their digital consumption. For example, if a user has a daily Facebook limit of 30 minutes but opens Facebook past this limit, the user's phone will issue gentle vibrations every five seconds, but the vibration stops once the user navigates away from Facebook. We evaluated the intervention through a three-week controlled experiment with 50 participants on Amazon's Mechanical Turk platform with findings that show daily digital consumption was successfully reduced by over 20%. Although the reduction did not persist after the intervention was removed, insights from qualitative feedback suggest that the intervention made participants more aware of their app usage habits; and we discuss design implications of episodically applying our intervention in specific everyday contexts such as education, sleep, and work. Taken together, our findings advance the HCI community's understanding of how to curb digital overload.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {4},
numpages = {12},
keywords = {smartphones, social media, intervention, negative reinforcement, digital nudge, vibration, digital overload},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229449,
author = {Xu, Xuhai and Dancu, Alexandru and Maes, Pattie and Nanayakkara, Suranga},
title = {Hand Range Interface: Information Always at Hand with a Body-Centric Mid-Air Input Surface},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229449},
doi = {10.1145/3229434.3229449},
abstract = {Most interfaces of our interactive devices such as phones and laptops are flat and are built as external devices in our environment, disconnected from our bodies. Therefore, we need to carry them with us in our pocket or in a bag and accommodate our bodies to their design by sitting at a desk or holding the device in our hand. We propose Hand Range Interface, an input surface that is always at our fingertips. This body-centric interface is a semi-sphere attached to a user's wrist, with a radius the same as the distance from the wrist to the index finger. We prototyped the concept in virtual reality and conducted a user study with a pointing task. The input surface can be designed as rotating with the wrist or fixed relative to the wrist. We evaluated and compared participants' subjective physical comfort level, pointing speed and pointing accuracy on the interface that was divided into 64 regions. We found that the interface whose orientation was fixed had a much better performance, with 41.2% higher average comfort score, 40.6% shorter average pointing time and 34.5% lower average error. Our results revealed interesting insights on user performance and preference of different regions on the interface. We concluded with a set of guidelines for future designers and developers on how to develop this type of new body-centric input surface.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {5},
numpages = {12},
keywords = {interface evaluation, body-centric interaction, mid-air input surface},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229442,
author = {Gomes, Antonio and Priyadarshana, Lahiru Lakmal and Visser, Aaron and Carrascal, Juan Pablo and Vertegaal, Roel},
title = {Magicscroll: A Rollable Display Device with Flexible Screen Real Estate and Gestural Input},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229442},
doi = {10.1145/3229434.3229442},
abstract = {We present MagicScroll, a rollable tablet with 2 concatenated flexible multitouch displays, actuated scrollwheels and gestural input. When rolled up, MagicScroll can be used as a rolodex, smartphone, expressive messaging interface or gestural controller. When extended, it provides full access to its 7.5" high-resolution multitouch display, providing the display functionality of a tablet device. We believe that the cylindrical shape in the rolled-up configuration facilitates gestural interaction, while its shape changing and input capabilities allow the navigation of continuous information streams and provide focus plus context functionality. We investigated the gestural affordances of MagicScroll in its rolled-up configuration by means of an elicitation study.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {6},
numpages = {11},
keywords = {tablet, scroll, flexible display interface, displayobject, rollable display, organic user interface},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229470,
author = {Pai, Yun Suen and Chen, Zikun and Chan, Liwei and Isogai, Megumi and Kimata, Hideaki and Kunze, Kai},
title = {Pinchmove: Improved Accuracy of User Mobility for near-Field Navigation in Virtual Environments},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229470},
doi = {10.1145/3229434.3229470},
abstract = {Navigation and mobility mechanics for virtual environments aim to be realistic or fun, but rarely prioritize the accuracy of movement. We propose PinchMove, a highly accurate navigation mechanic utilizing pinch gestures and manipulation of the viewport for confined environments that prefers accurate movement. We ran a pilot study to first determine the degree of simulator sickness caused by this mechanic, and a comprehensive user study to evaluate its accuracy in a virtual environment. We found that utilizing an 80° tunneling effect at a maximum speed of 15.18° per second was deemed suitable for PinchMove in reducing motion sickness. We also found our system to be at average, more accurate in enclosed virtual environments when compared to conventional methods. This paper makes the following three contributions: 1) We propose a navigation solution in near-field virtual environments for accurate movement, 2) we determined the appropriate tunneling effect for our method to minimize motion sickness, and 3) We validated our proposed solution by comparing it with conventional navigation solutions in terms of accuracy of movement. We also propose several use- case scenarios where accuracy in movement is desirable and further discuss the effectiveness of PinchMove.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {7},
numpages = {11},
keywords = {accurate navigation, simulator sickness, virtual environment, near-field, pinch gesture},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229461,
author = {Steer, Cameron and Robinson, Simon and Pearson, Jennifer and Sahoo, Deepak and Mabbett, Ian and Jones, Matt},
title = {A Liquid Tangible Display for Mobile Colour Mixing},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229461},
doi = {10.1145/3229434.3229461},
abstract = {Digital painting is an increasingly popular medium of expression for many artists, yet when compared to its traditional equivalents of physical brushes and viscous paint it lacks a dimension of tangibility. We conducted observations and interviews with physical and digital artists, which gave us a strong understanding of the types of interactions used to create both physical and digital art, and the important role tangibility plays within these experiences. From this, we developed a unique liquid-like tangible display for mobile, digital colour mixing. Using a chemical hydrogel that changes its viscosity depending on temperature, we are able to create some resemblances to the feeling of mixing paint with a finger. This paper documents the information gathered from working with artists, how this process informed the development of a mobile painting attachment, and an exploration of its capabilities. After returning with our prototype, we found that it provided artists with sensations of oil and acrylic paint mixing and also successfully mimicked how paints are laid out on a paint palette.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {8},
numpages = {7},
keywords = {textures, digital painting, hydrogel, artists},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229450,
author = {Krieter, Philipp and Breiter, Andreas},
title = {Analyzing Mobile Application Usage: Generating Log Files from Mobile Screen Recordings},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229450},
doi = {10.1145/3229434.3229450},
abstract = {Logging mobile application usage on smartphones is limited to rather general system events unless one has access to the operating system's or applications' source code. In this paper, we present a method for analyzing mobile application usage in detail by generating log files based on mobile screen output. We are combining long-term log file analysis and short-term screen recording analysis by utilizing existing computer vision and machine learning methods. To validate the log results of our approach and implementation we collect 118 sample screen recordings of phone usage sessions and evaluate the resulting log file manually. Besides that, we explore the performance of our approach with different video quality parameters: frame rate and bit rate. We show that our method provides detailed data about application use and can work with low-quality video under certain circumstances.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {9},
numpages = {10},
keywords = {video analysis, log files, mobile application usage, screen recordings},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229474,
author = {Peltonen, Ella and Lagerspetz, Eemil and Hamberg, Jonatan and Mehrotra, Abhinav and Musolesi, Mirco and Nurmi, Petteri and Tarkoma, Sasu},
title = {The Hidden Image of Mobile Apps: Geographic, Demographic, and Cultural Factors in Mobile Usage},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229474},
doi = {10.1145/3229434.3229474},
abstract = {While mobile apps have become an integral part of everyday life, little is known about the factors that govern their usage. Particularly the role of geographic and cultural factors has been understudied. This article contributes by carrying out a large-scale analysis of geographic, cultural, and demographic factors in mobile usage. We consider app usage gathered from 25,323 Android users from 44 countries and 54,776 apps in 55 categories, and demographics information collected through a user survey. Our analysis reveals significant differences in app category usage across countries and we show that these differences, to large degree, reflect geographic boundaries. We also demonstrate that country gives more information about application usage than any demographic, but that there also are geographic and socio-economic subgroups in the data. Finally, we demonstrate that app usage correlates with cultural values using the Value Survey Model of Hofstede as a reference of cross-cultural differences.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {10},
numpages = {12},
keywords = {mobile applications, usage modeling, cultural factors},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229441,
author = {Katevas, Kleomenis and Arapakis, Ioannis and Pielot, Martin},
title = {Typical Phone Use Habits: Intense Use Does Not Predict Negative Well-Being},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229441},
doi = {10.1145/3229434.3229441},
abstract = {Not all smartphone owners use their device in the same way. In this work, we uncover broad, latent patterns of mobile phone use behavior. We conducted a study where, via a dedicated logging app, we collected daily mobile phone activity data from a sample of 340 participants for a period of four weeks. Through an unsupervised learning approach and a methodologically rigorous analysis, we reveal five generic phone use profiles which describe at least 10% of the participants each: limited use, business use, power use, and personality- &amp; externally induced problematic use. We provide evidence that intense mobile phone use alone does not predict negative well-being. Instead, our approach automatically revealed two groups with tendencies for lower well-being, which are characterized by nightly phone use sessions.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {11},
numpages = {13},
keywords = {unsupervised-learning, user groups, machine-learning, user study, mobile phone use, well-being, clustering, multi-level modelling, personality traits},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229440,
author = {Healey, Jennifer and Denman, Pete and Syed, Haroon and Nachman, Lama and Raj, Susanna},
title = {Circles vs. Scales: An Empirical Evaluation of Emotional Assessment GUIs for Mobile Phones},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229440},
doi = {10.1145/3229434.3229440},
abstract = {Natural emotional experiences happen "in the wild" as people are mobile, living their daily lives. To capture these experiences, emotion researchers often give participants smartphone applications with various graphical user interfaces (GUIs) to record how they are feeling, however, there exist few empirical tests that assess the comparative benefits and drawbacks of different GUI designs. This paper presents two empirical evaluations of three types of GUI designs for capturing emotion using both a 10 participant in-lab trial and a 100 participant AMT trial. We define GUI scoring metrics and report on participants' ability to rate real world scenarios and evocative images, respectively, in ways that are consistent with population norms and with respect their own emotion word choices. We additionally report on users preferences for different designs, their perceived ease of use and the average time taken to complete an assessment for the different designs.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {12},
numpages = {11},
keywords = {emotion models, affective computing, circumplex, Plutchik, ecological momentary assessment (EMA)},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229447,
author = {Jain, Mohit and Nawhal, Megha and Duppati, Saicharan and Dechu, Sampath},
title = {Mobiceil: Cost-Free Indoor Localizer for Office Buildings},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229447},
doi = {10.1145/3229434.3229447},
abstract = {Location awareness of people inside commercial establishments can help with occupancy-based dynamic energy management and indoor navigation. In this paper, we propose MobiCeil, a novel phone-based indoor localization technique. The proposed technique is offline, automated, and uses image captured from phone's camera to identify the unique ceiling structure of any particular location in the office building. The proposed method is based on these assumptions: (a) in office, employees tend to keep their phones lying on the table, and (b) the layout of ceiling landmarks in a portion of the ceiling structure (as captured by the phone's camera on the table) is unique. We validated these assumptions by checking the phone placement of 47 employees randomly at their cubicle or meeting room, and collecting ceiling layout data from 18 meeting rooms and 6 cubicles in an IT office building. To evaluate the performance of MobiCeil, we collected images of the ceiling as seen by the phone (front and back) camera in three different rotations of the phone placed on the table, to capture a total of 960 ceiling images. Our approach achieved an accuracy of 88.2% for identifying locations, with a low computation time of 2.8s per image.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {13},
numpages = {6},
keywords = {occupancy-based energy management, office buildings, workplaces, ceiling tracking, indoor localization, indoor navigation, computer vision, smartphones, camera},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229464,
author = {Borojeni, Shadan Sadeghian and Weber, Lars and Heuten, Wilko and Boll, Susanne},
title = {From Reading to Driving: Priming Mobile Users for Take-over Situations in Highly Automated Driving},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229464},
doi = {10.1145/3229434.3229464},
abstract = {Highly automated vehicles, occasionally require users to resume vehicle control from non-driving related tasks by issuing cues called take-over request (TOR). Due to being engaged in non-driving related tasks (NDRT), users have a decreased level of situational awareness of the driving context. Therefore, user interface designs for TORs should ensure smooth transitions from the NDRTs to vehicle control. In this paper, we investigated the role of decision priming cues as TORs across different levels of NDRT engagement. In a driving simulator, users performed a reading span task while driving in automated mode. They received audio-visual TORs which primed them with an appropriate maneuver (steering vs. braking), depending on the traffic situation. Our results showed that priming users with upcoming maneuvers results in faster responses and longer time to collision to obstacles. However, the level of engagement in NDRT does not affect user responses to TORs.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {14},
numpages = {12},
keywords = {decision priming, automated driving, non-driving related task engagement, take-over request},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229479,
author = {Matviienko, Andrii and Ananthanarayan, Swamy and Borojeni, Shadan Sadeghian and Feld, Yannick and Heuten, Wilko and Boll, Susanne},
title = {Augmenting Bicycles and Helmets with Multimodal Warnings for Children},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229479},
doi = {10.1145/3229434.3229479},
abstract = {Child cyclists are often at greater risk for traffic accidents. This is in part due to the development of children's motor and perceptual-motor abilities. To facilitate road safety for children, we explore the use of multimodal warning signals to increase their awareness and prime action in critical situations. We developed a bicycle simulator instrumented with these signals and conducted two controlled experiments. We found that participants spent significantly more time perceiving visual than auditory or vibrotactile cues. Unimodal signals were the easiest to recognize and suitable for encoding directional cues. However, when priming stop actions, reaction time was shorter when all three modalities were used simultaneously. We discuss the implications of these outcomes with regard to design of safety systems for children and their perceptual-motor learning.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {15},
numpages = {13},
keywords = {child cyclists, collision avoidance, bicycle safety, multimodal warnings, bicycle simulator},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229459,
author = {Schartm\"{u}ller, Clemens and Riener, Andreas and Wintersberger, Philipp and Frison, Anna-Katharina},
title = {Workaholistic: On Balancing Typing- and Handover-Performance in Automated Driving},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229459},
doi = {10.1145/3229434.3229459},
abstract = {Automated driving eliminates the permanent need for vehicle control and allows to engage in non-driving related tasks. As literature identifies office work as one potential activity, we estimate that advanced input devices will shortly appear in automated vehicles. To address this matter, we mounted a keyboard on the steering wheel, aiming to provide an exemplary safe and productive working environment. In a driving simulator study (n=20), we evaluated two feedback mechanisms (heads-up augmentation on a windshield, conventional heads-down display) and assessed both typing effort and driving performance in handover situations. Results indicate that the windshield alternative positively influences handovers, while heads-down feedback results in better typing performance. Text difficulty (two levels) showed no significant impact on handover time. We conclude that for a widespread acceptance of specialized interfaces for automated vehicles, a balance between safety aspects and productivity must be found in order to attract customers while retaining driving safety.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {16},
numpages = {12},
keywords = {augmented reality, windshield displays, handover/take-over requests, typing, automated driving, non-driving related tasks},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229476,
author = {Venolia, Gina and Tang, John C. and Inkpen, Kori and Unver, Baris},
title = {Wish You Were Here: Being Together through Composite Video and Digital Keepsakes},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229476},
doi = {10.1145/3229434.3229476},
abstract = {We developed a prototype which overlays local and remote participants in a video call and enables them to take group pictures together. These pictures serve as keepsakes of the event. The application uses real-time chroma key background removal to composite the remote person into the scene with the local group. We tested the prototype in a museum setting, and compared it to a more standard picture-in-picture (PiP) model. Users rated the composite mode as being significantly more fun, creating a greater sense of copresence and involvement than the PiP mode. Composite snapshots were also strongly preferred over picture-in-picture. Based on results from the study, we added a pinch-zoom and positioning interface to make it easier to frame remote people together into the snapshot, and conducted a second study. We conclude that combining composite video calls and picture-taking on a mobile device enables people to socially construct a shared activity with a remote person.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {17},
numpages = {11},
keywords = {video calling, video conferencing, mobile video, shared experiences, snapshots},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229467,
author = {Li, Hong and H\"{a}kkil\"{a}, Jonna and V\"{a}\"{a}n\"{a}nen, Kaisa},
title = {Review of Unconventional User Interfaces for Emotional Communication between Long-Distance Partners},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229467},
doi = {10.1145/3229434.3229467},
abstract = {New form factors and user interfaces for computer-mediated communication are emerging. The possibilities to use these systems for emotional communication are interesting, and recent years have witnessed the appearance of a versatile range of prototypes. In this paper, we present the results of a systematic literature review on research addressing the design of systems with unconventional user interfaces for emotional communication, focusing on the use case of facilitating long-distance relationships. We reviewed a body of 150 papers resulting from a systematic search, further analysis scoping the body to 47 papers, containing altogether 52 prototypes that were relevant for our focus. We then analysed the characteristics affecting the interaction mediated by these systems and their user interfaces. We present the results related to the design attributes, e.g., form factors, modalities, and message types of the systems, as well as to the evaluation approaches. As salient findings, touch input and visual output are the most common interaction modalities in these systems, and their evaluations lack in-the-wild studies, especially on long-term usage.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {18},
numpages = {10},
keywords = {romantic communication, intimate communication, presence-in-absence, emotional communication, romantic, long-distance relationship, communication of emotions, remote presence, literature review},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229468,
author = {Aranda, Julie H. and Baig, Safia},
title = {Toward "JOMO": The Joy of Missing out and the Freedom of Disconnecting},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229468},
doi = {10.1145/3229434.3229468},
abstract = {We took an ethnographic approach to explore the continuum between excessive smartphone use and healthy disconnection. We conducted a qualitative mixed-methods study in Switzerland and the United States to understand the nature of the problem, how it evolves, the workarounds that users employ to disconnect, and their experience of smartphone disconnection. We discussed two negative behavioral cycles: an internal experience of habit and excessive use, and an externally reinforced cycle of social obligation. We presented a taxonomy of non-use based on the dimensions of time and user level of control. We highlighted 3 potential areas for solutions around short-term voluntary disconnection and describe recommendations for how the mobile industry and app developers can address this issue.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {19},
numpages = {8},
keywords = {communication obligation, tech addiction, non-use, smartphone addiction, mindfulness, smartphone},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229477,
author = {Bexheti, Agon and Fedosov, Anton and Elhart, Ivan and Langheinrich, Marc},
title = {Memstone: A Tangible Interface for Controlling Capture and Sharing of Personal Memories},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229477},
doi = {10.1145/3229434.3229477},
abstract = {Today's sensor-rich mobile and wearable devices allow us to seamlessly capture an increasing amount of our daily experiences in digital format. This process can support human memory by producing "memory cues", e.g., an image or a sound that can help trigger our memories of a past event. However, first-person captures such as those coming from wearable cameras are not always ideal for triggering remembrance. One interesting option is thus to combine our own capture streams with those coming from co-located peers, in or even infrastructure sensors (e.g., a surveillance camera) in order to create more powerful memory cues. Given the significant privacy and security concerns of a system that shares personal experience streams with co-located peers, we developed a tangible user interface (TUI) that allows users to in-situ control the capture and sharing of their experience streams through a set of five physical gestures. We report on the design of the device, as well as the results of a user study with 20 participants that evaluated its usability and efficiency in the context of a meeting capture. Our results show that our TUI outperforms a comparable smartphone application, but also uncovers user concerns regarding the need for additional control devices.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {20},
numpages = {13},
keywords = {information sharing, mixed-method inquiry, lifelogging, tangible interface, gesture interactions, memory augmentation},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229435,
author = {Yamanaka, Shota},
title = {Effect of Gaps with Penal Distractors Imposing Time Penalty in Touch-Pointing Tasks},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229435},
doi = {10.1145/3229434.3229435},
abstract = {Targets on touchscreens should be large enough so that they can be tapped by fingers. In addition to the size of a target, properties of unintended targets around the intended target (e.g., margins) could affect user performance. In this study, we investigate the negative effects of such unintended targets (or distractors), which impose a penalty time when tapped for which users have to wait. Our participants sometimes purposely tapped an empty space on the opposite side of the distractor to avoid tapping it, and such behavior was affected by (1) the size of the intended target, (2) gap between the intended target and distractors, and (3) dimensionality of pointing tasks (1D or 2D). We also found that we could not estimate user performance by using Fitts' and FFitts' laws, probably because tap positions tended to shift away from distractors.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {21},
numpages = {11},
keywords = {finger touching, distractors, pointing, graphical user interfaces, touchscreens},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229482,
author = {Gaur, Varun and Uddin, Md. Sami and Gutwin, Carl},
title = {Multiplexing Spatial Memory: Increasing the Capacity of FastTap Menus with Multiple Tabs},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229482},
doi = {10.1145/3229434.3229482},
abstract = {The capacity of spatial multi-touch menus such as FastTap is limited by device screen size. We explore the idea of using multiple tabs to increase capacity - multiplexing the tablet's screen space so each location holds multiple items. Earlier work has shown potential of this idea for smartwatches, but no evaluations have considered larger devices. To assess issues with interference-based errors and spatial memory development, we built two FastTap systems with multiple tabs and conducted two studies. We first tested user learning of 16 targets with a training game, and found that participants easily adapted to the multi-tab model, were able to perform memory-based shortcuts, and made few interference-based errors. The second study used realistic drawing tasks and showed that people successfully used the multi-tab FastTap system, with 88% of selections made using shortcuts by the study's end. Our work demonstrates that spatial memory can successfully be multiplexed, and that tabs are a promising way to increase command set sizes for spatial interfaces.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {22},
numpages = {13},
keywords = {command selection, expertise, spatial memory, tablet},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229465,
author = {Vatavu, Radu-Daniel and Anthony, Lisa and Wobbrock, Jacob O.},
title = {$Q: A Super-Quick, Articulation-Invariant Stroke-Gesture Recognizer for Low-Resource Devices},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229465},
doi = {10.1145/3229434.3229465},
abstract = {We introduce $Q, a super-quick, articulation-invariant point-cloud stroke-gesture recognizer for mobile, wearable, and embedded devices with low computing resources. $Q ran up to 142X faster than its predecessor $P in our benchmark evaluations on several mobile CPUs, and executed in less than 3% of $P's computations without any accuracy loss. In our most extreme evaluation demanding over 99% user-independent recognition accuracy, $P required 9.4s to run a single classification, while $Q completed in just 191ms (a 49X speed-up) on a Cortex-A7, one of the most widespread CPUs on the mobile market. $Q was even faster on a low-end 600-MHz processor, on which it executed in only 0.7% of $P's computations (a 142X speed-up), reducing classification time from two minutes to less than one second. $Q is the next major step for the "$-family" of gesture recognizers: articulation-invariant, extremely fast, accurate, and implementable on top of $P with just 30 extra lines of code.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {23},
numpages = {12},
keywords = {point-cloud recognizer, mobile devices, low-resource devices, $-family, $Q, gesture recognition, stroke recognition, $P, $1},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229451,
author = {Pinder, Charlie and Vermeulen, Jo and Cowan, Benjamin R.},
title = {Subliminal Semantic Number Processing on Smartphones},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229451},
doi = {10.1145/3229434.3229451},
abstract = {One potential method of improving the efficiency of human-computer interaction is to display information subliminally. Such information cannot be recalled consciously, but has some impact on the perceiver. However, it is not yet clear whether people can extract meaning from subliminal presentation of information in mobile contexts. We therefore explored subliminal semantic priming on smartphones. This builds on mixed evidence for subliminal priming across HCI in general, and mixed evidence for the effect of subliminal affective priming on smartphones. Our semi-controlled experiment (n=103) investigated subliminal processing of numerical information on smartphones. We found evidence that concealed transfer of information is possible to a very limited extent, but little evidence of a semantic effect. Overall, the impact is effectively negligible for practical applications. We discuss the implications of our results for real-world deployments and outline future research themes as HCI moves beyond mobile.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {24},
numpages = {13},
keywords = {subliminal priming, semantic priming, smartphone, nonconscious behaviour change},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229462,
author = {Richardson, Dan and Jarusriboonchai, Pradthana and Montague, Kyle and Kharrufa, Ahmed},
title = {Parklearn: Creating, Sharing and Engaging with Place-Based Activities for Seamless Mobile Learning},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229462},
doi = {10.1145/3229434.3229462},
abstract = {The potential for mobile technology to support bespoke learning activities seamlessly across learning contexts has not been fully realized. We contribute insights gained from four months of field studies of place-based mobile learning in two different contexts: formal education with a primary school and informal, community-led learning with volunteers in a nearby park. For these studies we introduced ParkLearn: a platform for creating, sharing and engaging with place-based mobile learning activities through seamless learning experiences. The platform enables the creation of easily configurable learning activities that leverage the targeted learning environment and mobile devices' hardware to support situated learning. Learners' uploaded responses to activities can be viewed and shared via a website, supporting seamless follow-up classroom activities. By supporting creativity and independence for both learners and activity designers, ParkLearn promoted a sense of ownership, increased engagement in follow-up activities and supported the leveraging of physical and social communal learning resources.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {25},
numpages = {12},
keywords = {community curriculum, outdoor learning, digital civics, civic learning, mobile learning},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229480,
author = {Lyckvi, Sus and Torgersson, Olof},
title = {Privacy and Design Ethics vs Designing for Curiosity, Communication and Children: Lessons Learned},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229480},
doi = {10.1145/3229434.3229480},
abstract = {This paper describes the lessons learned when designing an empathy-oriented image-exchange app for fifth-grade pupils. The aim was to evoke curiosity and empathy towards someone living elsewhere or under different socio-economic circumstances. In addition, we strived to apply design ethics (e.g. protecting users from insults, humiliation, inappropriate content etc) and take users' privacy into account. By setting up these boundaries for this user group we found ourselves confronted with a set of conflicting design decisions which ultimately led to a lesser and different user experience than we had expected. Here, we discuss the interplay between our design decisions and the consequences thereof, and evaluate the mistakes we made. Moreover we discuss how to balance anonymity and curiosity, and comment on the benefits of making a pre-analysis of potential clashes related to intended UX and other core design decisions.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {26},
numpages = {8},
keywords = {communication, image sharing, curiosity, ethics, privacy, empathy, design for children},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229454,
author = {Husmann, Maria and Murolo, Alfonso and Kick, Nicolas and Di Geronimo, Linda and Norrie, Moira C.},
title = {Supporting out of Office Software Development Using Personal Devices},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229454},
doi = {10.1145/3229434.3229454},
abstract = {Software developers typically use multiple large screens in their office setup. However, they often work away from the office where such a setup is not available, instead only working with a laptop computer and drastically reduced screen real estate. We explore how developers can be better supported in ad-hoc scenarios, for example when they work in a cafe, an airport, or at a client's site. We present insights into current work practices and challenges when working away from the usual office desk sourced from a survey of professional software developers. Based on these insights, we introduce an IDE that makes use of additional personal devices, such as a phone or a tablet. Parts of the IDE can be offloaded to these mobile devices, for example the application that is being developed, a debugging console or navigational elements. A qualitative evaluation with professional software developers showed that they appreciate the increased screen real estate.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {27},
numpages = {11},
keywords = {distributed user interfaces, multi-device, cross-device},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229473,
author = {Uzor, Stephen and Baillie, Lynne and Htun, Nyi Nyi and Smit, Philip},
title = {Inclusively Designing IDA: Effectively Communicating Falls Risk to Stakeholders},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229473},
doi = {10.1145/3229434.3229473},
abstract = {Although gait/balance analysis methods have proven effective for assessing falls risk (FR), they are mostly confined to the laboratory and rely on expensive specialist equipment. Recent sensor technologies have made it possible to capture FR data accurately; however, no exploration has been done on how to effectively communicate these data to seniors in both healthcare and free-living settings. We describe IDA (Insole Device for Assessment of Falls Risk), comprising a relatively inexpensive insole and prototype application that provides feedback to stakeholders. To explore what level of FR data should best be communicated to different stakeholders, we conducted workshops with 26 seniors and interviewed 7 healthcare workers in the UK. We highlight stakeholder preferences on viewing FR data to foster greater understanding of outcomes and enhance communication between stakeholders. Finally, we identify opportunities for design on enhancing understanding of gait/balance outcomes; these have potential applications in other areas of physical rehabilitation.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {28},
numpages = {10},
keywords = {gait analysis, falls, insole, falls risk assessment, balance, design, user-centered},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229444,
author = {Mayer, Sven and Le, Huy Viet and Henze, Niels},
title = {Designing Finger Orientation Input for Mobile Touchscreens},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229444},
doi = {10.1145/3229434.3229444},
abstract = {A large number of today's systems use interactive touch surfaces as the main input channel. Current devices reduce the richness of touch input to two-dimensional positions on the screen. A growing body of work develops methods that enrich touch input to provide additional degrees of freedom for touch interaction. In particular, previous work proposed to use the finger's orientation as additional input. To efficiently implement new input techniques which make use of the new input dimensions, we need to understand the limitations of the input. Therefore, we conducted a study to derive the ergonomic constraints for using finger orientation as additional input in a two-handed smartphone scenario. We show that for both hands, the comfort and the non-comfort zone depend on how the user interacts with a touch surface. For two-handed smart-phone scenarios, the range is 33.3% larger than for tabletop scenarios. We further show that the phone orientation correlates with the finger orientation. Finger orientations which are harder to perform result in phone orientations where the screen does not directly face the user.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {29},
numpages = {9},
keywords = {pitch, ergonomics, surface, finger orientation, non-comfort zone, ergonomic zone, touch, yaw, mobile},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229446,
author = {Suzuki, Kenji and Sakamoto, Ryuuki and Sakamoto, Daisuke and Ono, Tetsuo},
title = {Pressure-Sensitive Zooming-out Interfaces for One-Handed Mobile Interaction},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229446},
doi = {10.1145/3229434.3229446},
abstract = {We present new alternative interfaces for zooming out on a mobile device: Bounce Back and Force Zoom. These interfaces are designed to be used with a single hand. They use a pressure-sensitive multitouch technology in which the pressure itself is used to zoom. Bounce Back senses the intensity of pressure while the user is pressing down on the display. When the user releases his or her finger, the view is bounced back to zoom out. Force Zoom also senses the intensity of pressure, and the zoom level is associated with this intensity. When the user presses down on the display, the view is scaled back according to the intensity of the pressure. We conducted a user study to investigate the efficiency and usability of our interfaces by comparing with previous pressure-sensitive zooming interface and Google Maps zooming interface as a baseline. Results showed that Bounce Back and Force Zoom was evaluated as significantly superior to that of previous research; number of operations was significantly lower than default mobile Google Maps interface and previous research.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {30},
numpages = {8},
keywords = {ZUI, zooming user interface, one-handed interaction, user study, mobile interaction},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229453,
author = {Shi, Lei and Ashoori, Maryam and Zhang, Yunfeng and Azenkot, Shiri},
title = {Knock Knock, What's There: Converting Passive Objects into Customizable Smart Controllers},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229453},
doi = {10.1145/3229434.3229453},
abstract = {Knocking is a way of interacting with everyday objects. We introduce BeatIt, a novel technique that allows users to use passive, everyday objects to control a smart environment by recognizing the sounds generated from knocking on the objects. BeatIt uses a BeatSet, a series of percussive sound samples, to represent the sound signature of knocking on an object. A user associates a BeatSet with an event. For example, a user can associate the BeatSet of knocking on a door with the event of turning on the lights. Decoder, a signal-processing module, classifies the sound signals into one of the recorded BeatSets, and then triggers the associated event. Unlike prior work, BeatIt can be implemented on microphone-enabled commodity devices. Our user studies with 12 participants showed that our proof-of-concept implementation based on a smartwatch could accurately classify eight BeatSets using a user-independent classifier.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {31},
numpages = {13},
keywords = {interacting with passive objects, smart environments},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229478,
author = {Leiva, Luis A. and Mart\'{\i}n-Albo, Daniel and Vatavu, Radu-Daniel},
title = {GATO: Predicting Human Performance with Multistroke and Multitouch Gesture Input},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229478},
doi = {10.1145/3229434.3229478},
abstract = {We introduce GATO, a human performance analysis technique grounded in the Kinematic Theory that delivers accurate predictions for the expected user production time of stroke gestures of all kinds: unistrokes, multistrokes, multitouch, or combinations thereof. Our experimental results obtained on several public datasets (82 distinct gesture types, 123 participants, ≈36k gesture samples) show that GATO predicts user-independent gesture production times that correlate rs &gt; .9 with groundtruth, while delivering an average relative error of less than 10% with respect to actual measured times. With its accurate estimations of users' a priori time performance with stroke gesture input, GATO will help researchers to understand better users' gesture articulation patterns on touchscreen devices of all kinds. GATO will also benefit practitioners to inform highly effective gesture set designs.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {32},
numpages = {11},
keywords = {stroke gestures, human performance, kinematic theory, touch gestures, gesture input, production time},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229481,
author = {Fischer, Michael and Campagna, Giovanni and Xu, Silei and Lam, Monica S.},
title = {Brassau: Automatic Generation of Graphical User Interfaces for Virtual Assistants},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229481},
doi = {10.1145/3229434.3229481},
abstract = {This paper presents Brassau, a graphical virtual assistant that converts natural language commands into GUIs. A virtual assistant with a GUI has the following benefits compared to text or speech based virtual assistants: users can monitor multiple queries simultaneously, it is easy to re-run complex commands, and user can adjust settings using multiple modes of interaction. Brassau introduces a novel template-based approach that leverages a large corpus of images to make GUIs visually diverse and interesting. Brassau matches a command from the user to an image to create a GUI. This approach decouples the commands from GUIs and allows for reuse of GUIs across multiple commands. In our evaluation, users prefer the widgets produced by Brassau over plain GUIs.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {33},
numpages = {12},
keywords = {aesthetics, graphical user interfaces, virtual assistants, visual preferences},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229460,
author = {Birnholtz, Jeremy},
title = {I Don't Want to Seem Trashy: Exploring Context and Self-Presentation through Young Gay and Bisexual Males' Attitudes toward Shirtless Selfies on Instagram},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229460},
doi = {10.1145/3229434.3229460},
abstract = {Mobile devices and social media have made it possible to share photos, often selfies, nearly instantaneously with potentially large networks of contacts and followers. Selfies have become a frequent component of young people's online self-presentations and shirtless male selfies, a common trope among some gay Instagram users, present an interesting self-presentation dilemma. Images of shirtless males, normatively appropriate, attractive and innocuous in some contexts, can also be vulnerable to misinterpretation or unintended sexualization in ways that can negatively impact others' impressions. This paper reports on an interview study of 15-24 year-old gay and bisexual Instagram users' attitudes toward and experiences with shirtless selfies. Results suggest that they see a clear tension between these images conveying attractiveness and possible negative connotations such as promiscuity, and have different strategies for navigating this tension. The results have implications for consideration of the contexts in which mobile social media content is produced and consumed.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {34},
numpages = {12},
keywords = {selfies, self-presentation, social media},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229466,
author = {Paay, Jeni and Kjeldskov, Jesper and Internicola, Daniele and Thomasen, Mikkel},
title = {Motivations and Practices for Cheating in Pok\'{e}Mon GO},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229466},
doi = {10.1145/3229434.3229466},
abstract = {Since the emergence of video computer games in the early 70's, the concept of "cheating" has been a hot issue in video gaming research. Adding mobility and location-based capabilities to computer games introduces a whole new set of behaviours, motivations and justifications that challenge gaming communities to reconsider what constitutes "cheating", and what is simply an acceptable extension of game play. Using the specific case of Pok\'{e}mon GO, we investigate players' perceptions on cheating in this mobile location-based game. In our research, we identified 10 ways that players circumvent the rules of Pok\'{e}mon GO. Through analysis of online forums, field observations, interviews, and a focus group with local players, we realised that players' attitudes vary as to what constitutes "cheating", and whether playing outside the rules is acceptable. We found players "cheat" to enhance game experience, to compensate for limitations in the game's design, or to keep up with other cheaters. While this has been observed in online gaming before, our study contributes to research by relating these specifically to the game's mobile location-based nature. We offer implications for design of location-based games.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {35},
numpages = {13},
keywords = {Pok\'{e}mon GO, interaction design, location-based gaming, mobile gaming, user experience design, cheating},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229448,
author = {de Oliveira, Rodrigo and Pentoney, Christopher and Pritchard-Berman, Mika},
title = {YouTube Needs: Understanding User's Motivations to Watch Videos on Mobile Devices},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229448},
doi = {10.1145/3229434.3229448},
abstract = {Watching online videos on mobile devices has been massively present in people's daily activities. However, different users can watch the same video for different purposes, and hence develop different expectations for their experience. Understanding people's motivations for watching videos on mobile can help address this problem by giving designers the information needed to craft the whole watching journey better adapting to user's expectations. To obtain this understanding, a comprehensive framework of viewer motivations is necessary. We present research that provides several contributions to understanding mobile video watchers: a thorough framework of user motivations to watch videos on mobile devices, a detailed procedure for collecting and categorizing these motivations, a set of challenges that viewers experience to address each motivation, insights on usage of mobile and non-mobile devices, and design recommendations for video sharing systems.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {36},
numpages = {11},
keywords = {video, experience sampling method, motivations},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229443,
author = {Mauriello, Matthew Louis and McNally, Brenna and Buntain, Cody and Bagalkotkar, Sapna and Kushnir, Samuel and Froehlich, Jon E.},
title = {A Large-Scale Analysis of YouTube Videos Depicting Everyday Thermal Camera Use},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229443},
doi = {10.1145/3229434.3229443},
abstract = {The emergence of low-cost thermographic cameras for mobile devices provides users with new practical and creative prospects. While recent work has investigated how novices use thermal cameras for energy auditing tasks in structured activities, open questions remain about "in the wild" use and the challenges or opportunities therein. To study these issues, we analyzed 1,000 YouTube videos depicting everyday uses of thermal cameras by non-professional, novice users. We coded the videos by content area, identified whether common misconceptions regarding thermography were present, and analyzed questions within the comment threads. To complement this analysis, we conducted an online survey of the YouTube content creators to better understand user behaviors and motivations. Our findings characterize common thermographic use cases, extend discussions surrounding the challenges novices encounter, and have implications for the design of future thermographic systems and tools.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {37},
numpages = {12},
keywords = {YouTube, sustainable HCI, thermal cameras, user-generated content, OSNs, social media, thermography},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229452,
author = {Khamis, Mohamed and Alt, Florian and Bulling, Andreas},
title = {The Past, Present, and Future of Gaze-Enabled Handheld Mobile Devices: Survey and Lessons Learned},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229452},
doi = {10.1145/3229434.3229452},
abstract = {While first-generation mobile gaze interfaces required special-purpose hardware, recent advances in computational gaze estimation and the availability of sensor-rich and powerful devices is finally fulfilling the promise of pervasive eye tracking and eye-based interaction on off-the-shelf mobile devices. This work provides the first holistic view on the past, present, and future of eye tracking on handheld mobile devices. To this end, we discuss how research developed from building hardware prototypes, to accurate gaze estimation on unmodified smartphones and tablets. We then discuss implications by laying out 1) novel opportunities, including pervasive advertising and conducting in-the-wild eye tracking studies on handhelds, and 2) new challenges that require further research, such as visibility of the user's eyes, lighting conditions, and privacy implications. We discuss how these developments shape MobileHCI research in the future, possibly the next 20 years.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {38},
numpages = {17},
keywords = {gaze interaction, eye tracking, mobile devices, smartphones, tablets, gaze estimation},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229437,
author = {Gruenefeld, Uwe and Stratmann, Tim Claudius and Ali, Abdallah El and Boll, Susanne and Heuten, Wilko},
title = {RadialLight: Exploring Radial Peripheral LEDs for Directional Cues in Head-Mounted Displays},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229437},
doi = {10.1145/3229434.3229437},
abstract = {Current head-mounted displays (HMDs) for Virtual Reality (VR) and Augmented Reality (AR) have a limited field-of-view (FOV). This limited FOV further decreases the already restricted human visual range and amplifies the problem of objects going out of view. Therefore, we explore the utility of augmenting HMDs with RadialLight, a peripheral light display implemented as 18 radially positioned LEDs around each eye to cue direction towards out-of-view objects. We first investigated direction estimation accuracy of multi-colored cues presented on one versus two eyes. We then evaluated direction estimation accuracy and search time performance for locating out-of-view objects in two representative 360° video VR scenarios. Key findings show that participants could not distinguish between LED cues presented to one or both eyes simultaneously, participants estimated LED cue direction within a maximum 11.8° average deviation, and out-of-view objects in less distracting scenarios were selected faster. Furthermore, we provide implications for building peripheral HMDs.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {39},
numpages = {6},
keywords = {wide field-of-view, directional cues, peripheral display, LEDs, augmented reality, virtual reality, head-mounted displays},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229438,
author = {Gruenefeld, Uwe and Ali, Abdallah El and Boll, Susanne and Heuten, Wilko},
title = {Beyond Halo and Wedge: Visualizing out-of-View Objects on Head-Mounted Virtual and Augmented Reality Devices},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229438},
doi = {10.1145/3229434.3229438},
abstract = {Head-mounted devices (HMDs) for Virtual and Augmented Reality (VR/AR) enable us to alter our visual perception of the world. However, current devices suffer from a limited field of view (FOV), which becomes problematic when users need to locate out of view objects (e.g., locating points-of-interest during sightseeing). To address this, we developed and evaluated in two studies HaloVR, WedgeVR, HaloAR and WedgeAR, which are inspired by usable 2D off-screen object visualization techniques (Halo, Wedge). While our techniques resulted in overall high usability, we found the choice of AR or VR impacts mean search time (VR: 2.25s, AR: 3.92s) and mean direction estimation error (VR: 21.85°, AR: 32.91°). Moreover, while adding more out-of-view objects significantly affects search time across VR and AR, direction estimation performance remains unaffected. We provide implications and discuss the challenges of designing for VR and AR HMDs.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {40},
numpages = {11},
keywords = {visualization technique, head-mounted, augmented reality, off-screen, virtual reality, out-of-view, Wedge, Halo},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229456,
author = {Tseng, Vincent W.-S. and Abdullah, Saeed and Costa, Jean and Choudhury, Tanzeem},
title = {AlertnessScanner: What Do Your Pupils Tell about Your Alertness},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229456},
doi = {10.1145/3229434.3229456},
abstract = {Alertness is a crucial component of our cognitive performance. Reduced alertness can negatively impact memory consolidation, productivity and safety. As a result, there has been an increasing focus on continuous assessment of alertness. The existing methods usually require users to wear sensors, fill out questionnaires, or perform response time tests periodically, in order to track their alertness. These methods may be obtrusvie to some users, and thus have limited capability. In this work, we propose AlertnessScanner, a computer-vision-based system that collects in-situ pupil information to model alertness in the wild. We conducted two in-the-wild studies to evaluate the effectiveness of our solution, and found that AlertnessScanner passively and unobtrusively assess alertness. We discuss the implications of our findings and present opportunities for mobile applications that measure and act upon changes in alertness.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {41},
numpages = {11},
keywords = {pupil segmentation, alertness, fatigue, sensing, mobile},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229475,
author = {Chen, Yu-An and Wu, Te-Yen and Chang, Tim and Liu, Jun You and Hsieh, Yuan-Chang and Hsu, Leon Yulun and Hsu, Ming-Wei and Taele, Paul and Yu, Neng-Hao and Chen, Mike Y.},
title = {ARPilot: Designing and Investigating AR Shooting Interfaces on Mobile Devices for Drone Videography},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229475},
doi = {10.1145/3229434.3229475},
abstract = {Drones offer camera angles that are not possible with traditional cameras and are becoming increasingly popular for videography. However, flying a drone and controlling its camera simultaneously requires manipulating 5-6 degrees of freedom (DOF) that needs significant training. We present ARPilot, a direct-manipulation interface that lets users plan an aerial video by physically moving their mobile devices around a miniature 3D model of the scene, shown via Augmented Reality (AR). The mobile devices act as the viewfinder, making them intuitive to explore and frame the shots. We leveraged AR technology to explore three 6DOF video-shooting interfaces on mobile devices: AR keyframe, AR continuous, and AR hybrid, and compared against a traditional touch interface in a user study. The results show that AR hybrid is the most preferred by the participants and expends the least effort among all the techniques, while the users' feedback suggests that AR continuous empowers more creative shots. We discuss several distinct usage patterns and report insights for further design.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {42},
numpages = {8},
keywords = {mobile device, virtual camera control, interaction techniques, tangible, human-drone interaction, augmented reality},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229457,
author = {Kurosawa, Hiroki and Sakamoto, Daisuke and Ono, Tetsuo},
title = {MyoTilt: A Target Selection Method for Smartwatches Using the Tilting Operation and Electromyography},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229457},
doi = {10.1145/3229434.3229457},
abstract = {We present the MyoTilt target selection method for smartwatches, which employs a combination of a tilt operation and electromyography (EMG). First, a user tilts his/her arm to indicate the direction of cursor movement on the smartwatch; then s/he applies forces on the arm. EMG senses the force and moves the cursor to the direction where the user is tilting his/her arm to manipulate the cursor. In this way, the user can simply manipulate the cursor on the smartwatch with minimal effort, by tiling the arm and applying force to it. We conducted an experiment to investigate its performance and to understand its usability. Result showed that participants selected small targets with an accuracy greater than 93.89%. In addition, performance significantly improved compared to previous tilting operation methods. Likewise, its accuracy was stable as targets became smaller, indicating that the method is unaffected by the "fat finger problem".},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {43},
numpages = {11},
keywords = {electromyography, target selection, smartwatch, tilt-based interaction},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229455,
author = {Tojo, Takaki and Kato, Tsuneo and Yamamoto, Seiichi},
title = {BubbleFlick: Investigating Effective Interface for Japanese Text Entry on Smartwatches},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229455},
doi = {10.1145/3229434.3229455},
abstract = {We propose BubbleFlick, an effective interface for Japanese text entry on smartwatches. While various ideas have been proposed to provide easy and fast text entry for the Latin alphabet, Japanese text entry has additional challenges such as having more than fifty syllabary characters, or kana, to enter and the subsequent kana-kanji conversion, which translates a sequence of the syllabary characters into a standard expression with a mixture of kanji and kana characters. This paper focuses on interfaces for entry of kana syllabary characters. We designed and prototyped three interfaces: 1) Japanese kana syllabary keyboard, 2) Dial&amp;Flick interface, and 3) DualBelts interface. Through a comparative pilot study of the prototypes, we refined the most promising Dial&amp;Flick interface into BubbleFlick. BubbleFlick provides the widest possible area for easy flick operations while also leaving an area for editing text. We conducted a 30-day consecutive user study on BubbleFlick in comparison with Google's latest Japanese text-entry method based on a numeric keypad. After thirty days, BubbleFlick showed a text-entry speed of over 35 characters per minute, which was comparable to Google's numeric-keypad-based method for novice participants. Through the user study, BubbleFlick showed a lower error rate and gave us informative hints for further improvement.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {44},
numpages = {12},
keywords = {software keyboard, smartwatch, text entry, Japanese kana, BubbleFlick},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229471,
author = {Jung, Jingun and Lee, Sangyoon and Ahn, Sunggeun and Lee, Geehyuk},
title = {Auto-Switching List Search Interface for Touchscreen Smartwatches},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229471},
doi = {10.1145/3229434.3229471},
abstract = {As smartwatch functions expand, target selection among many items will probably become a common task. List search interfaces (LSIs) for a smartwatch use a prefix matching query to search for items, and need two modes because of the small screen size: query input mode and list navigation mode. Despite the modes, LSIs may be more efficient than list interfaces (LIs), which involve no text querying, for large pools. Actually, we could show that the LSI outperformed the LI for pool sizes over 60. However, we also found that the LSI users experience overhead when deciding whether to switch the modes or not. To reduce the overhead, we designed two auto-switching LSIs: input-length-based auto-switching LSI (IA-LSI) and list-size-based auto-switching LSI (LA-LSI). We could show both auto-switching LSIs outperformed the conventional LSI for pool sizes over 60. We also conducted experiments with the auto-switching LSIs for various pool sizes, and provided their results and guidelines for the optimal switching criteria for the LSIs.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {45},
numpages = {10},
keywords = {auto-switching list search interface, smartwatch user interface},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229458,
author = {Tannert, Benjamin and Sch\"{o}ning, Johannes},
title = {Disabled, but at What Cost? An Examination of Wheelchair Routing Algorithms},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229458},
doi = {10.1145/3229434.3229458},
abstract = {Platforms like Google Maps or Bing Maps are used by a large number of users to find the shortest path to their destinations. While these services mainly focus on supporting drivers and pedestrians, first services exist that support wheelchair users. Routing algorithms for wheelchair users try to avoid obstacles like stairs or bollards and optimize on criteria like surface properties and slope of the route. In this study, we undertake the first controlled examination of wheelchair routing approaches. By analyzing three routing platforms, including two wheelchair routing algorithms and three pedestrian routing algorithms, across fifteen major cities in Germany, our results highlight that the routes for wheelchair users are significantly longer and partially also more complex than those for pedestrians. In addition, we show that today's pedestrian routing algorithms also output very diverse routes.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {46},
numpages = {7},
keywords = {routing, HCI, pedestrian navigation, disability, city planning, wheelchair users, accessibility},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229472,
author = {Regal, Georg and Mattheiss, Elke and Sellitsch, David and Tscheligi, Manfred},
title = {Mobile Location-Based Games to Support Orientation &amp; Mobility Training for Visually Impaired Students},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229472},
doi = {10.1145/3229434.3229472},
abstract = {Orientation and Mobility (O&amp;M) training is an important aspect in the education of visually impaired students. In this work we present a scavenger hunt-like location-based game to support O&amp;M training. In two comparative studies with blind and partially sighted students and interviews with teachers we investigate if a mobile game played in the real world is a suitable approach to support O&amp;M training and if a mobile location-based O&amp;M training game is preferred over a game played in a virtual world. Our results show that a mobile location-based game is a fruitful approach to support O&amp;M training for visually impaired students, and that a mobile location based game is preferred over playing in a virtual world. Based on the gathered insights we discuss implications for using mobile location-based games in O&amp;M training.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {47},
numpages = {12},
keywords = {visually impaired, orientation &amp; mobility training, location-based games, serious games},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229469,
author = {Gibson, Ryan Colin and Bouamrane, Matt-Mouley and Dunlop, Mark},
title = {Mobile Support for Adults with Mild Learning Disabilities during Clinical Consultations},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229469},
doi = {10.1145/3229434.3229469},
abstract = {Previous studies have suggested that the learning disability (LD) population face significant communication barriers when interacting with health professionals. Such obstacles may be considered as preventable; however, there is a surprising lack of research-based technologies available that intend to promote this communication. We aim to address this issue by investigating the potential use of mobile technologies to support adults with mild LDs during clinical consultations. To achieve this, we interviewed 10 domain experts including government advisors, academics, support workers and General Practitioners. The extracted information was used to develop an initial technology probe, which was evaluated by a subset of the aforementioned experts. The overall contribution of this research is a set of design guidelines for the development of Augmentative and Communicative technologies that target the clinical needs of adults with mild LDs.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {48},
numpages = {8},
keywords = {clinical consultations, AAC technologies, learning disabilities},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229483,
author = {Alabdulhafith, Maali and Alqarni, Abdulhadi and Sampalli, Srinivas},
title = {Customized Communication between Healthcare Members during the Medication Administration Stage},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229483},
doi = {10.1145/3229434.3229483},
abstract = {Communication between nurses and other healthcare member is essential during the bedside medication administration stage to provide effective patient care and prevent medication errors. The nurse provides information to the physician and pharmacist when consultation regarding medication errors or concern is needed. This information is very critical as it affects the situation assessment and clinical judgment. Insufficient information can lead to failure in treatment and jeopardize patient health. The research to date has focused on improving tools for general communication between healthcare members. However, none of them have been customized to effectively fit the medication administration stage nor have considered the content of the communication. Therefore, in this paper, we propose a novel idea of customized communication that precisely applies to the medication administration stage. We developed the Medication Administration Communication (MAC) application that generates the essential content of communication between the nurse and other healthcare members. We evaluated the application by testing its usability from the nurses perspective.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {49},
numpages = {8},
keywords = {smartphone application, communication, medication errors, medication administration},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3229434.3229484,
author = {Duval, Jared and Rubin, Zachary and Segura, Elena M\'{a}rquez and Friedman, Natalie and Zlatanov, Milla and Yang, Louise and Kurniawan, Sri},
title = {SpokeIt: Building a Mobile Speech Therapy Experience},
year = {2018},
isbn = {9781450358989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229434.3229484},
doi = {10.1145/3229434.3229484},
abstract = {SpokeIt is a mobile serious game for health designed to support speech articulation therapy. Here, we present SpokeIt as well as 2 preceding speech therapy prototypes we built, all of which use a novel offline critical speech recognition system capable of providing feedback in real-time. We detail key design motivations behind each of them and report on their potential to help adults with speech impairment co-occurring with developmental disabilities. We conducted a qualitative within-subject comparative study on 5 adults within this target group, who played all 3 prototypes. This study yielded refined functional requirements based on user feedback, relevant reward systems to implement based on user interest, and insights on the preferred hybrid game structure, which can be useful to others designing mobile games for speech articulation therapy for a similar target group.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {50},
numpages = {12},
keywords = {serious game for health, SpokeIt, developmental disabilities, user-centered design, speech therapy},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

