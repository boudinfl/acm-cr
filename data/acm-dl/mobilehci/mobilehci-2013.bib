@inproceedings{10.1145/2493190.2493197,
author = {Pielot, Martin and Oliveira, Rodrigo de},
title = {Peripheral Vibro-Tactile Displays},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493197},
doi = {10.1145/2493190.2493197},
abstract = {We report from a study exploring the boundaries of the peripheral perception of vibro-tactile stimuli. For three days, we exposed 15 subjects to a continual vibration pattern that was created by a mobile device worn in their trouser pocket. In order to guarantee that the stimuli would not require the subjects' focal attention, the vibration pattern was tested and refined to minimise its obtrusiveness, and during the study, the participants adjusted its intensity to just above their personal detection threshold. At random times, the vibration stopped and participants had to acknowledge these events as soon as they noticed them. Only 6.5% of the events were acknowledged fast enough to assume that the cue had been on the focus of the participants' attention. The majority of events were answered between 1 and 10 minutes, which indicates that the participants were aware of the cue without focussing on it. In addition, participants reported not to be annoyed by the signal in 94.4% of the events. These results provide evidence that vibration patterns can form non-annoying, lightweight information displays, which can be consumed at the periphery of a user's attention.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {1–10},
numpages = {10},
keywords = {haptics, ubiquitous computing / smart environments, peripheral displays, ambient displays, tactile uis},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493199,
author = {Spelmezan, Daniel and Appert, Caroline and Chapuis, Olivier and Pietriga, Emmanuel},
title = {Side Pressure for Bidirectional Navigation on Small Devices},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493199},
doi = {10.1145/2493190.2493199},
abstract = {Virtual navigation on a mobile touchscreen is usually performed using finger gestures: drag and flick to scroll or pan, pinch to zoom. While easy to learn and perform, these gestures cause significant occlusion of the display. They also require users to explicitly switch between navigation mode and edit mode to either change the viewport's position in the document, or manipulate the actual content displayed in that viewport, respectively. SidePress augments mobile devices with two continuous pressure sensors co-located on one of their sides. It provides users with generic bidirectional navigation capabilities at different levels of granularity, all seamlessly integrated to act as an alternative to traditional navigation techniques, including scrollbars, drag-and-flick, or pinch-to-zoom. We describe the hardware prototype, detail the associated interaction vocabulary for different applications, and report on two laboratory studies. The first shows that users can precisely and efficiently control SidePress; the second, that SidePress can be more efficient than drag-and-flick touch gestures when scrolling large documents.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {11–20},
numpages = {10},
keywords = {pressure input, single-handed interaction, scrolling task, mobile device, side pressure, pressure-based interaction},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493220,
author = {Boland, Daniel and Murray-Smith, Roderick},
title = {Finding My Beat: Personalised Rhythmic Filtering for Mobile Music Interaction},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493220},
doi = {10.1145/2493190.2493220},
abstract = {A novel interaction style is presented, allowing in-pocket music selection by tapping a song's rhythm on a device's touchscreen or body. We introduce the use of rhythmic queries for music retrieval, employing a trained generative model to improve query recognition. We identify rhythm as a fundamental feature of music which can be reproduced easily by listeners, making it an effective and simple interaction technique for retrieving music. We observe that users vary in which instruments they entrain with and our work is the first to model such variability. An experiment was performed, showing that after training the generative model, retrieval performance improved two-fold. All rhythmic queries returned a highly ranked result with the trained generative model, compared with 47% using existing methods. We conclude that generative models of subjective user queries can yield significant performance gains for music retrieval and enable novel interaction techniques such as rhythmic filtering.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {21–30},
numpages = {10},
keywords = {rhythm, tapping, machine learning, music},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493193,
author = {Hwang, Sungjae and Bianchi, Andrea and Wohn, Kwang-yun},
title = {VibPress: Estimating Pressure Input Using Vibration Absorption on Mobile Devices},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493193},
doi = {10.1145/2493190.2493193},
abstract = {This paper introduces VibPress, a software technique that enables pressure input interaction on mobile devices by measuring the level of vibration absorption with the built-in accelerometer when the device is in contact with a damping surface (e.g., user's hands). This is achieved using a real-time estimation algorithm running on the device. Through a user evaluation, we provide evidence that this system is faster than previous software-based approaches, and accurate as hardware-augmented approaches (up to 99.7% accuracy). With this work, we also provide an insight about the maximum number of pressure levels that users can reliably distinguish, reporting usability metrics (time, errors and cognitive load) for different pressure levels and types of gripping gestures (press and squeeze).},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {31–34},
numpages = {4},
keywords = {haptics, mobile, accelerometer, pressure input},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493245,
author = {Reilly, Derek and MacKay, Bonnie},
title = {Annotating Ecology: Looking to Biological Fieldwork for Mobile Spatial Annotation Workflows},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493245},
doi = {10.1145/2493190.2493245},
abstract = {We present findings from a qualitative study of the spatial practices of biological fieldwork. We argue that these fieldwork practices inform a vision of decentralized spatial annotation in which a variety of motivations, needs, and perspectives coexist, and may support each other synergistically. We contrast this with current and past designs of mobile spatial annotation systems in the literature. From our analysis we identify three guidelines for mobile annotation systems design in biological fieldwork that we argue also extend to other domains: allowing the management of space through user control over annotation processes, promoting structured but flexible annotation through user-defined annotation formats, and providing robust and comprehensive integration of disparate data sources to allow ad hoc, exploratory queries.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {35–44},
numpages = {10},
keywords = {mobile spatial annotation, fieldwork},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493205,
author = {Briggs, Jo and Blythe, Mark},
title = {Apps for Art's Sake: Resistance and Innovation},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493205},
doi = {10.1145/2493190.2493205},
abstract = {The paper reports on the growing phenomena of art-making on mobile devices and contributes findings from two studies of artists' responses to iPad painting apps: the first is a series of exploratory workshops where artists were recruited to engage with a range of art apps, the second is a series of in-depth interviews with two artists who had incorporated the device and Brushes app into their painting practice over a period of months and years. The artists in both studies generally agreed that the devices and apps were easy to use and enjoyable but remained ambivalent about the technologies and outcomes. Although there was excitement around new creative possibilities there were also tensions around the status of the work being produced. The paper reflects on the role of popular digital production apparatus and information exchange on the constitution of artist-identities at a time of rapid techno-cultural change. It argues that while tablet computing and art apps have democratized certain artistic processes these technologies have generated conflict with traditional conceptions of art and curation.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {45–54},
numpages = {10},
keywords = {identity, ipad, apps, leisure, authorship, tablet devices, digital art, digital culture},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493212,
author = {Jentsch, Marc and Ramirez, Leonardo and Wood, Lisa and Elmasllari, Erion},
title = {The Reconfiguration of Triage by Introduction of Technology},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493212},
doi = {10.1145/2493190.2493212},
abstract = {Triage is the process of sorting patients by order of treatment necessity in large scale emergencies. Usually, a paper tag is attached to each patient containing their classification and the results of an initial, quick diagnosis. Several projects have aimed to electronically augment the process by using ubiquitous computing components. In this paper we present drawbacks of introducing technology to the process, which have not been discussed elsewhere, based on an extensive set of expert workshops discussing the employment of technology in triage with the aid of technology probes. Our main finding is that the common set of functionalities of electronic triage systems involves unwanted reconfiguration of triage processes. By presenting a set of implications for the design of these mobile technologies, we show how potential negative effects can be mitigated.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {55–64},
numpages = {10},
keywords = {triage, cooperative design, emergency response},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493203,
author = {Centieiro, Pedro and Rom\~{a}o, Teresa and Dias, A. Eduardo},
title = {Enhancing Remote Live Sports Experiences through an Eyes-Free Interaction},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493203},
doi = {10.1145/2493190.2493203},
abstract = {When using mobile screen apps on touch-based mobile devices to interact with live sports TV broadcasts, people need to keep an eye on those devices, since they do not provide tactile feedback. If the app on the mobile screen requires user interaction when something exciting is about to happen on the TV screen, the user needs to shift attention from the TV to the mobile screen, spoiling the whole experience. This paper presents WeBet, a mobile game that prompts users to bet if a goal is about to happen during a football match, without requiring their visual attention. WeBet aims to study if this concept conveys an exciting user experience by allowing a natural interaction with the mobile screen without looking at it. Results from a preliminary user test helped to validate our approach and identified important refinements for future work.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {65–68},
numpages = {4},
keywords = {user experience, emotions, remote interaction, sports fans, live sports, second screen, touch-based mobile devices},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493244,
author = {Sakamoto, Daisuke and Komatsu, Takanori and Igarashi, Takeo},
title = {Voice Augmented Manipulation: Using Paralinguistic Information to Manipulate Mobile Devices},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493244},
doi = {10.1145/2493190.2493244},
abstract = {We propose a technique called voice augmented manipulation (VAM) for augmenting user operations in a mobile environment. This technique augments user interactions on mobile devices, such as finger gestures and button pressing, with voice. For example, when a user makes a finger gesture on a mobile phone and voices a sound into it, the operation will continue until stops making the sound or makes another finger gesture. The VAM interface also provides a button-based interface, and the function connected to the button is augmented by voiced sounds. Two experiments verified the effectiveness of the VAM technique and showed that repeated finger gestures significantly decreased compared to current touch-input techniques, suggesting that VAM is useful in supporting user control in a mobile environment.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {69–78},
numpages = {10},
keywords = {navigation, panning, input technique, zooming, tablet, voice input, mobile phone/device},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493215,
author = {Wang, Yuntao and Yu, Chun and Liu, Jie and Shi, Yuanchun},
title = {Understanding Performance of Eyes-Free, Absolute Position Control on Touchable Mobile Phones},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493215},
doi = {10.1145/2493190.2493215},
abstract = {Many eyes-free interaction techniques have been proposed for touchscreens, but few researches have studied human's eyes-free pointing ability with mobile phones. In this paper, we investigate the single-handed thumb performance of eyes-free, absolute position control on mobile touch screens. Both 1D and 2D experiments were conducted. We explored the effects of target size and location on eyes-free touch patterns and accuracy. Our findings show that variance of touch points per target will converge as target size decreases. The centroid of touch points per target tends to be offset to the left of target center along horizontal direction, and shift toward screen center along vertical direction. Average accuracy drops from 99.6% of 2\texttimes{}2 layout to 85.0% of 4\texttimes{}4 layout, and average per target varies depending on the location of target. Our findings and design implications provide a foundation for future researches based on eyes-free, absolute position control using thumb on mobile devices.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {79–88},
numpages = {10},
keywords = {absolute position control, pointing performance, eyes-free interaction},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@dataset{10.1145/review-2493190.2493215_R49835,
author = {De, Debraj},
title = {Review ID:R49835 for DOI: 10.1145/2493190.2493215},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-2493190.2493215_R49835}
}

@inproceedings{10.1145/2493190.2493232,
author = {Delamare, William and Coutrix, C\'{e}line and Nigay, Laurence},
title = {Mobile Pointing Task in the Physical World: Balancing Focus and Performance While Disambiguating},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493232},
doi = {10.1145/2493190.2493232},
abstract = {We address the problem of mobile distal selection of physical objects when pointing at them in augmented environments. We focus on the disambiguation step needed when several objects are selected with a rough pointing gesture. A usual disambiguation technique forces the users to switch their focus from the physical world to a list displayed on a handheld device's screen. In this paper, we explore the balance between change of users' focus and performance. We present two novel interaction techniques allowing the users to maintain their focus in the physical world. Both use a cycling mechanism, respectively performed with a wrist rolling gesture for P2Roll or with a finger sliding gesture for P2Slide. A user experiment showed that keeping users' focus in the physical world outperforms techniques that require the users to switch their focus to a digital representation distant from the physical objects, when disambiguating up to 8 objects.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {89–98},
numpages = {10},
keywords = {focus, pointing, physical interaction, performance, mobile interaction, disambiguation},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493234,
author = {Grubert, Jens and Schmalstieg, Dieter},
title = {Playing It Real Again: A Repeated Evaluation of Magic Lens and Static Peephole Interfaces in Public Space},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493234},
doi = {10.1145/2493190.2493234},
abstract = {We repeated a study on the usage of a magic lens and a static peephole interface for playing a find-and-select game in a public space. While we reproduced the study setup and procedure the task was conducted in a public transportation stop with different characteristics. The results on usage duration and user preference were significantly different from those reported for previous conditions. We investigate possible causes, specifically the differences in the spatial characteristics and the social contexts in which the study took place.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {99–102},
numpages = {4},
keywords = {proxemics, space, social context, augmented reality, static peephole, field trial, in-the-wild, magic lens, place},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493240,
author = {Apostu, Silviu and Al-Nuaimi, Anas and Steinbach, Eckehard and Fahrmair, Michael and Song, Xiaohang and M\"{o}ller, Andreas},
title = {Towards the Design of an Intuitive Multi-View Video Navigation Interface Based on Spatial Information},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493240},
doi = {10.1145/2493190.2493240},
abstract = {A Multi-View Video (MVV) is a set of related videos that capture an interesting scene from different perspectives at overlapping times. The work at hand is concerned with the design of innovative user interfaces (UI) for viewing MVVs. As a first contribution four different MVV UIs are designed. While different in design, their common aim is to allow a pleasant viewing and perspective switching experience by reducing the cognitive effort associated with constructing a mental map of the scene. This is achieved by incorporating the spatial relationships of the available views in the UI elements. As a second contribution a quality model is developed and a methodical evaluation process is designed. This is used to evaluate and compare the UIs. In a third contribution we use principal component analysis (PCA) to reveal information about the perceptual quality space which helps validating our proposed quality model. Based on the findings, a series of conclusions for best design practices are provided.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {103–112},
numpages = {10},
keywords = {quality of experience, spatial context, usability, user interface, multi-view video},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493210,
author = {Pahud, Michel and Hinckley, Ken and Iqbal, Shamsi and Sellen, Abigail and Buxton, Bill},
title = {Toward Compound Navigation Tasks on Mobiles via Spatial Manipulation},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493210},
doi = {10.1145/2493190.2493210},
abstract = {We contrast the Chameleon Lens, which uses 3D movement of a mobile device held in the nonpreferred hand to support panning and zooming, with the Pinch-Flick-Drag metaphor of directly manipulating the view using multi-touch gestures. Lens-like approaches have significant potential because they can support navigation-selection, navigation-annotation, and other such compound tasks by off-loading navigation to the nonpreferred hand while the preferred hand annotates, marks a location, or draws a path on the screen. Our experimental results show that the Chameleon Lens is significantly slower than Pinch-Flick-Drag for the navigation subtask in isolation. But our studies also reveal that for navigation between a few known targets the lens performs significantly faster, that differences between the Chameleon Lens and Pinch-Flick-Drag rapidly diminish as users gain experience, and that in the context of a compound navigation-annotation task, the lens performs as well as Pinch-Flick-Drag despite its deficit for the navigation subtask itself.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {113–122},
numpages = {10},
keywords = {sensors, pan &amp; zoom, compound tasks, handheld devices, spatial input, chunking, data navigation, 3d interaction},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493208,
author = {Steins, Christian and Gustafson, Sean and Holz, Christian and Baudisch, Patrick},
title = {Imaginary Devices: Gesture-Based Interaction Mimicking Traditional Input Devices},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493208},
doi = {10.1145/2493190.2493208},
abstract = {We propose Imaginary Devices, a set of freehand gestures that mimic the use of physical input devices. Imaginary Devices allow users to choose the input modality best suited for the task at hand, such as a steering wheel for a driving game or a joystick for a flight simulator. Exploiting the skills that users have acquired using physical input devices, they can instantly begin interacting with an Imaginary Device. Since no physical device is involved, users can switch quickly and effortlessly among a number of devices.We demonstrate the potential of Imaginary Devices with Grand Theft Auto, a game that requires players to change between roles often and quickly, and we examine the viability of the concept in two user studies. In the first study, we found that participants produced a wide range of postures to represent each device but all were able to reproduce the correct posture after a short demonstration. In the second study, we found that Imaginary Devices afford precise input control and approach the baseline performance set by physical devices.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {123–126},
numpages = {4},
keywords = {gaming, imaginary interfaces, gesture input, input device},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493204,
author = {Raptis, Dimitrios and Tselios, Nikolaos and Kjeldskov, Jesper and Skov, Mikael B.},
title = {Does Size Matter? Investigating the Impact of Mobile Phone Screen Size on Users' Perceived Usability, Effectiveness and Efficiency.},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493204},
doi = {10.1145/2493190.2493204},
abstract = {Given the wide adoption of smartphones, an interesting debate is taking place regarding their optimal screen size and specifically whether possible portability issues counterbalance the obvious benefits of a larger screen. Moreover, the lack of scientific evidence about the concrete impact of mobile phones' screen size on usability raises questions both to practitioners and researchers. In this paper, we investigate the impact of a mobile phone's screen size on users' effectiveness, efficiency and perceived usability as measured using System Usability Scale (SUS). An experiment was conducted with 60 participants, which interacted with the same information seeking application on three different devices of the same brand that differed on their screen size. A significant effect of screen size on efficiency was derived, leading to an important finding that users who interact with larger than 4.3in screens are more efficient during information seeking tasks.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {127–136},
numpages = {10},
keywords = {efficiency, mobile devices, perceived usability, attractiveness, prior experience, brand, screen size, desire, effectiveness, sus},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493224,
author = {Heikkinen, Jani and M\"{a}kinen, Erno and Lylykangas, Jani and Pakkanen, Toni and V\"{a}\"{a}n\"{a}nen-Vainio-Mattila, Kaisa and Raisamo, Roope},
title = {Mobile Devices as Infotainment User Interfaces in the Car: Contextual Study and Design Implications},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493224},
doi = {10.1145/2493190.2493224},
abstract = {The spreading of mobile devices to all areas of everyday life impacts many contexts of use, including cars. Even though driving itself has remained relatively unchanged, there are now a wide variety of new in-car tasks, which people perform with both integrated infotainment systems and their mobile devices. To gain insights into this new task context and how it could be improved, we conducted a qualitative, contextual study in which we observed real-life car journeys with eight participants. The focus was on user interaction with touchscreen mobile devices, due to their wide range of functions and services. The findings show that the car is an extension of other contexts and it contains a rich set of infotainment tasks, including use of social media. Drivers emphasized gesture interaction and the use of non-visual modalities, for replacing visual information and notifying of changes in the driving context. Based on the findings, we present design implications for future in-car infotainment systems.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {137–146},
numpages = {10},
keywords = {contextual inquiry, in-car infotainment, design implications, mobile devices},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493228,
author = {Deegan, Robin},
title = {Managing Distractions in Complex Settings},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493228},
doi = {10.1145/2493190.2493228},
abstract = {Mobile devices are being used in more and more complex settings such as cars or medical environments and these environments are causing serious distractions for the mobile user. This paper presents novel research that investigates mobile user experiences when interacting with cognitively demanding distractions. This research finds that, surprisingly, the user's primary task is not always affected by the distraction but, in this case, the actual interaction between user and device is. This observation initially appears to contradict current research which suggests that a distraction will affect the primary task. The main conclusion of this paper is that a user, when dealing with distraction, can balance their cognitive processes by applying less cognitive resources to the mobile device interaction in order to maintain their performance at the primary task. Essentially, the interface can appear more difficult and less user friendly.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {147–150},
numpages = {4},
keywords = {mobile hci, mobile learning, mobile usability, cognitive load theory, cognitive science},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493221,
author = {Tran, Jessica J. and Trewin, Shari and Swart, Calvin and John, Bonnie E. and Thomas, John C.},
title = {Exploring Pinch and Spread Gestures on Mobile Devices},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493221},
doi = {10.1145/2493190.2493221},
abstract = {Pinching and spreading gestures are prevalent in mobile applications today, but these gestures have not yet been studied extensively. We conducted an exploratory study of pinch and spread gestures with seated participants on a phone and a tablet device. We found device orientation did not have a significant effect on gesture performance, most pinch and spread tasks were completed in a single action, and they were executed in 0.9-1.2 seconds. We also report how participants chose to sit with the mobile device, variations in gesture execution method, and the effect of varying target width and gesture size. Our task execution times for different gesture distances and precision levels display a surprisingly good fit to a simple Fitts's Law model. We conclude with recommendations for future studies.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {151–160},
numpages = {10},
keywords = {multi-touch gestures, mobile devices},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493243,
author = {Fukatsu, Yoshitomo and Shizuki, Buntarou and Tanaka, Jiro},
title = {No-Look Flick: Single-Handed and Eyes-Free Japanese Text Input System on Touch Screens of Mobile Devices},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493243},
doi = {10.1145/2493190.2493243},
abstract = {We present a single-handed and eyes-free Japanese kana text input system on touch screens of mobile devices. We first conducted preliminary experiments to investigate the accuracy with which subjects could single-handedly point to and flick without using their eyes. We found from the results that users can point at a screen that was divided into 2 x 2 with 100% accuracy and that users can flick at a 2 x 2 grid without using their eyes with 96.1% accuracy using our algorithm for flick recognition. The system used kana letter input based on two-stroke input with three keys to enable accurate eyes-free typing. First, users flick for consonant input, and then similarly flick for vowel input. We conducted a long-term user study to measure basic text entry speed and error rate performance under eyes-free conditions, and readability of transcribed phrases. As a result, the mean text entry speed was 51.2 characters per minute (cpm) in the 10th session of the user study and the mean error rate was 0.6% of all characters. The mean text entry speed was 33.9 cpm in the 11th session, which was conducted under totally eyes-free conditions and the mean error rate was 4.8% of all characters. We not only measured cpm and error rate, but also measured error rate of reading, which we devised as a novel metric to measure how accurately users can read transcribed phrases. The mean error rate of reading in the 11th session was 5.7% of all phrases.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {161–170},
numpages = {10},
keywords = {text entry, touch typing, touchscreen phones, one-handed interaction, shoulder surfing, heads-up writing, pointing, multi-tap},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493222,
author = {Valderrama Baham\'{o}ndez, Elba del Carmen and Kubitza, Thomas and Henze, Niels and Schmidt, Albrecht},
title = {Analysis of Children's Handwriting on Touchscreen Phones},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493222},
doi = {10.1145/2493190.2493222},
abstract = {Drawing and handwriting play a central role in primary schools. So far handwriting is practiced mainly on paper and blackboards. Providing tasks on paper can be challenging in developing countries. With the potential availability of mobile phones in classrooms, there is a new medium that can be used. We determined the effect of different touch technologies on children's handwriting for 18 third grade and 20 sixth grade participants. Children drew and wrote using different input techniques. We measured their performance and asked teachers to assess the legibility. We show that writing on touchscreens is less legible and slower than on paper. Further, the comparison of touchscreen technologies indicates that capacitive screens operated with a stylus yield the highest readability and are faster to use for writing than resistive screens. In contrast to these quantitative findings participants from third grade indicated that they prefer resistive screens with a thin stylus compared to using capacitive screens with a stylus or fingers.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {171–174},
numpages = {4},
keywords = {stylus, children, handwriting, touchscreen phones, school},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493233,
author = {Schoenleben, Oliver and Oulasvirta, Antti},
title = {Sandwich Keyboard: Fast Ten-Finger Typing on a Mobile Device with Adaptive Touch Sensing on the Back Side},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493233},
doi = {10.1145/2493190.2493233},
abstract = {This Note introduces a keyboard design that affords ten-finger touch typing by utilizing a touch sensor on the back side of a device. Previous work has used physical buttons. Using a touch sensor has the benefit that it retains the form factor and does not insist on a peripheral device. Moreover, any layout can be used. However, it is difficult to hit targets on a flat surface with no haptic feedback. Sandwich Keyboard is a prototype that folds any three-row keyboard layout and thus, by retaining the finger-to-letter assignment, supports transfer. Sandwich Keyboard includes an algorithm for constant adaptation of key targets in the back. We also learned that the detection of key presses from finger release enhances the performance of touch-typing on a multitouch sensor. After eight hours of training, experienced typists of the QWERTY and of the Dvorak Standard Keyboard (DSK) layout reached 26.1 and 46.2 wpm, respectively. We discuss improvements necessary for further increasing both speed and accuracy.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {175–178},
numpages = {4},
keywords = {text entry, touch sensor input, back-of-device interaction, mobile devices, touch-typing},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493200,
author = {Kienzle, Wolf and Hinckley, Ken},
title = {Writing Handwritten Messages on a Small Touchscreen},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493200},
doi = {10.1145/2493190.2493200},
abstract = {We present a method for composing handwritten messages on a small touchscreen device. A word is entered by drawing overlapped, screen sized letters on top of each other. The system does not require gestures or timeouts to delimit characters within a word - it automatically segments the overlapping strokes and renders the message in real-time as the user is writing. The auto-segmentation algorithm was designed for practicality; it is extremely simple, requires only public domain data for training, and runs very fast on low-power devices. Drawings may also be included with the text. Experimental data indicates the effectiveness of our system, even for novice users.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {179–182},
numpages = {4},
keywords = {messaging, ink, handwriting, notes, mobile input},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493209,
author = {Guha, Shion and Birnholtz, Jeremy},
title = {Can You See Me Now? Location, Visibility and the Management of Impressions on Foursquare},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493209},
doi = {10.1145/2493190.2493209},
abstract = {Location based social networking applications enable people to share their location with friends for social purposes by "checking in" to places they visit. Prior research suggests that both privacy and impression management motivate location disclosure concerns. In this interview study of foursquare users, we explore the ways people think about location sharing and its effects on impression management and formation. Results indicate that location-sharing decisions depend on the perceived visibility of the check-in, blur boundaries between public and private venues, and can initiate tensions within the foursquare friend network. We introduce the concept of "check-in transience" to explain factors contributing to impression management and argue that sharing location is often used as a signaling strategy to achieve social objectives.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {183–192},
numpages = {10},
keywords = {impression management, foursquare, visibility, check-in},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493229,
author = {Vaittinen, Tuomas and Salminen, Miikka and Olsson, Thomas},
title = {City Scene: Field Trial of a Mobile Street-Imagery-Based Navigation Service},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493229},
doi = {10.1145/2493190.2493229},
abstract = {Mobile navigation services are becoming more than merely digital maps; many of them include imagery from the street level. The user can benefit from enriching the 2D-map-based navigation with panoramic imagery from the citizen's perspective, hence gaining an authentic view of the frequent landmarks that urban environments include. In this paper we describe the user-centered design of a mobile street-imagery-based navigation service supporting navigation and exploration of unfamiliar cities. The service was evaluated with a field trial using tourists as participants. The participants used the service freely for the pedestrian navigation tasks that were relevant to them during the trial period. This approach shed light on issues that have not been raised by previous studies on image-based navigation, which have relied on more formal test tasks. The study confirmed that the images help with detecting the destination or assessing the atmosphere of a remote location but brought into focus the real world challenges related to downloading times and positioning accuracy.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {193–202},
numpages = {10},
keywords = {panorama image, location-based service, navigation, field trial, user-centered design},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493235,
author = {Wither, Jason and Au, Carmen E. and Rischpater, Raymond and Grzeszczuk, Radek},
title = {Moving beyond the Map: Automated Landmark Based Pedestrian Guidance Using Street Level Panoramas},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493235},
doi = {10.1145/2493190.2493235},
abstract = {In the past people have used very different forms of directions depending on how those directions were acquired. If a person is giving another person directions in a familiar area, he will frequently use landmarks to describe the route [10]. If the person gets the route from a personal navigation system though, it will be displayed on a map and make use of street names for the directions.In this paper we present a system to automatically give landmark based navigation to pedestrians by using panoramic imagery to both find salient landmarks along a route automatically, and to present those landmarks to a pedestrian navigator in an immersive and intuitive manner. Our system primarily uses automatically detected business signs as landmarks, and currently works in a half dozen cities around the world. We have also evaluated our system and found that people can effectively navigate solely using landmark enhanced panoramas of decision points along the route.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {203–212},
numpages = {10},
keywords = {natural guidance, navigation, mixed reality},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493198,
author = {Amini, Shahriyar and Setlur, Vidya and Xi, Zhengxin and Hayashi, Eiji and Hong, Jason},
title = {Investigating Collaborative Mobile Search Behaviors},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493198},
doi = {10.1145/2493190.2493198},
abstract = {People use mobile devices to search, locate and discover local information around them. Mobile local search is frequently a social activity. This paper presents the results of a survey and an exploratory user study of collaborative mobile local search. The survey results show that people frequently search with others and that these searches often involve the use of more than one mobile device. We prototyped a collaborative mobile search app, which we used as a tool to investigate users' collaborative mobile search behavior. Our study results provide insights into how users collaborate while performing search. We also provide design considerations to inform future mobile local search technologies.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {213–216},
numpages = {4},
keywords = {local search, mobile search, collaborative search},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493218,
author = {Poulsen, Esben Skouboe and Morrison, Ann and Andersen, Hans J\o{}rgen and Jensen, Ole B.},
title = {Responsive Lighting: The City Becomes Alive},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493218},
doi = {10.1145/2493190.2493218},
abstract = {We distributed fourteen controllable street lamps in a city square and recorded three comparative and one 'usual' condition, operating the public lighting as if it were an interactive stage. First tested was adaptive lighting that responded to people's occupancy patterns. Second was a mobile phone application that allowed people to customise color and responsive behaviours in the overhead lighting system. Third was ambient lighting, responding to wind velocity. The study extends the discussion on multiuser interaction design in public lighting by asking: how can interactions using mobile phones, thermal tracking and wind inputs afford new social behaviors, without disturbing the usual public functions of street lighting? This research lays foundational work on the affordances of mobile phones for engagement and interaction with public lighting. The study indicates the use of personal phones as a tool for interaction in this setting has potential to provide a stronger ownership to urban place.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {217–226},
numpages = {10},
keywords = {mobile interaction, urban lighting, experiment, responsive lighting, public space, experience, interaction design},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493239,
author = {Derthick, Katie and Scott, James and Villar, Nicolas and Winkler, Christian},
title = {Exploring Smartphone-Based Web User Interfaces for Appliances},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493239},
doi = {10.1145/2493190.2493239},
abstract = {We describe the SAWUI architecture by which smartphones can easily show user interfaces for nearby appliances, with no modification or pre-installation of software on the phone, no reliance on cloud services or networking infrastructure, and modest additional hardware in the appliance. In contrast to appliances? physical user interfaces, which are often as simple as buttons, icons and LEDs, SAWUIs leverage smartphones? powerful UI hardware to provide personalized, self-explanatory, adaptive, and localized UIs.To explore the opportunities created by SAWUIs, we conducted a study asking designers to redesign two appliances to include SAWUIs. Task characteristics including frequency, proximity, and complexity were used in deciding whether to place functionality on the physical UI, the SAWUI, or both. Furthermore, results illustrate how, in addition to support for accomplishing tasks, SAWUIs have the potential to enrich human experiences around appliances by increasing user autonomy and supporting better integration of appliances into users' social and personal lives.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {227–236},
numpages = {10},
keywords = {appliances, smartphones, user interfaces},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493238,
author = {Kildal, Johan and Lucero, Andr\'{e}s and Boberg, Marion},
title = {Twisting Touch: Combining Deformation and Touch as Input within the Same Interaction Cycle on Handheld Devices},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493238},
doi = {10.1145/2493190.2493238},
abstract = {We present a study that investigates the potential of combining, within the same interaction cycle, deformation and touch input in a handheld device. Using a flexible, input-only device connected to an external display, we compared a multitouch input technique and two hybrid deformation-plus-touch input techniques (bending and twisting the device, plus either front- or back-touch), in an image-docking task. We compared and analyzed the performance (completion time) and user experience (UX) obtained in each case, using multiple assessment metrics. We found that combining device deformation with front-touch produced the best UX. All the interaction techniques showed the same efficiency in task completion. This was a surprising finding, since multitouch (an integral input technique) was expected to be the most efficient technique in an image docking task (an interaction in an integral perceptual space). We discuss these findings in relation to self-reported qualitative data and observed interaction-procedure metrics. We found that the interaction procedures with the hybrid techniques were more sequential but also more paced. These findings suggest that the benefits of deformation input can still be observed when deformation and touch are combined in an input device.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {237–246},
numpages = {10},
keywords = {organic ui, twist, user interface, bend, deformable ui},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493242,
author = {Weigel, Martin and Boring, Sebastian and Steimle, J\"{u}rgen and Marquardt, Nicolai and Greenberg, Saul and Tang, Anthony},
title = {ProjectorKit: Easing Rapid Prototyping of Interactive Applications for Mobile Projectors},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493242},
doi = {10.1145/2493190.2493242},
abstract = {Researchers have developed interaction concepts based on mobile projectors. Yet pursuing work in this area - particularly in building projector-based interactions techniques within an application - is cumbersome and time-consuming. To mitigate this problem, we contribute ProjectorKit, a flexible open-source toolkit that eases rapid prototyping mobile projector interaction techniques.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {247–250},
numpages = {4},
keywords = {mobile projectors, rapid prototyping, toolkit},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493213,
author = {Chiang, Hsin-Yi and Chiasson, Sonia},
title = {Improving User Authentication on Mobile Devices: A Touchscreen Graphical Password},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493213},
doi = {10.1145/2493190.2493213},
abstract = {Typing text passwords is challenging when using touchscreens on mobile devices and this is becoming more problematic as mobile usage increases. We designed a new graphical password scheme called Touchscreen Multi-layered Drawing (TMD) specifically for use with touchscreens. We conducted an exploratory user study of three existing graphical passwords on smart phones and tablets with 31 users. From this, we set our design goals for TMD to include addressing input accuracy issues without having to memorize images, while maintaining an appropriately secure password space. Design features include warp cells which allow TMD users to continuously draw their passwords across multiple layers in order to create more complex passwords than normally possible on a small screen. We compared the usability of TMD to Draw A Secret (DAS) on a tablet computer and a smart phone with 90 users. Results show that TMD improves memorability, addresses the input accuracy issues, and is preferred as a replacement for text passwords on mobile devices.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {251–260},
numpages = {10},
keywords = {touchscreen, mobile devices, authentication},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493231,
author = {von Zezschwitz, Emanuel and Dunphy, Paul and De Luca, Alexander},
title = {Patterns in the Wild: A Field Study of the Usability of Pattern and Pin-Based Authentication on Mobile Devices},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493231},
doi = {10.1145/2493190.2493231},
abstract = {Graphical password systems based upon the recall and reproduction of visual patterns (e.g. as seen on the Google Android platform) are assumed to have desirable usability and memorability properties. However, there are no empirical studies that explore whether this is actually the case on an everyday basis. In this paper, we present the results of a real world user study across 21 days that was conducted to gather such insight; we compared the performance of Android-like patterns to personal identification numbers (PIN), both on smartphones, in a field study. The quantitative results indicate that PIN outperforms the pattern lock when comparing input speed and error rates. However, the qualitative results suggest that users tend to accept this and are still in favor of the pattern lock to a certain extent. For instance, it was rated better in terms of ease-of-use, feedback and likeability. Most interestingly, even though the pattern lock does not provide any undo or cancel functionality, it was rated significantly better than PIN in terms of error recovery; this provides insight into the relationship between error prevention and error recovery in user authentication.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {261–270},
numpages = {10},
keywords = {pin, likeability, usability, pattern, authentication},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493223,
author = {Muslukhov, Ildar and Boshmaf, Yazan and Kuo, Cynthia and Lester, Jonathan and Beznosov, Konstantin},
title = {Know Your Enemy: The Risk of Unauthorized Access in Smartphones by Insiders},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493223},
doi = {10.1145/2493190.2493223},
abstract = {Smartphones store large amounts of sensitive data, such as SMS messages, photos, or email. In this paper, we report the results of a study investigating users' concerns about unauthorized data access on their smartphones (22 interviewed and 724 surveyed subjects). We found that users are generally concerned about insiders (e.g., friends) accessing their data on smartphones. Furthermore, we present the first evidence that the insider threat is a real problem impacting smartphone users. In particular, 12% of subjects reported a negative experience with unauthorized access. We also found that younger users are at higher risk of experiencing unauthorized access. Based on our results, we propose a stronger adversarial model that incorporates the insider threat. To better reflect users' concerns and risks, a stronger adversarial model must be considered during the design and evaluation of data protection systems and authentication methods for smartphones.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {271–280},
numpages = {10},
keywords = {user study, theft, stranger, smartphone, physical threats, loss, insider},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493211,
author = {Regal, Georg and Busch, Marc and Deutsch, Stephanie and Hochleitner, Christina and Lugmayr, Martin and Tscheligi, Manfred},
title = {Money on the Move Workload, Usability and Technology Acceptance of Second-Screen Atm-Interactions},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493211},
doi = {10.1145/2493190.2493211},
abstract = {In this paper we compare one single-screen touch interaction with an automated teller machine (ATM) against two alternative second-screen ATM interactions using a smartphone. In an experimental laboratory study, those three ATM interactions were compared by means of workload (NASA-TLX), usability (SEQ, UMUX) and technology acceptance (selected TAM3-scales and additional scales for trust and security) in a randomized, controlled within-subjects design (n=24). In one smartphone ATM interaction the Personal Identification Number (PIN) was entered on the mobile phone, in the other smartphone ATM interaction the PIN was entered on the PIN-pad of the ATM. The results indicate that overall second-screen ATM interaction all interaction done on the mobile phone -- performed best.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {281–284},
numpages = {4},
keywords = {usability, second-screen-interaction, workload, technology acceptance, experimental study, atm},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493227,
author = {Dumas, Bruno and Sol\'{o}rzano, Mar\'{\i}a and Signer, Beat},
title = {Design Guidelines for Adaptive Multimodal Mobile Input Solutions},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493227},
doi = {10.1145/2493190.2493227},
abstract = {The advent of advanced mobile devices in combination with new interaction modalities and methods for the tracking of contextual information, opens new possibilities in the field of context-aware user interface adaptation. One particular research direction is the automatic context-aware adaptation of input modalities in multimodal mobile interfaces. We present existing adaptive multimodal mobile input solutions and position them within closely related research fields. Based on a detailed analysis of the state of the art, we propose eight design guidelines for adaptive multimodal mobile input solutions. The use of these guidelines is further illustrated through the design and development of an adaptive multimodal calendar application.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {285–294},
numpages = {10},
keywords = {context-aware systems, design guidelines, user interface adaptation, mobile multimodal interaction},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493216,
author = {Mau\'{e}s, Rodrigo de A. and Barbosa, Simone Diniz Junqueira},
title = {Keep Doing What i Just Did: Automating Smartphones by Demonstration},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493216},
doi = {10.1145/2493190.2493216},
abstract = {Automating tasks can make a smartphone easier to use and more battery efficient. However, currently little work has been done to help end-users to create such automations. In this paper, we explore an approach for automating smartphone tasks by demonstration. We have developed a mobile application called Keep Doing It that continuously records users' interactions with their smartphones. After users performed a task that they would like to automate, they can ask our application to create the automation based on their latest actions. Since users only have to use their smartphones, as they would naturally do, to demonstrate automations, we believe that our approach can lower the barrier for creating smartphone automations. Overall, an initial evaluation of the approach suggests that users would be willing to automate their phones by demonstration.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {295–303},
numpages = {9},
keywords = {mobile computing, programming by demonstration, context-aware systems, smartphone automation, end-user development, ubiquitous computing},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493226,
author = {Leitner, Michael and Cockton, Gilbert and Yee, Joyce S.R.},
title = {At the Mobile Experience Flicks: Making Short Films to Make Sense for Mobile Interaction Design},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493226},
doi = {10.1145/2493190.2493226},
abstract = {We introduce four short films to analyse, display and make sense of mobile experience and mobile context for design purposes. The films were scripted and produced on the basis of diary and interview data looking at mobile texting and mobile social media use. We experience the making of the films as a way to understand, frame and focus the design space for researchers and designers. We reflect on our own process of making these short films and discuss the value of such an approach for mobile interaction design.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {304–307},
numpages = {4},
keywords = {mobile experience, short films, mobile context, generative techniques, design-centered user research},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493195,
author = {Pearson, Jennifer and Robinson, Simon and Jones, Matt and Nanavati, Amit and Rajput, Nitendra},
title = {ACQR: Acoustic Quick Response Codes for Content Sharing on Low End Phones with No Internet Connectivity},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493195},
doi = {10.1145/2493190.2493195},
abstract = {In this paper we introduce Acoustic Quick Response codes to facilitate sharing between Interactive Voice Response (IVR) service users. IVRs are telephone-based, and similar to the world wide web in many aspects, but currently lack support for content sharing. Our approach uses 'audio codes' to let people share their call positions, and allows callers to hold their normal (low-end) handsets together to synchronise. The technique uses remote generation and recognition of audio codes to ensure that sharing is possible on any type of phone without the need for textual literacy or an internet connection. We begin by exploring existing user needs for sharing, then evaluate the technical robustness of our audio-based design. We demonstrate the value of the approach for voice service users over several separate studies--including an eight-month extended field deployment--then conclude with a discussion of future possibilities for such scenarios.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {308–317},
numpages = {10},
keywords = {mobile sharing, developing regions, audio codes},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493201,
author = {Mathur, Akhil and Jaiswal, Sharad},
title = {Exploring the Interplay between Community Media and Mobile Web in Developing Regions},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493201},
doi = {10.1145/2493190.2493201},
abstract = {In this paper, we present the lessons learned by bringing in content and content-creators from local community media initiatives into the technological fold of the web. Our work focuses on the Community Radio (CR) ecosystem in India, and through extensive field-studies we develop an in-depth understanding of the operations, strengths and challenges of a CR station. Based on this, we outline the design of a system that combines the content creation processes of a CR station with the mobile web. The system was evaluated with the users of a CR station in Bangalore over a month long deployment. Our key take-away is that the incorporation of a mobile web based delivery system can play a critical role in expanding the reach and consumption of community media in the target communities. Conversely, the relevance of CR content and role of radio jockeys (as trusted members of the community) can be a key driver in adoption of the mobile web in these communities. Together, such a hybrid approach points the way forward for more successful deployments of community media systems, and reveals several interesting HCI issues to be studied further.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {318–327},
numpages = {10},
keywords = {ict4d, mobile web, hci4d, community radio},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493236,
author = {Panjwani, Saurabh and Ghosh, Mohona and Kumaraguru, Ponnurangam and Singh, Soumya Vardhan},
title = {The Paper Slip Should Be There! Perceptions of Transaction Receipts in Branchless Banking},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493236},
doi = {10.1145/2493190.2493236},
abstract = {Mobile-based branchless banking has become a key mechanism for enabling financial inclusion in the developing world. A key component of all branchless banking systems is a mechanism to provide receipts to users after each transaction as evidence for successful transaction completion. In this paper, we present results from a field study that explores user perceptions of different receipt delivery mechanisms in the context of a branchless banking system in India. Our study shows that users have an affinity for paper receipts: despite the provision of an SMS receipt functionality by the system developers and their discouragement of the use of paper, users have pro-actively initiated a practice of issuing and accepting paper receipts. Several users are aware of the security limitations of paper receipts but continue to use them because of their usability benefits. We conclude with design recommendations for receipt delivery systems in branchless banking.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {328–331},
numpages = {4},
keywords = {security, user study, branchless banking, mobile, receipts},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493207,
author = {Cycil, Chandrika and Perry, Mark and Laurier, Eric and Taylor, Alex},
title = { 'Eyes Free' in-Car Assistance: Parent and Child Passenger Collaboration during Phone Calls},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493207},
doi = {10.1145/2493190.2493207},
abstract = {This paper examines routine family car journeys, looking specifically at how passengers assist during a mobile telephone call while the drivers address the competing demands of handling the vehicle, interacting with various artefacts and controls in the cabin, and engage in co-located and remote conversations while navigating through busy city roads. Based on an analysis of video fragments, we see how drivers and child passengers form their conversations and requests around the call so as to be meaningful and paced to the demands, knowledge and abilities of their co-occupants, and how the conditions of the road and emergent traffic are oriented to and negotiated in the context of the social interaction that they exist alongside. The study provides implications for the design of car-based collaborative media and considers how hands- and eyes-free natural interfaces could be tailored to the complexity of activities in the car and on the road.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {332–341},
numpages = {10},
keywords = {qualitative, family, collaboration, in-car assistance, ethnography},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493237,
author = {B\"{o}hmer, Matthias and Saponas, T. Scott and Teevan, Jaime},
title = {Smartphone Use Does Not Have to Be Rude: Making Phones a Collaborative Presence in Meetings},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493237},
doi = {10.1145/2493190.2493237},
abstract = {Our personal smartphones are our daily companions, coming with us everywhere, including into enterprise meetings. This paper looks at smartphone use in meetings. Via a survey of 398 enterprise workers, we find that people believe phone use interferes with meeting productivity and collaboration. While individuals tend to think that they make productive use of their own phones in meetings, they perceive others as using their phones for unrelated tasks. To help smartphones create a more collaborative meeting environment, we present an application that identifies and describes meeting attendees. We deploy the application to 114 people at real meetings, and find that users value being able to access information about the other people in the room, particularly when those people are unfamiliar. To prevent users from disengaging from the meeting while using their phones, we employ a gaming approach that asks trivia questions about the other attendees. We observe that gameplay focuses attention within the meeting context and sparks conversations. These findings suggest ways smartphone applications might help users engage with the people around them in enterprise environments, rather than removing them from their immediate social context.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {342–351},
numpages = {10},
keywords = {smartphones, mobile, trivia game, social, meetings},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493225,
author = {Church, Karen and de Oliveira, Rodrigo},
title = {What's up with Whatsapp? Comparing Mobile Instant Messaging Behaviors with Traditional SMS},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493225},
doi = {10.1145/2493190.2493225},
abstract = {With the advent of instant mobile messaging applications, traditional SMS is in danger of loosing it's reign as the king of mobile messaging. Applications like WhatsApp allow mobile users to send real-time text messages to individuals or groups of friends at no cost. While there is a vast body of research on traditional text messaging practices, little is understood about how and why people have adopted and appropriated instant mobile messaging applications. The goal of this work is to provide a deeper understanding of the motives and perceptions of a popular mobile messaging application called WhatsApp and to learn more about what this service offers above and beyond traditional SMS. To this end, we present insights from two studies an interview study and a large-scale survey highlighting that while WhatsApp offers benefits such as cost, sense of community and immediacy, SMS is still considered a more reliable, privacy preserving technology for mobile communication.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {352–361},
numpages = {10},
keywords = {mobile instant messaging, short message service, interview, text messaging, user study, sms, survey, whatsapp},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493230,
author = {Sahami Shirazi, Alireza and Henze, Niels and Dingler, Tilman and Kunze, Kai and Schmidt, Albrecht},
title = {Upright or Sideways? Analysis of Smartphone Postures in the Wild},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493230},
doi = {10.1145/2493190.2493230},
abstract = {In this paper, we investigate how smartphone applications, in particular web browsers, are used on mobile phones. Using a publicly available widget for smart phones, we recorded app usage and the phones' acceleration and orientation from 1,330 devices. Combining app usage and sensor data we derive the device's typical posture while different apps are used. Analyzing motion data shows that devices are moved more while messaging and navigation apps are used as opposed to browser and other common applications. The time distribution between landscape and portrait depicts that most of the landscape mode time is used for burst interaction (e.g., text entry), except for Media apps, which are mostly used in landscape mode. Additionally, we found that over 31% of our users use more than one web browser. Our analysis reveals that the duration of mobile browser sessions is longer by a factor of 1.5 when browsers are explicitly started through the system's launcher in comparison to being launched from within another app. Further, users switch back and forth between apps and web browsers, which suggest that a tight and smooth integration of web browsers with native apps can improve the overall usability. From our findings we derive design guidelines for app developers.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {362–371},
numpages = {10},
keywords = {accelerometer, web browser, session, mobile phone, device motion, www, orientation, surfing, posture},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493217,
author = {Juhlin, Oskar and Weilenmann, Alexandra},
title = {Making Sense of Screen Mobility: Dynamic Maps and Cartographic Literacy in a Highly Mobile Activity},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493217},
doi = {10.1145/2493190.2493217},
abstract = {Dynamic, digital maps are increasingly used in many settings. It is an emerging domain of technology extending on previous maps studies and positioning technology. We draw upon ethnographic field studies of collaborative hunting, where hunting dogs are tracked and their location made visible on digital maps. We discuss mobility of two different kinds. First, we refer to mobility as the practice of physical movements of hunters, dogs and prey. Second, we refer to the movement of symbolic objects on a digital map screen, i.e. screen mobility, and the interpretational work that the hunters do to make sense of it. Representations of motion on a screens, are of ongoing practical concern for the hunters. We show how they interpret such mobility in terms of accelerations, distance, trajectories and temporal alignments. The findings are used to revisit mobility theories and populate them with new notions to inspire design in broad domains.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {372–381},
numpages = {10},
keywords = {dynamic maps, hunting, screen mobility, gps, mobile technology, dogs, ethnography},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493206,
author = {Buschek, Daniel and Rogers, Simon and Murray-Smith, Roderick},
title = {User-Specific Touch Models in a Cross-Device Context},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493206},
doi = {10.1145/2493190.2493206},
abstract = {We present a machine learning approach to train user-specific offset models, which map actual to intended touch locations to improve accuracy. We propose a flexible framework to adapt and apply models trained on touch data from one device and user to others. This paper presents a study of the first published experimental data from multiple devices per user, and indicates that models not only improve accuracy between repeated sessions for the same user, but across devices and users, too. Device-specific models outperform unadapted user-specific models from different devices. However, with both user- and device-specific data, we demonstrate that our approach allows to combine this information to adapt models to the targeted device resulting in significant improvement. On average, adapted models improved accuracy by over 8%. We show that models can be obtained from a small number of touches (≈60). We also apply models to predict input-styles and identify users.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {382–391},
numpages = {10},
keywords = {device-specific, touch, probabilistic modelling, machine learning, user-specific, regression},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493219,
author = {Hang, Alina and De Luca, Alexander and Hartmann, Jonas and Hussmann, Heinrich},
title = {Oh App, Where Art Thou? On App Launching Habits of Smartphone Users},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493219},
doi = {10.1145/2493190.2493219},
abstract = {In this paper, we present the results of a four-week real world study on app launches on smartphones. The results show that smartphone users are confident in the way they navigate on their devices, but that there are many opportunities for refinements. Users in our study tended to sort apps based on frequency of use, putting the most frequently used apps in places that they considered fastest to reach. Interestingly, users start most apps from within other apps, followed by the use of the homescreen.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {392–395},
numpages = {4},
keywords = {logging study, smartphone, app launch},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493202,
author = {Yu, Neng-Hao and Huang, Da-Yuan and Hsu, Jia-Jyun and Hung, Yi-Ping},
title = {Rapid Selection of Hard-to-Access Targets by Thumb on Mobile Touch-Screens},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493202},
doi = {10.1145/2493190.2493202},
abstract = {Current touch-based UIs commonly employ regions near the corners and/or edges of the display to accommodate essential functions. As the screen size of mobile phones is ever increasing, such regions become relatively distant from the thumb and hard to reach for single-handed use. In this paper, we present two techniques: CornerSpace and BezelSpace, designed to accommodate quick access to screen targets outside the thumb's normal interactive range. Our techniques automatically determine the thumb's physical comfort zone and only require minimal thumb movement to reach distant targets on the edge of the screen. A controlled experiment shows that BezelSpace is significantly faster and more accurate. Moreover, both techniques are application-independent, and instantly accommodate either hand, left or right.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {400–403},
numpages = {4},
keywords = {touch-screens, interaction techniques, mobile devices, thumb-based interaction, one-handed interaction},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493241,
author = {Weir, Daryl and Buschek, Daniel and Rogers, Simon},
title = {Sparse Selection of Training Data for Touch Correction Systems},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493241},
doi = {10.1145/2493190.2493241},
abstract = {Touch offset models which improve input accuracy on mobile touch screen devices typically require the use of a large number of training points. In this paper, we describe a method for selecting training points such that high performance can be attained with fewer data. We use the Relevance Vector Machine (RVM) algorithm, and show that performance improvements can be obtained with fewer than 10 training examples. We show that the distribution of training points is conserved across users and contains interesting structure, and compare the RVM to two other offset prediction models for small training set sizes.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {404–407},
numpages = {4},
keywords = {machine learning, sparse methods, touch},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493214,
author = {Lukander, Kristian and Jagadeesan, Sharman and Chi, Huageng and M\"{u}ller, Kiti},
title = {OMG! A New Robust, Wearable and Affordable Open Source Mobile Gaze Tracker},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493214},
doi = {10.1145/2493190.2493214},
abstract = {We present a novel, robust, affordable and wearable, mobile gaze tracker. The tracker takes a model-based approach to tracking gaze and maps the calculated gaze on to a scene video. The system is built from standard off-the-shelf components, and is the first to our knowledge using a 3D printed frame. The system will be published as open source, and the total cost of the components for building the system is 350∈. The model-based tracking provides a solution robust to changing lighting conditions and frame slippage on the head of the user.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {408–411},
numpages = {4},
keywords = {mobile, wearable, eye tracking, gaze tracking},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493194,
author = {Hwang, Sungjae and Bianchi, Andrea and Ahn, Myungwook and Wohn, Kwangyun},
title = {MagPen: Magnetically Driven Pen Interactions on and around Conventional Smartphones},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493194},
doi = {10.1145/2493190.2493194},
abstract = {This paper introduces MagPen, a magnetically driven pen interface that works both on and around mobile devices. The proposed device is accompanied by a new vocabulary of gestures and techniques that increase the expressiveness of the standard capacitive stylus. These techniques are: 1) detecting the orientation that the stylus is pointing to, 2) selecting colors using locations beyond screen boundaries, 3) recognizing different spinning gestures associated with different actions, 4) inferring the pressure being applied to the pen, and 5) identifying various pens associated with different operational modes. These techniques are achieved using commonly available smartphones that sense and analyze the magnetic field produced by a permanent magnet embedded in a standard capacitive stylus. This paper explores how magnets can be used to expand the design space of current pen interaction, and proposes a new technology to achieve such results.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {412–415},
numpages = {4},
keywords = {magnetometer, magnets, mobile device, pen interaction},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494087,
author = {Holleis, Paul and Luther, Marko and Broll, Gregor and Souville, Bertrand},
title = {A DIY Power Monitor to Compare Mobile Energy Consumption in Situ},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494087},
doi = {10.1145/2493190.2494087},
abstract = {Many current smartphones need to be recharged every day despite only average usage. This problem is intensified when phones need to track their location on a continuous basis. We provide a yet unavailable platform for accurately measuring the energy consumption of different hardware/software solutions comparing up to three variants in a mobile setting. We show that the accuracy of our system is similar to a fixed, commercial system but is especially useful to evaluate and optimize technology and algorithms that require the phone to be used on the go.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {416–421},
numpages = {6},
keywords = {energy consumption optimization, mobility measurement, mobile power monitor},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494088,
author = {Meschtscherjakov, Alexander and Gschwendtner, Christine and Tscheligi, Manfred and Sundstr\"{o}m, Petra},
title = {Co-Designing for NFC and ATMs: An Inspirational Bits Approach},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494088},
doi = {10.1145/2493190.2494088},
abstract = {This paper addresses the field of mobile payment through Near Field Communication (NFC) and Automated Teller Machines (ATMs). We report how we used the Inspirational Bits method to inspire the design of novel NFC usage scenarios and application ideas for ATMs in a joint industry and academia project. We describe a set of small applications exposing different properties of NFC (henceforth, referred to as NFC-Bits) and how they informed and inspired collaborative design ideas for different use cases. The NFC-Bits reveal a broad range of NFC characteristics in a playful manner. We outline some of the developed use cases and describe features of MyPocketATM, an application stemmed from these ideas.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {422–427},
numpages = {6},
keywords = {inspirational bits, nfc, atm},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494086,
author = {Bijani, Chaya and White, Brent-Kaan and Vilrokx, Mark},
title = {Giving Voice to Enterprise Mobile Applications},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494086},
doi = {10.1145/2493190.2494086},
abstract = {Speech technology is gaining popularity and is used in a wide range of mobile applications and consumer devices. Speech-based interfaces circumvent the typing difficulties posed by soft keyboards. This paper describes the design and research of a speech-based interface for an enterprise sales application that combines speech recognition, natural language processing, and web services to drive the application. The paper further delves into the research methods applied--paper prototype testing, structured interviews, and a lab study--to gather feedback at various design stages. Finally, the paper describes how each of the three distinct research methods provided researchers with a more complete understanding of the user experience and led to a redesign.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {428–433},
numpages = {6},
keywords = {design, speech based interfaces, mobile interfaces, research methods, enterprise application, voice input},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494089,
author = {Osswald, Sebastian and Zehe, Daniel and Mundhenk, Philipp and Sheth, Pratik and Schaller, Martin and Schickram, Stephan and Gleyzes, Daniel},
title = {HMI Development for a Purpose-Built Electric Taxi in Singapore},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494089},
doi = {10.1145/2493190.2494089},
abstract = {In this paper, we describe the development process of a human-machine interface (HMI) for a purpose-built electric taxi in Singapore. Based on the requirements of taxi drivers and passengers, we developed an automo-tive communication structure to support the connectivity between the vehicle HMI and mobile devices, support taxi-related functionalities and emphasize the charac-teristics of the electric vehicle. We carried out seven requirement studies in Singapore, implemented an integrated HMI platform, and developed different inter-face prototypes for the vehicle HMI and smart devices. This work contributes to the emerging field of automo-tive user interfaces by proposing a fully integrated HMI for an electric taxi.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {434–439},
numpages = {6},
keywords = {requirement engineering, taxi, development platform, hmi development, electric vehicle, human factors},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494658,
author = {Attarwala, Abbas and Munteanu, Cosmin and Baecker, Ronald},
title = {An Accessible, Large-Print, Listening and Talking e-Book to Support Families Reading Together},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494658},
doi = {10.1145/2493190.2494658},
abstract = {Reading is an activity that is not only informative or pleasurable, but can have significant social benefits. Especially in a family setting, it is part of the interaction between children and their parents, it helps create a bond between children and their grandparents, and even bring adults and their older parents closer. However, with families increasingly living or spending time in different locations or managing busy schedules that afford very little time together, the social opportunities enabled by reading are often lost. Furthermore, reading can be a challenge for older adults or for those with impaired eyesight. To address these problems, we are proposing ALLT -- an Accessible, Large-Print, Listening and Talking e-book. ALLT is a tablet-based e-reading application that enhances the capabilities of e-book readers through customizable and intelligent accessibility features. It provides support for asynchronous "reading together" by synchronizing the audio recording of one user with the text that is later read by another user. This addresses the needs of a variety of users, from visually impaired adults reading together with a loved one, to children being able to replay an interactive story previously read together with their grandparents. In this demo paper we present ALLT's features and detail how they support asynchronously reading together.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {440–443},
numpages = {4},
keywords = {assistive technology, aging, visual impairment, collaborative reading},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494659,
author = {H\"{u}rst, Wolfgang and Beurskens, Jannes and van Laar, Marco},
title = {An Experimentation Environment for Mobile 3D and Virtual Reality},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494659},
doi = {10.1145/2493190.2494659},
abstract = {Unlike desktop screens, mobile devices can be moved around freely. This allows us to create different experiences when exploring 3D data and virtual environments on such handhelds. Yet, this additional degree of freedom not only introduces exciting new possibilities, but also potential issues when used in actual applications. We present a demo environment that enables users to explore different kinds of 3D visualizations on smartphones and tablets, experiment with various characteristics and implementation options, and experience the advantages and disadvantages of these different realizations.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {444–447},
numpages = {4},
keywords = {interactive 3d, mobile 3d, mobile virtual reality},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494660,
author = {\v{C}opi\v{c} Pucihar, Klen and Coulton, Paul and Alexander, Jason},
title = {Creating a Stereoscopic Magic-Lens to Improve Depth Perception in Handheld Augmented Reality},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494660},
doi = {10.1145/2493190.2494660},
abstract = {Handheld Augmented Reality (AR) is often presented using the magic-lens paradigm where the handheld device is portrayed as if it was transparent. Such a virtual transparency is usually implemented using video captured by a single camera rendered on the device's screen. This removes binocular-disparity, which may undermine user's ability to correctly estimate depth when seeing the world through the magic-lens. To confirm such an assumption this paper presents a qualitative user study that compares a magic-lens implemented on a mobile phone and a transparent glass replica. Observational results and questionnaire analysis indicate that binocular-disparity may play a significant role in participants' depth perception. These promising results led to the subsequent implementation of a stereoscopic magic-lens prototype on a commercially available mobile device.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {448–451},
numpages = {4},
keywords = {stereoscopic rendering, parallax, mobile, depth perception, virtual transparency, handheld, binocular disparity, user study},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494654,
author = {\v{C}opi\v{c} Pucihar, Klen and Coulton, Paul},
title = {Enhanced Virtual Transparency in Handheld Ar: Digital Magnifying Glass},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494654},
doi = {10.1145/2493190.2494654},
abstract = {Handheld Augmented Reality (AR) is often presented using the magic-lens paradigm in which a magic-lens is a transparent interface. Such transparency is usually implemented by rendering camera captured video on the device's screen. The transparency quality is limited by the video stream quality which may be affected by: unfocused camera lens, poor lighting conditions and limited video stream resolution. All these factors may reduce the readability of the AR scene. To address quality of rendering and increase scene readability, this paper presents an enhanced virtual transparency solution where segments of the scene are replaced by high definition digital content. The proposed enhanced virtual transparency isdemonstrated through the design of a digital magnifying glass which has been implemented on of-the-shelf mobile phone.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {452–455},
numpages = {4},
keywords = {handheld, mobile, augmented reality, virtual transparency, magnifying glass, rendering},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494657,
author = {Hickey, Seamus and Karhu, Antti and Hyv\"{a}rinen, Juha and Arhippainen, Leena and Antoniac, Peter},
title = {Interaction with Services Using an Augmented Reality User Interface},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494657},
doi = {10.1145/2493190.2494657},
abstract = {This paper introduces a new concept of interacting with mobile devices while on the move. The concept is using a combination of interaction techniques, like AR, touch, tilting and rotating the mobile device. The validation is done using some basic scenarios in which the users interact with various services available on the street. The user interface elements are represented by AR anchored objects (still pictures) and floating objects that are giving visual cues to where services are available. The users are aided in their actions by freezing the user interface when the device is down tilted so that they could easily manipulate the objects.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {456–459},
numpages = {4},
keywords = {augmented reality, mobile touch screen device, sensors, 3d user interface},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494653,
author = {Fedosov, Anton and Eble, Tobias},
title = {Mobile Augmented Reality: Exploring Content in Natural and Controlled Settings Using 3d Tracking},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494653},
doi = {10.1145/2493190.2494653},
abstract = {The advent of the mobile device has propelled education, adoption and implementation of Augmented Reality on a massive scale. The success and evolution of this event and the underlying technology over the last 5 years is nearly indisputable proof that the industry is progressing, though incredible examples of useful augmented reality applications are yet to be explored. The ability to recognize images, markers and 3D objects is one of the most important aspects of Augmented Reality. With present work we are proposing three application examples, which make recognition and visual search more intuitive, natural and accessible.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {460–463},
numpages = {4},
keywords = {natural settings, poi, 3d tracking, augmented reality, slam, controlled settings, information visualization},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494652,
author = {Kato, Haruhisa and Yanagihara, Hiromasa},
title = {PACMAN UI: Vision-Based Finger Detection for Positioning and Clicking Manipulations},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494652},
doi = {10.1145/2493190.2494652},
abstract = {This paper proposes an intuitive input interface that can handle various operations based on finger image recognition. It receives continuous analog input by detecting a knuckle of the user's clenched fist. In contrast to the conventional wireless mouse, whose sensitivity cannot be changed dynamically, the proposed method brings not only stable positioning but also quick clicking with a small finger gesture. In order to evaluate operability, we conducted a user experiment: a time trial for target selection. The subjects completed the task with the proposed controller in 44% less time than with a conventional wireless mouse. We confirmed that the proposed method can reliably follow finger gestures.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {464–467},
numpages = {4},
keywords = {natural user interface, finger detection, computer vision},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494661,
author = {Feij\'{o} Filho, Jackson and Prata, Wilson and Valle, Thiago},
title = {Pufftext: A Puff Controlled Software-Based Hands-Free Spin Keyboard for Mobile Phones},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494661},
doi = {10.1145/2493190.2494661},
abstract = {This work proposes the use of a low-cost software-based puff controlled hands-free spinning keyboard for mobile phones as an alternative interaction technology for people with motor disabilities. It attempts to explore the processing of the audio from the microphone in mobile phones to select characters from a spinning keyboard. A proof of concept of this work is demonstrated by the implementation and experimentation of a mobile application prototype that enables users to perform text input through "puffing" interaction.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {468–471},
numpages = {4},
keywords = {breathing, text input, alternative hci, mobile, accessibility},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494655,
author = {Poppinga, Benjamin and Oehmcke, Stefan and Heuten, Wilko and Boll, Susanne},
title = {Storyteller: In-Situ Reflection on Study Experiences},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494655},
doi = {10.1145/2493190.2494655},
abstract = {Diary studies are often applied in HCI research to collect qualitative user impressions. Unfortunately, the period between creation of a diary entry and the later reflection can be too long, which leads to a limited currentness and contextuality. This eventually results in incomplete or misinterpreted data. In this paper we present Storyteller, a mobile application that allows a quick creation of diary entries and encourages users to reflect on these in-situ through a storytelling approach. We argue that this can lead to more accurate and substantial qualitative insights.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {472–475},
numpages = {4},
keywords = {diary study, storytelling, evaluation method},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494656,
author = {Timmermann, Janko and Poppinga, Benjamin and Pielot, Martin and Heuten, Wilko and Boll, Susanne},
title = {TimedNavigation: A Time-Based Navigation Approach},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494656},
doi = {10.1145/2493190.2494656},
abstract = {Travelers sometimes need to reach a destination in a given amount of time. However, today's navigation systems try to route users to the destination as fast as possible. In this paper, we present the concept of time-based pedestrian navigation. We use a map on a smartphone that highlights streets depending on whether they will lead to a destination in time. Our map also allows the users to choose between route alternatives during the walk.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {476–479},
numpages = {4},
keywords = {time, abstract maps, navigation},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494441,
author = {Lee, Jiunde and Chang, Yu-ning},
title = {A Study of Developing Auditory Icons for Mobile Phone Service Applications in Taiwan Region},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494441},
doi = {10.1145/2493190.2494441},
abstract = {Mobile technology advancements incite expansive opportunity spaces in value-added services to users. This however also introduces critical user experience problems (visual occlusion/clutter). Dynamic contexts of use of mobile phones and diverse functionalities compete for users' visual attention. Interaction designers might need to rethink the design and evaluation of such user interface types. The present study aimed to explore the possible design concepts and rationales of auditory icons for mobile phone service applications in Taiwan region.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {480–485},
numpages = {6},
keywords = {auditory icon, mental models, metaphor},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494435,
author = {Arif, Ahmed and Pahud, Michel and Hinckley, Ken and Buxton, WIlliam},
title = {A Tap and Gesture Hybrid Method for Authenticating Smartphone Users},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494435},
doi = {10.1145/2493190.2494435},
abstract = {This paper presents a new tap and gesture hybrid method for authenticating mobile device users. The new technique augments four simple gestures - up, down, left, and right, to the dominant digit lock technique, allowing users to either tap or perform any one of the four gestures on the digit keys. It offers in total 6250000 unique four-symbol password combinations, which is substantially more than the conventional techniques. Results of a pilot study showed that the new technique was slower and more error prone than the digit lock technique. However, we believe with practice it could get faster and more accurate. Also, most users were comfortable and all of them felt more secured while using the new technique.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {486–491},
numpages = {6},
keywords = {authentication, hybrid, security, pin, mobile, gesture},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494432,
author = {Stefanis, Vassilios and Komninos, Andreas and Plessas, Athanasios and Garofalakis, John},
title = {An Interface for Context-Aware Retrieval of Mobile Contacts},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494432},
doi = {10.1145/2493190.2494432},
abstract = {Our work discusses a mobile contact retrieval interface which attempts to contextually predict the contacts a user is likely to need access to, in order to facilitate the retrieval process. We compare our prototype implementation with retrieval from traditional applications (contact list and call log) in a preliminary lab experiment and discuss our findings from user behaviour. We conclude with suggestions on how to improve this interface in order to further enhance the retrieval process.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {492–497},
numpages = {6},
keywords = {context-awareness, mobile information retrieval, mobile contact lists},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494429,
author = {Costa, Pedro Maur\'{\i}cio and Pitt, Jeremy and Galv\~{a}o, Teresa and Falc\~{a}o e Cunha, Jo\~{a}o},
title = {Assessing Contextual Mood in Public Transport: A Pilot Study},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494429},
doi = {10.1145/2493190.2494429},
abstract = {In recent years, the technological developments in mobile and communication networks have paved the way for smart environments, whose final goal is to provide users with enhanced experiences. The measure of user experience satisfaction, or quality of experience, may be defined as an affective state in response to a service. Thus, an experiment was devised to explore the relationship between users' affective state and their context, for assessing quality of experience in urban public transport services. A pilot study, conducted to evaluate the feasibility and requirements of such an experiment is presented, leading to a large scale field study.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {498–503},
numpages = {6},
keywords = {quality of user experience, affective pervasive computing, experience sampling, pilot study, smart systems},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494428,
author = {Vihavainen, Sami and Karhu, Kimmo and Botero, Andrea},
title = {Connecting Stakeholders through Context Logging},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494428},
doi = {10.1145/2493190.2494428},
abstract = {Logging users' physical context through the sensors of mobile devices and the abilities to track users' online behavior has increasingly evolved. In this poster abstract we will present a work in progress in understanding how various stakeholders can be connected through context logging technologies, and the underlying motivations. We will (1) describe the general components and information flow of context logging systems, (2) frame the primary stakeholders from the context logging point of view, (3) discuss the application areas through the different stakeholders and their goals for applying context logging. Moreover, we will present ContextLogger3, a context-logging tool that combines automatically acquired sensor and mobile activity data and manually user created textual notes.We conclude by suggesting that designers should systematically consider implementing affordances for fostering design conversation between the users that log/are logged and the other stakeholders. We also suggest considering affordances that enable users to have manual control on context logging to narrow the gap between the realities represented by the automatic context logging and those perceived by the user.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {504–509},
numpages = {6},
keywords = {mobile, context logging, automation, stakeholders, manual},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494434,
author = {Prost, Sebastian and Schrammel, Johann and R\"{o}derer, Kathrin and Tscheligi, Manfred},
title = {Contextualise! Personalise! Persuade! A Mobile HCI Framework for Behaviour Change Support Systems},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494434},
doi = {10.1145/2493190.2494434},
abstract = {This paper presents a context-aware, personalised, persuasive (CPP) system design framework applicable to the sustainable transport field and other behaviour change support system domains. It operates on a situational, a user, and a target behaviour layer. Emphasis is placed on interlinking each layer's behaviour change factors for greater effectiveness. A prototype CPP system for more sustainable travel behaviour is introduced to demonstrate how the framework can be applied in practice.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {510–515},
numpages = {6},
keywords = {personalisation, persuasion, context-awareness},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494427,
author = {Kammer, Dietrich and Schmidt, Deborah and Keck, Mandy and Groh, Rainer},
title = {Developing Mobile Interface Metaphors and Gestures},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494427},
doi = {10.1145/2493190.2494427},
abstract = {Interaction design for mobile applications is challenging due to the diversity of technologies and devices. In addition, the now ubiquitous multi-touch screens demand novel and engaging interface metaphors. In this paper, we report insights and three practical results from a workshop with undergraduate students. The aim was to experiment with new technologies by providing a set of creativity techniques for ideation. By tackling interaction design both for tablets and smartphones, flexible interface metaphors were developed.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {516–521},
numpages = {6},
keywords = {creatitvity, multi-touch, design, gestures, multi-device},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494430,
author = {Wolbert, Michael and El Ali, Abdallah},
title = {Evaluating NFC and Touchscreen Interactions in Collaborative Mobile Pervasive Games},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494430},
doi = {10.1145/2493190.2494430},
abstract = {This paper presents the motivation, design, and pilot evaluation of CountMeIn, a pervasive collaborative game to improve the waiting time experience (e.g., waiting for a train, or traffic light to turn green). We tested two versions of CountMeIn, an NFC-based and touchscreen version in a small pilot study. Our early results showed that the NFC-based version increases collaboration, and was overall more positively perceived than the touchscreen version. We discuss the challenges ahead in deploying CountMeIn in a real-world setting.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {522–527},
numpages = {6},
keywords = {games, nfc, collaborative, urban, pervasive, touchscreen},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494431,
author = {D\"{o}ring, Tanja and Sylvester, Axel and Schmidt, Albrecht and Malaka, Rainer},
title = {Exploring Mobile Representations of Folksonomies to Support the Example Context of a Community Gardening Project},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494431},
doi = {10.1145/2493190.2494431},
abstract = {In this paper, we present results from an ongoing participatory design research project that focuses on the design and evaluation of a tagging-system and corresponding visualizations to support actors and visitors of an urban gardening project. The contributions of this work are threefold. First, it addresses the yet underexplored field of integrating context information beyond location and time into mobile tagging systems and folksonomies. Second, it suggests novel visualizations to explore tagged data and folksonomies beyond tag clouds and simple map representations on mobile phones. And third, it gives novel insights into supporting the embedded practices of a DIY (Do-It-Yourself) urban gardening community with interactive systems based on ethnographic fieldwork and participatory design.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {528–533},
numpages = {6},
keywords = {urban gardening, mobile visualizations, folksonomies, context, participatory design, mobile tagging system},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494422,
author = {Seipp, Karsten and Devlin, Kate},
title = {Landscape vs Portrait Mode: Which is Faster to Use on Your Smart Phone?},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494422},
doi = {10.1145/2493190.2494422},
abstract = {Touchscreen smart phones can be operated in portrait (P) and landscape (L) orientation. However, whether a device is faster to operate in P or L and where to put a button in each layout for best findability and operability remains unclear. This research makes a first attempt to examine in which orientation a touch-operated interface is faster to use and whether certain "zones" can be identified that have a particularly good performance in either orientation. Our results indicate that such zones exist in both L and P, and that L is faster to use than P. However, the effects are only visible when the user has not been primed with the target name. We conclude our study with practical advice for designers to improve usability and efficiency of time-critical applications and dialogues.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {534–539},
numpages = {6},
keywords = {scanning pattern, mobile hci, device orientation},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494436,
author = {Mayer, Julia M. and Zach, Jelena},
title = {Lessons Learned from Participatory Design with and for People with Dementia},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494436},
doi = {10.1145/2493190.2494436},
abstract = {In this paper we describe challenges and lessons learned from developing a mobile touch screen based assistive tool for people with dementia. We focus not on features of the tool but the general participatory design process that was applied. Insights presented are gained from interviews, focus groups and observations. We found that projecting problems on imaginary characters and using simple games specifically customized for people with dementia ease the process of eliciting user needs and realistic prototypes allow design evaluations with people with dementia early on.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {540–545},
numpages = {6},
keywords = {participatory design, dementia, assistive technology},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494439,
author = {Kocielinski, Daniel and Brzostek-Paw\l{}owska, Jolanta},
title = {Linear Interface for Graphical Interface of Touch-Screen: A Pilot Study on Improving Accessibility of the Android-Based Mobile Devices},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494439},
doi = {10.1145/2493190.2494439},
abstract = {The article presents the idea of creating a blind-friendly Versatile Multimodal Linear Interface (VMLI) model for touchscreen-based devices as well as the results of pilot study and implementation of VMLI.VMLI enhances operation of touchscreen-based mobile devices and facilitates the use of a touchscreen by the blind. VMLI transforms a planar layout of objects into hierarchically structured linear lists that allow non-sequential access to items. List items are read by VMLI using installed text-to-speech software; users navigate through lists and select items using specific touchscreen gestures preferred by the blind. VMLI allows choosing user's preferred method of text input: using a virtual QWERTY keyboard, a virtual Braille keyboard or a physical one of a Braille notetaker. The results of preliminary research on pilot implementation of VMLI into the Android system, concerning functions like managing contacts and composing messages to selected recipients, give grounds for continuing works on VMLI.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {546–551},
numpages = {6},
keywords = {touch screen, mobile accessibility, blind users},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494423,
author = {Deru, Matthieu and Bergweiler, Simon},
title = {Milky: On-Product App for Emotional Product to Human Interactions},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494423},
doi = {10.1145/2493190.2494423},
abstract = {In this paper we present a new way of emotional interaction with products. Based on the rapid prototyping Microsoft Gadgeteer platform, we concretized our vision of an on-product app by implementing an anthropomorphic intelligent milk carton. The purpose of this realization is to give customers a better view of a product's life-cycle. This realization also demonstrates that the frontier between pure mobile applications development and the creation of tangible objects is very thin and opens new way to integrate the Internet of Things over an anthropomorphic user interface, thus leading to a new product to human interaction form.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {552–557},
numpages = {6},
keywords = {product to human interactions, gadgeteer, anthropomorphic, embedded platform, internet of things, tangible objects, emotion, product life-cycle},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494433,
author = {Matthies, Denys J.C. and Nguyen, Ngo Dieu Huong and Lucas, Shaunna Janine and Botz, Daniel},
title = {Moving Shapes: A Multiplayer Game Based on Color Detection Running on Public Displays},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494433},
doi = {10.1145/2493190.2494433},
abstract = {Over the past few years the use of public displays has increased drastically, with the most common public displays being flat surface LED walls or projections on walls. Presently interactive public displays often make use of depth cameras. This paper introduces a cheaper variant that allows people to interact with the display and each other by using the color detection abilities of an ordinary webcam. As proof of concept a simple game was created that demonstrates how people are able to control and interact with photographed shapes via their own smartphones. Alternately a special hardware interface was built for users who do not own a smartphone. Contrary to ordinary games, this game works without points; instead, the leading user is awarded the ability to make decisions about game speed and is able to influence the audio through his movements.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {558–563},
numpages = {6},
keywords = {game, camera tracking, public displays, physical interface, art, mobile devices, interactive installation},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494443,
author = {M\"{u}ller, Heiko and Fortmann, Jutta and Timmermann, Janko and Heuten, Wilko and Boll, Susanne},
title = {Proximity Sensor: Privacy-Aware Location Sharing},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494443},
doi = {10.1145/2493190.2494443},
abstract = {In this paper we report on a participatory design study with young girls. Our goal was to create a mobile phone app to display the spatial proximity of friends in an abstract and privacy-aware manner. A group of 16 girls worked along the user-centred design process to create initial paper-based designs of an app that respects one's friends privacy while displaying their proximity to allow for spontaneous meetings or re-grouping after separation. Our participants created very promising results which we intend to implement and evaluate further against a broader audience.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {564–569},
numpages = {6},
keywords = {smartphone, privacy, participatory design, navigation},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494442,
author = {Law, Effie Lai-Chong and Bedall-Hill, Nicola Louise and Parry, Ross and Richards, Adair and Hawker, Melissa},
title = {Representing and Interpreting Reformation in the Wild},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494442},
doi = {10.1145/2493190.2494442},
abstract = {A mobile educational app combining the strengths of sound pedagogical frameworks Generic Learning Outcomes and of interactive technologies Augmented Reality (AR) and Quick Response (QR) code to deliver the historical information of the Howard family in the Reformation is being developed by an interdisciplinary team. Visiting the relatively small archaeological site -- Thetford Priory- with the new technologies may contribute to resolving an age-old puzzle through interpretations from multiple perspectives and to fostering the sustainability of such a site.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {570–575},
numpages = {6},
keywords = {qr code, reformation, cultural heritage, generic learning outcomes, archaeological sites, augmented reality},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494425,
author = {B\"{o}hmer, Matthias and Gehring, Sven and Hempel, Jonas and Kr\"{u}ger, Antonio},
title = {Revisiting Phone Call UIs for Multipurpose Mobile Phones},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494425},
doi = {10.1145/2493190.2494425},
abstract = {While mobile phones made a significant evolution in recent years from single-purpose communication devices to multi-purpose devices, the fundamental design of phone call applications did not evolve accordingly. While its implementation leveraged from new hardware and software capabilities, the fundamental decisions people are able to make when they receive a call did not change. Currently, when a call comes in, a modal dialog opens where the callee can either decline or accept the call. A recent study found that the current user interfaces of phone call applications (phone UIs) often lead to an increased overhead when application usage is being interrupted by phone calls [6]. In this paper, we revisit phone call UIs for multipurpose smartphones. We contribute a new design space for mobile phone call UIs, going beyond the simple accept-or-decline dilemma. We present a prototype implementation and discuss open challenges.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {576–581},
numpages = {6},
keywords = {phone calls, smartphones, design space, interruptions},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494440,
author = {Lotito, Antonio and Spoto, Giovanni Luca and Frisiello, Antonella and Macchia, Vito and Bolognesi, Thomas and Ru\`{a}, Francesco},
title = {Smart2poster. Bridging Information and Locality},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494440},
doi = {10.1145/2493190.2494440},
abstract = {This paper presents the Smart2Poster concept, a solution proposing an interaction modality aimed at bridging the information and the surrounding physical world, by means of familiar objects (a poster, a smartphone and/or a TV screen) and based on the Near Field Communication technology (NFC) that enables a local Peer-to-Peer communication without requiring further connectivity. Moreover, the paper describes the usage scenarios that driven the design and the implementation of the first prototype.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {582–587},
numpages = {6},
keywords = {proximity, smart-poster, smart tags, nfc, gestural interaction, user interaction},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494426,
author = {Micallef, Nicholas and Just, Mike and Baillie, Lynne and Kayacik, Gunes},
title = {Stop Questioning Me! Towards Optimizing User Involvement during Data Collection on Mobile Devices},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494426},
doi = {10.1145/2493190.2494426},
abstract = {Current methods of behavioral data collection from mobile devices either require significant involvement from participants to verify the 'ground truth' of the data, or approximations that involve post-experiment comparisons to seed data. In this paper we argue that user involvement can be gracefully reduced by performing more intelligent seed comparisons. We aim to reduce the participant involvement to the 'most interesting' temporal slots, both during the experiment and in post-experiment verification. We carried out a 2 week study with 4 users, consisting of an initial opportunistic gathering of mobile sensor data. Our findings suggest that by using such a method we can significantly reduce user involvement.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {588–593},
numpages = {6},
keywords = {mobile, hci, sensing},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494438,
author = {Terrenghi, Lucia and Garcia-Barrio, Laura and Oshlyansky, Lidia},
title = {Tablets Use in Emerging Markets: An Exploration},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494438},
doi = {10.1145/2493190.2494438},
abstract = {Tablet sales are growing worldwide and changing the landscape of personal computing. This is true across mature markets as well as emerging ones. However, little research has been done on the influence of tablets in the emerging markets. This paper presents insights gained during an exploratory study on the use of tablets in four cities: Sao Paulo, Mexico City, Jakarta and Bangalore. The results uncover similarities and differences in the use of tablets in mature markets versus emerging markets and identify implications for design across markets.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {594–599},
numpages = {6},
keywords = {tablets, emerging markets, user study},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494424,
author = {Alce, G\"{u}nter and Hermodsson, Klas and Wallerg\r{a}rd, Mattias},
title = {WozARd: A Wizard of Oz Tool for Mobile AR},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494424},
doi = {10.1145/2493190.2494424},
abstract = {Wizard of Oz methodology is useful when conducting user studies of a system that is in early development. It is essential to be able to simulate part of the system and to collect feedback from potential users. Using a human to act as the system is one way to do this.The Wizard of Oz tool presented here is called WozARd and it aims at offering a set of tools that help the test leader control the visual, tactile and auditive output that is presented to the test participant. Additionally, it is suitable for using in an augmented reality environment where images are overlaid on the phone's camera view or on glasses.The main features that were identified as necessary include presentation of media such as images, video and sound, navigation and location based triggering, automatically taking photos, capability to log test results and visual feedback, and the integration of Sony SmartWatch for interaction possibilities.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {600–605},
numpages = {6},
keywords = {augmented reality, multimodal interaction, wizard of oz, multimodal user interface},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2494437,
author = {Humayoun, Shah Rukh and Hess, Steffen and Kiefer, Felix and Ebert, Achim},
title = {I2ME: A Framework for Building Interactive Mockups},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2494437},
doi = {10.1145/2493190.2494437},
abstract = {Providing interactive mockups of mobile applications in designing phases introduces new challenges for interaction designers compared to the traditional way of static mockups. In particular, the complexity of this process increases when it comes to enabling the user to actually explore the intended user experience of the mobile environment by enhancing traditional handmade sketches or tool-generated wireframes with concrete mobile interaction elements. We introduce a framework, called i2ME (interactive Mockup-Building for Mobile Environment), for building interactive mockups with concrete mobile interaction elements. The framework enhances the static mockups (handmade or tool-generated) with screen transitions and multi-touch gestures, and enables the deployment of the resulting HTML5+JavaScript based interactive mockups to multiple platforms and device classes.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {606–611},
numpages = {6},
keywords = {interactive mockups, interaction design, multi-touch gestures},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2499469,
author = {Rost, Mattias and Morrison, Alistair and Cramer, Henriette and Bentley, Frank},
title = {Informing Future Design via Large-Scale Research Methods and Big Data},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2499469},
doi = {10.1145/2493190.2499469},
abstract = {With the launch of 'app stores' on several mobile platforms and the great uptake of smartphones among the general population, researchers have begun utilising these distribution channels to deploy research software to large numbers of users. Previous Research In The Large workshops have sought to establish base-line practice in this area. We have seen the use of app stores as being successful as a methodology for gathering large amounts of data, leading to design implications, but we have yet to explore the full potential for this data's use and interpretation. How is it possible to leverage the practices of large-scale research, beyond the current approaches, to more directly inform future designs? We propose that the time is right to re-energise discussions on large-scale research, looking further than the basic methodological issues and assessing the potential for informing the design of new mobile software.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {612–615},
numpages = {4},
keywords = {large-scale mobile hci, research in the large, design, mass participation, user study, app stores},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2499472,
author = {Seichter, Hartmut and Grubert, Jens and Langlotz, Tobias},
title = {Designing Mobile Augmented Reality},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2499472},
doi = {10.1145/2493190.2499472},
abstract = {The development of mobile Augmented Reality application became increasingly popular over the last few years. However, many of the existing solutions build on the reuse of available standard metaphors for visualization and interaction without considering the manifold contextual factors of their use. Within this workshop we want to discuss theoretical design approaches and practical tools which should help developers to make more informed choices when exploring the design space of Augmented Reality interfaces in mobile contexts.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {616–621},
numpages = {6},
keywords = {design, adaptive user interfaces, mobile, augmented reality},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2499468,
author = {Wilfinger, David and Meschtscherjakov, Alexander and Tscheligi, Manfred and Sundstr\"{o}m, Petra and Szostak, Dalila and McCall, Roderick},
title = {Entertainment Technology in Transportation against Frustration, Aggression and Irrationality},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2499468},
doi = {10.1145/2493190.2499468},
abstract = {This workshop addresses two strong fields within the Mobile HCI community: games &amp; entertainment and transportation user interfaces. Using transportation technology (e.g., a car, plane, or traveling in public transportation) can be frustrating due to crowded streets, delays, and other travelers. Frustration may lead to aggression and negative experiences of other road members and passengers [4] leading to irrational behaviors [6]. Games &amp; entertainment technology offer potential to resolve these negative user experiences. This workshop brings together entertainment and transportation user interface experts, who are willing to understand mobile entertainment technology as a potential solution to improve the experience of all travelers, drivers, and workers within the transportation field. The overall aim of the workshop is to create a common understanding of the challenges of entertainment in transportation, as well as further extend the research agenda for entertainment in this context from both from a scientific and an industrial perspective.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {622–625},
numpages = {4},
keywords = {transportation, entertainment, user interfaces, games},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2499471,
author = {Nanavati, Amit A. and Rajput, Nitendra and Srivastava, Saurabh and Erkut, Cumhur and Jylh\"{a}, Antti and Rudnicky, Alexander I. and Serafin, Stefania and Turunen, Markku},
title = {SiMPE: 8th Workshop on Speech and Sound in Mobile and Pervasive Environments},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2499471},
doi = {10.1145/2493190.2499471},
abstract = {The SiMPE workshop series started in 2006 with the goal of enabling speech processing on mobile and embedded devices. The SiMPE 2012 workshop extended the notion of audio to non-speech "Sounds" and thus the expansion became "Speech and Sound". SiMPE 2010 and 2011 brought together researchers from the speech and the HCI communities. Speech User interaction in cars was a focus area in 2009. Multimodality got more attention in SiMPE 2008. In SiMPE 2007, the focus was on developing regions.With SiMPE 2013, the 8th in the series, we continue to explore the area of speech along with sound. Akin to language processing and text-to-speech synthesis in the voice-driven interaction loop, sensors can track continuous human activities such as singing, walking, or shaking the mobile phone, and non-speech audio can facilitate continuous interaction. The technologies underlying speech processing and sound processing are quite different and these communities have been working mostly independent of each other. And yet, for multimodal interactions on the mobile, it is perhaps natural to ask whether and how speech and sound can be mixed and used more effectively and naturally.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {626–629},
numpages = {4},
keywords = {sound and music computing, sonic interaction, speech processing, mobile computing, sound},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2499470,
author = {Chiasson, Sonia and Crawford, Heather and Egelman, Serge and Irani, Pourang},
title = {U-PriSM 2: The Second Usable Privacy and Security for Mobile Devices Workshop},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2499470},
doi = {10.1145/2493190.2499470},
abstract = {The Second Usable Privacy and Security for Mobile Devices Workshop (U-PriSM 2) was held with MobileHCI'13. The U-PriSM 2 workshop was an opportunity for researchers and practitioners to discuss research challenges and experiences around the usable privacy and security of mobile devices (smart phones and tablets). Security often involves having non-security experts, or even novice users, regularly making important security decisions while their main focus is on other primary tasks. This is especially true for mobile devices where users can quickly and easily install apps, where user interfaces are minimal due to space constraints, and where users are often distracted by their environment.Participants had a chance to explore mobile device usage and the unique usable security and privacy challenges that arise, discuss proposed systems and ideas that address these needs, and work towards the development of design principles to inform future development in the area.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {630–632},
numpages = {3},
keywords = {usable privacy, mobile devices, usable security, human-computer interaction},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2499467,
author = {Humayoun, Shah Rukh and Hess, Steffen and Ebert, Achim},
title = {Workshop on Prototyping to Support the Interaction Designing in Mobile Application Development (PID-MAD 2013)},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2499467},
doi = {10.1145/2493190.2499467},
abstract = {Recent changes in the mobile environment; such as the addition of multi-touch gestures, usage of sensors, or single-focused mobile apps; brought several challenges for interaction designers in communicating their ideas and thoughts enduring early design activities. Traditional prototyping techniques may not provide sufficient support due to the lack of current mobile interaction paradigms in them. Therefore, a shift is required in prototyping techniques and approaches in order to support properly the interaction design process of mobile application development for the current mobile environment.Targeting these concerns, the workshop envisions that the research must address the need for a change in existing prototyping techniques as well as focusing on novel prototyping approaches and frameworks that would support not only the interaction design process but the whole development process of mobile application development.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {633–636},
numpages = {4},
keywords = {interaction design, prototyping, mobile application development},
location = {Munich, Germany},
series = {MobileHCI '13}
}

