@inproceedings{10.1145/3180308.3180309,
author = {Goto, Jun and Miyazaki, Taro and Takei, Yuka and Makino, Kiminobu},
title = {Automatic Tweet Detection Based on Data Specified through News Production},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180309},
doi = {10.1145/3180308.3180309},
abstract = {Posts made by individuals on social media have become an indispensable source of information for existing media. In this paper, we describe an automatic Tweet detection system for supporting news production to acquire useful information from social media such as Twitter. The system uses character-based bi-directional long short-term memory with an attention mechanism and learns newsworthy Tweets specified by news producers as training data on a continuous basis. The performance of the system is expected to be improved by learning additional training data generated from its own operation history in news production site.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {1},
numpages = {2},
keywords = {Social Media, News Production, Twitter, Tweet Detection, Neural Network},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180310,
author = {Cai, Minghao and Masuko, Soh and Tanaka, Jiro},
title = {Gesture-Based Mobile Communication System Providing Side-by-Side Shopping Feeling},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180310},
doi = {10.1145/3180308.3180310},
abstract = {This demo is a gesture-based communication framework designed for an immersive joint shopping scenario. It focuses on enhancing the mobile human-to-human interaction between two geographically separated users: an in-house user and an in-store user. By exploring the uses of depth-based tracking techniques and the integration of head-mounted displays and the spherical camera, we (1) construct an immersive virtual shopping environment for the in-house user remaining in a house and (2) offer a novel way for the users to achieve a real-time gestural communication in the physical shopping environment which the in-store user stays in, while (3) the latter gets an augmented reality experience. Through this demo, both users could share a feeling that they go for shopping side by side in the same place.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {2},
numpages = {2},
keywords = {Gesture communication, Immersive shopping, Remote collaboration},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180311,
author = {Zhang, Yiwei and Hu, Jiani and Sano, Shunmpei and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
title = {Computer Vision Based and FPRank Based Tag Recommendation for Social Popularity Enhancement},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180311},
doi = {10.1145/3180308.3180311},
abstract = {In previous work, we have proposed Folk Popularity Rank [4], which can score text tags based on their influence to images' popularity. In this work, we have implemented a tag recommendation system which can assist users in the tagging process. And we have proved that our recommended tags can help users gain higher popularity.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {3},
numpages = {2},
keywords = {Tagging, Social Media, Data Mining},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180312,
author = {Nakae, Kazuya and Tsukada, Koji},
title = {Support System to Review Manufacturing Workshop through Multiple Videos},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180312},
doi = {10.1145/3180308.3180312},
abstract = {With the increasing popularity of digital machine tools such as 3D printers, even common people are being involved in personal fabrication (Fab age). For example, manufacturing workshops using digital machine tools are often held by the Fab community (e.g., FabLab1). In such workshops, manufacturing processes are important experiences for users (both organizers and attendees). However, such processes are often difficult to record and review. This paper proposes a system for recording manufacturing workshops from multiple viewpoints, and helping users review manufacturing processes through multiple videos.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {4},
numpages = {2},
keywords = {manufacturing workshops, video edit, review, digital machine tools, personal fabrication},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180313,
author = {Ko, Meng-Chieh and Lin, Zih-Hong},
title = {CardBot: A Chatbot for Business Card Management},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180313},
doi = {10.1145/3180308.3180313},
abstract = {This article presents an application which combines chatbot and OCR techniques. The core value of CardBot is helping people manage business card cleverly and intuitively, and providing personalized services as a virtual assistant.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {5},
numpages = {2},
keywords = {Business Card, Conversational UI, Chatbot},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180314,
author = {Okuyama, Mizuki and Matoba, Yasushi and Siio, Itiro},
title = {Cylindrical M-Sequence Markers and Its Application to AR Fitting System for Kimono Obi},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180314},
doi = {10.1145/3180308.3180314},
abstract = {This paper proposes an m-sequence cylindrical marker, which is an optical barcode marker that can be applied on cylindrical objects, such as parts of a human body and everyday things such as furniture, bottles, and cups. We use two cycles of an m-sequence barcode to maintain continuity at the joint of the cylindrical marker. By using the m-sequence characteristics, the marker is able to acquire the rotation angle by recognizing only a certain part of the barcode. To confirm feasibility, we have made a cylindrical marker that fits on the waist of a human, and have implemented an AR fitting application for kimono obi.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {6},
numpages = {2},
keywords = {barcode optical marker, virtual fitting, Augmented Reality},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180315,
author = {Otsubo, Goro},
title = {Search Interface for Deep Thinking},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180315},
doi = {10.1145/3180308.3180315},
abstract = {Today's search engines such as Google are designed to provide "correct" answers to queries in the shortest time. However, in real life, we encounter issues for which no clear answers might exist. Such issues demand deep thought. We developed a search interface that supports deep thinking. A user inputs a proposition in the form of sentences. The system decomposes a sentence into components and searches the web using those components. Consequently, a user will obtain a related, but not an exact answer that might be effective to consider a proposition from various perspectives. Furthermore, the system searches any Idea Notes that the user might have saved when using the web service. Reviewing the results and collecting key phrases from them will encourage a user to think. We discuss the design and some user results.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {7},
numpages = {2},
keywords = {Visual interface, Search interface, Thinking},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180316,
author = {Massimo, David and Not, Elena and Ricci, Francesco},
title = {User Behaviour Analysis in a Simulated IoT Augmented Space},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180316},
doi = {10.1145/3180308.3180316},
abstract = {In this paper we present a demo application aimed at supporting the research in the field of tourism and mobility support in IoT augmented areas. The application collects tourists' choices while browsing Points of Interest (POIs) descriptions through a map-based interface that simulates user movement between POIs. Collected observations serve two purposes: the computation and testing of recommendation strategies for POIs (both for on-line and off-line studies); the generation of simulated users' behaviour under alternative scenario and context conditions (e.g., weather, or the presence of a novel POI).},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {8},
numpages = {2},
keywords = {Recommender Systems, IoT, Simulation Environment},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180317,
author = {Nishimura, Ayano and Itoh, Takayuki},
title = {Implementation of an Interactive System for the Translation of Lyrics},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180317},
doi = {10.1145/3180308.3180317},
abstract = {Recent evolution of the Internet has made it possible to listen to music that is composed worldwide in foreign languages. However, lyrics that are entirely scripted in foreign languages are difficult to understand. An interactive system for the translation of lyrics into Japanese is proposed herein, and its implementation is explained.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {9},
numpages = {2},
keywords = {Music, Interactive System, Translation of Lyrics, Lyrics},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180318,
author = {Zhi, Qiyu and Lin, Suwen and He, Shuai and Metoyer, Ronald and Chawla, Nitesh V.},
title = {VisPod: Content-Based Audio Visual Navigation},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180318},
doi = {10.1145/3180308.3180318},
abstract = {Current audio player interfaces generally provide brief information such as title and duration time and support basic playback control functions. These features alone are not sufficient for certain user tasks, such as quickly finding a previously-visited location or browsing the main topics covered in the audio content. We present VisPod, a visual audio player that visually displays the main topics and keywords extracted from the transcript. VisPod supports (1) audio content browsing, (2) topic-based and keyword-based navigation, (3) communication of transcript and speaker information in real time, and (4) content-based query. VisPod encodes audio as a donut chart comprised of topic segments, and uses text processing algorithms to segment the transcript into independent topics and utilizes a deep learning model to generate human-readable topic names. An informal study suggests users prefer VisPod over traditional audio playback approaches specifically with regards to its benefits for audio browsing and navigation.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {10},
numpages = {2},
keywords = {Topic Separation, Topic Generation, Audio Browsing, Deep Learning, Audio Navigating},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180319,
author = {Hubbard, Layne Jackson and Erkocevic, Boskin and Cassady, Dylan and Cheng, Chen Hao and Chamorro, Andrea and Yeh, Tom},
title = {MindScribe: Toward Intelligently Augmented Interactions in Highly Variable Early Childhood Environments},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180319},
doi = {10.1145/3180308.3180319},
abstract = {Early childhood is a period of critical development, with impacts that can last a lifetime. And inequities in the quality of care for this vulnerable population---especially for those at-risk due to disability, family income, or trauma---can perpetuate further downstream health and school-readiness effects. Technology-enabled solutions have the ability to bridge quality-of-care gaps by intelligently augmenting daily activities. However, many traditional computational approaches to natural language interactions are not yet feasible nor afford-able in highly variable and dynamic early childhood environments. Yet for rapidly developing preliterate young children, solutions are needed now. We present MindScribe, an interactive robotic object that leverages open-ended 'serve and return' natural language interactions to intelligently support reflective inquiry and school-readiness in highly variable and imaginative early childhood environments.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {11},
numpages = {2},
keywords = {Early childhood development, Voice interaction, Accessibility, Natural language, Human-agent collaboration, Constructionism, User-centered design, Interactive robotic objects},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180320,
author = {Wang, Xuan and Lu, Chunmeng and Masuko, Soh and Tanaka, Jiro},
title = {Interactive Online Shopping with Personalized Robot Agent},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180320},
doi = {10.1145/3180308.3180320},
abstract = {Recently, the interaction between human and intelligent agents has become an increasingly hot topic. In this area, entertainment has always played a very important role as an essential element in enriching the user experience. At this research we combined Robohon, a new generation of smart robots, to act as a new type of agent to achieve a better shopping experience through interaction with personal computers and human, which we called as tripartite guiding system. A lot of voice and action is used as the main elements to interact with humans. In the process, the intelligent robot acts as a guide to help people with a better shopping experience step by step.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {12},
numpages = {2},
keywords = {Robohon, Intelligent agent, Shopping experience, Human-computer interaction},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180321,
author = {Kawai, Yasuo and Kaizu, Yurie and Kawahara, Kenta and Obuchi, Youhei and Otsuka, Satoshi and Tomimatsu, Shiori},
title = {Development of a Tsunami Evacuation Behavior Simulation System with Massive Evacuation Agents},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180321},
doi = {10.1145/3180308.3180321},
abstract = {We present an evacuation behavior simulation system at the time of tsunami using the evacuation agent of a game engine. The tsunami evacuation behavior simulation system is a system that enables local governments and residents to work together to formulate disaster prevention plans. As a representation of a simple tsunami, in this system, a slightly oblique plane is inserted as the sea surface to a three-dimensional model of the terrain, building, and road, which is generated by geographical information system data. Mass agents of evacuees randomly placed on the road will move toward the nearest wide evacuation shelter or tsunami evacuation building. As a result, problems were found in the shape of some roads and the position of evacuation buildings.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {13},
numpages = {2},
keywords = {Tsunami, Disaster Prevention Planning, Evacuation Behavior, Natural Disaster Evacuee, Disaster Visualization},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180322,
author = {Araki, Hayato and Ikeda, Taichi and Ozawa, Takumi and Kawahara, Kenta and Kawai, Yasuo},
title = {Development of a Horror Game That Route Branches by the Player's Pulse Rate},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180322},
doi = {10.1145/3180308.3180322},
abstract = {In this work, we developed a horror game that changes the effect of fear by player's physiological information. We adopt pulse rate as physiological information in which fear emotions appear and decides to measure using a pulse rate meter as a physiological information sensor. In this system, we prepared a mechanism to select a route that gives fear emotions to players according to the pulse rate. As a result of the trial play, we were able to effectively give fear feelings to players who did not feel fear.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {14},
numpages = {2},
keywords = {Pulse Rates, Fear, Horror, Game Engine, Body Reaction},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180323,
author = {Sato, Kunihiko and Rekimoto, Jun},
title = {Detecting Utterance Scenes of a Specific Person},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180323},
doi = {10.1145/3180308.3180323},
abstract = {We propose a system that detects the scene, where a specific speaker is speaking in the video, and displays the site as a heat map in the video's timeline. This system enables users to skip to the timeline they want to hear by detecting scenes in a drama, talk show, or discussion TV program, where a specific speaker is speaking. To detect a specific speaker's utterance, we develop a deep neural network (DNN) to extract only a specific speaker from the original sound source. We also implement the detection algorithm based on the output of the proposed DNN and the interface for displaying the detection result.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {15},
numpages = {2},
keywords = {video interface, Scene detection, deep learning, sound source separation, timeline},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180324,
author = {Shmueli-Scheuer, Michal and Sandbank, Tommy and Konopnicki, David and Nakash, Ora Peled},
title = {Exploring the Universe of Egregious Conversations in Chatbots},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180324},
doi = {10.1145/3180308.3180324},
abstract = {As part of the raging commercial development of chatbot systems, the ability to improve the quality of conversations quickly and consistently is crucial. In this work, we focus on egregious conversations and the dialog failure points that lead to these extremely bad dialogs. We present a tool that helps chatbot designers exploring and prioritizing failure points that need to be handled. Each failure is accompanied with an explanation and a suggested remedy.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {16},
numpages = {2},
keywords = {Egregious, Tooling, Chatbots systems},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180325,
author = {Rateau, Hanae and Rekik, Yosra and Lank, Edward and Grisoni, Laurent},
title = {Ether-Toolbars: Evaluating Off-Screen Toolbars for Mobile Interaction},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180325},
doi = {10.1145/3180308.3180325},
abstract = {In mobile interaction, the use of touchscreen interaction, while beneficial from the perspective of portability, has limited spatial accuracy due to the "fat finger problem". As a result, an important challenge on mobile interaction is to find solutions to balance the size of individual widgets against the number of widgets needed during interaction. In this work, to address display space limitations, we explore the design of invisible off-screen toolbars (ether-toolbars) that leverage computer vision to expand application features by placing widgets adjacent to the display screen. We demonstrate a prototype system consisting of an inexpensive 3D printed mount for mirror that supports ether-toolbar implementations.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {17},
numpages = {2},
keywords = {ether-toolbar, ether-widgets, Around-device interaction, mobile interaction},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180326,
author = {Toch, Eran and Rager, Netta and Florentin, Tal and Linenberg, Dan and Sellman, Daya and Shomron, Noam},
title = {Augmented-Genomics: Protecting Privacy for Clinical Genomics with Inferential Interfaces},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180326},
doi = {10.1145/3180308.3180326},
abstract = {Recent advances in genetic technology enable large-scale genome sequencing, creating new avenues for research and clinical care. At the same time, these advances raise growing concerns about data protection and privacy. In this demonstration, we present Augmented-Genomics, a system that puts the individuals in control over their genetic information. To envision user-controllable privacy in the hospital, and in other complex clinical situations, we demonstrate several techniques accessible through a mobile application. The system infers the risk of exposing certain parts of the genome and provide a simple interface for users to set their desired level of exposure. Patients and caregivers (such as doctors) exchange visual keys that are used to decrypt genomic data while indirectly fostering discussion and negotiation over the patient's privacy. After the patient provides the permission, the caregiver can access information about essential genes and mutations through a mobile interface or through an augmented reality glasses.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {18},
numpages = {2},
keywords = {Usable Privacy, Mobile Applications, Genetic Data},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180327,
author = {Prange, Alexander and Barz, Michael and Sonntag, Daniel},
title = {Medical 3D Images in Multimodal Virtual Reality},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180327},
doi = {10.1145/3180308.3180327},
abstract = {We present a multimodal medical 3D image system for radiologists in an virtual reality (VR) environment. Users can walk freely inside the virtual room and interact with the system using speech, going through patient records, and manipulate 3D image data with hand gestures. Medical images are retrieved from the hospital's Picture and Archiving System (PACS) and displayed as 3D objects inside VR. Our system incorporates a dialogue-based decision support system for treatments. A central supervised patient database provides input to our predictive model and allows us, first, to add new examination reports by a pen-based mobile application on-the-fly, and second, to get therapy prediction results in real-time. This demo includes a visualisation of real patient records, 3D DICOM radiology image data, and real-time therapy predictions in VR.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {19},
numpages = {2},
keywords = {decision support, hand and pen gestures, multimodality, position sensor, radiology, 3D images, speech, virtual reality},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180328,
author = {Agrawal, Harshit and Yamaoka, Junichi and Kakehi, Yasuaki},
title = {(Author)Rise: Artificial Intelligence Output Via the Human Body},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180328},
doi = {10.1145/3180308.3180328},
abstract = {We are increasingly offloading a lot of our mental and subjective tasks to machines. Over the last few years, with advances in machine learning, especially in deep learning, machines are becoming increasingly capable of modeling and being more intimately involved in our everyday tasks. Though a lot of the tasks we do today are related with manipulating virtual data, our bodies still belong in a physical space and make up a large part of our activities. Artificial intelligence (AI) interactions are generally limited to on-screen or audio format, and therefore lack support in our physical tasks where our bodies play a major role. Also, being behind the screen and virtual in nature, we often do not acknowledge the extent to which AI plays a role in a lot of our subjective tasks. We present (author)rise, a physical human-machine intelligence based handwriting system, where machines generate handwritten text in continuation to human handwriting and move the human hand on the paper to write it out, thereby tightly coupling human and machine intelligence in the same physical input-output space.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {20},
numpages = {2},
keywords = {physical interface, handwriting, authorship, Artificial Intelligence},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180329,
author = {Roemmele, Melissa and Gordon, Andrew S.},
title = {Automated Assistance for Creative Writing with an RNN Language Model},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180329},
doi = {10.1145/3180308.3180329},
abstract = {This work demonstrates an interface, Creative Help, that assists people with creative writing by automatically suggesting new sentences in a story. Authors can freely edit the generated suggestions, and the application tracks their modifications. We make use of a Recurrent Neural Network language model to generate suggestions in a simple probabilistic way. Motivated by the theorized role of unpredictability in creativity, we vary the degree of randomness in the probability distribution used to generate the sentences, and find that authors' interactions with the suggestions are influenced by this randomness.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {21},
numpages = {2},
keywords = {Computational Creativity, Story Generation, Writing Support},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180330,
author = {Axtell, Benett and Munteanu, Cosmin},
title = {Frame of Mind: Using Storytelling for Speech-Based Clustering of Family Pictures},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180330},
doi = {10.1145/3180308.3180330},
abstract = {Mobile technologies, like tablets, allow complete family picture collections to be accessed from anywhere on a single, portable device, but still do not support browsing and reminiscing from family pictures as one would from an album. This is especially a problem for older adults who are more motivated to share their memories, but often have less access to their physical pictures due smaller living spaces. Frame of Mind offers simple tablet interactions for family picture reminiscence. The app uses the implicit speech interaction of oral storytelling to automatically organize pictures into album-like sets from the prompted memories. Users can modify and filter the sets, but do not need to manually sort and organize their pictures. This simplifies the overall process of family picture interactions by leveraging one enjoyable aspect to automate a more effortful one.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {22},
numpages = {2},
keywords = {implicit speech interaction, explainable interactions, Digital storytelling, older adults},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180331,
author = {Itou, Shiori and Iseki, Masaaki and Kato, Shingo and Nakamoto, Takamichi},
title = {Olfactory and Visual Presentation Using Olfactory Display Using SAW Atomizer and Solenoid Valves},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180331},
doi = {10.1145/3180308.3180331},
abstract = {This article introduces our demonstration content employing a desktop type olfactory display and a head mount display(HMD). An olfactory display is a device that delivers fragrance to one or more users. We developed the olfactory display using a surface acoustic wave (SAW) device and high-speed solenoid valves. Since the SAW device can atomize liquid droplets forcibly based on SAW streaming phenomenon, even low-volatile odor compound can be presented. By using high-speed solenoid valves, it is possible to perform quantitative control in multichannel precisely, so that olfactory display can produce various fragrances. By using this olfactory display together with the HMD, more realistic feeling can be given to the user. We have developed the content using both visual and olfactory cues. Through the HMD, people walk in the maze and get cocktail ingredients in to virtual environment. In the maze goal people can enjoy the scent from olfactory display.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {23},
numpages = {2},
keywords = {atomizer, olfaction, solenoid valve, Olfactory display, SAW, scent},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180332,
author = {Shibata, Daisaku and Wakamiya, Shoko and Ito, Kaoru and Miyabe, Mai and Kinoshita, Ayae and Aramaki, Eiji},
title = {VocabChecker: Measuring Language Abilities for Detecting Early Stage Dementia},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180332},
doi = {10.1145/3180308.3180332},
abstract = {Recently, dementia patients have been increasing in number worldwide, necessitating the development of techniques to detect dementia as early as possible. Considering that a typical symptom of dementia, especially Alzheimer's disease, is language impairment, speech-based dementia detection approaches have drawn much attention. This paper presents a smartphone-based dementia screening application, VocabChecker, which measures language abilities from a speech narrative via automatic speech recognition (ASR). It measures four language abilities related to dementia: number of tokens (token), number of types (type), type token ratio (TTR), and potential vocabulary size (PVS). We also reported that the use of VocabChecker has distinguished dementia patients from elderly people.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {24},
numpages = {2},
keywords = {NLP, Dementia, iOS application, Language ability},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180333,
author = {Gopalakrishnan, Gopakumar and Aithal, Madhusudhan M. and Pasala, Anjaneyulu},
title = {Visual Analytics of Organizational Performance Network},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180333},
doi = {10.1145/3180308.3180333},
abstract = {In this demo paper, we present an integrated visualization application with an interactive user interface for analyzing organizational performance. The aim of this application is to provide holistic view of the performance of multiple teams and help initiate focused actions to improve it. The data about team structures, members and, their achievements against goals during different performance periods is visualized through various interactive interfaces. Further, the application analyzes and highlights performance variations across different metrics, teams and, members to provide actionable insights.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {25},
numpages = {2},
keywords = {user interface, Organizational performance, visual analytics, information visualization},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180334,
author = {Wang, Yuanyuan and Zhang, Yihong and Siriaraya, Panote and Kawai, Yukiko and Jatowt, Adam},
title = {Language Density Driven Route Navigation System for Pedestrians Based on Twitter Data},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180334},
doi = {10.1145/3180308.3180334},
abstract = {Navigation systems often aid tourists traveling in unfamiliar cities. This work presents a novel navigation system for suggesting routes based on traveler density estimated from Twitter. We aim to recommend two types of routing: one is for tourist spots on a route frequently visited by large groups of tourists, and the other one is for suggesting a route with a small number of expected tourists. Our method is based on shallow processing of tweets and can be used in online applications. We demonstrate online route navigation system that recommends routes based on diversity of foreign languages spoken in two touristically popular cites: San Francisco and Kyoto.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {26},
numpages = {2},
keywords = {foreign languages, Twitter, route navigation},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180335,
author = {Hamanishi, Natsuki and Kono, Michinari and Suwa, Shunichi and Miyaki, Takashi and Rekimoto, Jun},
title = {Flufy: Recyclable and Edible Rapid Prototyping Using Fluffed Sugar},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180335},
doi = {10.1145/3180308.3180335},
abstract = {Low-Fab (low-fidelity fabrication) has allowed designers to speed up the initial prototyping of 3D objects. However, the method is used to quickly verify the key aspects of the model, and are not the final desired object. Therefore, the object generated by Low-Fab are temporal objects in which the disposability and/or reusability of the material should be considered. Here we present a novel prototyping method beyond the concept of Low-Fab, to quickly fabricate large, disposable, reusable and edible objects by using fluffed sugar for the material. We have implemented a fabrication system that supports the user to create desired objects in high-speed, which is based on the idea of cotton candy making. Our idea is to combine food fabrication techniques with rapid prototyping methods, which we believe that it can contribute to both designers and gastronomy.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {27},
numpages = {2},
keywords = {Rapid Prototyping, Edible, Interactive Fabrication, Food},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180336,
author = {Takano, Riku and Wakita, Ken},
title = {Fluid UI for HIGH-Dimensional Analysis of Social Networks},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180336},
doi = {10.1145/3180308.3180336},
abstract = {Social Viewpoint Finder (SVF) is a visual analytics tool for social networks and complex networks. SVF lays out the input network data on a HIGH-dimensional (500-3,000 dimensional) Euclidean space, offers a simple, but unique dragging-based UI to trigger computation-intensive, high-dimensional rotation of the presented network, and let the user investigate the clustering structure of the network. This demonstration presents the effectiveness of SVF as well as the employed implementation techniques that enabled the fluid, complex user interaction. To achieve fluidness, SVF heavily relies on modern OpenGL technologies. It achieves massively parallel computing through the use of the compute shader, and graphic pipelines. A large volume of the network data and its layout information is stored in a shader storage buffer. A fragment shader-based, efficient object identification method is devised.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {28},
numpages = {2},
keywords = {Modern OpenGL, Network Visualization, Interaction},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180337,
author = {Pirrung, Meg and Hilliard, Nathan and O'Brien, Nancy and Yankov, Artem and Corley, Court D. and Hodas, Nathan O.},
title = {SHARKZOR: Human in the Loop ML for User-Defined Image Classification},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180337},
doi = {10.1145/3180308.3180337},
abstract = {Sharkzor is a web-based user interface for user defined image classification. We present here a human in the loop system with interactions focusing on 3 main user tasks. The user triages a number of images by organizing them into arbitrary groups with few examples. Sharkzor's sophisticated few-shot learning back end then approximates the user's mental model and automates organization of the entire dataset.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {29},
numpages = {2},
keywords = {Human in the Loop, Deep Learning, User Experience, Automation, One Shot Learning},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180339,
author = {Miyazaki, Koichi and Tobita, Hiroaki},
title = {SinkAmp: Interactive Sink to Detect Living Habits for Healthcare and Quality of Life},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180339},
doi = {10.1145/3180308.3180339},
abstract = {We have developed SinkAmp which detects living habits (e.g., hand wash, face wash, gargle) from the sound of water flow and talks interactively to users through synthetic sounds. Healthcare has been getting important recently, especially for the elderly. In such situations, a monitoring service is one of the solutions to watch a target person continuously through a camera and report whether the person is healthy or not through e-mail. However, camera-based monitoring makes people's activities limited, because the camera becomes a mental barrier to people. In contrast, our SinkAmp is set inside the sink where people wash their hands and often drink water, so invisible and ambient monitoring is possible. This paper describes our SinkAmp focusing on its implementation and initial evaluation.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {30},
numpages = {2},
keywords = {machine learning, healthcare, quality of life, support vector machine, sink, Interactive system},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180340,
author = {Reinhardt, Daniel and Hurtienne, J\"{o}rn},
title = {Cursor Entropy Reveals Decision Fatigue},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180340},
doi = {10.1145/3180308.3180340},
abstract = {Recognizing impairments of the user's ability to make goal-related decisions is an important feature for intelligent user interfaces, because this points to a higher need to support the user by the intelligent system. Here, we introduce using the entropy of cursor movements as an indicator of decision fatigue. We report an empirical proof-of-concept study that manipulates the amount of decision fatigue in participants. The results show that cursor entropy is increased for people with higher decision fatigue than for people with lower decision fatigue. Thus, intelligent user interfaces become capable of detecting decision fatigue on the basis of cursor entropy and could recognize when users need assistance in their decision making.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {31},
numpages = {2},
keywords = {Decision fatigue, Cursor Entropy, Ego Depletion},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180341,
author = {Pituxcoosuvarn, Mondheera and Ishida, Toru and Yamashita, Naomi and Takasaki, Toshiyuki and Mori, Yumiko},
title = {Supporting a Children's Workshop with Machine Translation},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180341},
doi = {10.1145/3180308.3180341},
abstract = {Previous studies have investigated the characteristics of machine translation(MT)-mediated communication in lab settings and suggested various ways to improve it [1]. Unfortunately, we still lack an understanding of how MT is used in real-world settings, particularly when people use it to support face-to-face communication. In this paper, we report on a field study of a multilingual workshop where children from various language regions used MT to communicate with each other. We investigate how children use various information such as non-verbal cues and drawings to compensate for the mistranslations of MT. For example, children tried to understand the mistranslated messages by reading alternative translations and used web browsers to search for pictures of unknown objects. Such findings provide insights for designing future multilingual support systems.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {32},
numpages = {2},
keywords = {multilingual workshop, machine translation, field study},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180342,
author = {Umezawa, Yuki and Hirayama, Takatsugu and Enokibori, Yu and Mase, Kenji},
title = {Egocentric Video Multi-Viewer for Analyzing Skilled Behaviors Based on Gaze Object},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180342},
doi = {10.1145/3180308.3180342},
abstract = {In many intellectual tasks, efficient succession of human physical and sensory skills is a long-standing issue. In order to analyze skilled behaviors, a useful approach is to compare same task scenes among workers or days and to understand these differences. In this paper, we propose an egocentric scene classification method based on objects which the worker turned the gaze to and a multi-viewer for egocentric videos comparison. We have experimented on proposed classification method with videos of painting watercolor.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {33},
numpages = {2},
keywords = {multi-viewer, skill analysis, gaze, Egocentric video},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180343,
author = {Lu, Yichao and Dong, Ruihai and Smyth, Barry},
title = {Convolutional Matrix Factorization for Recommendation Explanation},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180343},
doi = {10.1145/3180308.3180343},
abstract = {In this paper, we introduce a novel recommendation model, which harnesses a convolutional neural network to mine meaningful information from customer reviews, and integrates it with matrix factorization algorithm seamlessly. It is a valid method to improve the transparency of CF algorithms.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {34},
numpages = {2},
keywords = {Recommendation Explanation, Matrix Factorization, Customer Reviews, Convolutional Neural Network},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180344,
author = {Chen, Ching-Chun and Wu, Chia-Min and Shen, I-Chao and Chen, Bing-Yu},
title = {A Deep Learning Based Method For 3D Human Pose Estimation From 2D Fisheye Images},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180344},
doi = {10.1145/3180308.3180344},
abstract = {We propose a deep learning based method to directly estimate the human joint positions in 3D space from 2D fisheye images captured in an egocentric manner. The core of our method is a novel network architecture based on Inception-v3 [4], featuring the asymmtric convolutional filter size, the long short-term memory module, and the anthropomorphic weights on the training loss. We demonstrate our method outperform state-of-the-art method under different tasks. Our method can be helpful to develop useful deep learning network for human-machine interaction and VR/AR applications.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {35},
numpages = {2},
keywords = {3D Human Pose Estimation, Fisheye Image, Convolutional Neural Networks, Egocentric View, Anthropomorphic Weights},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180345,
author = {Hayashi, Toshinori and Wang, Yuanyuan and Kawai, Yukiko and Sumiya, Kazutoshi},
title = {A Recommender System Based on Detected Users' Complaints by Analyzing Reviews},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180345},
doi = {10.1145/3180308.3180345},
abstract = {Even though the popularity of e-commerce recommender systems continues to spread, traditional recommender systems might not recommend alternatives that can solve the problems of the items flagged by users. Since users need to choose options on existing services, e.g., screen quality and size, it remains difficult to satisfy their requirements. Therefore, we propose a novel item recommender system based on our analysis of complaint data and review comments on e-commerce. Our system first generates negative feature vectors from complaint data and positive feature vectors from review comments. Next, it calculates the similarities of these two vectors and identifies reviews that can solve the complaints about the items checked by users, and provides alternatives for each user complaint. In this paper, we describe our proposed recommendation method based on complaint and review data.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {36},
numpages = {2},
keywords = {review, e-commerce, recommender system, complaint},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180346,
author = {Totsuka, Takashi and Kinoshita, Yuichiro and Shiraga, Shota and Go, Kentaro},
title = {Impression-Based Fabrication: A Framework to Reflect Personal Preferences in the Fabrication Process},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180346},
doi = {10.1145/3180308.3180346},
abstract = {This study proposes impression-based fabrication, a new framework of digital fabrication that takes into account personal preferences and impressions. Based on users' desired impressions, the two-component framework generates multiple designs for a target product. The first component, a design generation unit, is where the designs are generated. It converts the input desired impressions into physical features. These designs are then subjected to an impression evaluation model where their appropriateness to the desired impression is evaluated. The user then receives the 3D printing data of high-scoring designs. A custom design tool for picture frames was constructed as a case study to demonstrate the feasibility of the framework. A case study confirmed that the design tool was able to create designs that had the desired impression on users.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {37},
numpages = {2},
keywords = {user perception modeling, personal fabrication, design support, Impression},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180347,
author = {Arinami, Suguru and Suzuki, Yu},
title = {Information Display Method to Give the Non-Mechanical Impression by Imitating the Communication with Pets},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180347},
doi = {10.1145/3180308.3180347},
abstract = {Display devices often present information in a mechanical manner to users. In this study, we propose an information display method providing a non-mechanical representation. The method makes the information display resemble communication with pets. As an example, we developed the Medaka Clock, which uses the position of killifish to respond to the user's request for the time of day.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {38},
numpages = {2},
keywords = {taxis, interactive display, killifish, animal, pet},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180350,
author = {Yang, Chi-Lan and Wang, Hao-Chuan},
title = {Can You Help Me without Knowing Much? Exploring Cued-Knowledge Sharing for Instructors' Tutorial Generation},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180350},
doi = {10.1145/3180308.3180350},
abstract = {How-to tutorials and videos are valuable for self-learning. While it is common for task instructors to produce tutorials alone, sharing knowledge when performing physical tasks without external support can be challenging to the experts. The resulting tutorials may also appear incomprehensible to learners who are not involved in the process of tutorial generation. We investigate cued-knowledge sharing, which pairs instructors with partners of different levels of expertise who also participate tutorial generation and give cues (prompts, questions etc.) to the instructors to facilitate their knowledge externalization. In a laboratory study, experienced cooks performed a cooking task and think aloud with cues from three types of paired partners, another experienced cook, a novice cook, or no partner. We noted that experts produced more clarifications when being paired with novice-partners than experienced-partners. The results demonstrate that (1) having a partner participate and provide cues may influence the content of knowledge sharing, and (2) even if the partner doesn't know much about the task (i.e., novice), their external cues remain helpful, which shed light on developing intelligent support for instructors' tutorial generation in the procedural and physical domains.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {41},
numpages = {2},
keywords = {think aloud, learner, Knowledge sharing, instructor, how-to knowledge, intelligent tutorial generation support},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180351,
author = {Senno, Bachar and Barcha, Pedro},
title = {Customizing User Experience with Adaptive Virtual Reality},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180351},
doi = {10.1145/3180308.3180351},
abstract = {Virtual reality is becoming more prominent than ever. The improved performance of mobile devices, along with the growing interest in more immersive user experiences, have allowed virtual reality to gain solid standing in this regard: nowadays, a smartphone and a pair of VR goggles are all that is needed for a full VR experience. The uses of VR systems extend beyond home entertainment: in particular, we are tackling the possibility that VR can be used by therapists to aid patients suffering from neurodevelopmental disorders (NDD) [1,2]. To that, an adaptive system that can change different aspects of the VR environment, using machine learning techniques, can go a long way in increasing the efficiency of NDD treatments.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {42},
numpages = {2},
keywords = {Mobile Virtual Reality, Machine Learning, Adaptive System},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180352,
author = {Shmueli-Scheuer, Michal and Herzig, Jonathan and Sandbank, Tommy and Konopnicki, David},
title = {On the Expression of Agent Emotions in Customer Support Dialogs in Social Media},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180352},
doi = {10.1145/3180308.3180352},
abstract = {Providing customer support through social media channels is gaining popularity. In this context, it is important for the agent, either human or automated, to express emotions when needed. In this paper we study the patterns of human agents emotional expression and question the need of Affective NLG. By analyzing real dialogs that were tagged by the crowd, we show that emotional responses contribute to the quality of the dialog and relate to customer satisfaction, and that human agents express emotions by simply adding emotional acknowledgment at the beginning of the response.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {43},
numpages = {2},
keywords = {NLG, Dialog, Affective, Twitter, Customer service},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180353,
author = {Kim, Heesun and Lee, Dongeon and Kim, Min Gyeong and Jang, Hyejin and Park, Ji-Hyung},
title = {Omni-Gesture: A Hand Gesture Interface for Omnidirectional Devices},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180353},
doi = {10.1145/3180308.3180353},
abstract = {We present "Omni-Gesture", a new omnidirectional hand gesture interface for smart interactive devices. Unlike voice user interface, this interface recognizes the direction of a user's gesture and provides visual feedback accordingly. We developed an experimental prototype system with Omni-Gesture in order to explore it in usability. The system has a cylindrical screen, implementing an interactive aquarium application. Omni-Gesture provides more accurate feedbacks for users in three stages. First, the system recognizes the gesture by analyzing camera images. Second, it integrates the recognized data into direction information. Last, it identifies the user's control between "Access" and "Shift", and then it provides feedback toward the direction. Through the preliminary study, we found that using Omni-Gesture for omnidirectional devices is promising if the recognition speed can be improved, especially when moving. We also discuss future work for developing various gesture sets to use alongside other interfaces and applying Omni-Gesture to other omnidirectional platforms.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {44},
numpages = {2},
keywords = {Omnidirectional Devices, Hand Gesture, 360 degree recognition},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180354,
author = {Brishtel, Iuliia and Ishimaru, Shoya and Augereau, Olivier and Kise, Koichi and Dengel, Andreas},
title = {Assessing Cognitive Workload on Printed and Electronic Media Using Eye-Tracker and EDA Wristband},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180354},
doi = {10.1145/3180308.3180354},
abstract = {With the expansion of e-learning platforms, we receive a great opportunity to learn and study just using an electronic device. In this paper, we measured the differences in information processing on screen and paper with 18 participants using an eye-tracker and an EDA wristband. Our findings show that the media type has a significant influence on cognitive workload and understandability of the content. The results of this work are of vital importance for the design of new intelligent user interfaces and reveal the necessity to take mental processes of users more into account.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {45},
numpages = {2},
keywords = {Reading, Cognitive Workload, Information Processing, Eye-Tracking, Electrodermal Activity, E-learning, User-Interface},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180355,
author = {George, Ceenu and Eiband, Malin and Hufnagel, Michael and Hussmann, Heinrich},
title = {Trusting Strangers in Immersive Virtual Reality},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180355},
doi = {10.1145/3180308.3180355},
abstract = {Social interactions in immersive virtual reality (IVR) benefit from more realistic designed avatars whilst head mounted displays (HMD) are simultaneously offering virtual reality experiences with improving levels of immersion and presence. The combination of these developments creates a need to understand how users remit trust towards avatars in IVR. We evaluated trust towards two categories of avatars (robot vs. human-like) in VR by conducting a lab study (N=2l) where participants had to play a trust game (TG) with each avatar. Our findings highlight that although the trust game revealed equal trust levels towards both categories of avatars, participants felt a significant sense of "togetherness" with the human-like avatar compared to the robot.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {46},
numpages = {2},
keywords = {VR, Avatar, Trust, HRI},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180356,
author = {Okada, Kaya and Yoshida, Mitsuo and Itoh, Takayuki and Czauderna, Tobias and Stephens, Kingsley},
title = {Spatio-Temporal Visualization of Tweet Data around Tokyo Disneyland Using VR},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180356},
doi = {10.1145/3180308.3180356},
abstract = {Social media analysis is important to understand people behavior. Human behavior in social media is often related to time and location, which is often difficult to understand the characteristics appropriately and quickly. We chose to visualize the date applying a virtual reality technology, which makes us easier to explore the data interactively and intuitively. This poster presents our visualization with tweets of microblogs with location information. Our system includes a three-dimensional temporal visualization which consists of the two-dimensional map and a time axis. In particular, we aggregate the number of tweets of each coordinate, calculate scores and display them as piled cubes. We highlight only specific cubes so that users can understand the overall tendency of datasets. We also developed user interfaces for operating these cubes and panels which indicate details of tweets.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {47},
numpages = {2},
keywords = {Virtual Reality, immersive visualization, Temporal visualization, Tweet},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180357,
author = {Hast, Anders and Vats, Ekta},
title = {An Intelligent User Interface for Efficient Semi-Automatic Transcription of Historical Handwritten Documents},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180357},
doi = {10.1145/3180308.3180357},
abstract = {Transcription of large-scale historical handwritten document images is a tedious task. Machine learning techniques, such as deep learning, are popularly used for quick transcription, but often require a substantial amount of pre-transcribed word examples for training. Instead of line-by-line word transcription, this paper proposes a simple training-free gamification strategy where all occurrences of each arbitrarily selected word is transcribed once, using an intelligent user interface implemented in this work. The proposed approach offers a fast and user-friendly semi-automatic transcription that allows multiple users to work on the same document collection simultaneously.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {48},
numpages = {2},
keywords = {Transcription, Gamification, Handwritten text recognition},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180358,
author = {Ben-Shoshan, Hagit and Mokryn, Osnat},
title = {ActiveMap: Visual Analysis of Temporal Activity in Social Media Sites},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180358},
doi = {10.1145/3180308.3180358},
abstract = {Social media users are becoming more active by the day. Tracking their temporal activity patterns along time demands high processing and large storage volume. Analyzing and understanding both the global activity characteristics and personal activity patterns for different users is challenging. We examine here a visualization method that draws a snapshot of the temporal activity, specifically of content generated by users. Our visualization enables comparisons between sites and within site activities for different periods, while giving both a global and a comparative personal view in a single snapshot. This inner comparative view also highlights irregular activities. We demonstrate our findings with three different social media datasets obtained from Amazon, IMDb and Twitter.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {49},
numpages = {2},
keywords = {Social Media, Temporal Activity, Treemap},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180359,
author = {Yoo, Sangbong and Jeong, Sujin and Jang, Yun},
title = {Gaze Data Clustering and Analysis},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180359},
doi = {10.1145/3180308.3180359},
abstract = {Eye-Gaze contains useful information, especially, unconscious intention of human. However, it is difficult to distinguish between noise and intentional or unconscious movement from the gaze data. Moreover, matching stimuli with the data is not easy since many Area of Interest (AoI) clustering algorithms generate unwanted information. Therefore, the gaze data analysis and utilization are still limited. The intentions in the gaze data can be categorized as navigational and informational. Hence, different visualization technique and analysis should be applied for each intention. In this paper, we study gaze data analysis using various data processing techniques, such as cluster and filter with corresponding visualization.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {50},
numpages = {2},
keywords = {Gaze data visualization, Data clustering, Gaze analysis},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180360,
author = {Yong, Seraphina and Wang, Hao-Chuan},
title = {Using Spatialized Audio to Improve Human Spatial Knowledge Acquisition in Virtual Reality},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180360},
doi = {10.1145/3180308.3180360},
abstract = {Picture schematization, the construction of spatial object maps from images, has useful applications ranging from indoor exploration to augmented reality. Since using human spatial knowledge improves schematization, crowdsourcing workflows are introduced for extracting spatial information from pictures. As 360° pictures are now available in virtual reality (VR), crowdsourced 360° picture schematization also becomes essential. Yet, the vergence-accommodation conflict (VAC) in head-mounted displays (HMDs) causes inaccurate spatial perception in VR. We propose integration of spatial audio in VR as a cost-effective and intuitive feature to support spatial perception. This study indicates spatial audio cues in VR are naturally incorporated by humans to significantly improve human spatial knowledge accuracy and, subsequently, crowdsourcing schematization.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {51},
numpages = {2},
keywords = {spatial audio, virtual reality, crowdsourcing, picture schematization, Spatial memory},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180361,
author = {Caremel, Cedric and Liu, Gemma and Chernyshov, George and Kunze, Kai},
title = {Muscle-Wire Glove: Pressure-Based Haptic Interface},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180361},
doi = {10.1145/3180308.3180361},
abstract = {This work follows a novel approach to information presentation through haptics using muscle-wire. The muscle-wire is extremely lightweight, silent and does not require complex circuitry. Its property to shrink in length when an electric current is applied, can have many interesting applications, some of which we present as a proof of concept of our vision. In this paper, we describe our initial prototype and a first series of user tests that demonstrates the possibility of representing discrete and continuous informations.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {52},
numpages = {2},
keywords = {Soft actuators, Haptic interface},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180362,
author = {Chen, Mei-Ling and Wang, Hao-Chuan},
title = {How Personal Experience and Technical Knowledge Affect Using Conversational Agents},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180362},
doi = {10.1145/3180308.3180362},
abstract = {Conversational agents (CA) use dialogues to interact with users so as to offer an experience of naturalistic interaction. However, due to the low transparency and poor explanability of mechanism inside CA, individual's understanding of CA's capabilities may affect how the individual interacts with CA and the sustainability of CA use. To examine how users' understanding affect perceptions and experiences of using CA, we conducted a laboratory study asking 41 participants performed a set of tasks using Apple Siri. We independently manipulated two factors: (1) personal experience of using CA, and (2) technical knowledge about CA's system model. We conducted mixed-method analyses of post-task usability measures and interviews, and confirmed that use experience and technical knowledge affects perceived usability and mental models differently.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {53},
numpages = {2},
keywords = {mental models, Conversational agents, explainable intelligent user interfaces},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180363,
author = {Sethi, Ricky J. and Buell, Catherine A. and Seeley, William P.},
title = {WAIVS: An Intelligent Interface for Visual Stylometry Using Semantic Workflows},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180363},
doi = {10.1145/3180308.3180363},
abstract = {In this paper, we present initial work towards creating an intelligent interface that can act as an open access laboratory for visual stylometry called WAIVS, Workflows for Analysis of Images and Visual Stylometry. WAIVS allows scholars, students, and other interested parties to explore the nature of artistic style using cutting-edge research methods in visual stylometry. We create semantic workflows for this interface using various computer vision algorithms that not only facilitate artistically significant analyses but also impose intelligent semantic constraints on complex analyses. In the interface, we combine these workflows with a manually-curated dataset for analysis of artistic style based on either the school of art or the medium.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {54},
numpages = {2},
keywords = {Semantic Workflows, Artistic Style, Visual Stylometry},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180364,
author = {Han, June and Hope, Tom},
title = {Pair Matching: Transdisciplinary Study for Introducing Computational Intelligence to Guide Dog Associations},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180364},
doi = {10.1145/3180308.3180364},
abstract = {There have been attempts to tackle visual impairment issues in Human-Computer Interaction (HCI). However, we can scarcely see HCI research outcomes are widely used by the blind. In this research, we aim to investigate new design possibilities as a transdisciplinary approach for guide dog training and education. We work with guide dog trainers, the blind, and guide dogs. In detail, we focus on matching guide dogs with visually impaired people. This work contributes to igniting research on designing interactive systems for the community of the visually impaired and also other human-animal-matching processes.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {55},
numpages = {2},
keywords = {Machine Learning, Visual Impairment, Human-Computer Interaction, Ethnography, Guide Dogs},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180365,
author = {Akbar, Fatema and Grover, Ted and Mark, Gloria and Zhou, Michelle X.},
title = {The Effects of Virtual Agents' Characteristics on User Impressions and Language Use},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180365},
doi = {10.1145/3180308.3180365},
abstract = {Virtual agent (VA) interaction is becoming more pervasive in work and home life. We present a study of text-based VA-user interaction in the context of a real-world job interview. Two VAs were designed with distinct personalities and different genders. Virtual interviews were conducted by the VAs with 316 job applicants for an entry-level position at a financial firm. We investigated how different VA characteristics affect impression of, and interaction with, the VA. Results show that when gender of users and VAs were dissimilar, VAs were perceived more extroverted and personable. When genders were similar, users tended to rate VAs as more trustworthy. We also found differences in authenticity by users who interacted with different VAs.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {56},
numpages = {2},
keywords = {Virtual agents, computer supported social interaction, computer personality, intelligent user interface, human-agent interaction, virtual interviewer},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180366,
author = {Costa, Felipe and Ouyang, Sixun and Dolog, Peter and Lawlor, Aonghus},
title = {Automatic Generation of Natural Language Explanations},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180366},
doi = {10.1145/3180308.3180366},
abstract = {An interesting challenge for explainable recommender systems is to provide successful interpretation of recommendations using structured sentences. It is well known that user-generated reviews, have strong influence on the users' decision. Recent techniques exploit user reviews to generate natural language explanations. In this paper, we propose a character-level attention-enhanced long short-term memory model to generate natural language explanations. We empirically evaluated this network using two real-world review datasets. The generated text present readable and similar to a real user's writing, due to the ability of reproducing negation, misspellings, and domain-specific vocabulary.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {57},
numpages = {2},
keywords = {Neural Network, Explanations, Natural Language Generation, Explainability, Recommender systems},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180367,
author = {Zheng, Yong},
title = {Personality-Aware Decision Making In Educational Learning},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180367},
doi = {10.1145/3180308.3180367},
abstract = {Personality, as one of the human factors, has been demonstrated as an influential element in decision makings. Its impact in educational learning is still under investigation and there are very few of available data sets in this area. In this paper, we introduce one data that is collected from user studies, describe our exploratory analysis and discuss the corresponding research topics. Furthermore, we encourage more discussions or ideas about the data collection, experimental design, promising topics and research challenges.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {58},
numpages = {2},
keywords = {educational learning, decision making, personality},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180368,
author = {Tsai, Chun-Hua and Brusilovsky, Peter},
title = {Explaining Social Recommendations to Casual Users: Design Principles and Opportunities},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180368},
doi = {10.1145/3180308.3180368},
abstract = {Recommender systems have become popular in recent years, and ordinary users are more likely to rely on such service when completing various daily tasks. The need to design and build explainable recommender interfaces is increasing rapidly. Most of the designs of such explanations are intended to reflect the underlying algorithms by which the recommendations are computed. These approaches have been shown to be useful for obtaining system transparency and trust. However, little is known about how to design explanation interfaces for causal (non-expert) users to achieve different explanatory goals. As a first step toward understanding the user interface design factors, we conducted an international (across 13 countries) online survey of 14 active users of a social recommender system. This study captures user feedback in the field and frames it in terms of design principles and opportunities.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {59},
numpages = {2},
keywords = {Explainable Intelligent User Interface, Social Recommender},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180369,
author = {Cioat\u{a}, Petru-Vasile and Vatavu, Radu-Daniel},
title = {In Tandem: Exploring Interactive Opportunities for Dual Input and Output on Two Smartwatches},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180369},
doi = {10.1145/3180308.3180369},
abstract = {We introduce, explore, and prototype in this work new interactive opportunities for dual input and output on two smartwatches that users wear on both hands and operate simultaneously. To this end, we present practical implementations of wearable applications designed for two smartwatches working "in tandem" that demonstrate new ways to perform generic input and output tasks, such as item selection from a list, text entry on a tiny soft keyboard, and generic content manipulation on small screens. We hope that this work will draw the community's attention towards the richness of interactions that become possible by utilizing not one, but two smartwatches in tandem and, consequently, will inspire new and rich interface designs for the computers that we wear on our wrists.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {60},
numpages = {2},
keywords = {Bimanual input, Interactive prototypes, Text entry, Item selection, Touch input, Synchronous input, Gesture input, Input/output devices, Smartwatches, Wearables},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180370,
author = {Xiao, Ziang and Yao, Yuqi and Fu, Wai-Tat},
title = {An Intelligent Educational Platform for Training Spatial Visualization Skills},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180370},
doi = {10.1145/3180308.3180370},
abstract = {Current research has demonstrated the crucial role of spatial visualization skills in the Science, technology, engineering, and mathematics (STEM) related fields. Given the nationwide trend of the increasing population of university engineering students, more and more effort has been invested into how to effectively train students' spatial visualization skills. In this study, we designed and developed a scalable online platform for training spatial visualization skills. Our online platform is capable of intelligent learning features such as individualized learning trajectory and personalized hints. In the current stage of the project, we deployed the online platform with over 600 university students. By analyzing student's data, we identified a key question that has most predictive power on student's spatial visualization skill. The promising result shows a bright future for an intelligent learning platform.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {61},
numpages = {2},
keywords = {Education, Individualized Learning, Learning Analytics, Spatial Visualization Skills, Personalized Hints, STEM},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180371,
author = {Kawahara, Takumi and Iwai, Daisuke and Sato, Kosuke},
title = {Dynamic Path Planning of Flying Projector Considering Collision Avoidance with Observer and Bright Projection},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180371},
doi = {10.1145/3180308.3180371},
abstract = {In this paper, we propose to compute a quadcopter-mounted projector's position so that the luminance of the projected image is maximized while the image covers the entire surface. We also compute a path of the flying projector to move close to the surface while avoiding the collision with an observer. In addition, the projected image is geometrically corrected to display undistorted image on a projection surface. They are solutions to inherent problems of the flying projector. First, a small quadcopter has a low payload and cannot carry a bright projector, because such projector is typically heavy. Second, there is a danger of collision between quadcopter and observer. We confirmed the effectiveness of our methods through an experiment.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {62},
numpages = {2},
keywords = {quadcopter, Projector, augmented reality},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180372,
author = {Bahirat, Paritosh and Sun, Qizhang and Knijnenburg, Bart P.},
title = {Scenario Context v/s Framing and Defaults in Managing Privacy in Household IoT},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180372},
doi = {10.1145/3180308.3180372},
abstract = {The Internet of Things provides household device users with an ability to connect and manage numerous devices over a common platform. However, the sheer number of possible privacy settings creates issues such as choice overload. This article outlines a data-driven approach to understand how users make privacy decisions in household IoT scenarios. We demonstrate that users are not just influenced by the specifics of the IoT scenario, but also by aspects immaterial to the decision, such as the default setting and its framing.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {63},
numpages = {2},
keywords = {Internet of Things, Data-Driven Design, Privacy},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

@inproceedings{10.1145/3180308.3180373,
author = {Ruane, Elayne and Faure, Th\'{e}o and Smith, Ross and Bean, Dan and Carson-Berndsen, Julie and Ventresque, Anthony},
title = {BoTest: A Framework to Test the Quality of Conversational Agents Using Divergent Input Examples},
year = {2018},
isbn = {9781450355711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180308.3180373},
doi = {10.1145/3180308.3180373},
abstract = {Quality of conversational agents is important as users have high expectations. Consequently, poor interactions may lead to the user abandoning the system. In this paper, we propose a framework to test the quality of conversational agents. Our solution transforms working input that the conversational agent accurately recognises to generate divergent input examples that introduce complexity and stress the agent. As the divergent inputs are based on known utterances for which we have the 'normal' outputs, we can assess how robust the conversational agent is to variations in the input. To demonstrate our framework we built ChitChatBot, a simple conversational agent capable of making casual conversation.},
booktitle = {Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion},
articleno = {64},
numpages = {2},
keywords = {Conversational Agent Quality Assessment, Chatbot, Conversational Agent Testing},
location = {Tokyo, Japan},
series = {IUI '18 Companion}
}

