@inproceedings{10.1145/2876456.2882847,
author = {Kerren, Andreas and Cernea, Daniel and Pohl, Margit},
title = {Workshop on Emotion and Visualization: EmoVis 2016},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2882847},
doi = {10.1145/2876456.2882847},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {1–2},
numpages = {2},
keywords = {visualization, affective computing, emotion measurement, emotion detection, emotion-enhanced visualization, emotion-enhanced interaction, emotion-driven adaptation},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2882849,
author = {Schnelle-Walka, Dirk and Limonad, Lior and Grosse-Puppendahl, Tobias and Lanir, Joel and M\"{u}ller, Florian and Mecella, Massimo and Luyten, Kris and Kuflik, Tsvi and Brdiczka, Oliver and M\"{u}hlh\"{a}user, Max},
title = {SCWT: A Joint Workshop on Smart Connected and Wearable Things},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2882849},
doi = {10.1145/2876456.2882849},
abstract = {The increasing number of smart objects in our everyday life shapes how we interact beyond the desktop. In this workshop we discuss how advanced interactions with smart objects in the context of the Internet-of-Thingsshould be designed from various perspectives, such as HCI and AI as well as industry and academia.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {3–5},
numpages = {3},
keywords = {novel interaction, multimodal and adapter interaction, hci, enabling techologies, embodied interaction, smart objects, context-awareness, tangible interaction},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2882846,
author = {Knijnenburg, Bart P.},
title = {Evaluating Intelligent User Interfaces with User Experiments},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2882846},
doi = {10.1145/2876456.2882846},
abstract = {User experiments are an essential tool to evaluate the user experience of intelligent user interfaces. This tutorial teaches the practical aspects of designing and setting up user experiments, as well as state-of-the-art methods to statistically evaluate the outcomes of such experiments.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {6–8},
numpages = {3},
keywords = {user experiments, user experience, intelligent user interfaces, user-centric evaluation},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879475,
author = {Shimonishi, Kei and Kawashima, Hiroaki and Schaffer, Erina and Matsuyama, Takashi},
title = {Tracing Temporal Changes of Selection Criteria from Gaze Information},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879475},
doi = {10.1145/2876456.2879475},
abstract = {To design interactive systems that proactively assist users' decision making, the users' gaze information is an important cue for the system to estimate users' selection criteria. Users sometimes change selection criteria while browsing content. Therefore, temporal changes of those criteria need to be traced from gaze data in short time scales. In this paper, we propose an approach to detecting users' distinctive browsing periods with its appropriate time-scale by leveraging multiscale exact tests so that the system can trace temporal changes of selection criteria. We demonstrate the applicability of the proposed method through a toy example and experiments.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {9–12},
numpages = {4},
keywords = {multiscale exact test, decision making, gaze information},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879476,
author = {Suzuki, Genta and Murase, Taichi and Fujii, Yusaku},
title = {Projecting Recorded Expert Hands at Real Size, at Real Speed, and onto Real Objects for Manual Work},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879476},
doi = {10.1145/2876456.2879476},
abstract = {Expert manual workers in factories assemble more efficiently than novices because their movements are optimized for the tasks. In this paper, we present an approach to projecting the hand movements of experts at real size, and real speed and onto real objects in order to match the manual work movements of novices to those of experts. We prototyped a projector-camera system, which projects the virtual hands of experts. We conducted a user study in which users worked after watching experts work under two conditions: using a display and using our prototype system. The results show our prototype users worked more precisely and felt the tasks were easier. User ratings also show our prototype users watched videos of experts more fixedly, memorized them more clearly and distinctly tried to work in the same way shown in the videos as compared with display users.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {13–17},
numpages = {5},
keywords = {projector-camera system, augmented reality},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879477,
author = {Srinivasan, Balaji Vasan and Goyal, Tanya and Syal, Varun and Suman Singh, Shubhankar and Sharma, Vineet},
title = {Environment Specific Content Rendering &amp; Transformation},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879477},
doi = {10.1145/2876456.2879477},
abstract = {The evolution of digital technology has resulted in the consumption of content on a multitude of environments (desktop, mobile, etc). Content now needs to be appropriately delivered to all these environments. This calls for a mechanism to automate the process for rendering the content in its appropriate form on a targeted environment. In this paper, we propose an algorithm that takes the content along with the a set of environment-specific layouts where it has to be rendered to automatically decide the mapping and transformation of the content for the right rendition. Metrics to measure the `goodness' of the resulting rendition is also proposed to choose the right layout for the given content.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {18–22},
numpages = {5},
keywords = {content distribution, content transformation, layout selection},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879478,
author = {Chander, Ajay and Mirzazad Barijough, Sanam},
title = {The Lifeboard: Improving Outcomes via Scarcity Priming},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879478},
doi = {10.1145/2876456.2879478},
abstract = {We introduce the Lifeboard: a dynamic information interface designed to render personal data so as to positively influence wellness outcomes. We report on the results of an experiment that compares the effect of presenting clinically significant data to subjects on their activity levels, with the effect of presenting the same data using the Lifeboard. The statistically significant increase in this wellness outcome in the Lifeboard group vs. the Data-only group suggests that the Lifeboard effectively leverages the scarcity response [4] in the service of improved wellness outcomes. Moreover, the significant week-on-week decrease in this wellness outcome in the Data-only group points to the need for care when exposing clinical data to users.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {23–27},
numpages = {5},
keywords = {wellness, scarcity, mindset, lifeboard, information interface},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879479,
author = {Chen, Jessie Y.C. and Barnes, Michael J. and Selkowitz, Anthony R. and Stowers, Kimberly and Lakhmani, Shan G. and Kasdaglis, Nicholas},
title = {Human-Autonomy Teaming and Agent Transparency},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879479},
doi = {10.1145/2876456.2879479},
abstract = {We developed the user interfaces for two Human-Robot Interaction (HRI) tasking environments: dismounted infantry interacting with a ground robot (Autonomous Squad Member) and human interaction with an intelligent agent to manage a team of heterogeneous robotic vehicles (IMPACT). These user interfaces were developed based on the Situation awareness-based Agent Transparency (SAT) model. User testing showed that as agent transparency increased, so did overall human-agent team performance. Participants were able to calibrate their trust in the agent more appropriately as agent transparency increased.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {28–31},
numpages = {4},
keywords = {human-robot interaction, autonomy, human-agent teaming, situation awareness, agent transparency},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879480,
author = {Pienta, Robert and Xiong, Leilei and Grijalva, Santiago and Chau, Duen Horng (Polo) and Kahng, Minsuk},
title = {STEPS: A Spatio-Temporal Electric Power Systems Visualization},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879480},
doi = {10.1145/2876456.2879480},
abstract = {As the bulk electric grid becomes more complex, power system operators and engineers have more information to process and interpret than ever before. The information overload they experience can be mitigated by effective visualizations that facilitate rapid and intuitive assessment of the system state. With the introduction of non-dispatchable renewable energy, flexible loads, and energy storage, the ability to temporally explore system states becomes critical. This paper introduces STEPS, a new 3D Spatio-temporal Electric Power Systems visualization tool suitable for steady-state operational applications.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {32–35},
numpages = {4},
keywords = {power system visualization, spatio-temporal visualization},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879481,
author = {Yamaya, Akito and Topi\'{c}, Goran and Aizawa, Akiko},
title = {Fixation-to-Word Mapping with Classification of Saccades},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879481},
doi = {10.1145/2876456.2879481},
abstract = {Eye movement is expected to provide important clues for analyzing the human reading process. However, the noisy tracking environment makes it difficult to map the gaze data captured by eye-trackers to the user's intended word. In this paper, we propose an effective approach for accurately mapping a fixation to a word in the text. Our method regards consecutive horizontally progressive fixations as a sequential reading segment. We first classify transitions between segments according to six classes, and then identify the set of segments associated with each line of the document. Our experiments demonstrate that the proposed method achieves 87% mapping accuracy (15% higher than our previous work) with a classification performance of 84%. We also confirmed that manual annotation time can be reduced by using our approach as a reference. We believe that our method provides sufficiently good accuracy to warrant future analysis.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {36–40},
numpages = {5},
keywords = {fixation-to-word mapping, eye-tracking, reading},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879482,
author = {Wan, Bo and Vi, Chi and Subramanian, Sriram and Martinez Plasencia, Diego},
title = {Enhancing Interactivity with Transcranial Direct Current Stimulation},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879482},
doi = {10.1145/2876456.2879482},
abstract = {Transcranial Direct Current Stimulation (tDCS) is a non-invasive type of neural stimulation known for modulation of cortical excitability leading to positive effects on working memory and attention. The availability of low-cost and consumer grade tDCS devices has democratized access to such technology allowing us to explore its applicability to HCI. We review the relevant literature and identify potential avenues for exploration within the context of enhancing interactivity and use of tDCS in the context of HCI.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {41–44},
numpages = {4},
keywords = {neurostimulation, hci, tdcs},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879483,
author = {Chuan, Ching-Hua and Guardino, Caroline Anne},
title = {Designing SmartSignPlay: An Interactive and Intelligent American Sign Language App for Children Who Are Deaf or Hard of Hearing and Their Families},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879483},
doi = {10.1145/2876456.2879483},
abstract = {This paper describes an interactive mobile application that aims to assist children who are deaf or hard of hearing (D/HH) and their families to learn and practice American Sign Language (ASL). Approximately 95% of D/HH children are born to hearing parents. Research indicates that the lack of common communication tools between the parent and child often results in delayed development in the child's language and social skills. Benefiting from the interactive advantages and popularity of touchscreen mobile devices, we created SmartSignPlay, an app to teach D/HH children and their families everyday ASL vocabulary and phrases. Vocabulary is arranged into context-based lessons where the vocabulary is frequently used. After watching the sign demonstrated by an animated avatar, the user performed the sign by drawing the trajectory of the hand movement and selecting the correct handshape. While the app is still under iterative development, preliminary results on the usability are provided.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {45–48},
numpages = {4},
keywords = {american sign language, feedback, context-aware learning, assistive technology, touchscreen},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879484,
author = {Ara\'{u}jo, Rafael D. and Ferreira, Hiran N.M. and Dor\c{c}a, Fabiano A. and Cattelan, Renan G.},
title = {Learning Objects Authoring Supported by Ubiquitous Learning Environments},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879484},
doi = {10.1145/2876456.2879484},
abstract = {Learning objects authoring is still a complex and time-consuming task for instructors, which requires attention to technical and pedagogical aspects. However, one can take advantage of the Ubiquitous Learning Environments characteristics to make it a mild process by means of automatic or semi-automatic processes. In this way, this paper presents an approach for creating learning objects and their metadata in such environments considering collaborative interactions among users. The proposed approach is being integrated to a real multimedia capture system used as a complementary tool in a university.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {49–53},
numpages = {5},
keywords = {learning object metadata, collaborative authoring, learning objects authoring, ubiquitous learning environments},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879485,
author = {Ocegueda-Hern\'{a}ndez, Vladimir and Mendizabal-Ruiz, Gerardo},
title = {Computational Methods for the Natural and Intuitive Visualization of Volumetric Medical Data},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879485},
doi = {10.1145/2876456.2879485},
abstract = {Modern medical image technologies are capable of providing meaningful structural and functional information in the form of volumetric digital data. However current standard systems for the visualization and interaction with such data fail to provide a natural-intuitive way to interact with these data. In this paper, we present our advances towards the development of computational methods for the natural and intuitive visualization of volumetric medical data.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {54–57},
numpages = {4},
keywords = {intuitive, human computer interaction, volume visualizer., natural, medical image},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879486,
author = {Itoh, Masahiko and Yoshinaga, Naoki and Toyoda, Masashi},
title = {Spatio-Temporal Event Visualization from a Geo-Parsed Microblog Stream},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879486},
doi = {10.1145/2876456.2879486},
abstract = {We devised a method of visualizing spatio-temporal events extracted from a geo-parsed microblog stream by using a multi-layered geo-locational word-cloud representation. In our method, real-time geo-parsing geo-locates posts in the stream, in order to recognize words appearing on a user-specified location and time grid as temporal local events. The recognized temporal local events (textit{e.g.}, sports games) are then displayed on a map as multi-layered word-clouds and are then used for finding global events (textit{e.g.}, earthquakes), in order to avoid occlusions among the local and global events. We showed the effectiveness of our method by testing it on real events extracted from our archive of five years worth of Twitter posts.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {58–61},
numpages = {4},
keywords = {social media, spatio-temporal visualization, text analysis},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879487,
author = {Kangasr\"{a}\"{a}si\"{o}, Antti and Chen, Yi and Glowacka, Dorota and Kaski, Samuel},
title = {Dealing with Concept Drift in Exploratory Search: An Interactive Bayesian Approach},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879487},
doi = {10.1145/2876456.2879487},
abstract = {In exploratory search, when the user formulates a query iteratively through relevance feedback, it is likely that the feedback given earlier requires adjustment later on. The main reason for this is that the user learns while searching, which causes changes in the relevance of items and features as estimated by the user -- a phenomenon known as {it concept drift}. It might be helpful for the user to see the recent history of her feedback and get suggestions from the system about the accuracy of that feedback. In this paper we present a timeline interface that visualizes the feedback history, and a Bayesian regression model that can estimate jointly the user's current interests and the accuracy of each user feedback. We demonstrate that the user model can improve retrieval performance over a baseline model that does not estimate accuracy of user feedback. Furthermore, we show that the new interface provides usability improvements, which leads to the users interacting more with it.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {62–66},
numpages = {5},
keywords = {user interfaces, probabilistic user models, interactive user modeling, exploratory search, concept drift},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879488,
author = {Yordanova, Kristina},
title = {From Textual Instructions to Sensor-Based Recognition of User Behaviour},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879488},
doi = {10.1145/2876456.2879488},
abstract = {There are various activity recognition approaches that rely on manual definition of precondition-effect rules to describe user behaviour. These rules are later used to generate computational models of human behaviour that are able to reason about the user behaviour based on sensor observations. One problem with these approaches is that the manual rule definition is time consuming and error prone process. To address this problem, in this paper we outline an approach that extracts the rules from textual instructions. It then learns the optimal model structure based on observations in the form of manually created plans and sensor data. The learned model can then be used to recognise the behaviour of users during their daily activities.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {67–73},
numpages = {7},
keywords = {natural language processing, ontology learning, activity recognition, model learning, user behaviour models},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879489,
author = {Salvado, Luis Miguel and Arsenio, Artur},
title = {Sleeve Sensing Technologies and Haptic Feedback Patterns for Posture Sensing and Correction},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879489},
doi = {10.1145/2876456.2879489},
abstract = {The world population is aging rapidly. There is an increasing need for health assistance personnel, such as nurses and physiotherapeutic experts, in developed countries. On the other hand, there is a need to improve health care assistance to the population, and especially to elderly people. This will mostly benefit specific user groups, such as elderly, patients recovering from physical injury, or athletes. This paper describes a wearable sleeve being developed under the scope of the Augmented Human Assistance (AHA) project for assisting people. It proposes a new architecture for providing haptic feedback through patterns created by multiple actuators. Different sensing technologies are analyzed and discussed.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {74–78},
numpages = {5},
keywords = {augmented health assistance, haptic feedback, wearable computing, posture sensing},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879469,
author = {Gatti, Lorenzo and Ozbal, Gozde and Guerini, Marco and Stock, Oliviero and Strapparava, Carlo},
title = {Heady-Lines: A Creative Generator Of Newspaper Headlines},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879469},
doi = {10.1145/2876456.2879469},
abstract = {In this paper we present Heady-Lines, a creative system that produces news headlines based on well-known expressions. The algorithm is composed of several steps that identify keywords from a news article, select an appropriate well-known expression and modify it to produce a novel one, using state-of-the-art natural language processing and linguistic creativity techniques. The system has a simple web-interface that abstracts the technical details from users and lets them concentrate on the task of producing creative headlines.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {79–83},
numpages = {5},
keywords = {nlp and journalism, computational creativity},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879470,
author = {Garvey, Matthew and Das, Nilaksh and Su, Jiaxing and Natraj, Meghna and Verma, Bhanu},
title = {PASSAGE: A Travel Safety Assistant with Safe Path Recommendations for Pedestrians},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879470},
doi = {10.1145/2876456.2879470},
abstract = {Atlanta has consistently ranked as one of the most dangerous cities in America with over 2.5 million crime events recorded within the past six years. People who commute by walking are highly susceptible to crime here. To address this problem, our group has developed a mobile application, PASSAGE, that integrates Atlanta-based crime data to find "safe paths" between any given start and end locations in Atlanta. It also provides security features in a convenient user interface to further enhance safety while walking.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {84–87},
numpages = {4},
keywords = {bi-objective path optimization, safe paths, safe navigation, pulse algorithm},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879471,
author = {Vogl, Richard and Knees, Peter},
title = {An Intelligent Musical Rhythm Variation Interface},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879471},
doi = {10.1145/2876456.2879471},
abstract = {The drum tracks of electronic dance music are a central and style-defining element. Yet, creating them can be a cumbersome task, mostly due to lack of appropriate tools and input devices. In this work we present an artificial-intelligence-powered software prototype, which supports musicians composing the rhythmic patterns for drum tracks. Starting with a basic pattern (seed pattern), which is provided by the user, a list of variations with varying degree of similarity to the seed pattern is generated. The variations are created using a generative stochastic neural network. The interface visualizes the patterns and provides an intuitive way to browse through them. A user study with ten experts in electronic music production was conducted to evaluate five aspects of the presented prototype. For four of these aspects the feedback was generally positive. Only regarding the use case in live environments some participants showed concerns and requested safety features.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {88–91},
numpages = {4},
keywords = {neural networks, restricted boltzmann machines, generative stochastic models., machine learning, rhythm pattern generation},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879472,
author = {Gandhi, Ankit and Biswas, Arijit and Shrivastava, Kundan and Kumar, Ranjeet and Loomba, Sahil and Deshmukh, Om},
title = {Easy Navigation through Instructional Videos Using Automatically Generated Table of Content},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879472},
doi = {10.1145/2876456.2879472},
abstract = {The amount of instructional videos available online, already in tens of thousands of hours, is growing steadily. A major bottleneck in their wide spread usage is the lack of tools for easy consumption of these videos. In this demonstration, we present MMToC: Multimodal Method for Table of Content, a technique that automatically generates a table of content for a given instructional video and enables text-book-like efficient navigation through the video. MMToC quantifies word saliency for visual words extracted from the slides and spoken words obtained from the lecture transcript. These saliency scores are combined using a dynamic programming based segmentation algorithm to identify likely points in the video where the topic has changed. MMToC is a web-based modular solution that can be used as a stand alone video navigation solution or can be integrated with any e-platform for multimedia content management. MMToC can be seen in action on a sample video at http://104.130.241.45:8080/TopicTransitionV2/index.html.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {92–96},
numpages = {5},
keywords = {user interface for navigation, table of content, efficient indexing, temporal segmentation, instructional videos, non-linear navigation},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879473,
author = {Tanase, Claudiu and Giangreco, Ivan and Rossetto, Luca and Schuldt, Heiko and Seddati, Omar and Dupont, Stephane and Altiok, Ozan Can and Sezgin, Metin},
title = {Semantic Sketch-Based Video Retrieval with Autocompletion},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879473},
doi = {10.1145/2876456.2879473},
abstract = {The IMOTION system is a content-based video search engine that provides fast and intuitive known item search in large video collections. User interaction consists mainly of sketching, which the system recognizes in real-time and makes suggestions based on both visual appearance of the sketch (what does the sketch look like in terms of colors, edge distribution, etc.) and semantic content (what object is the user sketching). The latter is enabled by a predictive sketch-based UI that identifies likely candidates for the sketched object via state-of-the-art sketch recognition techniques and offers on-screen completion suggestions. In this demo, we show how the sketch-based video retrieval of the IMOTION system is used in a collection of roughly 30,000 video shots. The system indexes collection data with over 30 visual features describing color, edge, motion, and semantic information. Resulting feature data is stored in ADAM, an efficient database system optimized for fast retrieval.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {97–101},
numpages = {5},
keywords = {sketch interface, content-based video retrieval},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2879474,
author = {Deloatch, Robert and Gou, Liang and Kau, Chris and Mahmud, Jalal and Zhou, Michelle},
title = {ScopeG: A Mobile Application for Exploration and Comparison of Personality Traits},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2879474},
doi = {10.1145/2876456.2879474},
abstract = {The language people use on social media has been shown to provide insight into their personality characteristics. We developed a mobile system that aids the exploration and comparison of personal personality profiles with those of others. We conducted a user study to evaluate system usability, gauge user interaction of interest, and the system's performance in completing exploration and comparison tasks. Our study shows that the system is easy to use and enables users effectively explore and compare personality profiles, and users were interested in comparing their personality traits with the personality traits of friends, role models, and celebrities.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {102–105},
numpages = {4},
keywords = {personality, mobile system, visualization and comparison},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2876457,
author = {Tsiakas, Konstantinos},
title = {Facilitating Safe Adaptation of Interactive Agents Using Interactive Reinforcement Learning},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2876457},
doi = {10.1145/2876456.2876457},
abstract = {In this paper, we propose a learning framework for the adaptation of an interactive agent to a new user. We focus on applications where safety and personalization are essential, as Rehabilitation Systems and Robot Assisted Therapy. We argue that interactive learning methods can be utilised and combined into the Reinforcement Learning framework, aiming at a safe and tailored interaction.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {106–109},
numpages = {4},
keywords = {interaction management, interactive reinforcement learning, learning agents, adaptation},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2876458,
author = {Zhao, Yuchen},
title = {Usable Privacy in Location-Sharing Services},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2876458},
doi = {10.1145/2876456.2876458},
abstract = {Location-sharing services such as Facebook and Foursquare have become increasingly popular. These services can be helpful for us but can also pose threats to people's privacy. Usability issues in existing location-privacy protection mechanisms are one of the main reasons why people fail to protect their location privacy properly. Most people are not capable and find it cumbersome to configure location-privacy preferences by themselves. My PhD research aims to address these usability issues by using recommenders, understand people's acceptance of, and concerns about, such recommenders, and to alleviate their concerns.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {110–113},
numpages = {4},
keywords = {recommender systems, usable privacy, location-based services},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2876459,
author = {Samsonov, Pavel Andreevich},
title = {Improving Interactions with Spatial Context-Aware Services},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2876459},
doi = {10.1145/2876456.2876459},
abstract = {We have seen a recent rise of context- as well as location-based-mobile services. Finally, those services entering applications and adding features to mobile operating system to make everyday user interactions handier. Nevertheless, those services still have certain limitations, such as lack of certain data types that limit them to exploit their full potentials. My research is situated in the area of human-computer interaction with strong links to the field of intelligent user interfaces and aims to improve interactions with spatial context-aware services by combining methods from computer vision and artificial intelligence.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {114–117},
numpages = {4},
keywords = {context-aware applications, geospatial information, geohci, scenic routes, location-based services, space usage rules},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2876460,
author = {Smith, Sean-Ryan},
title = {Dynamic Online Computerized Neuropsychological Testing System},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2876460},
doi = {10.1145/2876456.2876460},
abstract = {Traditional cognitive testing for detecting cognitive impairment (CI) can be inaccessible, expensive, and time consuming. This dissertation aims to develop an automated online computerized neuropsychological testing system for rapidly tracking an individual's cognitive performance throughout the user's daily or weekly schedule in an unobtrusive way. By utilizing embedded microsensors within tablet devices, the proposed context-aware system will capture ambient and behavioral data pertinent to the real-world contexts and times of testing to compliment psychometric results, by providing insight into the contextual factors relevant to the user's testing efficacy and performance.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {118–121},
numpages = {4},
keywords = {mobile sensors, context-aware, older persons, cognitive test, assistive technology},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2876461,
author = {Hoque, Enamul},
title = {Visual Text Analytics for Online Conversations: Design, Evaluation, and Applications},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2876461},
doi = {10.1145/2876456.2876461},
abstract = {Analyzing and gaining insights from a large amount of textual conversations can be quite challenging for a user, especially when the discussions become very long. During my doctoral research, I have focused on integrating Information Visualization (InfoVis) with Natural Language Processing (NLP) techniques to better support the user's task of exploring and analyzing conversations. For this purpose, I have designed a visual text analytics system that supports the user exploration, starting from a possibly large set of conversations, then narrowing down to a subset of conversations, and eventually drilling-down to a set of comments of one conversation. While so far our approach is evaluated mainly based on lab studies, in my on-going and future work I plan to evaluate our approach via online longitudinal studies.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {122–125},
numpages = {4},
keywords = {interactive topic modeling, computer-mediated communication, text visualization, asynchronous conversation},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2876462,
author = {Rajanna, Vijay Dandur},
title = {Gaze and Foot Input: Toward a Rich and Assistive Interaction Modality},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2876462},
doi = {10.1145/2876456.2876462},
abstract = {Transforming gaze input into a rich and assistive interaction modality is one of the primary interests in eye tracking research. Gaze input in conjunction with traditional solutions to the "Midas Touch" problem, dwell time or a blink, is not matured enough to be widely adopted. In this regard, we present our preliminary work, a framework that achieves precise "point and click" interactions in a desktop environment through combining the gaze and foot interaction modalities. The framework comprises of an eye tracker and a foot-operated quasi-mouse that is wearable. The system evaluation shows that our gaze and foot interaction framework performs as good as a mouse (time and precision) in the majority of tasks. Furthermore, this dissertation work focuses on the goal of realizing gaze-assisted interaction as a primary interaction modality to substitute conventional mouse and keyboard-based interaction methods. In addition, we consider some of the challenges that need to be addressed, and also present the possible solutions toward achieving our goal.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {126–129},
numpages = {4},
keywords = {foot input, gaze and foot interaction, authentication, tabletop interaction, eye tracking},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2876463,
author = {Lioulemes, Alexandros},
title = {Adaptive User and Haptic Interfaces for Smart Assessment and Training},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2876463},
doi = {10.1145/2876456.2876463},
abstract = {My research is focusing on developing smart robotic rehabilitation interfaces that use machine intelligence to adjust the level of difficulty, assess physical and mental obstacles on the part of the user, and provide analysis of the multi-sensing data collected in real time as the user exercises. The main goal of the interfaces is to engage the patient in repetitive exercise sessions and to provide better data visualization to the therapist for the patient's recovery progress. In this doctoral consortium, I will present three prototype user interfaces that can be applied in assistive environments and enhance the productivity and interaction among therapist and patient. The data processing and the decision making algorithms compose the core components of this study.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {130–133},
numpages = {4},
keywords = {user behavior modeling, adaptive serious games, computer vision, physical human-robot interaction, reinforcement learning, haptic interfaces, force/torque control},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2876464,
author = {Gao, Mingkun},
title = {Intelligent Interface for Organizing Online Social Opinions on Reddit},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2876464},
doi = {10.1145/2876456.2876464},
abstract = {Lots of posts containing social opinions are published on Reddit in a messy and staggered format with sub-Reddit labels summarizing their contents only. It's hard for users to have a global insight across different positions and opinions for a specific topic in a short time, especially for a controversial topic. We propose an intelligent mechanism which combines social opinion clustering and information visualization together. First, we cluster the Reddit posts into different categories based on crowd position and opinions, and generate informative clustering labels using the human computation technique. Second, we create an intelligent user interface with Reddit posts category visualization. This would expose categorized posts of different positions and opinions to users, and motivate users to hunt for posts supported by unlike-minded people.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {134–137},
numpages = {4},
keywords = {intelligent user interface, clustering, social opinion, human computation.},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2876465,
author = {Huang, Cheng-Zhi Anna},
title = {ChordRipple: Adaptively Recommending and Propagating Chord Changes for Songwriters},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2876465},
doi = {10.1145/2876456.2876465},
abstract = {Songwriting is the interplay of a composer's creative intent and an idiom's language. This language both facilitates and poses stylistic constraints on a composer's expressivity. Novice composers often find it difficult to go beyond common chord progressions, to find the chords that realize their intentions. To make it easier for composers to experiment with radical chord choices and to prototype "what-if" ideas, we are building a creativity support tool, ChordRipple, which (1) makes chord recommendations that aim to be both diverse and appropriate to the current context, (2) infers a composer's intention to help her more quickly prototype ideas. Composers can use it to help select the next chord, to replace sequences of chords in an internally consist manner, or to edit one part of a sequence and see the whole sequence change in that direction. To make such recommendations, we adapt neural-network models such as Word2Vec to the music domain as Chord2Vec. This model learns chord embeddings from a corpus of chord sequences, placing chords nearby when they are used in similar contexts. The learned embeddings support creative substitutions between chords, and also exhibit topological properties that correspond to musical structure. For example, the major and minor chords are both arranged in the latent space in shapes corresponding to the circle-of-fifths. To support the dynamic nature of the creative process, we propose to infer a composer's intentions for adaptive recommendation. As a composer makes chord changes, she is moving in the embedding space. We can infer a composer's intention from the gradient of her edits' trace and use this gradient to help her fine-tune her current changes or to project the sequence into the future to give recommendations on how the sequence could look like if more edits in that direction were performed.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {138–141},
numpages = {4},
keywords = {neural language models, intelligent user interfaces, recommender systems, creativity support tools, music, harmony, songwriting},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2876466,
author = {Oduola, Cassandra},
title = {Assessing Empathy through Mixed Reality},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2876466},
doi = {10.1145/2876456.2876466},
abstract = {This research seeks to produce a new way of assessing empathy in individuals. The current widely used diagnostic tools are questionnaires. These questionnaires are easy to "pass" if the individual simply lies and chooses the answers that would be most beneficial to them. Furthermore, it is shown, assessing empathy is harder in a clinical setting because it is not the natural world, a person may purposely inhibit their behavior to seem more "normal". Finding methods that would assess affect while interacting with a computer could yield higher accuracy in diagnosis},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {142–145},
numpages = {4},
keywords = {affective computing, mixed reality, virtual reality},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2876467,
author = {Wauck, Helen},
title = {Exploring the Development of Spatial Skills in a Video Game},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2876467},
doi = {10.1145/2876456.2876467},
abstract = {This document gives an overview of my current research project investigating how children develop spatial reasoning skills through video game training. I describe the motivation and goals of the project and the progress made so far.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {146–149},
numpages = {4},
keywords = {spatial reasoning, cognitive science, video games, learning, children, education},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

@inproceedings{10.1145/2876456.2876468,
author = {Tanveer, M. Iftekhar},
title = {Understanding and Intervening Communicational Behavior Using Artificial Intelligence},
year = {2016},
isbn = {9781450341400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876456.2876468},
doi = {10.1145/2876456.2876468},
abstract = {Portable and inexpensive technologies have the potential to capture a huge variety of signals about human being. Systematic analysis of these signals can provide deep understanding on the basic nature of interpersonal communication. I am interested in taking a machine learning approach for analyzing human behaviors---atleast in a formal, well-established setting (e.g. in public speaking, job interview etc.). Understanding human behavior will enable us to design systems capable to make people self-aware. In many cases they might be useful for behavior modification as well.},
booktitle = {Companion Publication of the 21st International Conference on Intelligent User Interfaces},
pages = {150–153},
numpages = {4},
keywords = {affective computing, user interface design, machine learning, behavior analysis},
location = {Sonoma, California, USA},
series = {IUI '16 Companion}
}

