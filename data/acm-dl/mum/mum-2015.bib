@inproceedings{10.1145/2836041.2836042,
author = {McMillan, Donald and Brown, Barry and Sellen, Abigail and Lindley, Si\^{a}n and Martens, Roy},
title = {Pick up and Play: Understanding Tangibility for Cloud Media},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836042},
doi = {10.1145/2836041.2836042},
abstract = {The transition from local and personally owned file-based media management to cloud-based streaming services such as Spotify and Netflix brings new opportunities for users, but also leaves gaps in their understanding and practice. In this paper we present findings from an interview study that explored early adopters' complex relationships with their collections which spanned physical, digital and cloud media. From this we entered a design process focussing on new material forms for cloud based media. Based on this we discuss our design and point to areas where, tangible or not, affordances from physical and digital media are available to be explored in the cloud. Looking in particular at the concepts of scarcity, gifting, and identity we outline possible reasons why, and why not, they could be incorporated into cloud media services.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {1–13},
numpages = {13},
keywords = {media, cloud, music, tangible},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836043,
author = {Opitz, Bernd and Sztyler, Timo and Jess, Michael and Knip, Florian and Bikar, Christian and Pfister, Bernd and Scherp, Ansgar},
title = {On-the-Fly Entity Resolution from Distributed Social Media Sources for Mobile Search and Exploration},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836043},
doi = {10.1145/2836041.2836043},
abstract = {We present an approach and mobile application for the interactive exploration and search of geo-located social media entities from different, distributed data providers on the web. When querying the providers, the returned results typically have some overlap. In addition, one has no guarantee that the providers reply within a given time interval. Thus, in order to provide users with geo-located entities in their vicinity in a timely manner, we need to take the asynchronous nature of the data providers' replies into account. Our novel on-the-fly entity resolution engine starts the entity resolution once it retrieves the first responses. It incrementally extends the entity resolution model when more responses arrive. Entities are propagated to the client once the resolution engine has processed them for the first time. Resolution results produced at a later point in time are sent as updates to the client and improve earlier, incomplete results. Our experiments show a matching precision of 95% and scalability of the on-the-fly entity resolution w.r.t. the number of resources being simultaneously processed.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {14–24},
numpages = {11},
keywords = {entity resolution, mobile exploration, social media},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836044,
author = {Assal, Hala and Hurtado, Stephanie and Imran, Ahsan and Chiasson, Sonia},
title = {What's the Deal with Privacy Apps? A Comprehensive Exploration of User Perception and Usability},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836044},
doi = {10.1145/2836041.2836044},
abstract = {We explore mobile privacy through a survey and through usability evaluation of three privacy-preserving mobile applications. Our survey explores users' knowledge of privacy risks, as well as their attitudes and motivations to protect their privacy on mobile devices. We found that users have incomplete mental models of privacy risks associated with such devices. And, although participants believe they are primarily responsible for protecting their own privacy, there is a clear gap between their perceived privacy risks and the defenses they employ. For example, only 6% of participants use privacy-preserving applications on their mobile devices, but 83% are concerned about privacy. Our usability studies show that mobile privacy-preserving tools fail to fulfill fundamental usability goals such as learnability and intuitiveness---potential reasons for their low adoption rates. Through a better understanding of users' perception and attitude towards privacy risks, we aim to inform the design of privacy-preserving mobile applications. We look at these tools through users' eyes, and provide recommendations to improve their usability and increase user-acceptance.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {25–36},
numpages = {12},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836045,
author = {Wen, James and \"{U}nl\"{u}er, Ay\c{c}a},
title = {Redefining the Fundamentals of Photography with Cooperative Photography},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836045},
doi = {10.1145/2836041.2836045},
abstract = {Cooperative photography redefines some fundamental notions of the photographic process by using mobile devices to synchronously coordinate multiple parties in a collaborative image capturing task. Using wireless communication and geo-location technologies, the photographer takes on a support role as content enabler rather than as the content creator, opening up creative possibilities. However, unreliable wireless networks and misunderstandings between users can make the coordination challenging and lead to a premature termination of the task. We present the evolution of the iterative design process we used to find a suitable set of interactions to encourage users to complete cooperative photography tasks. We report on our design motivations and their associated user studies, which suggest that user experiences dealing with higher level user behavior rather than lower level system awareness may be central in facilitating the successful completion of cooperative photography tasks.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {37–47},
numpages = {11},
keywords = {photography, collocation, mobile devices, user interaction, collaboration, selfie, cooperative work},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836046,
author = {Pesonen, Emilia and Jumisko-Pyykk\"{o}, Satu and V\"{a}\"{a}t\"{a}j\"{a}, Heli},
title = {User Experience of Digital News: Two Semi-Long Term Field Studies},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836046},
doi = {10.1145/2836041.2836046},
abstract = {Reading of digital news on personal devices has dramatically increased. Parallel to new devices, novel services and content types are created forming new habits and experiences for readers. Previous research is limited in understanding temporal aspects of such users' experiences (UX). This study aims to understand UX of mobile news reading in a real context of use over one week in two different case studies. UX of digital replicas, browser optimized versions of digital news, and a novel media authentication method for news reading and ordering were explored with actual news readers (N=36) in field using their own tablets and personal computers. Data-collection included daily diaries with UX questionnaires, and retrospective interviews. The results showed that the studied forms of digital news and authentication method provided positive UX, and they were appealing for future digital news. UX also showed a tendency of improving over a time. The user's habits of reading digital news reflected the conventions of reading a print newspaper.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {51–63},
numpages = {13},
keywords = {digital replica, reading, online, authentication, digital news, user experience, computer, tablet, browser},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836047,
author = {Alavesa, Paula and Ojala, Timo},
title = {Street Art Gangs: Location Based Hybrid Reality Game},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836047},
doi = {10.1145/2836041.2836047},
abstract = {We present a location based mixed reality game called Street Art Gangs that we have developed to explore the playful appropriation of the hybrid reality comprising of a city center and its detailed virtual replica represented as a 3D virtual model. In the real streets SAG is played with a mobile phone app that allows tagging predefined locations around the city to claim their ownership and busting nearby players of competing gangs. The virtual game world is viewed with a PC app that allows observing the current owners of taggable locations, the locations of other players, and the locations of patrolling virtual policemen busting players. We have developed two incremental versions of SAG that have been evaluated with tournaments in the wild. We conceptualize the findings of the tournaments with de Souza e Silva's theoretical framework for hybrid reality games. Our findings suggest that players preferred to play the game on real city streets while the added value of the virtual game world remained marginal. The size of the game both in terms of area and the number of taggable locations turned out to have a major impact on gameplay.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {64–74},
numpages = {11},
keywords = {pervasive gaming, hybrid space, virtual worlds, hybrid reality games, location based games},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836048,
author = {Reinhardt, Delphine and Engelmann, Franziska and Moerov, Andrey and Hollick, Matthias},
title = {Show Me Your Phone, I Will Tell You Who Your Friends Are: Analyzing Smartphone Data to Identify Social Relationships},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836048},
doi = {10.1145/2836041.2836048},
abstract = {Access control is a key principle to protect user privacy online. The combination of both the wealth of user-generated data in online social networks and overly complex user interfaces lead to a high user burden for privacy control, hence making the observance of the above principles difficult. We investigate how communication metadata on smartphones can facilitate providing tailored suggestions for restricted audience groups, thus limiting the sharing of data to the intended users only. To this end, we have performed a user study collecting a dataset including contact names, calls, SMS, MMS, and e-mail on personal smartphones in everyday use. In this paper, we examine which are the key features determining the social relationship category of a contact using machine learning. We obtain promising results for an automated classification of contacts into work-related, family-related and other social-interaction-related, thus enabling the possibility of user assistance for privacy control. Obtaining a more fine-grained categorization of the latter category into acquaintances, friends, and university-mates is shown to be difficult, since these categories blur in our study group.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {75–83},
numpages = {9},
keywords = {interpersonal relationships mining},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836049,
author = {Korinke, Christoph and Worzyk, Nils Steffen and Boll, Susanne},
title = {Exploring Touch Interaction Methods for Image Segmentation on Mobile Devices},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836049},
doi = {10.1145/2836041.2836049},
abstract = {Photo taking is more and more moving to mobile devices. With this transition, we observe that consumers are also directly editing and sharing photos. Editing of the whole image became very popular, for example with the filters Instagram offers. Some effects, however, require to edit a specific object of an image and for this an interactive segmentation is needed. This process is still tedious, as existing segmentation approaches focus on the algorithms rather than on the input methods. This paper explores different interaction techniques for image segmentation on mobile devices. We conducted two studies. The first to identify intuitive input methods and the second to evaluate the findings of the first study. As a result, we identified preferred gestures for the initial step to select a foreground object in an interactive image segmentation process. The results give first insights which input methods users intuitively use, how the users behave and what are the problems of the methods. This insights should be taken in consideration by the development of new interactive segmentation algorithms for mobile touch-based devices.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {84–93},
numpages = {10},
keywords = {bounding box, interactive image segmentation, outline, input methods evaluation, colouring, mobile touch-based devices},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836050,
author = {Vanderhulst, Geert and Mashhadi, Afra and Dashti, Marzieh and Kawsar, Fahim},
title = {Detecting Human Encounters from WiFi Radio Signals},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836050},
doi = {10.1145/2836041.2836050},
abstract = {We present the design, implementation and evaluation of a novel human encounter detection framework for measuring and analysing human behaviour in social settings. We propose the use of WiFi probes, management frames of WiFi, that periodically radiate from mobile devices (as proxies for humans), and existing WiFi access points to automatically capture radio signals and detect human copresence. Based on the spatio-temporal properties of this copresence and their interplay we defined a model, borrowing theories from sociology, to detect human encounters -- short-lived, spontaneous human interactions. We evaluated our framework using controlled and in-the-wild experiments yielding a detection performance of 96% and 86% respectively. As such, our framework opens up interesting opportunities for designing proxemic and group applications, as well as conducting large-scale studies in the areas of computational social sciences.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {97–108},
numpages = {12},
keywords = {spontaneous encounters, hybrid sensing, WiFi signals},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836051,
author = {Ch\'{a}vez-Mart\'{\i}nez, Gilberto and Ruiz-Correa, Salvador and Gatica-Perez, Daniel},
title = {Happy and Agreeable? Multi-Label Classification of Impressions in Social Video},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836051},
doi = {10.1145/2836041.2836051},
abstract = {The mobile and ubiquitous nature of conversational social video has placed video blogs among the most popular forms of online video. For this reason, there has been an increasing interest in conducting studies of human behavior from video blogs in affective and social computing. In this context, we consider the problem of mood and personality trait impression inference using verbal and nonverbal audio-visual features. Under a multi-label classification framework, we show that for both mood and personality trait binary label sets, not only the simultaneous inference of multiple labels is feasible, but also that classification accuracy increases moderately for several labels, compared to a single-label approach. The multi-label method we consider naturally exploits label correlations, which motivate our approach, and our results are consistent with models proposed in psychology to define human emotional states and personality. Our approach points to the automatic specification of co-occurring emotional states and personality, by inferring several labels at once, compared to single-label approaches. We also propose a new set of facial features, based on emotion valence from facial expressions, and analyze their suitability in the multi-label framework.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {109–120},
numpages = {12},
keywords = {social video, mood, personality, classification, YouTube},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836052,
author = {Dietrich, Manuel and Berlin, Eugen and van Laerhoven, Kristof},
title = {Assessing Activity Recognition Feedback in Long-Term Psychology Trials},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836052},
doi = {10.1145/2836041.2836052},
abstract = {The physical activities we perform throughout our daily lives tell a great deal about our goals, routines, and behavior, and as such, have been known for a while to be a key indicator for psychiatric disorders. This paper focuses on the use of a wrist-watch with integrated inertial sensors. The algorithms that deal with the data from these sensors can automatically detect the activities that the patient performed from characteristic motion patterns. Such a system can be deployed for several weeks continuously and can thus provide the consulting psychiatrist an insight in their patient's behavior and changes thereof. Since these algorithms will never be flawless, however, a remaining question is how we can support the psychiatrist in assigning confidence to these automatic detections. To this end, we present a study where visualizations at three levels from a detection algorithm are used as feedback, and examine which of these are the most helpful in conveying what activities the patient has performed. Results show that just visualizing the classifier's output performs the best, but that user's confidence in these automated predictions can be boosted significantly by visualizing earlier pre-processing steps.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {121–130},
numpages = {10},
keywords = {context-aware services, interaction design, activity recognition, visualization methods},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836053,
author = {Findling, Rainhard Dieter and Mayrhofer, Ren\'{e}},
title = {Towards Device-to-User Authentication: Protecting against Phishing Hardware by Ensuring Mobile Device Authenticity Using Vibration Patterns},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836053},
doi = {10.1145/2836041.2836053},
abstract = {Users usually authenticate to mobile devices before using them (e.g. PIN, password), but devices do not do the same to users. Revealing the authentication secret to a non-authenticated device potentially enables attackers to obtain the secret, by replacing the device with an identical-looking malicious device. The revealed authentication secret could be transmitted to the attackers immediately, who then conveniently authenticate to the real device. Addressing this attack scenario, we analyze different approaches towards mobile device-to-user (D2U) authentication, for which we provide an overview of advantages/drawbacks, potential risks and device authentication data bandwidth estimations. We further analyze vibration as one D2U feedback channel that is unobtrusive and hard to eavesdrop, including a user study to estimate vibration pattern recognition using a setup of ~7 bits per second (b/s). Study findings indicate that users are able to distinguish vibration patterns with median correctness of 97.5% (without taking training effects into account) -- which indicates that vibration could act as authentication feedback channel and should be investigated further in future research.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {131–135},
numpages = {5},
keywords = {phishing hardware, feedback, vibration, mobile authentication},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836054,
author = {Tayama, Yuki and Kato, Ryuga and Okada, Ken-ichi},
title = {Triage Training System: Adjusting the Difficulty Level According to User Proficiency},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836054},
doi = {10.1145/2836041.2836054},
abstract = {At times of mass casualty incidents, medical resources such as personnel and equipment are limited. By performing triage, the act of deciding the priority of treating patients by severity of their condition, efficient use of the resources can be achieved. In Japan, triage became widely known after the 2011 Japan Earthquake, and the necessity of triage training has been acknowledged. Frequent training helps in acquiring skills needed to act in real situations, but current training does not consider the skill level of each trainee, since several people train simultaneously. This leads to the decline in motivation and inefficient training sessions. Moreover, creating a customized scenario for each trainee is difficult and costly. To address this issue, we designed a training system which adjusts the level of difficulty according to user proficiency. The system automatically generates the scenario for each user, and provides visual feedback after training. We conducted an evaluation experiment to measure whether the trainee can acquire the necessary knowledge by using our system. The results showed that training with our system helps in learning the skills quickly and accurately.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {139–147},
numpages = {9},
keywords = {user proficiency, triage, training},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836055,
author = {L\"{o}chtefeld, Markus and J\"{a}ckel, Christian and Kr\"{u}ger, Antonio},
title = {TwitSoccer: Knowledge-Based Crowd-Sourcing of Live Soccer Events},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836055},
doi = {10.1145/2836041.2836055},
abstract = {Nowadays, fans post status updates on social media that describe and express their opinion about live sports events. By doing this, they not only comment on the game but also provide information about the current state of play. In this paper we present TwitSoccer, a real-time live soccer score ticker crowd-sourced from tweets. TwitSoccer combines a pattern-based approach, to process the tweets, with a soccer knowledge-database. This allows to detect reoccurring events, such as goals or red cards, more precisely and ensures that these events are assigned to the right teams. The results of our evaluation for 42 soccer matches of the German Bundesliga, show that TwitSoccer automatically detects soccer events reliably with a recall value of 94% for goals and 71% for red cards which resembles a major enhancement over existing work.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {148–151},
numpages = {4},
keywords = {crowd-sourcing, sports, Twitter, live scores, microblogging},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836056,
author = {Niforatos, Evangelos and Vourvopoulos, Athanasios and Langheinrich, Marc},
title = {Weather with You: Evaluating Report Reliability in Weather Crowdsourcing},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836056},
doi = {10.1145/2836041.2836056},
abstract = {Several mobile and social media weather apps support the incorporation of human input in increasing their coverage and accuracy of current weather conditions. This practice is also known as participatory sensing: the act of using sensors (i.e. smartphones) carried by volunteers to acquire highly localized measurements of physical phenomena. In order to assess the accuracy of such user contributed weather reports, we created an android app called Atmos that allows for the in situ collection of weather data in the form of descriptive manual input. Based on a yearlong study with Atmos deployed on the Google Play store, we investigate the ability of mobile users to both report current weather conditions accurately, and to predict future weather developments. We found that mobile users can be sufficiently accurate when reporting current conditions, though report accuracy was affected by hour of day. Users were also able to provide accurate short-term predictions, particularly for temperature and wind intensity. We also present results from an online survey that gathered data from 12 countries in order to understand the role weather plays in users' daily life, which helped us design Atmos.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {152–162},
numpages = {11},
keywords = {participatory sensing, crowdsourcing, mobile sensing, experience sampling (ESM)},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836057,
author = {Tobita, Hiroaki},
title = {Comic-Crowd: Interactive Comic Creation That Supports Multiple Storylines, Visualizations, and Platforms},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836057},
doi = {10.1145/2836041.2836057},
abstract = {Comic-Crowd is an interactive digital comic system for developing and presenting multiple comic-book storylines and multiple visualizations. To date, digital comic books have had the same format and level of interaction since they first appeared. Thus, such comics have one storyline, and the interaction is akin to turning pages. In contrast, our approach supports multiple storylines, so that readers can select a storyline, with say, a happy or sad ending, depending on their preferences. Thus, our format and level of interaction are more dynamic than those of conventional digital comics. Moreover, since our format can provide multiple visualizations, it would be useful for a wide variety of applications. In this paper, we describe our Comic-Crowd concept and an implementation that expresses its unique interaction features and format.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {163–172},
numpages = {10},
keywords = {branching narratives, information visualization, multi-hop derivation, contents creation, interactive system, multiple storylines, comic and manga},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836058,
author = {Cruz, Luis and Rubin, Jonathan and Abreu, Rui and Ahern, Shane and Eldardiry, Hoda and Bobrow, Daniel G.},
title = {A Wearable and Mobile Intervention Delivery System for Individuals with Panic Disorder},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836058},
doi = {10.1145/2836041.2836058},
abstract = {Panic disorder is a serious condition that affects approximately six million adults in the United States per year. Reducing the severity of panic attack symptoms would allow a better quality of life for panic attack sufferers. This paper presents steps towards a mobile and wearable system that aims to help reduce the severity of symptoms experienced by individuals with this condition. The system provides a way to continuously monitor the physiological data of an individual via a wearable device. Users are able to report when panic attacks take place, along with a rating of the severity of symptoms experienced. Reported episodes provide ground truth data to build panic prediction models. The eventual goal of the system is to make predictions about approaching panic attacks and to deliver interventions that help the individual to cope with the approaching episode. We describe a mobile-based intervention that has been developed, which instructs the individual to perform breathing and relaxation exercises. Presently, the system has been utilized in a small pilot study where 10 individuals who suffer from panic disorder reported 29 panic attacks while collecting physiological data, along with the severity of symptoms. We found that out of 15 symptoms the ones with high severity reported were anxiety, worry and shortness of breath. Furthermore, physiological differences were observed between panic and non-panic intervals.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {175–182},
numpages = {8},
keywords = {mobile applications, physiological monitoring, ubiquitous computing, wearables, mHealth},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836059,
author = {Schwarz, Stephanie and Salazar, Estefan\'{\i}a Palacio and Bobeth, Jan and Bersia, Nicoletta and Tscheligi, Manfred},
title = {Help Radar: Ubiquitous Assistance for Newly Arrived Immigrants},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836059},
doi = {10.1145/2836041.2836059},
abstract = {Help Radar consists of a ubiquitous assistance service which enables immigrants to connect with volunteers, who in turn provide information on a variety of topics, guidance and practical assistance. The system was developed to foster the early integration of immigrants in the host country. In this paper, we present the methodological approach and results obtained from the field evaluation of Help Radar which involved Turkish immigrants in Graz, Austria, as well as immigrants from Latin-American and Arabic speaking countries in London, United Kingdom. During and after the field phase, quantitative and qualitative data were gathered to determine the usage activity, satisfaction, privacy concerns and trust towards the system. Based on results differences in usage among the involved user groups, types of assistance received and potentials of Help Radar toward immigrants' integration are discussed.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {183–194},
numpages = {12},
keywords = {community building, privacy, ubiquitous assistance, field study, immigrants, empowerment, volunteers},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836060,
author = {Himmelsbach, Julia and Garschall, Markus and Egger, Sebastian and Steffek, Susanne and Tscheligi, Manfred},
title = {Enabling Accessibility through Multimodality? Interaction Modality Choices of Older Adults},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836060},
doi = {10.1145/2836041.2836060},
abstract = {In this work, we identify influencing factors on modality choices of older adults. In detail, we investigated when and why older adults prefer speech over touch interaction and vice versa when interacting with a mobile multimodal health and wellbeing service. We conducted a study with 19 older adults using a mobile application with a duration of three to six weeks. Due to this long duration of the study we were able to gain highly external valid insights as our results are based on real world experiences. We identify additional influencing factors within the areas of user characteristics, contextual factors and perceived system characteristics. We outline the impact of the factors and highlight the importance of several of these factors to enable accessible user interfaces. Our results provide first steps towards a more holistic model of modality choices taking into account interdependencies of different factors.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {195–199},
numpages = {5},
keywords = {older adults, interaction modalities, multimodal interaction},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836061,
author = {Walsh, Tanja and Petrie, Helen and Zhang, Anqi},
title = {Localization of Storyboards for Cross-Cultural User Studies},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836061},
doi = {10.1145/2836041.2836061},
abstract = {Storyboards are useful for presenting ideas visually to users helping them understand possible uses of technology allowing them to identify with use situations, especially when no prototypes are available to demonstrate. Storyboards are good for cross-cultural user studies, because they reduce the amount of text users with different native languages have to read. Storyboards are easy to implement in online surveys, which are convenient in gathering data from geographically dispersed groups of users. However, creating localized storyboards requires considering a number of culture related factors. Little research exists in Human-Computer Interaction about how to create localized storyboards for online UX surveys although the need for gathering global user feedback of technology products and services noticeable. We used two focus groups with Chinese participants to inform the design of localization of storyboards for an online survey. Results showed that localization was successful and some design implications were found of localizing storyboards.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {200–209},
numpages = {10},
keywords = {online surveys, storyboards, localization, cross-cultural user studies},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836062,
author = {Kritzler, Mareike and B\"{a}ckman, Martin and Tenf\"{a}lt, Anders and Michahelles, Florian},
title = {Wearable Technology as a Solution for Workplace Safety},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836062},
doi = {10.1145/2836041.2836062},
abstract = {Occupational accidents cause physical harm to impacted employees and financial harm to employers. The most effective protection against occupational accidents is the proper usage of personal protective equipment (PPE). A huge variety of wearable sensors is available for different purposes, they have already proven their potential to support personal fitness and health. This paper describes the idea and implementation of a safety system for PPE based on such wearable sensors and wireless technology. The goal of the system is to ensure that the right PPE required for a specific task is worn. Furthermore, conclusions based on stakeholder interviews are presented. These include the feasibility of the suggested implementation on a smartwatch as well as a confirmation from domain experts that wearable sensing can improve workplace safety.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {213–217},
numpages = {5},
keywords = {wearable devices, occupational safety and health, location-aware, personal protective equipment},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836063,
author = {Kerber, Frederic and Schardt, Philipp and L\"{o}chtefeld, Markus},
title = {WristRotate: A Personalized Motion Gesture Delimiter for Wrist-Worn Devices},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836063},
doi = {10.1145/2836041.2836063},
abstract = {In this paper, we present WristRotate, a personalized motion gesture delimiter that enables separation of non-relevant motion from gesture input. In an extensive data collection, we acquired 435.1 hours of smartwatch acceleration data during everyday usage. We implemented a gesture recognition system based on Dynamic Time Warping to partition a stream of accelerometer readings to identify possible gestures and to classify them accordingly. Through our analysis, we were able to identify a gesture that is (1) uncommon in daily life; (2) quick and easy to execute and (3) easily and reliably detectable. The gesture is executed by simply rotating the lower arm and wrist outwards and back inwards (twice).},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {218–222},
numpages = {5},
keywords = {same-side interaction, gestural interaction, smartwatch},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836064,
author = {H\"{a}kkil\"{a}, Jonna and Vahabpour, Farnaz and Colley, Ashley and V\"{a}yrynen, Jani and Koskela, Timo},
title = {Design Probes Study on User Perceptions of a Smart Glasses Concept},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836064},
doi = {10.1145/2836041.2836064},
abstract = {Until today, mobile computing has been very much confined to conventional computing form factors, i.e. laptops, tablets and smartphones, which have achieved de facto design standards in outlook and shape. However, wearable devices are emerging, and especially glasses are an appealing form factor for future devices. Currently, although companies such as Google have productized a solution, little user research and design exploration has been published on either the user preferences or the technology. We set ourselves to explore the design directions for smart glasses with user research grounded use cases and design alternatives. We describe our user research utilizing a smart glasses design probe in an experience sampling method study (n=12), and present a focus group based study (n=14) providing results on perceptions on alternative industrial designs for smart glasses.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {223–233},
numpages = {11},
keywords = {experience sampling method, user experience, technology acceptance, user studies, wearable computing},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836065,
author = {Wolf, Katrin and Abdelrahman, Yomna and Schmid, David and Dingler, Tilman and Schmidt, Albrecht},
title = {Effects of Camera Position and Media Type on Lifelogging Images},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836065},
doi = {10.1145/2836041.2836065},
abstract = {With an increasing number of new camera devices entering the market, lifelogging has turned into a viable everyday practice. The promise of comprehensively capturing our life's happenings has caused adoption rates to grow, but approaches to do so greatly differ. In this paper we evaluate existing visual lifelogging capture approaches through a user study with two main capture dimensions: (1) comparing the body position where a lifelogging camera is worn: head versus chest (2) comparing the media captures: video versus stills. We equipped 30 participants with cameras on their heads and chests. That data was evaluated by subjective user ratings as well as by objective image processing analysis. Our findings indicate that (1) chest-worn devices are more stable and contain less motion blur through which feature detection by image processing algorithms works better than from head-worn cameras; 2) head-worn video cameras, however, seem to be the better choice for lifelogging as they capture more important autobiographical cues than chest-worn devices, e.g., faces that have been shown to be most relevant for recall.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {234–244},
numpages = {11},
keywords = {wearable camera, lifelogging, ego-centric camera},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836066,
author = {L\"{o}chtefeld, Markus and Schardt, Phillip and Kr\"{u}ger, Antonio and Boring, Sebastian},
title = {Detecting Users Handedness for Ergonomic Adaptation of Mobile User Interfaces},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836066},
doi = {10.1145/2836041.2836066},
abstract = {Often, we operate mobile devices using only one hand. The hand thereby serves two purposes: holding the device and operating the touch screen with the thumb. The current trend of increasing screen sizes however, makes it close to impossible to reach all parts of the screen (especially the top area) for users with average hand sizes. One solution is to offer adaptive user interfaces for such one-handed interactions. These modes have to be triggered manually and thus induce a critical overhead. They are further designed to bring all content closer, regardless of whether the phone is operated with the left or right hand. In this paper, we present an algorithm that allows determining the users' interacting hand from their unlocking behavior. Our algorithm correctly distinguishes one- and two-handed usage as well as left- and right handed unlocking in 98.51% of all cases. This is achieved through a k-nearest neighbor comparison of the internal sensor readings of the smartphone during the unlocking process.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {245–249},
numpages = {5},
keywords = {sensor fusion, ergonomics, adaptive interfaces, handedness, unlocking},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836067,
author = {Funk, Markus and Kosch, Thomas and Greenwald, Scott W. and Schmidt, Albrecht},
title = {A Benchmark for Interactive Augmented Reality Instructions for Assembly Tasks},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836067},
doi = {10.1145/2836041.2836067},
abstract = {With the opportunity to customize ordered products, assembly tasks are becoming more and more complex. To meet these increased demands, a variety of interactive instruction systems have been introduced. Although these systems may have a big impact on overall efficiency and cost of the manufacturing process, it has been difficult to optimize them in a scientific way. The challenge is to introduce performance metrics that apply across different tasks and find a uniform experiment design. In this paper, we address this challenge by proposing a standardized experiment design for evaluating interactive instructions and making them comparable with each other. Further, we introduce a General Assembly Task Model, which differentiates between task-dependent and task-independent measures. Through a user study with 12 participants, we evaluate the experiment design and the proposed task model using an abstract pick-and-place task and an artificial industrial task. Finally, we provide paper-based instructions for the proposed task as a baseline for evaluating Augmented Reality instructions.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {253–257},
numpages = {5},
keywords = {benchmark, experiment design, remote collaboration, evaluation, instruction giving, augmented reality},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836068,
author = {Henze, Niels and Olsson, Thomas and Schneegass, Stefan and Shirazi, Alireza Sahami and V\"{a}\"{a}n\"{a}nen-Vainio-Mattila, Kaisa},
title = {Augmenting Food with Information},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836068},
doi = {10.1145/2836041.2836068},
abstract = {Eating is not only one of the most fundamental human needs but also among the most regular activities. Acquiring food, preparing meals, and socializing around food are deeply rooted in all human cultures. In this paper we show how food can not only serve to satisfy hunger but also become a new display technology. Through food augmentation, a dinner could communicate its ingredients, convey messages, or provide instructions such as the recipe of a meal. We show how to augment a large range of food with laser. We conducted a series of focus groups to gather people's first impressions and derive a broad range of meaningful augmentation scenarios. We discuss the perceived benefits, opportunities, and concerns. Additionally, we evaluated a number of scenarios through an online survey. The most readily accepted augmentation scenarios include adding practical information, increasing awareness about the food, and augmenting food items with a natural skin.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {258–266},
numpages = {9},
keywords = {information, augmentation, laser cutter, cooking, eating, food},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836069,
author = {Matviienko, Andrii and Rauschenberger, Maria and Cobus, Vanessa and Timmermann, Janko and M\"{u}ller, Heiko and Fortmann, Jutta and L\"{o}cken, Andreas and Trappe, Christoph and Heuten, Wilko and Boll, Susanne},
title = {Deriving Design Guidelines for Ambient Light Systems},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836069},
doi = {10.1145/2836041.2836069},
abstract = {Recent interest in the development of ambient light systems has initialized a new research area, where the number of ambient light systems is expected to increase in the next years. To support the development of future ambient light systems, we need clear, explicit, and structured design guidelines. In this paper we present an evaluation of light patterns in a controlled laboratory study with two complementary parts. In the first part, our aim was to reveal and analyze light patterns that encode different types of everyday information. In the second part, we verified the results from the first part by asking another group of participants about their understanding of information encoded with light. Together, our results allowed us to establish light patterns and guidelines for building new ambient light systems and applications in the future.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {267–277},
numpages = {11},
keywords = {information encoding, ambient light systems, light evaluation, peripheral display, design guidelines},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836070,
author = {Li, Ming and Arning, Katrin and Vervier, Luisa and Ziefle, Martina and Kobbelt, Leif},
title = {Influence of Temporal Delay and Display Update Rate in an Augmented Reality Application Scenario},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836070},
doi = {10.1145/2836041.2836070},
abstract = {In mobile augmented reality (AR) applications, highly complex computing tasks such as position tracking and 3D rendering compete for limited processing resources. This leads to unavoidable system latency in the form of temporal delay and reduced display update rates. In this paper we present a user study on the influence of these system parameters in an AR point'n'click scenario. Our experiment was conducted in a lab environment to collect quantitative data (user performance as well as user perceived ease of use). We can show that temporal delay and update rate both affect user performance and experience but that users are much more sensitive to longer temporal delay than to lower update rates. Moreover, we found that the effects of temporal delay and update rate are not independent as with longer temporal delay, changing update rates tend to have less impact on the ease of use. Furthermore, in some cases user performance can actually increase when reducing the update rate in order to make it compatible to the latency. Our findings indicate that in the development of mobile AR applications, more emphasis should be put on delay reduction than on update rate improvement and that increasing the update rate does not necessarily improve user performance and experience if the temporal delay is significantly higher than the update interval.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {278–286},
numpages = {9},
keywords = {user study, ease of use, latency, mobile augmented reality, display update rate, perception tolerance, temporal delay, point'n'click},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836071,
author = {Santos, Rossana and Correia, Nuno},
title = {Building Interactive Experiences Block by Block},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836071},
doi = {10.1145/2836041.2836071},
abstract = {This paper proposes a system, created with the goal of simplifying the design and development of interactive experiences to help explore a space. It has four components: XploreDescription, XploreBuilder, urSpace and an infrastructure.The XploreDescription, is a language grammar to describe interactive experiences with interactions based on gestures and user's actions. The XploreBuilder is a visual tool used to design interactive experiences by assembling building blocks. urSpace creates interactive experiences and is built on top of an infrastructure that uses existing video camera and wireless networks to provide means to support interactive experiences using mobile devices. Description files of the space and interactive experience generated by XploreBuilder are parsed by urSpace, that creates the interactive experience application. This application can be immediately accessed using the user's mobile device.Interactive experiences based on interaction with the space like mobile augmented reality games can be created quickly and with little or no experience of programming.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {289–301},
numpages = {13},
keywords = {domain specific languages, gaming, augmented reality, space exploration, game development tools, mobile applications},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836072,
author = {G\"{u}ldenpfennig, Florian and Fitzpatrick, Geraldine},
title = {De+re: A Design Concept for Provoking Meaningful Interactive Experiences},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836072},
doi = {10.1145/2836041.2836072},
abstract = {While much effort in interactive system design is centered on the recognition and representation of precise contextual information, where ambiguity or missing information is considered disruptive, other work has sought to play with notions of contextual ambiguity as deliberate design strategies. In this paper, we build on the latter idea by intentionally removing contextual cues to explore the concept of de- and re-contextualization (de+re) for creating thought-provoking experiences through 'contextual puzzles'. In particular, we introduce the interactive installation Hearsay, which presents readers' comments (i.e., texts) on online news stories (i.e., context) as de-contextualized items of information to be interpreted by the user. Our study suggests that this activity of re-contextualization or "putting the puzzle back into place" captured the participants' attention and engaged them in deep considerations of the presented information. Drawing on these findings and on further examples from the literature, we propose the principal of de+re as a Strong Concept in interaction design.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {302–312},
numpages = {11},
keywords = {critical design, interaction design, interactive installation, strong concept, context, derStandard, interactive art piece, qualitative study},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836073,
author = {Ghiani, Giuseppe and Manca, Marco and Patern\`{o}, Fabio},
title = {Authoring Context-Dependent Cross-Device User Interfaces Based on Trigger/Action Rules},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836073},
doi = {10.1145/2836041.2836073},
abstract = {Current authoring environments provide the possibility of developing user interfaces with limited adaptation capacities. The most widely adopted tools follow the responsive design approach and allow developers to obtain user interfaces that can adapt mainly to the screen size and orientation. We present a solution able to support development of user interfaces able to adapt to the various types of contextual events (that can be related to users, devices, environments, and social relationships), with the added possibility of distributing the user interface elements across multiple devices. The context-dependent behavior is modelled through trigger / action rules, and can even be applied to Web applications that were not originally designed to be context-aware. This paper describes the design and main features of the novel authoring environment and reports on a first user study.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {313–322},
numpages = {10},
keywords = {context-awareness, cross-device user interfaces, ubiquitous computing},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2836074,
author = {Kukka, Hannu and Goncalves, Jorge and Samodelkin, Alexander and Ojala, Timo},
title = {Opportunistic At-Glance Information Acquisition on Interactive Public Displays},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836074},
doi = {10.1145/2836041.2836074},
abstract = {The interaction process with interactive public displays can be viewed as a set of interaction phases. In this paper we report a wizard-of-Oz study that explores the last two phases: 1) subtle interaction, where users can interact with the display through gestures or movement, and 2) direct interaction, when users interact with the display by directly manipulating it through e.g. a touch-screen interface. We investigate the effect of these two interaction phases on at-glance information acquisition, and demonstrate that the presentation of such at-glance information items can help shorten interaction times during the direct interaction phase.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {323–327},
numpages = {5},
keywords = {wizard-of-Oz, at-glance information acquisition, public display, information behavior, information encountering},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841202,
author = {Nur, Kamruddin and Rashid, Zulqarnain and Pous, Rafael},
title = {A Smartphone Application for Voice Browsing RFID Smart Shelves},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841202},
doi = {10.1145/2836041.2841202},
abstract = {In this paper, we present a smartphone application for voice browsing products in smart shelves using Radio-Frequency Identification (RFID). In a retail store, a person usually perceives product information by looking at the product, product label, and sometimes touching the product. A visual impaired person or a person with other disabilities often seeks another persons' assistance to find out a preferred product in the shop. For a visual impaired, when there is no person to help out, finding a product using voice can be a great help. Here, we present an Android application and a back-end web service for product information browsing of RFID smart shelves using few voice dialogs.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {331–336},
numpages = {6},
keywords = {ubiquitous computing, radio-frequency identification},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841203,
author = {Covaci, Stefan and Frick, Maximilian and Kr\"{a}mer, Florian and P\"{o}rsch, Julian},
title = {Destmaster: Improved Destination Input System for the UR-Walking Application},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841203},
doi = {10.1145/2836041.2841203},
abstract = {Fast, intuitive interaction combined with high quality results are very important aspects, which need consideration, when implementing a search engine. With this in mind, the project "improved destination input system for the UR-Walking application" aims at optimizing the destination input for the Campus Navigation Web-Application at the University of Regensburg. Improving this type of search system involves handling ambivalent search requests. For example, keywords like the name of a professor could generate a misleading result, because the professors' name could lead either to his office or to one of his seminars. In this paper, we describe our implemented approaches to cope with a vast spectrum of different search requests concerning the campus navigation application.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {337–340},
numpages = {4},
keywords = {pedestrian navigation},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841204,
author = {Bodarw\'{e}, Karl-Arnold and Noack, Jenny and Jean-Jacques, Philipp},
title = {Emotion-Based Music Recommendation Using Supervised Learning},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841204},
doi = {10.1145/2836041.2841204},
abstract = {Music recommendation systems are well explored and commonly used but are normally based on manually tagged parameters and simple similarity calculation. Our project proposes a recommendation system based on emotional computing, automatic classification and feature extraction, which recommends music based on the emotion expressed by the song.To achieve this goal a set of features is extracted from the song, including the MFCC (mel-frequency cepstral coefficients) following the works of McKinney et al. [6] and a machine learning system is trained on a set of 424 songs, which are categorized by emotion. The categorization of the song is performed manually by multiple persons to avoid error. The emotional categorization is performed using a modified version of the Tellegen-Watson-Clark emotion model [7], as proposed by Trohidis et al. [8]. The System is intended as desktop application that can reliably determine similarities between the main emotion in multiple pieces of music, allowing the user to choose music by emotion. We report our findings below.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {341–344},
numpages = {4},
keywords = {supervised learning, music recommendation, naive bayes, MFCC, RMS, chroma, music emotion},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841205,
author = {Gaag, Philip and Granvogl, Daniel and Jackermeier, Robert and Ludwig, Florian and Rosenl\"{o}hner, Johannes and Uitz, Alexander},
title = {FROY: Exploring Sentiment-Based Movie Recommendations},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841205},
doi = {10.1145/2836041.2841205},
abstract = {Any unassisted decision a user has to make can be difficult, even if it entails the simple task of selecting which movie to watch. The sheer volume of movies offered by streaming platforms makes this task all the more difficult and time consuming. Many platforms attempt to combat this problem through recommendation systems. These however seem more likely to be making wild suggestions than being a constructive aid to the selection process. In order to offer more accurate recommendations, we propose a system that is based on a user's current emotion, which is matched with the sentiments contained in the movies' spoken language. A study involving our newly designed mobile sentiment-based movie recommender named 'FROY' shows highly promising results. As it turns out, sentiment analysis of spoken language leads to appropriate recommendations.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {345–349},
numpages = {5},
keywords = {normalized discounted cumulative gain, movie-recommender, NRC emotion lexicon},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841206,
author = {Popovici, Dorin-Mircea and Vatavu, Radu-Daniel and Polceanu, Mihai},
title = {GRASPhere: A Prototype to Augment Indirect Touch with Grasping Gestures},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841206},
doi = {10.1145/2836041.2841206},
abstract = {We present in this work GRASPhere, a prototype device that enables users to select and manipulate on-screen objects with grasping gestures. We demonstrate GRASPhere as an extension of a widely-employed force-sensing device, the Phantom OMNI. The hardware design of GRASPhere employs a minimum of components that cost less than $20, which makes our prototype easily replicable for practitioners interested in incorporating grasping gestures into their own force-feedback interactive applications. We discuss application opportunities for GRASPhere, such as exploring multimedia data with physical metaphors and providing assistance to people with visual impairments during indirect touch interaction.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {350–354},
numpages = {5},
keywords = {haptic feedback, object manipulation, grasping gestures, object selection, prototype, Phantom OMNI},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841207,
author = {Wu, Jianming and Yazaki, Tomonori},
title = {InfoFinder: Just-in-Time Information Interface from the Combination of an HWD with a Smartwatch},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841207},
doi = {10.1145/2836041.2841207},
abstract = {We present InfoFinder, a novel interface running on wearable devices that allows a user to perceive information using a simple finger-framing gesture. InfoFinder works in combination with a see-through head-worn display (HWD) and a smartwatch. Whenever the smartwatch detects a user's finger-framing gesture, it activates the HWD to extract the finger-framing area and display recognized object's information. InfoFinder avoids continuously tracking with the HWD camera, offering the advantages of fast, easily intuitive information acquisition as well as decreasing the possibility of misrecognition. An experiment shows InfoFinder responds in a timely manner to information acquisition requests (4.1 seconds), more than 5 times faster than using an HWD's conventional controller. We also verified that the task success rate of InfoFinder was improved by 32.8% compared to a reference prototype using real-time gesture tracking on the HWD.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {355–359},
numpages = {5},
keywords = {HWD, smartwatch, combination, just-in-time},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841208,
author = {Graf, Bettina and Kr\"{u}ger, Maike and M\"{u}ller, Felix and Ruhland, Alexander and Zech, Andrea},
title = {Nombot: Simplify Food Tracking},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841208},
doi = {10.1145/2836041.2841208},
abstract = {Quantified self is a growing research area in human -- computer interaction. New techniques help users to track and optimize their daily activities. Some data is collected automatically. Others like information about nutrition have to be entered manually by the user. This process is labor -- intensive and quite often the motivation is fading over the time. In this paper, an approach is introduced to simplify food tracking. This developed procedure assimilates into daily routines. The instant messaging service called Telegram is used to propose the chatbot Nombot to the user. This bot communicates with the user and collects data about the nutrition of its chat partner. Different motivation types are considered. To evaluate the described implementation, an A/B study is conducted.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {360–363},
numpages = {4},
keywords = {quantified self, tracking, human computer interaction, chatbot},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841209,
author = {Bockes, Florian and Edel, Laura and Ferstl, Matthias and Schmid, Andreas},
title = {Collaborative Landmark Mining with a Gamification Approach},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841209},
doi = {10.1145/2836041.2841209},
abstract = {In the field of pedestrian navigation some systems use prominent identifying features, so called landmarks. Attributes of high-quality landmarks are recognizability, actuality, uniqueness and noticeability. One of the challenges of this kind of navigation systems is to collect and evaluate landmarks with consistent quality. The system we developed solves these struggles with a crowdsourcing approach. We combine this with gamification elements in order to reach many users and to assure long-term motivation. Our system shows images of existing landmarks to the player, which he is afterwards asked to assign to a map of the university. Depending on the distance of his guess to the real position the player earns points. The application strives to encourage users to upload and rate pictures of existing landmarks. A multiplayer mode which allows challenging other users keeps them involved. In contrast to other products, our system does not rely on localization via GPS. Another goal was to implement a self-running system with a minimal amount of dedicated administration needed. Therefore the users with the highest scores are rating the submitted content.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {364–367},
numpages = {4},
keywords = {landmarks, gamification, crowdsourcing},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841210,
author = {Spiesberger, Paul and Jungwirth, Florian and W\"{o}ss, Christoph and Bachl, Stefan and Harms, Johannes and Grechenig, Thomas},
title = {Woody: A Location-Based Smartphone Game to Increase Children's Outdoor Activities in Urban Environments},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841210},
doi = {10.1145/2836041.2841210},
abstract = {This work describes the design of a smartphone-based game that aims to increase children's outdoor activity through location-based interactions. In the game, players have to physically go outside, find real trees, and interact with them using their smartphones in order to keep the game's character (a timber worm) alive. First evaluation results show that children enjoyed playing and increased their outdoor activity.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {368–372},
numpages = {5},
keywords = {mobile, location-based, physical activity, game},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841211,
author = {Ferron, Michela and Mana, Nadia and Mich, Ornella},
title = {Mobile for Older Adults: Towards Designing Multimodal Interaction},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841211},
doi = {10.1145/2836041.2841211},
abstract = {The paper presents the preliminary studies carried out in ECOMODE (Event-Driven Compressive Vision for Multimodal Interaction with Mobile Devices), an EU project aiming to design a multimodal interaction suitable for elderly people using mobile technology. The project exploits EDC (Event Driven Compressive) paradigm to realize a new generation of low-power multimodal human-computer interfaces for mobile devices. In our studies we investigated (1) how older adults interact with mobile devices and (2) how they use applications based on mid-air gesture interaction. Our studies basically confirm the findings of previous works about usability and accessibility issues, specifically characterizing elderly users' interaction with mobile technology.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {373–378},
numpages = {6},
keywords = {elderly users, speech-based interaction, mid-air gesture-based interaction, mobile technology, multimodal interaction},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841212,
author = {Helgath, Jana and Provinsky, Simon and Schaschek, Timo},
title = {Landmark Mining on a Smartwatch Using Speech Recognition},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841212},
doi = {10.1145/2836041.2841212},
abstract = {The smartwatch is a representative of the wearables, mini-computers worn on the body, which often only require casual interaction. In this project the advantages of smartwatches should be used to add landmarks to a database as performantly as possible and provide them for further usage. The mobile terminal should therefore be operated with voice input in a clearly structured speech dialogue to make the interaction as simple and intuitive as possible. Additionally a smartphone paired with the smartwatch distributes the user's GPS-data which help to gather data for landmarks in URWalking, a research project for pedestrian navigation at the University of Regensburg. The special problem is on the one hand to find solutions for hardware-related limitations such as a very small display size. On the other hand a way for the translation of plain-language input to technically evaluable data has to be found. In a final evaluation the system should be tested under real-life circumstances.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {379–383},
numpages = {5},
keywords = {landmark-mining, mobile computing, user assistance, speech recognition, smartwatch, wearables},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841213,
author = {Siebra, Clauirton and Gouveia, Tatiana and Macedo, Jefte and Correia, Walter and Penha, Marcelo and Silva, Fabio and Santos, Andre and Anjos, Marcelo and Florentin, Fabiana},
title = {Usability Requirements for Mobile Accessibility: A Study on the Vision Impairment},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841213},
doi = {10.1145/2836041.2841213},
abstract = {When multimedia applications intend to support accessibility, aspects of usability must be reviewed to adapt or extend common functional requirements that are implemented to ensure an easy use of applications. Furthermore, these requirements must be identified and analyzed in a contextualized way, since different types of impairments require different types of requirements. This work analyzed 247 scientific and technological articles to identify requirements that are being considered to different types of impairments. The collected information was consolidated and classified according to groups of impairments. As result, this paper brings a checklist proposal focused on requirements for vision impaired users that should be considered by mobile multimedia applications to ensure accessibility with usability.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {384–389},
numpages = {6},
keywords = {mobile devices, user interfaces, accessibility},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841214,
author = {Riegler, Andreas and Holzmann, Clemens},
title = {UI-CAT: Calculating User Interface Complexity Metrics for Mobile Applications},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841214},
doi = {10.1145/2836041.2841214},
abstract = {Currently, there is a lack of possibilities to quantify the complexity of mobile user interfaces with metrics. User studies for evaluating the user experience are costly and time consuming, but there are no tools available to automate this process. A tool for automatically calculating complexity metrics would make user interfaces quantifiable and thus allow for easily comparing different versions of an application or different apps among each other. We started developing a service for the Android OS to calculate user interface complexity metrics. In this paper, we present our approach and describe how the Android service is implemented.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {390–394},
numpages = {5},
keywords = {mobile applications, user interface complexity},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841215,
author = {Hahn, J\"{u}rgen and Ludwig, Bernd and Wolff, Christian},
title = {Augmented Reality-Based Training of the PCB Assembly Process},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841215},
doi = {10.1145/2836041.2841215},
abstract = {In this paper we propose an augmented reality (AR) based assistance system for reliably teaching the assembly process of printed circuit boards (PCB) to workers by using a smart glass running a self-developed software. The system is operated freehand by looking at QR-Codes and highlights a component's retrieval location and installation point in the user's field of vision by using four markers. A study executed in a production line of an Electronics Manufacturing Services (EMS)-company resulted in an errorless performance of each individual participant who was equipped with the system. This paper describes the related work, concept and implementation of the software as well as the conducted study and its results. Finally a conclusion summarizes the success of the system and hints at future work.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {395–399},
numpages = {5},
keywords = {industrial assembly, augmented reality, mobile information processing},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841216,
author = {Bexheti, Agon and Langheinrich, Marc},
title = {Understanding Usage Control Requirements in Pervasive Memory Augmentation Systems},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841216},
doi = {10.1145/2836041.2841216},
abstract = {Mobile and wearable devices allow people to capture different aspects of their life experiences (e.g. family holidays, work meetings, running activities, etc.) in the form of photos, videos, physiological data, etc. An interesting avenue to explore is the usage of such captured experiences to support and augment human memory. Experiences of different events can be used to generate retrieval memory cues in order to trigger recall of those recorded events. In addition, captured experiences can be shared with other (co-located) people of the same event. The focus of this work is on understanding the privacy challenges with regard to using and sharing captured experiences for memory augmentation purposes. With the ultimate goal of an usage control model for the protection of personal memory cues, here we provide insights on: how sharing captured experiences is different from sharing experiences in social media networks, and what are some challenges in designing an usage control model for memory cues.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {400–404},
numpages = {5},
keywords = {usage control, life-logging, memory augmentation, sharing, episodic memory, privacy},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841218,
author = {Grossauer, Christian and Holzmann, Clemens and Steiner, Dustin and Guetz, Andreas},
title = {Interaction Visualization and Analysis in Automation Industry},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841218},
doi = {10.1145/2836041.2841218},
abstract = {For a mobile developer, being able to see how users interact with a mobile application represents a valuable additional means for improving the user experience. In the automation industry, applications that are easy to use lead to less frustration and therefore shorter downtime of the machines. In this paper, we propose a visualization prototype and some general recommendations for visualizing and analyzing user interaction data of the automation industry.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {407–411},
numpages = {5},
keywords = {interaction analysis, automation industry, sankey diagram},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841219,
author = {Friedl, Andreas and Diephuis, Jeremiah and Kostov, Georgi and Naderer, Otto},
title = {MoCo Motion: Integrating Mobile Devices into a Multiplayer Floor-Based Gaming Environment},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841219},
doi = {10.1145/2836041.2841219},
abstract = {Although optical tracking systems greatly facilitate co-located multiplayer gaming experiences, they offer somewhat limited input and output modalities for increased player interaction. This demo presents MoCo, a framework for integrating mobile devices into a floor-based gaming environment to utilize their sensors and input/output capabilities. Two game prototypes, LazorLab and LazorArena, demonstrate how mobile devices are integrated into an application with a custom floor-projection and laser-tracking system.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {412–416},
numpages = {5},
keywords = {mobile, game development, laser-tracking, co-located game},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841220,
author = {Schiavo, Gianluca and Osler, Simonetta and Mana, Nadia and Mich, Ornella},
title = {Gary: Combining Speech Synthesis and Eye Tracking to Support Struggling Readers},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841220},
doi = {10.1145/2836041.2841220},
abstract = {Children with reading difficulties face several obstacles in learning to fluently read written material. Multimedia applications integrating text-to-speech (TTS) synthesisers are valuable tools for supporting reading activities. The paper presents GARY, an application that combines TTS synthesis with eye tracking. GARY is meant to be used on a tablet device coupled with an eye tracker. Making use of the information from reader's eye movement, the system allows users to adapt the speed rate of the synthesised voice to their pace of reading. The paper describes the system, its functioning and future steps in designing a tool for supporting readers' ability in making the connection between the sounds heard and the letters read.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {417–421},
numpages = {5},
keywords = {eye tracking, reading difficulties, speech synthesis, accessibility, technology-assisted reading},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841221,
author = {Rantakari, Juho and Colley, Ashley and H\"{a}kkil\"{a}, Jonna},
title = {Exploring AR Poster as an Interface to Personal Health Data},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841221},
doi = {10.1145/2836041.2841221},
abstract = {In this demo, we present a set-up that utilizes augmented reality (AR) in a display set-up combining a public and a private display for presenting health related information. Our installation includes a life-size poster of a human body, which is viewed through a mobile phone camera viewfinder that augments different body parts with health related data. We envision that this kind of set-up could be utilized in contexts where informative posters or similar are commonly used, and where people would benefit from accessing personal wellness or health information, such as a gym or medical doctor's waiting room.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {422–425},
numpages = {4},
keywords = {privacy, health, wellness, augmented reality, public displays},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841222,
author = {B\"{o}hm, Stephan and Igler, Bodo and Morales, Roberto and Sand, Frank and Ertan, Attila},
title = {Sauberes Wiesbaden App: Introducing Mobile e-Participation for a Clean City Project in Germany},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841222},
doi = {10.1145/2836041.2841222},
abstract = {Most of the current business services are accessed by traditional mechanisms of communication like calls to customer service centers, web forms or by email. Today's mobile technologies allow the consumption of those services transparently and anonymously inviting users to participate actively, anytime and anywhere, and without disrupting their daily activities. In this paper we present the preliminary results from a collaboration project between Wiesbaden's waste disposal services (ELW), Wiesbaden Council and the RheinMain University of Applied Sciences. The project aims to promote the participation of the citizens to quickly and easily report illegal garbage in the area of Wiesbaden, Germany. Applying a user centered approach; we were able to analyze the current internal business process to properly design the new processes that satisfy the user and business requirements. Based on the experience and knowledge of the people involved in the garbage collection process chain, we created a custom architecture for a mobile app and an interface to an existing backend system that allows citizens to create and send reports from mobile devices.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {426–429},
numpages = {4},
keywords = {user centered design, mobile applications, location based services, e-participation, mobile app usage},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841223,
author = {Probst, Kathrin},
title = {Active Office: Designing for Physical Activity in Digital Workplaces},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841223},
doi = {10.1145/2836041.2841223},
abstract = {Over the past centuries, our lives have become increasingly sedentary. Having an office job today, all too often involves sitting at a computer, performing the same small repetitive movements with our fingers, hands, and eyes over and over again. The shape, size and form of computing technology have influenced and placed limits on the physical movements that we perform throughout the day. Our research thus focuses on the question how human-computer interaction can fluently embed interactive technology into our workplaces to allow for more physically active working styles, and re-introduce diverse bodily movements in to our work routine. The present work describes how we are approaching this topic from a dual design perspective: macro-level support of smooth transition between different work postures, and micro-level design of bodily interaction technologies that increasingly acknowledge the richness of human motor skills.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {433–438},
numpages = {6},
keywords = {workplace design, embodied interaction, physical activity promotion},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841224,
author = {Assal, Hala},
title = {Collaborative Security Code Review},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841224},
doi = {10.1145/2836041.2841224},
abstract = {With millions of mobile applications available for download, and the proliferation of these types of software in our daily lives, it is becoming increasingly important to ensure the security of these applications. Previous research showed that developers have little knowledge of security and privacy regulations, and existing vulnerability detection tools have usability issues that prevent developers from using them. In my research I look at the human factor; e.g., how developers conduct security reviews of their code and what issues they face when using existing tools. In addition, I aim to develop tools and methodologies that support vulnerability detection while seamlessly integrating with the Software Development Lifecycle.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {439–444},
numpages = {6},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/2836041.2841225,
author = {Koelle, Marion and Kranz, Matthias},
title = {The Mind behind the Glass: Human Factors in the Design of Collaborative AR Environments},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841225},
doi = {10.1145/2836041.2841225},
abstract = {Immersive AR environments are situated information spaces that employ the Augmented Reality display paradigm. They tightly couple abstract, digital information and physical elements present in the user's real-world environment. As a consequence of this mixed digital-physical spatiality, interface design has the chance to build upon the user's cognitive map (spatial mental model) of her environment. The proposed PhD project investigates how knowledge about a user's spatial mental model of an augmented scene can be translated to best practices in the design of remotely shared AR environments.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {445–449},
numpages = {5},
keywords = {human factors, augmented reality, collaborative computing},
location = {Linz, Austria},
series = {MUM '15}
}

