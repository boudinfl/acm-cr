@inproceedings{10.1145/3365610.3365620,
author = {Gruenefeld, Uwe and Pr\"{a}del, Lars and Heuten, Wilko},
title = {Locating Nearby Physical Objects in Augmented Reality},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365620},
doi = {10.1145/3365610.3365620},
abstract = {Locating objects in physical environments can be an exhausting and frustrating task, particularly when these objects are out of the user's view or occluded by other objects. With recent advances in Augmented Reality (AR), these environments can be augmented to visualize objects for which the user searches. However, it is currently unclear which visualization strategy can best support users in locating these objects. In this paper, we compare a printed map to three different AR visualization strategies: (1) in-view visualization, (2) out-of-view visualization, and (3) the combination of in-view and out-of-view visualizations. Our results show that in-view visualization reduces error rates for object selection accuracy, while additional out-of-view object visualization improves users' search time performance. However, combining in-view and out-of-view visualizations leads to visual clutter, which distracts users.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {1},
numpages = {10},
keywords = {head-mounted, augmented reality, out-of-view, occlusion, in-view},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365612,
author = {Wolf, Dennis and Besserer, Daniel and Sejunaite, Karolina and Schuler, Anja and Riepe, Matthias and Rukzio, Enrico},
title = {CARe: An Augmented Reality Support System for Geriatric Inpatients with Mild Cognitive Impairment},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365612},
doi = {10.1145/3365610.3365612},
abstract = {Cognitive impairment such as memory loss, an impaired executive function and decreasing motivation can gradually undermine instrumental activities of daily living (IADL). With an older growing population, previous works have explored assistive technologies (ATs) to automate repetitive components of therapy and thereby increase patients' autonomy and reduce dependence on carers. While most ATs were built around screens and projection-based augmented reality (AR), the potential of head-mounted displays (HMDs) for therapeutic assistance is still under-explored. As a contribution to this effort we present cARe, an HMD-based AR framework that uses in-situ instructions and a guidance mechanism to assist patients with manual tasks. In a case study with six geriatric patients, we investigated the prototype's feasibility during a cooking task in comparison to a regular paper-based recipe. Qualitative and quantitative results indicate that cARe has potential to offer assistance to older individuals with declining cognitive function in their day-to-day tasks and increase their independence in an enjoyable way.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {2},
numpages = {11},
keywords = {IADL, in-situ, mixed reality, dementia, assistive technology, augmented reality},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365634,
author = {Lakhnati, Younes and Springer, Raphael and White, Edward and Gerken, Jens},
title = {Is It Real? Understanding Interaction Mechanics within the Reality-Virtuality Continuum},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365634},
doi = {10.1145/3365610.3365634},
abstract = {The concept of Mixed Reality has existed in research for decades but has experienced rapid growth in recent years, mainly due to technological advances and peripherals such as the Microsoft HoloLens reaching the market. Despite this, certain design aspects of Mixed Reality experiences, such as the different nuances of real and virtual elements, remain largely unexplored.This paper presents an explorative study with 15 participants which aims to investigate and gain a better understanding of the different qualities of real and virtual objects. To that end, we developed a Mixed Reality board game that offered different combinations of real and virtual game components, such as the board, the pieces and the dice. Our analysis shows that the participants generally preferred the completely virtual variant but appreciated different qualities of real and virtual elements. The results also indicate that virtual interaction elements work better on a real background than vice versa. However, this conflicts with some participants' preference of using physical pieces for the haptic experience, creating a design trade-off. This study represents a first step in exploring how the experience changes when swapping elements of differing realities for one another and identifying these trade-offs.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {3},
numpages = {10},
keywords = {mixed reality, board game, augmented virtuality, augmented reality, user study, reality-virtuality trade-off},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365631,
author = {Darbar, Rajkumar and Roo, Joan Sol and Lain\'{e}, Thibault and Hachet, Martin},
title = {DroneSAR: Extending Physical Spaces in Spatial Augmented Reality Using Projection on a Drone},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365631},
doi = {10.1145/3365610.3365631},
abstract = {Spatial Augmented Reality (SAR) transforms real-world objects into interactive displays by projecting digital content using video projectors. SAR enables co-located collaboration immediately between multiple viewers without the need to wear any special glasses. Unfortunately, one major limitation of SAR is that visual content can only be projected onto its physical supports. As a result, displaying User Interfaces (UI) widgets such as menus and pop-up windows in SAR is very challenging. We are trying to address this limitation by extending SAR space in mid-air. In this paper, we propose Drone-SAR, which extends the physical space of SAR by projecting digital information dynamically on the tracked panels mounted on a drone. DroneSAR is a proof of concept of novel SAR User Interface (UI), which provides support for 2D widgets (i.e., label, menu, interactive tools, etc.) to enrich SAR interactive experience. We also describe the implementation details of our proposed approach.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {4},
numpages = {7},
keywords = {projections, spatial augmented reality, flying user interface, mid-air display, 3D interaction, drones},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365643,
author = {Eguchi, Ryosuke and Isoyama, Naoya and Terada, Tsutomu and Tsukamoto, Masahiko},
title = {Evaluation of Attention Inducing Effects Using Ubiquitous Humanlike Face Robots},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365643},
doi = {10.1145/3365610.3365643},
abstract = {A human gaze not only has the meaning that someone is looking at something, but also has various effects on the surrounding people. For example, previous studies have shown that attention is induced in the gaze direction of others and that consciousness of being seen by others affects human behavior. In the future, android robots will play an active role in society. We can predict that communication and information transmission with such gaze information will be possible between humans and robots. However, to the best of our knowledge, no study has yet investigated the effect of robot gazes on people in a situation where many robots exist ubiquitously in daily life. Therefore, in this study, we aim to evaluate the attention inducing effect of the gaze of the face robots installed in the daily living environment. In this paper, we present several examples of services in a situation, where face robots are established ubiquitously in daily life. We verified the attention inducing effect in three test cases. As a result of the experiment, the number of times when the participants focused their attention in the direction of the gaze of the face robot was small. However, the feasibility of the service was demonstrated through the results of the experiment, such as surrounding situation, user's consciousness about the face robots, and attention induction timing.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {5},
numpages = {11},
keywords = {gaze cued attention, social attention, attention induction, human-robot interaction},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365622,
author = {M\"{u}ller, Jonas and Rieger, Lea and Aslan, Ilhan and Anneser, Christoph and Sandstede, Malte and Schwarzmeier, Felix and Petrak, Bj\"{o}rn and Andr\'{e}, Elisabeth},
title = {Mouse, Touch, or Fich: Comparing Traditional Input Modalities to a Novel Pre-Touch Technique},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365622},
doi = {10.1145/3365610.3365622},
abstract = {Finger touch and mouse-based interaction are today's predominant modalities to interact with screen-based user interfaces. Related work suggests that new techniques interweaving pre-touch sensing and touch are useful future alternatives. In this paper, we introduce Fich, a novel pre-touch technique that augments conventional touch interfaces with tooltips and further "fingerover" effects, opening up the space in front of the screen for user interaction. To study Fich in-depth, we developed a Fich-enabled weather application and compared user experience and interface discovery ("serendipity") of Fich against the traditional input modalities Mouse and finger Touch in a user study with 42 subjects. We report on the results, implying Fich's user experience to be rated significantly higher in terms of hedonic quality and significantly lower in terms of pragmatic quality, as compared to traditional input modalities.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {6},
numpages = {7},
keywords = {user study, input techniques, tooltips, pre-touch, user experience},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365629,
author = {Castellucci, Steven J. and MacKenzie, I. Scott and Misra, Mudit and Pandey, Laxmi and Arif, Ahmed Sabbir},
title = {TiltWriter: Design and Evaluation of a No-Touch Tilt-Based Text Entry Method for Handheld Devices},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365629},
doi = {10.1145/3365610.3365629},
abstract = {Touch is the predominant method of text entry on mobile devices. However, these devices also facilitate tilt-based input using accelerometers and gyroscopes. This paper presents the design and evaluation of TiltWriter, a non-touch, tilt-based text entry technique. TiltWriter aims to supplement conventional techniques when precise touch is not convenient or possible (e.g., the user lacks sufficient motor skills). Two keyboard layouts were designed and evaluated: Qwerty, and "Custom", a layout inspired by the telephone keypad. Novice participants in a longitudinal study achieved speeds of 12.1 wpm for Qwerty, 10.7 wpm for Custom. Error rate averaged 0.76% for Qwerty, 0.62% for Custom. A post-study extended session yielded 15.2 wpm with Custom, versus 11.5 wpm with Qwerty. Results and participant feedback suggest that a selection dwell time between 700 - 800 ms benefits accuracy.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {7},
numpages = {8},
keywords = {handheld devices, tilt-input, mobile device, text input},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365633,
author = {Wimmer, Christoph and Schl\"{o}gl, Richard and Kappel, Karin and Grechenig, Thomas},
title = {Measuring Mobile Text Entry Performance and Behaviour in the Wild with a Serious Game},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365633},
doi = {10.1145/3365610.3365633},
abstract = {Entering text is a fundamental part of how we interact with computing devices. The predominant form of text input on smartphones are virtual keyboards, which provide greater flexibility and customization options compared to hardware keyboards. Mobile text entry performance has been widely studied in HCI research, with most experiments being conducted in a controlled, artifical lab environment. To escape the boundaries of the lab and observe mobile text entry behaviour in a realistic and natural environment, we developed and publicly released the game Hyper Typer on Google Play Store. Hyper Typer is a serious research game for measuring mobile text entry performance and behaviour on a large scale in the real world. Publishing the game on Google Play Store resulted in a total of 2,359 valid transcribed phrases with 71,963 keystrokes. In this paper we discuss the game design of Hyper Typer, present the results collected over a time period of one year after the release of the game and reflect on the advantages, disadvantages and challenges of deploying a research game publicly.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {8},
numpages = {11},
keywords = {text input, evaluation methodology, text entry, crowd science, serious games, games with a purpose},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365648,
author = {Abdelrahman, Yomna and Woundefinedniak, Pawe\l{} W. and Knierim, Pascal and Weber, Dominik and Pfeuffer, Ken and Henze, Niels and Schmidt, Albrecht and Alt, Florian},
title = {Exploring the Domestication of Thermal Imaging},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365648},
doi = {10.1145/3365610.3365648},
abstract = {Recent work demonstrated the opportunities of thermal imaging in the development of novel interactive systems. However, the exploration is limited to controlled lab setups. Hence, little we know about how thermal imaging could be useful for a broader range of daily applications by novice users. To investigate the potential of domestication of thermal imaging, we conducted an exploration with a technology-cultural probe. Ten households (26 individuals) used a mobile thermal camera in their daily life. We collected thermal photos taken by the participants and conducted interviews after using the camera. We found that the users were excited about using thermal cameras in their everyday lives and found many practical uses for them. Our study provides insights into how novice users wish to use thermal imaging technology to augment their vision in daily setups, as well as identifying and classifying common thermal imaging use cases. Our work contributes implications for designing thermal imaging devices targeted towards novice users.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {9},
numpages = {7},
keywords = {thermal cameras, FLIR one, everyday use},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365626,
author = {Drewes, Heiko and Khamis, Mohamed and Alt, Florian},
title = {DialPlates: Enabling Pursuits-Based User Interfaces with Large Target Numbers},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365626},
doi = {10.1145/3365610.3365626},
abstract = {In this paper we introduce a novel approach for smooth pursuits eye movement detection and demonstrate that it allows up to 160 targets to be distinguished. With this work we advance the well-established smooth pursuits technique, which allows gaze interaction without calibration. The approach is valuable for researchers and practitioners, since it enables novel user interfaces and applications to be created that employ a large number of targets, for example, a pursuits-based keyboard or a smart home where many different objects can be controlled using gaze. We present findings from two studies. In particular, we compare our novel detection algorithm based on linear regression with the correlation method. We quantify its accuracy for around 20 targets on a single circle and up to 160 targets on multiple circles. Finally, we implemented a pursuits-based keyboard app with 108 targets as proof-of-concept.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {10},
numpages = {10},
keywords = {distinguishable pursuit targets, smooth pursuit detection, gaze-only text entry, linear regression},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365625,
author = {Weidner, Florian and Broll, Wolfgang},
title = {Interact with Your Car: A User-Elicited Gesture Set to Inform Future in-Car User Interfaces},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365625},
doi = {10.1145/3365610.3365625},
abstract = {In recent years, stereoscopic 3D (S3D) displays have shown promising results on user experience, for navigation, and critical warnings when applied in cars. However, previous studies have only investigated these displays in non-interactive use-cases. So far, interacting with stereoscopic 3D content in cars has not been studied. Hence, we investigated how people interact with large S3D dashboards in automated vehicles (SAE level 4). In a user-elicitation study (N=23), we asked participants to propose interaction techniques for 24 referents while sitting in a driving simulator. Based on video recordings and motion tracking data of 1104 proposed interactions containing gestures and other input modalities, we grouped the gestures per task. Overall, we can report a chance-corrected agreement rate of k = 0.232 and by that, a medium agreement among participants. Based on the agreement rates, we defined two sets of gestures: a basic and a holistic version. Our results show that participants intuitively interact with S3D dashboards and that they prefer mid-air gestures that either directly manipulate the virtual object or operate on a proxy object. We further compare our results with similar results in different settings and provide insights on factors that have shaped our gesture set.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {11},
numpages = {12},
keywords = {non-driving related tasks, driving simulation, stereoscopic 3D, elicitation study, natural user interfaces},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365614,
author = {Maurer, Steffen and Scatturin, Lara and Rukzio, Enrico},
title = {Playing Guardian Angel: Using a Gamified Approach to Overcome the Overconfidence Bias in Driving},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365614},
doi = {10.1145/3365610.3365614},
abstract = {It is likely that a highly automated vehicle will be able to detect dangerous situations and also determine if the driver is reacting accordingly while driving manually. The car could then take over control to avoid imminent danger. A study was conducted to identify situations that are perceived as highly dangerous by drivers. To eliminate the general impression of having the situation under control, participants had to decide about system interventions for another driver in a self-programmed game. Results show that by using this approach, situations, like tailgating, can be identified that are perceived as dangerous by everybody. Additionally, collecting direct feedback about situations, where people would want their car to interfere with their own driving, is showing discrepancies to the results of the gamified approach. Using elements of gamification is a promising procedure for studies like this but must be considered during the analysis of the results.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {12},
numpages = {12},
keywords = {study design, guardian angel, automated driving, gamification, user-study, method, cooperative driving},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365644,
author = {Faltaous, Sarah and Sch\"{o}nherr, Chris and Detjen, Henrik and Schneegass, Stefan},
title = {Exploring Proprioceptive Take-over Requests for Highly Automated Vehicles},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365644},
doi = {10.1145/3365610.3365644},
abstract = {The uprising levels of autonomous vehicles allow the drivers to shift their attention to non-driving tasks while driving (i.e., texting, reading, or watching movies). However, these systems are prone to failure and, thus, depending on human intervention becomes crucial in critical situations. In this work, we propose using human actuation as a new mean of communicating take-over requests (TOR) through proprioception. We conducted a user study via a driving simulation in the presence of a complex working memory span task. We communicated TORs through four different modalities, namely, vibrotactile, audio, visual, and proprioception. Our results show that the vibrotactile condition yielded the fastest reaction time followed by proprioception. Additionally, proprioceptive cues resulted in the second best performance of the non-driving task following auditory cues.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {13},
numpages = {6},
keywords = {proprioception, electrical muscle stimulation, autonomous vehicles, take-over requests},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365632,
author = {Matviienko, Andrii and Ananthanarayan, Swamy and Brewster, Stephen and Heuten, Wilko and Boll, Susanne},
title = {Comparing Unimodal Lane Keeping Cues for Child Cyclists},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365632},
doi = {10.1145/3365610.3365632},
abstract = {Child cyclists are at greater risk for car-to-cyclist accidents than adults. This is in part due to developmental differences in the motor and perceptual-motor abilities of children and adults, and missing cycling infrastructure. To address these issues, we examine unimodal and projection-based techniques to support children in maintaining a good lane position in the absence of bicycle lanes. We present safety-relevant information using unimodal cues: vibration on the handlebar, ambient light in a cycle helmet, projected heads-up display indicators, and on-road laser projection. As a first step, we interviewed twelve children about their cycling issues. We then conducted a lab experiment (N=25) in a bicycle simulator using the unimodal cues in the presence of a visual search task, followed by a controlled test-track experiment (N=15). We found that cycling performance with lane keeping cues was comparable to situations without them, however children found them helpful and expressed subjective preferences for the LED helmet and vibration on the handlebar.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {14},
numpages = {11},
keywords = {unimodal feedback, lane keeping, cycling safety, child cyclists},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365642,
author = {Hoppe, Matthias and Burger, Marinus and Schmidt, Albrecht and Kosch, Thomas},
title = {DronOS: A Flexible Open-Source Prototyping Framework for Interactive Drone Routines},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365642},
doi = {10.1145/3365610.3365642},
abstract = {We present DronOS, a rapid prototyping framework that can track, control, and automate drone routines. Previous research in the domain of Human-Drone Interaction relied on hardware or proprietary vendor-dependent libraries that had to be exclusively programmed for specific use cases. This forces users to stick with a drone manufacturer or model as well as limiting users in transferring their drone control logic to other drones. To overcome the aforementioned issues, our framework uses low-cost off-the-shelf hardware and applies to a variety of already available or self-crafted drones. To assess the usability of DronOS, we evaluate three drone programming modes: Unity Scripting, Vive Scripting, and Vive Realtime. We find that Vive Scripting required the least subjective workload in programming drone routines while Unity Scripting yielded the highest accuracy and Vive Realtime the least task completion time. We anticipate requirements for drone prototyping frameworks that target novice and expert users as operators.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {15},
numpages = {7},
keywords = {human-drone interaction, drones, automation},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365623,
author = {Draxler, Fiona and Schneegass, Christina and Lippner, Nicole and Schmidt, Albrecht},
title = {Exploring Visualizations for Digital Reading Augmentation to Support Grammar Learning},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365623},
doi = {10.1145/3365610.3365623},
abstract = {Reading foreign language texts is a frequently used strategy for language learning. Visual text augmentation methods further support the learning experience, e.g., by annotating vocabulary or grammar. Common approaches are integrated dictionaries or static grammar highlights. This work investigates how we can further support grammar learning with the dynamic visualization and interaction opportunities offered by digital reading devices. In collaboration with teachers and potential learners, we identify difficulties learners experience with English grammar and gather ideas for suitable interactive text augmentations. Based on this, we design four different concepts that augment adjectives and adverbs in English-language texts using typographic cues and interactive information displays. The concepts are evaluated in a within-subject study (N = 16). Results show that participants preferred concepts that presented case-specific support, did not distract too much from the text, and gave details on demand. We conclude with design recommendations for designing text augmentation for language learning.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {16},
numpages = {11},
keywords = {grammar, reading, textual enhancement, language learning},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365615,
author = {Herbig, Nico and Schuck, Patrick and Kr\"{u}ger, Antonio},
title = {User Acceptance of Cognition-Aware e-Learning: An Online Survey},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365615},
doi = {10.1145/3365610.3365615},
abstract = {The idea to enhance the learning experience of e-learning platforms by incorporating measures about the user's cognitive state, e.g. the cognitive load, boredom, or attention, has been proposed several times and shows promising results. However, the works have mostly dealt with conceptual implications and technical possibilities to detect the state, without considering the user acceptance. This paper therefore investigates whether users would actually be willing to provide access to sensor data such as heart or skin measurements for the sake of making e-learning systems cognition-aware. The results of an online survey with 50 participants show that people would provide access to behavioral data like keyboard input without major concerns; however, other sensors are considered more sensitive and would require strong learning experience improvements to make disclosure worthwhile. Participants also appear less concerned about sensors that are integrated into consumer devices than about less widespread ones. Furthermore, we report the general opinions regarding cognition-aware e-learning and discuss ideas on how best to adapt to the cognitive state.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {17},
numpages = {6},
keywords = {cognition-awareness, privacy, e-learning, sensor data},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365645,
author = {Jokela, Tero and Ojala, Jarno and V\"{a}\"{a}n\"{a}nen, Kaisa},
title = {How People Use 360-Degree Cameras},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365645},
doi = {10.1145/3365610.3365645},
abstract = {A variety of new kinds of consumer cameras that capture the full 360-degree surroundings in a single panorama photo or video has recently been introduced to the market. In addition to conventional viewing devices, such as smartphones and computers, the 360-degree photos and videos can also be viewed with a virtual reality headset for a more immersive experience. In this paper, we present a field study where 14 consumers used 360-degree cameras freely in their everyday lives for a period of four weeks. We describe the strategies that the participants applied to capture 360-degree content in different situations and discuss the opportunities, challenges, and limitations of using a 360-degree camera compared to conventional cameras. We identify four common practices of consumer 360-degree camera use: Panorama Capture, Experience Capture, Automatic Capture, and Document Capture. We also report on the participants' habits of viewing, sharing, editing, and managing consumer-captured 360-degree content.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {18},
numpages = {10},
keywords = {panorama, video, photo, consumer photography, immersive, 360-degree camera, omni-directional, virtual reality},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365613,
author = {Inget, Virve and M\"{u}ller, Heiko and H\"{a}kkil\"{a}, Jonna},
title = {Private and Public Aspects of Smart Jewellery: A Design Exploration Study},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365613},
doi = {10.1145/3365610.3365613},
abstract = {In recent years, design and form factors of wearables have converged towards traditional jewellery, such as rings, necklaces and bracelets. Even though the aesthetics of smart jewellery have been gradually improved, many current commercial pieces are still mainly functionality and technology-driven. In this paper, we take steps to better understand the design and wearing aspects of smart jewellery. We investigate how jewellery wearers associate functionalities with body locations in a design exploration session with industrial design students using the terms "private" and "public" as keys. Results suggest that with privacy in mind, smart jewellery designs move towards the wearer's hands and fingers and selected data sources originating from the wearer's body were preferred, while designs for "public" information presentation tended to be moved to higher locations on the body, i.e. head, neck and chest, and to be fed from external data sources.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {19},
numpages = {7},
keywords = {wearable computing, design exploration, smart jewellery},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365635,
author = {Jarusriboonchai, Pradthana and H\"{a}kkil\"{a}, Jonna},
title = {Customisable Wearables: Exploring the Design Space of Wearable Technology},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365635},
doi = {10.1145/3365610.3365635},
abstract = {This paper explores the design space of wearable technology from the customisation point of view. We identify emerging customisation practices and customisable attributes from previous work in research and in a dataset of 129 commercial wearable devices. We distinguish between attributes on functional customisation, interaction techniques, location on the body, and appearance. While people have a variety of conventional clothing and accessories options to fulfil their personal preferences and lifestyle, this is a lot less for wearable devices. The commercial wearables still cluster around few use cases, especially fitness, and wrist-worn form factors.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {20},
numpages = {9},
keywords = {style, aesthetics, fashion, customisation, review, personalisation, wearables},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365616,
author = {Yang, Jing and S\"{o}r\"{o}s, G\'{a}bor},
title = {Audio-Augmented Museum Experiences Using Wearable Visual-Inertial Odometry},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365616},
doi = {10.1145/3365610.3365616},
abstract = {The auditory sense is an intuitive and immersive channel to experience our surroundings, which motivates us to augment our perception of the real world with digital auditory content. We present a wearable audio augmented reality prototype that tracks the user with six degrees of freedom in a known environment, synthesizes 3D sounds, and plays spatialized audio from arbitrary objects to the user. Our prototype is built using head-mounted visual-inertial odometry, a sound simulation engine on a laptop, and off-the-shelf headphones. We demonstrate an application in a gallery scenario in which visitors can hear objects and scenes drawn in the paintings, feeling audio-visually engaged in the depicted surroundings. In a user study involving 26 participants, we observed that the audio-enhanced exhibition improved people's experience, as well as helped them remember more lively details of the artworks.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {21},
numpages = {6},
keywords = {visual-inertial odometry, user experience, museum exhibition, audio augmented reality, wearable},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365639,
author = {Mitake, Hiroto and Watanabe, Hiroki and Sugimoto, Masanori},
title = {Footsteps and Inertial Data-Based Road Surface Condition Recognition Method},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365639},
doi = {10.1145/3365610.3365639},
abstract = {We propose a method to recognize road surface conditions using footsteps and inertial data. In areas where the road surface conditions change significantly with the seasons and weather, bad road conditions cause dangerous such as falls. If the road surface condition can be determined in advance, danger can be averted by selecting safe routes and suitable shoes. In this study, we focus on the footsteps and inertial data that change depending on road surface conditions, such as dry pavement, puddle, soil, and mud. We implemented the prototype device and evaluated the proposed method on six road surface conditions with eight participants. The evaluation results confirmed that the recognition accuracy was 83.0% in a low-noise environment. When there was noise, we compared the standard approach, which combines footsteps and inertial data, and the revised method, which changes the confidence of the result of footstep recognition by the signal-noise ratio (SNR). The evaluation results also confirmed that the recognition rate increased by a maximum of 16.4% using the revised method (when the SNR was 1 dB, the average accuracy was improved from 37.5% to 53.9%).},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {22},
numpages = {10},
keywords = {road surface condition recognition, footstep, wearable computing, machine learning},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365618,
author = {Krieter, Philipp},
title = {Can I Record Your Screen? Mobile Screen Recordings as a Long-Term Data Source for User Studies},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365618},
doi = {10.1145/3365610.3365618},
abstract = {Mobile screen recordings are a rich data source to follow user interactions, but at the same time are highly privacy-invasive and manual analysis is time-consuming. This data source is mostly used in small, short-term user studies. Cloud-based approaches can automate the screen recording analysis but lack addressing that recordings contain sensitive private data. This makes ist hard to find participants for user studies involving long-term screen recording. In this paper, we present an approach to automatically analyzing mobile screen recordings on the user's mobile device that respects privacy and makes the approach scalable. We evaluate our privacy concept in a survey study of 35 participants, which indicates our concept increases the willingness of participants to participate in research studies using this method. For technical validation, we carry out test runs on several different devices and show that our approach can provide long-term results reliably in a pilot study.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {23},
numpages = {10},
keywords = {methods, screen recordings, log files, mobile application usage, long-term, privacy},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365611,
author = {Weber, Dominik and Voit, Alexandra and Kollotzek, Gisela and Henze, Niels},
title = {Annotif: A System for Annotating Mobile Notifcations in User Studies},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365611},
doi = {10.1145/3365610.3365611},
abstract = {Notifications are an essential feature of smartphones. While they support users in staying up-to-date, they are also a prominent source of interruptions. A deeper understanding of mobile notifications is required to avoid adverse effects. However, assessing mobile notifications is challenging as user studies on mobile notifications are typically conducted in-situ. Surveying users may lead to additional interruptions, and the content of notifications is inherently private. In this paper, we introduce a privacy-aware system for annotating mobile notifications in user studies. In an in-situ case study, participants annotated their notifications for one week. Participants perceived 38.91% of their notifications as not important and over half (51.75%) as non-urgent. Only 6.33% of the notifications were rated as both very important and very urgent. We discuss influencing factors, including a breakdown of messaging notifications, and implications for future smart notification systems that continue to fulfill users' information need while respecting their digital well-being.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {24},
numpages = {12},
keywords = {mobile notifications, in-the-wild, urgency, importance, smart-phones, interruptions, in-situ, annotation},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365636,
author = {Lischke, Lars and Schwind, Valentin and Schweigert, Robin and Woundefinedniak, Pawe\l{} W. and Henze, Niels},
title = {Understanding Pointing for Workspace Tasks on Large High-Resolution Displays},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365636},
doi = {10.1145/3365610.3365636},
abstract = {Navigating on large high-resolution displays (LHRDs) using devices built for traditional desktop computers can be strenuous and negatively impact user experience. As LHRDs transition to everyday use, new user-friendly interaction techniques need to be designed to capitalise on the potential offered by the abundant screen space on LHRDs. We conducted a study which compared mouse pointing and eye-tracker assisted pointing (MAGIC pointing) on LHRDs. In a controlled experiment with 35 participants, we investigated user performance in a one-dimensional pointing task and a map-based search task. We determined that MAGIC pointing had a lower throughput, but participants had the perception of higher performance. Our work contributes insights for the design of pointing techniques for LHRDs. The results indicate that the choice of technique is scenario-dependent which contrasts with desktop computers.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {25},
numpages = {9},
keywords = {MAGIC pointing, eye-tracking, pointing, large high-resolution displays},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365640,
author = {Burova, Alisa and Kelling, Chelsea and Keskinen, Tuuli and Hakulinen, Jaakko and Kallioniemi, Pekka and V\"{a}\"{a}t\"{a}j\"{a}, Heli and Turunen, Markku},
title = {Promoting Local Culture and Enriching Airport Experiences through Interactive Storytelling},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365640},
doi = {10.1145/3365610.3365640},
abstract = {Experiences in airports may shape future travel plans and contribute to tourism destination development. However, a chaotic environment and time-consuming procedural routines in airports may result in negative associations towards the host country and its culture. Despite the existence of assistive airport applications, little attention is given to facilitating travelers' engagement with cultural exploration.This paper introduces a concept of interactive personalized storytelling that provides both a cultural learning adventure and connection to local retailing. Our application generates an imaginative Finnish storyline unique to every user to guide them through local shops in the airport. A field evaluation was conducted with 15 travelers of different nationalities. Travelers perceived the interactive storytelling experience as an interesting and unique way to spend waiting time at the airport while increasing cultural exposure. Moreover, we found this method to be effective in persuading travelers to explore local products at the airport. Further, our results give insight to designing storytelling applications for large public places.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {26},
numpages = {7},
keywords = {airport experience, digital storytelling, mobile application, field study},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365628,
author = {Prange, Sarah and Mecke, Lukas and Stadler, Michael and Balluff, Maximilian and Khamis, Mohamed and Alt, Florian},
title = {Securing Personal Items in Public Space: Stories of Attacks and Threats},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365628},
doi = {10.1145/3365610.3365628},
abstract = {While we put great effort in protecting digital devices and data, there is a lack of research on usable techniques to secure personal items that we carry in public space. To better understand situations where ubiquitous technologies could help secure personal items, we conducted an online survey (N=101) in which we collected real-world stories from users reporting on personal items, either at risk of, or actually being lost, damaged or stolen. We found that the majority of cases occurred in (semi-)public spaces during afternoon and evening times, when users left their items. From these results, we derived a model of incidents involving personal items in public space as well as a set of properties to describe situations where personal items may be at risk. We discuss reoccurring properties of the scenarios, potential multimedia-based protection mechanisms for securing personal items in public space as well as future research suggestions.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {27},
numpages = {8},
keywords = {online survey, everyday life, personal items, public space, usable security, real world stories},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365647,
author = {Eghbali, Pouya and V\"{a}\"{a}n\"{a}nen, Kaisa and Jokela, Tero},
title = {Social Acceptability of Virtual Reality in Public Spaces: Experiential Factors and Design Recommendations},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365647},
doi = {10.1145/3365610.3365647},
abstract = {With the latest advancements in Virtual Reality (VR), the possible use of VR devices in public and social contexts has increased. Since the use of VR typically requires wearing a Head-Mounted Display (HMD), the user is not able to see others - the spectators - present in the same context. This may lead to a decrease of social acceptability of VR by both the users and the spectators. We conducted a field experiment to explore what are the experiential factors of the users of VR (N=10) and spectators of VR use (N=30). We found experiential factors for the users to be adjustment of interaction, uninterruptable immersion, un-intrusive communication, freedom to switch between realities, sense of safety, physical privacy, shared experience, and sense of belonging. For the spectators, the main factors are shared experience, enticing curiosity, feeling normal, and sense of safety. We then run three sessions with user experience (UX) experts (N=9) to create a set of design recommendations for socially acceptable VR. The resulting ten recommendations provide a holistic view to designing acceptable experiences for VR in public spaces.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {28},
numpages = {11},
keywords = {VR, spectators, design recommendations, user experience, public context, virtual reality, social acceptability},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365630,
author = {Oziom, Monika and Bachl, Stefan and Wimmer, Christoph and Grechenig, Thomas},
title = {GroFin: Enhancing in-Store Grocery Shopping with a Context-Aware Smartphone App},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365630},
doi = {10.1145/3365610.3365630},
abstract = {Grocery shopping is a regular necessity typically conducted in-store, with available alternatives such as online grocery shopping still not fully established yet. To address challenges experienced by shoppers in a store environment, two supporting modes were derived, implemented in a prototypical smartphone application using a minimal attention user interface and evaluated in an experimental field study. The two supporting modes consist of a novel map representation of shopping lists and a contextual proximity notification system. The evaluation results show an increase in efficiency in the process of grocery shopping, resulting in reduced shopping time, optimized shopping paths in the store, minimized dependency on external support and preservation of the shopper's cognitive effort. Acceptance of the introduced supporting modes was high which is a crucial indicator for future work.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {29},
numpages = {11},
keywords = {proximity, beacon, shopping list, context awareness, user study, grocery shopping},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365627,
author = {Habler, Florian and Peisker, Marco and Henze, Niels},
title = {Differences between Smart Speakers and Graphical User Interfaces for Music Search Considering Gender Effects},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365627},
doi = {10.1145/3365610.3365627},
abstract = {The ubiquitous availability of smart speakers allows hands- and eyes-free interaction through Voice User Interfaces (VUIs). Control-ling music playback is the most commonly used feature of VUIs. Previous work investigated how users naturally interact with smart speakers and suggested that users' gender could affect the devices' usability. The usability of commercial devices compared to other interactive systems and the effects of users' gender is, however, unclear. Therefore, we conducted a study with 20 participants using an Amazon Echo Dot and a laptop device. Participants searched for artists and titles using a Graphical User Interface (GUI) and a VUI. In addition, they performed different tasks such as saving a song in a playlist or adding songs into a queue. The analysis revealed that the VUI provides significantly lower usability because it lacks features, requires higher mental effort, and provides confusing answers. In contrast to previous concerns, the analysis did not reveal significant device\texttimes{}gender effects.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {30},
numpages = {7},
keywords = {music search, graphical user interface, voice user interface, gender, smart speaker},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365619,
author = {Hasan, Khalad and Mondal, Debajyoti and Thoma, Brent and Magnus, Alexander},
title = {MedGuide: A Smartphone Approach to Guide People through Important Information on Medicine Labels},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365619},
doi = {10.1145/3365610.3365619},
abstract = {The information on a non-prescription (over-the-counter) medicine label helps patients to make informed decisions when purchasing a medicine. Since non-prescription medicines can be purchased without consulting a healthcare professional, there is a growing concern that people do not put enough emphasis on reading medicine label information, resulting in drug misuse with possible health consequences. In this paper, we investigate patients' use of medicine labels with the goal of developing a smartphone application to guide them toward reading the important information (e.g., warnings, dosage) on the labels. We first conducted two studies examining (i) users' rating on the information that they commonly read and (ii) healthcare professionals' rating on the information that patients should read before purchasing a non-prescription medicine. Our results revealed that patients put less emphasis on reading many information such as dosage, warnings and precautions, that the healthcare professions highly recommend the patients to read. Inspired by the findings, we designed a smartphone application to make users aware of the important information on non-prescription medicines. Along the way, we conduct a study examining different information presentation techniques to show medicine labels on smartphones, where our results show that icons and texts are more accurate and preferred techniques by users. In a further study where users explore medicines with our smartphone application, we observed a significant increase (mean 27%) in the general users' rating on the information categories that were recommended by healthcare professionals. This suggests that the users were guided to read important information by the smartphone application.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {31},
numpages = {11},
keywords = {medicine label design, visual representation, smartphone application, non-prescription drugs},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365641,
author = {Tuomela, Sanna and Iivari, Netta and Svento, Rauli},
title = {User Values of Smart Home Energy Management System: Sensory Ethnography in VSD Empirical Investigation},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365641},
doi = {10.1145/3365610.3365641},
abstract = {Ubiquitous computing continues to transform our lives, including our homes and leisure activities. Smart home energy management system (SHEMS) are one example of such a technology. It connects homes to a smart grid and may increase the use of renewable energy by directing the demand to off-peak hours and reducing the overall energy demand. User values of such a technology may be critical in the acquisition, adoption and assimilation of the technology. This research fills the gap of understanding user values of SHEMS users. We studied new, potential and experienced users of SHEMS and their values. Sensory ethnography interview method was applied in the value sensitive design empirical investigation to elicit key user values of SHEMS in 28 families. The users relate to SHEMS values such as economic gains, environmental sustainability, comfort and security. Some SHEMS users' values such as stimulation, creativity, and autonomy, can be in conflict with the values of other family members, and with those which are currently built in the SHEMS technologies. The recognized values of SHEMS stakeholders act as an input for the design of smart grid and smart home services and products. In addition, the research contributes to the theory-building of smart home technology user research.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {32},
numpages = {12},
keywords = {sensory ethnography, energy management system, smart home energy management systems, stakeholder analysis, user values, value-sensitive design},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365624,
author = {Hoffmann, Fabian and Tyroller, Miriam-Ida and Wende, Felix and Henze, Niels},
title = {User-Defined Interaction for Smart Homes: Voice, Touch, or Mid-Air Gestures?},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365624},
doi = {10.1145/3365610.3365624},
abstract = {Smart home appliances and smart homes, in general, are on the verge of ubiquity. Research and industry proposed a range of modalities, including speech, mid-air gestures, and touch displays, to control smart homes. While previous work designed for the individual modalities, it is unclear how they compare from a user-centered perspective. Therefore, we conducted an elicitation study that asked participants to propose commands using speech, mid-air gestures, and a touch display. Also, we asked participants to rate their suggestions and the modalities. The results show that using voice commands or a touch display is clearly preferred compared to the use of mid-air gestures. As we found high agreement scores for voice commands, our results also highlight the potential of elicitation studies for voice interfaces.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {33},
numpages = {7},
keywords = {voice control, smart home, display control, mid-air gestures},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365646,
author = {Blattgerste, Jonas and Renner, Patrick and Pfeiffer, Thies},
title = {Authorable Augmented Reality Instructions for Assistance and Training in Work Environments},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365646},
doi = {10.1145/3365610.3365646},
abstract = {Augmented Reality (AR) is a promising technology for assistance and training in work environments, as it can provide instructions and feedback contextualised. Not only, but especially impaired workers can benefit from this technology. While previous work mostly focused on using AR to assist or train specific predefined tasks, "general purpose" AR applications, that can be used to intuitively author new tasks at run-time, are widely missing.The contribution of this work is twofold: First we develop an AR authoring tool on the Microsoft HoloLens in combination with a Smartphone as an additional controller following considerations based on related work, guidelines and focus group interviews. Then, we evaluate the usability of the authoring tool itself and the produced AR instructions on a qualitative level in realistic scenarios and gather feedback. As the results reveal a positive reception, we discuss authorable AR as a viable form of AR assistance or training in work environments.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {34},
numpages = {11},
keywords = {augmented reality, authoring, annotation, training, assistance, cognitive impairments, mixed reality},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365621,
author = {Oppenlaender, Jonas and Hosio, Simo},
title = {Design Recommendations for Augmenting Creative Tasks with Computational Priming},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365621},
doi = {10.1145/3365610.3365621},
abstract = {Supporting creativity is a grand challenge in HCI. A critical component of creativity is the ability for divergent thinking, and divergent thinking can be fostered through looking at the problem through the lens of a different person, by assuming a role. Prior work found that assuming a role and affective stimulation with images may lead individuals to be more creative. In this work, we investigate the use of roles in stimulating the creativity of individuals in two complementary studies. In the first study, we implemented an online instrument for augmenting creativity with roles and images, and recruited crowd workers (n = 60) to complete a divergent thinking task while assuming a role. Interestingly, and in contrast to earlier findings, our analysis could not confirm the computational priming having an effect on the outcome of a small batch of creative tasks. In the second study, we observed the effect of roles on the ideation process of individuals when they reach an impasse in the flow of ideas. Our complementary studies highlight that adopting roles can help when one runs out of ideas, but this is not a silver bullet for improving divergent thinking, especially in online crowd-sourcing environments that are increasingly being used for experiments and data collection in behavioural science. Our work informs the design of future crowd-powered creativity support tools and contributes a timely case study to the body of literature in the growing field of creativity support online.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {35},
numpages = {13},
keywords = {crowdsourcing, computational priming, creativity, creativity support},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365637,
author = {Le, Khanh-Duy and Woundefinedniak, Pawe\l{} W. and Alavi, Ali and Fjeld, Morten and Kunz, Andreas},
title = {DigiMetaplan: Supporting Facilitated Brainstorming for Distributed Business Teams},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365637},
doi = {10.1145/3365610.3365637},
abstract = {While facilitated brainstorming is a proven ideation method for professional teams, distributed teams are currently not able to enjoy its benefits. As workers are shifting towards collaborating in distributed settings, understanding how interactive systems can support facilitated brainstorming is becoming necessary to ensure that distributed teams remain creative. To address this challenge we designed, implemented, and evaluated DigiMetaplan---an interactive surface-based system for distributed facilitated brainstorming for co-located and remote users. The design of DigiMetaplan was inspired by a widely-used facilitated brainstorming method called Metaplan, where the brainstorming process of a group is coordinated by a facilitator. We evaluated the usability of DigiMetaplan with five hybrid teams consisting of a co-located facilitator and two team members connected with one remote participant. Results showed that the features used in DigiMetaplan on interactive surfaces effectively supported teams in performing facilitated collaborative brainstorming in partially distributed settings. We contribute knowledge on how a brainstorming environment translated into a multi-surface distributed system affects facilitated collaboration.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {36},
numpages = {12},
keywords = {brainstorming, remote collaboration, mobile devices, large displays, creative problem solving},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365617,
author = {Fiore, Dario and Baldauf, Matthias and Thiel, Christian},
title = {"Forgot Your Password Again?": Acceptance and User Experience of a Chatbot for in-Company IT Support},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365617},
doi = {10.1145/3365610.3365617},
abstract = {Over the last few years, chatbots have become a common channel for customer service interactions. In contrast, their usage for in-company applications and respective scientific knowledge about so-called virtual enterprise assistants from a user perspective are still scarce. In this paper, we studied the acceptance and user experience of a chatbot for in-company IT support. In a user study, 12 employees of a bank and a hospital evaluated and assessed a respective chatbot prototype supporting three typical use cases. Our results indicate that such an in-company chatbot is well-suited for structured use cases, such as resetting a password and releasing an email attachment from quarantine. Participants appreciated the simplicity, the pro-active guidance and immediate feedback. The participants assessed a chatbot and the phone as preferred channels to the IT help desk.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {37},
numpages = {11},
keywords = {help desk, user study, IT support, virtual enterprise assistant, chatbot},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3365638,
author = {Riedl, Felix and Sageder, Julia and Henze, Niels},
title = {Do <i>Knob</i> Disturb: A Tangible Controller for a Distraction-Free Work Environment},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3365638},
doi = {10.1145/3365610.3365638},
abstract = {Disruptive colleagues, procrastinative web-browsing or low-priority e-mail are just a few types of distractions in the modern workplace. They reduce efficiency and increase perceived workload. Previous work shows that digital and social distractions can be reduced by tangible artifacts that signal phases of high concentration to colleagues or block websites. In this paper, we present the knob a holistic approach to distraction blocking. It simultaneously serves as a controller for blocking websites, managing smartphones' state, and signaling availability to colleagues. We evaluated the system through an in-situ deployment to understand how the artifact can reduce distractions. We show that the knob has the potential to improve users' self-discipline and provide suggestions for future distraction blocking solutions.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {38},
numpages = {7},
keywords = {workflow, interuptability, tangible interaction, distraction},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368408,
author = {Pekkanen, Siina and V\"{a}yrynen, Jani and Lappalainen, Tuomas and Colley, Ashley and H\"{a}kkil\"{a}, Jonna},
title = {Liquid Slider: Evaluating the Experience of Liquid on a Touch Input Surface},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368408},
doi = {10.1145/3365610.3368408},
abstract = {We investigate an unconventional user interface (UI) material, water, and how applying it to a finger-touch based input affects the user experience (UX). We compare the use of touchscreen, mechanical and liquid covered sliders, and three different approaches of how water is applied to the surface of the slider. The salient findings of our user study (n=25) show that liquid changes the perception of the slider control making it more hedonic and experiental. When interacting with water, users prefer larger and more free-form gestures, and factors such as depth and temperature of the liquid affect the experience. Our work provides information for interaction designers focusing on user experience, unconventional materials for interactive systems, aesthetics, and calm UIs.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {39},
numpages = {5},
keywords = {tangible user interfaces, aesthetics, water, interactive surfaces, user studies, user experience, materiality},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368409,
author = {Jiang, Keren and Zhang, Di and Iino, Tsubasa and Kimura, Risa and Nakajima, Tatsuo and Shimizu, Kana and Ohue, Masahito and Akiyama, Yutaka},
title = {A Playful Tool for Predicting Protein-Protein Docking},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368409},
doi = {10.1145/3365610.3368409},
abstract = {In this paper, we propose a playful tool to predict protein-protein docking. The tool uses human bodies to explore better protein-protein docking, where it offers natural interactions to dock protein molecules in a flexible way and playful interactions through two users' cooperative body movements. We present the design and implementation of the proposed tool and show potential opportunities and pitfalls of the current tool.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {40},
numpages = {5},
keywords = {gesture, citizen science, body action, protein-protein docking},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368410,
author = {Colley, Ashley and Raudanjoki, \"{O}zge and Mikkonen, Kirsi and H\"{a}kkil\"{a}, Jonna},
title = {Plant Shadow Morphing as a Peripheral Display},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368410},
doi = {10.1145/3365610.3368410},
abstract = {The amount of status and notification information available to people is increasing beyond the capacity of the current primary delivery channel of smartphones. Ambient displays, e.g. embedded in the home or work environment present an alternative channel, suited to less critical information. We created an ambient display by manipulating the natural shadows cast by a house plant to carry information. To explore user perceptions of the shadow display we evaluated it in a focus group (n = 6). Whilst participants saw some potential in the approach, there were concerns about identifying the meaning of the display and the psychological effects of distorting reality. The work highlights the potential for designers to explore the use of functional shadows to carry non-critical information.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {41},
numpages = {5},
keywords = {plants, shadows, nature, ambient displays, peripheral displays, information visualization},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368411,
author = {Luiro, Elina and Hannula, Petri and Launne, Emilia and Mustonen, Sanni and Westerlund, Toni and H\"{a}kkil\"{a}, Jonna},
title = {Exploring Local History and Cultural Heritage through a Mobile Game},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368411},
doi = {10.1145/3365610.3368411},
abstract = {We describe our work on developing a mobile game that utilizes local history and cultural heritage in its storyline and content. The game is depicted in the town of Kemij\"{a}rvi, Northern Finland, in the 1920's, and its aim is to encourage visitors and locals to get to know the town's history. The game was designed with input from history experts on the topic, and it introduces local history in the form of a narrated story, where the user has to visit the town's historical places and characters. We present the design process, the game concept and a user evaluation. As the lessons learnt and findings from our design process, we highlight the challenge of finding the balance between historical accuracy and engaging game narrative, and the importance of selecting the target user group and involving them when refining the concept and user interface design.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {42},
numpages = {4},
keywords = {cultural heritage, user experience, history, location based services, mobile game design},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368412,
author = {K\"{u}hn, Romina and Korzetz, Mandy and Schlegel, Thomas},
title = {User Strategies for Mobile Device-Based Interactions to Prevent Shoulder Surfing},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368412},
doi = {10.1145/3365610.3368412},
abstract = {Shoulder surfing, also known as visual hacking, is the activity of obtaining information from or about others by observing visual content of displays that actually should be kept secret, such as PINs, passwords, or private text messages. Approaches that address shoulder surfing on mobile devices mainly focus on ways to recognize observers or to complicate visual presentations for them from the system's perspective. However, users also have developed their own strategies to keep their input secret. With this work, we contribute an investigation of strategies to prevent shoulder surfing from the users' perspective. We performed a user study and observed 32 participants while interacting with smartphones using different kinds of eyes-free device-based interaction techniques. We identified several strategies that users had to prevent shoulder surfing. These strategies help us to develop effective ways to design useful interactions that overcome shoulder surfing issues.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {43},
numpages = {5},
keywords = {shoulder surfing, mobile devices, collocated collaboration},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368413,
author = {Karhu, Mari and H\"{a}kkil\"{a}, Jonna and Timonen, Eija},
title = {Collaborative Media as a Platform for Community Powered Ecological Sustainability Transformations: A Case Study},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368413},
doi = {10.1145/3365610.3368413},
abstract = {We explore collaborative media as a tool for empowering people towards sustainability and seeking solutions against climate change. As a case study, we investigate the Youth4Climate collaborative media site, where people can post an idea for tackling climate change, and comment and vote on others' ideas. By analysing the body of 266 ideas and 1406 comments posted on the collaborative media platform, we identify eight functions on how collaborative media supports community powered ecological sustainability transformations. Our work contributes in understanding and structuring collaborative media in citizen activism.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {44},
numpages = {5},
keywords = {activism, citizen participation, social movement, climate change, sustainability, collaborative media},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368414,
author = {Pandey, Laxmi and Arif, Ahmed Sabbir},
title = {Context-Sensitive App Prediction on the Suggestion Bar of a Mobile Keyboard},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368414},
doi = {10.1145/3365610.3368414},
abstract = {This work augments context-sensitive app prediction feature to the suggestion bar of a mobile virtual keyboard to accommodate fast and easy information acquisition and sharing in textual conversations. The purpose is to eliminate the need for switching between apps while typing. A user study revealed that the proposed method improves performance both in terms of speed and effort for common tasks, such as playing a song or finding and sharing the address of a restaurant. Post-study questionnaire revealed that all participants found the method fast, easy, and likely to facilitate more engaging and meaningful textual conversations. All wanted to keep using it on their mobile devices.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {45},
numpages = {5},
keywords = {text entry, smartphones, suggestion bar, apps, prediction bar},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368415,
author = {Yang, Jing and Chan, Cheuk Yu},
title = {Audio-Augmented Museum Experiences with Gaze Tracking},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368415},
doi = {10.1145/3365610.3368415},
abstract = {In this work, we enrich landscape and genre paintings by spatializing sounds for the drawn objects and scenes, which expands visitors' perception of the paintings and immerses them in the depicted scenarios. Plus, we personalize such spatial audio perception based on visitors' viewing behavior by applying gaze tracking. Through a preliminary user study with 14 participants, we observed that the gaze tracking-based audio augmentation helped people better focus on the areas of interest in the paintings, and enhanced their overall viewing experience.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {46},
numpages = {5},
keywords = {user experience, museum exhibition, gaze tracking, audio augmented reality},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368416,
author = {Franco, J\'{e}ssica and Cabral, Diogo},
title = {Augmented Object Selection through Smart Glasses},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368416},
doi = {10.1145/3365610.3368416},
abstract = {Several interaction methods for smart glasses have been studied, but it is not clear which method could be the best to interact with augmented objects. In this study, an Augmented Reality (AR) marker-based application for smart glasses was developed, implementing three different interaction methods for marker selection: one using mid-air hand gestures, other using head movements and double-tap in the glasses, and the last one using a handheld controller. We studied the users' interaction times, preferences, and their willingness to perform the interaction method in public. We find that users' felt more comfortable using a familiar handheld controller. However, the solution with the smart glass's tap function and head movement got results close to the results of smart glass's controller.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {47},
numpages = {5},
keywords = {augmented reality, interaction methods, touchless inputs, marker-based, touch inputs, smart glasses},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368417,
author = {Lopes, La\'{\i}s and Campos, Pedro},
title = {SCAARF: A Subtle Conditioning Approach for Anxiety Relief Facilitation},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368417},
doi = {10.1145/3365610.3368417},
abstract = {Anxiety disorders are one of the most prevalent mental health conditions, affecting more than 280 million people worldwide. This paper presents a novel approach to assistive technologies for mental health. In our approach, the paired combination of a wearable device --- in the form of a scarf --- and an accompanying mobile app, helps mitigate anxiety symptoms whenever they start to arise. This approach differs from conventional ubiquitous technologies developed for dealing with mental health, both in the role the mobile app plays in the process and in the way the intervention is delivered to users. SCAARF offers a more subtle, less obtrusive method for coping with anxiety. We discuss the results of an initial (three-week-long) evaluation of SCAARF with anxiety-suffering users. Qualitative results suggest that the SCAARF mobile app is effective in helping users relax and cope with anxious states of mind.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {48},
numpages = {5},
keywords = {mental health, mobile, wearable, anxiety disorders, assistive technologies, subliminal},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368418,
author = {Bottoni, Paolo and Di Tommaso, Francesco and Panizzi, Emanuele},
title = {Capturing and Using Context in a Mobile Annotation Application},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368418},
doi = {10.1145/3365610.3368418},
abstract = {We present an approach to integrating extemporary annotations on topics of interest to a user with information on the context in which the annotation is being taken. Context is here defined in terms of the set of surrounding devices (Bluetooth Low Energy enabled smartphones, WiFi hotspots) and the current calendar event. Contextify is a context-aware Android application which detects the current context and organizes the notes taken within context, allowing a form of context-based retrieval. Thus, the detected context represents location, other people present (referring to their BLE equipped smartphones as user proxies), and events. Users can easily retrieve notes when they return to the context where they created them. A context similarity algorithm derived from the Jaro-Winkler string similarity algorithm is used to compare contexts. Each note is tagged by the user and the system suggests the most appropriate tag, among the already used ones, at annotation creation time: the suggestion is based on the similarity of the current context with the contexts associated with previously tagged notes.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {49},
numpages = {4},
keywords = {context annotation, context-aware computing, android, Bluetooth low energy, notes},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368419,
author = {Pulina, Francesca and Patern\`{o}, Fabio},
title = {Supporting Cross-Device Interactions with Gestures between Personal and Public Devices},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368419},
doi = {10.1145/3365610.3368419},
abstract = {Seamless interaction across personal and public devices is still problematic. Gestural interaction can be a useful support for this purpose, but how to exploit it in cross-device frameworks is still unclear. We present an elicitation study aiming to contribute in identifying the most intuitive single or combined gestures that people can perform to interact with cross-device applications. This study led us to the definition of a possible gesture vocabulary for the typical interactive tasks in such applications. We then applied the resulting preferred gestures in an example cross-device Web application exploiting it. This application has finally been tested in order to evaluate the gestures' actual usability in cross-device interactions.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {50},
numpages = {5},
keywords = {user-defined gestures, cross-device interaction techniques, gesture vocabulary, cross-device user interfaces, elicitation study},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368420,
author = {Vaquero-Melchor, Diego and Bernardos, Ana M.},
title = {Alternative Interaction Techniques for Drone-Based Mission Definition: From Desktop UI to Wearable AR},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368420},
doi = {10.1145/3365610.3368420},
abstract = {In this paper we address the adaptation of applications for conventional platforms (web, tablets and smartphones) to Augmented Reality (AR) settings. We do this on a specific use case, a tool to help users to define drone missions, which has been implemented with equivalent functionalities both for web-enabled devices and AR wearable devices (in particular, HoloLens). First, a brief description of the application workings is presented as well as the creation process for the mission and the manipulation of its components. Then, we present a preliminary validation with 8 users to analyze the degree of acceptance of the AR version, taking the web user interface as a baseline. Despite the technical limitations of the AR version (mainly related to the device visualization constraints and weight) and the reduction in efficiency (tasks need more time to be completed), several users have expressed their preference for this version. However, with the problem of precision in AR Head Mounted Display context always present, the need to establish new interaction techniques is once more highlighted.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {51},
numpages = {5},
keywords = {head mounted display, interface adaptation, augmented reality, user validation},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368421,
author = {Mori, Giulio and Santoro, Carmen and Patern\`{o}, Fabio},
title = {Understanding Indoor Orientation through Wearable Vibrotactile Feedback},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368421},
doi = {10.1145/3365610.3368421},
abstract = {The main goal of this work is to better understand how vibrotactile feedback obtained through wearable actuators can support indoor orientation in unfamiliar buildings. We designed different wearable vibrotactile prototypes (two wristbands, a flexible/rigid glove, one wristband, a cap and two-bands), and we analysed them in a preliminary test with 7 users to identify the design aspects that are most relevant for a solution guiding a person indoor. We describe the design and its evaluation with 36 users, discussing the results that can be useful for developers who want to use this technology within applications that need support for indoor orientation.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {52},
numpages = {5},
keywords = {orientation &amp; navigation, usability, vibrotactile},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368422,
author = {Kostakos, Panos and Alavesa, Paula and Oppenlaender, Jonas and Hosio, Simo},
title = {VR Ethnography: A Pilot Study on the Use of Virtual Reality 'go-along' Interviews in Google Street View},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368422},
doi = {10.1145/3365610.3368422},
abstract = {Go-along interviewing is an emerging qualitative research method for eliciting contextualised perspectives in which informants and observers conduct mobile interviews while navigating in real or imagined sites. This paper describes results of a pilot study that use virtual reality (VR) go-along interviews to explore university community members' (N=6) contextualized perceptions of urban habitat fragmentation due to new transportation infrastructure. Participants were immersed into the popular Google Street View and asked to navigate from the University campus to the city center. Along that route, construction sites featured in 360° images acted as prompts for discussing ecological change. Preliminary results indicate that VR go-along interviews are able to evoke emotions and inform a broad range of research questions with regards to both verbal and non-verbal feedback received from the informants.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {53},
numpages = {5},
keywords = {VR, go-along, Google street view, ethnography},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368423,
author = {Kim, A-young and An, Eun-bin and Seo, Kwang-deok},
title = {An Advanced QER Selection Algorithm for 360-Degree VR Video Streaming},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368423},
doi = {10.1145/3365610.3368423},
abstract = {The 360-degree virtual reality (VR) video streaming faces many challenging issues since VR video contains a large volume of data and requires streaming omnidirectional view. For consuming VR video in a reasonable quality, the streaming system is required to efficiently use bandwidth and reduce the computational complexity of the client device. Quality Emphasized Region (QER) based streaming has been introduced as a kind of viewport adaptive streaming scheme for reducing waste of bandwidth. This scheme requires QER selection for providing the adaptive quality of QER. We propose an advanced QER selection algorithm consisting of two stages. The first stage is the calculation of the Quality Emphasis Center distance which employs preprocessed QER_ID_MAP for reducing the computational complexity. As the second stage, we propose an adaptive adjustment of the signaling interval between signaling messages. The simulations show that the proposed algorithm results in reduction of computational complexity and bandwidth waste.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {54},
numpages = {5},
keywords = {immersive media streaming, quality emphasize region (QER) selection, QER-based streaming, virtual reality (VR)},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368424,
author = {Tolino, Umberto and Mariani, Ilaria and Livio, Tommaso and Marangoni, Stefano},
title = {Variable and Situated User Interfaces: Assumptions, Potentials and Design Issues},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368424},
doi = {10.1145/3365610.3368424},
abstract = {Increasing computational capacity, small-size and small-cost sensors are leading the way towards widespread and pervasive presence of smart and IoT-based devices. The contribution presents an ongoing research conducted within DecoChrom, a Horizon 2020 project working on interfaces that have the potential to modify the functionality of a smart object by appearing and disappearing. Since 2018, we started developing interfaces that draw themselves accordingly to the user need. Drawing User Interfaces have the ability to react to external stimuli---as the information gathered from the physical world via sensors---, modifying their aesthetics and function accordingly. Following this logic, an object can display different interfaces depending on its positioning, physical variation of the environment, or even the presence of other smart objects.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {55},
numpages = {4},
keywords = {user experience, IoT, embedded technology, smart objects, user interface, physical interaction},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368425,
author = {H\"{a}kkil\"{a}, Jonna and Hannula, Petri and Luiro, Elina and Launne, Emilia and Mustonen, Sanni and Westerlund, Toni and Colley, Ashley},
title = {Visiting a Virtual Graveyard: Designing Virtual Reality Cultural Heritage Experiences},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368425},
doi = {10.1145/3365610.3368425},
abstract = {Virtual reality offers a potential solution to enable visiting inaccessible cultural heritage sites. We present the design, prototyping and evaluation (n = 6) of a virtual visit to a historic graveyard. The Salla World War II graveyard is located in an inaccessible border zone between Finland and Russia. Our virtual graveyard, accessed through a head mounted display, aimed to create an as accurate as possible simulation of the Salla graveyard, including its atmosphere. Users are enabled to navigate the virtual graveyard and place a candle on a grave. Although the simulation was considered immersive, participants wished for more authenticity and details, e.g. being able to light the virtual candle. The work opens discussion on the need for dignity in the design of virtual experiences for sensitive cultural heritage sites.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {56},
numpages = {4},
keywords = {death-scapes, virtual reality, graveyard, museum, WW2, cultural heritage, cemetery, death, virtual heritage, experience design},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368471,
author = {Lakhnati, Younes and Springer, Raphael and Gerken, Jens},
title = {Mensch ARgere Dich Nicht: A Board Game Testbed for Mixed Reality Interactions},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368471},
doi = {10.1145/3365610.3368471},
abstract = {"Mensch ARgere Dich Nicht" is a Mixed Reality (MR) game based on a popular German board game "Mensch \"{A}rgere Dich Nicht", from the early 1900s, similar to Pachisi. Developed to be a test bed for investigating mixed interactions, the application offers real and virtual variations of all core game elements (board, dice, pieces) for a fully modifiable experience. Using the Microsoft HoloLens, players can interact with the virtual game elements and receive additional information about the current game state. The game also recognizes interactions with real game elements and translates them into the virtual world.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {57},
numpages = {5},
keywords = {augmented reality, mixed reality, board game, reality-virtuality trade-off, augmented virtuality},
location = {Pisa, Italy},
series = {MUM '19}
}

@inproceedings{10.1145/3365610.3368472,
author = {Wolf, Dennis and Besserer, Daniel and Sejunaite, Karolina and Schuler, Anja and Riepe, Matthias and Rukzio, Enrico},
title = {A Demonstration of CARe: An Augmented Reality Support System for Geriatric Inpatients with Mild Cognitive Impairment},
year = {2019},
isbn = {9781450376242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365610.3368472},
doi = {10.1145/3365610.3368472},
abstract = {Cognitive impairment such as memory loss, an impaired executive function and decreasing motivation can gradually undermine instrumental activities of daily living (IADL). With an older growing population, previous works have explored assistive technologies (ATs) to automate repetitive components of therapy and thereby increase patients' autonomy and reduce dependence on carers. While most ATs were built around screens and projection-based augmented reality (AR), the potential of head-mounted displays (HMDs) for therapeutic assistance is still under-explored. In this interactive demonstration, we present cARe, an HMD-based AR framework that uses in-situ instructions and a guidance mechanism to assist patients with manual tasks. In a case study with six geriatric patients, we investigated the prototype's feasibility during a cooking task in comparison to a regular paper-based recipe. Qualitative and quantitative results indicate that cARe has potential to offer assistance to older individuals with declining cognitive function in their day-to-day tasks and increase their independence in an enjoyable way.},
booktitle = {Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {58},
numpages = {4},
keywords = {in-situ, assistive technology, augmented reality, dementia, mixed reality, IADL},
location = {Pisa, Italy},
series = {MUM '19}
}

