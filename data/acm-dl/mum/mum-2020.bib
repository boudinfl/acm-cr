@inbook{10.1145/3428361.3428467,
author = {Vatavu, Radu-Daniel and Vanderdonckt, Jean},
title = {Design Space and Users’ Preferences for Smartglasses Graphical Menus: A Vignette Study},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428467},
abstract = { We address in this work visual menus for smartglasses by proposing a wide range of design options, such as shape, location, orientation, and presentation modalities, which we compile in a design space with eight dimensions. From this design space, we select a subset of fourteen 2-D menus, for which we collect users’ preferences during a vignette experiment with N=251 participants. We report numerical measures of absolute, relative, and aggregate preference for smartglasses menus, and employ a particular Thurstone’s pairwise comparison technique, the Bradley-Terry model, to evaluate menu designs. Our results highlight key variables influencing users’ preferences regarding the visual appearance of smartglasses menus, which we use to discuss opportunities for future work.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {1–12},
numpages = {12}
}

@inbook{10.1145/3428361.3428470,
author = {Clarke, Christopher and Ehrich, Peter and Gellersen, Hans},
title = {Motion Coupling of Earable Devices in Camera View},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428470},
abstract = { Earables, earphones augmented with inertial sensors and real-time data accessibility, provide the opportunity for private audio channels in public settings. One of the main challenges of achieving this goal is to correctly associate which device belongs to which user without prior information. In this paper, we explore how motion of an earable, as measured by the on-board accelerometer, can be correlated against detected faces from a webcam to accurately match which user is wearing the device. We conduct a data collection and explore which type of user movement can be accurately detected using this approach, and investigate how varying the speed of the movement affects detection rates. Our results show that the approach achieves greater detection results for faster movements, and that it can differentiate the same movement across different participants with a detection rate of 86%, increasing to 92% when differentiating a movement against others.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {13–17},
numpages = {5}
}

@inbook{10.1145/3428361.3428409,
author = {Schiewe, Alexander and Krekhov, Andrey and Kerber, Frederic and Daiber, Florian and Kr\"{u}ger, Jens},
title = {A Study on Real-Time Visualizations During Sports Activities on Smartwatches},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428409},
abstract = {Nowadays, many wearable devices such as smartwatches exist that can be used to track and analyze sports activities. Generally, these devices are equipped with high-resolution screens, but most applications provide only textual status information as real-time visual feedback during the respective activities. This limited amount of information is particularly the case for running, which is among the most frequently tracked sports activities with wearable devices. So far, only a few products and prototypes provide assistance and feedback related to running technique and efficiency, but also predominantly by means of textual data representations. This work investigates visualization approaches on the smartwatch for real-time feedback. We conducted two user studies in order to evaluate the feasibility and user acceptance of visualizations for running-technique training that assists the runner in implementing a forefoot running style. Despite frequent glances at the smartwatch, the results confirm that a runner’s performance is not impaired in comparison to traditional training. Further, the results indicate that runners benefit from the visualizations in various ways: They feel more motivated and supported, improve their self-assessment, and have the certainty that they perform the new technique correctly. Most participants also took a very positive view on the intuitiveness of the visualizations. },
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {18–31},
numpages = {14}
}

@inbook{10.1145/3428361.3428393,
author = {Le, Huy Viet and Mayer, Sven and Henze, Niels},
title = {Imprint-Based Input Techniques for Touch-Based Mobile Devices},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428393},
abstract = { Touchscreens translate touches of all kinds into 2D coordinates. This limits the input vocabulary and constrains effective interaction to touches by the fingertip. Previous tabletop research extended the input vocabulary with a myriad of promising input techniques using the shape of fingers and hands. However, these techniques are not applicable to mobile devices due to differences in size, ergonomics, and technology. We conducted ideation sessions (N=17) to explore novel input techniques and use cases for imprint-based touch sensing on mobile devices. As a case study, we present FlexionTouch, a novel input technique that recognizes the finger flexion on a touchscreen. Using the finger flexion as an additional input dimension, FlexionTouch provides an always-available shortcut and can be used for value inputs, document previews, and gestures. We propose five example use cases for FlexionTouch input which we evaluated in a second user study (N=20). While the low resolution of the capacitive images leads to a less accurate input compared to tabletops, participants still find the presented use cases helpful. As our input technique is purely software-based, it can be readily deployed to every mobile device with a capacitive touchscreen.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {32–41},
numpages = {10}
}

@inbook{10.1145/3428361.3428391,
author = {Stefanis, Vassilios and Komninos, Andreas and Garofalakis, John},
title = {Challenges in Mobile Text Entry Using Virtual Keyboards for Low-Vision Users},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428391},
abstract = { Mobile text entry for sighted and blind persons has received much research attention. However, much less is known about meeting the text entry needs of persons with low vision, whose ability to use the visual channel alongside audio, speech and haptic modalities, may open unexplored opportunities for efficient and privacy-preserving mobile text entry. We present findings from a qualitative study with 9 low vision users, revealing current text entry challenges for this user group, and providing future directions for text entry research.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {42–46},
numpages = {5}
}

@inbook{10.1145/3428361.3428466,
author = {Voit, Alexandra and Weber, Dominik and Abdelrahman, Yomna and Salm, Marie and Woundefinedniak, Pawe\l{}&nbsp;W. and Wolf, Katrin and Schneegass, Stefan and Henze, Niels},
title = {Exploring Non-Urgent Smart Home Notifications Using a Smart Plant System},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428466},
abstract = {With the rise of the Internet of Things, home appliances become connected and they can proactively provide status information to users. Facing a steadily increasing number of notification sources, it is unclear how information from smart home devices should be provided without overloading the users’ attention. In this paper, we investigate the design of non-urgent smart home notifications using a smart plant system. Based on feedback from focus groups, we designed four notification types and compared them in an eight-week in-situ study. We show that notifications displayed on smart home devices are preferred to those received on smartphones. Event-based notifications are unobtrusive, actionable and are preferred to persistent notifications. We derive guidelines that address the need of being in control, opportune locations for notification delivery at opportune moments, notification blindness, the importance of discretizing continuous information, and combining related notifications. },
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {47–58},
numpages = {12}
}

@inbook{10.1145/3428361.3428400,
author = {Papachristos, Eleftherios and Merritt, Timothy Robert and Jacobsen, Tobias and Bagger, Jimmi},
title = {Designing Ambient Multisensory Notification Devices: Managing Disruptions in the Home},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428400},
abstract = {An increasing number of technologies are battling for our attention constantly throughout the day. The majority of distractions come in the form of notifications from mobile instant messaging services, which under certain circumstances can make us feel overwhelmed and stressed. The goal of this paper is to add to the growing body of research that aims to find solutions and coping mechanisms for the problems associated with excessive notifications. Instead of focusing on software solutions on the mobile device itself, we explore the use of multisensory ambient information displays as notification delivery systems in the home context. Our goal was to use the attention periphery to create sufficient awareness of incoming messages without overwhelming the user. We conducted a three-part investigation that began with a laboratory study with 23 participants exploring initial impressions with notifications across a variety of sensory channels and modalities. Insights from this study were then used in a design workshop that helped us develop four different ambient notification prototypes. We present qualitative findings from a subsequent study in which seven participants used those devices for a period of four days in their homes. Finally, we compare the results of different notification modalities and provide implications for the design of ambient multisensory notifications in the home context. },
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {59–70},
numpages = {12}
}

@inbook{10.1145/3428361.3428469,
author = {Voit, Alexandra and Niess, Jasmin and Eckerth, Caroline and Ernst, Maike and Weing\"{a}rtner, Henrike and Woundefinedniak, Pawe\l{} W.},
title = {‘It’s Not a Romantic Relationship’: Stories of Adoption and Abandonment of Smart Speakers at Home},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428469},
abstract = { Smart speakers become increasingly ubiquitous in our homes. Consequently, we need to study how smart speakers affect the members of a household. Understanding the adoption of a smart speaker can assure it does not negatively influence the social dynamics within a household and create opportunities for further assistance. We deployed an Amazon Echo dot in nine households with 20 inhabitants who were new smart speaker users. We conducted multiple interviews, inquiring how a smart speaker was integrated into a household from day one. We investigated the development of social rules around using the device and how the smart speaker was appropriated. Users developed different strategies of using the device which altered social behaviours in some households. Further, we identified barriers and unmet requirements in introducing smart speakers to home environments. Our work contributes to an understanding of ubiquitous assistance for user groups at home.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {71–82},
numpages = {12}
}

@inbook{10.1145/3428361.3428464,
author = {Marky, Karola and Prange, Sarah and Krell, Florian and M\"{u}hlh\"{a}user, Max and Alt, Florian},
title = {“You Just Can’t Know about Everything”: Privacy Perceptions of Smart Home Visitors},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428464},
abstract = { IoT devices can harvest personal information of any person in their surroundings and this includes data from visitors. Visitors often cannot protect their privacy in a foreign smart environment. This might be rooted in a poor awareness of privacy violations by IoT devices, a lack of knowledge, or a lack of coping strategies. Thus, visitors are typically unaware of being tracked by IoT devices or lack means to influence which data is collected about them. We interviewed 21 young adults to investigate which knowledge visitors of smart environments need and wish to be able and protect their privacy. We found that visitors consider their relation to the IoT device owner and familiarity with the environment and IoT devices when making decisions about data sharing that affect their privacy. Overall, the visitors of smart environments demonstrated similar privacy preferences like the owners of IoT devices but lacked means to judge consequences of data collection and means to express their privacy preferences. Based on our results, we discuss prerequisites for enabling visitor privacy in smart environments, demonstrate gaps in existing solutions and provide several methods to improve the awareness of smart environment visitors.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {83–95},
numpages = {13}
}

@inbook{10.1145/3428361.3428382,
author = {Manabe, Minori and Uriu, Daisuke and Funatsu, Takeshi and Izumihara, Atsushi and Yazaki, Takeru and Chen, I-Hsin and Liao, Yi-Ya and Liu, Kang-Yi and Ko, Ju-Chun and Kashino, Zendai and Hiyama, Atsushi and Inami, Masahiko},
title = {Exploring in the City with Your Personal Guide:Design and User Study of T-Leap, a Telepresence System},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428382},
abstract = {This paper describes a field study conducted with our system, T-Leap, a telepresence system connecting one person (the Viewer), situated indoors, with multiple destinations (the Nodes), that roam outdoors. Here, each Node is a person wearing a module that includes a 360-degree camera and a microphone-speaker. Through our study, we demonstrate that T-Leap enables the Viewer to perform various interactions with the Nodes including being helped by them, collaborating with them, and guiding them. These interactions were demonstrated through three studies completing different tasks: 1) Nodes purchasing souvenirs for the Viewer, 2) Nodes finding objects in the park, and 3) Viewer guiding Nodes to purchase things. The studies were primarily conducted with Taiwanese locals and Japanese visitors in Taipei. Throughout the studies, we found that T-Leap worked especially well for mediating communication between a Viewer with local knowledge acting as a guide and several Nodes who were being guided. To conclude the paper, we broadly discuss our findings, the lessons we learned from our field study, and present recommendations for the future development of mobile and wearable telepresence systems. },
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {96–106},
numpages = {11}
}

@inbook{10.1145/3428361.3428396,
author = {Pozo, Ada and Phan, Thanh-Trung and Gatica-Perez, Daniel},
title = {Learning Urban Nightlife Routines from Mobile Data},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428396},
abstract = { The use of smartphone sensing for public health studies is appealing to understand routines. We present an approach to learn nightlife routines in a smartphone sensing dataset volunteered by 184 young people (1586 weekend nights with location data captured between 8PM and 4AM.) Human activity is represented at two levels, namely as the types of places visited and as the areas of the city where those places are. Routines extracted with two topic models (Latent Dirichlet Allocation and Hierarchical Dirichlet Process) are semantically meaningful and represent different moments of the weekend night, depicting activities such as pub crawling. The inference capacity of the routine representation is demonstrated with two classification tasks of value for alcohol research (alcohol consumption throughout the night, and heavy alcohol consumption.) The results suggest that nightlife routine mining could be used as a complementary tool to traditional survey-based methods in public health studies, and also inform other institutional actors interested in understanding and supporting youth well-being.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {107–118},
numpages = {12}
}

@inbook{10.1145/3428361.3428390,
author = {Riegler, Andreas and Riener, Andreas and Holzmann, Clemens},
title = {A Research Agenda for Mixed Reality in Automated Vehicles},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428390},
abstract = { With the increasing knowledge advancements and availability of mixed reality (MR), the number of its purposes and applications in vehicles rises. Mixed reality may help to increase road safety, support more immersive (non-) driving related activities, and finally enhance driving and passenger experience. MR may also be the enabling technology to increase trust and acceptance in automated vehicles and therefore help on the transition towards automated driving. Further, automated driving extends use cases of virtual reality and other immersive technologies. However, there are still a number of challenges with the use of mixed reality when applied in vehicles, and also several human factors issues need to be solved. This paper aims at presenting a research agenda for using mixed reality technology for automotive user interfaces (UIs) by identifying opportunities and challenges.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {119–131},
numpages = {13}
}

@inbook{10.1145/3428361.3428395,
author = {Dellana, Sarah Grace and Johansson, Sune and Poulsen, Simon Boel and van Oosterhout, Anke and Skov, Mikael and Merritt, Timothy},
title = {Collaboration Around an Interactive Tabletop Map: Comparing Voice Interactions and a Tangible Shape-Changing Controller},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428395},
abstract = {Tabletop interactions can support various social activities such as collaborative review of dynamic sales information in the business setting or route planning with an interactive map among others. Sharing control between users of interactive surfaces can be difficult to ensure participation and efficiency for co-located contexts. We explore the differences between two modalities for interacting with a tabletop map including interactions with a voice assistant and tangible interactions with a shape-changing controller. To simulate a shared goal-oriented situation, we use the context of shared autonomous mobility and compare the tangible controller with voice input through simulated tasks of route planning and detours. The findings suggest that the most suitable interaction modality depends on user preferences, scenarios, and contexts. Tangible controls work well for communicating the state of the system and facilitating discussion, while voice can be more pragmatic and task oriented. },
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {132–142},
numpages = {11}
}

@inbook{10.1145/3428361.3428405,
author = {Riegler, Andreas and Weigl, Klemens and Riener, Andreas and Holzmann, Clemens},
title = {StickyWSD: Investigating Content Positioning on a Windshield Display for Automated Driving},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428405},
abstract = { Windshield displays (WSDs) are a promising new technology to augment the entire windscreen with additional information about vehicle state, highlight critical objects in the surrounding, or use the screen as replacement for a conventional display. Typically, augmentation is provided in a screen-fixed manner as overlay on the windscreen. However, it is unclear to date if this is optimal in terms of usability/UX and further there is no golden standard suggesting where to place and how to manage content on such large displays in a vehicular environment. In this work, we propose ”StickyWSD” – a world-fixed positioning strategy – and evaluate its impact on quantitative and qualitative measures compared to screen-fixed positioning. Results from a user study conducted in a virtual reality driving simulator (N = 23) suggest that the dynamic world-fixed positioning technique shows increased task performance and lowered error rates as well as faster take-over times. Subjective evaluations show no clear preferences between both conditions. We propose to display text content on the WSD in world-fixed modality but further studies on context- and content-awareness are required.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {143–151},
numpages = {9}
}

@inbook{10.1145/3428361.3428403,
author = {Pimenta, Francisca and Lopes, La\'{\i}s and Gon\c{c}alves, Frederica and Campos, Pedro},
title = {Designing Positive Behavior Change Experiences: A Systematic Review and Sentiment Analysis Based on Online User Reviews of Fitness and Nutrition Mobile Applications},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428403},
abstract = { While mobile devices have become ubiquitous, illnesses derived from poor lifestyle habits are on the rise. However, our understanding of design mechanisms that induce healthier behavior change through mobile devices is still limited. Using the BCT Taxonomy, and online user reviews as an indicator of experience satisfaction, we make a three-folded contribution to designing interactive systems for behavior change: (i) a systematic review of applications for physical activity and healthier eating habits, coding BCTs; (ii) sentiment analysis performed on 20492 review sentences of these apps; and (iii) design implications regarding the implementation features for each BCT cluster, considering the highest-scored features in terms of sentiment analysis. Positive expressions referred to the framing/reframing technique. Contrarily, negative expressions were mostly related to reward and threat. Findings from this study can be used to benchmark interactions between users and behavior change interfaces, and provide design insights to support positive user experiences.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {152–161},
numpages = {10}
}

@inbook{10.1145/3428361.3428463,
author = {Meegahapola, Lakmal and Ruiz-Correa, Salvador and Gatica-Perez, Daniel},
title = {Alone or With Others? Understanding Eating Episodes of College Students with Mobile Sensing},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428463},
abstract = { Understanding food consumption patterns and contexts using mobile sensing is fundamental to build mobile health applications that require minimal user interaction to generate mobile food diaries. Many available mobile food diaries, both commercial and in research, heavily rely on self-reports, and this dependency limits the long term adoption of these apps by people. The social context of eating (alone, with friends, with family, with a partner, etc.) is an important self-reported feature that influences aspects such as food type, psychological state while eating, and the amount of food, according to prior research in nutrition and behavioral sciences. In this work, we use two datasets regarding the everyday eating behavior of college students in two countries, namely Switzerland (Nch=122) and Mexico (Nmx=84), to examine the relation between the social context of eating and passive sensing data from wearables and smartphones. Moreover, we design a classification task, namely inferring eating-alone vs. eating-with-others episodes using passive sensing data and time of eating, obtaining accuracies between 77% and 81%. We believe that this is a first step towards understanding more complex social contexts related to food consumption using mobile sensing. },
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {162–166},
numpages = {5}
}

@inbook{10.1145/3428361.3428362,
author = {Baldauf, Matthias and Fr\"{o}ehlich, Peter and Endl, Rainer},
title = {Trust Me, I’m a Doctor – User Perceptions of AI-Driven Apps for Mobile Health Diagnosis},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428362},
abstract = { First consumer-facing apps for medical self-diagnosis through Artificial Intelligence have hit the market only recently. These promise to detect malicious skin changes from photos or respiratory diseases from cough noises captured by the smartphone microphone, for example. While there is a large body of research on HCI-related aspects of mobile health applications, knowledge about the user perceptions of such novel AI-driven self-diagnosis apps and factors affecting their acceptance and adoption is scarce. In an online survey, we investigated the participants’ overall willingness-to-use (considering four types of captured and processed data) and identified trust factors and desirable features. We found that more than half of the participants would use AI-driven self-diagnosis apps, yet mainly integrated into prevailing general practitioner care. Based on the results, we draw conclusions which can guide the design, development, and launch of AI-driven self-diagnosis apps.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {167–178},
numpages = {12}
}

@inbook{10.1145/3428361.3428402,
author = {Gruenefeld, Uwe and Br\"{u}ck, Yvonne and Boll, Susanne},
title = {Behind the Scenes: Comparing X-Ray Visualization Techniques in Head-Mounted Optical See-through Augmented Reality},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428402},
abstract = { Locating objects in the environment can be a difficult task, especially when the objects are occluded. With Augmented Reality, we can alternate our perceived reality by augmenting it with visual cues or removing visual elements of reality, helping users to locate occluded objects. However, to our knowledge, it has not yet been evaluated which visualization technique works best for estimating the distance and size of occluded objects in optical see-through head-mounted Augmented Reality. To address this, we compare four different visualization techniques derived from previous work in a laboratory user study. Our results show that techniques utilizing additional aid (textual or with a grid) help users to estimate the distance to occluded objects more accurately. In contrast, a realistic rendering of the scene, such as a cutout in the wall, resulted in higher distance estimation errors.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {179–185},
numpages = {7}
}

@inbook{10.1145/3428361.3428384,
author = {Hassib, Mariam and Abdelmoteleb, Hatem and Khamis, Mohamed},
title = {Are My Apps Peeking? Comparing Nudging Mechanisms to Raise Awareness of Access to Mobile Front-Facing Camera},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428384},
abstract = {Mobile applications that are granted permission to access the device’s camera can access it at any time without necessarily showing the camera feed to the user or communicating that it is being used. This lack of transparency raises privacy concerns, which are exacerbated by the increased adoption of applications that leverage front-facing cameras. Through a focus group we identified three promising approaches for nudging the user that the camera is being accessed, namely: notification bar, frame, and camera preview. We experimented with accompanying each nudging method with vibrotactile and audio feedback. Results from a user study (N=15) show that while using frame nudges is the least annoying and interrupting, but was less understandable than the camera feed and notifications. On the other hand, participants found that indicating camera usage by showing its feed or by using notifications is easy to understand. We discuss how these nudges raise user awareness and the effects on app usage and perception. },
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {186–190},
numpages = {5}
}

@inbook{10.1145/3428361.3428380,
author = {Epp, Felix Anand and Hirskyj-Douglas, Ilyena and Karyda, Maria and McGookin, David and Lucero, Andr\'{e}s},
title = {Collocated Sharing of Presentations of Self in Public Settings},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428380},
abstract = { Various mobile technologies proofed to enhance peoples collocated social interactions. In particular, user-generated presentations of self have proven beneficial, albeit in specific social settings. This field study interviewed 30 participants for their attitudes towards personal sharing in six public settings in a Nordic metropolitan area. We asked participants to draw what they want to share on an attachable paper sticker. We observed retention towards sharing in places with a more heterogeneous audience. Predominantly people’s attitudes towards sharing depended on an individual’s current context. Our results highlight the symbolic act of sharing in public as a factor for placing personal public displays. Further, we suggest leveraging the different strategies of extroverts and introverts for collocated social interactions.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {191–200},
numpages = {10}
}

@inbook{10.1145/3428361.3428397,
author = {Kariryaa, Ankit and Sch\"{o}ning, Johannes},
title = {MoiPrivacy: Design and Evaluation of a Personal Password Meter},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428397},
abstract = { Passwords commonly contain personal information. However, there is limited awareness about its detrimental effect on the user’s online security. Current password meters do not take into account personal information and, therefore, their users are susceptible to targeted password guessing. In this paper, we present the MoiPrivacy password meter, that extends a neural network- and heuristic-based approach and considers a user’s personal information, while calculating the password strength and feedback. To do so, we analyzed the type of personal information used in passwords through an online survey (n = 62). We conducted a second user study (n = 49) for evaluating the MoiPrivacy browser extension. Our results show that MoiPrivacy significantly limits the inclusion of personal information in passwords.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {201–211},
numpages = {11}
}

@inproceedings{10.1145/3428361.3428468,
author = {Meegahapola, Lakmal and Ruiz-Correa, Salvador and Gatica-Perez, Daniel},
title = {Protecting Mobile Food Diaries from Getting Too Personal},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428468},
doi = {10.1145/3428361.3428468},
abstract = {Smartphone applications that use passive sensing to support human health and well-being primarily rely on: (a) generating low-dimensional representations from high-dimensional data streams; (b) making inferences regarding user behavior; and (c) using those inferences to benefit application users. Meanwhile, sometimes these datasets are shared with third parties as well. Human-centered ubiquitous systems need to ensure that sensitive attributes of users are protected when applications provide utility to people based on such behavioral inferences. In this paper, we demonstrate that inferences of sensitive attributes of users (gender, body mass index category) are possible using low-dimensional and sparse data coming from mobile food diaries (a combination of sensor data and self-reports). After exposing this potential risk, we demonstrate how deep learning techniques can be used for feature transformation to preserve sensitive user information while achieving high accuracies for application-related inferences (e.g. inferring the type of consumed food). Our work is based on two datasets of daily eating behavior of 160 young adults from Switzerland (NCH=122) and Mexico (NMX=38). Results show that using the proposed approach, accuracies in the order of 75%-90% can be achieved for application related inferences, while reducing the sensitive inference to almost random performance. },
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {212–222},
numpages = {11},
keywords = {mobile sensing, privacy, demographic attribute, food journal, smartphone sensing, food diary, sensitive attribute, eating behavior},
location = {Essen, Germany},
series = {MUM 2020}
}

@inbook{10.1145/3428361.3428465,
author = {Theil, Arthur and Buchweitz, Lea and Gay, James and Lindell, Eva and Guo, Li and Persson, Nils-Krister and Korn, Oliver},
title = {Tactile Board: A Multimodal Augmentative and Alternative Communication Device for Individuals with Deafblindness},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428465},
abstract = {Deafblindness, also known as dual sensory loss, is the combination of sight and hearing impairments of such extent that it becomes difficult for one sense to compensate for the other. Communication issues are a key concern for the Deafblind community. We present the design and technical implementation of the Tactile Board: a mobile Augmentative and Alternative Communication (AAC) device for individuals with deafblindness. The Tactile Board allows text and speech to be translated into vibrotactile signs that are displayed real-time to the user via a haptic wearable. Our aim is to facilitate communication for the deafblind community, creating opportunities for these individuals to initiate and engage in social interactions with other people without the direct need of an intervener.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {223–228},
numpages = {6}
}

@inbook{10.1145/3428361.3428407,
author = {Kaul, Oliver Beren and Rohs, Michael and Mogalle, Marc},
title = {Design and Evaluation of On-the-Head Spatial Tactile Patterns},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428407},
abstract = { We propose around-the-head spatial vibrotactile patterns for representing different kinds of notifications. The patterns are defined in terms of stimulus location, intensity profile, rhythm, and roughness modulation. A first study evaluates recall and distinguishability of 30 patterns, as well as agreement on meaning without a predetermined context: Agreement is low, yet the recognition rate is surprisingly high. We identify which kinds of patterns users recognize well and which ones they prefer. Static stimulus location patterns have a higher recognition rate than dynamic patterns, which move across the head as they play. Participants preferred dynamic patterns for comfort. A second study shows that participants are able to distinguish substantially more around-the-head spatial patterns than smartphone-based patterns. Spatial location has the highest positive impact on accuracy among the examined features, so this parameter allows for a large number of levels.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {229–239},
numpages = {11}
}

@inbook{10.1145/3428361.3428404,
author = {Nakao, Takuro and Kunze, Kai and Isogai, Megumi and Shimizu, Shinya and Pai, Yun Suen},
title = {FingerFlex: Shape Memory Alloy-Based Actuation on Fingers for Kinesthetic Haptic Feedback},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428404},
abstract = { The tactile and kinesthetic sensation of pushing a button is usually lost when interacting with modern devices like touchscreens and/or virtual reality platforms. We present FingerFlex, a standalone glove wearable actuating the metacarpophalangeal joint (MCP) of each finger via shape memory alloy (SMA). SMA actuation is subtle, silent, and light, making it ideal for actuation of the fingers which we use to simulate the sensation of pressing a button. For our first study, we evaluated the engineering performance of FingerFlex by altering the current and triggering different levels of stimuli to the user’s fingers. We show that users can perceive at least 3 levels of actuation with an accuracy of 73%. For our second study, we found FingerFlex to perform significantly better in terms of input error on a virtual numblock of a keyboard with no significant change in perceived workload.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {240–244},
numpages = {5}
}

@inbook{10.1145/3428361.3428388,
author = {Elbehery, Mostafa and Weidner, Florian and Broll, Wolfgang},
title = {Haptic Space: The Effect of a Rigid Hand Representation on Presence When Interacting with Passive Haptics Controls in VR},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428388},
abstract = { Many virtual reality (VR) applications rely on passive haptics where virtual objects have a real counterpart that provides tactile feedback. In addition to that, many VR applications do not provide accurate hand representations, especially when there is a high chance of occlusion as this makes vision-based tracking problematic. Hence, we investigated how a simple hand representation affects user experience and presence when interacting with passive haptic controls in a virtual environment. We report on a between-subject user study where N = 45 participants experienced one of three conditions (no hands at all, hands represented as a rigid 3D model, and hands represented as a rigid 3D model with a snapping mechanism). Our results indicate that a simple hand representation using a 3D model of hands paired with a snapping mechanism significantly increases presence and user experience. That indicates that this simple and low-cost technique is effective to improve the VE as a whole. This, in return, provides a chance for improvement for many VR applications with passive haptics.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {245–253},
numpages = {9}
}

@inbook{10.1145/3428361.3428389,
author = {Faltaous, Sarah and Neuwirth, Joshua and Gruenefeld, Uwe and Schneegass, Stefan},
title = {SaVR: Increasing Safety in Virtual Reality Environments via Electrical Muscle Stimulation},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428389},
abstract = { One of the main benefits of interactive Virtual Reality (VR) applications is that they provide a high sense of immersion. As a result, users lose their sense of real-world space which makes them vulnerable to collisions with real-world objects. In this work, we propose a novel approach to prevent such collisions using Electrical Muscle Stimulation (EMS). EMS actively prevents the movement that would result in a collision by actuating the antagonist muscle. We report on a user study comparing our approach to the commonly used feedback modalities: audio, visual, and vibro-tactile. Our results show that EMS is a promising modality for restraining user movement and, at the same time, rated best in terms of user experience.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {254–258},
numpages = {5}
}

@inbook{10.1145/3428361.3428386,
author = {Kiss, Francisco and Woundefinedniak, Pawe\l{} W. and Biener, Verena and Knierim, Pascal and Schmidt, Albrecht},
title = {VUM: Understanding Requirements for a Virtual Ubiquitous Microscope},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428386},
abstract = { Augmented Reality (AR) and wearable sensors offer new possibilities to expand our senses and change how we interact with the world. Sensory augmentation can be integrated into everyday activities, but controls remain a challenge for user experience. In this paper, we investigate how users can control a futuristic interface that enables in-situ magnification. We designed an interactive system to enable users to zoom in on objects up to a microscopic level and implemented a prototype using the Microsoft Hololens. In a user-study, we compared full-screen to windowed visualizations and four interaction techniques for zooming: a clicker, two types of gestures, and voice. Our results indicate that the clicker enabled users to zoom at the fastest rate and lowered cognitive load. We also found a preference for windowed views. With our work, we provide insights for future augmented vision systems.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {259–266},
numpages = {8}
}

@inbook{10.1145/3428361.3428394,
author = {Fanger, Yara and Pfeuffer, Ken and Helmbrecht, Udo and Alt, Florian},
title = {PIANX – A Platform for Piano Players to Alleviate Music Performance Anxiety Using Mixed Reality},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428394},
abstract = { We present PIANX, a platform to assist piano players in alleviating Music Performance Anxiety (MPA). Our work is motivated by the ability of Virtual Reality (VR) to create environments closely resembling the real world. For musicians, settings such as auditions or concerts are of particular interest, since they allow practicing in situations which evoke stress as a result of stage fright. Current approaches are limited: while they provide a virtual scene, realistic haptic feedback (i.e. playing on a real piano) and an authentic representation of their hands is missing. We close this gap with the design of a Mixed Reality platform, consisting of a MIDI (Musical Instrument Digital Interface) stage piano and an HTC Vive Pro VR headset. The platform offers (a) two approaches to finger tracking and visualization – a virtual representation based on LeapMotion hand tracking (baseline) and a real representation using see-through VR; in addition, it provides (b) three different settings in which users can practice (home, audition, concert hall) and (c) a mechanism for real time feedback. We created a series of videos demonstrating the system and collected feedback from 23 participants in an online study, assessing their views towards our platform. Results reveal key insights for the design of virtual MPA training platforms from a scientific and consumer perspective. },
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {267–276},
numpages = {10}
}

@inbook{10.1145/3428361.3428398,
author = {Illing, Jannike and Klinke, Philipp and Gr\"{u}nefeld, Uwe and Pfingsthorn, Max and Heuten, Wilko},
title = {Time is Money! Evaluating Augmented Reality Instructions for Time-Critical Assembly Tasks},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428398},
abstract = { Manual assembly tasks require workers to precisely assemble parts in 3D space. Often additional time pressure increases the complexity of these tasks even further (e.g., adhesive bonding processes). Therefore, we investigate how Augmented Reality (AR) can improve workers’ performance in time and spatial dependent process steps. In a user study, we compare three conditions: instructions presented on (a) paper, (b) a camera-based see-through tablet, and (c) a head-mounted AR device. For instructions we used selected work steps from a standardized adhesive bonding process as a representative for common time-critical assembly tasks. We found that instructions in AR can improve the performance and understanding of time and spatial factors. The tablet instruction condition showed the best subjective results among the participants, which can increase motivation, particularly among less-experienced workers.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {277–287},
numpages = {11}
}

@inbook{10.1145/3428361.3428406,
author = {Kurzweg, Marco and Reinhardt, Jens and Stoll, Moritz and Wirth, Julia and Wolf, Katrin},
title = {PLAY ME! Influencing Game Decisions through Suggestions Made by Augmented Characters},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428406},
abstract = {During physical games, we love to socially interact with other players through bluffing or giving them hits. This work aims to enrich AR characters by adding a suggestive behavior to them intended to playfully influence game decisions. In a user study, we evaluated such behaviors presented as body postures by animated card characters using an AR trading card game. Our results indicate that AR characters can indeed influence the player’s game decisions through postures that encourage or discourage to play a certain card. Our approach enriches the game design space, can make the game more interesting, and finally adds a social component to the game. },
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {288–298},
numpages = {11}
}

@inbook{10.1145/3428361.3428379,
author = {Knierim, Pascal and Kiss, Francisco and Rauh, Maximilian and Schmidt, Albrecht},
title = {Tangibility is Overrated: Comparing Learning Experiences of Physical Setups and Their Virtual Equivalent in Augmented Reality},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428379},
abstract = { Augmented Reality (AR) is gaining increasing importance in science, education, and entertainment. A fundamental characteristic of AR is blending the virtual and physical world into a coherent environment. In this paper, we examine the effect of substituting the physical components of lab experiments with tangible replicas and virtual representations. We conducted a user study with thirty participants who carried out the experiment in three different abstraction levels (original lab equipment, non-functional tangible props, virtual representation). We compared the users’ performance regarding setup time, experienced workload, quality of measurements, and concept comprehension of the learning task. We found no effect on comprehension but significant differences in setup time and quality of measures. The results indicate that substitution reduces the experiment setup duration without affecting knowledge transfer. These results help to shape future AR learning environments, and we offer insights for creating complex mixed reality learning materials. },
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {299–305},
numpages = {7}
}

@inbook{10.1145/3428361.3432081,
author = {Faltaous, Sarah and Schneegass, Stefan},
title = {HCI Model: A Proposed Extension to Human-Actuation Technologies},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3432081},
abstract = { Human computer interaction models deliver a conceptual overview of the fundamental communication processes between the computer and the human. The aim of these models is to provide an easy understanding of a system’s behaviour on a meta level. Existing HCI models cover most of the actual interaction modalities. New arising technologies provide the possibility of directly manipulating the human action – called actuation. Such interaction, however, is not covered by classical models. In this work, we propose additional relations extending current HCI models to include novel interactions using human actuation.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {306–308},
numpages = {3}
}

@inbook{10.1145/3428361.3432074,
author = {Colley, Ashley and Suoheimo, Mari and H\"{a}kkil\"{a}, Jonna},
title = {Exploring VR and AR Tools for Service Design},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3432074},
abstract = { We explore the possibilities of augmented and virtual reality (AR and VR) as co-design tools for service design. Four expert service designers, working in industry, tried out and discussed different VR and AR based tools: Head mounted display (HMD) based VR and AR, and projected environments. The findings emphasize that HMD techniques are not favored, as they isolate the wearer from the co-design situation and hinder the observation of emotions and expressions. Projection of the design context was positively commented, as supporting collaboration in co-design sessions.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {309–311},
numpages = {3}
}

@inbook{10.1145/3428361.3432077,
author = {Thomas, Derianna and Holmquist, Lars Erik},
title = {WristAR: A Wrist-Mounted Augmented Reality Interface},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3432077},
abstract = { The use of both smartwatches and augmented reality (AR) has steadily grown in the past decade, however there are few examples of combining these technologies to enhance user experiences. We introduce WristAR, a wrist-worn prototype that supports the use of hands-free AR, removing the need to wear a head-mounted display or hold a smartphone, aiming to give a more natural way to experience AR. The prototype is equipped with a camera and screen, making it possible to run AR applications in a form-factor that emulates a smartwatch. In order to test the concept we built an AR application which allows users to play a simple card game displaying animated AR characters. In a user study with 12 participants, we found that although the device was unfamiliar and has restricted movement, participants suggested that with improvements it could become a viable alternative to current AR interfaces.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {312–314},
numpages = {3}
}

@inbook{10.1145/3428361.3432078,
author = {Tamanini, Jill-Valerie and Elberzhager, Frank},
title = {MyMiniLautern: A Game for Experiencing the Opportunities of Mobility Change},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3432078},
abstract = {One aspect of smart cities is the change in mobility that is urgently needed due to environmental pollution and global warming. The aim is to find ways in which the digital transformation can support smart cities in this endeavor. In this context, an app game was developed that presents new mobility concepts and enables players to decide which mobility measures should be activated in a virtual environment reflecting a concrete smart city district. In the game, a citizens' council consisting of a representative group of citizens reacts to the selected mobility measures and shows how satisfied they are with the players’ decisions. Also, social implications of such (sometimes) difficult decisions are shown. The players realize that there is no easy way of change, but also that many new mobility concepts already exist that can support cities in becoming greener.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {315–316},
numpages = {2}
}

@inbook{10.1145/3428361.3432076,
author = {Colley, Ashley and Wolf, Dennis and Kammerer, Klaus and Rukzio, Enrico and H\"{a}kkil\"{a}, Jonna},
title = {Exploring the Performance of Graphically Designed AR Markers},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3432076},
abstract = {The design of graphical augmented reality (AR) markers requires compromise between the aesthetic appearance and tracking reliability. To investigate the topic, we created a virtual reality (VR) pipeline to evaluate marker performance, and validated it against real-world performance for a set of graphical AR markers. We report that, with the well known Vuforia framework and typical smartphone hardware, well designed 20 \texttimes{} 20 cm markers can be tracked at distances of up to 68 cm. We note that the number of feature points is particularly important to a marker’s angular performance. },
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {317–319},
numpages = {3}
}

@inbook{10.1145/3428361.3432071,
author = {Schrapel, Maximilian and Liebers, Jonathan and Rohs, Michael and Schneegass, Stefan},
title = {Skiables: Towards a Wearable System Mounted on a Ski Boot for Measuring Slope Conditions},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3432071},
abstract = { Winter sports like skiing are becoming increasingly popular for both competitive and recreational activities. To minimize the risk of injury, new innovations in skiing equipment have been developed in recent years. However, unexpected slope conditions can still increase risks during skiing. The static categorisation of ski slopes in winter sports resorts does not take into account dynamic changes of difficulty due to high traffic volumes or sudden weather changes. Up to now, efforts have been made to measure the current conditions via satellite imaging or installations on the slope. However, this requires intervention in nature and causes high maintenance costs. To solve these issues we present our preliminary design of a wearable system to let skiers implicitly measure current slope conditions during their skiing experience. Audio and motion data are recorded from a prototype mounted on a ski boot. We show that the data generated by the prototype can be successfully classified with a neural network. We collected data from a skiing activity to demonstrate our concept and discuss the identified challenges in fitting the proposed approach to winter sports equipment.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {320–322},
numpages = {3}
}

@inbook{10.1145/3428361.3432088,
author = {Pampar\u{a}u, Cristian and Vatavu, Radu-Daniel},
title = {A Research Agenda Is Needed for Designing for the User Experience of Augmented and Mixed Reality: A Position Paper},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3432088},
abstract = { With this position paper, we want to draw the attention of the community toward theoretical work and practical opportunities that have been overlooked so far regarding concepts, principles, and design knowledge for the user experience of Augmented and Mixed Reality content, I/O devices, interactions, applications, and systems. Despite considerable innovations in commercial products and research prototypes enabling Augmented and Mixed Reality worlds, how to design great user experiences in such worlds has been overall neglected at core, while the information and knowledge currently available to practitioners cover usability aspects mostly. Therefore, we advocate for theoretical foundations of the user experience in Augmented and Mixed Reality and propose several directions for more scientific research in this regard.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {323–325},
numpages = {3}
}

@inbook{10.1145/3428361.3432089,
author = {Pampar\u{a}u, Cristian and Aiordachioae, Adrian and Vatavu, Radu-Daniel},
title = {From Do You See What I See? To Do You Control What I See? Mediated Vision, From a Distance, for Eyewear Users},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3432089},
abstract = { We discuss engineering aspects for shifting from “do you see what I see?” applications that stream the user’s field of view to remote viewers toward “do you control what I see?” features in which remote viewers are given the opportunity and tool to control the primary user’s field of view. To this end, we present two applications for (1) smartglasses with embedded video camera for live video streaming and (2) the HoloLens HMD that presents users with mediated versions of the visual world controlled by remote viewers.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {326–328},
numpages = {3}
}

@inbook{10.1145/3428361.3432080,
author = {Aiord\u{a}chioae, Adrian and Gherasim, David and Maciuc, Alexandru-Ilie and Gheran, Bogdan-Florin and Vatavu, Radu-Daniel},
title = {Addressing Inattentional Blindness with Smart Eyewear and Vibrotactile Feedback on the Finger, Wrist, and Forearm},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3432080},
abstract = { We present a concept, hardware prototype, and wearable system designed to deliver vibrotactile feedback at finger, wrist, and forearm level for everyday situations of inattentional blindness. Objects and phenomena automatically detected from the video stream acquired by a pair of glasses with embedded video camera are matched against a list of concepts of interest and converted into vibrotactile patterns delivered to the user on their finger, wrist, or forearm according to the location of the wearable device. Our demonstrations employ an off-the-shelf smart ring (Ring Zero) and armband (Myo) and a custom bracelet that we designed for this work.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {329–331},
numpages = {3}
}

@inbook{10.1145/3428361.3432079,
author = {Bongard, Jessica and Baldauf, Matthias and Fr\"{O}ehlich, Peter},
title = {Grasping Everyday Automation – A Design Space for Ubiquitous Automated Systems},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3432079},
abstract = { No longer only experts are confronted with (semi-)automated systems, yet automation has found its way into our everyday lifes in various forms and applications. Based on a thorough literature review, we introduce a design space for “everyday automation” to uncover eight core dimensions of respective systems. These dimensions include the domain, the task type, the type of user interaction, and the automation level, among others. Visualized as a “morphological box”, this design space is particularly supposed to support the ideation of novel automated systems for everyday life.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {332–334},
numpages = {3}
}

@inbook{10.1145/3428361.3432075,
author = {Yang, Bo and Wu, Jianming and Hattori, Gen},
title = {Facial Expression Recognition with the Advent of Face Masks},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3432075},
abstract = { With the worldwide spread of COVID-19, wearing face masks while interaction in public is becoming a common behavior to protect against infection. Thus, how to improve effectiveness of existing facial expression recognition (FER) technology on masked faces has become an urgent issue. However, there are no publicly available masked facial expression recognition datasets that take facial orientation into consideration. To address this issue, we propose a method that can add face masks to existing FER datasets automatically using differently shaped masks according to facial orientations. The FER models based on VGG19 and MobileNet are trained on public and private FER datasets added with mask. As part of our contribution, we collected real-world masked faces from the Internet using emotional keywords and constructed a masked FER test dataset for a fair performance evaluation. The experimental results show that training an FER model based on a simulated masked FER dataset is feasible. },
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {335–337},
numpages = {3}
}

@inbook{10.1145/3428361.3431192,
author = {Gen\c{c}, \c{C}a\u{g}lar and Kantola, Veera and H\"{a}kkil\"{a}, Jonna},
title = {DecoLive Jacket with Battery-Free Dynamic Graphics},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3431192},
abstract = { We present the design and prototype of the DecoLive Jacket, a dynamic clothing that allows its wearer to switch between opened and closed states of flower patterns as subtle indication of availability for social interactions. The Jacket embodies two electrochromic (EC) displays and NFC-based wireless energy harvesting components, enabling battery-free operation. The wearer controls the garment’s patterning by placing a smartphone in one of the two pockets of the jacket, activating either the open or closed flower visualization, and expressing a subtle message about their willingness to interact with others. Our implementation demonstrates the potential for battery-free EC displays on aesthetic wearables to enable dynamic control of the non-verbal messages communicated by clothing.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {338–340},
numpages = {3}
}

@inbook{10.1145/3428361.3431193,
author = {Wei\ss{}, Sebastian and Kowalski, Christian and Gliesche, Pascal and Heuten, Wilko},
title = {Virtual Laboratories for Educating The Public On State-Of-The-Art Health-Care Technology},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3431193},
abstract = { The fast pace of market development of medical devices combined with a slow adoption rate in health care often results in unawareness about technology that can aid the nursing process. Often, knowledge about the availability of such helpers only arises when the employer implements such a system. In that case, employees are ill-prepared and overwhelmed by the sudden change in their routine or are surprised that the device in question has not been adopted earlier. To alleviate that situation, research institutes maintain real labs that can be toured by the public, getting an early look on technology or learn about products that are readily available on the market. Using virtual reality, we go one step further to improve the reach and further decrease the cost of maintaining such labs in order to educate nursing professionals on state-of-the-art products.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {341–342},
numpages = {2}
}

@inbook{10.1145/3428361.3431194,
author = {M\"{u}ller, Lisa-Maria and Mandon, Kilian and Gliesche, Pascal and Wei\ss{}, Sebastian and Heuten, Wilko},
title = {Visualization of Eye Tracking Data in Unity3D},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3431194},
abstract = { For nurses, analyzing a patient’s room upon entry is crucial because health history and current status are essential for decision making in case of emergency. In order to understand a nurses perception of the situation and the environment, researchers have turned to Virtual Reality (VR). Eye tracking can be used to understand how users perceive virtual environments and can contribute to a deeper understanding of users’ cognitive processes. In this paper we present a demo-setup for recording, processing and visualizing eye tracking data in Unity3D. It enables tracking a user’s gaze data and its visualization in the virtual environment it was recorded in. The goal of this demo is to provide an easy-to-use tool for recording and visualizing eye tracking data using the HTC Vive Pro Eye and Unity3D. In future work, results can be used for training semi- and full-autonomous systems which e.g. report the patients current status. },
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {343–344},
numpages = {2}
}

@inbook{10.1145/3428361.3431195,
author = {Li, Hong and Kalving, Matilda and Khan, Awais Hameed and H\"{a}kkil\"{a}, Jonna},
title = {Designing Unconventional Communication Systems for Long-Distance Relationships Using the Flexi Card Game: A Card-Based Design Toolkit},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3431195},
abstract = { In this demo paper, we present the Flexi Card Game – a card-based generative design toolkit to support designers and non-designers with different backgrounds in participatory structures in the development of unconventional communication systems, with a particular focus on supporting couples in long-distance relationships.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {345–347},
numpages = {3}
}

@inbook{10.1145/3428361.3431196,
author = {Knierim, Pascal and Schmidt, Albrecht and Kosch, Thomas},
title = {Demonstrating Thermal Flux: Using Mixed Reality to Extend Human Sight by Thermal Vision},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3431196},
abstract = { Human vision underlies several constraints in its spatial, temporal and spectral resolution. One limitation is the visual sensation of temperatures, which can be a useful addition for human sight in education or work scenarios. This demo presents how mixed reality applications can virtually superimpose objects’ thermal properties with the physical world into a coherent environment. We present a prototype that utilizes either a smartphone or a head-mounted display to mediate thermal flux. We show the feasibility of our prototype in the context of a physics lab experiment that fosters the understanding of heat conduction for different materials. We envision thermal vision as an addition to the human sight that can be accomplished with already available technologies. Finally, we discuss how our prototype can be designed as a context-aware in-situ interface that toggles between regular and thermal vision.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {348–350},
numpages = {3}
}

@inbook{10.1145/3428361.3431197,
author = {Raudanjoki, \"{O}zge and Gen\c{c}, \c{C}a\u{g}lar and Hurtig, Kuisma and H\"{a}kkil\"{a}, Jonna},
title = {ShadowSparrow: An Ambient Display for Information Visualization and Notification},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3431197},
abstract = { We present the design and prototype of ShadowSparrow, an ambient display utilizing lights &amp; shadows. The concept uses illustrative bird images and other relevant visuals (e.g., dishwasher and e-mail icons) cast on the surroundings for ambient information visualizations and notifications. Our work creates an alternative channel to convey non-critical information in an unobtrusive and aesthetic way.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {351–353},
numpages = {3}
}

