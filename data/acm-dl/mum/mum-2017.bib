@inproceedings{10.1145/3152832.3152833,
author = {Brock, Anke M. and Hecht, Brent and Signer, Beat and Sch\"{o}ning, Johannes},
title = {Bespoke Map Customization Behavior and Its Implications for the Design of Multimedia Cartographic Tools},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152833},
doi = {10.1145/3152832.3152833},
abstract = {While popular digital maps support an unprecedented number of use cases, new reference map customization tools have been created for purposes for which those maps fall short. With the goal of informing the design of this new class of cartographic tools, we present the first study of naturalistic ("bespoke") map customization behavior. Through a mixed methods and mixed-media approach involving a survey, the analysis of a corpus of customized maps, and an interview with a power user, we find that bespoke map customization is a relatively common activity and identify frequent use cases as well as map customization strategies. We discuss these use cases and strategies in detail, and propose design implications for future customization tools, such as the use of templates for common use cases, adaptability for various customization styles and the support of multimedia interaction.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {1–11},
numpages = {11},
keywords = {multimedia, paper maps, design implications, cartography, digital maps, geographic HCI (GeoHCI)},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152842,
author = {Baldauf, Matthias and Brandner, Alex and Wimmer, Christoph},
title = {Mobile and Gamified Blended Learning for Language Teaching: Studying Requirements and Acceptance by Students, Parents and Teachers in the Wild},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152842},
doi = {10.1145/3152832.3152842},
abstract = {While mobile gamified apps for language learning such as Duolingo are highly successful, knowledge about the use and suitability of such concepts for blended learning in schools is scarce. Pursuing a holistic approach, this work investigates the general requirements and acceptance of a modern gamified learning companion for traditional class-based teaching. We conducted several surveys with students, teachers and parents and studied the usage of a custom research prototype by 13/14-year-olds in three classes in English (as a foreign) language under real-world conditions. According to our results, all considered involved parties are positive about such a mobile learning companion when specific requirements are met. We did not observe any community-related negative impact such as teasing for bad results, for example, and conclude that the considered gamification features such as challenges with classmates and rankings are suitable for the investigated school context. Based on the results, we derive a set of guidelines for the design and development of a mobile learning companion, its introduction in class and its practical usage.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {13–24},
numpages = {12},
keywords = {mobile learning, gamification, field study, blended learning},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152847,
author = {Voit, Alexandra and Weber, Dominik and Stowell, Elizabeth and Henze, Niels},
title = {Caloo: An Ambient Pervasive Smart Calendar to Support Aging in Place},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152847},
doi = {10.1145/3152832.3152847},
abstract = {Many countries are confronted with aging societies. With the increasing need for elder care, it is necessary to investigate how technology can support aging in place. In this paper, we propose an ambient smart calendar system that supports the self-sufficiency and activeness of older adults. We report the results of a survey of older adults on their use of physical and digital calendars. Based on the results, we developed Caloo (Calendar of opportunities) - a prototypical smart wall calendar which supports older users by generating awareness about their daily schedules as well as supporting them in remaining active in their lives through event suggestions. We evaluated Caloo in a study with retired older adults. Overall, we received positive feedback and identified data sources for events, which the smart calendar can suggest to the user. Our results indicate the potential of deploying an ambient and pervasive smart calendar system that supports aging in place.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {25–30},
numpages = {6},
keywords = {smart calendar, ambient information system, aging in place},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152851,
author = {Khamis, Mohamed and Bandelow, Linda and Schick, Stina and Casadevall, Dario and Bulling, Andreas and Alt, Florian},
title = {They Are All after You: Investigating the Viability of a Threat Model That Involves Multiple Shoulder Surfers},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152851},
doi = {10.1145/3152832.3152851},
abstract = {Many of the authentication schemes for mobile devices that were proposed lately complicate shoulder surfing by splitting the attacker's attention into two or more entities. For example, multimodal authentication schemes such as GazeTouchPIN and GazeTouchPass require attackers to observe the user's gaze input and the touch input performed on the phone's screen. These schemes have always been evaluated against single observers, while multiple observers could potentially attack these schemes with greater ease, since each of them can focus exclusively on one part of the password. In this work, we study the effectiveness of a novel threat model against authentication schemes that split the attacker's attention. As a case study, we report on a security evaluation of two state of the art authentication schemes in the case of a team of two observers. Our results show that although multiple observers perform better against these schemes than single observers, multimodal schemes are significantly more secure against multiple observers compared to schemes that employ a single modality. We discuss how this threat model impacts the design of authentication schemes.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {31–35},
numpages = {5},
keywords = {threat model, gaze gestures, multimodal authentication, multiple observers, privacy, shoulder surfing},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152835,
author = {Kukka, Hannu and Ylipulli, Johanna and Goncalves, Jorge and Ojala, Timo and Kukka, Matias and Syrj\"{a}l\"{a}, Mirja},
title = {Creator-Centric Study of Digital Art Exhibitions on Interactive Public Displays},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152835},
doi = {10.1145/3152832.3152835},
abstract = {We present a mixed-methods study aimed at assessing artists' experiences of a digital art exhibition service, called StreetGallery, on a network of interactive displays situated in public urban locations. We ground our analysis using survey responses and in-depth interviews of artists who have exhibited their art in StreetGallery over the years. Findings from these studies indicate that the artists highly value StreetGallery's open and egalitarian access to art, and its contribution towards fusing novel digital technologies and art in public urban spaces. We conclude that platforms such as StreetGallery have the potential to challenge traditional paradigms of art gallery practices and public urban spaces as a stage for consumption and commerce.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {37–48},
numpages = {12},
keywords = {longitudinal deployment, urban computing, interactive public displays, qualitative research, in-the-wild, digital art},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152841,
author = {Prange, Sarah and M\"{u}ller, Victoria and Buschek, Daniel and Alt, Florian},
title = {Quakequiz: A Case Study on Deploying a Playful Display Application in a Museum Context},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152841},
doi = {10.1145/3152832.3152841},
abstract = {In this paper, we present a case study in which we designed and implemented an interactive museum exhibit. In particular, we extended a section of the museum with an interactive quiz game. The project is an example of an opportunistic deployment where the needs of different stakeholders (museum administration, visitors, researchers) and the properties of the space needed to be considered. It is also an example of how we can apply knowledge on methodology and audience behavior collected over the past years by the research community. At the focus of this paper is (1) the design and concept phase that led to the initial idea for the exhibit, (2) the implementation phase, (3) a roll-out and early insights phase where we tested and refined the application in an iterative design process on-site, and (4) the final deployment as a permanent exhibit of the museum. We hope our report to be useful for researchers and practitioners designing systems for similar contexts.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {49–56},
numpages = {8},
keywords = {case study, deployment, museum},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152843,
author = {Ueno, Keiichi and Go, Kentaro and Kinoshita, Yuichiro},
title = {Investigation of Smartwatch Touch Behavior with Different Postures},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152843},
doi = {10.1145/3152832.3152843},
abstract = {This paper investigates smartwatch touch behaviors of users with different hand postures by conducting a laboratory experiment with 12 participants and analyzing their tap events on a smartwatch with four postures: the no-support (NS) posture, thumb support (TS) posture, index-finger-support (IS) posture, and thumb-and-middle-finger-support (TMS) posture. The participants' movement time and the error rate of the tapping tasks were measured for each posture and for two target positions (edge and center) and three target sizes (large, medium, and small). The results indicate much faster response times for the small, center targets for the TS and TMS postures compared with the NS posture. Based on this result, this paper proposes five design guidelines for arranging touch targets on a smartwatch touchscreen based on four postures.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {57–61},
numpages = {5},
keywords = {touch interaction, touch distribution, hand posture, smartwatches},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152844,
author = {Mohammad, Yasser and Matsumoto, Kazunori and Hoashi, Keiichiro},
title = {A Dataset for Activity Recognition in an Unmodified Kitchen Using Smart-Watch Accelerometers},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152844},
doi = {10.1145/3152832.3152844},
abstract = {Activity recognition from smart devices and wearable sensors is an active area of research due to the widespread adoption of smart devices and the benefits it provide for supporting people in their daily lives. Many of the available datasets for fine-grained primitive activity recognition focus on locomotion or sports activities with less emphasis on real-world day-to-day behavior. This paper presents a new dataset for activity recognition in a realistic unmodified kitchen environment. Data was collected using only smart-watches from 10 lay participants while they prepared food in an unmodified rented kitchen. The paper also providing baseline performance measures for different classifiers on this dataset. Moreover, a deep feature learning system and more traditional statistical features based approaches are compared. This analysis shows that - for all evaluation criteria - data-driven feature learning allows the classifier to achieve best performance compared with hand-crafted features.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {63–68},
numpages = {6},
keywords = {activity recognition, deep learning, mobile activity recognition},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152858,
author = {Barsotti, Marco and Patern\`{o}, Fabio and Pulina, Francesca},
title = {A Web Framework for Cross-Device Gestures between Personal Devices and Public Displays},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152858},
doi = {10.1145/3152832.3152858},
abstract = {In order to exploit the wide availability of public displays and personal devices on the mass market at affordable prices it is important to provide developers with frameworks that ease obtaining cross-device user interfaces able to exploit such device ecosystems. We present the design and implementation of a Web framework for the development of cross-device user interfaces able to take advantage of both personal devices and public displays, and support various types of gestures and their combinations in such multi-device environments. We introduce the design space addressed, describe the framework functionality, its application interface and run-time support, show some example applications, and report on a first test with developers.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {69–78},
numpages = {10},
keywords = {personal devices, cross-device interaction, public displays, framework, gestures, web applications},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152838,
author = {Le, Khanh-Duy and Zhu, Kening and Fjeld, Morten},
title = {Mirrortablet: Exploring a Low-Cost Mobile System for Capturing Unmediated Hand Gestures in Remote Collaboration},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152838},
doi = {10.1145/3152832.3152838},
abstract = {Direct and natural images of hand gestures have been shown to benefit remote collaboration on physical tasks in several settings, including ad-hoc ones. However, to capture such unmediated hand gestures within collaborative tasks, existing approaches require stationary hardware systems or heavily instrumented mobile devices, making them unfeasible for use at ad-hoc workplaces. We present MirrorTablet, a low-cost mirror-based system taking advantage of the built-in front-facing camera of a tablet to capture the user's unmediated hand interactions on and above the screen. This system requires minimal instrumentation of the tablet and can be easily (un-)mounted, making it suitable for mobile usage. A user study with ten pairs of participants on a helper-worker setup working on construction tasks showed that MirrorTablet improved task completion time and had positive effects on participants' perceived workload when working with unfamiliar tasks compared to using a common sketch-only interface. In addition, qualitative feedback yielded design considerations on hand visualization for mobile device remote collaboration on physical tasks.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {79–89},
numpages = {11},
keywords = {mobile devices, remote collaboration, hand interactions, hand gestures, physical tasks},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152853,
author = {Kotsev, Vasil and Nikolev, Atanas and Pawlak, Kasper and L\"{o}chtefeld, Markus},
title = {Investigating the Usage of Thermal Feedback as an Active Game Element},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152853},
doi = {10.1145/3152832.3152853},
abstract = {While a variety of modalities and techniques have been explored to create feedback in gaming, thermal stimuli so far have been mostly neglected, even though they are an substantial part of the experience when interacting with real world objects. In this paper we investigate the use of thermal stimuli as a replacement for currently common feedback mechanisms such as visual or vibro-tactile for four different game mechanics. Instead of developing new game mechanics we opted to use already well established ones, namely Quick Time Events, Power Bars, Cooldowns and Area of Effect. Our evaluation with 13 participants indicate that for Quick Time Events and Area of Effect, thermal feedback is a valid replacement that could potentially pave the way for novel modalities in gaming.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {91–95},
numpages = {5},
keywords = {game elements, game mechanics, thermal feedback},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152863,
author = {Pescara, Erik and Wolpert, Alexander and Budde, Matthias and Schankin, Andrea and Beigl, Michael},
title = {Lifetact: Utilizing Smartwatches as Tactile Heartbeat Displays in Video Games},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152863},
doi = {10.1145/3152832.3152863},
abstract = {Wearables like smartwatches or fitness trackers are increasingly entering everyday life. This offers new and unexplored opportunities to use their unique features - like the ability to act as vibrotactile actuators - as interfaces in computer games to increase game immersion. Current tactile cues in games are mostly restricted to event based-feedback and practically exclusive to console gaming. This paper presents LifeTact, a system which informs players about remaining hit points through a tactile heartbeat. The Tactile heartbeat is emitted via existing wearables like smartwatches or fitness trackers. The system was implemented and evaluated in a between-subject study with gamers, who rated the system as innovative, beneficial to the user experience and immersive.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {97–101},
numpages = {5},
keywords = {haptic interaction, tactile display, immersive technology, wrist-worn, vibrotactile patterns},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3157381,
author = {Tobita, Hiroaki},
title = {Finger-Navi: Mobile Navigation Integrated Smartphone with Physical Finger},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3157381},
doi = {10.1145/3152832.3157381},
abstract = {We introduce our Finger-Navi that integrates the smartphone with a physical finger. Smartphones are widely used for communication and entertainment, and have characteristic features such an even surface and a few buttons. However, the interaction with them is quite simple and limited. In contrast, we have found a way to use a physical finger attached to a smartphone. A real finger has many capabilities such as pointing and gestures. For example, we use our finger to point at something or someone, or to count a number, so we use such features for interactions to mobile navigation. The finger approach makes a smartphone more intuitive and familiar for novice and elderly users who are not good at manipulating smartphones. In this paper, we describe our design concepts, prototype implementation and application possibilities.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {103–106},
numpages = {4},
keywords = {3D printing, physical metaphor, direct navigation, mobile computing, computer-mediated communication, finger motion},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152870,
author = {Colley, Ashley and Inget, Virve and Rantala, Inka and H\"{a}kkil\"{a}, Jonna},
title = {Investigating Interaction with a Ring Form Factor},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152870},
doi = {10.1145/3152832.3152870},
abstract = {In this note, we study interaction with a finger worn ring, focusing on interaction enabled by the ring form factor. We report on 2 user studies, the first (n=13) investigating preferences for different interactions, whilst the second (n=7) explores usage contexts and applications. Twelve different ways of interacting with a ring were evaluated, including e.g. changing the placement of the ring on the fingers and moving the ring along or around a finger. Based the study results, the practical usability and concerns with each of the ring interactions is discussed. Whereas the concept was generally well received, the main concerns related to false positives, losing the ring e.g. when changing finger, and limitations from the rigid size of the form factor.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {107–111},
numpages = {5},
keywords = {wearables, ring, user study, mobile interaction, tangible interaction, form factors},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152850,
author = {H\"{a}kkil\"{a}, Jonna and Colley, Ashley and Roinesalo, Paula and V\"{a}yrynen, Jani},
title = {Clothing Integrated Augmented Reality Markers},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152850},
doi = {10.1145/3152832.3152850},
abstract = {The future vision of commonplace, everyday wear of augmented reality glasses, brings potential to the fashion world, where physical and virtual aspects can be blended to create an overall aesthetic. An enabler for this is the integration of computer-readable visual markers as part of clothing design. We introduce a design space for the use of visual markers on garments, with aspects of visual style, tag size, content, content type, customizability, attachment technique, and reader application. Two prototypes of clothing-integrated visual marker solutions, viewed with a mobile augmented reality (AR) application, are presented and evaluated via a focus group study, an online survey and an in-the-wild study. Our findings highlight that when designing wearable computing garments, the visual design is very important for acceptability, and that a user's personal style largely dictates their preference for the concept and willingness to wear the garment. Wearable AR markers were considered as suitable for wear in social situations, such as parties or events, and as a tool for group cohesion or for showing campaign or ideological information.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {113–121},
numpages = {9},
keywords = {outfit centric design, clothing design, wearable computing, self-expression tools, mobile AR, visual markers},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152856,
author = {M\"{u}ller, Jens and Butscher, Simon and Feyer, Stefan P. and Reiterer, Harald},
title = {Studying Collaborative Object Positioning in Distributed Augmented Realities},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152856},
doi = {10.1145/3152832.3152856},
abstract = {Augmented Reality (AR) displays have been suggested as shared-space technology to support remote collaboration, e.g., in design and building tasks. But with AR displays, the shared space typically consists of only the virtual work objects (e.g., design artifacts) while collaborators' interaction is grounded in their individual, physical environment. This can become problematic during activities that involve the positioning of virtual objects because the collaborators may require shared spatial references to coordinate their actions. In a lab experiment with 16 dyads, we studied how collaborators deal with that issue, and whether the provisioning of additive, virtual landmarks influences collaboration. As a result the landmarks improved user experience and decreased the reported temporal demand. In addition, we identified task-specific problem situations and provide implications for the design of distributed ARs to facilitate the collaborative positioning of virtual objects.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {123–132},
numpages = {10},
keywords = {object positioning, user experience, virtual landmarks, augmented reality, remote collaboration},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152861,
author = {Peuhkurinen, Antti and Mikkonen, Tommi},
title = {Mixed Reality Application Paradigm for Multiple Simultaneous 3D Applications},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152861},
doi = {10.1145/3152832.3152861},
abstract = {Hardware that is capable of running mixed reality applications is rapidly becoming commonplace in mass markets. As this trend continues, such applications will soon be available in mobile devices, where they are naturally blended into the real world to extend the perceived reality and to improve user experience in various use cases where computing devices are already involved. With mixed reality's infinite screen, our vision is that there will be tens of applications blended into the real world, allowing users to multi task in various ways. In this paper, we study the graphics system needs for mixed reality 3D application paradigms. In particular, we compare two solutions for this problem and share our experiences from their implementations.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {133–141},
numpages = {9},
keywords = {mobile devices, 3D applications, application paradigm},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152837,
author = {Colley, Ashley and Virtanen, Lasse and Knierim, Pascal and H\"{a}kkil\"{a}, Jonna},
title = {Investigating Drone Motion as Pedestrian Guidance},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152837},
doi = {10.1145/3152832.3152837},
abstract = {Flying drones have the potential to act as navigation guides for pedestrians, providing more direct guidance than the use of handheld devices. Rather than equipping a drone with a display or indicators, we explore the potential for the drone's movements to communicate the route to the walker. For example, should the drone maintain a constant distance a few meters in front of the pedestrian, or should it position itself further along the navigation route, acting as a beacon to walk towards? We created a set of flying drone gestures and evaluated them in an online survey (n = 100) and an in-the-wild user test (n = 10) where participants were guided on a walking route by a flying drone. As a result, we propose an initial set of drone gestures for pedestrian navigation and provide further design recommendations.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {143–150},
numpages = {8},
keywords = {drone, gestures, spatial interaction, human-robot interaction, pedestrian navigation},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152854,
author = {Paasovaara, Susanna and Jarusriboonchai, Pradthana and Olsson, Thomas},
title = {Understanding Collocated Social Interaction between Pok\'{e}Mon GO Players},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152854},
doi = {10.1145/3152832.3152854},
abstract = {Pok\'{e}mon GO, a location-based mobile game with millions of users, offers an opportunity to study how mobile technology can encourage collocated social interaction between people. Despite being predominantly a single-player game, Pok\'{e}mon GO has been discussed to induce a variety of social interactions between the players. We conducted a qualitative online survey to gain insight on collocated interactions and encounters that take place around the game, and to understand how they relate to the game design. Our analysis shows that the game design promotes encounters between players, the idle time during the game allows various forms of social interaction to take place, and further, the players gain various benefits from exchanging information with each other. Based on the findings, we present design implications for the design of mobile applications or games aiming to encourage collocated social interaction.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {151–163},
numpages = {13},
keywords = {social interaction, collocated interaction, pok\'{e}mon, location-based game, face-to-face interaction, game design, pok\'{e}mon go},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152868,
author = {Dionisio, Mara and Bala, Paulo and Nisi, Valentina and Nunes, Nuno},
title = {Fragments of Laura: Incorporating Mobile Virtual Reality in Location Aware Mobile Storytelling Experiences},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152868},
doi = {10.1145/3152832.3152868},
abstract = {As 360° immersive Mobile Virtual Reality (MVR) experiences are reaching a wider public, thanks to inexpensive and more powerful mobile technology available on the shelves, it is also rapidly growing as a research arena to investigate how to best design such VR experiences. Adding to this research direction, in this paper we report on an exploratory study designed to evaluate users perception in "Fragments of Laura", a mobile location-aware storytelling experience that uses MVR in a public setting. Results from the study encourage us to pursuit investigation as the experience was well received by the participants and provided an enjoyable experience, however, we discovered that there are still design challenges to overcome in order for MVR to be widely adopted within a public setting. Such as, finding the balance between multimedia and immersive multimedia, providing an "onboarding" time for such media and proper locations for the content consumption.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {165–176},
numpages = {12},
keywords = {user experience, user study, mixed-reality, mobile VR, location aware},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152871,
author = {Hochleitner, Wolfgang and Sellitsch, David and Rammer, Daniel and Aschauer, Andrea and Mattheiss, Elke and Regal, Georg and Tscheligi, Manfred},
title = {No Need to Stop: Exploring Smartphone Interaction Paradigms While Cycling},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152871},
doi = {10.1145/3152832.3152871},
abstract = {Current apps for cyclists follow the "stop-to-interact" paradigm, neglecting that people interact with their smartphones in motion. We conducted two studies to explore paradigms for interaction that can be applied while cycling. In an enactment study, participants freely explored movements suitable for interaction while using a bicycle trainer and discussed respective requirements and constraints. The analysis of the interaction movements and the group discussion showed that users preferred to keep their hands on the handlebars while performing subtle gestures with their fingers. Based on this we performed an outdoor study focused on interacting with a smartphone game while riding a bicycle, using three interaction options: buttons on the handlebars, the phone's touchscreen, and a wristband activated by flipping the wrist. Using buttons resulted in a significantly lower physical demand and significantly lower frustration compared to the other alternatives, as well as better task performance compared to interacting using the wristband.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {177–187},
numpages = {11},
keywords = {enactment, interaction design, cycling, input modalities, mobile interaction},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152864,
author = {Pai, Yun Suen and Kunze, Kai},
title = {Armswing: Using Arm Swings for Accessible and Immersive Navigation in AR/VR Spaces},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152864},
doi = {10.1145/3152832.3152864},
abstract = {Navigating in a natural way in augmented reality (AR) and virtual reality (VR) spaces is a large challenge. To this end, we present ArmSwingVR, a locomotion solution for AR/VR spaces that preserves immersion, while being low profile compared to current solutions, particularly walking-in-place (WIP) methods. The user simply needs to swing their arms naturally to navigate in the direction where the arms are swung, without any feet or head movement. The benefits of ArmSwingVR are that arm swinging feels natural for bipedal organisms second only to leg movement, no additional peripherals or sensors are required, it is less obtrusive to swing our arms as opposed to WIP methods, and requires less energy allowing prolong uses for AR/VR. A conducted user study found that our method does not sacrifice immersion while also being more low profile and less energy consumption compared to WIP.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {189–198},
numpages = {10},
keywords = {arm swing, locomotion, virtual reality, immersive, navigation},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152869,
author = {Kallioniemi, Pekka and Keskinen, Tuuli and Hakulinen, Jaakko and Turunen, Markku and Karhu, Jussi and Ronkainen, Kimmo},
title = {Effect of Gender on Immersion in Collaborative IODV Applications},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152869},
doi = {10.1145/3152832.3152869},
abstract = {Interactive omnidirectional video (iODV) is a media format that allows the user to explore and interact with a 360-degree view of the recorded scenery. Recently, novel collaborative applications for presenting iODV content have emerged. Often their goal is to offer as immersive as possible experience to the users. Previous studies suggest that gender affects the §of immersion in virtual environments and other media, but there has been only little research on immersion in iODVs, and nothing in the context of collaborative interaction. In this research, we studied gender effect with participants (N=30, 15 pairs) performing a collaborative wayfinding task. Subjective data gathered with a customized immersion questionnaire showed statistically significant differences between male and female participants in Spatial Immersion and Involvement subscales. There were no statistical differences in Interaction, Realness, Physical and Auditory subscales. Several possibly affecting factors were observed during the task completion. Our results also indicate that performing interactive, collaborative tasks in iODV applications helps building a shared understanding between the users.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {199–207},
numpages = {9},
keywords = {interactive omnidirectional videos, gender differences, immersion, collaborative virtual environments},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152872,
author = {Kelling, Chelsea and V\"{a}\"{a}t\"{a}j\"{a}, Heli and Kauhanen, Otto},
title = {Impact of Device, Context of Use, and Content on Viewing Experience of 360-Degree Tourism Video},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152872},
doi = {10.1145/3152832.3152872},
abstract = {With the rapid advancement and development of emerging technologies, more in-depth understanding of user interactions and experiences are needed. In this study, we explored the reactions, impressions, and emotions elicited by a 360-degree video that markets an airport and local attractions for a distant destination. Differences in presence and viewing experiences on a mobile phone and VR headset were examined in two contexts: a semi-public setting and a private setting. Our results showed a preference for the private setting, not only because of distractions present in the semi-public location, but also due to social and cultural anxieties felt by participants. Furthermore, we suggest a set of guidelines that relate to the experiential elements of viewing 360-degree videos that can aid designers and researchers in the creation of unique content and novel services, in which we recommend establishing an emotional connection, providing engagement, guiding viewer attention, encouraging exploration, understanding the appropriate viewing context, and avoiding technical flaws.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {211–222},
numpages = {12},
keywords = {omnidirectional video, virtual reality, user study, guidelines, user experience design, engagement, tourism, device comparison, airport and travel, presence, affective content},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152873,
author = {Shimizu, Tomoyuki and Futami, Kyosuke and Terada, Tsutomu and Tsukamoto, Masahiko},
title = {In-Clock Manipulator: Information-Presentation Method for Manipulating Subjective Time Using Wearable Devices},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152873},
doi = {10.1145/3152832.3152873},
abstract = {People often feel dissatisfied when their subjective time (i.e. time that is subjectively experienced) is different from the objective time (i.e. time that is determined objectively). Because it is difficult for one to control his/her sense of time, manipulating subjective time easily with computing technology can enhance a user's satisfaction in many situations. In this paper, therefore, we propose a method for controlling a user's subjective time by using sensory stimuli designed based on filled-duration illusion (FDI) for wearable-computer environments. Filled-duration illusion is an illusion in which subjective time is affected by the amount of sensory stimuli within a certain duration. We evaluated three prototype systems involving our method using three stimulus modalities, i.e., visual, auditory, and tactile presented on wearable devices. In the experiment using prototype visual-stimulus system, we confirmed that user's time estimation increased significantly by about 18 % under the condition of stimuli-decrease pattern compared with no stimuli.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {223–230},
numpages = {8},
keywords = {time perception, smart watch, HMD, filled-duration illusion, psychology, wearable computer, information presentation, earphones},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152849,
author = {Araki, Tomohiro and Komuro, Takashi},
title = {On-Mouse Projector: Peephole Interaction Using a Mouse with a Projector},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152849},
doi = {10.1145/3152832.3152849},
abstract = {In this paper, we propose a system that combines a mouse with a projector and that allows users to perform peephole interaction, which enables stable operation in a wide workspace. In our prototype system, a mobile projector is attached above a mouse and projects part of the workspace on the plane in front of the mouse. The user can change the area he/she wishes to see by moving the mouse and can perform operations in a virtually extended workspace. We conducted an experiment to compare the methods of changing the displayed region and the cursor position when the mouse moves. Our results show that the method with the highest ratio of view transition to mouse movement, in which the workspace is not fixed to the real space, outperformed the other methods and was also highly evaluated by participants. Furthermore, the task completion time decreased with repeating trials with fixed target positions, which suggests that spatial memory is helpful even when the workspace is not fixed to the real space.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {231–239},
numpages = {9},
keywords = {mouse input, peephole interaction, information space, mobile projector},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152852,
author = {Lischke, Lars and Mayer, Sven and Hoffmann, Jan and Kratzer, Philipp and Roth, Stephan and Wolf, Katrin and Woundefinedniak, Pawe\l{}},
title = {Interaction Techniques for Window Management on Large High-Resolution Displays},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152852},
doi = {10.1145/3152832.3152852},
abstract = {Large high-resolution displays (LHRDs) present new opportunities for interaction design in areas such as interactive visualization and data analytics. Design processes for graphical interfaces for LHRDs are still challenging. In this paper, we explore the design space of graphical interfaces for LHRDs by engaging in the creation of four prototypes for supporting office work. Specifically, we investigate how users can effectively manage application windows on LHRDs using four window alignment techniques: curved zooming, window grouping, window spinning and side pane navigation. We present the design and implementation of these window alignment techniques in a sample office application. Based on a mixed-methods user study of our prototypes, we contribute insights on designing future graphical interfaces for LHRDs. We show that potential users appreciate techniques, which enhance focus switching without changing the spatial relation between related windows.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {241–247},
numpages = {7},
keywords = {office environments, large high-resolution displays, desktop environments, interface design},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152855,
author = {Zagermann, Johannes and Pfeil, Ulrike and Acevedo, Carmela and Reiterer, Harald},
title = {Studying the Benefits and Challenges of Spatial Distribution and Physical Affordances in a Multi-Device Workspace},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152855},
doi = {10.1145/3152832.3152855},
abstract = {In recent years, research on cross-device interaction has become a popular topic in HCI leading to novel interaction techniques mutually interfering with new evolving theoretical paradigms. Building on previous research, we implemented an individual multi-device work environment for creative activities. In a study with 20 participants, we compared a traditional toolbar-based condition with two conditions facilitating spatially distributed tools on digital panels and on physical devices. We analyze participants' interactions with the tools, encountered problems and corresponding solutions, as well as subjective task load and user experience. Our findings show that the spatial distribution of tools indeed offers advantages, but also elicits new problems, that can partly be leveraged by the physical affordances of mobile devices.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {249–259},
numpages = {11},
keywords = {cross-device interaction, smartphones},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152862,
author = {Kiss, Francisco and Schwind, Valentin and Schneegass, Stefan and Henze, Niels},
title = {Design and Evaluation of a Computer-Actuated Mouse},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152862},
doi = {10.1145/3152832.3152862},
abstract = {Although interaction with computing systems has become remarkably diverse in recent years, the computer mouse remained the primary pointing device for daily computer use. Being solely an input device, the classical mouse decouples input from output. In this paper, we propose to extend the mouse to a device that can be actuated by the user and the computer. We developed a mouse that allows its position and button state to be actuated. In a technical evaluation, we test the spatial resolution of our system and how effectively feedback is communicated to the user. In a subjective assessment, we explore users' reactions to four use cases including games and office applications, highlighting the potential of the device. Through a quantitative assessment, we investigate whether perceiving the movement of the mouse helps to learn gestures. Finally, we discuss how a mouse providing feedback can be used to build novel interaction techniques.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {261–271},
numpages = {11},
keywords = {mouse, actuated device, pointing device, force feedback},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152836,
author = {Kassem, Khaled and Salah, Jailan and Abdrabou, Yasmeen and Morsy, Mahesty and El-Gendy, Reem and Abdelrahman, Yomna and Abdennadher, Slim},
title = {DiVA: Exploring the Usage of Pupil <u>Di</u>Ameter to Elicit <u>V</u>Alence and <u>A</u>Rousal},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152836},
doi = {10.1145/3152832.3152836},
abstract = {Most of the typical digital systems are not fully aware of the users' affect states. Adapting to the users' state showed great potential for enhancing user experiences. However, most approaches for sensing affective states, specifically arousal and valence, involve expensive and obtrusive technologies, such as physiological sensors attached to users' bodies. This paper present an indicator of the users' affect based on eye tracking. We use a commercial eye tracker to monitor the user's pupil size to estimate their arousal and valence in response to videos of different content. To assess the effect of different content (namely pleasant and unpleasant) influencing the arousal and valence on the pupil diameter, we conducted a user study with 25 participants. The study showed that different content of videos affect the pupil diameter, thereby giving an indicator about the user's state. We provide empirical evidence showing how to unobtrusively detect changes in users' state. Our initial investigation gives rise to eye-based user's tracking, which introduces the potential of new applications in the field of affect-aware computing.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {273–278},
numpages = {6},
keywords = {pupil diameter, eye tracker, valence, arousal},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152839,
author = {Lander, Christian and Gehring, Sven and L\"{o}chtefeld, Markus and Bulling, Andreas and Kr\"{u}ger, Antonio},
title = {Eyemirror: Mobile Calibration-Free Gaze Approximation Using Corneal Imaging},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152839},
doi = {10.1145/3152832.3152839},
abstract = {Gaze is a powerful measure of people's attracted attention and reveals where we are looking within our current FOV. Hence gaze-based interfaces are gaining in importance. However, gaze estimation usually requires extensive hardware and depends on a calibration that has to be renewed regularly. We present EyeMirror, a mobile device for calibration-free gaze approximation on surfaces (e.g., displays). It consists of a head-mounted camera, connected to a wearable mini-computer, capturing the environment reflected on the human cornea. The corneal images are analyzed using natural feature tracking for gaze estimation on surfaces. In two lab studies we compared variations of EyeMirror against established methods for gaze estimation in a display scenario, and investigated the effect of display content (i.e. number of features). EyeMirror achieved 4.03° gaze estimation error, while we found no significant effect of display content.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {279–291},
numpages = {13},
keywords = {pervasive, corneal image, gaze approximation, mobile device, feature tracking},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152848,
author = {Wu, Jianming and Hagiya, Toshiyuki and Tang, Yujin and Hoashi, Keiichiro},
title = {Effects of Objective Feedback of Facial Expression Recognition during Video Support Chat},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152848},
doi = {10.1145/3152832.3152848},
abstract = {This paper examines the effects of objective feedback of facial expression recognition on the operators of a video support chat system. We have built a facial expression feedback (FEF) system, which recognizes the users' facial expressions, and displays a visual summary of the recognition results to the user, in order to raise self-awareness of his/her expressions during video communication. We evaluated our system in a supposed simulation of a call center, in which operators provide assistance to customers about how to use their smartphones. Experimental results have verified that our approach is useful to help operators improve their self-awareness of facial expression in the video support chat.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {293–297},
numpages = {5},
keywords = {facial expression recognition, smile rate, objective feedback, self-awareness, video chat},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152859,
author = {Greis, Miriam and Karolus, Jakob and Schuff, Hendrik and Woundefinedniak, Pawe\l{} W. and Henze, Niels},
title = {Detecting Uncertain Input Using Physiological Sensing and Behavioral Measurements},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152859},
doi = {10.1145/3152832.3152859},
abstract = {Interactive systems, such as online search interfaces, require appropriate input if they are to produce accurate information. Without this, they can be inaccurate if the user is uncertain about the keywords. Current systems do not have the means to detect uncertainty, which may lead to a negative user experience. We explore physiological and behavioral measurements as tools to implicitly detect users' uncertainty, and provide a method to integrate input variability in interactive systems. We conducted a laboratory study where participants answered questions of varying difficulty, recording physiological data via a key logger, an eye tracker, and a heart rate sensor. Our results show that participants spent significantly more time on difficult questions and looked longer at their answers before submitting them. Based on our results, we provide initial insights on how data from physiological sensors and logged user behavior can be utilized to enrich interactive systems and evaluate a user's uncertainty level.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {299–304},
numpages = {6},
keywords = {interactive systems, uncertainty, user input},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152865,
author = {Hassib, Mariam and Khamis, Mohamed and Friedl, Susanne and Schneegass, Stefan and Alt, Florian},
title = {Brainatwork: Logging Cognitive Engagement and Tasks in the Workplace Using Electroencephalography},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152865},
doi = {10.1145/3152832.3152865},
abstract = {Today's workplaces are dynamic and complex. Digital data sources such as email and video conferencing aim to support workers but also add to their burden of multitasking. Psychophysiological sensors such as Electroencephalography (EEG) can provide users with cues about their cognitive state. We introduce BrainAtWork, a workplace engagement and task logger which shows users their cognitive state while working on different tasks. In a lab study with eleven participants working on their own real-world tasks, we gathered 16 hours of EEG and PC logs which were labeled into three classes: central, peripheral and meta work. We evaluated the usability of BrainAtWork via questionnaires and interviews. We investigated the correlations between measured cognitive engagement from EEG and subjective responses from experience sampling probes. Using random forests classification, we show the feasibility of automatically labeling work tasks into work classes. We discuss how BrainAtWork can support workers on the long term through encouraging reflection and helping in task scheduling.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {305–310},
numpages = {6},
keywords = {EEG, multitasking, workplace logging},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152834,
author = {Visuri, Aku and Hosio, Simo and Ferreira, Denzil},
title = {Exploring Mobile Ad Formats to Increase Brand Recollection and Enhance User Experience},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152834},
doi = {10.1145/3152832.3152834},
abstract = {Digital marketing is increasingly moving from desktop (e.g., browser) to mobile environments (e.g., within mobile applications). The means for delivering ads however, remains largely unchanged: banners and videos. In this work, we explore transforming ad delivery methods to the mobile environment while mitigating issues causing frustration and distractions to the users, evident in both web and mobile marketing. We demonstrate that softly enforcing interaction with the ad - with minimal usable screen space reduction - can improve user's attitude towards mobile advertising. Brand recollection is also influenced via increased interactions with the ad delivery method.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {311–319},
numpages = {9},
keywords = {advertising, interfaces, smartphone},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152846,
author = {Samsonov, Pavel and Heller, Florian and Sch\"{o}ning, Johannes},
title = {Autobus: Selection of Passenger Seats Based on Viewing Experience for Touristic Tours},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152846},
doi = {10.1145/3152832.3152846},
abstract = {Choosing a seat for traveling can be a complex evaluation of constraints depending on personal preferences. There are websites that help to choose the best seat in a bus, in a train, or on an airplane. However, these recommendations only consider seat-related factors and not the view from the window. While a scenic view rarely influences the decision for a seat on a plane, it is much more important for train rides and especially for scenic bus tours. Therefore, travel website users often discuss which side offers the best view on a specific trip. We propose an algorithm, which decides on which side of the road the view is the most scenic based on Google Street View images. These results can be used by travelers to choose a seat and by scenic tour providers to balance the scenic views between sides or add options during checkout.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {321–326},
numpages = {6},
keywords = {google street view, scenic routes, machine learning},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152857,
author = {Phan, Thanh-Trung and Gatica-Perez, Daniel},
title = {Healthy #fondue #dinner: Analysis and Inference of Food and Drink Consumption Patterns on Instagram},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152857},
doi = {10.1145/3152832.3152857},
abstract = {Social media generate large-scale data to study food and drink consumption in everyday life. Using Instagram posts in Switzerland over five years, our goal is two-fold. First, we extract key food &amp; drink consumption patterns, through the lenses of a data-driven dictionary of popular items extracted from hashtags, and of a food categorization system used by the Swiss Federal government for national statistics purposes. Patterns related to spatial and temporal distributions of food &amp; drink consumption, demographics, and eating events are extracted and compared to official statistics. Second, using the insights from this analysis, we define two eating event classification tasks, including a two-class task (healthy vs. unhealthy) and a six-class task (the three main meals break-fast/lunch/dinner/plus brunch/coffee/tea). Both tasks use hash-tags as labels for supervised learning. We study how content (hashtags and food categories), context (time and location), and social features (likes) can discriminate these eating events. A random forest and a combination of content and context features can classify healthy vs. unhealthy eating posts with 85.8% accuracy, and the six daily eating occasions with 61.7% accuracy.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {327–338},
numpages = {12},
keywords = {foursquare, social media, consumption patterns, hashtags, human-centered computing, drink, food, instagram},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152866,
author = {Muralidhar, Skanda and Gatica-Perez, Daniel},
title = {Examining Linguistic Content and Skill Impression Structure for Job Interview Analytics in Hospitality},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152866},
doi = {10.1145/3152832.3152866},
abstract = {First impressions are critical to professional interactions especially in the context of employment interviews. This work investigates connections between linguistic content and first impressions in job interviews and the structure of ten soft skills and overall impressions. Towards this, we transcribe 169 role-played job interviews conducted at a hospitality school and analyze the linguistic content using off-the-shelf software. To understand the structure of the soft skill impressions, we conduct a principal component analysis. We then develop methods to automatically infer impressions using verbal and nonverbal features and their combination. Results indicate low predictive power of verbal cues for overall impression (R2 = 0.11). Combined verbal and nonverbal cues explain up to 34% of variance, a marginal improvement over R2 = 0.32 using only nonverbal cues. The use of principal components reveals a major component associated to overall positive and negative impressions that when used as labels for supervised learning results in a regression performance of R2 = 0.41.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {339–343},
numpages = {5},
keywords = {hospitality, first impressions, verbal content, job interviews},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156629,
author = {Stigberg, Susanne Koch},
title = {A Critical Review on Participation in Mobile Interaction Design Research},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156629},
doi = {10.1145/3152832.3156629},
abstract = {This paper presents a review on participation in mobile interaction design research, analyzing 158 publications regarding user involvement and design method. Results show that user involvement in general is applied when evaluating interaction artifacts. In the typical mobile interaction design study a prototype is developed and 14 students are recruited for user testing. They have to fill out a questionnaire and are asked about their opinion in a concluding interview. Additional user participation during the design process could lead to new insights for mobile interaction design and provide new knowledge to the research community. Here I discuss three opportunities for further participation in mobile interaction design research.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {345–354},
numpages = {10},
keywords = {participation, literature review, mobile interaction},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156614,
author = {Aiyoshizawa, Toshiaki and Komuro, Takashi},
title = {Comparative Study on Text Entry Methods for Mobile Devices with a Hover Function},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156614},
doi = {10.1145/3152832.3156614},
abstract = {In this paper, we compared text entry methods for mobile devices such as smartphones, in which part of a software keyboard around the finger position is enlarged by using a hover function. We examined four methods: fixing the center of enlargement, not fixing the center of enlargement, adding a scrolling function, and using a standard software keyboard for reference. We recruited 12 participants and evaluated the text entry speed and error rates. The method in which the center of enlargement was fixed had the slowest text entry speed, but showed an error rate that was significantly lower than the method in which the center of enlargement was not fixed. This result shows that fixing the center of enlargement enables users to select keys stably and enter target letters accurately.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {355–361},
numpages = {7},
keywords = {text entry, hover input, touch screen, zooming interface},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156624,
author = {K\"{o}rber, Yannick},
title = {Device Selection for Speech Output in a Multi-User Scenario},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156624},
doi = {10.1145/3152832.3156624},
abstract = {Creating proactive behavior in intelligent environments requires a context-aware procedure, which can dynamically select output devices and adapt the output according to the user's needs. However, selecting loudspeakers automatically for presentation in a multi-user scenario is challenging as it depends on numerous factors like position, device characteristics or user profiles. This paper describes a model to represent context, states technical requirements and presents multiple rules that a system can apply to achieve smarter audio playback in intelligent environments.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {363–370},
numpages = {8},
keywords = {intelligent environments, audio playback, intelligent multimedia presentation},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156617,
author = {Bala, Paulo and Dion\'{\i}sio, Mara and Trindade, Rui and Olim, Sandra and Nisi, Valentina and Nunes, Nuno},
title = {Evaluating the Influence of Location and Medium Applied to Mobile VR Storytelling},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156617},
doi = {10.1145/3152832.3156617},
abstract = {This paper investigates whether location and medium affect the experience of interactive storytelling in the context of mobile Virtual Reality systems. For this purpose, we discuss the development of Fragments of Laura, a location-aware multimedia application in a larger interactive transmedia story, where users can view a 360° narrative in a 3D environment. We conducted a user study with the intention of measuring Presence, Flow and Narrative Transportation and evaluate four scenarios resulting from the combination of two independent variables - location (existence and absence of links between the test location and the narrative location) and medium (tablet and mobile virtual reality with smartphones). Our results show that the user experience while watching a narrative fluctuates depending on the location where is viewed and the device in which is viewed. Locations linked with the content lead to a significantly increased Flow, Presence, and Narrative Transportation.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {371–378},
numpages = {8},
keywords = {location-aware, flow, digital storytelling, virtual reality},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156622,
author = {Voit, Alexandra and Schneegass, Stefan},
title = {FabricID: Using Smart Textiles to Access Wearable Devices},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156622},
doi = {10.1145/3152832.3156622},
abstract = {Wearable devices like smart watches or eye-wear computers are storing a myriad of personal information. Today, wearable devices support input techniques such as speech and gesture input. These user input methods however, are not well suited for authentication. With the development of smart textiles the design space for interaction for small smart devices can be increased. In this paper, we present the concept of FabricID a system which identifies users' hand-prints with a smart textile integrated into the sleeve of the user. To evaluate our concept, we recorded hand-prints of 16 users. We classified all recorded hand-prints and received an identification rate of 82.5% for all 16 users and on average 93.62% for groups of 4 users.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {379–385},
numpages = {7},
keywords = {smart textiles, authentication, wearable computing, identification},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156620,
author = {Root, Erika and Steinkamp, Maren and Coldewey, Beatrice and Poloczek, Christin and Scharnowski, Frederik and Kettner, Mark and Koelle, Marion and Ananthanarayan, Swamy and Willms, Marlon},
title = {Grasping Algorithms: Exploring Toys That Teach Computational Thinking},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156620},
doi = {10.1145/3152832.3156620},
abstract = {Computational thinking has received increased attention over the past several years and is considered by many to be a fundamental and necessary literacy for children. One approach to teaching this skill has been through tangible programming since it facilitates intuitive interaction with children. In this paper, we introduce CodeTrain and CodeBox, two motorized toys whose behavior can be programmed through wooden building blocks without the use of a computer, tablet, or screen-based interface. The goal with both these designs is to foster playful exploration of computational thinking in pre-adolescent children through tangible objects. We describe the design process from the conceptual design to the functional prototype. Furthermore, we present first impressions of the prototypes from a pilot study and highlight some lessons learned from our first iteration.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {387–392},
numpages = {6},
keywords = {playfully interaction, computational toys, tangible programming, children, computational thinking},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156613,
author = {Pakanen, Minna and Alavesa, Paula and Kukka, Hannu and Nuottaj\"{a}rvi, Peetu and Hellberg, Zacharias and Orjala, Lari-Matias and Kupari, Niko and Ojala, Timo},
title = {Hybrid Campus Art: Bridging Two Realities through 3D Art},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156613},
doi = {10.1145/3152832.3156613},
abstract = {We describe an interactive digital art application deployed at a university campus with the goal of investigating participants' initial experiences of bridging two realities through 3D artwork in a hybrid reality space. The underlying purpose of the installation is to bring liveliness and increased social interaction to both the virtual and the physical world. We used the HTC Vive and a 3D painting program (Google Tilt Brush) for the creation of the artwork. After creation, the artworks were transferred to an immersive virtual mirror world, in our case a version of our university campus, using an export tool. Results from an initial user study with 32 participants suggest that people found our system attractive, and there was only slight difference in how much they enjoyed seeing their work in the Tilt Brush view versus the virtual campus gallery view.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {393–399},
numpages = {7},
keywords = {virtual gallery, 3D painting, digital art, virtual reality},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156619,
author = {Hoppe, Matthias and Boldt, Robin and Strau\ss{}, Jan and Lischke, Lars and Weber, Dominik and Henze, Niels},
title = {Image Browsing on Large High-Resolution Displays},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156619},
doi = {10.1145/3152832.3156619},
abstract = {Visual exploration of large image data sets is a common and widely performed task. With the ubiquity of cameras, personal image collections are growing. Even when working with large high-resolution displays, collections are typically too large to display all images. In this work, we present two full-body, hands-free zooming interaction techniques for image exploration on large high-resolution displays that allow users to freely move in front of the display. We compare these techniques with state-of-the-art image browsing, where a large number of images is presented in a static grid. The first technique selects and enlarges whole columns by full-body-tracking of the user. The second technique additionally uses the height of the users head to select a single picture as a focus point. The results show that novel interaction techniques and visualizations for image sets on large high-resolution displays are required and can enhance the advantages of such displays.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {401–405},
numpages = {5},
keywords = {high resolution, visual exploration, image browsing, zoom, large displays, fisheye, full-body interaction},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156615,
author = {Schiavo, Gianluca and Mich, Ornella and Ferron, Michela and Mana, Nadia},
title = {Mobile Multimodal Interaction for Older and Younger Users: Exploring Differences and Similarities},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156615},
doi = {10.1145/3152832.3156615},
abstract = {Since they can integrate a wide range of interactive modalities, multimodal interfaces are considered to improve accessibility for a variety of users, including older adults. However, only few works have actually explored how older adults approach multimodal interaction outside specific contexts and have done so mainly in comparison to much younger users. This study explores how older (65+ years old), middle-aged (55-65 years old) and younger adults (25-35 years old) use mobile multimodal interaction in an everyday activity (i.e. taking photos with a tablet) by using midair gestures and voice commands, and investigates the differences and similarities between the considered age groups. Preliminary findings from a video-analysis show that all groups easily combine the proposed modalities when interacting with a tablet device. Furthermore, compared to younger adults, older and middle-aged adults show similarities in the way they perform gesture and voice commands.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {407–414},
numpages = {8},
keywords = {mid-air gesture-based interaction, older adults, multimodal interaction, speech-based interaction},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156621,
author = {Voit, Alexandra and Greis, Miriam and Weber, Dominik and G\"{o}nner, Katharina and Kutger, Isabella and Mantz, Tamara and Schmidt, Simone and van der Vekens, Lucas and Schmidt, Albrecht},
title = {My Painting Shows My Stats: Towards a Personal Colorful Activity Display},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156621},
doi = {10.1145/3152832.3156621},
abstract = {Regular physical activity reduces the risks of illness, obesity, and falling. Thus, many personal devices support users in monitoring their physical activity to initiate behavior changes. However, activity data is prone to measurement errors; for example a user is seated but typing increases the step count. Such false positives could be easily detected if data from multiple sensors would be connected. We envision a system that combines data from multiple sensors in our surroundings to reduce such measurement errors. Furthermore, the system shows the user's historical data for the past seven days, the activity level of the current day, and a forecast about physical activity for the next seven days in an artistic and configurable digital wall painting. We argue that this strengthens connectedness and privacy. We are convinced that our system can help to increase user's trust in activity data and raise awareness for behavior change regarding physical activity.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {415–420},
numpages = {6},
keywords = {informative art, physical activity, ambient information system, behavior change, self quantification},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156627,
author = {Walter, Jonas and Abendroth, Bettina and Agarwal, Nupur},
title = {PRICON: Self-Determined Privacy in the Connected Car Motivated by the Privacy Calculus Model},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156627},
doi = {10.1145/3152832.3156627},
abstract = {The advent of connected vehicles has increased the relevance of privacy in cars. Thus, a system is required that increases transparency and helps passengers to control their privacy in a self-determined manner. Here, we derive an interface based on the privacy calculus model and prove the validity of the theory-driven design approach in course of a usability test. Based on fourteen participants, we demonstrate an above-average usability of the theory-driven design which incorporates the visual comparison of functional benefits and privacy risks. Interviews reveal that users especially acknowledge this direct comparison and underscore the successful implementation of the privacy calculus model in a vehicular privacy user interface.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {421–427},
numpages = {7},
keywords = {self-determined privacy, vehicular user interface},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156618,
author = {Okada, Masae and Higuchi, Masakazu and Komuro, Takashi and Ogawa, Kayo},
title = {Recognition of Typing Motions on AR Typing Interface},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156618},
doi = {10.1145/3152832.3156618},
abstract = {We have proposed Augmented Reality (AR) Typing Interface as a new input interface for mobile devices. By using this interface, users can utilize a wide space in the air in inputting texts. This interface uses a camera which is attached to the back of the device. A virtual keyboard is overlaid on the image captured by a camera using AR technology. In this paper, we propose a new method to recognize typing motions more precisely. In this method, feature vectors are generated by using information of time-series optical flows. Typing motions are recognized by using support vector machine (SVM). We confirmed that the proposed method was able to recognize typing motions with about 90% accuracy for the recorded video of typing in the air.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {429–434},
numpages = {6},
keywords = {virtual keyboard, support vector machine, optical flow, multi-finger input, augmented reality},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156626,
author = {Wei, Shih-Yao and Lo, Yi-Ping and Lin, Chia-Yi and Lin, Tse-Yu and Chou, Yin-Yu and Huang, Jung-Tang and Yang, Rong-Sen and Hung, Yi-Ping},
title = {SaFeplay: A Lightweight Portable Sensing System to Estimate Knee Adduction Moment},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156626},
doi = {10.1145/3152832.3156626},
abstract = {Traditional lower limb biomechanics detection system are not only expansive but difficult to monitor user in the whole day. It is not friendly to the large scale of patients with knee joint disease such as knee osteoarthritis. Therefore, we design a lightweight portable lower limbs motion monitoring, biomechanics measurement, and analysis system, Smart Footwear Platform (SaFePlay). We apply SaFePlay to estimate the index of knee joint loading, knee adduction moment (KAM), by the lever-arm approach. We get information about user's foot pressure and knee bending angle, then we can derive KAM with body parameters. We conduct an experiment to verify the accuracy of the proposed method by comparing with motion capture and 3DoF force plates systems as ground truth. The result shows that the curve of KAM from SaFePlay is similar to the ground truth. The proposed method not only makes KAM evaluation portable but also requires only lightweight devices.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {435–440},
numpages = {6},
keywords = {wearable device, knee adduction moment, algorithms tackling technical challenges of mobile systems, mobile applications, internet of things},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156628,
author = {Venesvirta, Hanna and \v{S}pakov, Oleg and Gizatdinova, Yulia and Tuisku, Outi and Rantanen, Ville and Verho, Jarmo and Vetek, Akos and Lekkala, Jukka and Surakka, Veikko},
title = {Smile to Save It: Facial Expressions for Lifelogging},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156628},
doi = {10.1145/3152832.3156628},
abstract = {Current aim was to introduce and initially evaluate the performance of a system called Extended Cognition, which was developed for enhancing the filtering of emotionally meaningful information from visual lifelogging data. The system is proposed to add facial expression markers to the stream of visual lifelogging data in order to later find special, possibly emotionally meaningful moments from the data. In an initial user study we collected subjective evaluations (N=10) about the ergonomics of the data measurement setup and the use of voluntary smiles and frowns for adding markers. Experience evaluations about the system were also collected. Results showed that data measurement setup was ergonomic to use and the system was evaluated positively. Smiling was rated as pleasant, natural, and functional expression for data marking purposes. Participants considered that, for example, the system would support memorization, and that it could be used to share important memories with others.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {441–448},
numpages = {8},
keywords = {lifelogging, affective computing, expression detection},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156623,
author = {Reschke, Daniela and B\"{o}hmer, Matthias and Sorg, Manuel},
title = {Tactifloor: Design and Evaluation of Vibration Signals for Doorway Reminder Systems},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156623},
doi = {10.1145/3152832.3156623},
abstract = {The forgetting of objects is an everyday problem. Current research projects develop doorway reminder systems which are supposed to remind the user of certain objects or events before leaving home. At present the focus of research is mainly on the technical design of the recognition of persons and objects. For this reason we are concentrating on the design and evaluation of possible system feedback. In particular, the target group of the digital natives is familiar with the concept of tactile feedback for reminder functions. Therefore known vibration patterns were used, which are to be transferred to the user via the floor. This solution was implemented in a first prototype. A user study with 19 volunteers showed a general acceptance of tactile feedback via floor panel and the cognitive link of the vibration patterns with reminder notifications.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {449–455},
numpages = {7},
keywords = {digital natives, tactile feedback, floor, reminder},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156625,
author = {Wolf, Katrin and Funk, Markus and Khalil, Rami and Knierim, Pascal},
title = {Using Virtual Reality for Prototyping Interactive Architecture},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156625},
doi = {10.1145/3152832.3156625},
abstract = {Even though, three-dimensional representations of architectural models exist, experiencing these models like one would experience a fully constructed building is still a major challenge. With Virtual Reality (VR) it is now possible to experience a number of scenarios in a virtual environment. Also prototyping interactive architecture elements, which might be very expensive, becomes possible. Thus, researchers and designers can already start to define user interfaces for interactive architectural elements, before they were even built. However, it is still an open question how exactly VR technologies can support experiencing interactive architecture. To answer this question, we compared experiencing three-dimensional architectural models and interactive architectural elements through a 2D screen &amp; mouse+keyboard navigation, an head mounted display (HMD) &amp; keyboard navigation, and an HMD &amp; walking for navigation. The results of our study show challenges and opportunities regarding the immersion of these experiences.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {457–464},
numpages = {8},
keywords = {virtual reality, interactive architecture},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156616,
author = {Baytas, Mehmet Aydin and Batis, Emmanuel and Bylund, Mathias and \c{C}ay, Damla and Yanta\c{c}, Asim Evren and Fjeld, Morten},
title = {Viewfinder: Supporting the Installation and Reconfiguration of Multi-Camera Motion Capture Systems with a Mobile Application},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156616},
doi = {10.1145/3152832.3156616},
abstract = {We present ViewFinder, a cross-platform mobile application to support the installation and reconfiguration of marker-based motion capture systems with multiple cameras. ViewFinder addresses a common issue when installing or reconfiguring motion capture systems: that system components such as cameras and the host computer can be physically separate and/or difficult to reach, requiring personnel to maneuver between them frequently and laboriously. ViewFinder allows setup technicians or end-users to visualize the output of each camera in the system in a variety of ways in real time, on a smartphone or tablet, while also providing a means to make adjustments to system parameters such as exposure or marker thresholds on the fly. The app has been designed and evaluated through a process observing user-centered design principles, and effectively reduces the amount of work involved in installing and reconfiguring motion capture systems.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {465–471},
numpages = {7},
keywords = {remote feedback, viewfinder, motion capture},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3157814,
author = {Holzmann, Clemens and Steiner, Dustin and Riegler, Andreas and Grossauer, Christian},
title = {An Android Toolkit for Supporting Field Studies on Mobile Devices},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3157814},
doi = {10.1145/3152832.3157814},
abstract = {Evaluating mobile user interfaces in the field is a time-consuming and cumbersome task. In order to figure out how users interact with mobile apps or devices over an extended period of time, an automated logging of device usage and context information is necessary. In this paper, we present an Android app called automate toolkit for logging such data across arbitrary apps in a convenient and customizable way. It allows to trace usage information like visited screens and performed gestures as well as information about the context of use like device orientation and light conditions. The data is stored on the device for further analysis with statistical software. We made the toolkit available as open source software in order to support developers, designers and researchers in conducting field studies on Android devices.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {473–479},
numpages = {7},
keywords = {software toolkit, usability evaluation, field study, android app},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3157811,
author = {Moorthy, Preetha and Honauer, Michaela and Hornecker, Eva and M\"{u}hlenberend, Andreas},
title = {Hello World: A Children's Touch and Feel Books Enhanced with DIY Electronics},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3157811},
doi = {10.1145/3152832.3157811},
abstract = {A variety of interactive books, including 'touch and feel books', enable children to physically learn and develop knowledge. We developed an interactive touch and feel book for children that integrates sound and LED light to enhance the child's interaction. Our project explores the development of textile-based electronic touch and feel books, integrating textiles with ready-made and self-made sensors and actuators in a book format. It is called 'Hello World' as this is the common name of the very first program by someone new to software or hardware development, and thought ta good metaphor for children exploring their very first touch-and-feel book enhanced with interactive features.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {481–488},
numpages = {8},
keywords = {arduino, crafting, DIY electronics, children, e-textiles},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3157812,
author = {Kempfle, Jochen and Van Laerhoven, Kristof},
title = {Human Posture Capture and Editing from Heterogeneous Modalities},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3157812},
doi = {10.1145/3152832.3157812},
abstract = {Estimating human pose can be done with a variety of technologies. Models are typically derived from a motion capturing system that aims at generating a skeleton model as accurate and fast as possible. To date, a large variety of solutions is available to obtain capture data, often either from the environment, such as from depth cameras, or person-based, such as by body-worn inertial sensors. There are, however, few frameworks that are able to combine different types of human motion data into one environment. To this end, we present a modular and open architecture that allows specifically to integrate different capture sources and different filters to capture, process, and fuse these, in real time, into a human posture model. We show how our proposed solution leads to a fast and flexible way to deal with multiple motion capture modalities, and how such a framework can be used for editing and combining postures.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {489–494},
numpages = {6},
keywords = {human pose estimation, motion capture, inertial measurement},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3157808,
author = {Seiger, Ronny and Korzetz, Mandy and Gohlke, Maria and A\ss{}mann, Uwe},
title = {Mixed Reality Cyber-Physical Systems Control and Workflow Composition},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3157808},
doi = {10.1145/3152832.3157808},
abstract = {With the emergence of Cyber-physical Systems (CPS) more and more smart devices become parts of our daily lives-letting us sense and control our physical surroundings via software. However, with the number of smart devices raises the complexity of understanding and managing all devices and processes within one's vicinity. In this demo we present the HoloFlows mixed reality app, which allows users to directly explore, monitor and control physical devices within their surroundings. Simple workflows can be created between sensors and actuators to automate repetitive tasks in an end-user friendly way. We use gestural embodied interactions combined with real world metaphors to provide an intuitive and easy to learn interactive application for controlling complex CPS.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {495–500},
numpages = {6},
keywords = {workflows, natural interaction, mixed reality, cyber-physical systems},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3157810,
author = {Nomura, Ryota and Unuma, Yuko and Komuro, Takashi and Yamamoto, Shoji and Tsumura, Norimichi},
title = {Mobile Augmented Reality for Providing Perception of Materials},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3157810},
doi = {10.1145/3152832.3157810},
abstract = {In this paper, we propose an AR system using a mobile display that allows the user to manipulate a virtual object using the user's own hand to provide the user with perception of materials. To realize more natural interaction with a virtual object, our proposed system provides visual information that is consistent with the user's somatic sensation by displaying user-perspective images on the mobile display. The system allows the user to manipulate the virtual object with six degrees of freedom (three degrees of freedom for translation and three degrees of freedom for rotation) by mapping the hand pose to the object's translation and rotation. Material appearance of objects is presented by rendering the virtual object using Unity 3D. We showed the effectiveness of our proposed system by showing how the user manipulates the virtual object and obtains the gloss and burnish of the object using the proposed system.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {501–506},
numpages = {6},
keywords = {mobile device, object manipulation, material appearance, visual and somatic sensation, depth camera},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3157809,
author = {Fastnacht, Till and Fischer, Patrick Tobias and Hornecker, Eva and Zierold, Sabine and Aispuro, Abraham Ornelas and Marschall, Johannes},
title = {The Hedonic Value of Sonnengarten: Touching Plants to Trigger Light},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3157809},
doi = {10.1145/3152832.3157809},
abstract = {Sonnengarten is an interactive light installation, controlled by touching plants. It was developed for an urban festival, with the aim of increasing attractiveness of a courtyard and passageway. Light patterns were varied over the course of the festival. Requiring prolonged touch for a more complex light reaction increased interaction duration compared to the initial 1-step process and resulted in the installation being rated higher in hedonic quality in AttrakDiff questionnaires.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {507–514},
numpages = {8},
keywords = {media architecture, interactive lighting, urban HCI},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3157813,
author = {Mai, Christian and Rambold, Lukas and Khamis, Mohamed},
title = {TransparentHMD: Revealing the HMD User's Face to Bystanders},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3157813},
doi = {10.1145/3152832.3157813},
abstract = {While the eyes are very important in human communication, once a user puts on a head mounted display (HMD), the face is obscured from the outside world's perspective. This leads to communication problems when bystanders approach or collaborate with an HMD user. We introduce transparentHMD, which employs a head-coupled perspective technique to produce an illusion of a transparent HMD to bystanders. We created a self contained system, based on a mobile device mounted on the HMD with the screen facing bystanders. By tracking the relative position of the bystander using the smartphone's camera, we render an adapting perspective view in realtime that creates the illusion of a transparent HMD. By revealing the user's face to bystanders, our easy to implement system allows for opportunities to investigate a plethora of research questions particularly related to collaborative VR systems.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {515–520},
numpages = {6},
keywords = {virtual reality, head-mounted displays, eyes, gaze, face},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156612,
author = {Hasbani, Cynthia-\"{e}l and Khoury, Sandra and Berkel, Isma\"{\i}l and K\"{u}hnle, Florian and Rohrmoser, Claudia and Wolf, Katrin},
title = {Chaos/Order//Chaos: An Interactive Multimedia Sculpture},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156612},
doi = {10.1145/3152832.3156612},
abstract = {This work is motivated by the believe that the perpetual search for order in life is to search for harmony and completion. Order is born from the success of one random combination in a sea of several chaotic possibilities, but achieving it only opens the door to another chaotic situation. Chaos/Order//Chaos is a multimedia installation using video projection on a slit canvas which is working with effects caused by the distortion of the image through changing perception. The installation creates an experience of transitions from chaos to order in an infinite loop.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {521–525},
numpages = {5},
keywords = {anamorphosis, video art, installation, project mapping},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156608,
author = {Wodiczko, Krzysztof and von der Heide, Anke and Fischer, Patrick Tobias and Burkhardt, Timm and M\"{u}ller, Roy and Brugnoli, Lea and Ghatasheh, Hala and Schuster, Sebastian and Paffrath, Christian and Ali, Muzaffar and Eisenberg, Mark and Ohaya, Michael and Schlamp, Christian Rene Manzano and Leroy, David and Hornecker, Eva and Sattler, Wolfgang and Bachhuber, Liz},
title = {Die Ermittler: A Dialogue about Displacement, Refuge, and Home},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156608},
doi = {10.1145/3152832.3156608},
abstract = {The Goethe-Schiller monument at the centre of Weimar's Theatre Square is a tourist attraction that embodies German national cultural heritage. In summer 2016, an interactive participatory live projection mapping lent refugees in Weimar a voice by projecting them onto the statues of German poets Goethe and Schiller and to engage Weimar citizens in a dialogue.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {527–533},
numpages = {7},
keywords = {video, participatory art, media architecture},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156609,
author = {Honauer, Michaela and Wiegert, Christian and Sohaib, Tahira and Monsalve, Fernando C\'{a}rdenas and Zhao, Jing and Effenberg, Maike Alisha and Alshomary, Milad and Hornecker, Eva},
title = {Mermaids Do Not Exist? Interactive Costumes Do!},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156609},
doi = {10.1145/3152832.3156609},
abstract = {We here present two interactive costumes deployed for a fairy tale performance of a children and youth ballet. These are the costumes of two underwater world characters, a Jellyfish and a Seahorse. Both costumes sense the performers' motion and decode them to an individual on-body light concept, underlining the role's expressivity while respecting the overall appearance of the show. A major prospect was to develop the interaction concept according to the choreographic work that evolved simultaneously. Finally, the artistic statement of the costume creators and the choreography are integrated concurrently, and resulted in a harmonious and aesthetically pleasant performance.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {535–540},
numpages = {6},
keywords = {e-textiles, ballet performance, interactive costumes},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156610,
author = {Maurer, Bernhard and Van Rheden, Vincent and Murer, Martin and Krischkowsky, Alina and Tscheligi, Manfred},
title = {Reign in Blood: Exploring Blood as a Material for Game Interaction Design},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156610},
doi = {10.1145/3152832.3156610},
abstract = {Reign in Blood is an interactive art piece based on intentionally breaking touch interaction for gameplay purposes (i.e., making a game increasingly harder to play by dripping blood on a touch screen). It explores how in-game events can be translated into the real world, further bridging the digital/physical divide in touch-based interaction by exploring alternative forms of interacting with existing technology and extending the notion of tangible interactions. We report on reactions the game created during a public art exhibit and reflect on insights we gathered from the design process.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {541–547},
numpages = {7},
keywords = {blood, game design, interaction design, critical design},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156611,
author = {Livia, Claudia and Wolf, Katrin},
title = {Whisper: Sensing and Enciphering Secrets through a Kinetic Installation},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156611},
doi = {10.1145/3152832.3156611},
abstract = {the installation Whisper is transforming spoken secrets into motion and light reflections. The thereby created visualization of the secrets is not understandable using common communication modalities. As such, Whisper is creating a magic moment of information translation into a language of motion and light. Whisper allows visitors to see the whispering of a secret without hearing it, and therefore it allows the secret to keep its most important feature, not to be revealed. As such, Whisper enables an engagement of the public with their hidden intimate part and thus, it is creating moments of intimacy and playful beauty.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {549–553},
numpages = {5},
keywords = {light choreography, encipher, kinetic sculpture, installation},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152860,
author = {Bolock, Alia El and Salah, Jailan and Abdennadher, Slim and Abdelrahman, Yomna},
title = {Character Computing: Challenges and Opportunities},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152860},
doi = {10.1145/3152832.3152860},
abstract = {Systems that adapt to the states of users show great potential for creating novel user experiences. However, most approaches in affective computing only rely on the current state of the user. Furthermore, personality computing has recently evolved, it shows a few examples of how personality traits can be inferred from different types of behavioral evidence. We envision introducing novel user experiences by learning more about the user through extracting the building blocks of their character including; morals, mentalities and believes. In this workshop we aim to introduce Character Computing, a novel approach to examine and leverage characters to build ubiquitous and non-obtrusive character-aware adaptive systems. This workshop aims at engaging participants in reflection and practical work exploring the evolving research strand in hand.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {555–559},
numpages = {5},
keywords = {character computing, personality computing, affective computing},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3152840,
author = {Weber, Dominik and Voit, Alexandra and Exler, Anja and Schr\"{o}der, Svenja and B\"{o}hmer, Matthias and Okoshi, Tadashi},
title = {Intelligent Notification and Attention Management on Mobile Devices},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3152840},
doi = {10.1145/3152832.3152840},
abstract = {Today, many users of mobile devices are continuously confronted with a huge variety of information: notifications from Facebook, new application updates, won badges, or reminders. This leads to an information overload, which makes it hard to stay focused. This workshop will investigate approaches towards smart attention management systems. We will discuss the fundamental challenges of smart notifications and the design of proactive notification mechanisms. We invite submissions that focus on the understanding of users and their current, mobile information handling. We further appreciate contributions that propose design concepts for the interaction with smart attention management systems. The expected workshop outcome is a summary of emerging challenges in the design and development of smart attention management systems as well as approaches to address them.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {561–565},
numpages = {5},
keywords = {ubiquitous computing, multimodal interaction, smart systems, attention management, mobile devices},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156559,
author = {Le, Huy Viet and Mayer, Sven and Henze, Niels},
title = {Machine Learning with Tensorflow for Mobile and Ubiquitous Interaction},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156559},
doi = {10.1145/3152832.3156559},
abstract = {Due to the increasing amount of sensors integrated into the environment and worn by the user, a sheer amount of context-sensitive data become available. While interpreting them with traditional methods (e.g., formulas and simple heuristics) is challenging, the latest machine learning techniques require only a set of labeled data. TensorFlow is an open-source library for machine learning which implements a wide range of neural network models. With TensorFlow Mobile, researchers and developers can further deploy the trained models on low-end mobile devices for ubiquitous scenarios. This facilitates the model export and offers techniques to optimize the model for a mobile deployment. In this tutorial, we teach attendees two basic steps to a deployment of neural networks on smartphones: Firstly, we will teach how to develop neural network architectures and train them in TensorFlow. Secondly, we show the process to run the trained models on a mobile phone using TensorFlow Mobile.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {567–572},
numpages = {6},
keywords = {tensorflow, machine learning, classification, supervised learning, mobile device, ubiquitous computing},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3156560,
author = {Glatz, Christiane and Ditz, Jonas and Kosch, Thomas and Schmidt, Albrecht and Lahmer, Marie and Chuang, Lewis L.},
title = {Reading the Mobile Brain: From Laboratory to Real-World Electroencephalography},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3156560},
doi = {10.1145/3152832.3156560},
abstract = {It is increasingly viable to measure the brain activity of mobile users, as they go about their everyday business in their natural world environment. This is due to: (i) modern signal processing methods, (ii) lightweight and cost-effective measurement devices, and (iii) a better, albeit incomplete, understanding of how measurable brain activity relates to mental processes. Here, we address how brain activity can be measured in mobile users and how this contrasts with measurements obtained under controlled laboratory conditions. In particular, we will focus on electroencephalography (EEG) and will cover: (i) hardware and software implementation, (ii) signal processing techniques, (iii) interpretation of EEG measurements. This will consist of hands-on analyses of real EEG data and a basic theoretical introduction to how and why EEG works.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {573–579},
numpages = {7},
keywords = {mental workload, neuroergonomics, signal processing, EEG, notifications},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3157807,
author = {Haque, Md Sanaul},
title = {Persuasive Applications for the Healthy Lifestyle},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3157807},
doi = {10.1145/3152832.3157807},
abstract = {People spend a large portion of time at their workplace and henceforth, need actions to recharge their mind and body. Therefore, it is important to highlight on health promotion activities in the workplace that focus on changing personal health behaviours such as diet. Research has revealed that physical activities increase work efficiency [1] and planned naps taken during work keep suitable levels of waking function for day-time and night-time work [2]. Most relevant research has been done without considering the theoretical concept and empirical approach. This study aims to develop persuasive applications incorporating psychological theories to combine these implicit measures such as consuming a healthy diet, physical activity and napping at the workplace. The quantitative and qualitative approach will be used to empirically measuring the effectiveness of the applications in the context of employees' health behaviour change. It is hoped that users will benefit from improved health using the applications.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {581–586},
numpages = {6},
keywords = {psychological theories, persuasive applications, user-centered design, empirical analysis},
location = {Stuttgart, Germany},
series = {MUM '17}
}

@inproceedings{10.1145/3152832.3157806,
author = {Kettner, Romina},
title = {Sensing Stress and Emotions: Towards the Development of an User-Adaptive System},
year = {2017},
isbn = {9781450353786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152832.3157806},
doi = {10.1145/3152832.3157806},
abstract = {Stress is one of the main problems affecting humans' health all over the world. Each individual can relate to stressful situations since it is a everyday phenomenon, no matter if chronic stress or acute stress. Recent work investigated that stress is often caused by information overload or for example too many smartphone notifications. Thus, my research focuses on the detection of stress using physiological parameters such as heart beat, electrodermal activity and skin temperature. Moreover, I am exploring techniques that help individuals to cope with stress which also involves the development of user-adaptive systems.},
booktitle = {Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia},
pages = {587–590},
numpages = {4},
keywords = {affective computing, emotions, stress},
location = {Stuttgart, Germany},
series = {MUM '17}
}

