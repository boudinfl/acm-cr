@inproceedings{10.1145/2541831.2541845,
author = {Bergstrom-Lehtovirta, Joanna and Eklund, Tommy and Jylh\"{a}, Antti and Kuikkaniemi, Kai and An, Chao and Jacucci, Giulio},
title = {BubblesDial: Exploring Large Display Content Graphs on Small Devices},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541845},
doi = {10.1145/2541831.2541845},
abstract = {Large interfaces are fixed to a certain use context, for example to a physical smart space. Mobile counterparts of public interfaces can allow user to continue interacting with the content also when leaving the space. However, wall applications make use of a large display surface and fitting the same user interface to the constraints of a mobile screen is challenging. Starting with Bubble Wall -- an information exploration application for multitouch walls -- we developed interfaces for browsing the same content graphs on mobile devices. A comparison study in exploration and navigation tasks was conducted with two mobile interfaces reproducing the large screen interactions: BubbleSpace with a more faithful redesign and BubblesDial reducing the interactions for better fit to a small screen. The BubblesDial scored significantly better in usability and performance evaluation, especially when priming with use of the Bubble Wall. We also present implications for the redesign of these large content graph interactions for mobile use.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {1},
numpages = {10},
keywords = {information visualization, visualizing graphs, mobile interfaces, multitouch},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541835,
author = {Ostkamp, Morin and H\"{u}lsermann, Jonas and Kray, Christian and Bauer, Gernot},
title = {Using Mobile Devices to Enable Visual Multiplexing on Public Displays: Three Approaches Compared},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541835},
doi = {10.1145/2541831.2541835},
abstract = {Public displays have become ubiquitous in many places regularly visited by large numbers of people (e.g., traffic hubs or malls). In addition to advertising they often provide information related to the location (e.g., time tables). However, individuals can have difficulties to find information relevant to them -- either due to information overload or lack of personalization. Multiplexing, as defined in information theory, can help to address this issue by increasing the number of available channels. We propose three methods for visual multiplexing, and report on a controlled lab-based comparison study. Our results indicate that visual multiplexing via mobile devices can be a feasible solution to provide personalized multimedia content on public displays, and that the three methods tested differ in terms of performance. We found that the content type shown has an impact on which method works best, and that self-reported workload differed according to content type and multiplexing method.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {2},
numpages = {10},
keywords = {CDM, visual multiplexing, public display, TDM, FDM},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541842,
author = {Schaefers, Klaus and Ribeiro, David and de Barros, Ana Correia},
title = {Beyond Heat Maps: Mining Common Swipe Gestures},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541842},
doi = {10.1145/2541831.2541842},
abstract = {Heat maps are a common tool in research to visualize human-computer-interaction. Despite being widely used to track navigation, clicks, cursor moves or eye gaze, heat maps have not yet been explored as a means to understand users' gestural interaction with mobile devices. This understanding is particularly relevant in the case of older adult users who are often novice users and may also struggle with accuracy in gesture performance. This paper explores the application of the DBScan clustering algorithm to uncover the most relevant swipe gestures in a data sets containing the user interaction of two mobile applications. An intuitive visualization of the clustering results will be presented and compared in a case study with a heat map visualization, discussing the novelty and usefulness of these visualizations for user behaviour and usability studies.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {3},
numpages = {4},
keywords = {user studies, mobile devices, gesture clustering, gestures, heat maps},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541851,
author = {Kerber, Frederic and Lessel, Pascal and Mauderer, Michael and Daiber, Florian and Oulasvirta, Antti and Kr\"{u}ger, Antonio},
title = {Is Autostereoscopy Useful for Handheld AR?},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541851},
doi = {10.1145/2541831.2541851},
abstract = {Some recent mobile devices have autostereoscopic displays that enable users to perceive stereoscopic 3D without lenses or filters. This might be used to improve depth discrimination of objects overlaid to a camera viewfinder in augmented reality (AR). However, it is not known if autostereoscopy is useful in the viewing conditions typical to mobile AR. This paper investigates the use of autostereoscopic displays in an psychophysical experiment with twelve participants using a state-of-the-art commercial device. The main finding is that stereoscopy has a negligible if any effect on a small screen, even in favorable viewing conditions. Instead, the traditional depth cues, in particular object size, drive depth discrimination.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {4},
numpages = {4},
keywords = {empirical and quantitative user study, mobile devices, augmented reality, autostereoscopy, depth discrimination},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541854,
author = {H\"{a}kkil\"{a}, Jonna and Posti, Maaret and Vent\"{a}-Olkkonen, Leena and Koskenranta, Olli and Colley, Ashley},
title = {Mobile Photo Sharing through Collaborative Space in Stereoscopic 3D},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541854},
doi = {10.1145/2541831.2541854},
abstract = {Mobile user interfaces (UIs) utilizing stereoscopic 3D have so far been driven predominantly by visual design, and when interaction design has been considered, the focus has been on single user use cases. In this paper, we introduce a novel way to utilize stereoscopy in a mobile user interface in a collaborative task. We define a shared space, appearing below the screen level, which is used in a collaborative task of sharing mobile phone photos. Two users are able to share content by moving items between the private layer, at zero parallax, and shared layer, at positive parallax. We developed the application through an iterative design process including two user studies (n=27, n=19). We report positive evaluation results especially in regard of the utilitarian aspects of the UI design.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {5},
numpages = {4},
keywords = {stereoscopic user interfaces, photos, mobile devices, CSCW},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541834,
author = {Cardoso, Jorge C. S. and Jos\'{e}, Rui},
title = {Evaluation of a Programming Toolkit for Interactive Public Display Applications},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541834},
doi = {10.1145/2541831.2541834},
abstract = {Interaction is repeatedly pointed out as a key enabling element towards more engaging and valuable public displays. Still, most digital public displays today do not support any interactive features. We argue that this is mainly due to the lack of efficient and clear abstractions that developers can use to incorporate interactivity into their applications. As a consequence, interaction represents a major overhead for developers, and users are faced with inconsistent interaction models across different displays. This paper describes the results of the evaluation of a widget toolkit for generalized interaction with public displays. Our toolkit was developed for web-based applications and it supports multiple interaction mechanisms, automatically generated graphical interfaces, asynchronous events and concurrent interaction. We have evaluated the toolkit along various dimensions - system performance, API usability, and real-world deployment - and we present and discuss the results in this paper.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {6},
numpages = {10},
keywords = {public display applications, toolkit, interaction abstraction},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541840,
author = {Keskinen, Tuuli and Hakulinen, Jaakko and Heimonen, Tomi and Turunen, Markku and Sharma, Sumita and Miettinen, Toni and Luhtala, Matti},
title = {Evaluating the Experiential User Experience of Public Display Applications in the Wild},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541840},
doi = {10.1145/2541831.2541840},
abstract = {Studying pervasive systems in the wild has recently gained significant interest. However, few methods exist that focus on the subjective of user experience of such systems rather than objective metrics, like performance and task success. Especially multimodal interaction in this context poses challenges to understanding how different input and output methods affect the users' experience. We present a new method for evaluating the experiential user experience of interactive systems. It combines two existing approaches from different fields: a questionnaire-based evaluation method called SUXES, intended for evaluating user expectations and experiences, and a theoretical experience framework, Experience Pyramid, originally developed for analyzing and improving experiential tourism products. The new method was used in two field studies of multimodal public display applications. Our findings show that the method is a practical approach for user experience evaluation in the wild, especially in the case of pervasive applications that aim to provide novel experiences rather than facilitate task-oriented information access.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {7},
numpages = {10},
keywords = {user experience, measurement, public displays, in situ evaluation},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541836,
author = {Rader, Markus and Holzmann, Clemens and Rukzio, Enrico and Seifert, Julian},
title = {MobiZone: Personalized Interaction with Multiple Items on Interactive Surfaces},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541836},
doi = {10.1145/2541831.2541836},
abstract = {Current interactive surfaces do not support user identification. Hence, personalized applications that consider user-specific access control are not possible. Diverse approaches for identifying and distinguishing users have been investigated in previous research. Token-based approaches -- e.g., which utilize the user's mobile phone -- are especially promising, as they also allow for consideration of the user's personal digital context (e.g., stored messages, contacts, or media data). However, existing interaction techniques are limited regarding their ability to enable users to manipulate (e.g., select or copy) multiple items at the same time, as they are cumbersome when the number of files exceeds a certain amount. We present MobiZone, a technique that enables users to interact with large numbers of items on an interactive surface while enabling personalized access by using the mobile phone as a token. MobiZone provides a spatial zone that can be positioned, resized and associated with any action according to the user's needs; items enclosed by the zone can be manipulated simultaneously. We present three interaction techniques (FlashLight&amp;Control, Remote&amp;Control, and Place&amp;Control) that enable users to control the zone. Additionally, we report the results of a comparative user study in which we compared the different interaction techniques for MobiZone. The results indicate that users are fastest with Remote&amp;Control, and they also rated Remote&amp;Control slightly higher than the other techniques.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {8},
numpages = {10},
keywords = {spatial control, mobile phone, interactive surface},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541860,
author = {Palviainen, Jarmo and Kuusinen, Kati and V\"{a}\"{a}n\"{a}n\"{a}nen-Vainio-Mattila, Kaisa},
title = {Designing for Presence in Social Television Interaction},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541860},
doi = {10.1145/2541831.2541860},
abstract = {In the past years, people have started to use social media to interact actively about TV content. However, despite of over a decade of active research and product development, Social TV has not been adopted by large populations. This paper aims to support designing interaction for Social TV services and, more specifically, designing for the presence and togetherness between viewers. Our constructive research consisted of a series of user studies and iterative prototyping. We conducted user studies with altogether 51 participants both in laboratory and in real-life contexts. To support the presence, the final prototype includes three different modalities of communication -- voice and text-based chat, and animated gestures with avatars. The qualitative findings imply that gestures with avatars in virtual space support Social TV and the experience of presence. Finally, we present an analysis of Social TV heuristics and their validity in the context of our designs.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {9},
numpages = {10},
keywords = {presence, voice chat, text chat, focus group, interaction design, emoticons, gestures, social television, field test},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541848,
author = {Simeone, Adalberto L. and Seifert, Julian and Schmidt, Dominik and Holleis, Paul and Rukzio, Enrico and Gellersen, Hans},
title = {A Cross-Device Drag-and-Drop Technique},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541848},
doi = {10.1145/2541831.2541848},
abstract = {Many interactions naturally extend across smart-phones and devices with larger screens. Indeed, data might be received on the mobile but more conveniently processed with an application on a larger device, or vice versa. Such interactions require spontaneous data transfer from a source location on one screen to a target location on the other device. We introduce a cross-device Drag-and-Drop technique to facilitate these interactions involving multiple touchscreen devices, with minimal effort for the user. The technique is a two-handed gesture, where one hand is used to suitably align the mobile phone with the larger screen, while the other is used to select and drag an object between devices and choose which application should receive the data.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {10},
numpages = {4},
keywords = {mobile devices, drag-and drop, data transfer},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541844,
author = {S\"{o}r\"{o}s, G\'{a}bor and Fl\"{o}rkemeier, Christian},
title = {Blur-Resistant Joint 1D and 2D Barcode Localization for Smartphones},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541844},
doi = {10.1145/2541831.2541844},
abstract = {With the proliferation of built-in cameras barcode scanning on smartphones has become widespread in both consumer and enterprise domains. To avoid making the user precisely align the barcode at a dedicated position and angle in the camera image, barcode localization algorithms are necessary that quickly scan the image for possible barcode locations and pass those to the actual barcode decoder. In this paper, we present a barcode localization approach that is orientation, scale, and symbology (1D and 2D) invariant and shows better blur invariance than existing approaches while it operates in real time on a smartphone. Previous approaches focused on selected aspects such as orientation invariance and speed for 1D codes or scale invariance for 2D codes. Our combined method relies on the structure matrix and the saturation from the HSV color system. The comparison with three other real-time barcode localization algorithms shows that our approach outperforms the state of the art with respect to symbology and blur invariance at the expense of a reduced speed.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {11},
numpages = {8},
keywords = {localization, barcode, visual codes, pattern recognition},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541866,
author = {Oleksik, Gerard and Jetter, Hans-Christian and Gerken, Jens and Milic-Frayling, Natasa and Jones, Rachel},
title = {Towards an Information Architecture for Flexible Reuse of Digital Media},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541866},
doi = {10.1145/2541831.2541866},
abstract = {Our research is concerned with designing support for ubiquitous access, organization, and interpretation of digital assets that users produce and store across multiple devices and computing platforms. Through a co-design study with scientists we identified specific aspects of conceptual maps they wish to create for their projects in order to make sense of diverse collections of digital media related to their individual and collective work. By the use of a technology probe called DeskPiles we observed the mechanisms involved in such a process. We identified the practice of using sub-document elements to compose the views of digital collections and diverse linking strategies to express semantic relations among concepts and ideas and to enable access to supporting digital assets. Our findings reveal the need to expand traditional information architectures and include (1) an extended range of content reference options and (2) transformation services to enable extraction, conversion, and linking of digital content. We advocate a granular and multi-format representation of content as the basis for reuse and ubiquitous access of digital media in diverse multi-device environments.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {12},
numpages = {10},
keywords = {content re-appropriation, digital content assemblies, information architecture, collection views, compositions, cloud architecture},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541856,
author = {Houben, Steven and Nielsen, S\o{}ren and Esbensen, Morten and Bardram, Jakob E.},
title = {NooSphere: An Activity-Centric Infrastructure for Distributed Interaction},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541856},
doi = {10.1145/2541831.2541856},
abstract = {Distributed interaction is a computing paradigm in which the interaction with a computer system is distributed over multiple devices, users and locations. Designing and developing distributed interaction systems is intrinsically difficult as it requires the engineering of a stable infrastructure to support the actual system and user interface. As an approach to this re-engineering problem, we introduce NooSphere, an activity-centric infrastructure and programming framework that provides a set of fundamental distributed services that enables quick development and deployment of distributed interactive systems. In this paper, we describe the requirements, design and implementation of NooSphere and validate the infrastructure by implementing three canonical real deployable applications constructed on top of the NooSphere infrastructure.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {13},
numpages = {10},
keywords = {distributed computing, distributed interaction, activity-centric computing, activity cloud, NooSphere},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541847,
author = {Higuchi, Masakazu and Komuro, Takashi},
title = {AR Typing Interface for Mobile Devices},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541847},
doi = {10.1145/2541831.2541847},
abstract = {We propose a new user interface system for mobile devices. By using augmented reality (AR) technology, the system overlays virtual objects on real images captured by a camera attached to the back of a mobile device, and the user can operate the mobile device by manipulating the virtual objects with his/her hand in the space behind the mobile device. This system allows the user to operate the device in a wide three-dimensional space and to select small objects easily. Also, the AR technology provides the user with a sense of reality in operating the device. We developed a typing application using our system and verified the effectiveness by user studies. The results showed that more than half of the subjects felt that the operation area of the proposed system is larger than that of a smartphone and that both AR and unfixed key-plane are effective for improving typing speed.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {14},
numpages = {8},
keywords = {virtual keyboard, augmented reality, finger tracking},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541846,
author = {Strothoff, Sven and Hinrichs, Klaus},
title = {Adding Context to Multi-Touch Region Selections},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541846},
doi = {10.1145/2541831.2541846},
abstract = {As applications originating on desktop computers find their way onto multi-touch enabled mobile devices many interaction tasks that were designed for computer mice spread to new touch-based environments. One example is region selection, for instance in image editing applications. Several studies already investigated multi-touch object selections however, region selections have not been closely examined.Our proposed selection technique was designed for multi-touch interaction and better suits mobile devices. Taking advantage of multiple touches enables the user to easily extend, modify and refine selections based on the order and relative position---the context---of touches.We were concerned that controlling more degrees of freedom with our technique could negatively impact simple selections. To evaluate its performance we present a user study that compares it to currently used techniques. Results show that our multi-touch region selection represents a good compromise of speed and precision, while providing the possibility to easily refine the selected region.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {15},
numpages = {8},
keywords = {multi-touch interaction, selection technique, context information, selection refinement, region selection},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541833,
author = {Lv, Zhihan and Halawani, Alaa and Lal Khan, Muhammad Sikandar and R\'{e}hman, Shafiq Ur and Li, Haibo},
title = {Finger in Air: Touch-Less Interaction on Smartphone},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541833},
doi = {10.1145/2541831.2541833},
abstract = {In this paper we present a vision based intuitive interaction method for smart mobile devices. It is based on markerless finger gesture detection which attempts to provide a 'natural user interface'. There is no additional hardware necessary for real-time finger gesture estimation. To evaluate the strengths and effectiveness of proposed method, we design two smart phone applications, namely circle menu application - provides user with graphics and smart phone's status information, and bouncing ball game- a finger gesture based bouncing ball application. The users interact with these applications using finger gestures through the smart phone's camera view, which trigger the interaction event and generate activity sequences for interactive buffers. Our preliminary user study evaluation demonstrates effectiveness and the social acceptability of proposed interaction approach.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {16},
numpages = {4},
keywords = {HCI, hand gesture, smart phone games, touch-less, mobile},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541865,
author = {L\"{o}chtefeld, Markus and Hirtz, Christoph and Gehring, Sven},
title = {Evaluation of Hybrid Front- and Back-of-Device Interaction on Mobile Devices},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541865},
doi = {10.1145/2541831.2541865},
abstract = {With the recent trend of increasing display sizes of mobile devices, one-handed interaction has become increasingly difficult when a user wants to maintain a safe grip around the device at the same time. In this paper we evaluate how a combination of hybrid front- and back-of-device touch input can be used to overcome the difficulties when using a mobile device with one hand. Our evaluation shows that, even though such a technique is slower than conventional front-of-device input, it allows for accurate and safe input.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {17},
numpages = {4},
keywords = {hybrid input, large scale mobile displays, mobile, back of the device interaction},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541832,
author = {Nguyen-Dinh, Long-Van and Blanke, Ulf and Tr\"{o}ster, Gerhard},
title = {Towards Scalable Activity Recognition: Adapting Zero-Effort Crowdsourced Acoustic Models},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541832},
doi = {10.1145/2541831.2541832},
abstract = {Human activity recognition systems traditionally require a manual annotation of massive training data, which is laborious and non-scalable. An alternative approach is mining existing online crowd-sourced repositories for open-ended, free annotated training data. However, differences across data sources or in observed contexts prevent a crowd-sourced based model reaching user-dependent recognition rates.To enhance the use of crowd-sourced data in activity recognition, we take an essential step forward by adapting a generic model based on crowd-sourced data to a personalized model. In this work, we investigate two adapting approaches: 1) a semi-supervised learning to combine crowd-sourced data and unlabeled user data, and 2) an active-learning to query the user for labeling samples where the crowd-sourced based model fails to recognize. We test our proposed approaches on 7 users using auditory modality on mobile phones with a total data of 14 days and up to 9 daily context classes. Experimental results indicate that the semi-supervised model can indeed improve the recognition accuracy up to 21% but is still significantly outperformed by a supervised model on user data. In the active learning scheme, the crowd-sourced model can reach the performance of the supervised model by requesting labels of 0.7% of user data only. Our work illustrates a promising first step towards an unobtrusive, efficient and open-ended context recognition system by adapting free online crowd-sourced data into a personalized model.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {18},
numpages = {10},
keywords = {adaptation, ambient sound, personalization, crowdsourcing, semi-supervised learning, activity recognition, mobile phone, active learning},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541859,
author = {Budde, Matthias and El Masri, Rayan and Riedel, Till and Beigl, Michael},
title = {Enabling Low-Cost Particulate Matter Measurement for Participatory Sensing Scenarios},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541859},
doi = {10.1145/2541831.2541859},
abstract = {This paper presents a mobile, low-cost particulate matter sensing approach for the use in Participatory Sensing scenarios. It shows that cheap commercial off-the-shelf (COTS) dust sensors can be used in distributed or mobile personal measurement devices at a cost one to two orders of magnitude lower than that of current hand-held solutions, while reaching meaningful accuracy. We conducted a series of experiments to juxtapose the performance of a gauged high-accuracy measurement device and a cheap COTS sensor that we fitted on a Bluetooth-enabled sensor module that can be interconnected with a mobile phone. Calibration and processing procedures using multi-sensor data fusion are presented, that perform very well in lab situations and show practically relevant results in a realistic setting. An on-the-fly calibration correction step is proposed to address remaining issues by taking advantage of co-located measurements in Participatory Sensing scenarios. By sharing few measurement across devices, a high measurement accuracy can be achieved in mobile urban sensing applications, where devices join in an ad-hoc fashion. A performance evaluation was conducted by co-locating measurement devices with a municipal measurement station that monitors particulate matter in a European city, and simulations to evaluate the on-the-fly cross-device data processing have been done.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {19},
numpages = {10},
keywords = {mobile dust sensor, novel sensing, crowd sensing, air quality, PM10, participatory sensing, urban sensing, PM2.5, wearable, environmental sensing, particulate matter},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541850,
author = {Borazio, Marko and Van Laerhoven, Kristof},
title = {Using Time Use with Mobile Sensor Data: A Road to Practical Mobile Activity Recognition?},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541850},
doi = {10.1145/2541831.2541850},
abstract = {Having mobile devices that are capable of finding out what activity the user is doing, has been suggested as an attractive way to alleviate interaction with these platforms, and has been identified as a promising instrument in for instance medical monitoring. Although results of preliminary studies are promising, researchers tend to use high sampling rates in order to obtain adequate recognition rates with a variety of sensors. What is not fully examined yet, are ways to integrate into this the information that does not come from sensors, but lies in vast data bases such as time use surveys. We examine using such statistical information combined with mobile acceleration data to determine 11 activities. We show how sensor and time survey information can be merged, and we evaluate our approach on continuous day-and-night activity data from 17 different users over 14 days each, resulting in a data set of 228 days. We conclude with a series of observations, including the types of activities for which the use of statistical data has particular benefits.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {20},
numpages = {10},
keywords = {wearable computing, time use surveys, probability model, mobile devices, activity recognition},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541852,
author = {Zhu, Zack and Blanke, Ulf and Calatroni, Alberto and Tr\"{o}ster, Gerhard},
title = {Human Activity Recognition Using Social Media Data},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541852},
doi = {10.1145/2541831.2541852},
abstract = {Human activity recognition is a core component of context-aware, ubiquitous computing systems. Traditionally, this task is accomplished by analyzing signals of wearable motion sensors. While such signals can effectively distinguish various low-level activities (e.g. walking or standing), two issues exist: First, high-level activities (e.g. watching movies or attending lectures) are difficult to distinguish from motion data alone. Second, instrumentation of complex body sensor network at population scale is impractical. In this work, we take an alternative approach of leveraging rich, dynamic, and crowd-generated self-report data as the basis for in-situ activity recognition. By treating the user as the "sensor", we make use of implicit signals emitted from natural use of mobile smart-phones. Applying an L1-regularized Linear SVM on features derived from textual content, semantic location, and time, we are able to infer 10 meaningful classes of daily life activities with a mean accuracy of up to 83.9%. Our work illustrates a promising first step towards comprehensive, high-level activity recognition using free, crowd-generated, social media data.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {21},
numpages = {10},
keywords = {crowd sensing, activity recognition, web mining},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541864,
author = {Sanchez-Cortes, Dairazalia and Biel, Joan-Isaac and Kumano, Shiro and Yamato, Junji and Otsuka, Kazuhiro and Gatica-Perez, Daniel},
title = {Inferring Mood in Ubiquitous Conversational Video},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541864},
doi = {10.1145/2541831.2541864},
abstract = {Conversational social video is becoming a worldwide trend. Video communication allows a more natural interaction, when aiming to share personal news, ideas, and opinions, by transmitting both verbal content and nonverbal behavior. However, the automatic analysis of natural mood is challenging, since it is displayed in parallel via voice, face, and body. This paper presents an automatic approach to infer 11 natural mood categories in conversational social video using single and multimodal nonverbal cues extracted from video blogs (vlogs) from YouTube. The mood labels used in our work were collected via crowdsourcing. Our approach is promising for several of the studied mood categories. Our study demonstrates that although multimodal features perform better than single channel features, not always all the available channels are needed to accurately discriminate mood in videos.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {22},
numpages = {9},
keywords = {nonverbal behavior, moods, verbal content, sentiment analysis},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541839,
author = {de Barros, Ana Correia and Cevada, Jo\~{a}o and Bay\'{e}s, \`{A}ngels and Alcaine, Sheila and Mestre, Berta},
title = {User-Centred Design of a Mobile Self-Management Solution for Parkinson's Disease},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541839},
doi = {10.1145/2541831.2541839},
abstract = {Parkinson's disease (PD) is a highly prevalent and disabling condition, requiring frequent medication adjustments. In parallel, non-adherence to medical treatment might lead to severe consequences. Therefore, a solution to monitor PD symptoms, allowing neurologists to make informed decisions about medication adjustments, and one which could promote medical treatment adherence would be beneficial for both the patient and the medical doctor. In this paper we present the rationale and user-centred process for the design of four smartphone applications for the self-management of PD. We present the methods for evaluation and the results of usability tests. The results show that user-centred methods were efficient and that people with PD were able to achieve high task completion rates on usability tests with three of the applications for PD self-management. Future work should focus on detailed improvement of touch screen sensitivity to optimize error prevention.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {23},
numpages = {10},
keywords = {user interfaces, usability, smartphone applications, Parkinson's disease, design patterns, disease management, design process},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541862,
author = {\r{A}kerman, Panu and Puikkonen, Arto},
title = {Losing Your Creativity - Storytelling Comparison between Children and Adolescents},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541862},
doi = {10.1145/2541831.2541862},
abstract = {In this paper we study pico projector based storytelling among adolescents. We compare the results of our user study of 17 students to the results of our earlier study among young children. Our main focus was on creativity, playfulness and fun as well as on the ubiquitous nature of the technology and use of environment. The comparison highlighted interesting differences. The nature of creativity seems to be changing, but the sources of fun and playfulness still share similarities. Groups also utilize surroundings and the ubiquitous nature of the technology in a slightly different manner. The perceived capabilities of the provided technology also had a more profound effect on the adolescents, even to the extent of it restricting their creativity.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {24},
numpages = {4},
keywords = {creativity, pico projector, fun, playfulness, storytelling, field trial},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541858,
author = {Chamberlain, Alan and Griffiths, Chloe},
title = {Moths at Midnight: Design Implications for Supporting Ecology-Focused Citizen Science},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541858},
doi = {10.1145/2541831.2541858},
abstract = {This paper presents some initial findings, which form a set of design implications from a study that relates to the increasingly popular activity of people setting up moth traps in private gardens. Moth trapping can either be done by an individual or a small group and involves setting a trap that will safely catch moths overnight. The trap is opened in the morning and the contents identified and recorded. This information is usually reported to the local records centre (LRC). This research is based on a rapid ethnographic study and interviews, which demonstrate a series of intervention points that would augment this branch of citizen science, (also known as crowd-sourced science) where mobile ubiquitous technology could both support the fore-mentioned activity and enhance the user's experience. These points relate to: the identification of species; habitat; flight season; verification; learning; reporting; and associated social information sharing.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {25},
numpages = {4},
keywords = {design, ethnography, crowd sourcing, ecology, ubicomp, citizen science, mobile computing},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541843,
author = {Costa, Pedro Maur\'{\i}cio and Vasalou, Asimina and Pitt, Jeremy and Galv\~{a}o, Teresa and e Cunha, Jo\~{a}o Falc\~{a}o},
title = {The Railway Blues: Affective Interaction for Personalised Transport Experiences},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541843},
doi = {10.1145/2541831.2541843},
abstract = {The convergence of personal devices, pervasive communication networks and remote computing has caused a fundamental shift in the user interaction paradigm. Multiple methods have enabled an implicit loop of interaction that goes beyond the traditional graphical interfaces. Human emotion is one of such dimensions, supporting the development of empathic systems. Thus, quality of user experience, a subjective measure, may be defined as the resulting affective state from an interaction, which can be dynamically assessed. In mobile ubiquitous settings, leveraging this affective interaction for providing personalisation and immersive digital services has the potential to significantly impact user experience. This paper investigates the relationship between user affect and experience in the context of urban public transport.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {26},
numpages = {4},
keywords = {urban mobility, quality of user experience, affective interaction},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541853,
author = {Vent\"{a}-Olkkonen, Leena and Posti, Maaret and H\"{a}kkil\"{a}, Jonna},
title = {Early Perceptions of an Augmented Home},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541853},
doi = {10.1145/2541831.2541853},
abstract = {In this paper, we focus on charting future ubiquitous computing use cases utilizing mixed reality (MR) in domestic environments, which has so far been an unexplored domain for MR. We describe our early user research, where during one week participants from 12 households were asked to brainstorm and assess their perceptions of imaginary mixed reality in their homes. During the one-week period this resulted in 167 user generated concept ideas. We focus on the thematic findings that emerged from the collected material, and describe how people most often related the potential of MR concepts with applications related to wellbeing, home-related information, communication and entertainment.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {27},
numpages = {4},
keywords = {user studies, concept creation, augmented reality},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541837,
author = {Schneider, Mark and Scherp, Ansgar and Hunz, Jochen},
title = {A Comparative User Study of Faceted Search in Large Data Hierarchies on Mobile Devices},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541837},
doi = {10.1145/2541831.2541837},
abstract = {We compare two approaches for exploring large, hierarchical data spaces on mobile devices using facets. While the first approach arranges the facets in a 3x3 grid, the second approach makes use of a scrollable list of facets for exploring the data. As concrete scenario, we use category hierarchies that are dynamically obtained from different, distributed social media sources. We have conducted a between-group experiment of the two approaches with 64 subjects (41 male, 23 female) executing the same set of tasks of typical mobile users' information needs. The results show that subjects using the grid-based approach require significantly more time and more clicks for completing the tasks. However, regarding the subjects' satisfaction there is no significant difference between the two approaches. Thus, if efficiency is not the primary objective, the grid-based approach might be of interest as it provides a navigation element that allows the users to see the exact position in the data space. This might be a very useful feature in scenarios where knowing the exact position is quite crucial such as browsing classification schemas in digital libraries or other formal taxonomies.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {28},
numpages = {10},
keywords = {category hierarchies, FaThumb, faceted search},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541838,
author = {Sakamoto, Mizuki and Nakajima, Tatsuo},
title = {<i>Micro-Crowdfunding</i>: Achieving a Sustainable Society through Economic and Social Incentives in Micro-Level Crowdfunding},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541838},
doi = {10.1145/2541831.2541838},
abstract = {This paper introduces a new approach, named micro-crowdfunding, for motivating people to participate in achieving a sustainable society. Increasing people's awareness of how they participate in maintaining the sustainability of common resources, such as public sinks, toilets, shelves, and office areas, is central to achieving a sustainable society. Micro-crowdfunding, as proposed in the paper, is a new type of community-based crowdsourcing architecture that is based on the crowdfunding concept and uses the local currency idea as a tool for encouraging people who live in urban environments to increase their awareness of how important it is to sustain small, common resources through their minimum efforts. Because our approach is lightweight and uses a mobile phone, people can participate in micro-crowdfunding activities with little effort anytime and anywhere.We present the basic concept of micro-crowdfunding and a prototype system. We also describe our experimental results, which show how economic and social factors are effective in facilitating micro-crowdfunding. Our results show that micro-crowdfunding increases the awareness about social sustainability, and we believe that micro-crowdfunding makes it possible to motivate people for achieving a sustainable society.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {29},
numpages = {10},
keywords = {internet of things, virtual economy, local currency, mechanism design, sustainability, social media, crowdfunding, community, crowdsourcing},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541863,
author = {Roalter, Luis and Kranz, Matthias and M\"{o}ller, Andreas and Diewald, Stefan and Stockinger, Tobias and Koelle, Marion and Lindemann, Patrick},
title = {Visual Authentication: A Secure Single Step Authentication for User Authorization},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541863},
doi = {10.1145/2541831.2541863},
abstract = {User authentication on publicly exposed terminals with established mechanisms, such as typing the credentials on a virtual keyboard, can be insecure e.g. due to shoulder surfing or due to a hacked terminal. In addition, username and password entry can be time-consuming and thus improvable with relation to usability. As security and comfort are often competing with each other, novel authentication and authorization methods especially for public terminals are desirable. In this paper, we present an approach on a distributed authentication and authorization system, where the user can be easily identified and enabled to use a service with his smartphone. The smartphone (as personal and private device the user is always in control of) can provide a highly secure authentication token that is renewed and exchanged in the background without the user's participation. The claimed improvements were supported by a user survey with an implementation of a digital room management system as an example for a public display. The proposed authentication procedure would increase security and yet enable fast authentication within publicly exposed terminals.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {30},
numpages = {4},
keywords = {public displays, authentication, smartphones, intelligent environment, authorization, security},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541855,
author = {Shirokura, Takumi and Munekata, Nagisa and Ono, Tetsuo},
title = {AffectiView: Mobile Video Camera Application Using Physiological Data},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541855},
doi = {10.1145/2541831.2541855},
abstract = {In recent years, devices such as smartphones have made it increasingly easier for people to capture videos and then share those videos on Social Networking Services. Shared videos are able of representing almost all emotional experiences, such as interest, astonishment and excitement. However, it is difficult for people who have little knowledge concerning professional video shooting to express these experiences using only video. To address this issue, we developed AffectiView -- a mobile video camera application that captures users' affective responses while they are capturing videos, and also provides a way for sharing that data with other users. In this System, affective responses are measured using physiological proxies. We have organized into three styles the representation of physiological signals between users found in prior works. This system provides video shooters' emotional experiences to viewers by applying each of these three styles to the captured video. A preliminary user study has revealed the positive effects of capturing and sharing physiological signals together with video.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {31},
numpages = {4},
keywords = {physiological computing, affective computing, mobile video experience, skin conductance level},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541867,
author = {Polacek, Ondrej and Grill, Thomas and Tscheligi, Manfred},
title = {NoseTapping: What Else Can You Do with Your Nose?},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541867},
doi = {10.1145/2541831.2541867},
abstract = {Touch-screen interfaces on smart devices became ubiquitous in our everyday lives. In specific contextual situations, capacitive touch interfaces used on current mobile devices are not accessible when, for example, wearing gloves during a cold winter. Although the market has responded by providing capacitive styluses or touchscreen-compatible gloves, these solutions are not widely accepted and appropriate in such particular situations. Using the nose instead of fingers is an easy way to overcome this problem. In this paper, we present in-depth results of a user study on nose-based interaction. The study was complemented by an online survey to elaborate the potential and acceptance of the nose-based interaction style. Based on the insights gained in the study, we identify the main challenges of nose-based interaction and contribute to the state of the art of design principles for this interaction style by adding two new design principles and refining one already existing principle. In addition, we investigated in the emotional effect of nose-based interaction based on the user experiences evolved during the user study.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {32},
numpages = {9},
keywords = {design guidelines, gloves, winter, touchscreen, NoseTapping},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541849,
author = {Kallioniemi, Pekka and Hakulinen, Jaakko and Keskinen, Tuuli and Turunen, Markku and Heimonen, Tomi and Pihkala-Posti, Laura and Uusi-M\"{a}kel\"{a}, Mikael and Hietala, Pentti and Okkonen, Jussi and Raisamo, Roope},
title = {Evaluating Landmark Attraction Model in Collaborative Wayfinding in Virtual Learning Environments},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541849},
doi = {10.1145/2541831.2541849},
abstract = {In Virtual Learning Environments efficient navigation is a major issue, especially when it is used as a component in the learning process. This paper addresses the challenges in creating meaningful navigation routes from language learning perspective. The work is grounded on findings from a specific case on German language learning, wherein two remotely located users communicated in a wayfinding guidance scenario. The users navigated through 360-degree virtual panoramic images using body gestures and could receive communication help via spoken hints by pointing at objects in the scenery. An important design consideration is how to choose these objects, as they have both navigational importance and pedagogical significance in terms of learning the desired language. Wayfinding interactions from 21 participants were compared to the values provided by a landmark attraction model applied on the landmarks along the routes. The results show that there was a clear connection between prominence of landmarks and time spent on each panorama. This indicates that together with pedagogical planning, the model can aid in selecting the interactive content for language learning applications in virtual environments.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {33},
numpages = {10},
keywords = {wayfinding, virtual environments, gesture-based interfaces, second language learning, embodied interaction},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541841,
author = {Bienk, Stefan and Kattenbeck, Markus and Ludwig, Bernd and M\"{u}ller, Manuel and Ohm, Christina},
title = {I Want to View It My Way: Interfaces to Mobile Maps Should Adapt to the User's Orientation Skills},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541841},
doi = {10.1145/2541831.2541841},
abstract = {Efficient human-computer-interaction is a key to success for navigation systems, in particular when pedestrians are using them. Due to the increasing computational power of recent mobile devices, complex multimedia user interfaces to pedestrian navigation systems can be implemented. In order to be able to provide the best-suited interface to each user, we present a user study comparing not only three map presentation modes (bird's eye, egocentric and a combined one), but also involving the users' sense of direction as a second independent factor. In the experiment conducted, we did not focus on a global navigation task, but on the repeated subtask of locating objects on the map. ANOVA analysis of the task completion time revealed a significant interaction effect of presentation mode and the sense of direction of the test persons. Consequently, we advocate user-adaptive presentation modes for pedestrian navigation systems.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {34},
numpages = {9},
keywords = {adaptive interfaces, map presentation, pedestrian navigation, collaborative landmark mining},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541857,
author = {Dashdorj, Zolzaya and Serafini, Luciano and Antonelli, Fabrizio and Larcher, Roberto},
title = {Semantic Enrichment of Mobile Phone Data Records},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541857},
doi = {10.1145/2541831.2541857},
abstract = {The pervasiveness of mobile phones creates an unprecedented opportunity for analyzing human dynamics with the help of the data they generate. This enables a novel human-driven approach for service creation in a variety of domains (e.g., healthcare, transportation, etc.) Telecom operators own and manage billions of mobile network events (Call Detailed Records - CDRs) per day: interpreting such a big stream of data needs a deep understanding of the events' context through the available background knowledge. We introduce an ontological and stochastic model (HRBModel) to interpret mobile human behavior using merged mobile network data and the geo-referenced background knowledge (e.g., OpenStreetMap, etc.) The model characterizes locations with human activities that can happen (with a given likelihood) there. This allows us to predicatively compile sets of tasks that people are likely to engage in under certain contextual conditions or to characterize exceptional events detected from anomalies in the CDR. An experimental evaluation of the approach is presented.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {35},
numpages = {10},
keywords = {context aware computing, human activity recognition, mobile phone data records, machine learning, qualitative methods, linked open data, human behavior, knowledge management, semantics, ontology},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541861,
author = {Aoki, Shunsuke and Kobayashi, Hiroki and Sezaki, Kaoru},
title = {Cicada Fingerprinting System: From Artificial to Sustainable},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541861},
doi = {10.1145/2541831.2541861},
abstract = {Location estimation with artificial infrastructures for mobile computing has been actively studied, but originally, people get a lot of information from nature during a course of a day. Nature including animals and plants supplies various information acoustically and visually. In this context, we present the concept of Cicada Fingerprinting System, which is a future localization scheme that will enable us to make the most of the information from nature. In our system, users collect the chirp of cicadas as acoustic data via smartphone embedded with microphone. Using the chirp of cicadas such as Wi-Fi fingerprinting, we can specify the location of users regardless of the existence of a roof. That is to say, cicada fingerprinting system applies cicada's instinctive behaviour to a localization. Furthermore, by our system, users are able to feel a sense of belonging to a nature even in urban area, where we spend much time in daily life. This novel system is designed for making general users conscious of presence of nature around.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {36},
numpages = {4},
keywords = {mobile computing, nature interface, HCBI (human computer biosphere interaction), sustainability, localization, smart fusion, ubiquitous},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541868,
author = {Turner, Jayson and Bulling, Andreas and Alexander, Jason and Gellersen, Hans},
title = {Eye Drop: An Interaction Concept for Gaze-Supported Point-to-Point Content Transfer},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541868},
doi = {10.1145/2541831.2541868},
abstract = {The shared displays in our environment contain content that we desire. Furthermore, we often acquire content for a specific purpose, i.e., the acquisition of a phone number to place a call. We have developed a content transfer concept, Eye Drop. Eye Drop provides techniques that allow fluid content acquisition, transfer from shared displays, and local positioning on personal devices using gaze combined with manual input. The eyes naturally focus on content we desire. Our techniques use gaze to point remotely, removing the need for explicit pointing on the user's part. A manual trigger from a personal device confirms selection. Transfer is performed using gaze or manual input to smoothly transition content to a specific location on a personal device. This work demonstrates how techniques can be applied to acquire and apply actions to content through a natural sequence of interaction. We demonstrate a proof of concept prototype through five implemented application scenarios.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {37},
numpages = {4},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541872,
author = {Heimonen, Tomi and Turunen, Markku and Kangas, Sanna and Pallos, Tam\'{a}s and Pekkala, Pasi and Saarinen, Santeri and Tiitinen, Katariina and Keskinen, Tuuli and Luhtala, Matti and Koskinen, Olli and Okkonen, Jussi and Raisamo, Roope},
title = {Seek'N'Share: A Platform for Location-Based Collaborative Mobile Learning},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541872},
doi = {10.1145/2541831.2541872},
abstract = {We present a location-based collaborative mobile learning platform called Seek'N'Share. It is comprised of a Web-based learning assignment editor and a mobile application for exploring and capturing multimedia content in the field. The editor enables drag-and-drop creation of learning tasks, areas and points of interest using an intuitive Web interface. Assignments are accessed with an Android application that uses location information to provide content and tasks to learners as they explore the environment. The mobile application enables the learners to record audio, video and take pictures of their environments. This supports the overall goal of putting together a presentation as the outcome of the learning activity by combining predefined, contextual information with user-generated content. The platform is currently piloted with local schools. Its novelty lies in its flexible support for creating location-based learning activities for unconstrained environments, and the possibility for the learners to collaboratively document their learning outcomes in situ.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {38},
numpages = {4},
keywords = {learning content authoring, pervasive interaction, location-based mobile learning},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541875,
author = {Jing, Lei and Cheng, Zixue and Zhou, Yinghui and Wang, Junbo and Huang, Tongjun},
title = {Magic Ring: A Self-Contained Gesture Input Device on Finger},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541875},
doi = {10.1145/2541831.2541875},
abstract = {Control and Communication in the computing environment with diverse equipment could be clumsy, obtrusive, and frustrating even just for finding the right input device or getting familiar with the input interface. In this paper, we present Magic Ring (MR), a finger ring shape input device using inertial sensor to detect the subtle finger gestures and routine daily activities. As a self-contained, always-available, and hands-free input device, we believe that MR will enable diverse applications in the intelligent computing environment. In this demonstration, we will show a prototype design of MR and three proof-of-concept application systems: a remote controller to control the electrical appliance like TV, radio, and lamp using simple finger gestures; a natural communication tools to chat using the simplified sign languages; a daily activity tracker to record daily activities such as room cleaning, eating, cooking, writing with only one MR on the index finger.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {39},
numpages = {4},
keywords = {wearable computing, hand gesture, application, Magic Ring},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541879,
author = {Simeone, Adalberto L. and Seifert, Julian and Schmidt, Dominik and Holleis, Paul and Rukzio, Enrico and Gellersen, Hans},
title = {Technical Framework Supporting a Cross-Device Drag-and-Drop Technique},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541879},
doi = {10.1145/2541831.2541879},
abstract = {We present the technical framework supporting a cross-device Drag-and-Drop technique designed to facilitate interactions involving multiple touchscreen devices. This technique supports users that need to transfer information received or produced on one device to another device which might be more suited to process it. Furthermore, it does not require any additional instrumentation. The technique is a two-handed gesture where one hand is used to suitably align the mobile phone with the larger screen, while the other is used to select and drag an object from one device to the other where it can be applied directly onto a target application. We describe the implementation of the framework that enables spontaneous data-transfer between a mobile device and a desktop computer.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {40},
numpages = {3},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541880,
author = {Keskinen, Tuuli and Melto, Aleksi and Hakulinen, Jaakko and Turunen, Markku and Saarinen, Santeri and Pallos, Tam\'{a}s and Kallioniemi, Pekka and Danielsson-Ojala, Riitta and Salanter\"{a}, Sanna},
title = {Mobile Dictation for Healthcare Professionals},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541880},
doi = {10.1145/2541831.2541880},
abstract = {We demonstrate a mobile dictation application utilizing automatic speech recognition for healthcare professionals. Development was done in close collaboration between human-technology interaction and nursing science researchers and professionals working in the area. Our work was motivated by the need for improvements in getting spoken patient information to the next treatment steps without additional steps. In addition, we wanted to enable truly mobile spoken information entry, i.e., dictation can take place on the spot. In order to study the applicability we conducted a small-scale Wizard-of-Oz evaluation in a real hospital environment with real nurses. Our main focus was to gather subjective expectations and experiences from the actual nurses themselves. The results show true potential for our mobile dictation application and its further development.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {41},
numpages = {4},
keywords = {mobile interaction, speech recognition, healthcare dictation, evaluation, user experience, user expectations},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541869,
author = {Stockinger, Tobias and Koelle, Marion and Lindemann, Patrick and Witzani, Lukas and Kranz, Matthias},
title = {SmartPiggy: A Piggy Bank That Talks to Your Smartphone},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541869},
doi = {10.1145/2541831.2541869},
abstract = {Saving money is usually a tedious task that requires a high degree of self-control for many of us. Some people have one or more specific savings targets in mind and thus need to prioritize them. We propose connecting a savings box with a personal smartphone. Thus, people become motivated to keep track of their savings for multiple targets. Using a savings box capable of counting money and connecting it to an app, we believe people stick to their plans to save with higher motivation and are happier with their behavior. In this paper, we present first evidence for the success of this concept. We gathered feedback through an online user study in which participants were shown a video prototype. We propose further research directions with our SmartPiggy, to confirm the feasibility of behavioral economics in HCI.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {42},
numpages = {2},
keywords = {self-control, gamification, prototyping},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@dataset{10.1145/review-2541831.2541869_R49842,
author = {Manrique, Cecilia G.},
title = {Review ID:R49842 for DOI: 10.1145/2541831.2541869},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-2541831.2541869_R49842}
}

@inproceedings{10.1145/2541831.2541870,
author = {L\"{o}chtefeld, Markus and B\"{o}hmer, Matthias and Ganev, Lyubomir},
title = {AppDetox: Helping Users with Mobile App Addiction},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541870},
doi = {10.1145/2541831.2541870},
abstract = {With the increasing adoption of smartphones also a problematic phenomena become apparent: People are changing their habits and become addicted to different services that these devices provide. In this paper we present AppDetox: an app that allows users to purposely create rules that keep them from using certain apps. We describe our deployment of the app on a mobile application store, and present initial findings gained through observation of about 11,700 users of the application. We find that people are rather rigorous when restricting their app use, and that mostly they suppress use of social networking and messaging apps.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {43},
numpages = {2},
keywords = {technology addiction, smartphone use, digital detox, apps},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541871,
author = {Norrie, Lauren and Koelle, Marion and Murray-Smith, Roderick and Kranz, Matthias},
title = {Putting Books Back on the Shelf: Situated Interactions with Digital Book Collections on Smartphones},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541871},
doi = {10.1145/2541831.2541871},
abstract = {We consider the reasons why we organise books in a physical environment and investigate whether situating interactions with a smartphone could improve the user experience of e-readers. Our prototype uses the Kinect depth sensor to detect the position of a user in relation to sections of a physical bookshelf. We also built a mobile application that allows users to browse and organise digital books by moving between each section. We present our initial observations of a user study that evaluated search and categorisation tasks with our prototype. Our findings motivate reasons to explore digital books in a physical environment and indicate issues to consider when designing situated interactions with e-readers.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {44},
numpages = {2},
keywords = {user experience, mobile interaction, situated information},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541873,
author = {Hauber, Michael and Bachmann, Anja and Budde, Matthias and Beigl, Michael},
title = {JActivity: Supporting Mobile Web Developers with HTML5/JavaScript Based Human Activity Recognition},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541873},
doi = {10.1145/2541831.2541873},
abstract = {Human Activity Recognition (HAR) using accelerometers has been studied intensively in the past decade. Recent HTML5 methods allow sampling a mobile phone's sensors from within web pages. Our objective is to leverage this for the creation of individual activity recognition modules that can be included into web applications to allow them to gain context-awareness. In this work, jActivity, a first prototype of such a platform-independent HTML5/JavaScript framework is presented, along with experiments to determine the general feasibility and challenges for HAR in web applications. Our results indicate that the realization looks promising, albeit so far limited to certain devices/user agents.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {45},
numpages = {2},
keywords = {JavaScript, mobile sensing, HTML5, context, activity recognition, responsive design, progressive enhancement},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541874,
author = {Buchner, Roland and Kluckner, Patricia M. and Weiss, Astrid and Tscheligi, Manfred},
title = {Assisting Maintainers in the Semiconductor Factory: Iterative Co-Design of a Mobile Interface and a Situated Display},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541874},
doi = {10.1145/2541831.2541874},
abstract = {Maintaining machines in semiconductor factories is a challenging task that, so far, is not sufficiently supported by mobile interactive technology. This paper describes the early development of a maintainer support system. Our goal was to develop a user-experience prototype, which consists of a mobile and a situated interface, to support maintenance activities and the coordination between maintainers and shift-leads. The interfaces are meant to reduce the amount of information and to improve awareness for defective equipment. Efforts described in this paper include the development of a conceptual user experience prototype, following an iterative user-centered design approach. Based on the requirements analysis, an initial mock-up of both interfaces was developed and later on discussed with maintainers in a workshop. With an interactive Wizard of Oz (WOz) prototype we examined the cooperative aspect as well as user experience factors (e.g., distraction, trust, usability) in a simulated factory environment.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {46},
numpages = {2},
keywords = {evaluation, factory, User-centered design, mobile and situated interface, user-experience prototyping, maintenance},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541876,
author = {Khan, Muhammad Sikandar Lal and R\'{e}hman, Shafiq ur and Lu, Zhihan and Li, Haibo},
title = {Tele-Embodied Agent (TEA) for Video Teleconferencing},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541876},
doi = {10.1145/2541831.2541876},
abstract = {We propose a design of teleconference system which express nonverbal behavior (in our case head gesture) along with audio-video communication. Previous audio-video conferencing systems are abortive in presenting nonverbal behaviors which we, as human, usually use in face to face interaction. Recently, research in teleconferencing systems has expanded to include nonverbal cues of remote person in their distance communication. The accurate representation of non-verbal gestures for such systems is still challenging because they are dependent on hand-operated devices (like mouse or keyboard). Furthermore, they still lack in presenting accurate human gestures. We believe that incorporating embodied interaction in video teleconferencing, (i.e., using the physical world as a medium for interacting with digital technology) can result in nonverbal behavior representation. The experimental platform named Tele-Embodied Agent (TEA) is introduced which incorperates remote person's head gestures to study new paradigm of embodied interaction in video teleconferencing. Our preliminary test shows accuracy (with respect to pose angles) and efficiency (with respect to time) of our proposed design. TEA can be used in medical field, factories, offices, gaming industry, music industry and for training.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {47},
numpages = {2},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541877,
author = {H\"{a}kkil\"{a}, Jonna and Posti, Maaret and Koskenranta, Olli and Vent\"{a}-Olkkonen, Leena},
title = {Co-Creating a Digital 3D City with Children},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541877},
doi = {10.1145/2541831.2541877},
abstract = {In this paper, we present our co-creation work on a digital 3D city model, where school children were asked create drawings for the city landscape of their hometown. Altogether 40 drawings were then integrated into the 3D virtual world model to form a decorated 3D view of the city square and the main street. Our work presents a novel use case in using the local 3D city model as a creativity platform for children, opening a new viewpoint on how virtual world presentations can be used with e.g. to facilitate the perception of the local community.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {48},
numpages = {2},
keywords = {creative tools, communities, 3D virtual worlds, design with children, user created content},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541878,
author = {Vent\"{a}-Olkkonen, Leena and Kinnula, Marianne and Dean, Graham and Stockinger, Tobias and Z\'{u}\~{n}iga, Claudia},
title = {<i>Who's There?</i>: Experience-Driven Design of Urban Interaction Using a Tangible User Interface},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541878},
doi = {10.1145/2541831.2541878},
abstract = {During recent years public displays relying on new types of display technologies have made their way to the city scene. In this paper, we present a concept that combines tangible interfaces with such ubiquitous urban interaction. We set out to create a tangible connection between different cities and employed an experience-driven design process towards our concept called 'Who's There?'. We evaluated the concept by using a cardboard prototype with a group of fifteen users in a busy market square, where it generated considerable engagement and discussion with members of the public.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {49},
numpages = {2},
keywords = {experience-driven design, tangible user interfaces, connected cities, public displays},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2541881,
author = {H\"{a}kkil\"{a}, Jonna and Vent\"{a}-Olkkonen, Leena and Shi, Henglin and Karvonen, Ville and He, Yun and H\"{a}yrynen, Mikko},
title = {Jogging in a Virtual City},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541881},
doi = {10.1145/2541831.2541881},
abstract = {In our research, we explore the possibilities for combining digital 3D city representation with a wellness application, and introduce our demo that aims to make running in a gym more inspiring and motivating. We present a prototype, where the running exercise on a treadmill is converted to a distance on the local city map, and visualized in a 3D mirror world presentation of the city. The user leaves a personalized tag on the spot reached, and in addition to his achievement, is able to see the performance of other runners on the streets of the virtual world. We evaluated the system in the gym, where 32 people tried out the prototype. The application was perceived entertaining and interesting, and especially the ability to compare the results with earlier runners was perceived motivating.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {50},
numpages = {2},
keywords = {maps, exergames, sports, virtual worlds},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2543881,
author = {Ojala, Timo},
title = {2<sup>nd</sup> International UBI Challenge 2013},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2543881},
doi = {10.1145/2541831.2543881},
abstract = {This paper summarizes the 2nd UBI Challenge that invited the global R&amp;D community to design, implement, deploy and evaluate novel applications and services in real world setting atop the open urban computing testbed in Oulu, Finland. The paper first recaps the 1st UBI Challenge and then provides a procedural description of the 2nd UBI Challenge. The paper concludes with a discussion on issues related to participation in the UBI Challenge.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {51},
numpages = {4},
keywords = {in the wild, testbed, urban informatics, ubiquitous computing, urban computing},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2543694,
author = {Komninos, Andreas and Besharat, Jeries and Ferreira, Denzil and Garofalakis, John},
title = {HotCity: Enhancing Ubiquitous Maps with Social Context Heatmaps},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2543694},
doi = {10.1145/2541831.2543694},
abstract = {In this paper we present HotCity, a service that demonstrates how collecting and mining the interactions that users make with the urban environment through social networks, can help tourists better plan activities, through sharing the collectively generated social context of a smart, connected city, as a background layer to mapped POI. The data for our service stems from the collection and analysis of 1-month worth of collected human-physical environment interactions (i.e., Foursquare check-ins) data for Oulu, a medium-sized city in Finland, where our service is deployed in ubiquitous public displays. Our analysis demonstrates that a good model of the city's dynamics can be built despite the low popularity of Foursquare amongst locals. Our findings from the field-based trial of the HotCity service yield several useful insights and important contributions. We found that the method of using a heatmap as an intermediate layer of environmental context does not negatively affect the experience of users at the cognitive level, compared with a more traditional map and POI type of interface, where temporal aspects of context are not present. In the concluding sections, we discuss how this cloud-based service can also be used in a variety of ubiquitous computing platforms.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {52},
numpages = {10},
keywords = {heatmap visualization, ubiquitous maps, social context},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{10.1145/2541831.2543695,
author = {Holappa, Jukka and Heikkinen, Tommi and Roininen, Elina},
title = {Martians from Outer Space: Experimenting with Location-Aware Cooperative Multiplayer Gaming on Public Displays},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2543695},
doi = {10.1145/2541831.2543695},
abstract = {In this paper, we describe our experiences with location-aware cooperative multiplayer game on public displays. The game world is modelled after the city of Oulu, Finland where players protect the city from a Martian invasion. We investigate the potential of the used platform, the effects of locality and how a more complex and challenging gaming experience on public displays is received. We demonstrate that locality does have a significant effect on the game-play especially when the player can actually see the familiar surroundings in the game world. We also show that while the use of the different services vary a lot from place to place, our game can maintain a very good ranking when compared to other, more casual games.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {53},
numpages = {10},
keywords = {public displays, location-based gaming, field trial},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

