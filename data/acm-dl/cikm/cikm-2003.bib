@inproceedings{10.1145/3244144,
author = {Zhai, ChengXiang},
title = {Session Details: Information Retrieval Session 1: Adhoc Retrieval},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244144},
doi = {10.1145/3244144},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956866,
author = {Billerbeck, Bodo and Scholer, Falk and Williams, Hugh E. and Zobel, Justin},
title = {Query Expansion Using Associated Queries},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956866},
doi = {10.1145/956863.956866},
abstract = {Hundreds of millions of users each day use web search engines to meet their information needs. Advances in web search effectiveness are therefore perhaps the most significant public outcomes of IR research. Query expansion is one such method for improving the effectiveness of ranked retrieval by adding additional terms to a query. In previous approaches to query expansion, the additional terms are selected from highly ranked documents returned from an initial retrieval run. We propose a new method of obtaining expansion terms, based on selecting terms from past user queries that are associated with documents in the collection. Our scheme is effective for query expansion for web retrieval: our results show relative improvements over unexpanded full text retrieval of 26%--29%, and 18%--20% over an optimised, conventional expansion approach.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {2–9},
numpages = {8},
keywords = {query expansion, query association, web search},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956867,
author = {HE, Ben and Ounis, Iadh},
title = {A Study of Parameter Tuning for Term Frequency Normalization},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956867},
doi = {10.1145/956863.956867},
abstract = {Most current term frequency normalization approaches for information retrieval involve the use of parameters. The tuning of these parameters has an important impact on the overall performance of the information retrieval system. Indeed, a small variation in the involved parameter(s) could lead to an important variation in the precision/recall values. Most current tuning approaches are dependent on the document collections. As a consequence, the effective parameter value cannot be obtained for a given new collection without extensive training data. In this paper, we propose a novel and robust method for the tuning of term frequency normalization parameter(s), by measuring the normalization effect on the within document frequency of the query terms. As an illustration, we apply our method on Amati &amp; Van Rijsbergen's so-called normalization 2. The experiments for the ad-hoc TREC-6,7,8 tasks and TREC-8,9,10 Web tracks show that the new method is independent of the collections and able to provide reliable and good performance.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {10–16},
numpages = {7},
keywords = {information retrieval, parameter tuning, document length, term frequency normalization},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956868,
author = {Beitzel, Steven M. and Jensen, Eric C. and Chowdhury, Abdur and Grossman, David},
title = {Using Titles and Category Names from Editor-Driven Taxonomies for Automatic Evaluation},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956868},
doi = {10.1145/956863.956868},
abstract = {Evaluation of IR systems has always been difficult because of the need for manually assessed relevance judgments. The advent of large editor-driven taxonomies on the web opens the door to a new evaluation approach. We use the ODP (Open Directory Project) taxonomy to find sets of pseudo-relevant documents via one of two assumptions: 1) taxonomy entries are relevant to a given query if their editor-entered titles exactly match the query, or 2) all entries in a leaf-level taxonomy category are relevant to a given query if the category title exactly matches the query. We compare and contrast these two methodologies by evaluating six web search engines on a sample from an America Online log of ten million web queries, using MRR measures for the first method and precision-based measures for the second. We show that this technique is stable with respect to the query set selected and correlated with a reasonably large manual evaluation.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {17–23},
numpages = {7},
keywords = {web search, relevance judgments, automatic evaluation},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244145,
author = {Clarke, Charlie},
title = {Session Details: Database Session 1: Querying High-Dimensional Data},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244145},
doi = {10.1145/3244145},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956870,
author = {Berrani, Sid-Ahmed and Amsaleg, Laurent and Gros, Patrick},
title = {Approximate Searches: K-Neighbors + Precision},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956870},
doi = {10.1145/956863.956870},
abstract = {It is known that all multi-dimensional index structures fail to accelerate content-based similarity searches when the feature vectors describing images are high-dimensional. It is possible to circumvent this problem by relying on approximate search-schemes trading-off result quality for reduced query execution time. Most approximate schemes, however, provide none or only complex control on the precision of the searches, especially when retrieving the k nearest neighbors (NNs) of query points.In contrast, this paper describes an approximate search scheme for high-dimensional databases where the precision of the search can be probabilistically controlled when retrieving the k NNs of query points. It allows a fine and intuitive control over this precision by setting at run time the maximum probability for a vector that would be in the exact answer set to be missed in the approximate set of answers eventually returned. This paper also presents a performance study of the implementation using real datasets showing its reliability and efficiency. It shows, for example, that our method is 6.72 times faster than the sequential scan when it handles more than 5 106 24-dimensional vectors, even when the probability of missing one of the true nearest neighbors is below 0.01.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {24–31},
numpages = {8},
keywords = {multimedia databases, approximate nearest-neighbor searches, similarity searches},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956871,
author = {Chen, Chung-Min and Cheng, Christine T.},
title = {Replication and Retrieval Strategies of Multidimensional Data on Parallel Disks},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956871},
doi = {10.1145/956863.956871},
abstract = {Aside from enhancing data availability during disk failures, replication of data is also used to speed up I/O performance of read-intensive applications. There are two issues that need to be addressed: (a) data placement (Which disks should store the copies of each data block?) and (b) scheduling (Given a query Q, and a placement scheme P of the data, from which disk should each block in Q be retrieved so that retrieval time is minimized?) In this paper, we consider range queries and assume that the dataset is a multidimensional grid and r copies of each unit block of the grid must be stored among M disks. To accurately measure performance of a scheduling algorithm, we consider a metric that takes into account the scheduling overhead as well as the time it takes to retrieve the data blocks from the disks. We describe several combinations of data placement schemes and scheduling algorithms and analyze their performance for range queries with respect to the above metric. We then present simulation results for the most interesting case r=2, showing that the strategies do perform better than the previously known method, especially for large queries.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {32–39},
numpages = {8},
keywords = {disks, replication, multi-dimensional declustering, parallel},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956872,
author = {Baldwin, Chuck and Abdulla, Ghaleb and Critchlow, Terence},
title = {Multi-Resolution Modeling of Large Scale Scientific Simulation Data},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956872},
doi = {10.1145/956863.956872},
abstract = {To provide scientists and engineers with the ability to explore and analyze tera-scale size data-sets we are using a twofold approach. First, we model the data with the objective of creating a compressed yet manageable representation. Second, with that compressed representation, we provide the ability to query the resulting approximation in order to obtain approximate yet sufficient answers; a process called ad-hoc querying. This paper is concerned with a wavelet modeling technique that seeks to capture the important physical characteristics of the target scientific data. Our approach is driven by the compression, which is necessary for viable throughput, along with the end user requirements from the discovery process. Our work contrasts existing research which applies wavelets to range querying, change detection, and clustering problems by working directly with the wavelet decomposition of the data. The difference in this procedure is due primarily to the nature of the data and the requirements of the scientists and engineers. Our approach directly uses the wavelet coefficients of the data to compress as well as query. We describe how the wavelet decomposition is used to facilitate data compression and how queries are posed on the resulting compressed model. Results of this process will be shown for several problems of interest.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {40–48},
numpages = {9},
keywords = {wavelets, data modeling, compression, scientific data processing},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244146,
author = {Si, Luo},
title = {Session Details: Knowledge Management Session 1: Visual},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244146},
doi = {10.1145/3244146},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956874,
author = {Torres, Ricardo S. and Silva, Celmar G. and Medeiros, Claudia B. and Rocha, Heloisa V.},
title = {Visual Structures for Image Browsing},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956874},
doi = {10.1145/956863.956874},
abstract = {Content-Based Image Retrieval (CBIR) presents several challenges and has been subject to extensive research from many domains, such as image processing or database systems. Database researchers are concerned with indexing and querying, whereas image processing experts worry about extracting appropriate image descriptors. Comparatively little work has been done on designing user interfaces for CBIR systems. This, in turn, has a profound effect on these systems since the concept of image similarity is strongly influenced by user perception. This paper describes an initial effort to fill this gap, combining recent research in CBIR and Information Visualization, studied from a Human-Computer Interface perspective. It presents two visualization techniques based on Spiral and Concentric Rings implemented in a CBIR system to explore query results. The approach is centered on keeping user focus on both the query image, and the most similar retrieved images. Experiments conducted so far suggest that the proposed visualization strategies improves system usability.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {49–55},
numpages = {7},
keywords = {image browsing, focus+context views, content-based Image retrieval (CBIR), visual structures, information visualization},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956875,
author = {Gloor, Peter A. and Laubacher, Rob and Dynes, Scott B. C. and Zhao, Yan},
title = {Visualization of Communication Patterns in Collaborative Innovation Networks - Analysis of Some W3C Working Groups},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956875},
doi = {10.1145/956863.956875},
abstract = {Collaborative Innovation Networks (COINs) are groups of self-motivated individuals from various parts of an organization or from multiple organizations, empowered by the Internet, who work together on a new idea, driven by a common vision. In this paper we report first results of a project that examines innovation networks by analyzing the e-mail archives of some W3C (WWW consortium) working groups. These groups exhibit ideal characteristics for our purpose, as they form truly global networks working together over the Internet to develop next generation technologies. We first describe the software tools we developed to visualize the temporal communication flow, which represent communication patterns as directed acyclic graphs, We then show initial results, which revealed significant variations between the communication patterns and network structures of the different groups., We were also able to identify distinctive communication patterns among group leaders, both those who were officially appointed and other who were assuming unofficial coordinating roles.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {56–60},
numpages = {5},
keywords = {temporal information visualization, collaborative innovation network, collaborative applications, knowledge management, social network analysis},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244147,
author = {Clarke, Charlie},
title = {Session Details: Information Retrieval Session 2: Non-Text Retrieval},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244147},
doi = {10.1145/3244147},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956877,
author = {MIURA, Takao and SHIOYA, Isamu},
title = {Similarity among Melodies for Music Information Retrieval},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956877},
doi = {10.1145/956863.956877},
abstract = {Here we discuss how to look for similar melody in music databases by giving monophonic melody in sheet. In this work, we utilize text expression (or sheet music) to describe music and introduce pitch spectrum of melodies. By this feature, we concisely distinguish music from tempo, transposition or other arbitrary expressions. We show the usefulness by experimental results.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {61–68},
numpages = {8},
keywords = {music information retrieval, melody similarity},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956878,
author = {Weber, Roger and Mlivoncic, Michael},
title = {Efficient Region-Based Image Retrieval},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956878},
doi = {10.1145/956863.956878},
abstract = {Region-based image retrieval(RBIR) was recently proposed as an extension of content-based image retrieval(CBIR). An RBIR system automatically segments images into a variable number of regions, and extracts for each region a set of features. Then, a dissimilarity function determines the distance between a database image and a set of reference regions. Unfortunately, the large evaluation costs of the dissimilarity function are restricting RBIR to relatively small databases. In this paper, we apply a multi-step approach to enable region-based techniques for large image collections. We provide cheap lower and upper bounding distance functions for a recently proposed dissimilarity measure. As our experiments show, these bounding functions are so tight, that we have to evaluate the expensive distance function for less than 0.5%of the images. For a typical image database with more than 370,000images, our multi-step approach improved retrieval performance by a factor of more than5 compared to the currently fastest methods.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {69–76},
numpages = {8},
keywords = {CBIR, region-based image retrieval, RBIR},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956879,
author = {Gilbert, Juan E. and Zhong, Yapin},
title = {Speech User Interfaces for Information Retrieval},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956879},
doi = {10.1145/956863.956879},
abstract = {The research proposed here concentrates on the problem of designing and developing a spoken query retrieval (SQR) system to access large document databases via voice. The main challenge is to identify and address issues related to designing an effective and efficient speech user interface (SUI), especially if the aim is to facilitate spoken queries of large document databases. Furthermore, the task of presenting large query result sets aurally should be performed such that the user's short term memory is not overloaded. In this paper, a framework allowing information retrieval to large document databases via voice is presented and findings from a research study using the framework will be discussed as well.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {77–82},
numpages = {6},
keywords = {VoiceXML, spoken query retrieval, information retrieval, speech user interfaces},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956881,
author = {Tao, Yufei and Faloutsos, Christos and Papadias, Dimitris},
title = {The Power-Method: A Comprehensive Estimation Technique for Multi-Dimensional Queries},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956881},
doi = {10.1145/956863.956881},
abstract = {Existing estimation approaches for multi-dimensional databases often rely on the assumption that data distribution in a small region is uniform, which seldom holds in practice. Moreover, their applicability is limited to specific estimation tasks under certain distance metric. This paper develops the Power-method, a comprehensive technique applicable to a wide range of query optimization problems under various metrics. The Power-method eliminates the local uniformity assumption and is accurate even in scenarios where existing approaches completely fail. Furthermore, it performs estimation by evaluating only one simple formula with minimal computational overhead. Extensive experiments confirm that the Power-method outperforms previous techniques in terms of accuracy and applicability to various optimization scenarios.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {83–90},
numpages = {8},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956882,
author = {Singh, Amit and Ferhatosmanoglu, Hakan and Tosun, Ali \c{S}aman},
title = {High Dimensional Reverse Nearest Neighbor Queries},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956882},
doi = {10.1145/956863.956882},
abstract = {Reverse Nearest Neighbor (RNN) queries are of particular interest in a wide range of applications such as decision support systems, profile based marketing, data streaming, document databases, and bioinformatics. The earlier approaches to solve this problem mostly deal with two dimensional data. However most of the above applications inherently involve high dimensions and high dimensional RNN problem is still unexplored. In this paper, we propose an approximate solution to answer RNN queries in high dimensions. Our approach is based on the strong correlation in practice between k-NN and RNN. It works in two phases. In the first phase the k-NN of a query point is found and in the next phase they are further analyzed using a novel type of query Boolean Range Query (BRQ). Experimental results show that BRQ is much more efficient than both NN and range queries, and can be effectively used to answer RNN queries. Performance is further improved by running multiple BRQ simultaneously. The proposed approach can also be used to answer other variants of RNN queries such as RNN of order k, bichromatic RNN, and Matching Query which has many applications of its own. Our technique can efficiently answer NN, RNN, and its variants with approximately same number of I/O as running a NN query.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {91–98},
numpages = {8},
keywords = {reverse nearest neighbor, boolean range, nearest neighbor, query},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956883,
author = {Ogras, \"{U}mit Y. and Ferhatosmanoglu, Hakan},
title = {Dimensionality Reduction Using Magnitude and Shape Approximations},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956883},
doi = {10.1145/956863.956883},
abstract = {High dimensional data sets are encountered in many modern database applications. The usual approach is to construct a summary of the data set through a lossy compression technique, and use this lower dimensional synopsis to provide fast, approximate answers to the queries. In this paper, we develop a novel dimensionality reduction technique based on partitioning the high dimensional vector space into orthogonal subspaces. First, we find a relation between the Euclidian distance of two n-dimensional vectors and the Euclidian distances of their projections on the orthogonal subspaces. Then, based on this relation we develop a method to approximate the Euclidian distance using novel inner product approximation. This process allows us to incorporate the shape information of the vectors to this approximation. While the inner product approximation is symmetric, i.e., captures only the magnitude information of the data, the proposed method takes both the magnitude and shape information of the original vectors into account through partitioning. In the experiments, we demonstrate the effectiveness of our technique by comparing it with commonly used methods.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {99–107},
numpages = {9},
keywords = {similarity search, shape approximation, high dimensional data},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244148,
author = {Lee, Yugyung},
title = {Session Details: Knowledge Management Session 2: Semantic Web},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244148},
doi = {10.1145/3244148},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956885,
author = {Sun, Aixin and Lim, Ee-Peng},
title = {Web Unit Mining: Finding and Classifying Subgraphs of Web Pages},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956885},
doi = {10.1145/956863.956885},
abstract = {In web classification, most researchers assume that the objects to classify are individual web pages from one or more web sites. In practice, the assumption is too restrictive since a web page itself may not always correspond to a concept instance of some semantic concept (or category) given to the classification task. In this paper, we want to relax this assumption and allow a concept instance to be represented by a subgraph of web pages or a set of web pages. We identify several new issues to be addressed when the assumption is removed, and formulate the web unit mining problem. We also propose an iterative web unit mining (iWUM) method that first finds subgraphs of web pages using some knowledge about web site structure. From these web subgraphs, web units are constructed and classified into semantic concepts (or categories) in an iterative manner. Our experiments using the WebKB dataset showed that iWUM improves the overall classification performance and works very well on the more structured parts of a web site.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {108–115},
numpages = {8},
keywords = {web unit, web unit mining, web classification},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956886,
author = {Lin, Jimmy and Katz, Boris},
title = {Question Answering from the Web Using Knowledge Annotation and Knowledge Mining Techniques},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956886},
doi = {10.1145/956863.956886},
abstract = {We present a strategy for answering fact-based natural language questions that is guided by a characterization of real-world user queries. Our approach, implemented in a system called Aranea, extracts answers from the Web using two different techniques: knowledge annotation and knowledge mining. Knowledge annotation is an approach to answering large classes of frequently occurring questions by utilizing semi-structured and structured Web sources. Knowledge mining is a statistical approach that leverages massive amounts of Web data to overcome many natural language processing challenges. We have integrated these two different paradigms into a question answering system capable of providing users with concise answers that directly address their information needs.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {116–123},
numpages = {8},
keywords = {data-redundancy, semistructured data},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956887,
author = {Zhang, Zhu and Otterbacher, Jahna and Radev, Dragomir},
title = {Learning Cross-Document Structural Relationships Using Boosting},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956887},
doi = {10.1145/956863.956887},
abstract = {Multi-document discoure analysis has emerged with the potential of improving various information retrieval applications. Based on the newly proposed Cross-document Structure Theory (CST), this paper describes an empirical study that uses boosting to classify CST relationships between sentence pairs extracted from topically related documents. We show that the binary classifier for determining existence of structural relationships significantly outperforms the baseline. We also achieve promising results on the multi-class case in which the full taxonomy of relationships are considered.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {124–130},
numpages = {7},
keywords = {classification, boosting, cross-document structure, discourse analysis},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244149,
author = {Chowdhury, Abdur},
title = {Session Details: Information Retrieval Session 3: Cross Language Retrieval},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244149},
doi = {10.1145/3244149},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956889,
author = {Melucci, Massimo and Orio, Nicola},
title = {A Novel Method for Stemmer Generation Based on Hidden Markov Models},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956889},
doi = {10.1145/956863.956889},
abstract = {In this paper, we present a method based on Hidden Markov Models (HMMs) to generate statistical stemmers. Using a list of words as training set, the method estimates the HMM parameters which are used to calculate the most probable stem for an arbitrary word. Stemming is performed by computing the most probable path, through the HMM states, corresponding to the input word. Linguistic knowledge or a training set of manually stemmed words are not required. We describe the method and the results of the experiments carried out using standard test collections for five different languages.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {131–138},
numpages = {8},
keywords = {hidden Markov models, unsupervised training, HMM, multi-lingual information retrieval, stemming},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956890,
author = {AbdulJaleel, Nasreen and Larkey, Leah S.},
title = {Statistical Transliteration for English-Arabic Cross Language Information Retrieval},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956890},
doi = {10.1145/956863.956890},
abstract = {Out of vocabulary (OOV) words are problematic for cross language information retrieval. One way to deal with OOV words when the two languages have different alphabets, is to transliterate the unknown words, that is, to render them in the orthography of the second language. In the present study, we present a simple statistical technique to train an English to Arabic transliteration model from pairs of names. We call this a selected n-gram model because a two-stage training procedure first learns which n-gram segments should be added to the unigram inventory for the source language, and then a second stage learns the translation model over this inventory. This technique requires no heuristics or linguistic knowledge of either language. We evaluate the statistically-trained model and a simpler hand-crafted model on a test set of named entities from the Arabic AFP corpus and demonstrate that they perform better than two online translation sources. We also explore the effectiveness of these systems on the TREC 2002 cross language IR task. We find that transliteration either of OOV named entities or of all OOV words is an effective approach for cross language IR.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {139–146},
numpages = {8},
keywords = {out of vocabulary words, statistical transliteration, named entities, cross language information retrieval},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956891,
author = {Ballesteros, Lisa and Sanderson, Mark},
title = {Addressing the Lack of Direct Translation Resources for Cross-Language Retrieval},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956891},
doi = {10.1145/956863.956891},
abstract = {Most cross language information retrieval research concentrates on language pairs for which direct, rich, and often multiple translation resources already exist. However, for most language pairs, translation via an intermediate language is necessary. Two distinct methods for dealing with the additional ambiguity introduced by the extra translation step have been proposed and individually, shown to improve retrieval effectiveness. Two previous works indicated that in combination, the methods were ineffective. This paper provides strong empirical evidence that the methods can be combined to produce consistent and often significant improvements in retrieval effectiveness. The improvement is shown across a number of different intermediate languages and test collections.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {147–152},
numpages = {6},
keywords = {cross-lingual, transitive translation, cross-language information retrieval, pivot language, structured queries},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956893,
author = {Yee, Wai Gen and Navathe, Shamkant B.},
title = {Efficient Data Access to Multi-Channel Broadcast Programs},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956893},
doi = {10.1145/956863.956893},
abstract = {This paper studies fast access to data that are broadcast on multiple channels. Broadcast is a useful data dissemination technique because of its scalability, but is lacking when it comes to response time. Increasing the number of available broadcast channels is a logical way of increasing throughput. Little work, however, has considered the access structures necessary for making effective use of the additional channels. We propose various indexing schemes for a multi-channel broadcast program. We demonstrate the effectiveness of our techniques in decreasing response time and tuning time via extensive experiments over a wide range of parameters.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {153–160},
numpages = {8},
keywords = {indexing, data dissemination, query processing, scheduling, access methods, broadcast},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956894,
author = {Huang, Jiun-Long and Chen, Ming-Syan and Peng, Wen-Chih},
title = {Exploring Group Mobility for Replica Data Allocation in a Mobile Environment},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956894},
doi = {10.1145/956863.956894},
abstract = {The growth in wireless communication technologies attracts a considerable amount of attention in mobile ad-hoc networks. Since mobile hosts in an ad-hoc network usually move freely, the topology of the network changes dynamically and disconnection occurs frequently. These characteristics make a mobile ad-hoc network be likely to be separated into several disconnected partitions, and the data accessibility is hence reduced. Several schemes are proposed to alleviate the reduction of data accessibility by replicating data items. However, little research effort was elaborated upon exploiting the group mobility where the group mobility refers to the phenomenon that several mobile nodes tend to move together. In this paper, we address the problem of replica allocation in a mobile ad-hoc network by exploring group mobility. We first analyze the group mobility model and derive several theoretical results. In light of these results, we propose a replica allocation scheme to improve the data accessibility. Several experiments are conducted to evaluate the performance of the proposed scheme. The experimental results show that the proposed scheme is able to not only obtain higher data accessibility but also produce lower network traffic than prior schemes.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {161–168},
numpages = {8},
keywords = {mobile computing, data accessibility, replica allocation, ad-hoc networks},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244150,
author = {Seligman, Len},
title = {Session Details: Industry Session 1: Information Retrieval},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244150},
doi = {10.1145/3244150},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956896,
author = {Alonso, Rafael and Bloom, Jeffrey A. and Li, Hua},
title = {Lessons from the Implementation of an Adaptive Parts Acquisition EPortal},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956896},
doi = {10.1145/956863.956896},
abstract = {In recent work we have developed a novel approach to the design and implementation of an online portal (ePortal) to help application engineers find replacements for electronic parts that have become obsolete (and hence will no longer be produced). Our approach makes use of machine learning techniques to improve the performance of a database search function. However, the purpose of this note is not to describe in detail the application nor our technical solution - that has been done elsewhere (see [1,2]). Rather, it is our intention to present some of the lessons learned from our project. Below, we provide a brief introduction to the technical approach, concentrate on several of the most salient lessons, and conclude with a description of the current state of the project.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {169–171},
numpages = {3},
keywords = {k-nearest neighbor classification, adaptive search, user profiling, normalization, query by example, lessons leaned},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956897,
author = {Harada, Lilian and Hotta, Yuuji and Akaboshi, Naoki and Kubota, Kazumi and Ohmori, Tadashi and Take, Riichiro},
title = {Event Analyzer: A Tool for Sequential Data Processing},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956897},
doi = {10.1145/956863.956897},
abstract = {In this paper we present a tool called Event Analyzer that processes events that compose a sequence. We present the data model in which Event Analyzer is based, as well as its query language that allows the expression of complex patterns to be searched over the sequence of events. The Event Analyzer has been developed and it now integrates the Fujitsu Symfoware e-Business Intelligence Suite Premium.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {172–174},
numpages = {3},
keywords = {event, pattern, sequence},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956898,
author = {Nicola, Matthias and John, Jasmi},
title = {XML Parsing: A Threat to Database Performance},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956898},
doi = {10.1145/956863.956898},
abstract = {XML parsing is generally known to have poor performance characteristics relative to transactional database processing. Yet, its potentially fatal impact on overall database performance is being underestimated. We report real-word database applications where XML parsing performance is a key obstacle to a successful XML deployment. There is a considerable share of XML database applications which are prone to fail at an early and simple road block: XML parsing. We analyze XML parsing performance and quantify the extra overhead of DTD and schema validation. Comparison with relational database performance shows that the desired response times and transaction rates over XML data can not be achieved without major improvements in XML parsing technology. Thus, we identify research topics which are most promising for XML parser performance in database systems.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {175–178},
numpages = {4},
keywords = {SAX, XML, database, DOM, parser, performance, validation},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956899,
author = {Vanlande, Renaud and Cruz, Christophe and Nicolle, Christophe},
title = {Managing IFC for Civil Engineering Projects},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956899},
doi = {10.1145/956863.956899},
abstract = {The "Industrial Foundation Classes" (IFC) are an ISO norm to define all components of a building in a civil engineering project. IFC files are textual files whose size can reach 100 megabytes. Several IFC files can coexist on the same civil engineering project. Due to their size, their handling and sharing is a complex task. In this paper, we present an approach to automatically identify business objects in the IFC files and simplify their visualization and manipulation on the Internet. We construct an IFC Viewer which transforms the IFC file into a XML IFC tree manipulated through the 3D visualization of the building. The IFC Viewer composed a web-based platform called ACTIVe3D BUILD SERVER. This platform lets geographically dispersed project participants-from architects to electricians-directly use and exchange project documents in a centralized virtual environment during the life cycle of a civil engineering project.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {179–181},
numpages = {3},
keywords = {DBMS, IFC, 3D, semantic, collaborative application, XML},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244151,
author = {Pass, Greg},
title = {Session Details: Information Retrieval Session 4: General Retrieval Issues I},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244151},
doi = {10.1145/3244151},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956901,
author = {Cathey, Rebecca and Ma, Ling and Goharian, Nazli and Grossman, David},
title = {Misuse Detection for Information Retrieval Systems},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956901},
doi = {10.1145/956863.956901},
abstract = {We present a novel approach to detect misuse within an information retrieval system by gathering and maintaining knowledge of the behavior of the user rather than anticipating attacks by unknown assailants. Our approach is based on building and maintaining a profile of the behavior of the system user through tracking, or monitoring of user activity within the information retrieval system. Any new activity of the user is compared to the user profile to detect a potential misuse for the authorized user. We propose four different methods to detect misuse in information retrieval systems. Our experimental results on $2$ GB collection favorably demonstrate the validity of our approach.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {183–190},
numpages = {8},
keywords = {clustering, relevance feedback, information retrieval, user profile, misuse detection},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956902,
author = {Yang, Yiming and Kisiel, Bryan},
title = {Margin-Based Local Regression for Adaptive Filtering},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956902},
doi = {10.1145/956863.956902},
abstract = {Adaptive information filtering is an open challenge in information retrieval. One of the tough issues is the optimization of decision thresholds over time, based on partial relevance feedback on the system-retrieved documents in chronological order. We developed a new approach, namely margin-based local regression, that automatically adjusts the thresholds based on a sliding window over the truly positive examples for which the system predicted "yes" with respect to a particular class, and a second sliding window over the other documents being processed by the system. Using the means of the scores of the documents in the two windows, we monitor the temporal drifting of the margin that is a function of both the current classification model and the threshold calibration strategy, and that suggests the bounds for the optimal threshold at a given time. Examining this approach together with a Rocchio-style classifier on the TREC 2001 and TREC 2002 benchmark data sets in adaptive filtering, we obtained significant improvements in performance (measured using Fβ=0.5) over the baseline system that did not adapt the threshold over time, and the best result ever reported on the TREC 2002 benchmark corpus for adaptive filtering evaluations. These empirical results suggest that it is important to use both system-accepted and system-rejected documents to optimize thresholds instead of just using system-accepted documents alone, as well as to make the thresholding function temporally sensitive to the shifting centroids of on-topic and off-topic documents.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {191–198},
numpages = {8},
keywords = {threshold calibration, local regression, adaptive filtering, temporal sequences},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956903,
author = {Lu, Jie and Callan, Jamie},
title = {Content-Based Retrieval in Hybrid Peer-to-Peer Networks},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956903},
doi = {10.1145/956863.956903},
abstract = {Hybrid peer-to-peer architectures use special nodes to provide directory services for regions of the network ("regional directory services"). Hybrid peer-to-peer architectures are a potentially powerful model for developing large-scale networks of complex digital libraries, but peer-to-peer networks have so far tended to use very simple methods of resource selection and document retrieval. In this paper, we study the application of content-based resource selection and document retrieval to hybrid peer-to-peer networks. The directory nodes that provide regional directory services construct and use the content models of neighboring nodes to determine how to route query messages through the network. The leaf nodes that provide information use content-based retrieval to decide which documents to retrieve for queries. The experimental results demonstrate that using content-based retrieval in hybrid peer-to-peer networks is both more accurate and more efficient for some digital library environments than more common alternatives such as Gnutella 0.6.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {199–206},
numpages = {8},
keywords = {retrieval, peer-to-peer, search, content-based, hybrid},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956905,
author = {Clarke, Charles L. A. and Tilker, Philip L. and Tran, Allen Quoc-Luan and Harris, Kevin and Cheng, Antonio S.},
title = {A Reliable Storage Management Layer for Distributed Information Retrieval Systems},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956905},
doi = {10.1145/956863.956905},
abstract = {We present a storage management layer that facilitates the implementation of parallel information retrieval systems, and related applications, on networks of workstations. The storage management layer automates the process of adding and removing nodes, and implements a dispersed mirroring strategy to improve reliability. When nodes are added and removed, the document collection managed by the system is redistributed for load balancing purposes. The use of dispersed mirroring minimizes the impact of node failures and system modifications on query performance.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {207–215},
numpages = {9},
keywords = {distributed information retrieval, self-managing systems, cluster computing},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956906,
author = {Priebe, Torsten and Pernul, G\"{u}nther},
title = {Towards Integrative Enterprise Knowledge Portals},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956906},
doi = {10.1145/956863.956906},
abstract = {Knowledge portals make an important contribution to enabling enterprise knowledge management by providing users with a consolidated, personalized user interface that allows efficient access to various types of (structured and unstructured) information. Today's portal systems allow combining access modules to different information sources side by side on a single portal webpage. However, there is no interaction between those so called portlets. When a user navigates within one portlet, the others remain unchanged, which means that each source has to be searched individually for relevant information.This paper discusses integration aspects within enterprise knowledge portals and presents an approach for communicating the user context (revealing the user's information need) among portlets, utilizing Semantic Web technologies. For example, the query context of an OLAP portlet, which provides access to structured data stored in a data warehouse, can be used by an information retrieval portlet in order to automatically provide the user with related documents found in the organization's document management system. The paper shortly presents a prototype that we are building to evaluate our approach, demonstrating such an OLAP and information retrieval integration.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {216–223},
numpages = {8},
keywords = {knowledge management, semantic web, information retrieval, portals, OLAP, integration},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956907,
author = {Yang, Guizhen and Ramakrishnan, I. V. and Kifer, Michael},
title = {On the Complexity of Schema Inference from Web Pages in the Presence of Nullable Data Attributes},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956907},
doi = {10.1145/956863.956907},
abstract = {An increasingly large number of Web pages are machine-generated by filling in templates with data stored in backend databases. These templates can be viewed as the implicit schemas of those Web pages. The ability to infer the implicit schema from a collection of Web pages is important for scalable data extraction, since the inferred schema can be used to automatically identify schema attributes that are "encoded" in Web pages.However, the task of inferring a "good" schema is complicated due to the existence of nullable (missing) data attributes. Usually if an attribute contains a null value, then it will be omitted in the generated Web page, giving rise to different variations and permutations of layout structures in Web pages that are generated from the same template.In this paper we investigate the complexity of schema inference from Web pages in the presence of nullable data attributes. We introduce the notion of unambiguity as a quality measure for inferred schemas and prove that the problem of inferring "good" (unambiguous) schemas is NP-complete. Our complexity results imply that ambiguity resolution is one of the root causes of the computational difficulty underlying schema inference from Web pages.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {224–231},
numpages = {8},
keywords = {world wide web, data mining, schema inference, data extraction, machine learning, wrapper induction, web mining},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244160,
author = {Gloor, Peter A.},
title = {Session Details: Knowledge Management Session 3: Classification},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244160},
doi = {10.1145/3244160},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956909,
author = {Yu, Hwanjo and Zhai, ChengXiang and Han, Jiawei},
title = {Text Classification from Positive and Unlabeled Documents},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956909},
doi = {10.1145/956863.956909},
abstract = {Most existing studies of text classification assume that the training data are completely labeled. In reality, however, many information retrieval problems can be more accurately described as learning a binary classifier from a set of incompletely labeled examples, where we typically have a small number of labeled positive examples and a very large number of unlabeled examples. In this paper, we study such a problem of performing Text Classification WithOut labeled Negative data TC-WON). In this paper, we explore an efficient extension of the standard Support Vector Machine (SVM) approach, called SVMC (Support Vector Mapping Convergence) [17]for the TC-WON tasks. Our analyses show that when the positive training data is not too under-sampled, SVMC significantly outperforms other methods because SVMC basically exploits the natural "gap" between positive and negative documents in the feature space, which eventually corresponds to improving the generalization performance. In the text domain there are likely to exist many gaps in the feature space because a document is usually mapped to a sparse and high dimensional feature space. However, as the number of positive training data decreases, the boundary of SVMC starts overfitting at some point and end up generating very poor results.This is because when the positive training data is too few, the boundary over-iterates and trespasses the natural gaps between positive and negative class in the feature space and thus ends up fitting tightly around the few positive training data.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {232–239},
numpages = {8},
keywords = {text filtering, machine learning, text classification, SVM},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956910,
author = {Macskassy, Sofus A. and Hirsh, Haym},
title = {Adding Numbers to Text Classification},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956910},
doi = {10.1145/956863.956910},
abstract = {Many real-world problems involve a combination of both text- and numerical-valued features. For example, in email classification, it is possible to use instance representations that consider not only the text of each message, but also numerical-valued features such as the length of the message or the time of day at which it was sent. Text-classification methods have thus far not easily incorporated numerical features. In earlier work we described an approach for converting numerical features into bags of tokens so that text classification methods can be applied to numerical classification problems, and showed that the resulting learning methods are competitive with traditional numerical classification methods. In this paper we use this as a way to learn on problems that involve a combination of text and numbers. We show that the results outperform competing methods. Further, we show that selecting a best classification method using text-only features and then adding numerical features to the problem (as might happen if numerical features are only later added to a pre existing text-classification problem) gives performance that rivals a more time-consuming approach of evaluating all classification methods using the full set of both text and numerical features.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {240–246},
numpages = {7},
keywords = {machine learning, text classification, information retrieval, numerical features},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956911,
author = {Shanahan, James G. and Roma, Norbert},
title = {Boosting Support Vector Machines for Text Classification through Parameter-Free Threshold Relaxation},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956911},
doi = {10.1145/956863.956911},
abstract = {Support vector machine (SVM) learning algorithms focus on finding the hyperplane that maximizes the margin (the distance from the separating hyperplane to the nearest examples) since this criterion provides a good upper bound of the generalization error. When applied to text classification, these learning algorithms lead to SVMs with excellent precision but poor recall. Various relaxation approaches have been proposed to counter this problem including: asymmetric SVM learning algorithms (soft SVMs with asymmetric misclassification costs); uneven margin based learning; and thresholding. A review of these approaches is presented here. In addition, in this paper, we describe a new threshold relaxation algorithm. This approach builds on previous thresholding work based upon the beta-gamma algorithm. The proposed thresholding strategy is parameter free, relying on a process of retrofitting and cross validation to set algorithm parameters empirically, whereas our previous approach required the specification of two parameters (beta and gamma). The proposed approach is more efficient, does not require the specification of any parameters, and similarly to the parameter-based approach, boosts the performance of baseline SVMs by at least 20% for standard information retrieval measures.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {247–254},
numpages = {8},
keywords = {thresholding, text classification, support vector machines},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244152,
author = {Conrad, Jack},
title = {Session Details: Information Retrieval Session 5: General Retrieval Issues II},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244152},
doi = {10.1145/3244152},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956913,
author = {Amitay, Einat and Nelken, Rani and Niblack, Wayne and Sivan, Ron and Soffer, Aya},
title = {Multi-Resolution Disambiguation of Term Occurrences},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956913},
doi = {10.1145/956863.956913},
abstract = {We describe a system for extracting mentions of terms such as company and product names, in a large and noisy corpus of documents, such as the World Wide Web. Since natural language terms are highly ambiguous, a significant challenge in this task is disambiguating which occurrences of each term are truly related to the right meaning, and which are not. We describe our approach for disambiguation, and show that it achieves very high accuracy with only limited training. This serves as a necessary first step for applications that strive to do analytics on term mentions.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {255–262},
numpages = {8},
keywords = {disambiguation, information retrieval, text mining, natural language processing},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956914,
author = {Allan, James and Feng, Ao and Bolivar, Alvaro},
title = {Flexible Intrinsic Evaluation of Hierarchical Clustering for TDT},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956914},
doi = {10.1145/956863.956914},
abstract = {The Topic Detection and Tracking (TDT) evaluation program has included a "cluster detection" task since its inception in 1996. Systems were required to process a stream of broadcast news stories and partition them into non-overlapping clusters. A system's effectiveness was measured by comparing the generated clusters to "truth" clusters created by human annotators. Starting in 2003, TDT is moving to a more realistic model that permits overlapping clusters (stories may be on more than one topic) and encourages the creation of a hierarchy to structure the relationships between clusters (topics). We explore a range of possible evaluation models for this modified TDT clustering task to understand the best approach for mapping between the human-generated "truth" clusters and a much richer hierarchical structure. We demonstrate that some obvious evaluation techniques fail for degenerate cases. For a few others we attempt to develop an intuitive sense of what the evaluation numbers mean. We settle on some approaches that incorporate a strong balance between cluster errors (misses and false alarms) and the distance it takes to travel between stories within the hierarchy.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {263–270},
numpages = {8},
keywords = {evaluation, hierarchical clustering, cluster detection},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956916,
author = {Jiang, Qingchun and Chakravarthy, Sharma},
title = {Queueing Analysis of Relational Operators for Continuous Data Streams},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956916},
doi = {10.1145/956863.956916},
abstract = {Currently, stream data processing is an active area of research, which includes everything from algorithms and architectures for stream processing to modelling, and analysis of various components of a stream processing system. In this paper, we present an analysis of relational operators used for stream processing using queueing theory and study behaviors of streaming data in a query processing system. Our approach enables us to compute the fundamental performance metrics of relational operators ---select, project, and join over data streams. Furthermore, this approach establishes a way to find the probability distribution functions of both the number of tuples and the waiting time of tuples in the system. Finally, we designed and implemented a number of experiments to validate the accuracy and effectiveness of our analysis.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {271–278},
numpages = {8},
keywords = {relational operators, queueing analysis, data stream},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956917,
author = {Su, Hong and Jian, Jinhui and Rundensteiner, Elke A.},
title = {Raindrop: A Uniform and Layered Algebraic Framework for XQueries on XML Streams},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956917},
doi = {10.1145/956863.956917},
abstract = {XML stream applications bring the challenge of efficiently
processing queries on sequentially accessible token-based data.
While the automata model is naturally suited for pattern matching
on tokenized XML streams, the algebraic model in contrast is a
well-established technique for set-oriented processing of
self-contained tuples. However, neither automata nor algebraic
models are well-equipped to handle both computation paradigms.The goal of the Raindrop project is to accommodate these
two paradigms within one algebraic framework to take advantage of
both. In our query model, both tokenized data and self-contained
tuples are supported in a uniform manner. Query plans can be
flexibly rewritten using equivalence rules to change what
computation is done using tokenized data versus tuples. This paper
highlights the four abstraction levels in Raindrop, namely,
semantics-focused plan, stream logical plan, stream physical
plan and execution plan. Various optimization techniques
are provided at each level. The necessity of such a uniform and
layered plan is shown by experimental study},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {279–286},
numpages = {8},
keywords = {query processing, XQuery algebra, XML stream},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956918,
author = {Jin, Cheqing and Qian, Weining and Sha, Chaofeng and Yu, Jeffrey X. and Zhou, Aoying},
title = {Dynamically Maintaining Frequent Items over a Data Stream},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956918},
doi = {10.1145/956863.956918},
abstract = {It is challenge to maintain frequent items over a data stream, with a small bounded memory, in a dynamic environment where both insertion/deletion of items are allowed. In this paper, we propose a new novel algorithm, called hCount, which can handle both insertion and deletion of items with a much less memory space than the best reported algorithm. Our algorithm is also superior in terms of precision, recall and processing time. In addition, our approach does not request the preknowledge on the size of range for a data stream, and can handle range extension dynamically. Given a little modification, algorithm hCount can be improved to hCount*, which even owns significantly better performance than before.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {287–294},
numpages = {8},
keywords = {frequent items, algorithm, stream},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244153,
author = {Yu, Hwanjo},
title = {Session Details: Knowledge Management Session 4: Indexing},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244153},
doi = {10.1145/3244153},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956920,
author = {Adami, Giordano and Avesani, Paolo and Sona, Diego},
title = {Bootstrapping for Hierarchical Document Classification},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956920},
doi = {10.1145/956863.956920},
abstract = {Managing the hierarchical organization of data is starting to play a key role in the knowledge management community due to the great amount of human resources needed to create and maintain these organized repositories of information. Machine learning community has in part addressed this problem by developing hierarchical supervised classifiers that help maintainers to categorize new resources within given hierarchies. Although such learning models succeed in exploiting relational knowledge, they are highly demanding in terms of labeled examples, because the number of categories is related to the dimension of the corresponding hierarchy. Hence, the creation of new directories or the modification of existing ones require strong investments.This paper proposes a semi-automatic process (interleaved with human suggestions) whose aim is to minimize (simplify) the work required to the administrators when creating, modifying, and maintaining directories. Within this process, bootstrapping a taxonomy with examples represents a critical factor for the effective exploitation of any supervised learning model. For this reason we propose a method for the bootstrapping process that makes a first hypothesis of categorization for a set of unlabeled documents, with respect to a given empty hierarchy of concepts. Based on a revision of Self-Organizing Maps, namely TaxSOM, the proposed model performs an unsupervised classification, exploiting the a-priori knowledge encoded in a taxonomy structure both at the terminological and topological level. The ultimate goal of TaxSOM is to create the premise for successfully training a supervised classifier.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {295–302},
numpages = {8},
keywords = {k-means, TaxSOM, taxonomy bootstrapping process, text categorization, constrained clustering},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956921,
author = {Mayfield, James and McNamee, Paul and Piatko, Christine and Pearce, Claudia},
title = {Lattice-Based Tagging Using Support Vector Machines},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956921},
doi = {10.1145/956863.956921},
abstract = {Tagging algorithms have become increasingly important for identifying lexical and semantic features of unstructured text. We describe an approach to lattice-based tagging that estimates joint transition and emission probabilities using support vector machines. The technique offers several advantages over alternative methods, including the ability to accommodate non-local features, support for hundreds of thousands of features, and language-neutrality. We demonstrate the technique on two tagging applications: named entity recognition and part-of-speech tagging.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {303–308},
numpages = {6},
keywords = {named entity recognition, SVM-Lattice, part of speech tagging, support vector machines, tagging},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956922,
author = {Jin, Rong and Si, Luo and Zhai, ChengXiang and Callan, Jamie},
title = {Collaborative Filtering with Decoupled Models for Preferences and Ratings},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956922},
doi = {10.1145/956863.956922},
abstract = {In this paper, we describe a new model for collaborative filtering. The motivation of this work comes from the fact that two users with very similar preferences on items may have very different rating schemes. For example, one user may tend to assign a higher rating to all items than another user. Unlike previous models of collaborative filtering, which determine the similarity between two users only based on their rating performance, our model treats the user's preferences on items separately from the user's rating scheme. More specifically, for each user, we build two separate models: a preference model capturing which items are favored by the user and a rating model capturing how the user would rate an item given the preference information. The similarity of two users is computed based on the underlying preference model, instead of the surface ratings. We compare the new model with several representative previous approaches on two data sets. Experiment results show that the new model outperforms all the previous approaches that are tested consistently on both data sets.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {309–316},
numpages = {8},
keywords = {collaborative filtering, probabilistic model, rating model, application, preference model},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244154,
author = {Grossman, David},
title = {Session Details: Information Retrieval Session 6: Categorization},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244154},
doi = {10.1145/3244154},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956924,
author = {Li, Tao and Zhu, Shenghuo and Ogihara, Mitsunori},
title = {Efficient Multi-Way Text Categorization via Generalized Discriminant Analysis},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956924},
doi = {10.1145/956863.956924},
abstract = {Text categorization is an important research area and has been receiving much attention due to the growth of the on-line information and of Internet. Automated text categorization is generally cast as a multi-class classification problem. Much of previous work focused on binary document classification problems. Support vector machines (SVMs) excel in binary classification, but the elegant theory behind large-margin hyperplane cannot be easily extended to multi-class text classification. In addition, the training time and scaling are also important concerns. On the other hand, other techniques naturally extensible to handle multi-class classification are generally not as accurate as SVM. This paper presents a simple and efficient solution to multi-class text categorization. Classification problems are first formulated as optimization via discriminant analysis. Text categorization is then cast as the problem of finding coordinate transformations that reflects the inherent similarity from the data. While most of the previous approaches decompose a multi-class classification problem into multiple independent binary classification tasks, the proposed approach enables direct multi-class classification. By using Generalized Singular Value Decomposition (GSVD), a coordinate transformation that reflects the inherent class structure indicated by the generalized singular values is identified. Extensive experiments demonstrate the efficiency and effectiveness of the proposed approach.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {317–324},
numpages = {8},
keywords = {multi-class text categorization, GSVD, discriminant analysis},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956925,
author = {Gravano, Luis and Hatzivassiloglou, Vasileios and Lichtenstein, Richard},
title = {Categorizing Web Queries According to Geographical Locality},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956925},
doi = {10.1145/956863.956925},
abstract = {Web pages (and resources, in general) can be characterized according to their geographical locality. For example, a web page with general information about wildflowers could be considered a global page, likely to be of interest to a geographically broad audience. In contrast, a web page with listings on houses for sale in a specific city could be regarded as a local page, likely to be of interest only to an audience in a relatively narrow region. Similarly, some search engine queries (implicitly) target global pages, while other queries are after local pages. For example, the best results for query [wildflowers] are probably global pages about wildflowers such as the one discussed above. However, local pages that are relevant to, say, San Francisco are likely to be good matches for a query [houses for sale] that was issued by a San Francisco resident or by somebody moving to that city. Unfortunately, search engines do not analyze the geographical locality of queries and users, and hence often produce sub-optimal results. Thus query [wildflowers] might return pages that discuss wildflowers in specific U.S. states (and not general information about wildflowers), while query [houses for sale] might return pages with real estate listings for locations other than that of interest to the person who issued the query. Deciding whether an unseen query should produce mostly local or global pages---without placing this burden on the search engine users---is an important and challenging problem, because queries are often ambiguous or underspecify the information they are after. In this paper, we address this problem by first defining how to categorize queries according to their (often implicit) geographical locality. We then introduce several alternatives for automatically and efficiently categorizing queries in our scheme, using a variety of state-of-the-art machine learning tools. We report a thorough evaluation of our classifiers using a large sample of queries from a real web search engine, and conclude by discussing how our query categorization approach can help improve query result quality.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {325–333},
numpages = {9},
keywords = {search engines, web search, information retrieval, query modification, query classification},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956926,
author = {Shanks, Vaughan R. and Williams, Hugh E.},
title = {Index Construction for Linear Categorisation},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956926},
doi = {10.1145/956863.956926},
abstract = {Categorisation is a useful method for organising documents into subcollections that can be browsed or searched to more accurately and quickly meet information needs. On the Web, category-based portals such as Yahoo! and DMOZ are extremely popular: DMOZ is maintained by over 56,000 volunteers, is used as the basis of the popular Google directory, and is perhaps used by millions of users each day. Support Vector Machines (SVM) is a machine-learning algorithm which has been shown to be highly effective for automatic text categorisation. However, a problem with iterative training techniques such as SVM is that during their learning or training phase, they require the entire training collection to be held in main-memory; this is infeasible for large training collections such as DMOZ or large news wire feeds. In this paper, we show how inverted indexes can be used for scalable training in categorisation, and propose novel heuristics for a fast, accurate, and memory efficient approach. Our results show that an index can be constructed on a desktop workstation with little effect on categorisation accu-racy compared to a memory-based approach. We conclude that our techniques permit automatic categorisation using very large train-ing collections, vocabularies, and numbers of categories.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {334–341},
numpages = {8},
keywords = {categorisation, index construction, efficiency, support vector machines},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956928,
author = {Onizuka, Makoto},
title = {Light-Weight XPath Processing of XML Stream with Deterministic Automata},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956928},
doi = {10.1145/956863.956928},
abstract = {Several applications based on XML stream processing have recently emerged, such as those for air traffic control and the selective dissemination of information (SDI). Their common need is to process a large number of XPath expressions in continuous XML streams at high throughput.This paper proposes four techniques for XPath expression processing based on Deterministic Finite Automata (DFA) for two purposes: to improve the memory usage efficiency of the automata and to support the processing of branching XPath expressions. The first technique, called n-DFA, clusters the given XPath expressions into n clusters to reduce the number of DFA states. The second, called shared NFA state table, lets the Non-Deterministic Finite Automata (NFA) state set be shared among the DFA states. Our experiments show that memory usage in an 8-DFA can, with the shared NFA state table, be reduced to 1/40th that of the original 1-DFA. The optimized NFA conversion and general XPath expression processing algorithm techniques contribute to the processing of branching XPath expressions efficiently; overall performance is better than is possible with earlier approaches.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {342–349},
numpages = {8},
keywords = {XPath processing, automata, streaming XML, selective dissemination of information},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956929,
author = {Fisher, Damien K. and Lam, Franky and Shui, William M. and Wong, Raymond K.},
title = {Efficient Ordering for XML Data},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956929},
doi = {10.1145/956863.956929},
abstract = {With the increasing popularity of XML, there arises the need for managing and querying information in this form. Several query languages, such as XQuery, have been proposed which return their results in document order. However, most recent efforts focused on query optimization have disregarded order. This paper presents a simple yet elegant method to maintain document ordering for XML data. Analysis of our method shows that it is indeed efficient and scalable, even for changing data.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {350–357},
numpages = {8},
keywords = {order maintenance, XML, semi-structured data, dynamic},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956930,
author = {Aboulnaga, Ashraf and Naughton, Jeffrey F.},
title = {Building XML Statistics for the Hidden Web},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956930},
doi = {10.1145/956863.956930},
abstract = {There have been several techniques proposed for building statistics for static XML data. However, very little work has been done in the area of building XML statistics for data sources that export XML views of data that is stored in relational or other databases. For such data sources, we need statistics that are built in an on-line manner, by observing the XML queries to the data sources and their results. In this paper, we present a technique for building on-line XML statistics by observing the XPath queries issued to a data source and their result sizes. These XPath queries select parts of the virtual XML document representing the XML view of the data at the data source. We convert these XPath queries to a more abstract and generalized form that we call annotated path expressions. We present a technique for storing these annotated path expressions and information about their selectivity for use in estimating the selectivity of future XPath queries. We also present an experimental evaluation of our proposed approach.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {358–365},
numpages = {8},
keywords = {database statistics, query optimization, XML, hidden web, selectivity estimation},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244155,
author = {Li, Hua},
title = {Session Details: Industry Session 2: Meditation and Data Sharing},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244155},
doi = {10.1145/3244155},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956932,
author = {Obrst, Leo},
title = {Ontologies for Semantically Interoperable Systems},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956932},
doi = {10.1145/956863.956932},
abstract = {In this paper, we discuss the use of ontologies for semantic interoperability and integration. We argue that information technology has evolved into a world of largely loosely coupled systems and as such, needs increasingly more explicit, machine-interpretable semantics. Ontologies in the form of logical domain theories and their knowledge bases offer the richest representations of machine-interpretable semantics for systems and databases in the loosely coupled world, thus ensuring greater semantic interoperability and integration. Finally, we discuss how ontologies support semantic interoperability in the real, commercial and governmental world.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {366–369},
numpages = {4},
keywords = {semantic interoperability, ontological engineering, ontologies, semantic integration},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956933,
author = {Boucelma, Omar and Garinet, Jean-Yves and Lacroix, Zo\'{e}},
title = {The VirGIS WFS-Based Spatial Mediation System},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956933},
doi = {10.1145/956863.956933},
abstract = {The proliferation of spatial data on the Internet is beginning to allow a much wider access to data currently available in various Geographic Information Systems (GIS). In order to provide effective spatial database integration, we need to provide flexible and powerful GIS data integration solutions. Indeed, GIS are highly heterogeneous: not only they differ by their data representations, but they also offer radically different query languages. A GIS mediation approach should provide an integrated view of the data supplied by all sources, and a geographical query language to access and manipulate integrated data.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {370–374},
numpages = {5},
keywords = {geography markup language (GML), web feature server (WFS), mediation, XML, geographic information systems (GIS)},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956934,
author = {Smith, Kenneth and Swarup, Vipin and Jajodia, Sushil and Faatz, Donald and Cornett, Todd and Hoyt, Jeff},
title = {Securely Sharing Neuroimagery},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956934},
doi = {10.1145/956863.956934},
abstract = {Shared scientific data, such as neuroimagery, offers great benefits to science. However, data owners must exercise custodial responsibilities which can conflict with the unhindered sharing of their data. Given simple choices of sharing widely or not at all, the result will frequently be no sharing. We hypothesize that neuroimagery sharing will be enhanced if data owners are provided with well-defined intermediate levels of data visibility. In this paper, we describe a broadly applicable data sharing model, Structured Sharing Communities (SSC), in which data becomes incrementally visible to communities structured as a complete partial-order; the associated properties of Privacy and Fairness regulate access to shared data. Within SSC, a customized policy space is defined capturing the sharing relationships among specific collaborators.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {375–377},
numpages = {3},
keywords = {scientific data, neuroimagery, data sharing, community of interest},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244156,
author = {Goharian, Nazli},
title = {Session Details: Information Retrieval Session 7: Web},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244156},
doi = {10.1145/3244156},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956936,
author = {Fisher, Michelle and Everson, Richard},
title = {Representing Interests as a Hyperlinked Document Collection},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956936},
doi = {10.1145/956863.956936},
abstract = {We describe a latent variable model for representing a user's interests as a hyperlinked document collection. By collecting hyper-text documents that a user views, creates or updates whilst at their computer, we are able to use not only the content of these documents but also the inter-connectivity of the collection to model the user's interests. The model uses Probabilistic Latent Semantic Analysis and Probabilistic Hypertext Induced Topic Selection and decomposes the user's document collection into a set of factors each of which represents a user's interest. This model can be used to personalise information access tasks such as a personalised search engine or a personalised news service. Our latent variable model's performance is compared with that of a more conventional vector space clustering algorithm.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {378–385},
numpages = {8},
keywords = {hyperlinked/hypertext document collections, user interests, latent variable models, information access},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956937,
author = {Khoussainov, Rinat and Kushmerick, Nicholas},
title = {Automated Index Management for Distributed Web Search},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956937},
doi = {10.1145/956863.956937},
abstract = {Distributed heterogeneous search systems are an emerging phenomenon in Web search, in which independent topic-specific search engines provide search services, and metasearchers distribute user's queries to only the most suitable search engines. Previous research has investigated methods for engine selection and merging of search results (i.e. performance improvements from the user's perspective). We focus instead on performance from the service provider's point of view (e.g, income from queries processed vs. resources used to answer them). We consider a scenario in which individual search engines compete for user queries by choosing which documents (topics) to index. The difficulty here stems from the fact that the utilities of local engine actions should depend on the uncertain actions of competitors. Thus, naive strategies (e.g, blindly indexing lots of popular documents) are ineffective. We model the competition between search engines as a stochastic game, and propose a reinforcement learning approach to managing search index contents. We evaluate our approach using a large log of user queries to 47 real search engines.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {386–393},
numpages = {8},
keywords = {stochastic game, distributed web search, reinforcement learning},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956938,
author = {Calado, P\'{a}vel and Cristo, Marco and Moura, Edleno and Ziviani, Nivio and Ribeiro-Neto, Berthier and Gon\c{c}alves, Marcos Andr\'{e}},
title = {Combining Link-Based and Content-Based Methods for Web Document Classification},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956938},
doi = {10.1145/956863.956938},
abstract = {This paper studies how link information can be used to improve classification results for Web collections. We evaluate four different measures of subject similarity, derived from the Web link structure, and determine how accurate they are in predicting document categories. Using a Bayesian network model, we combine these measures with the results obtained by traditional content-based classifiers. Experiments on a Web directory show that best results are achieved when links from pages outside the directory are considered. Link information alone is able to obtain gains of up to 46 points in  F1, when compared to a traditional content-based classifier. The combination with content-based methods can further improve the results, but too much noise may be introduced, since the text of Web pages is a much less reliable source of information. This work provides an important insight on which measures derived from links are more appropriate to compare Web documents and how these measures can be combined with content-based algorithms to improve the effectiveness of Web classification.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {394–401},
numpages = {8},
keywords = {classification, link analysis, web, Bayesian networks},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956940,
author = {Bhowmick, Sourav S. and Vedagiri, Vivek and Laud, Amey},
title = {HyperThesis: The GRNA Spell on the Curse of Bioinformatics Applications Integration},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956940},
doi = {10.1145/956863.956940},
abstract = {In this paper, we describe a graphical workflow management system called HyperThesis to address the challenges of integrating bioinformatics applications. HyperThesis is an integral component of the Genomics Research Network Architecture (gRNA). The gRNA was designed and developed to address the challenges of developing new bioinformatics applications. Specifically, HyperThesis makes constructing workflows (pipelines of execution of applications) in the gRNA fast and intuitive for biologists and bio-programmers alike. It provides a large repository of interconnectable, parameterized workflow components for processing and relating diverse biological data and software programs. It also enables us to add new workflow components as new algorithms develop in ones area of interest. HyperThesis has been fully implemented using Java.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {402–409},
numpages = {8},
keywords = {visual components, bioinformatics, applications integration, gRNA, work flow, hyperThesis, XomatiQ, GUI},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956941,
author = {Subramaniam, L. Venkata and Mukherjea, Sougata and Kankar, Pankaj and Srivastava, Biplav and Batra, Vishal S. and Kamesam, Pasumarti V. and Kothari, Ravi},
title = {Information Extraction from Biomedical Literature: Methodology, Evaluation and an Application},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956941},
doi = {10.1145/956863.956941},
abstract = {Journals and conference proceedings represent the dominant mechanisms of reporting new biomedical results. The unstructured nature of such publications makes it difficult to utilize data mining or automated knowledge discovery techniques. Annotation (or markup) of these unstructured documents represents the first step in making these documents machine analyzable. In this paper we first present a system called BioAnnotator for identifying and annotating biological terms in documents. BioAnnotator uses domain based dictionary look-up for recognizing known terms and a rule engine for discovering new terms. The combination and dictionary look-up and rules result in good performance (87% precision and 94% recall on the GENIA 1.1 corpus for extracting general biological terms based on an approximate matching criterion). To demonstrate the subsequent mining and knowledge discovery activities that are made feasible by BioAnnotator, we also present a system called MedSummarizer that uses the extracted terms to identify the common concepts in a given group of genes.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {410–417},
numpages = {8},
keywords = {biological document processing, information extraction},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956942,
author = {Tang, Chun and Zhang, Aidong},
title = {Mining Multiple Phenotype Structures Underlying Gene Expression Profiles},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956942},
doi = {10.1145/956863.956942},
abstract = {DNA microarray technology is now widely used in basic biomedical research for mRNA expression profiling and are increasingly being used to explore patterns of gene expression in clinical research. Automatically detecting phenotype structures from gene expression profiles can provide deep insight into the nature of many diseases as well as lead in the development of new drugs. While most of the previous studies focus on only mining empirical phenotype structure which the experiment controls, it is also interesting to detect possible hidden phenotype structures underlying gene expression profiles.Since the number of samples is usually limited, such data sets are very sparse in high-dimensional gene space. Furthermore, most of the genes of interest are buried in large amount of noise. Unsupervised phenotype structure discovery of such sparse high-dimensional data sets present interesting but challenging problems. In this paper, we propose the model of simultaneously mining both empirical and hidden phenotype structures from gene expression data. We demonstrate the effectiveness and efficiency of the proposed method on various real-world data sets.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {418–425},
numpages = {8},
keywords = {array data, phenotype, informative genes, bioinformatics},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244157,
author = {Yee, Wai Gen},
title = {Session Details: Information Retrieval Session 8: Efficiency},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244157},
doi = {10.1145/3244157},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956944,
author = {Broder, Andrei Z. and Carmel, David and Herscovici, Michael and Soffer, Aya and Zien, Jason},
title = {Efficient Query Evaluation Using a Two-Level Retrieval Process},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956944},
doi = {10.1145/956863.956944},
abstract = {We present an efficient query evaluation method based on a two level approach: at the first level, our method iterates in parallel over query term postings and identifies candidate documents using an approximate evaluation taking into account only partial information on term occurrences and no query independent factors; at the second level, promising candidates are fully evaluated and their exact scores are computed. The efficiency of the evaluation process can be improved significantly using dynamic pruning techniques with very little cost in effectiveness. The amount of pruning can be controlled by the user as a function of time allocated for query evaluation. Experimentally, using the TREC Web Track data, we have determined that our algorithm significantly reduces the total number of full evaluations by more than 90%, almost without any loss in precision or recall. At the heart of our approach there is an efficient implementation of a new Boolean construct called WAND or Weak AND that might be of independent interest.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {426–434},
numpages = {9},
keywords = {document-at-a-time, WAND, efficient query evaluation},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956945,
author = {Chowdhury, Abdur and Pass, Greg},
title = {Operational Requirements for Scalable Search Systems},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956945},
doi = {10.1145/956863.956945},
abstract = {Prior research into search system scalability has primarily addressed query processing efficiency [1, 2, 3] or indexing efficiency [3], or has presented some arbitrary system architecture [4]. Little work has introduced any formal theoretical framework for evaluating architectures with regard to specific operational requirements, or for comparing architectures beyond simple timings [5] or basic simulations [6, 7]. In this paper, we present a framework based upon queuing network theory for analyzing search systems in terms of operational requirements. We use response time, throughput, and utilization as the key operational characteristics for evaluating performance. Within this framework, we present a scalability strategy that combines index partitioning and index replication to satisfy a given set of requirement.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {435–442},
numpages = {8},
keywords = {search scalability, operational requirements},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956946,
author = {Conrad, Jack G. and Guo, Xi S. and Schriber, Cindy P.},
title = {Online Duplicate Document Detection: Signature Reliability in a Dynamic Retrieval Environment},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956946},
doi = {10.1145/956863.956946},
abstract = {As online document collections continue to expand, both on the Web and in proprietary environments, the need for duplicate detection becomes more critical. Few users wish to retrieve search results consisting of sets of duplicate documents, whether identical duplicates or close matches. Our goal in this work is to investigate the phenomenon and determine one or more approaches that minimize its impact on search results. Recent work has focused on using some form of signature to characterize a document in order to reduce the complexity of document comparisons. A representative technique constructs a 'fingerprint' of the rarest or richest features in a document using collection statistics as criteria for feature selection. One of the challenges of this approach, however, arises from the fact that in production environments, collections of documents are always changing, with new documents, or new versions of documents, arriving frequently, and other documents periodically removed. When an enterprise proceeds to freeze a training collection in order to stabilize the underlying repository of such features and its associated collection statistics, issues of coverage and completeness arise. We show that even with very large training collections possessing extremely high feature correlations before and after updates, underlying fingerprints remain sensitive to subtle changes. We explore alternative solutions that benefit from the development of massive meta-collections made up of sizable components from multiple domains. This technique appears to offer a practical foundation for fingerprint stability. We also consider mechanisms for updating training collections while mitigating signature instability. Our research is divided into three parts. We begin with a study of the distribution of duplicate types in two broad-ranging news collections consisting of approximately 50 million documents. We then examine the utility of document signatures in addressing identical or nearly identical duplicate documents and their sensitivity to collection updates. Finally, we investigate a flexible method of characterizing and comparing documents in order to permit the identification of non-identical duplicates. This method has produced promising results following an extensive evaluation using a production-based test collection created by domain experts.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {443–452},
numpages = {10},
keywords = {duplicate document detection, data management, doc signatures},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956948,
author = {Abello, James and Kotidis, Yannis},
title = {Hierarchical Graph Indexing},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956948},
doi = {10.1145/956863.956948},
abstract = {Traffic analysis, in the context of Telecommunications or Internet and Web data, is crucial for large network operations. Data in such networks is often provided as large graphs with hundreds of millions of vertices and edges. We propose efficient techniques for managing such graphs at the storage level in order to facilitate its processing at the interface level(visualization). The methods are based on a hierarchical decomposition of the graph edge set that is inherited from a hierarchical decomposition of the vertex set. Real time navigation is provided by an efficient two level indexing schema called the gkd*-tree. The first level is a variation of a kd-tree index that partitions the edge set in a way that conforms to the hierarchical decomposition and the data distribution (the gkd-tree). The second level is a redundant R-tree that indexes the leaf pages of the gkd-tree. We provide computational results that illustrate the superiority of the gkd-tree against conventional indexes like the kd-tree and the R*-tree both in creation as well as query response times.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {453–460},
numpages = {8},
keywords = {navigation, graph, visualization, index},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956949,
author = {Bukauskas, Linas and Mark, Leo and Omiecinski, Edward and B\"{o}hlen, Michael H.},
title = {ITopN: Incremental Extraction of the N Most Visible Objects},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956949},
doi = {10.1145/956863.956949},
abstract = {The visual exploration of large databases calls for a tight coupling of database and visualization systems. Current visualization systems typically fetch all the data and organize it in a scene tree, which is then used to render the visible data. For immersive data explorations, where an observer navigates in a potentially huge data space and explores selected data regions this approach is inadequate. A scalable approach is to make the database system observer-aware and exchange the data that is visible and most relevant to the observer.In this paper we present iTopN an incremental algorithm for extracting the most visible objects relative to the current position of the observer. We implement iTopN and compare it to an improved version of the R-tree that extends LRU with the caching of the top levels of the R-tree (LW-LRU). Our experiments show that iTopN is orders of magnitude faster than LW-LRU given the same amount of memory. Our experiments also show that for LW-LRU to perform as fast as iTopN it needs three times as much memory.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {461–468},
numpages = {8},
keywords = {moving observer, top most visible objects, incremental observer relative data extraction, indexing visibility ranges},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244158,
author = {Herzog, Otthein},
title = {Session Details: Information Retievalk Session 9: Language Models},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244158},
doi = {10.1145/3244158},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956951,
author = {Li, Xiaoyan and Croft, W. Bruce},
title = {Time-Based Language Models},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956951},
doi = {10.1145/956863.956951},
abstract = {We explore the relationship between time and relevance using TREC ad-hoc queries. A type of query is identified that favors very recent documents. We propose a time-based language model approach to retrieval for these queries. We show how time can be incorporated into both query-likelihood models and relevance models. These models were used for experiments comparing time-based language models to heuristic techniques for incorporating document recency in the ranking. Our results show that time-based models perform as well as or better than the best of the heuristic techniques.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {469–475},
numpages = {7},
keywords = {relevance models, time-based language models, information retrieval, language models, recency queries},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956952,
author = {Srikanth, Munirathnam and Srihari, Rohini},
title = {Exploiting Syntactic Structure of Queries in a Language Modeling Approach to IR},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956952},
doi = {10.1145/956863.956952},
abstract = {Natural Language Processing (NLP) techniques have been explored to enhance the performance of Information Retrieval (IR) methods with varied results. Most efforts in using NLP techniques have been to identify better index terms for representing documents. This use in the indexing phase of IR has implicit effect on retrieval performance. However, the explicit use of NLP techniques during the retrieval or information seeking phase has been restricted to interactive or dialogue systems. Recent advances in IR are based on using Statistical Language Models (SLM) to represent documents and ranking them based on their model generating a given user query. This paper presents a novel method for using NLP techniques on user queries, specifically, a syntactic parse of a query, in the statistical language modeling approach to IR. In the proposed method, named Concept Language Models, a query is viewed as a sequence of concepts and a concept as a sequence terms. The paper presents different approximations to estimate the concept and term probabilities and compute the query likelihood estimate for documents. Some empirical results on TREC test collections comparing Concept Language Models with smoothed N-gram language models are presented.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {476–483},
numpages = {8},
keywords = {information retrieval, language models, query processing, natural language processing},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956953,
author = {Aslam, Javed A. and Pavlu, Virgiliu and Savell, Robert},
title = {A Unified Model for Metasearch, Pooling, and System Evaluation},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956953},
doi = {10.1145/956863.956953},
abstract = {We present a unified model which, given the ranked lists of documents returned by multiple retrieval systems in response to a given query, simultaneously solves the problems of (1) fusing the ranked lists of documents in order to obtain a high-quality combined list (metasearch); (2) generating document collections likely to contain large fractions of relevant documents (pooling); and (3) accurately evaluating the underlying retrieval systems with small numbers of relevance judgments (efficient system assessment). Our approach is based on the Hedge algorithm for on-line learning. In effect, our proposed system "learns" which documents are likely to be relevant from a sequence of on-line relevance judgments. In experiments using TREC data, our methodology is shown to outperform standard methods for metasearch, pooling, and system evaluation, often remarkably so.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {484–491},
numpages = {8},
keywords = {metasearch, pooling, evaluation, active learning},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/3244159,
author = {Seligman, Len},
title = {Session Details: Industry Session 3: Data Analysis, Mining, and Managing XML},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244159},
doi = {10.1145/3244159},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
numpages = {1},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956955,
author = {Han, Eui-Hong and Karypis, George and Mewhort, Doug and Hatchard, Keith},
title = {Intelligent Metasearch Engine for Knowledge Management},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956955},
doi = {10.1145/956863.956955},
abstract = {The explosive growth of available information sources and the resulting information overload pose several problems for users in many business organizations and educational institutions. First, searching through several information sources, one at a time, is a source of enormous frustration for users. Second, top-ranked documents in search results are frequently irrelevant to what users are interested in. To address these problems, we have developed ixmeta™, a powerful metasearch engine that gathers, evaluates, ranks, and reports the most relevant results from multiple information sources, including library catalogs, proprietary databases, intranets, and Web search engines. In addition to basic metasearch capabilities, ixmetafind uses personalization and clustering techniques to find the most relevant results for users. In this paper, we briefly describe technologies used in ixmetafind and present pinpoint™ from Sagebrush Corporation, the smart research tool™ in the kindergarten through twelfth grade (K-12) school environment. Pinpoint showcases ixmetafind in the knowledge management domain of the K-12 school environment.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {492–495},
numpages = {4},
keywords = {personalization, collection fusion, library automation, metasearch, collaboration, clustering},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956956,
author = {Holmes, David},
title = {SQL Text Parsing for Information Retrieval},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956956},
doi = {10.1145/956863.956956},
abstract = {The concept of using a relational database to perform information retrieval (IR) search functions is well established. Prior work demonstrates the capability to perform common functions and advanced ranking algorithms using standard, unchanged SQL. The previous work does not address the preprocessing of unstructured text within the relational model. In fact, the parsing of the unstructured data into a structured data set was done outside of the database, usually using sequential programming languages such as C. This work proves that IR preprocessing does not require proprietary application code to build the framework necessary for searching document databases. Furthermore, the resulting environment is relational and integrates with other data sources within an organization.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {496–499},
numpages = {4},
keywords = {text parsing, SQL},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956957,
author = {Infantes-Morris, Tahia and Bernhard, Philip J. and Fox, Kevin L. and Faulkner, Gary J. and Stripling, Kristina},
title = {Industrial Evaluation of a Highly-Accurate Academic IR System},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956957},
doi = {10.1145/956863.956957},
abstract = {In this paper we report the results of an independent experimental evaluation of an information retrieval (IR) system developed at the Illinois Institute of Technology (IIT). The system, which is called the Advanced Information Retrieval Engine (AIRE), consists of a set of tools and utilities providing indexing, extraction, searching and visualization. We evaluated AIRE on three data sets from the Text REtrieval Conference (TREC) - TREC 8, 9 and 10. Overall, our results indicate that AIRE is a highly accurate IR system. Compared with results published by IIT, in our experiments AIRE consistently scored higher in recall. AIRE also scored higher in precision, but only for automatic tasks. In manual tasks, AIRE scored lower in precision in our experiments, but we attributed that to factors external to AIRE. Our final conclusion is that AIRE is a highly accurate IR system.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {500–503},
numpages = {4},
keywords = {independent industrial evaluation, information retrieval},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956959,
author = {White, Ryen W. and Jose, Joemon M. and Ruthven, Ian},
title = {An Approach for Implicitly Detecting Information Needs},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956959},
doi = {10.1145/956863.956959},
abstract = {Searchers can have problems devising queries that accurately express their, often dynamic, information needs. In this paper we describe an adaptive approach that uses unobtrusive monitoring of interaction to help alleviate such problems and support searchers in their seeking. The approach we propose implicitly selects terms to better represent information needs, gathers evidence on potential changes in these needs, and uses this evidence to tailor the result presentation accordingly. A user evaluation of an interface implementing our approach, presented in [7], shows it can select terms that approximate current information needs and provide evidence to track changes in these needs.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {504–507},
numpages = {4},
keywords = {implicit feedback, query expansion, information need detection},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956960,
author = {Radev, Dragomir R. and Tam, Daniel},
title = {Summarization Evaluation Using Relative Utility},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956960},
doi = {10.1145/956863.956960},
abstract = {We present a series of experiments to demonstrate the validity of Relative Utility (RU) as a measure for evaluating extractive summarizers. RU is applicable in both single-document and multi-document summarization, is extendable to arbitrary compression rates with no extra annotation effort, and takes into account both random system performance and interjudge agreement. Our results using the JHU summary corpus indicate that RU is a reasonable and often superior alternative to several common evaluation metrics.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {508–511},
numpages = {4},
keywords = {evaluation, relative utility, text summarization},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956961,
author = {Ma, Ling and Goharian, Nazli and Chowdhury, Abdur and Chung, Misun},
title = {Extracting Unstructured Data from Template Generated Web Documents},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956961},
doi = {10.1145/956863.956961},
abstract = {We propose a novel approach that identifies web page templates and extracts the unstructured data. Extracting only the body of the page and eliminating the template increases the retrieval precision for the queries that generate irrelevant results. We believe that by reducing the number of irrelevant results; the users are encouraged to go back to a given site to search. Our experimental results on several different web sites and on the whole cnnfn collection demonstrate the feasibility of our approach.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {512–515},
numpages = {4},
keywords = {information retrieval, retrieval accuracy, text extraction, automatic template removal},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956962,
author = {Ramaswamy, Lakshmish and Iyengar, Arun and Liu, Ling and Douglis, Fred},
title = {Techniques for Efficient Fragment Detection in Web Pages},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956962},
doi = {10.1145/956863.956962},
abstract = {The existing approaches to fragment-based publishing, delivery and caching of web pages assume that the web pages are manually fragmented at their respective web sites. However manual fragmentation of web pages is expensive, error prone, and not scalable. This paper proposes a novel scheme to automatically detect and flag possible fragments in a web site. Our approach is based on an analysis of the web pages dynamically generated at given web sites with respect to their information sharing behavior, personalization characteristics and change patterns.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {516–519},
numpages = {4},
keywords = {fragment caching, fragment-based publishing, fragment detection},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956963,
author = {Deschler, Kurt and Rundensteiner, Elke},
title = {MASS: A Multi-Axis Storage Structure for Large XML Documents},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956963},
doi = {10.1145/956863.956963},
abstract = {Effective indexing for XML must consider both the query requirements of the XPath language and the dynamic nature of XML. We introduce MASS, a Multiple Axis Storage Structure, to provide scalable indexing for XPath expressions with guaranteed update performance. We describe the building blocks of MASS and provide results that demonstrate MASS's scalability. We show that MASS can outperform other state-of-the-art XML indexing solutions, even with constrained system resources.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {520–523},
numpages = {4},
keywords = {XPath, clustering, encoding, indexing, XML},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956964,
author = {Kao, Anne and Quach, Lesley and Poteet, Steve and Woods, Steve},
title = {User Assisted Text Classification and Knowledge Management},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956964},
doi = {10.1145/956863.956964},
abstract = {While there are many aspects to managing corporate knowledge, one key issue is how to organize corporate documents into categories of interest. In this paper, we focus on using user assisted text classification in conjunction with a web portal, multiple document management systems and an ontology, to provide a powerful solution for organizing information about a company's technology. We propose a system that interacts with an author using an automatic text classifier to suggest controlled keywords to be used as metadata. The proposed approach does not require professional librarians or that the end users have extensive training. The use of a controlled vocabulary allows for a more consistent description of corporate documents, and promotes easier access by people across the company. It is easier to find similar documents which use different nomenclature. Finally, the interactive nature of the system results in a more correct and precise description of each document than a fully automatic system would.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {524–527},
numpages = {4},
keywords = {web portal, ontology, text mining, knowledge management, text clustering, text summarization, text classification},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956965,
author = {Campbell, Christopher S. and Maglio, Paul P. and Cozzi, Alex and Dom, Byron},
title = {Expertise Identification Using Email Communications},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956965},
doi = {10.1145/956863.956965},
abstract = {A common method for finding information in an organization is to use social networks---ask people, following referrals until someone with the right information is found. Another way is to automatically mine documents to determine who knows what. Email documents seem particularly well suited to this task of "expertise location", as people routinely communicate what they know. Moreover, because people explicitly direct email to one another, social networks are likely to be contained in the patterns of communication. Can these patterns be used to discover experts on particular topics? Is this approach better than mining message content alone? To find answers to these questions, two algorithms for determining expertise from email were compared: a content-based approach that takes account only of email text, and a graph-based ranking algorithm (HITS) that takes account both of text and communication patterns. An evaluation was done using email and explicit expertise ratings from two different organizations. The rankings given by each algorithm were compared to the explicit rankings with the precision and recall measures commonly used in information retrieval, as well as the d' measure commonly used in signal-detection theory. Results show that the graph-based algorithm performs better than the content-based algorithm at identifying experts in both cases, demonstrating that the graph-based algorithm effectively extracts more information than is found in content alone.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {528–531},
numpages = {4},
keywords = {expertise identification, knowledge management},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956966,
author = {Sifer, Mark},
title = {A Visual Interface Technique for Exploring OLAP Data with Coordinated Dimension Hierarchies},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956966},
doi = {10.1145/956863.956966},
abstract = {Multi-dimensional data occurs in many domains while a wide variety of text based and visual interfaces for querying such data exists. But many of these interfaces are not applicable to OLAP, as they do not support use of dimension hierarchies for selection and aggregation. We introduce an interface technique which supports visual querying of OLAP data, that has been implemented in the SGViewer tool. It is based on a data graph rather than a data cube representation of the data. Our interface presents each dimension hierarchy in a zoomable panel which supports selection and aggregation at multiple levels. Users explore data and query it by making selections in several dimension views. Three view coordinations are identified; progressive, global and result only. Our main contribution, the progressive view coordination provides better support for query refinement than existing interfaces, by helping users decide the next query step with intermediate result overviews, and by helping users change a previous selection decision with retained selection context views. Our interface technique is demonstrated with a web log dataset of visits organised into time, download, visitor and referrer address dimensions.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {532–535},
numpages = {4},
keywords = {OLAP, hierarchies, data exploration, interface},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956967,
author = {Chang, Joong Hyuk and Lee, Won Suk},
title = {<i>EstWin</i>: Adaptively Monitoring the Recent Change of Frequent Itemsets over Online Data Streams},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956967},
doi = {10.1145/956863.956967},
abstract = {Knowledge embedded in a data stream is likely to be changed as time goes by. Consequently, identifying the recent change of the knowledge quickly can provide valuable information for the analysis of the data stream. However, most of mining algorithms or frequency approximation algorithms for a data stream do not able to extract the recent change of information in a data stream adaptively. This paper proposes a sliding window-based method that finds recently frequent itemsets over an online data stream adaptively. The size of a window defines a desired life-time of the information in a newly generated transaction. Consequently, only recently generated transactions in the range of the window are considered to find the frequent itemsets of a data stream.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {536–539},
numpages = {4},
keywords = {pruning, data streams, recent change of frequent itemsets, delayed-insertion, sliding window},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956968,
author = {Lalmas, Mounia and Reid, Jane},
title = {Automatic Identification of Best Entry Points for Focused Structured Document Retrieval},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956968},
doi = {10.1145/956863.956968},
abstract = {Focussed structured document retrieval employs the concept of best entry points (BEPs), which are intended to provide optimal starting-points from which users can browse to relevant document components. This paper describes two small-scale studies, using experimental data from the Shakespeare user study, which developed and evaluated different approaches to the problem of automatic identification of BEPs.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {540–543},
numpages = {4},
keywords = {best entry point algorithms, best entry points, focussed structured document retrieval, automatic identification of best entry points},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956969,
author = {Cheng, Pu-Jen and Chien, Lee-Feng},
title = {Auto-Generation of Topic Hierarchies for Web Images from Users' Perspectives},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956969},
doi = {10.1145/956863.956969},
abstract = {In this paper, we propose an approach to automatically generating a Yahoo!-like topic hierarchy for organizing Web images from users' perspectives. Relatively little effort has been devoted towards providing such a taxonomy simultaneously considering users' image requests for semantic and visual information. Based on the characteristic that a Web-image query may be refined by various attributes, the proposed approach hierarchically groups similar queries from search engine logs into topic classes at different semantic levels. The generated topic hierarchy has the advantages of organizing image data from users' perspectives for browsing, searching, annotation and users' needs analysis.A series of experiments have been conducted on real-world image search engine logs. Experimental results show that the proposed approach is feasible to generate topic hierarchies for Web images. Moreover, the generated hierarchy has been successfully applied to analysis of users' search interests, which have more focuses on some specific domains when compared with document requests.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {544–547},
numpages = {4},
keywords = {image classification, query clustering, topic hierarchy},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956970,
author = {Widyantoro, Dwi H. and Ioerger, Thomas R. and Yen, John},
title = {Tracking Changes in User Interests with a Few Relevance Judgments},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956970},
doi = {10.1145/956863.956970},
abstract = {Keeping track of changes in user interests from a document stream with a few relevance judgments is not an easy task. To tackle this problem, we propose a novel method that integrates (1) pseudo-relevance feedback mechanism, (2) assumption about the persistence of user interests and (3) incremental method for data clustering. This approach has been empirically evaluated using Reuters-21578 corpus in a setting for information filtering. The experiment results reveal that it significantly improves the performances of existing user-interest-tracking systems without requiring additional, actual relevance judgments.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {548–551},
numpages = {4},
keywords = {persistence assumption, user interest tracking, incremental data clustering, pseudo-relevance feedback},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956971,
author = {Decurtins, Corsin and Norrie, Moira C. and Signer, Beat},
title = {Digital Annotation of Printed Documents},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956971},
doi = {10.1145/956863.956971},
abstract = {We present a general model and information server for the digital annotation of printed documents. The resulting annotation framework supports both informal and structured annotations as well as context-dependent services. A demonstrator application for mammography that features both enhanced writing and reading activities is described.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {552–555},
numpages = {4},
keywords = {augmented paper, mammography, cross-media annotation},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956972,
author = {Liben-Nowell, David and Kleinberg, Jon},
title = {The Link Prediction Problem for Social Networks},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956972},
doi = {10.1145/956863.956972},
abstract = {Given a snapshot of a social network, can we infer which new interactions among its members are likely to occur in the near future? We formalize this question as the link prediction problem, and develop approaches to link prediction based on measures the "proximity" of nodes in a network. Experiments on large co-authorship networks suggest that information about future interactions can be extracted from network topology alone, and that fairly subtle measures for detecting node proximity can outperform more direct measures.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {556–559},
numpages = {4},
keywords = {link analysis, social networks, link prediction},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956973,
author = {Nallapati, Ramesh and Croft, Bruce and Allan, James},
title = {Relevant Query Feedback in Statistical Language Modeling},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956973},
doi = {10.1145/956863.956973},
abstract = {In traditional relevance feedback, researchers have explored relevant document feedback, wherein, the query representation is updated based on a set of relevant documents returned by the user. In this work, we investigate relevant query feedback, in which we update a document's representation based on a set of relevant queries. We propose four statistical models to incorporate relevant query feedback.To validate our models, we considered anchor text of incoming links to a given document as feedback queries and performed experiments on the home-page retrieval task of TREC 2001. Our results show that three of our four models outperform the query-likelihood baseline by at least 35% in MRR score on a test set.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {560–563},
numpages = {4},
keywords = {relevant document, relevance feedback, relevant query},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956974,
author = {Parkhomenko, Olena and Lee, Yugyung and Park, E. K.},
title = {Ontology-Driven Peer Profiling in Peer-to-Peer Enabled Semantic Web},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956974},
doi = {10.1145/956863.956974},
abstract = {Peer-to-peer (P2P) systems and Semantic Web are two novel technologies that face a lot of shortcomings if considered as isolated paradigms. We present an approach that utilizes ontologies to set up a peer profile containing all the data, necessary for peer-to-peer interoperability. Using this profile can help eliminate some major issues persistent in current P2P networks, such as security, resource aggregation, group management. We also consider applications of peer profiling for Semantic Web built on P2P networks, such as an improved semantic search for resources, not explicitly published on the Web, but available in a P2P system. We develop the ontology-based peer profile in RDF format and demonstrate its manifold benefits for peer communication and knowledge discovery in both P2P networks and Semantic Web.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {564–567},
numpages = {4},
keywords = {semantic web, profiling, ontology, P2P, peer profile},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956975,
author = {Wu, Yi-fang Brook and Shankar, Latha and Chen, Xin},
title = {Finding More Useful Information Faster from Web Search Results},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956975},
doi = {10.1145/956863.956975},
abstract = {In this paper, we propose a prototype system for automatic generation of concept hierarchies to be used as an overview of search results. The system sends a user's query to five search engines and receives a returned list of relevant web pages. The system then extracts query-oriented concept terms from snippets that come with the returned hits. Concept terms are organized into a concept hierarchy using a co-occurrence-based classification technique. Finally, concepts in returned documents are dynamically highlighted according to terms in the selected concept branch that lead to the chosen document. The user study shows that concept hierarchies do provide easy navigation and browsing of web returned documents. The results also show that users can find a document of interest no matter how low it is ranked in the retrieved list.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {568–571},
numpages = {4},
keywords = {concept hierarchy, hierarchical summarization},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

@inproceedings{10.1145/956863.956976,
author = {Lee, Jinho and Grossman, David and Orlandic, Ratko},
title = {An Evaluation of the Incorporation of a Semantic Network into a Multidimensional Retrieval Engine},
year = {2003},
isbn = {1581137230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956863.956976},
doi = {10.1145/956863.956976},
abstract = {This paper describes a new method for incorporating a hierarchical category dimension into an Information Retrieval framework. The approach is to use the synonym sets and the hyponym ("is-a") relations defined within Wordnet in order to derive a conceptual hierarchical category dimension. The hierarchical nature of a category dimension not only provides an overview of a set of documents but also facilitates the effectiveness and the efficiency of searching documents. An evaluation is performed on two different types of models and the multidimensional approach shows a significant reduction in the number of page accesses over a large document collection.},
booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
pages = {572–575},
numpages = {4},
location = {New Orleans, LA, USA},
series = {CIKM '03}
}

