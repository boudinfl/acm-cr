@inproceedings{10.1145/3255053,
author = {Nissim, Kobbi},
title = {Session Details: Session 1: 08:30--8:40},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255053},
doi = {10.1145/3255053},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
numpages = {1},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554799,
author = {Brakerski, Zvika and Vaikuntanathan, Vinod},
title = {Lattice-Based FHE as Secure as PKE},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554799},
doi = {10.1145/2554797.2554799},
abstract = {We show that (leveled) fully homomorphic encryption (FHE) can be based on the hardness of O(n1.5+ε)-approximation for lattice problems (such as GapSVP) under quantum reductions for any ε 〉 0 (or O(n2+ε)-approximation under classical reductions). This matches the best known hardness for "regular" (non-homomorphic) lattice based public-key encryption up to the ε factor. A number of previous methods had hit a roadblock at quasipolynomial approximation. (As usual, a circular security assumption can be used to achieve a non-leveled FHE scheme.)Our approach consists of three main ideas: Noise-bounded sequential evaluation of high fan-in operations; Circuit sequentialization using Barrington's Theorem; and finally, successive dimension-modulus reduction.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {1–12},
numpages = {12},
keywords = {fully homomorphic encryption, cryptography, public key encryption, lattice based cryptography},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554800,
author = {Brody, Joshua and Jakobsen, Sune K. and Scheder, Dominik and Winkler, Peter},
title = {Cryptogenography},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554800},
doi = {10.1145/2554797.2554800},
abstract = {We consider the following cryptographic secret leaking problem. A group of players communicate with the goal of learning (and perhaps revealing) a secret held initially by one of them. Their conversation is monitored by a computationally unlimited eavesdropper, who wants to learn the identity of the secret-holder. Despite the unavailability of key, some protection can be provided to the identity of the secret-holder. We call the study of such communication problems, either from the group's or the eavesdropper's point of view, cryptogenography.We introduce a basic cryptogenography problem and show that two players can force the eavesdropper to missguess the origin of a secret bit with probability 1/3; we complement this with a hardness result showing that they cannot do better than than 3/8. We prove that larger numbers of players can do better than 0.5644, but no group of any size can achieve 0.75.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {13–22},
numpages = {10},
keywords = {communication compelxity, cryptography, cryptogenography, information theory},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554801,
author = {Mahmoody, Mohammad and Maji, Hemanta K. and Prabhakaran, Manoj},
title = {Limits of Random Oracles in Secure Computation},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554801},
doi = {10.1145/2554797.2554801},
abstract = {The seminal result of Impagliazzo and Rudich (STOC 1989) gave a black-box separation between one-way functions and public-key encryption: a public-key encryption scheme cannot be constructed using one-way functions in a black-box way. In addition, their result implied black-box separations between one-way functions and protocols for certain Secure Function Evaluation (SFE) functionalities (in particular, Oblivious Transfer). Surprisingly, however, since then there has been no further progress in separating one-way functions and SFE functionalities. In this work, we present the complete picture for finite deterministic 2-party SFE functionalities, vis a vis one-way functions. We show that in case of semi-honest adversaries, one-way functions are black-box separated from all such SFE functionalities, except the ones which have unconditionally secure protocols (and hence do not rely on any computational hardness). In the case of active adversaries, a black-box one-way function is indeed useful for SFE, but we show that it is useful only as much as access to an ideal commitment functionality is useful.Technically, our main result establishes the limitations of random oracles for secure computation. We show that a two-party deterministic functionality f has a secure protocol in the random oracle model that is (statistically) secure against semi-honest adversaries if and only if f has a protocol in the plain model that is (perfectly) secure against semi-honest adversaries. Further, in the case of active adversaries, a deterministic SFE functionality f has a (UC or standalone) statistically secure protocol in the random oracle model if and only if f has a (UC or standalone) statistically secure protocol in the commitment-hybrid model.Our proof is based on a "frontier analysis" of two-party protocols, combining it with (extensions of) the "independence learners" of Impagliazzo-Rudich/Barak-Mahmoody. We make essential use of a combinatorial property, originally discovered by Kushilevitz (FOCS 1989), of functions that have semi-honest secure protocols in the plain model (and hence our analysis applies only to functions of polynomial-sized domains, for which such a characterization is known).Our result could be seen as a first step towards proving a conjecture that we put forth in this work and call it the Many-Worlds Conjecture. For every 2-party SFE functionality f, one can consider a "world" where f can be semi-honest securely realized in the computational setting. Many-Worlds Conjecture states that there are infinitely many "distinct worlds" between minicrypt and cryptomania in the universe of Impagliazzo's Worlds.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {23–34},
numpages = {12},
keywords = {one-way functions, random oracle model, secure function evaluation, black-box separation, impagliazzo's worlds},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554802,
author = {Vazirani, Umesh and Vidick, Thomas},
title = {Robust Device Independent Quantum Key Distribution},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554802},
doi = {10.1145/2554797.2554802},
abstract = {Quantum cryptography is based on the discovery that the laws of quantum mechanics allow levels of security that are impossible to replicate in a classical world [2, 8, 12]. Can such levels of security be guaranteed even when the quantum devices on which the protocol relies are untrusted? This fundamental question in quantum cryptography dates back to the early nineties when the challenge of achieving device independent quantum key distribution, or DIQKD, was first formulated [9]. We answer this challenge affirmatively by exhibiting a robust protocol for DIQKD and rigorously proving its security. The protocol achieves a linear key rate while tolerating a constant noise rate in the devices. The security proof assumes only that the devices can be modeled by the laws of quantum mechanics and are spatially isolated from each other and any adversary's laboratory. In particular, we emphasize that the devices may have quantum memory. All previous proofs of security relied either on the use of many independent pairs of devices [6, 4, 7], or on the absence of noise [10, 1].To prove security for a DIQKD protocol it is necessary to establish at least that the generated key is truly random even in the presence of a quantum adversary. This is already a challenge, one that was recently resolved [14]. DIQKD is substantially harder, since now the protocol must also guarantee that the key is completely secret from the quantum adversary's point of view, and the entire protocol is robust against noise; this in spite of the substantial amounts of classical information leaked to the adversary throughout the protocol, as part of the error estimation and information reconciliation procedures.Our proof of security builds upon a number of techniques, including randomness extractors that are secure against quantum storage [3] as well as ideas originating in the coding strategy used in the proof of the Holevo-Schumacher-Westmoreland theorem [5, 11] which we apply to bound correlations across multiple rounds in a way not unrelated to information-theoretic proofs of the parallel repetition property for multiplayer games. Our main result can be understood as a new bound on monogamy [13] of entanglement in the type of complex scenario that arises in a key distribution protocol.Precise statements of our results and detailed proofs can be found at arXiv:1210.1810.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {35–36},
numpages = {2},
keywords = {device-independence, quantum key distribution, certified randomness, chsh game, monogamy},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/3255054,
author = {Linial, Nati},
title = {Session Details: Session 2: 10:30--10:40},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255054},
doi = {10.1145/3255054},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
numpages = {1},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554803,
author = {Shpilka, Amir and Tal, Avishay and Volk, Ben lee},
title = {On the Structure of Boolean Functions with Small Spectral Norm},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554803},
doi = {10.1145/2554797.2554803},
abstract = {In this paper we prove results regarding Boolean functions with small spectral norm (the spectral norm of undefined is ||undefined||1 = ∑α|undefined(α)|). Specifically, we prove the following results for functions undefined :{0, 1}n → [0, 1}with ||undefined||1 = A. There is a subspace V of co-dimension at most A2 such that undefined|v is constant.undefined can be computed by a parity decision tree of size 2A2n2a. (a parity decision tree is a decision tree whose nodes are labeled with arbitrary linear functions.)undefined can be computed by a De Morgan formula of size O(2A2 n2A+2) and by a De Morgan formula of depth O( A2 + log(n) • A).If in addition undefined has at most s nonzero Fourier coefficients, then undefined can be computed by a parity decision tree of depth A2log s.For every ε &gt; 0 there is a parity decision tree of depth O(A2 + log(1/ε)) and size 2O(A2)} • min{ 1/ε2,O(log(1/ε))2A} that ε-approximates undefined. Furthermore, this tree can be learned, with probability 1--δ, using poly(n, exp(A2), 1/ε,log(1/δ)) membership queries. All the results above (except ref{abs:DeMorgan}) also hold (with a slight change in parameters) for functions f : Znp → {0, 1}.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {37–48},
numpages = {12},
keywords = {decision trees, analysis of boolean functions, spectral norm},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554805,
author = {Hrube\v{s}, Pavel and Wigderson, Avi},
title = {Non-Commutative Arithmetic Circuits with Division},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554805},
doi = {10.1145/2554797.2554805},
abstract = {We initiate the study of the complexity of arithmetic circuits with division gates over non-commuting variables. Such circuits and formulas compute non-commutative rational functions, which, despite their name, can no longer be expressed as ratios of polynomials. We prove some lower and upper bounds, completeness and simulation results, as follows.If X is n x n matrix consisting of n2 distinct mutually non-commuting variables, we show that:(i). X-1 can be computed by a circuit of polynomial size,(ii). every formula computing some entry of X-1 must have size at least 2Ω(n).We also show that matrix inverse is complete in the following sense:(i). Assume that a non-commutative rational function f can be computed by a formula of size s. Then there exists an invertible 2s x 2s-matrix A whose entries are variables or field elements such that f is an entry of A-1.(ii). If f is a non-commutative polynomial computed by a formula without inverse gates then A can be taken as an upper triangular matrix with field elements on the diagonal.We show how divisions can be eliminated from non-commutative circuits and formulae which compute polynomials, and we address the non-commutative version of the "rational function identity testing" problem. As it happens, the complexity of both of these procedures depends on a single open problem in invariant theory.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {49–66},
numpages = {18},
keywords = {free skew field, non-commutative rational function, arithmetic circuit complexity},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554806,
author = {Wan, Andrew and Wright, John and Wu, Chenggang},
title = {Decision Trees, Protocols and the Entropy-Influence Conjecture},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554806},
doi = {10.1145/2554797.2554806},
abstract = {Given undefined : {--1, 1}n → {-- 1, 1}, define the spectral distribution of undefined to be the distribution on subsets of [n] in which the set S is sampled with probability undefined(S)2. Then the Fourier Entropy-Influence (FEI) conjecture of Friedgut and Kalai [2] states that there is some absolute constant C such that H[undefined2] ≤ C ⋅ Inf[undefined]. Here, H[undefined2] denotes the Shannon entropy of undefined's spectral distribution, and Inf[undefined] is the total influence of undefined. This conjecture is one of the major open problems in the analysis of Boolean functions, and settling it would have several interesting consequences.Previous results on the FEI conjecture have been largely through direct calculation. In this paper we study a natural interpretation of the conjecture, which states that there exists a communication protocol which, given subset S of [n] distributed as undefined2, can communicate the value of S using at most C⋅Inf[undefined] bits in expectation. Using this interpretation, we are able show the following results: First, if undefined is computable by a read-k decision tree, then H[undefined2] ≤ 9k ⋅ Inf[undefined].Next, if undefined has Inf[undefined] ≥ 1 and is computable by a decision tree with expected depth d, then H[[undefined2] ≤ 12d⋅ Inf[undefined].Finally, we give a new proof of the main theorem of O'Donnell and Tan [8], i.e. that their FEI+ conjecture composes.In addition, we show that natural improvements to our decision tree results would be sufficient to prove the FEI conjecture in its entirety. We believe that our methods give more illuminating proofs than previous results about the FEI conjecture.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {67–80},
numpages = {14},
keywords = {fourier entropy-influence, analysis of boolean functions, information theory},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554807,
author = {Gopalan, Parikshit and Vadhan, Salil and Zhou, Yuan},
title = {Locally Testable Codes and Cayley Graphs},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554807},
doi = {10.1145/2554797.2554807},
abstract = {We give two new characterizations of ( 2-linear, smooth) locally testable error-correcting codes in terms of Cayley graphs over Fh2: A locally testable code is equivalent to a Cayley graph over h2 whose set of generators is significantly larger than h and has no short linear dependencies, bbut yields a shortest-path metric that embeds into l  with constant distortion. This extends and gives a converse to a result of Khot and Naor (2006), which showed that codes with large dual distance imply Cayley graphs that have no low-distortion embeddings into l .A locally testable code is equivalent to a Cayley graph over Fh2 that has significantly more than h eigenvalues near 1, which have no short linear dependencies among them and which "explain" all of the large eigenvalues. This extends and gives a converse to a recent construction of Barak et al. (2012), which showed that locally testable codes imply Cayley graphs that are small-set expanders but have many large eigenvalues.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {81–92},
numpages = {12},
keywords = {cayley graphs, metric embeddings, locally testable codes, spectral graph theory, fourier analysis},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/3255055,
author = {Kearns, Michael},
title = {Session Details: Session 3: 14:00--14:10},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255055},
doi = {10.1145/3255055},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
numpages = {1},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554808,
author = {Feige, Uriel and Tennenholtz, Moshe},
title = {Invitation Games and the Price of Stability},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554808},
doi = {10.1145/2554797.2554808},
abstract = {Given an arbitrary 2-player game G that we refer to as the basic game, we propose a notion of a multiplayer invitation game that proceeds for a fixed number of rounds, where in each round some player (whose identity is determined by a scheduler) gets to invite a player of his choice to play a match of the basic game. The question that we study is how does the price of stability of the invitation game compare to that of the basic game. For a wide range of schedulers we prove a dichotomy result, showing that there are only two types of basic games, those that we call invitation resistant in which the price of stability of the invitation version is equal to that of the basic game, and those that we call asymptotically efficient in which the price of stability tends to 0 as the number of rounds grows. 1 In particular, when the basic game is the prisoners dilemma the game is asymptotically efficient if and only if the payoff when both players defect is nonzero.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {93–102},
numpages = {10},
keywords = {prisoners dilemma, sub-game perfect equilibrium, multi-player games},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554809,
author = {Braverman, Mark and Pasricha, Kanika},
title = {The Computational Hardness of Pricing Compound Options},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554809},
doi = {10.1145/2554797.2554809},
abstract = {It is generally assumed that you can make a financial asset out of any underlying event or combination thereof, and then sell a security. We show that while this is theoretically true from the financial engineering perspective, compound securities might be intractable to price. Even given no information asymmetries, or adversarial sellers, it might be computationally intractable to put a value on these, and the associated computational complexity might afford an advantage to the party with more compute power. We prove that the problem of pricing an option on a single security with unbounded compounding is PSPACE hard, even when the behavior of the underlying security is computationally tractable. We also show that in the oracle model, even when compounding is limited to at most k layers, the complexity of pricing securities grows exponentially in k.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {103–104},
numpages = {2},
keywords = {pricing, financial securities, computational complexity, computational finance},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554810,
author = {Chakrabarty, Deeparnab and Swamy, Chaitanya},
title = {Welfare Maximization and Truthfulness in Mechanism Design with Ordinal Preferences},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554810},
doi = {10.1145/2554797.2554810},
abstract = {In this paper, we study mechanism design problems in the ordinal setting wherein the preferences of agents are described by orderings over outcomes, as opposed to specific numerical values associated with them. This setting is relevant when agents can compare outcomes, but aren't able to evaluate precise utilities for them. Such a situation arises in diverse contexts including voting and matching markets.Our paper addresses two issues that arise in ordinal mechanism design. To design social welfare maximizing mechanisms, one needs to be able to quantitatively measure the welfare of an outcome which is not clear in the ordinal setting. Second, since the impossibility results of Gibbard and Satterthwaite [14, 25] force one to move to randomized mechanisms, one needs a more nuanced notion of truthfulness.We propose rank approximation as a metric for measuring the quality of an outcome, which allows us to evaluate mechanisms based on worst-case performance, and lex-truthfulness as a notion of truthfulness for randomized ordinal mechanisms. Lex-truthfulness is stronger than notions studied in the literature, and yet flexible enough to admit a rich class of mechanisms circumventing classical impossibility results. We demonstrate the usefulness of the above notions by devising lex-truthful mechanisms achieving good rank-approximation factors, both in the general ordinal setting, as well as structured settings such as (one-sided) matching markets, and its generalizations, matroid and scheduling markets.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {105–120},
numpages = {16},
keywords = {computational social choice theory, truthfulness for randomized mechanisms, matroids, linear programming, algorithmic game theory, social welfare and rank approximation, ordinal preferences, mechanism design},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554811,
author = {Bhattacharya, Sayan and Im, Sungjin and Kulkarni, Janardhan and Munagala, Kamesh},
title = {Coordination Mechanisms from (Almost) All Scheduling Policies},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554811},
doi = {10.1145/2554797.2554811},
abstract = {We study the price of anarchy of coordination mechanisms for a scheduling problem where each job j has a weight wj, processing time pij, assignment cost hij, and communication delay (or release date) rij, on machine i. Each machine is free to declare its own scheduling policy. Each job is a selfish agent and selects a machine that minimizes its own disutility, which is equal to its weighted completion time plus its assignment cost. The goal is to minimize the total disutility incurred by all the jobs. Our model is general enough to capture scheduling jobs in a distributed environment with heterogeneous machines (or data centers) that are situated across different locations.Our main result is a characterization of scheduling policies that give a small (robust) Price of Anarchy. More precisely, we show that whenever each machine independently declares any scheduling policy that satisfies a certain bounded stretch condition introduced in this paper, the game induced between the jobs has a small Price of Anarchy. Our characterization is powerful enough to test almost all popular scheduling policies. On the technical side, to derive our results, we use a potential function whose derivative leads to an instantaneous smoothness condition, and linear programming and dual fitting. To the best of our knowledge, this is a novel application of these techniques in the context of coordination mechanisms, and we believe these tools will find more applications in analyzing PoA of games. We also extend our results to the lk-norms and l∞ norm (makespan) objectives.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {121–134},
numpages = {14},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/3255056,
author = {Xiao, David},
title = {Session Details: Session 4: 16:00--16:10},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255056},
doi = {10.1145/3255056},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
numpages = {1},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554812,
author = {Gelles, Ran and Sahai, Amit and Wadia, Akshay},
title = {Private Interactive Communication across an Adversarial Channel},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554812},
doi = {10.1145/2554797.2554812},
abstract = {Consider two parties Alice and Bob, who hold private inputs x and y, and wish to compute a function f(x, y) privately in the information theoretic sense; that is, each party should learn nothing beyond f(x, y). However, the communication channel available to them is noisy. This means that the channel can introduce errors in the transmission between the two parties. Moreover, the channel is adversarial in the sense that it knows the protocol that Alice and Bob are running, and maliciously introduces errors to disrupt the communication, subject to some bound on the total number of errors. A fundamental question in this setting is to design a protocol that remains private in the presence of large number of errors.If Alice and Bob are only interested in computing f(x, y) correctly, and not privately, then quite robust protocols are known that can tolerate a constant fraction of errors. However, none of these solutions is applicable in the setting of privacy, as they inherently leak information about the parties' inputs. This leads to the question whether we can simultaneously achieve privacy and error-resilience against a constant fraction of errors.We show that privacy and error-resilience are contradictory goals. In particular, we show that for every constant c &gt; 0, there exists a function f which is privately computable in the error-less setting, but for which no private and correct protocol is resilient against a c-fraction of errors. The same impossibility holds also for sub-constant noise rate, e.g., when c is exponentially small (as a function of the input size).},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {135–144},
numpages = {10},
keywords = {coding, private function evaluation, adversarial noise, information-theoretic security, interactive communication},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554813,
author = {Moore, Cristopher and Schulman, Leonard J.},
title = {Tree Codes and a Conjecture on Exponential Sums},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554813},
doi = {10.1145/2554797.2554813},
abstract = {We propose a new conjecture on some exponential sums. These particular sums have not apparently been considered in the literature. Subject to the conjecture we obtain the first effective construction of asymptotically good tree codes. The available numerical evidence is consistent with the conjecture and is sufficient to certify codes for significant-length communications.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {145–154},
numpages = {10},
keywords = {exponential sum, error-correcting codes, interactive communication},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554814,
author = {Cheraghchi, Mahdi and Guruswami, Venkatesan},
title = {Capacity of Non-Malleable Codes},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554814},
doi = {10.1145/2554797.2554814},
abstract = {Non-malleable codes, introduced by Dziembowski, Pietrzak and Wichs (ICS 2010), encode messages s in a manner so that tampering the codeword causes the decoder to either output s or a message that is independent of s. While this is an impossible goal to achieve against unrestricted tampering functions, rather surprisingly non-malleable coding becomes possible against every fixed family F of tampering functions that is not too large (for instance, when lF| ≤ 22αn for some α &lt; 1 where n is the number of bits in a codeword).In this work, we study the "capacity of non-malleable coding", and establish optimal bounds on the achievable rate as a function of the family size, answering an open problem from Dziembowski et al. (ICS 2010). Specifically, We prove that for every family F with lF| ≤ 22αn, there exist non-malleable codes against F with rate arbitrarily close to 1 - α (this is achieved w.h.p. by a randomized construction).We show the existence of families of size exp(;nO(1) 2αn) against which there is no non-malleable code of rate 1 - α (in fact this is the case w.h.p for a random family of this size).We also show that 1 - α is the best achievable rate for the family of functions which are only allowed to tamper the first αn bits of the codeword, which is of special interest.As a corollary, this implies that the capacity of non-malleable coding in the split-state model (where the tampering function acts independently but arbitrarily on the two halves of the codeword, a model which has received some attention recently) equals 1/2.We also give an efficient Monte Carlo construction of codes of rate close to 1 with polynomial time encoding and decoding that is non-malleable against any fixed c &gt; 0 and family F of size 2nc, in particular tampering functions with, say, cubic size circuits.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {155–168},
numpages = {14},
keywords = {cryptography, coding theory, tamper-resilient storage, probabilistic method, information theory, error detection},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554815,
author = {Druk, Erez and Ishai, Yuval},
title = {Linear-Time Encodable Codes Meeting the Gilbert-Varshamov Bound and Their Cryptographic Applications},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554815},
doi = {10.1145/2554797.2554815},
abstract = {A random linear code has good minimal distance with high probability. The conjectured intractability of decoding random linear codes has recently found many applications in cryptography. One disadvantage of random linear codes is that their encoding complexity grows quadratically with the message length. Motivated by this disadvantage, we present a randomized construction of linear error-correcting codes which can be encoded in linear time and yet enjoy several useful features of random linear codes. Our construction is based on a linear-time computable hash function due to Ishai, Kushilevitz, Ostrovsky and Sahai [25].We demonstrate the usefulness of these new codes by presenting several applications in coding theory and cryptography. These include the first family of linear-time encodable codes meeting the Gilbert-Varshamov bound, the first nontrivial linear-time secret sharing schemes, and plausible candidates for symmetric encryption and identification schemes which can be conjectured to achieve better asymptotic efficiency/security tradeoffs than all current candidates.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {169–182},
numpages = {14},
keywords = {gilbert-varshamov bound, error-correcting codes, lineartime encodable codes, cryptography},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/3255057,
author = {Daskalakis, Costis},
title = {Session Details: Session 5: 08:30--08:40},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255057},
doi = {10.1145/3255057},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
numpages = {1},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554816,
author = {Brand\~{a}o, Fernando G.S.L. and Harrow, Aram W. and Lee, James R. and Peres, Yuval},
title = {Adversarial Hypothesis Testing and a Quantum Stein's Lemma for Restricted Measurements},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554816},
doi = {10.1145/2554797.2554816},
abstract = {Recall the classical hypothesis testing setting with two convex sets of probability distributions P and Q. One receives either n i.i.d. samples from a distribution p ∈ P or from a distribution q ∈ Q and wants to decide from which set the points were sampled. It is known that the optimal exponential rate at which errors decrease can be achieved by a simple maximum-likelihood ratio test which does not depend on p or q, but only on the sets P and Q.We consider an adaptive generalization of this model where the choice of p ∈ P and q ∈ Q can change in each sample in some way that depends arbitrarily on the previous samples. In other words, in the kth round, an adversary, having observed all the previous samples in rounds 1, ..., κ-1, chooses pκ ∈ P and qκ ∈ Q, with the goal of confusing the hypothesis test. We prove that even in this case, the optimal exponential error rate can be achieved by a simple maximum-likelihood test that depends only on P and Q.We then show that the adversarial model has applications in hypothesis testing for quantum states using restricted measurements. For example, it can be used to study the problem of distinguishing entangled states from the set of all separable states using only measurements that can be implemented with local operations and classical communication (LOCC). The basic idea is that in our setup, the deleterious effects of entanglement can be simulated by an adaptive classical adversary.We prove a quantum Stein's Lemma in this setting: In many circumstances, the optimal hypothesis testing rate is equal to an appropriate notion of quantum relative entropy between two states. In particular, our arguments yield an alternate proof of Li and Winter's recent strengthening of strong subadditivity for quantum relative entropy.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {183–194},
numpages = {12},
keywords = {composite hypothesis testing, quantum information theory, entanglement testing},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554817,
author = {Azar, Yossi and Felge, Uriel and Feldman, Michal and Tennenholtz, Moshe},
title = {Sequential Decision Making with Vector Outcomes},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554817},
doi = {10.1145/2554797.2554817},
abstract = {We study a multi-round optimization setting in which in each round a player may select one of several actions, and each action produces an outcome vector, not observable to the player until the round ends. The final payoff for the player is computed by applying some known function f to the sum of all outcome vectors (e.g., the minimum of all coordinates of the sum). We show that standard notions of performance measure (such as comparison to the best single action) used in related expert and bandit settings (in which the payoff in each round is scalar) are not useful in our vector setting. Instead, we propose a different performance measure, and design algorithms that have vanishing regret with respect to our new measure.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {195–206},
numpages = {12},
keywords = {bandit, expert, vector outcome},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554818,
author = {Rabani, Yuval and Schulman, Leonard J. and Swamy, Chaitanya},
title = {Learning Mixtures of Arbitrary Distributions over Large Discrete Domains},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554818},
doi = {10.1145/2554797.2554818},
abstract = {We give an algorithm for learning a mixture of unstructured distributions. This problem arises in various unsupervised learning scenarios, for example in learning topic models from a corpus of documents spanning several topics. We show how to learn the constituents of a mixture of k arbitrary distributions over a large discrete domain [n]={1, 2, ...,n} and the mixture weights, using O(n polylog n) samples. (In the topic-model learning setting, the mixture constituents correspond to the topic distributions.)This task is information-theoretically impossible for k &gt; 1 under the usual sampling process from a mixture distribution. However, there are situations (such as the above-mentioned topic model case) in which each sample point consists of several observations from the same mixture constituent. This number of observations, which we call the "sampling aperture", is a crucial parameter of the problem.We obtain the first bounds for this mixture-learning problem without imposing any assumptions on the mixture constituents. We show that efficient learning is possible exactly at the information-theoretically least-possible aperture of 2k-1. Thus, we achieve near-optimal dependence on n and optimal aperture. While the sample-size required by our algorithm depends exponentially on k, we prove that such a dependence is unavoidable when one considers general mixtures.A sequence of tools contribute to the algorithm, such as concentration results for random matrices, dimension reduction, moment estimations, and sensitivity analysis.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {207–224},
numpages = {18},
keywords = {convex geometry, spectral techniques, linear programming, moment methods, topic models, mixture learning, randomized algorithms},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554819,
author = {Berry, Jonathan W. and Fostvedt, Luke K. and Nordman, Daniel J. and Phillips, Cynthia A. and Seshadhri, C. and Wilson, Alyson G.},
title = {Why Do Simple Algorithms for Triangle Enumeration Work in the Real World?},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554819},
doi = {10.1145/2554797.2554819},
abstract = {Triangle enumeration is a fundamental graph operation. Despite the lack of provably efficient (linear, or slightly super-linear) worst-case algorithms for this problem, practitioners run simple, efficient heuristics to find all triangles in graphs with millions of vertices. How are these heuristics exploiting the structure of these special graphs to provide major speedups in running time?We study one of the most prevalent algorithms used by practitioners. A trivial algorithm enumerates all paths of length 2, and checks if each such path is incident to a triangle. A good heuristic is to enumerate only those paths of length 2 where the middle vertex has the lowest degree. It is easily implemented and is empirically known to give remarkable speedups over the trivial algorithm.We study the behavior of this algorithm over graphs with heavy-tailed degree distributions, a defining feature of real-world graphs. The erased configuration model (ECM) efficiently generates a graph with asymptotically (almost) any desired degree sequence. We show that the expected running time of this algorithm over the distribution of graphs created by the ECM is controlled by the l4/3-norm of the degree sequence. As a corollary of our main theorem, we prove expected linear-time performance for degree sequences following a power law with exponent α ≥ 7/3, and non-trivial speedup whenever α ∈ (2,3).},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {225–234},
numpages = {10},
keywords = {heavy-tailed degree distribution, triangle enumeration, expected analysis, graph algorithms},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/3255058,
author = {Vaikuntanathan, Vinod},
title = {Session Details: Session 6: 10:30--10:40},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255058},
doi = {10.1145/3255058},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
numpages = {1},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554820,
author = {Brakerski, Zvika and Rothblum, Guy N.},
title = {Black-Box Obfuscation for d-CNFs},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554820},
doi = {10.1145/2554797.2554820},
abstract = {We show how to securely obfuscate a new class of functions: conjunctions of NC0d circuits. These are functions of the form C(→/x) = ∧mi=1 C1(→/x), where each C1 is a boolean NC0d circuits circuit, whose output bit is only a function of d = O(1) bits of the input →/x. For example, d-CNFs, where each clause is a disjunction of at most d variables, are in this class. Given such a function, we produce an obfuscated program that preserves the input-output functionality of the given function, but reveals nothing else. Our construction is based on multilinear maps, and can be instantiated using the recent candidates proposed by Garg, Gentry and Halevi (EUROCRYPT 2013) and by Coron, Lepoint and Tibouchi (CRYPTO 2013).We prove that the construction is a secure obfuscation in a generic multilinear group model, under the black-box definition of Barak et al. (CRYPTO 2001). Security is based on a new worst-case hardness assumption about exponential hardness of the NP-complete problem 3-SAT, the Bounded Speedup Hypothesis.One of the new techniques we introduce is a method for enforcing input consistency, which we call randomizing sub-assignments. We hope that this technique can find further application in constructing secure obfuscators.The family of functions we obfuscate is considerably richer than previous works that consider black-box obfuscation. As one application, we show how to achieve obfuscated functional point testing: namely, to construct a circuit that checks whether undefined(→/x) = →/y, where undefined is an arbitrary "public" polynomial-time computable function, but →/y is a "secret" point that is hidden in the obfuscation.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {235–250},
numpages = {16},
keywords = {program obfuscation, cryptography},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554821,
author = {Akavia, Adi and Bogdanov, Andrej and Guo, Siyao and Kamath, Akshay and Rosen, Alon},
title = {Candidate Weak Pseudorandom Functions in AC<sup>0</sup> ○ MOD<sub>2</sub>},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554821},
doi = {10.1145/2554797.2554821},
abstract = {Pseudorandom functions (PRFs) play a fundamental role in symmetric-key cryptography. However, they are inherently complex and cannot be implemented in the class AC0 (MOD2). Weak pseudorandom functions (weak PRFs) do not suffer from this complexity limitation, yet they suffice for many cryptographic applications.We study the minimal complexity requirements for constructing weak PRFs. To this end We conjecture that the function family FA(x) = g(Ax), where A is a random square GF(2) matrix and g is a carefully chosen function of constant depth, is a weak PRF. In support of our conjecture, we show that functions in this family are inapproximable by GF(2) polynomials of low degree and do not correlate with any fixed Boolean function family of subexponential size.We study the class AC0 ○ MOD2 that captures the complexity of our construction. We conjecture that all functions in this class have a Fourier coefficient of magnitude exp(- poly log n) and prove this conjecture in the case when the MOD2 function is typical.We investigate the relation between the hardness of learning noisy parities and the existence of weak PRFs in AC0 ○ MOD2.We argue that such a complexity-driven approach can play a role in bridging the gap between the theory and practice of cryptography.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {251–260},
numpages = {10},
keywords = {weak pseudorandom functions, inapproximability of ac0, parallel cryptography, ac0 ○ mod2, learning parity with noise},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554822,
author = {Miles, Eric},
title = {Iterated Group Products and Leakage Resilience against NC1},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554822},
doi = {10.1145/2554797.2554822},
abstract = {We show that if NC1 ≠ L, then for every element α of the alternating group At, circuits of depth O(log t) cannot distinguish between a uniform vector over (At)t with product = α and one with product = identity. Combined with a recent construction by the author and Viola in the setting of leakage-resilient cryptography [STOC '13], this gives a compiler that produces circuits withstanding leakage from NC1 (assuming NC1 ≠ L). For context, leakage from NC1 breaks nearly all previous constructions, and security against leakage from P is impossible.We build on work by Cook and McKenzie [J. Algorithms '87] establishing the relationship between L = logarithmic space and the symmetric group St. Our techniques include a novel algorithmic use of commutators to manipulate the cycle structure of permutations in At.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {261–268},
numpages = {8},
keywords = {iterated group products, leakage-resilient cryptography},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554823,
author = {Liu, Yi-Kai},
title = {Building One-Time Memories from Isolated Qubits: (Extended Abstract)},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554823},
doi = {10.1145/2554797.2554823},
abstract = {One-time memories (OTM's) are simple tamper-resistant cryptographic devices, which can be used to implement one-time programs, a very general form of software protection and program obfuscation. Here we investigate the possibility of building OTM's using quantum mechanical devices. It is known that OTM's cannot exist in a fully-quantum world or in a fully-classical world. Instead, we propose a new model based on isolated qubits - qubits that can only be accessed using local operations and classical communication (LOCC). This model combines a quantum resource (single-qubit measurements) with a classical restriction (on communication between qubits), and can be implemented using current technologies, such as nitrogen vacancy centers in diamond. In this model, we construct OTM's that are information-theoretically secure against one-pass LOCC adversaries that use 2-outcome measurements.Our construction resembles Wiesner's old idea of quantum conjugate coding, implemented using random error-correcting codes; our proof of security uses entropy chaining to bound the supremum of a suitable empirical process. In addition, we conjecture that our random codes can be replaced by some class of efficiently-decodable codes, to get computationally-efficient OTM's that are secure against computationally-bounded LOCC adversaries.In addition, we construct data-hiding states, which allow an LOCC sender to encode an (n-O(1))-bit messsage into n qubits, such that at most half of the message can be extracted by a one-pass LOCC receiver, but the whole message can be extracted by a general quantum receiver.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {269–286},
numpages = {18},
keywords = {quantum computation, data-hiding states, cryptography, one-time programs, local operations and classical communication, conjugate coding, oblivious transfer},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/3255059,
author = {Irani, Sandy},
title = {Session Details: Session 7: 14:00--14:10},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255059},
doi = {10.1145/3255059},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
numpages = {1},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554824,
author = {Angelino, Elaine and Kanade, Varun},
title = {Attribute-Efficient Evolvability of Linear Functions},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554824},
doi = {10.1145/2554797.2554824},
abstract = {In a seminal paper, Valiant (2006) introduced a computational model for evolution to address the question of complexity that can arise through Darwinian mechanisms. Valiant views evolution as a restricted form of computational learning, where the goal is to evolve a hypothesis that is close to the ideal function. Feldman (2008) showed that (correlational) statistical query learning algorithms could be framed as evolutionary mechanisms in Valiant's model. P. Valiant (2012) considered evolvability of real-valued functions and also showed that weak-optimization algorithms that use weak-evaluation oracles could be converted to evolutionary mechanisms.In this work, we focus on the complexity of representations of evolutionary mechanisms. In general, the reductions of Feldman and P. Valiant may result in intermediate representations that are arbitrarily complex polynomial-sized circuits). We argue that biological constraints often dictate that the representations have low complexity, such as constant depth and fan-in circuits. We give mechanisms for evolving sparse linear functions under a large class of smooth distributions. These evolutionary algorithms are attribute-efficient in the sense that the size of the representations and the number of generations required depend only on the sparsity of the target function and the accuracy parameter, but have no dependence on the total number of attributes.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {287–300},
numpages = {14},
keywords = {computational learning theory, regression, evolvability},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554825,
author = {Landau, Zeph and Vazirani, Umesh and Vidick, Thomas},
title = {An Efficient Algorithm for Finding the Ground State of 1D Gapped Local Hamiltonians},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554825},
doi = {10.1145/2554797.2554825},
abstract = {Computing ground states of local Hamiltonians is a fundamental problem in condensed matter physics. The problem is known to be QMA-complete, even for one-dimensional Hamiltonians [1]. This means that we do not even expect that there is a sub-exponential size description of the ground state that allows efficient computation of local observables such as the energy. In sharp contrast, the heuristic density matrix renormalization group (DMRG) algorithm invented two decades ago [5] has been remarkably successful in practice on one-dimensional problems. The situation is reminiscent of the unexplained success of the simplex algorithm before the advent of ellipsoid and interior-point methods. Is there a principled explanation for this, in the form of a large class of one-dimensional Hamiltonians whose ground states can be provably efficiently approximated? Here we give such an algorithm for gapped one-dimensional Hamiltonians: our algorithm outputs an (inverse-polynomial) approximation to the ground state, expressed as a matrix product state (MPS) of polynomial bond dimension. The running time of the algorithm is polynomial in the number of qudits n and the approximation quality δ, for a fixed local dimension d and gap Δ &gt; 0.A key ingredient of our algorithm is a new construction of an operator called an approximate ground state projector (AGSP), a concept first introduced in [2] to derive an improved area law for gapped one-dimensional systems [3]. For this purpose the AGSP has to be efficiently constructed; the particular AGSP we construct relies on matrix-valued Chernoff bounds [4]. Other ingredients of the algorithm include the use of convex programming, recently discovered structural features of gapped 1D quantum systems [2], and new techniques for manipulating and bounding the complexity of matrix product states.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {301–302},
numpages = {2},
keywords = {ground state, matrix product state, 1d algorithm, gapped hamiltonian, local hamiltonian},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554826,
author = {Antoniadis, Antonios and Barcelo, Neal and Nugent, Michael and Pruhs, Kirk and Scquizzato, Michele},
title = {Energy-Efficient Circuit Design},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554826},
doi = {10.1145/2554797.2554826},
abstract = {We initiate the theoretical investigation of energy-efficient circuit design. We assume that the circuit design specifies the circuit layout as well as the supply voltages for the gates. To obtain maximum energy efficiency, the circuit design must balance the conflicting demands of minimizing the energy used per gate, and minimizing the number of gates in the circuit; If the energy supplied to the gates is small, then functional failures are likely, necessitating a circuit layout that is more fault-tolerant, and thus that has more gates. By leveraging previous work on fault-tolerant circuit design, we show general upper and lower bounds on the amount of energy required by a circuit to compute a given relation. We show that some circuits would be asymptotically more energy efficient if heterogeneous supply voltages were allowed, and show that for some circuits the most energy-efficient supply voltages are homogeneous over all gates.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {303–312},
numpages = {10},
keywords = {energy efficiency, circuit design, near-threshold computing},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554827,
author = {Chen, Ho-Lin and Doty, David and Soloveichik, David},
title = {Rate-Independent Computation in Continuous Chemical Reaction Networks},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554827},
doi = {10.1145/2554797.2554827},
abstract = {Understanding the algorithmic behaviors that are in principle realizable in a chemical system is necessary for a rigorous understanding of the design principles of biological regulatory networks. Further, advances in synthetic biology herald the time when we'll be able to rationally engineer complex chemical systems, and when idealized formal models will become blueprints for engineering.Coupled chemical interactions in a well-mixed solution are commonly formalized as chemical reaction networks (CRNs). However, despite the widespread use of CRNs in the natural sciences, the range of computational behaviors exhibited by CRNs is not well understood. Here we study the following problem: what functions f : ∪k → ∪ can be computed by a chemical reaction network, in which the CRN eventually produces the correct amount of the "output" ∣ molecule, no matter the rate at which reactions proceed? This captures a previously unexplored, but very natural class of computations: for example, the reaction X1 + X2 → Y can be thought to compute the function y = min(x1, x2). Such a CRN is robust in the sense that it is correct whether its evolution is governed by the standard model of mass-action kinetics, alternatives such as Hill-function or Michaelis-Menten kinetics, or other arbitrary models of chemistry that respect the (fundamentally digital) stoichiometric constraints (what are the reactants and products?). We develop a formal definition of such computation using a novel notion of reachability, and prove that a function is computable in this manner if and only if it is continuous piecewise linear.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {313–326},
numpages = {14},
keywords = {chemical reaction networks, analog computation, piecewise-linear, mass-action},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/3255060,
author = {Filmus, Yuval},
title = {Session Details: Session 8: 16:00--16:10},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255060},
doi = {10.1145/3255060},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
numpages = {1},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554828,
author = {Bshouty, Nader},
title = {Testers and Their Applications},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554828},
doi = {10.1145/2554797.2554828},
abstract = {We develop a new notion called tester of a class M of functions f : A → C that maps the elements α ∈ A in the domain A of the function to a finite number (the size of the tester) of elements b1,...,bt in a smaller sub-domain B ⊂ A where the property f(α) ≠ 0 is preserved for all f ∈ M. I.e., for all f ∈ M and - ∈ A if f(α) ≠ 0 then f(bi) ≠ 0 for some i.We use tools from elementary algebra and algebraic function fields to construct testers of almost optimal size in deterministic polynomial time in the size of the tester. We then apply testers to deterministically construct new set of objects with some combinatorial and algebraic properties that can be used to derandomize some algorithms.We show that those new constructions are almost optimal and for many of them meet the union bound of the problem. Constructions include, d-restriction problems, perfect hash, universal sets, cover-free families, separating hash functions, polynomial restriction problems, black box polynomial identity testing for polynomials and circuits over small fields and hitting sets.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {327–352},
numpages = {26},
keywords = {polynomial identity testing (pit), separating hash functions, hitting sets, perfect hash, universal sets, d-restriction problems, polynomial restriction problems, derandomization, combinatorial objects, cover-free families},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554829,
author = {Mendel, Manor and Naor, Assaf},
title = {Expanders with Respect to Hadamard Spaces and Random Graphs: Extended Abstract},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554829},
doi = {10.1145/2554797.2554829},
abstract = {It is shown that there exists a sequence of 3-regular graphs {Gn}∞n=1 and a Hadamard space X such that {Gn}∞n=1 forms an expander sequence with respect to {X{, yet random regular graphs are not expanders with respect to {X{. This answers a question of [31]. {Gn}∞n=1 are also shown to be expanders with respect to random regular graphs, yielding a deterministic sublinear time constant factor approximation algorithm for computing the average squared distance in subsets of a random graph. The proof uses the Euclidean cone over a random graph, an auxiliary continuous geometric object that allows for the implementation of martingale methods.This extended abstract does not contain proofs. The full version of this paper can be found at arXiv:1306.5434.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {353–358},
numpages = {6},
keywords = {hadamard spaces, expanders, random regular graphs, metric embeddings, euclidean cone, cat(0)},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554830,
author = {Babai, L\'{a}szl\'{o}},
title = {On the Automorphism Groups of Strongly Regular Graphs I},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554830},
doi = {10.1145/2554797.2554830},
abstract = {We derive structural constraints on the automorphism groups of strongly regular (s.r.) graphs, giving a surprisingly strong answer to a decades-old problem, with tantalizing implications to testing isomorphism of s.r. graphs, and raising new combinatorial challenges.S.r. graphs, while not believed to be Graph Isomorphism (GI) complete, have long been recognized as hard cases for GI, and, in this author's view, present some of the core difficulties of the general GI problem. Progress on the complexity of testing their isomorphism has been intermittent (Babai 1980, Spielman 1996, BW &amp; CST (STOC'13) and BCSTW (FOCS'13)), and the current best bound is exp(\~{O}(n1/5)) (n is the number of vertices).Our main result is that if X is a s.r. graph then, with straightforward exceptions, the degree of the largest alternating group involved in the automorphism group Aut(X) (as a quotient of a subgroup) is O((ln n)2ln ln n). (The exceptions admit trivial linear-time GI testing.)The design of isomorphism tests for various classes of structures is intimately connected with the study of the automorphism groups of those structures. We include a brief survey of these connections, starting with an 1869 paper by Jordan on trees.In particular, our result amplifies the potential of Luks's divide-and-conquer methods (1980) to be applicable to testing isomorphism of s.r. graphs in quasipolynomial time.The challenge remains to find a hierarchy of combinatorial substructures through which this potential can be realized. We expect that the generality of our result will help in this regard; the result applies not only to s.r. graphs but to all graphs with strong spectral expansion and with a relatively small number of common neighbors for every pair of vertices. We state a purely mathematical conjecture that could bring us closer to finding the right kind of hierarchy. We also outline the broader GI context, and state conjectures in terms of "primitive coherent configurations." These are generalizations of s.r. graphs, relevant to the general GI problem.Another consequence of the main result is the strongest argument to date against GI-completeness of s.r. graphs: we prove that no polynomial-time categorical reduction of GI to isomorphism of s.r. graphs is possible. All known reductions between isomorphism problems of various classes of structures fit into our notion of "categorical reduction."The proof of the main result is elementary; it is based on known results in spectral graph theory and on a 1987 lemma on permutations by \'{A}kos Seress and the author.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {359–368},
numpages = {10},
keywords = {groups, algorithms, automorphism groups, graphs, isomorphism testing, strongly regular graphs},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554831,
author = {Gamarnik, David and Sudan, Madhu},
title = {Limits of Local Algorithms over Sparse Random Graphs},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554831},
doi = {10.1145/2554797.2554831},
abstract = {Local algorithms on graphs are algorithms that run in parallel on the nodes of a graph to compute some global structural feature of the graph. Such algorithms use only local information available at nodes to determine local aspects of the global structure, while also potentially using some randomness. Research over the years has shown that such algorithms can be surprisingly powerful in terms of computing structures like large independent sets in graphs locally. These algorithms have also been implicitly considered in the work on graph limits, where a conjecture due to Hatami, Lov\'{a}sz and Szegedy [17] implied that local algorithms may be able to compute near-maximum independent sets in (sparse) random d-regular graphs. In this paper we refute this conjecture and show that every independent set produced by local algorithms is smaller that the largest one by a multiplicative factor of at least 1/2+1/(2√2) ≈ .853, asymptotically as d → ∞.Our result is based on an important clustering phenomena predicted first in the literature on spin glasses, and recently proved rigorously for a variety of constraint satisfaction problems on random graphs. Such properties suggest that the geometry of the solution space can be quite intricate. The specific clustering property, that we prove and apply in this paper shows that typically every two large independent sets in a random graph either have a significant intersection, or have a nearly empty intersection. As a result, large independent sets are clustered according to the proximity to each other. While the clustering property was postulated earlier as an obstruction for the success of local algorithms, such as for example, the Belief Propagation algorithm, our result is the first one where the clustering property is used to formally prove limits on local algorithms.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {369–376},
numpages = {8},
keywords = {local algorithms, lower bounds, random graphs},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/3255061,
author = {Wichs, Daniel},
title = {Session Details: Session 9: 08:30--08:40},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255061},
doi = {10.1145/3255061},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
numpages = {1},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554832,
author = {Haramaty, Elad and Sudan, Madhu},
title = {Deterministic Compression with Uncertain Priors},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554832},
doi = {10.1145/2554797.2554832},
abstract = {Communication in "natural" settings, e.g., between humans, is distinctly different than that in classical designed settings, in that the former is characterized by the sender and receiver not being in perfect agreement with each other. Solutions to classical communication problems thus have to overcome an extra layer of uncertainty introduced by this lack of prior agreement. One of the classical goals of communication is compression of information, and in this context lack of agreement implies that sender and receiver may not agree on the "prior" from which information is being generated. Most classical mechanisms for compressing turn out to be non-robust when sender and receiver do not agree on the prior. Juba et al. (Proc. ITCS 2011) showed that there do exists compression schemes with shared randomness between sender and reciever that can compress information down roughly to its entropy.In this work we explore the assumption of shared randomness between the sender and receiver and highlight why this assumption is problematic when dealing with natural communication. We initiate the study of deterministic compression schemes amid uncertain priors, and expose some the mathematical facets of this problem. We show some non-trivial determinstic compression schemes, and some lower bounds on natural classes of compression schemes. We show that a full understanding of deterministic communication turns into challenging (open) questions in graph theory and communication complexity.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {377–386},
numpages = {10},
keywords = {source coding, graph coloring, communication complexity},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554833,
author = {Chandrasekaran, Karthekeyan and Thaler, Justin and Ullman, Jonathan and Wan, Andrew},
title = {Faster Private Release of Marginals on Small Databases},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554833},
doi = {10.1145/2554797.2554833},
abstract = {We study the problem of answering k-way marginal queries on a database D ϵ ({0,1}d)n, while preserving differential privacy. The answer to a k-way marginal query is the fraction of the database's records x in {0,1}d with a given value in each of a given set of up to k columns. Marginal queries enable a rich class of statistical analyses on a dataset, and designing efficient algorithms for privately answering marginal queries has been identified as an important open problem in private data analysis. For any k, we give a differentially private online algorithm that runs in time poly (n, 2o(d)) per query and answers any sequence of poly(n) many k-way marginal queries with error at most ±0.01 on every query, provided n ≥ d0.51. To the best of our knowledge, this is the first algorithm capable of privately answering marginal queries with a non-trivial worst-case accuracy guarantee for databases containing poly(d, k) records in time exp(o(d)). Our algorithm runs the private multiplicative weights algorithm (Hardt and Rothblum, FOCS '10) on a new approximate polynomial representation of the database.We derive our representation for the database by approximating the OR function restricted to low Hamming weight inputs using low-degree polynomials with coefficients of bounded L1-norm. In doing so, we show new upper and lower bounds on the degree of such polynomials, which may be of independent approximation-theoretic interest.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {387–402},
numpages = {16},
keywords = {approximation theory, differential privacy, marginal queries},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554834,
author = {Kearns, Michael and Pai, Mallesh and Roth, Aaron and Ullman, Jonathan},
title = {Mechanism Design in Large Games: Incentives and Privacy},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554834},
doi = {10.1145/2554797.2554834},
abstract = {We study the problem of implementing equilibria of complete information games in settings of incomplete information, and address this problem using "recommender mechanisms." A recommender mechanism is one that does not have the power to enforce outcomes or to force participation, rather it only has the power to suggestion outcomes on the basis of voluntary participation. We show that despite these restrictions, recommender mechanisms can implement equilibria of complete information games in settings of incomplete information under the condition that the game is large---i.e. that there are a large number of players, and any player's action affects any other's payoff by at most a small amount.Our result follows from a novel application of differential privacy. We show that any algorithm that computes a correlated equilibrium of a complete information game while satisfying a variant of differential privacy---which we call joint differential privacy---can be used as a recommender mechanism while satisfying our desired incentive properties. Our main technical result is an algorithm for computing a correlated equilibrium of a large game while satisfying joint differential privacy.Although our recommender mechanisms are designed to satisfy game-theoretic properties, our solution ends up satisfying a strong privacy property as well. No group of players can learn "much" about the type of any player outside the group from the recommendations of the mechanism, even if these players collude in an arbitrary way. As such, our algorithm is able to implement equilibria of complete information games, without revealing information about the realized types.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {403–410},
numpages = {8},
keywords = {differential privacy, game theory, equilibrium selection, mechanism design},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554835,
author = {Nissim, Kobbi and Vadhan, Salil and Xiao, David},
title = {Redrawing the Boundaries on Purchasing Data from Privacy-Sensitive Individuals},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554835},
doi = {10.1145/2554797.2554835},
abstract = {We prove new positive and negative results concerning the existence of truthful and individually rational mechanisms for purchasing private data from individuals with unbounded and sensitive privacy preferences. We strengthen the impossibility results of Ghosh and Roth (EC 2011) by extending it to a much wider class of privacy valuations. In particular, these include privacy valuations that are based on (ε δ)-differentially private mechanisms for non-zero δ, ones where the privacy costs are measured in a per-database manner (rather than taking the worst case), and ones that do not depend on the payments made to players (which might not be observable to an adversary).To bypass this impossibility result, we study a natural special setting where individuals have monotonic privacy valuations, which captures common contexts where certain values for private data are expected to lead to higher valuations for privacy (e. g. having a particular disease). We give new mechanisms that are individually rational for all players with monotonic privacy valuations, truthful for all players whose privacy valuations are not too large, and accurate if there are not too many players with too-large privacy valuations. We also prove matching lower bounds showing that in some respects our mechanism cannot be improved significantly.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {411–422},
numpages = {12},
keywords = {differential privacy, mechanism design},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/3255062,
author = {Chakrabarty, Deeparnab},
title = {Session Details: Session 10: 10:30--10:40},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255062},
doi = {10.1145/3255062},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
numpages = {1},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554836,
author = {Yoshida, Yuichi and Zhou, Yuan},
title = {Approximation Schemes via Sherali-Adams Hierarchy for Dense Constraint Satisfaction Problems and Assignment Problems},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554836},
doi = {10.1145/2554797.2554836},
abstract = {We consider approximation schemes for the maximum constraint satisfaction problems and the maximum assignment problems. Though they are NP-Hard in general, if the instance is "dense" or "locally dense", then they are known to have approximation schemes that run in polynomial time or quasi-polynomial time. In this paper, we give a unified method of showing these approximation schemes based on the Sherali-Adams linear programming relaxation hierarchy. We also use our linear programming-based framework to show new algorithmic results on the optimization version of the hypergraph isomorphism problem.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {423–438},
numpages = {16},
keywords = {sherali-adams hierarchy, locally-dense instances, dense instances, constraint satisfaction problems, assignment problems, approximation schemes},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554837,
author = {Guruswami, Venkatesan and Lee, Euiwoong},
title = {Complexity of Approximating CSP with Balance / Hard Constraints},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554837},
doi = {10.1145/2554797.2554837},
abstract = {We study two natural extensions of Constraint Satisfaction Problems (CSPs). Balance-Max-CSP requires that in any feasible assignment each element in the domain is used an equal number of times. An instance of Hard-Max-CSP consists of soft constraints and hard constraints, and the goal is to maximize the weight of satisfied soft constraints while satisfying all the hard constraints. These two extensions contain many fundamental problems not captured by CSPs, and challenge traditional theories about CSPs in a more general framework.Max-2-SAT and Max-Horn-SAT are the only two nontrivial classes of Boolean CSPs that admit a robust satisfibiality algorithm, i.e., an algorithm that finds an assignment satisfying at least (1 - g(ε)) fraction of constraints given a (1-ε)-satisfiable instance, where g(ε) → 0 as ε → 0, and g(0) = 0. We prove the inapproximability of these problems with balance or hard constraints, showing that each variant changes the nature of the problems significantly (in different ways). For instance, deciding whether an instance of 2-SAT admits a balanced assignment is NP-hard, and for Max-2-SAT with hard constraints, it is hard to find a constant-factor approximation even on (1-ε)-satisfiable instances (in particular, the version with hard constraints does not admit a robust satisfiability algorithm).},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {439–448},
numpages = {10},
keywords = {inapproximability, constraint satisfaction problem, dictatorship testing, complexity dichotomy, unique games conjecture, robust satisfiability},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554838,
author = {Chandrasekaran, Karthekeyan and Vempala, Santosh S.},
title = {Integer Feasibility of Random Polytopes: Random Integer Programs},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554838},
doi = {10.1145/2554797.2554838},
abstract = {We study the Chance-Constrained Integer Feasibility Problem, where the goal is to determine whether the random polytope P(A,b)={x ϵ Rn : Aix ≤ bi, i ϵ [m]} obtained by choosing the constraint matrix A and vector b from a known distribution is integer feasible with probability at least 1-ε. We consider the case when the entries of the constraint matrix A are i.i.d. Gaussian (equivalently are i.i.d. from any spherically symmetric distribution). The radius of the largest inscribed ball is closely related to the existence of integer points in the polytope. We find that for m up to 2O(√n) constraints (rows of A), there exist constants c0 &lt; c1 such that with high probability (ɛ = 1 /poly(n)), random polytopes are integer feasible if the radius of the largest ball contained in the polytope is at least c1√log(m/n)); and integer infeasible if the largest ball contained in the polytope is centered at (1/2,...,1/2) and has radius at most c0√log(m/n)). Thus, random polytopes transition from having no integer points to being integer feasible within a constant factor increase in the radius of the largest inscribed ball. Integer feasibility is based on a randomized polynomial-time algorithm for finding an integer point in the polytope.Our main tool is a simple new connection between integer feasibility and linear discrepancy. We extend a recent algorithm for finding low-discrepancy solutions to give a constructive upper bound on the linear discrepancy of random Gaussian matrices. By our connection between discrepancy and integer feasibility, this upper bound on linear discrepancy translates to the radius bound that guarantees integer feasibility of random polytopes.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {449–458},
numpages = {10},
keywords = {chance-constrained programming, integer programming, discrepancy, random matrices, probabilistic instances},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554839,
author = {Bandeira, Afonso S. and Charikar, Moses and Singer, Amit and Zhu, Andy},
title = {Multireference Alignment Using Semidefinite Programming},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554839},
doi = {10.1145/2554797.2554839},
abstract = {The multireference alignment problem consists of estimating a signal from multiple noisy shifted observations. Inspired by existing Unique-Games approximation algorithms, we provide a semidefinite program (SDP) based relaxation which approximates the maximum likelihood estimator (MLE) for the multireference alignment problem. Although we show this MLE problem is Unique-Games hard to approximate within any constant, we observe that our poly-time approximation algorithm for this problem appears to perform quite well in typical instances, outperforming existing methods. In an attempt to explain this behavior we provide stability guarantees for our SDP under a random noise model on the observations. This case is more challenging to analyze than traditional semi-random instances of Unique-Games: the noise model is on vertices of a graph and translates into dependent noise on the edges.Interestingly, we show that if certain positivity constraints in the relaxation are dropped, its solution becomes equivalent to performing phase correlation, a popular method used for pairwise alignment in imaging applications. Finally, we describe how symmetry reduction techniques from matrix representation theory can greatly decrease the computational cost of the SDP considered.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {459–470},
numpages = {12},
keywords = {unique-games, phase correlation, semidefinite relaxation, multireference alignment},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/3255063,
author = {Saraf, Shubhangi},
title = {Session Details: Session 11: 14:00--14:10},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255063},
doi = {10.1145/3255063},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
numpages = {1},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554840,
author = {Gupta, Rishi and Roughgarden, Tim and Seshadhri, C.},
title = {Decompositions of Triangle-Dense Graphs},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554840},
doi = {10.1145/2554797.2554840},
abstract = {High triangle density -- the graph property stating that a constant fraction of two-hop paths belong to a triangle -- is a common signature of social networks. This paper studies triangle-dense graphs from a structural perspective. We prove constructively that significant portions of a triangle-dense graph are contained in a disjoint union of dense, radius 2 subgraphs. This result quantifies the extent to which triangle-dense graphs resemble unions of cliques. We also show that our algorithm recovers planted clusterings in approximation-stable k-median instances.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {471–482},
numpages = {12},
keywords = {clustering, graph algorithms, social and information networks},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554841,
author = {Fischer, Eldar and Goldhirsh, Yonatan and Lachish, Oded},
title = {Partial Tests, Universal Tests and Decomposability},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554841},
doi = {10.1145/2554797.2554841},
abstract = {For a property P and a sub-property P', we say that P is P'-partially testable with q queries} if there exists an algorithm that distinguishes, with high probability, inputs in P' from inputs ε-far from P, using q queries. Some natural properties require many queries to test, but can be partitioned into a small number of subsets for which they are partially testable with very few queries, sometimes even a number independent of the input size.For properties over {0,1}, the notion of being thus partitionable ties in closely with Merlin-Arthur proofs of Proximity (MAPs) as defined independently in [14] a partition into r partially-testable properties is the same as a Merlin-Arthur system where the proof consists of the identity of one of the r partially-testable properties, giving a 2-way translation to an O(log r) size proof.Our main result is that for some low complexity properties a partition as above cannot exist, and moreover that for each of our properties there does not exist even a single sub-property featuring both a large size and a query-efficient partial test, in particular improving the lower bound set in [14]. For this we use neither the traditional Yao-type arguments nor the more recent communication complexity method, but open up a new approach for proving lower bounds.First, we use entropy analysis, which allows us to apply our arguments directly to 2-sided tests, thus avoiding the cost of the conversion in [14] from 2-sided to 1-sided tests. Broadly speaking we use "distinguishing instances" of a supposed test to show that a uniformly random choice of a member of the sub-property has "low entropy areas", ultimately leading to it having a low total entropy and hence having a small base set.Additionally, to have our arguments apply to adaptive tests, we use a mechanism of "rearranging" the input bits (through a decision tree that adaptively reads the entire input) to expose the low entropy that would otherwise not be apparent.We also explore the possibility of a connection in the other direction, namely whether the existence of a good partition (or MAP) can lead to a relatively query-efficient standard property test. We provide some preliminary results concerning this question, including a simple lower bound on the possible trade-off.Our second major result is a positive trade-off result for the restricted framework of 1-sided proximity oblivious tests. This is achieved through the construction of a "universal tester" that works the same for all properties admitting the restricted test. Our tester is very related to the notion of sample-based testing (for a non-constant number of queries) as defined by Goldreich and Ron in [13]. In particular it partially resolves an open problem raised by [13].},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {483–500},
numpages = {18},
keywords = {universal testing, partial testing, information theoretic lower bounds, property testing, sunflower theorems},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554842,
author = {Kaufman, Tali and Lubotzky, Alexander},
title = {High Dimensional Expanders and Property Testing},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554842},
doi = {10.1145/2554797.2554842},
abstract = {We show that the high dimensional expansion property as defined by Gromov, Linial and Meshulam, for simplicial complexes is a form of testability. Namely, a simplicial complex is a high dimensional expander iff a suitable property is testable. Using this connection, we derive several testability results.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {501–506},
numpages = {6},
keywords = {property testing, high dimensional expanders},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554843,
author = {Iwama, Kazuo and Yoshida, Yuichi},
title = {Parameterized Testability},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554843},
doi = {10.1145/2554797.2554843},
abstract = {This paper studies property testing for NP optimization problems with parameter k under the general graph model with an augmentation of random edge sampling capability. It is shown that a variety of such problems, including k-Vertex Cover, k-Feedback Vertex Set, k-Multicut, k-path-freeness and k-Dominating Set, are constant-time testable if k is constant. It should be noted that the first four problems are fixed parameter tractable (FPT) and it turns out that algorithmic techniques for their FPT algorithms (branch-and-bound search, color coding, etc.) are also useful for our testers. k-Dominating Set is $W[2]$-hard, but we can still test the property in constant time since the definition of ε-farness makes the problem trivial for non-sparse graphs that are the source of hardness for the original optimization problem. We also consider k-Odd Cycle Transversal, which is another well-known FPT problem, but we only give a sublinear-time tester when k is a constant.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {507–516},
numpages = {10},
keywords = {property testing, fixed parameter tractability},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/3255064,
author = {Etessami, Kousha},
title = {Session Details: Session 12: 16:00--16:10},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255064},
doi = {10.1145/3255064},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
numpages = {1},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554844,
author = {Kol, Gillat and Moran, Shay and Shpilka, Amir and Yehudayoff, Amir},
title = {Direct Sum Fails for Zero Error Average Communication},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554844},
doi = {10.1145/2554797.2554844},
abstract = {We show that in the model of zero error communication complexity, direct sum fails for average communication complexity as well as for external information cost. Our example also refutes a version of a conjecture by Braverman et al. that in the zero error case amortized communication complexity equals external information cost.In our examples the underlying distributions do not have full support. One interpretation of a distributions of non full support is as a promise given to the players (the players have a guarantee on their inputs). This brings up the issue of promise versus non-promise problems in this context.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {517–522},
numpages = {6},
keywords = {communication complexity, direct sum, amortized complexity, information complexity, average case complexity, promise problems, information theory},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554845,
author = {Guo, Siyao and Hub\'{a}\v{c}ek, Pavel and Rosen, Alon and Vald, Margarita},
title = {Rational Arguments: Single Round Delegation with Sublinear Verification},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554845},
doi = {10.1145/2554797.2554845},
abstract = {Rational proofs, recently introduced by Azar and Micali (STOC 2012), are a variant of interactive proofs in which the prover is neither honest nor malicious, but rather rational. The advantage of rational proofs over their classical counterparts is that they allow for extremely low communication and verification time. Azar and Micali demonstrated their potential by giving a one message rational proof for #SAT, in which the verifier runs in time O(n), where $n$ denotes the instance size. In a follow-up work (EC 2013), Azar and Micali proposed "super-efficient" and interactive versions of rational proofs and argued that they capture precisely the class TC0 of constant-depth, polynomial-size circuits with threshold gates.In this paper, we show that by considering rational arguments, in which the prover is additionally restricted to be computationally bounded, the class NC1, of search problems computable by log-space uniform circuits of O(log n)-depth, admits rational protocols that are simultaneously one-round and polylog(n) time verifiable. This demonstrates the potential of rational arguments as a way to extend the notion of "super-efficient" rational proofs beyond the class TC0.The low interaction nature of our protocols, along with their sub-linear verification time, make them well suited for delegation of computation. While they provide a weaker (yet arguably meaningful) guarantee of soundness, they compare favorably with each of the known delegation schemes in at least one aspect. They are simple, rely on standard complexity hardness assumptions, provide a correctness guarantee for all instances, and do not require preprocessing.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {523–540},
numpages = {18},
keywords = {rational cryptography, threshold circuits, succinct arguments, delegation of computation},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554846,
author = {Braverman, Mark and Chen, Jing and Kannan, Sampath},
title = {Optimal Provision-after-Wait in Healthcare},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554846},
doi = {10.1145/2554797.2554846},
abstract = {We investigate computational and mechanism design aspects of optimal scarce resource allocation, where the primary rationing mechanism is through waiting times. Specifically we consider the problem of allocating medical treatments to a population of patients. Each patient has demand for exactly one unit of treatment, and can choose to be treated in one of k hospitals, H1, ..., Hk. Different hospitals have different costs per treatment, which are fully paid by a third party ---the "payer"--- and do not accrue to the patients. The payer has a fixed budget B and can only cover a limited number of treatments in the more expensive hospitals. Access to over-demanded hospitals is rationed through waiting times: each hospital Hi will have waiting time wi. In equilibrium, each patient will choose his most preferred hospital given his intrinsic preferences and the waiting times. The payer thus computes the waiting times and the number of treatments authorized for each hospital, so that in equilibrium the budget constraint is satisfied and the social welfare is maximized.We show that even if the patients' preferences are known to the payer, the task of optimizing social welfare in equilibrium subject to the budget constraint is NP-hard. We also show that, with constant number of hospitals, if the budget constraint can be relaxed from B to (1+ε)B for an arbitrarily small constant ε, then the original optimum under budget B can be approximated very efficiently.Next, we study the endogenous emergence of waiting time from the dynamics between hospitals and patients, and show that there is no need for the payer to explicitly enforce the optimal equilibrium waiting times. When the patients arrive uniformly along time and when they have generic types, all that the payer needs to do is to enforce the total amount of money he would like to pay to each hospital. The waiting times will simply change according to the demand, and the dynamics will always converge to the desired waiting times in finite time.We then go beyond equilibrium solutions and investigate the optimization problem over a much larger class of mechanisms containing the equilibrium ones as special cases. In the setting with two hospitals, we show that under a natural assumption on the patients' preference profiles, optimal welfare is in fact attained by the randomized assignment mechanism, which allocates patients to hospitals at random subject to the budget constraint, but avoids waiting times.Finally, we discuss potential policy implications of our results, as well as follow-up directions and open problems.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {541–542},
numpages = {2},
keywords = {healthcare, budget constraint, algorithmic game theory, waiting times, mechanism design},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

@inproceedings{10.1145/2554797.2554847,
author = {Halpern, Joseph Y. and Pass, Rafael and Seeman, Lior},
title = {The Truth behind the Myth of the Folk Theorem},
year = {2014},
isbn = {9781450326988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554797.2554847},
doi = {10.1145/2554797.2554847},
abstract = {We study the problem of computing an ε-Nash equilibrium in repeated games. Earlier work by Borgs et al. [2010] suggests that this problem is intractable. We show that if we make a slight change to their model---modeling the players as polynomial-time Turing machines that maintain state (rather than stateless polynomial-time Turing machines)---and make some standard cryptographic hardness assumptions (the existence of public key encryption), the problem can actually be solved in polynomial time.},
booktitle = {Proceedings of the 5th Conference on Innovations in Theoretical Computer Science},
pages = {543–554},
numpages = {12},
keywords = {computing nash equilibrium, repeated games, folk theorem},
location = {Princeton, New Jersey, USA},
series = {ITCS '14}
}

