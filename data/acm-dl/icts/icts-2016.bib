@inproceedings{10.1145/2840728.2840731,
author = {Babichenko, Yakov and Papadimitriou, Christos and Rubinstein, Aviad},
title = {Can Almost Everybody Be Almost Happy?},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840731},
doi = {10.1145/2840728.2840731},
abstract = {We conjecture that PPAD has a PCP-like complete problem, seeking a near equilibrium in which all but very few players have very little incentive to deviate. We show that, if one assumes that this problem requires exponential time, several open problems in this area are settled. The most important implication, proved via a "birthday repetition" reduction, is that the nO(log n) approximation scheme of Lipton et al. [23] for the Nash equilibrium of two-player games is essentially optimum. Two other open problems in the area are resolved once one assumes this conjecture, establishing that certain approximate equilibria are PPAD-complete: Finding a relative approximation of two-player Nash equilibria (without the well-supported restriction of [14]), and an approximate competitive equilibrium with equal incomes [10] with small clearing error and near-optimal Gini coefficient.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {1–9},
numpages = {9},
keywords = {nash equilibrium, hardness of approximation, ppad, pcp},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840742,
author = {Ileri, Atalay M. and Micali, Silvio},
title = {Mechanisms With Costly Knowledge},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840742},
doi = {10.1145/2840728.2840742},
abstract = {We propose investigating the design and analysis of game theoretic mechanisms when the players have very unstructured initial knowledge about themselves, but can refine their own knowledge at a cost.We consider several set-theoretic models of "costly knowledge". Specifically, we consider auctions of a single good in which a player i's only knowledge about his own valuation, θi, is that it lies in a given interval [a,b]. However, the player can pay a cost, depending on a and b (in several ways), and learn a possibly arbitrary but shorter (in several metrics) sub-interval, which is guaranteed to contain θi.In light of the set-theoretic uncertainty they face, it is natural for the players to act so as to minimize their regret. As a first step, we analyze the performance of the second-price mechanism in regret-minimizing strategies, and show that, in all our models, it always returns an outcome of very high social welfare.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {11–19},
numpages = {9},
keywords = {model, second price auction, extensive form games, knowledge refinement, social welfare analysis, auction theory, uncertainty, game theory, mechanism design, costly knowledge, set theoretic uncertainty},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840736,
author = {Rubinstein, Aviad},
title = {On the Computational Complexity of Optimal Simple Mechanisms},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840736},
doi = {10.1145/2840728.2840736},
abstract = {We consider a monopolist seller facing a single buyer with additive valuations over n heterogeneous, independent items. It is known that in this important setting optimal mechanisms may require randomization [12], use menus of infinite size [9], and may be computationally intractable [8]. This has sparked recent interest in finding simple mechanisms that obtain reasonable approximations to the optimal revenue [10, 15, 3]. In this work we attempt to find the optimal simple mechanism.There are many ways to define simple mechanisms. Here we restrict our search to partition mechanisms, where the seller partitions the items into disjoint bundles and posts a price for each bundle; the buyer is allowed to buy any number of bundles.We give a PTAS for the problem of finding a revenue-maximizing partition mechanism, and prove that the problem is strongly NP-hard. En route, we prove structural properties of near-optimal partition mechanisms which may be of independent interest: for example, there always exists a near-optimal partition mechanism that uses only a constant number of non-trivial bundles (i.e. bundles with more than one item).},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {21–28},
numpages = {8},
keywords = {ptas, mechanism design, simple mechanisms, revenue maximization},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840735,
author = {Landsberg, Joseph M. and Ressayre, Nicolas},
title = {Permanent v. Determinant: An Exponential Lower Bound Assuming Symmetry},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840735},
doi = {10.1145/2840728.2840735},
abstract = {Grenet's determinantal representation for the permanent is optimal among determinantal representations that are equivariant with respect to left multiplication by permutation and diagonal matrices (roughly half the symmetry group of the permanent). In particular, if any optimal determinantal representation of the permanent must be polynomially related to one with such symmetry, then Valiant's conjecture on permanent v. determinant is true.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {29–35},
numpages = {7},
keywords = {determinant, geometric complexity theory, msc 68q15 (20g05), permanent},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840733,
author = {Khot, Subhash and Shinkar, Igor},
title = {On Hardness of Approximating the Parameterized Clique Problem},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840733},
doi = {10.1145/2840728.2840733},
abstract = {In the Gap-clique (k, k/2) problem, the input is an n-vertex graph G, and the goal is to decide whether G contains a clique of size k or contains no clique of size k/2. It is an open question in the study of fixed parameterized tractability whether the Gap-clique (k, k/2) problem is fixed parameter tractable, i.e., whether it has an algorithm that runs in time f(k) ⋅ nα, where f(k) is an arbitrary function of the parameter k and the exponent α is a constant independent of k.In this paper, we give some evidence that the Gap-clique(k, k/2) problem is not fixed parameter tractable. Specifically, we define a constraint satisfaction problem, which we call Deg-2-sat, where the input is a system of k' quadratic equations in k' variables over a finite field F of size n', and the goal is to decide whether there is a solution in F that satisfies all the equations simultaneously. The main result in this paper is an "FPT-reduction" from Deg-2-sat to the Gap-clique(k, k/2) problem. If one were to hypothesize that the Deg-2-sat problem is not fixed parameter tractable, then our reduction would imply that the Gap-clique(k, k/2) problem is not fixed parameter tractable either. The reduction relies on the algebraic techniques used in proof of the PCP theorem.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {37–45},
numpages = {9},
keywords = {parameterized complexity, clique, fixed parameter tractability, hardness of approximation},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840734,
author = {Cohen, Gil and Shinkar, Igor},
title = {The Complexity of DNF of Parities},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840734},
doi = {10.1145/2840728.2840734},
abstract = {We study depth 3 circuits of the form OR-AND-XOR, or equivalently -- DNF of parities. This model was first explicitly studied by Jukna (CPC'06) who obtained a 2 Ω(n) lower bound, using graph theoretic arguments, for explicit functions. Several related models have gained attention in the last few years, such as parity decision trees, the parity kill number and AC0-XOR circuits.For a Boolean function f on the n dimensional Boolean cube, we denote by DNFParity(f) the least integer s for which there exists an OR-AND-XOR circuit, with OR gate of fan-in s, that computes f. We summarize some of our results: For any affine disperser f for dimension k, it holds that DNFParity(f) is bounded below by 2 n-2k. By plugging Shaltiel's affine disperser (FOCS'11) we obtain an explicit 2 n-no(1) lower bound.We give a non-trivial general upper bound by showing that DNFParity(f) &lt; O(2n / n) for any function f on n bits. This bound is shown to be tight up to an O(log n) factor.We show that for any symmetric function f it holds that DNFParity(f) &lt; 1.5n poly(n). Furthermore, there exists a symmetric function f for which this bound is tight up to a polynomial factor.For threshold functions we show tighter bounds. For example, we show that the majority function has DNFParity complexity of 2 n/2 poly(n). This is also tight up to a polynomial factor.For the inner product function IP on n inputs we show that DNFParity(IP) = 2 n/2-1. Previously, Jukna gave a lower bound of Ω(2n/4) for the DNFParity complexity of this function. We further give bounds for any low degree polynomial.Finally, we obtain a 2n-o(n) average case lower bound for the parity decision tree model using affine extractors.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {47–58},
numpages = {12},
keywords = {affine extractors, affine disperser, dnf},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840738,
author = {Gopalan, Parikshit and Nisan, Noam and Servedio, Rocco A. and Talwar, Kunal and Wigderson, Avi},
title = {Smooth Boolean Functions Are Easy: Efficient Algorithms for Low-Sensitivity Functions},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840738},
doi = {10.1145/2840728.2840738},
abstract = {A natural measure of smoothness of a Boolean function is its sensitivity (the largest number of Hamming neighbors of a point which differ from it in function value). The structure of smooth or equivalently low-sensitivity functions is still a mystery. A well-known conjecture states that every such Boolean function can be computed by a shallow decision tree. While this conjecture implies that smooth functions are easy to compute in the simplest computational model, to date no non-trivial upper bounds were known for such functions in any computational model, including unrestricted Boolean circuits. Even a bound on the description length of such functions better than the trivial 2n does not seem to have been known.In this work, we establish the first computational upper bounds on smooth Boolean functions: We show that every sensitivity s function is uniquely specified by its values on a Hamming ball of radius 2s. We use this to show that such functions can be computed by circuits of size nO(s).We show that sensitivity $s$ functions satisfy a strong pointwise noise-stability guarantee for random noise of rate O(1/s). We use this to show that these functions have formulas of depth O(s log n).We show that sensitivity s functions can be (locally) self-corrected from worst-case noise of rate exp(-O(s)).All our results are simple, and follow rather directly from (variants of) the basic fact that the function value at few points in small neighborhoods of a given point determine its function value via a majority vote. Our results confirm various consequences of the conjecture. They may be viewed as providing a new form of evidence towards its validity, as well as new directions towards attacking it.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {59–70},
numpages = {12},
keywords = {self-correction, boolean functions, sensitivity, noise stability, formula depth},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840749,
author = {Mossel, Elchanan and Xu, Jiaming},
title = {Local Algorithms for Block Models with Side Information},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840749},
doi = {10.1145/2840728.2840749},
abstract = {There has been a recent interest in understanding the power of local algorithms for optimization and inference problems on sparse graphs. Gamarnik and Sudan (2014) showed that local algorithms are weaker than global algorithms for finding large independent sets in sparse random regular graphs thus refuting a conjecture by Hatami, Lov\'{a}sz, and Szegedy (2012). Montanari (2015) showed that local algorithms are suboptimal for finding a community with high connectivityin the sparse Erd\"{o}s-R\'{e}nyi random graphs. For the symmetric planted partition problem (also named community detection for the block models) on sparse graphs, a simple observation is that local algorithms cannot have non-trivial performance.In this work we consider the effect of side information on local algorithms for community detection under the binary symmetric stochastic block model. In the block model with side information each of the n vertices is labeled + or - independently and uniformly at random; each pair of vertices is connected independently with probability a/n if both of them have the same label or b/n otherwise. The goal is to estimate the underlying vertex labeling given 1) the graph structure and 2) side information in the form of a vertex labeling positively correlated with the true one. Assuming that the ratio between in and out degree a/b is θ(1) and the average degree (a+b) / 2 = no(1), we show that a local algorithm, namely, belief propagation run on the local neighborhoods, maximizes the expected fraction of vertices labeled correctly in the following three regimes: |a--b|&lt;2 and all 0 &lt; α &lt; 1/2(a--b)2 &gt; C (a+b) for some constant C and all 0 &lt; α &lt; 1/2For all a,b if the probability that each given vertex label is incorrect is at most α* for some constant α* ∈ (0,1/2).Thus, in contrast to the case of independent sets or a single community in random graphs and to the case of symmetric block models without side information, we show that local algorithms achieve optimal performance in the above three regimes for the block model with side information.To complement our results, in the large degree limit α → ∞, we give a formula of the expected fraction of vertices labeled correctly by the local belief propagation, in terms of a fixed point of a recursion derived from the density evolution analysis with Gaussian approximations.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {71–80},
numpages = {10},
keywords = {random graphs, community detection, local algorithms},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840759,
author = {Beimel, Amos and Gabizon, Ariel and Ishai, Yuval and Kushilevitz, Eyal},
title = {Distribution Design},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840759},
doi = {10.1145/2840728.2840759},
abstract = {Motivated by applications in cryptography, we introduce and study the problem of distribution design. The goal of distribution design is to find a joint distribution on $n$ random variables that satisfies a given set of constraints on the marginal distributions. Each constraint can either require that two sequences of variables be identically distributed or, alternatively, that the two sequences have disjoint supports. We present several positive and negative results on the existence and efficiency of solutions for a given set of constraints.Distribution design can be seen as a strict generalization of several well-studied problems in cryptography. These include secret sharing, garbling schemes, and non-interactive protocols for secure multiparty computation. We further motivate the problem and our results by demonstrating their usefulness towards realizing non-interactive protocols for ad-hoc secure multiparty computation, in which any subset of the parties may choose to participate and the identity of the participants should remain hidden to the extent possible.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {81–92},
numpages = {12},
keywords = {garbling schemes, obfuscation, multi-input functional encryption, secure multiparty computation, secret sharing},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840729,
author = {Canonne, Clement L. and Gouleakis, Themis and Rubinfeld, Ronitt},
title = {Sampling Correctors},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840729},
doi = {10.1145/2840728.2840729},
abstract = {In many situations, sample data is obtained from a noisy or imperfect source. In order to address such corruptions, this paper introduces the concept of a sampling corrector. Such algorithms use structure that the distribution is purported to have, in order to allow one to make "on-the-fly" corrections to samples drawn from probability distributions. These algorithms then act as filters between the noisy data and the end user.We show connections between sampling correctors, distribution learning algorithms, and distribution property testing algorithms. We show that these connections can be utilized to expand the applicability of known distribution learning and property testing algorithms as well as to achieve improved algorithms for those tasks. As a first step, we show how to design sampling correctors using proper learning algorithms. We then focus on the question of whether algorithms for sampling correctors can be more efficient in terms of sample complexity than learning algorithms for the analogous families of distributions. When correcting monotonicity, we show that this is indeed the case when also granted query access to the cumulative distribution function. We also obtain sampling correctors for monotonicity without this stronger type of access, provided that the distribution be originally very close to monotone (namely, at a distance O(1/log2 n)). In addition to that, we consider a restricted error model that aims at capturing "missing data" corruptions. In this model, we show that distributions that are close to monotone have sampling correctors that are significantly more efficient than achievable by the learning approach. We then consider the question of whether an additional source of independent random bits is required by sampling correctors to implement the correction process. We show that for correcting close-to-uniform distributions and close-to-monotone distributions, no additional source of random bits is required, as the samples from the input source itself can be used to produce this randomness.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {93–102},
numpages = {10},
keywords = {probability distributions, randomized algorithms, learning, property testing},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840732,
author = {Tell, Roei},
title = {On Being Far from Far and on Dual Problems in Property Testing: [Extended Abstract]},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840732},
doi = {10.1145/2840728.2840732},
abstract = {This work studies a new type of problems in property testing, called dual problems. For a set Ϊ in a metric space and δ &gt; 0, denote by Fδ(Ϊ) the set of elements that are δ-far from Ϊ. Then, in property testing, a δtester for π is required to accept inputs from Ϊ and reject inputs from Fδ(Ϊ). A natural dual problem is the problem of δ-testing the set of "no" instances, that is Fδ(Ϊ): A δ-tester for Fδ(Ϊ) needs to accept inputs from Fδ(Ϊ) and reject inputs that are δ-far from Fδ(Ϊ) that is, it rejects inputs from Fδ(Fδ(Ϊ)). When #938;=Fδ(Fδ(Ϊ)) the dual problem is essentially equivalent to the original one, but this equality does not hold in general.Many dual problems constitute appealing testing problems that are interesting by themselves. In this work we study sets of the form Fδ(Fδ(Ϊ)), and apply this study to investigate several natural dual problems. In particular, we derive lower bounds and upper bounds on the query complexity of several classes of natural dual problems: These include dual problems of properties of functions (e.g., testing error-correcting codes and testing monotone functions), of properties of distributions (e.g., testing equivalence to a known distribution), and of various graph properties in the dense graph model and in the bounded-degree model. We also show that testing any dual problem with one-sided error is either trivial or requires a linear number of queries.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {103–110},
numpages = {8},
keywords = {closure operator, metric spaces, property testing, dual problems},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840730,
author = {Hardt, Moritz and Megiddo, Nimrod and Papadimitriou, Christos and Wootters, Mary},
title = {Strategic Classification},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840730},
doi = {10.1145/2840728.2840730},
abstract = {Machine learning relies on the assumption that unseen test instances of a classification problem follow the same distribution as observed training data. However, this principle can break down when machine learning is used to make important decisions about the welfare (employment, education, health) of strategic individuals. Knowing information about the classifier, such individuals may manipulate their attributes in order to obtain a better classification outcome. As a result of this behavior -- often referred to as gaming -- the performance of the classifier may deteriorate sharply. Indeed, gaming is a well-known obstacle for using machine learning methods in practice; in financial policy-making, the problem is widely known as Goodhart's law. In this paper, we formalize the problem, and pursue algorithms for learning classifiers that are robust to gaming.We model classification as a sequential game between a player named "Jury" and a player named "Contestant." Jury designs a classifier, and Contestant receives an input to the classifier drawn from a distribution. Before being classified, Contestant may change his input based on Jury's classifier. However, Contestant incurs a cost for these changes according to a cost function. Jury's goal is to achieve high classification accuracy with respect to Contestant's original input and some underlying target classification function, assuming Contestant plays best response. Contestant's goal is to achieve a favorable classification outcome while taking into account the cost of achieving it.For a natural class of "separable" cost functions, and certain generalizations, we obtain computationally efficient learning algorithms which are near optimal, achieving a classification error that is arbitrarily close to the theoretical minimum. Surprisingly, our algorithms are efficient even on concept classes that are computationally hard to learn. For general cost functions, designing an approximately optimal strategy-proof classifier, for inverse-polynomial approximation, is NP-hard.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {111–122},
numpages = {12},
keywords = {classification, learning theory, game theory},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840766,
author = {Gupta, Rishi and Roughgarden, Tim},
title = {A PAC Approach to Application-Specific Algorithm Selection},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840766},
doi = {10.1145/2840728.2840766},
abstract = {The best algorithm for a computational problem generally depends on the "relevant inputs," a concept that depends on the application domain and often defies formal articulation. While there is a large literature on empirical approaches to selecting the best algorithm for a given application domain, there has been surprisingly little theoretical analysis of the problem.This paper adapts concepts from statistical and online learning theory to reason about application-specific algorithm selection. Our models capture several state-of-the-art empirical and theoretical approaches to the problem, ranging from self-improving algorithms to empirical performance models, and our results identify conditions under which these approaches are guaranteed to perform well. We present one framework that models algorithm selection as a statistical learning problem, and our work here shows that dimension notions from statistical learning theory, historically used to measure the complexity of classes of binary- and real-valued functions, are relevant in a much broader algorithmic context. We also study the online version of the algorithm selection problem, and give possibility and impossibility results for the existence of no-regret learning algorithms.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {123–134},
numpages = {12},
keywords = {algorithm selection, parameter tuning, online learning, pac learning, meta-algorithms},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840748,
author = {Borgs, Christian and Chayes, Jennifer and Marple, Adrian and Teng, Shang-Hua},
title = {An Axiomatic Approach to Community Detection},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840748},
doi = {10.1145/2840728.2840748},
abstract = {Inspired by social choice theory in voting and other contexts, we provide the first axiomatic approach to community identification in a social network. We start from an abstract framework, called preference networks, which, for each member, gives their ranking of all the other members of the network. This complete-information preference model enables us to focus on the fundamental conceptual question: What constitutes a community in a social network?Within this framework, we axiomatically study the formation and structures of communities in two different ways. First, we apply social choice theory and define communities indirectly by postulating that they are fixed points of a preference aggregation function obeying certain desirable axioms. Second, we directly postulate six desirable axioms for communities to satisfy, without reference to preference aggregation. For the second approach, we prove a taxonomy theorem that provides a beautiful structural characterization of the family of axiom-conforming community rules as a lattice. We complement this structural theorem with a complexity result, showing that, while for some rules in the lattice, community characterization is straightforward, it is coNP-complete to characterize subsets according to others. Our study also sheds light on the limitations of defining community rules solely based on preference aggregation, namely that many aggregation functions lead to communities which violate at least one of our community axioms. These include any aggregation function satisfying Arrow's "independence of irrelevant alternatives" axiom, as well as commonly used aggregation schemes like the Borda count or generalizations thereof. Finally, we give a polynomial-time rule consistent with five axioms and weakly satisfying the sixth axiom.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {135–146},
numpages = {12},
keywords = {axiomatic framework, taxonomy of community rules, netork analysis, social choice theory, community identification, social and information networks},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840764,
author = {Brakerski, Zvika and Vaikuntanathan, Vinod and Wee, Hoeteck and Wichs, Daniel},
title = {Obfuscating Conjunctions under Entropic Ring LWE},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840764},
doi = {10.1145/2840728.2840764},
abstract = {We show how to securely obfuscate conjunctions, which are functions f(x1,...,xn) = ∧i∈I yi where I ⊆ [n] and each literal yi is either just xi or ¬ xi e.g., f(xi,...,x_n) = xi ⊆ ¬ x3 ⊆ ¬ x7 ... ⊆ xn-1. Whereas prior work of Brakerski and Rothblum (CRYPTO 2013) showed how to achieve this using a non-standard object called cryptographic multilinear maps, our scheme is based on an "entropic" variant of the Ring Learning with Errors (Ring LWE) assumption. As our core tool, we prove that hardness assumptions on the recent multilinear map construction of Gentry, Gorbunov and Halevi (TCC 2015) can be established based on entropic Ring LWE. We view this as a first step towards proving the security of additional mutlilinear map based constructions, and in particular program obfuscators, under standard assumptions.Our scheme satisfies virtual black box (VBB) security, meaning that the obfuscated program reveals nothing more than black-box access to f as an oracle, at least as long as (essentially) the conjunction is chosen from a distribution having sufficient entropy.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {147–156},
numpages = {10},
keywords = {obfuscation},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840760,
author = {Halevi, Shai and Ishai, Yuval and Jain, Abhishek and Kushilevitz, Eyal and Rabin, Tal},
title = {Secure Multiparty Computation with General Interaction Patterns},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840760},
doi = {10.1145/2840728.2840760},
abstract = {We present a unified framework for studying secure multiparty computation (MPC) with arbitrarily restricted interaction patterns such as a chain, a star, a directed tree, or a directed graph. Our study generalizes both standard MPC and recent models for MPC with specific restricted interaction patterns, such as those studied by Halevi et al. (Crypto 2011), Goldwasser et al. (Eurocrypt 2014), and Beimel et al. (Crypto 2014).Since restricted interaction patterns cannot always yield full security for MPC, we start by formalizing the notion of "best possible security" for any interaction pattern. We then obtain the following main results: Completeness theorem. We prove that the star interaction pattern is complete for the problem of MPC with general interaction patterns.Positive results. We present both information-theoretic and computationally secure protocols for computing arbitrary functions with general interaction patterns. We also present more efficient protocols for computing symmetric functions, both in the computational and in the information-theoretic setting. Our computationally secure protocols for general functions necessarily rely on indistinguishability obfuscation while the ones for computing symmetric functions make simple use of multilinear maps.Negative results. We show that, in many cases, the complexity of our information-theoretic protocols is essentially the best that can be achieved.All of our protocols rely on a correlated randomness setup, which is necessary in our setting (for computing general functions). In the computational case, we also present a generic procedure to make any correlated randomness setup reusable, in the common random string model.Although most of our information-theoretic protocols have exponential complexity, they may be practical for functions on small domains (e.g., f0; 1g20), where they are concretely faster than their computational counterparts.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {157–168},
numpages = {12},
keywords = {secure multi party computation, interaction pattern, obfuscation},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840765,
author = {Canetti, Ran and Holmgren, Justin},
title = {Fully Succinct Garbled RAM},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840765},
doi = {10.1145/2840728.2840765},
abstract = {We construct the first fully succinct garbling scheme for RAM programs, assuming the existence of indistinguishability obfuscation for circuits and one-way functions. That is, the size, space requirements, and runtime of the garbled program are the same as those of the input program, up to poly-logarithmic factors and a polynomial in the security parameter. The scheme can be used to construct indistinguishability obfuscators for RAM programs with comparable efficiency, at the price of requiring sub-exponential security of the underlying primitives.In particular, this opens the door to obfuscated computations that are sublinear in the length of their inputs.The scheme builds on the recent schemes of Koppula-Lewko-Waters and Canetti-Holmgren-Jain-Vaikuntanathan [STOC 15]. A key technical challenge here is how to combine the fixed-prefix technique of KLW, which was developed for deterministic programs, with randomized Oblivious RAM techniques. To overcome that, we develop a method for arguing about the indistinguishability of two obfuscated randomized programs that use correlated randomness. Along the way, we also define and construct garbling schemes that offer only partial protection. These may be of independent interest.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {169–178},
numpages = {10},
keywords = {randomized encodings, obfuscation, delegation, garbling},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840769,
author = {Chen, Yu-Chi and Chow, Sherman S.M. and Chung, Kai-Min and Lai, Russell W.F. and Lin, Wei-Kai and Zhou, Hong-Sheng},
title = {Cryptography for Parallel RAM from Indistinguishability Obfuscation},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840769},
doi = {10.1145/2840728.2840769},
abstract = {Since many cryptographic schemes are about performing computation on data, it is important to consider a computation model which captures the prominent features of modern system architecture. Parallel random access machine (PRAM) is such an abstraction which not only models multiprocessor platforms, but also new frameworks supporting massive parallel computation such as MapReduce.In this work, we explore the feasibility of designing cryptographic solutions for the PRAM model of computation to achieve security while leveraging the power of parallelism and random data access. We demonstrate asymptotically optimal solutions for a wide-range of cryptographic tasks based on indistinguishability obfuscation. In particular, we construct the first publicly verifiable delegation scheme with privacy in the persistent database setting, which allows a client to privately delegate both computation and data to a server with optimal efficiency. Specifically, the server can perform PRAM computation on private data with parallel efficiency preserved (up to poly-logarithmic overhead). Our results also cover succinct randomized encoding, searchable encryption, functional encryption, secure multiparty computation, and indistinguishability obfuscation for PRAM.We obtain our results in a modular way through a notion of computational-trace indistinguishability obfuscation (CiO), which may be of independent interests.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {179–190},
numpages = {12},
keywords = {pram, computation-trace indistinguishability obfuscation, indistinguishability obfuscation, randomized encoding, computation trace, ram},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840737,
author = {Jakobsen, Sune K. and S\o{}rensen, Troels B. and Conitzer, Vincent},
title = {Timeability of Extensive-Form Games},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840737},
doi = {10.1145/2840728.2840737},
abstract = {Extensive-form games constitute the standard representation scheme for games with a temporal component. But do all extensive-form games correspond to protocols that we can implement in the real world? We often rule out games with imperfect recall, which prescribe that an agent forget something that she knew before. In this paper, we show that even some games with perfect recall can be problematic to implement. Specifically, we show that if the agents have a sense of time passing (say, access to a clock), then some extensive-form games can no longer be implemented; no matter how we attempt to time the game, some information will leak to the agents that they are not supposed to have. We say such a game is not exactly timeable. We provide easy-to-check necessary and sufficient conditions for a game to be exactly timeable. Most of the technical depth of the paper concerns how to approximately time games, which we show can always be done, though it may require large amounts of time. Specifically, we show that some games require time proportional to the power tower of height proportional to the number of players, which in practice would make them untimeable. We hope to convince the reader that timeability should be a standard assumption, just as perfect recall is today. Besides the conceptual contribution to game theory, we show that timeability has implications for onion routing protocols.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {191–199},
numpages = {9},
keywords = {computational game theory, equilibrium computation},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840741,
author = {Chen, Jing and Micali, Silvio},
title = {Auction Revenue in the General Spiteful-Utility Model},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840741},
doi = {10.1145/2840728.2840741},
abstract = {It is well accepted that, in some auctions, a player's "true utility" may depend not only on the price he pays and whether or not he wins the good, but also on various forms of externalities, such as the prices paid by his competitors, and the identity and true value of the actual winner.In this work, we study revenue generation in single-good auctions under a very general model of externalities: the General Spiteful-Utility Model. Specifically, we Put forward new revenue benchmarks and solution concepts;Design new mechanisms when some information about the players' externalities is known; andAnalyze the revenue of the second-price mechanism when only the players have information about each other.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {201–211},
numpages = {11},
keywords = {light bayesian setting, single-good auction, externality, revenue, undominated strategy, spitefulness},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840758,
author = {Azar, Pablo Daniel and Goldwasser, Shafi and Park, Sunoo},
title = {How to Incentivize Data-Driven Collaboration Among Competing Parties},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840758},
doi = {10.1145/2840728.2840758},
abstract = {The availability of vast amounts of data is changing how we can make medical discoveries, predict global market trends, save energy, and develop new educational strategies. In certain settings such as Genome Wide Association Studies or deep learning, the sheer size of data (patient files or labeled examples) seems critical to making discoveries. When data is held distributed by many parties, as often is the case, they must share itly to reap its full benefits.One obstacle is the reluctance of different entities to share their data, due to privacy concerns or loss of competitive edge. Work on cryptographic multi-party computation over the last 30 years address the privacy aspects, but sheds no light on individual parties' losses and gains when access to data carries tangible rewards. Is an individual collaborator better off by collaborating, even if it is clear that better overall conclusions can be drawn. Addressing this question is the topic of this paper.The order in which collaborators receive the outputs of a collaboration will be a crucial aspect of our modeling and solutions. We believe that timing is an important and unaddressed issue in data-based collaborations.Our contributions are as follows. We formalize a model of $n$-party collaboration for computing functions over private inputs in which the participants receive their outputs in sequence, and the order depends on their private inputs. Each output "improves" on all previous outputs according to a reward function. We say that a mechanism for collaboration achieves a collaborative equilibrium if it guarantees a higher reward for all participants when joining a collaboration compared to not joining it. We show that while in general computing a collaborative equilibrium is NP-complete, we can design polynomial-time algorithms for computing it for a range of natural model settings. When possible, we design mechanisms to compute a distribution of outputs and an ordering of output delivery, based on the n participants' private inputs, which achieves a collaborative equilibrium.The collaboration mechanisms we develop are in the standard model, and thus require a central trusted party; however, we show that this assumption is not necessary under standard cryptographic assumptions. We show how the mechanisms can be implemented in a decentralized way by n distrustful parties using new extensions of classical secure multiparty computation that impose order and timing constraints on the delivery of outputs to different players in addition to guaranteeing privacy and correctness.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {213–225},
numpages = {13},
keywords = {research, data-sharing, collaboration, multi-party computation},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840757,
author = {Papadimitriou, Christos and Piliouras, Georgios},
title = {From Nash Equilibria to Chain Recurrent Sets: Solution Concepts and Topology},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840757},
doi = {10.1145/2840728.2840757},
abstract = {Nash's universal existence theorem for his notion of equilibria was essentially an ingenious application of fixed point theorems, the most sophisticated result in his era's topology --- in fact, recent algorithmic work has established that Nash equilibria are in fact computationally equivalent to fixed points. Here, we shift focus to universal non-equilibrium solution concepts that arise from an important theorem in the topology of dynamical systems that was unavailable to Nash. This approach takes as input both a game and a learning dynamic, defined over mixed strategies. Nash equilibria are guaranteed to be fixed points of such dynamics; however, the system behavior is captured by a more general object that is known in dynamical systems theory as chain recurrent set. Informally, once we focus on this solution concept, every game behaves like a potential game with the dynamic converging to these states. We characterize this solution for simple benchmark games under replicator dynamics, arguably the best known evolutionary dynamic in game theory. For potential games it coincides with the notion of equilibrium; however, in simple zero sum games, it can cover the whole state space. We discuss numerous novel computational as well as structural, combinatorial questions that chain recurrence raises.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {227–235},
numpages = {9},
keywords = {invariant, entropy, algorithmic game theory, replicator dynamics},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840744,
author = {Chen, Jing and McCauley, Samuel and Singh, Shikha},
title = {Rational Proofs with Multiple Provers},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840744},
doi = {10.1145/2840728.2840744},
abstract = {Interactive proofs model a world where a verifier delegates computation to an untrustworthy prover, verifying the prover's claims before accepting them. These proofs have applications to delegation of computation, probabilistically checkable proofs, crowdsourcing, and more.In some of these applications, the verifier may pay the prover based on the quality of his work. Rational proofs, introduced by Azar and Micali (2012), are an interactive proof model in which the prover is rational rather than untrustworthy---he may lie, but only to increase his payment. This allows the verifier to leverage the greed of the prover to obtain better protocols: while rational proofs are no more powerful than interactive proofs, the protocols are simpler and more efficient. Azar and Micali posed as an open problem whether multiple provers are more powerful than one for rational proofs.We provide a model that extends rational proofs to allow multiple provers. In this model, a verifier can cross-check the answers received by asking several provers. The verifier can pay the provers according to the quality of their work, incentivizing them to provide correct information.We analyze rational proofs with multiple provers from a complexity-theoretic point of view. We fully characterize this model by giving tight upper and lower bounds on its power. On the way, we resolve Azar and Micali's open problem in the affirmative, showing that multiple rational provers are strictly more powerful than one (under standard complexity-theoretic assumptions). We further show that the full power of rational proofs with multiple provers can be achieved using only two provers and five rounds of interaction. Finally, we consider more demanding models where the verifier wants the provers' payment to decrease significantly when they are lying, and fully characterize the power of the model when the payment gap must be noticeable (i.e., at least 1/p where p is a polynomial).},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {237–248},
numpages = {12},
keywords = {scoring rules, interactive proofs, dc uniform circuit families, complexity theory, multi-prover rational interactive proofs},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840740,
author = {Beyersdorff, Olaf and Bonacina, Ilario and Leroy, Chew},
title = {Lower Bounds: From Circuits to QBF Proof Systems},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840740},
doi = {10.1145/2840728.2840740},
abstract = {A general and long-standing belief in the proof complexity community asserts that there is a close connection between progress in lower bounds for Boolean circuits and progress in proof size lower bounds for strong propositional proof systems. Although there are famous examples where a transfer from ideas and techniques from circuit complexity to proof complexity has been effective, a formal connection between the two areas has never been established so far. Here we provide such a formal relation between lower bounds for circuit classes and lower bounds for Frege systems for quantified Boolean formulas (QBF).Starting from a propositional proof system P we exhibit a general method how to obtain a QBF proof system P+∀red{P}, which is inspired by the transition from resolution to Q-resolution. For us the most important case is a new and natural hierarchy of QBF Frege systems C-Frege+∀red that parallels the well-studied propositional hierarchy of C-Frege systems, where lines in proofs are restricted to belong to a circuit class C.Building on earlier work for resolution [Beyersdorff, Chew and Janota, 2015a] we establish a lower bound technique via strategy extraction that transfers arbitrary lower bounds for the circuit class C to lower bounds in C-Frege+∀red.By using the full spectrum of state-of-the-art circuit lower bounds, our new lower bound method leads to very strong lower bounds for QBF FREGE systems: exponential lower bounds and separations for the QBF proof system ACo[p]-Frege+∀red for all primes p;an exponential separation of ACo[p]-Frege+∀red from TCo/d-Frege+∀red;an exponential separation of the hierarchy of constant-depth systems ACo/d-Frege+∀red by formulas of depth independent of d.In the propositional case, all these results correspond to major open problems.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {249–260},
numpages = {12},
keywords = {circuit complexity, proof complexity, qbf proof complexity, frege systems},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840746,
author = {Carmosino, Marco L. and Gao, Jiawei and Impagliazzo, Russell and Mihajlin, Ivan and Paturi, Ramamohan and Schneider, Stefan},
title = {Nondeterministic Extensions of the Strong Exponential Time Hypothesis and Consequences for Non-Reducibility},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840746},
doi = {10.1145/2840728.2840746},
abstract = {We introduce the Nondeterministic Strong Exponential Time Hypothesis (NSETH) as a natural extension of the Strong Exponential Time Hypothesis (SETH). We show that both refuting and proving NSETH would have interesting consequences.In particular we show that disproving NSETH would give new nontrivial circuit lower bounds. On the other hand, NSETH implies non-reducibility results, i.e. the absence of (deterministic) fine-grained reductions from SAT to a number of problems. As a consequence we conclude that unless this hypothesis fails, problems such as 3-SUM, APSP and model checking of a large class of first-order graph properties cannot be shown to be SETH-hard using deterministic or zero-error probabilistic reductions.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {261–270},
numpages = {10},
keywords = {seth, all-pairs shortest path, 3-sum, computational complexity, fine-grained complexity, conditional lower bounds, nondeterminism},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840739,
author = {Aaronson, Scott and Bouland, Adam and Fitzsimons, Joseph and Lee, Mitchell},
title = {The Space "Just Above" BQP},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840739},
doi = {10.1145/2840728.2840739},
abstract = {We explore the space "just above" BQP by defining a complexity class naCQP (non-adaptive Collapse-free Quantum Polynomial time) which is larger than BQP but does not contain NP relative to an oracle. The class is defined by imagining that quantum computers can perform (non-adaptive) measurements that do not collapse the wavefunction. This non-physical model of computation can efficiently solve problems such as Graph Isomorphism and Approximate Shortest Vector which are believed to be intractable for quantum computers. Furthermore, it can search an unstructured N-element list in \~{O}(N1/3) time, but no faster than Ω(N1/4), and hence cannot solve NP-hard problems in a black box manner. In short, this model of computation is more powerful than standard quantum computation, but only slightly so. This is surprising as most modifications of BQP increase the power of quantum computation to NP or beyond.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {271–280},
numpages = {10},
keywords = {statistical zero knowledge, quantum computing, oracle separation, lower bound, np., bqp},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840767,
author = {Cummings, Rachel and Ligett, Katrina and Radhakrishnan, Jaikumar and Roth, Aaron and Wu, Zhiwei Steven},
title = {Coordination Complexity: Small Information Coordinating Large Populations},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840767},
doi = {10.1145/2840728.2840767},
abstract = {We initiate the study of a quantity that we call coordination complexity. In a distributed optimization problem, the information defining a problem instance is distributed among n parties, who need to each choose an action, which jointly will form a solution to the optimization problem. The coordination complexity represents the minimal amount of information that a centralized coordinator, who has full knowledge of the problem instance, needs to broadcast in order to coordinate the n parties to play a nearly optimal solution.We show that upper bounds on the coordination complexity of a problem imply the existence of good jointly differentially private algorithms for solving that problem, which in turn are known to upper bound the price of anarchy in certain games with dynamically changing populations.We show several results. We fully characterize the coordination complexity for the problem of computing a many-to-one matching in a bipartite graph. Our upper bound in fact extends much more generally to the problem of solving a linearly separable convex program. We also give a different upper bound technique, which we use to bound the coordination complexity of coordinating a Nash equilibrium in a routing game, and of computing a stable matching.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {281–290},
numpages = {10},
keywords = {privacy, coordination complexity},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840762,
author = {Straszak, Damian and Vishnoi, Nisheeth K.},
title = {On a Natural Dynamics for Linear Programming},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840762},
doi = {10.1145/2840728.2840762},
abstract = {In this paper we study dynamics inspired by Physarum polycephalum (a slime mold) for solving linear programs [NTY00, IJNT11, JZ12]. These dynamics are arrived at by a local and mechanistic interpretation of the inner workings of the slime mold and a global optimization perspective has been lacking even in the simplest of instances. Our first result is an interpretation of the dynamics as an optimization process. We show that Physarum dynamics can be seen as a steepest-descent type algorithm on a certain Riemannian manifold. Moreover, we prove that the trajectories of Physarum are in fact paths of optimizers to a parametrized family of convex programs, in which the objective is a linear cost function regularized by an entropy barrier. Subsequently, we rigorously establish several important properties of solution curves of Physarum. We prove global existence of such solutions and show that they have limits, being optimal solutions of the underlying LP. Finally, we show that the discretization of the Physarum dynamics is efficient for a class of linear programs, which include unimodular constraint matrices. Thus, together, our results shed some light on how nature might be solving instances of perhaps the most complex problem in P: linear programming.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {291},
numpages = {1},
keywords = {linear programming, physarum, dynamical systems},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840750,
author = {Tauman Kalai, Yael and Raz, Ran and Regev, Oded},
title = {On the Space Complexity of Linear Programming with Preprocessing},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840750},
doi = {10.1145/2840728.2840750},
abstract = {It is well known that Linear Programming is P-complete, with a logspace reduction. In this work we ask whether Linear Programming remains P-complete, even if the polyhedron (i.e., the set of linear inequality constraints) is a fixed polyhedron, for each input size, and only the objective function is given as input. More formally, we consider the following problem: maximize c⋅x, subject to Ax ≤ b; x ∈ Rd, where A,b are fixed in advance and only c is given as an input.We start by showing that the problem remains P-complete with a logspace reduction, thus showing that n o(1)-space algorithms are unlikely. This result is proved by a direct classical reduction.We then turn to study approximation algorithms and ask what is the best approximation factor that could be obtained by a small space algorithm. Since approximation factors are mostly meaningful when the objective function is non-negative, we restrict ourselves to the case where x &gt; 0 and c &gt; 0. We show that (even in this possibly easier case) approximating the value of c⋅x (within any polynomial factor) is P-complete with a polylog space reduction, thus showing nthat 2(log n)o(1)-space approximation algorithms are unlikely.The last result is proved using a recent work of Kalai, Raz, and Rothblum, showing that every language in P has a no-signaling multi-prover interactive proof with poly-logarithmic communication complexity. To the best of our knowledge, our result gives the first space hardness of approximation result proved by a PCP-based argument.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {293–300},
numpages = {8},
keywords = {linear programming, p-completeness, preprocessing, space complexity},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840751,
author = {Awasthi, Pranjal and Charikar, Moses and Krishnaswamy, Ravishankar and Sinop, Ali Kemal},
title = {Spectral Embedding of K-Cliques, Graph Partitioning and k-Means},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840751},
doi = {10.1145/2840728.2840751},
abstract = {We introduce and study a new notion of graph partitioning, intimately connected to spectral clustering and k-means clustering. Formally, given a graph G on n vertices, we ask to find a graph H that is the union of k cliques on n vertices, such that LG &gt; λ LH where λ is maximized. Here LG and LH are the (normalized) Laplacians of the graphs G and H respectively. Informally, our graph partitioning objective asks for the optimal spectral simplification of a given graph as a disjoint union of k cliques.We justify this objective function in several ways. First and foremost, we show that a commonly used spectral clustering algorithm implicitly optimizes this objective function, up to a factor of O(k). Using this connection, we immediately get an O(k)-approximation algorithm to our new objective function by simply using the spectral clustering algorithm on G. Next, we demonstrate another application of our objective function: we use it as a means to proving that simple spectral clustering algorithms can solve some well-studied graph partitioning problems (such as partitioning into expanders). Additionally, we also show that (a relaxation of) this optimization problem naturally arises as the dual problem to the question of finding the worst-case integrality gap instance for the classical k-means SDP. Finally, owing to these close connection between some classical clustering techniques (such as k-means and spectral clustering), we argue that a more complete understanding of this optimization problem could lead to new algorithmic insights and techniques for the area of graph partitioning.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {301–310},
numpages = {10},
keywords = {clustering, graph partitioning, k-means},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840753,
author = {Andoni, Alexandr and Chen, Jiecao and Krauthgamer, Robert and Qin, Bo and Woodruff, David P. and Zhang, Qin},
title = {On Sketching Quadratic Forms},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840753},
doi = {10.1145/2840728.2840753},
abstract = {We undertake a systematic study of sketching a quadratic form: given an n x n matrix A, create a succinct sketch sk(A) which can produce (without further access to A) a multiplicative (1+ε)-approximation to xT A x for any desired query x ∈ Rn. While a general matrix does not admit non-trivial sketches, positive semi-definite (PSD) matrices admit sketches of size θ(ε-2 n), via the Johnson-Lindenstrauss lemma, achieving the "for each" guarantee, namely, for each query x, with a constant probability the sketch succeeds. (For the stronger "for all" guarantee, where the sketch succeeds for all x's simultaneously, again there are no non-trivial sketches.)We design significantly better sketches for the important subclass of graph Laplacian matrices, which we also extend to symmetric diagonally dominant matrices. A sequence of work culminating in that of Batson, Spielman, and Srivastava (SIAM Review, 2014), shows that by choosing and reweighting O(ε-2 n) edges in a graph, one achieves the "for all" guarantee. Our main results advance this front. For the "for all" guarantee, we prove that Batson et al.'s bound is optimal even when we restrict to "cut queries" x ∈ (0,1)n. Specifically, an arbitrary sketch that can (1+ε)-estimate the weight of all cuts (S, bar S) in an n-vertex graph must be of size Ω(ε-2 n) bits. Furthermore, if the sketch is a cut-sparsifier (i.e., itself a weighted graph and the estimate is the weight of the corresponding cut in this graph), then the sketch must have Ω(ε-2 n) edges.In contrast, previous lower bounds showed the bound only for spectral-sparsifiers.For the "for each" guarantee, we design a sketch of size \~{O}(ε-1 n) bits for "cut queries" x ∈{0,1}n. We apply this sketch to design an algorithm for the distributed minimum cut problem. We prove a nearly-matching lower bound of Ω(ε-1 n) bits. For general queries x ∈ Rn, we construct sketches of size \~{O}(ε-1.6 n) bits.Our results provide the first separation between the sketch size needed for the "for all" and "for each" guarantees for Laplacian matrices.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {311–319},
numpages = {9},
keywords = {sketching, lower bound, graph sparsification, quadratic forms},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840756,
author = {Demaine, Erik D. and Lynch, Jayson and Mirano, Geronimo J. and Tyagi, Nirvan},
title = {Energy-Efficient Algorithms},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840756},
doi = {10.1145/2840728.2840756},
abstract = {We initiate the systematic study of the energy complexity of algorithms (in addition to time and space complexity) based on Landauer's Principle in physics, which gives a lower bound on the amount of energy a system must dissipate if it destroys information. We propose energy-aware variations of three standard models of computation: circuit RAM, word RAM, and transdichotomous RAM. On top of these models, we build familiar high-level primitives such as control logic, memory allocation, and garbage collection with zero energy complexity and only constant-factor overheads in space and time complexity, enabling simple expression of energy-efficient algorithms. We analyze several classic algorithms in our models and develop low-energy variations: comparison sort, insertion sort, counting sort, breadth-first search, Bellman-Ford, Floyd-Warshall, matrix all-pairs shortest paths, AVL trees, binary heaps, and dynamic arrays. We explore the time/space/energy trade-off and develop several general techniques for analyzing algorithms and reducing their energy complexity. These results lay a theoretical foundation for a new field of semi-reversible computing and provide a new framework for the investigation of algorithms.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {321–332},
numpages = {12},
keywords = {reversible computing, algorithms, models of computation, landauer's principle},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840743,
author = {Jakobsen, Sune K. and Orlandi, Claudio},
title = {How To Bootstrap Anonymous Communication},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840743},
doi = {10.1145/2840728.2840743},
abstract = {We ask whether it is possible to anonymously communicate a large amount of data using only public (non-anonymous) communication together with a small anonymous channel. We think this is a central question in the theory of anonymous communication and to the best of our knowledge this is the first formal study in this direction.Towards this goal, we introduce the novel concept of anonymous steganography: think of a leaker Lea who wants to leak a large document to Joe the journalist. Using anonymous steganography Lea can embed this document in innocent looking communication on some popular website (such as cat videos on YouTube or funny memes on 9GAG). Then Lea provides Joe with a short decoding key dk which, when applied to the entire website, recovers the document while hiding the identity of Lea among the large number of users of the website. Our contributions include: Introducing and formally defining anonymous steganography,A construction showing that anonymous steganography is possible (which uses recent results in circuits obfuscation),A lower bound on the number of bits which are needed to bootstrap anonymous communication.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {333–344},
numpages = {12},
keywords = {steganography, cryptography, anonymity},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840745,
author = {Bitansky, Nir and Goldwasser, Shafi and Jain, Abhishek and Paneth, Omer and Vaikuntanathan, Vinod and Waters, Brent},
title = {Time-Lock Puzzles from Randomized Encodings},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840745},
doi = {10.1145/2840728.2840745},
abstract = {Time-lock puzzles are a mechanism for sending messages "to the future". A sender can quickly generate a puzzle with a solution s that remains hidden until a moderately large amount of time t has elapsed. The solution s should be hidden from any adversary that runs in time significantly less than t, including resourceful parallel adversaries with polynomially many processors.While the notion of time-lock puzzles has been around for 22 years, there has only been a single candidate proposed. Fifteen years ago, Rivest, Shamir and Wagner suggested a beautiful candidate time-lock puzzle based on the assumption that exponentiation modulo an RSA integer is an "inherently sequential" computation.We show that various flavors of randomized encodings give rise to time-lock puzzles of varying strengths, whose security can be shown assuming the mere existence of non-parallelizing languages, which are languages that require circuits of depth at least t to decide, in the worst-case. The existence of such languages is necessary for the existence of time-lock puzzles.We instantiate the construction with different randomized encodings from the literature, where increasingly better efficiency is obtained based on increasingly stronger cryptographic assumptions, ranging from one-way functions to indistinguishability obfuscation. We also observe that time-lock puzzles imply one-way functions, and thus the reliance on some cryptographic assumption is necessary.Finally, generalizing the above, we construct other types of puzzles such as proofs of work from randomized encodings and a suitable worst-case hardness assumption (that is necessary for such puzzles to exist).},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {345–356},
numpages = {12},
keywords = {time-lock puzzles, randomized encodings, proofs of work},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840761,
author = {Boyle, Elette and Naor, Moni},
title = {Is There an Oblivious RAM Lower Bound?},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840761},
doi = {10.1145/2840728.2840761},
abstract = {An Oblivious RAM (ORAM), introduced by Goldreich and Ostrovsky (JACM 1996), is a (probabilistic) RAM that hides its access pattern, i.e. for every input the observed locations accessed are similarly distributed. Great progress has been made in recent years in minimizing the overhead of ORAM constructions, with the goal of obtaining the smallest overhead possible.We revisit the lower bound on the overhead required to obliviously simulate programs, due to Goldreich and Ostrovsky. While the lower bound is fairly general, including the offline case, when the simulator is given the reads and writes ahead of time, it does assume that the simulator behaves in a "balls and bins" fashion. That is, the simulator must act by shuffling data items around, and is not allowed to have sophisticated encoding of the data.We prove that for the offline case, showing a lower bound without the above restriction is related to the size of the circuits for sorting. Our proof is constructive, and uses a bit-slicing approach which manipulates the bit representations of data in the simulation. This implies that without obtaining yet unknown superlinear lower bounds on the size of such circuits, we cannot hope to get lower bounds on offline (unrestricted) ORAMs.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {357–368},
numpages = {12},
keywords = {pram, cryptography, oram, multi-party computation, sorting},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840747,
author = {Bun, Mark and Nissim, Kobbi and Stemmer, Uri},
title = {Simultaneous Private Learning of Multiple Concepts},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840747},
doi = {10.1145/2840728.2840747},
abstract = {We investigate the {em direct-sum} problem in the context of differentially private PAC learning: What is the sample complexity of solving k learning tasks simultaneously under differential privacy, and how does this cost compare to that of solving k learning tasks without privacy? In our setting, an individual example consists of a domain element x labeled by k unknown concepts (c1,...,ck). The goal of a multi-learner is to output k hypotheses (h1,...,hk) that generalize the input examples.Without concern for privacy, the sample complexity needed to simultaneously learn $k$ concepts is essentially the same as needed for learning a single concept. Under differential privacy, the basic strategy of learning each hypothesis independently yields sample complexity that grows polynomially with k. For some concept classes, we give multi-learners that require fewer samples than the basic strategy. Unfortunately, however, we also give lower bounds showing that even for very simple concept classes, the sample cost of private multi-learning must grow polynomially in k.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {369–380},
numpages = {12},
keywords = {differential privacy, agnostic learning, pac learning, direct-sum},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840754,
author = {Tyagi, Himanshu and Venkatakrishnan, Shaileshh and Viswanath, Pramod and Watanabe, Shun},
title = {Information Complexity Density and Simulation of Protocols},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840754},
doi = {10.1145/2840728.2840754},
abstract = {A simulation of an interactive protocol entails the use of interactive communication to produce the output of the protocol to within a fixed statistical distance ε. Recent works have proposed that the information complexity of the protocol plays a central role in characterizing the minimum number of bits that the parties must exchange for a successful simulation, namely the distributional communication complexity of simulating the protocol. Several simulation protocols have been proposed with communication complexity depending on the information complexity of the simulated protocol. However, in the absence of any general lower bounds for distributional communication complexity, the conjectured central role of information complexity is far from settled. We fill this gap and show that the distributional communication complexity of ε-simulating a protocol is bounded below by the ε-tail λε of the information complexity density, a random variable with information complexity as its expected value. For protocols with bounded number of rounds, we give a simulation protocol that yields a matching upper bound. Thus, it is not information complexity but λε that governs the distributional communication complexity.As applications of our bounds, in the amortized regime for product protocols, we identify the exact second order term, together with the precise dependence on ε. For general protocols such as a mixture of two product protocols or for the amortized case when the repetitions are not independent, we derive a general formula for the leading asymptotic term. These results sharpen and significantly extend known results in the amortized regime. In the single-shot regime, our lower bound sheds light on the dependence of communication complexity on ε. We illustrate this with an example that exhibits an arbitrary separation between distributional communication complexity and information complexity for all sufficiently small $ep$.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {381–391},
numpages = {11},
keywords = {information complexity, simulation of protocols, interactive protocols},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840768,
author = {Chen, Ruiwen and Santhanam, Rahul},
title = {Satisfiability on Mixed Instances},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840768},
doi = {10.1145/2840728.2840768},
abstract = {The study of the worst-case complexity of the Boolean Satisfiability (SAT) problem has seen considerable progress in recent years, for various types of instances including CNFs, Boolean formulas and constant-depth circuits. We systematically investigate the complexity of solving mixed instances, where different parts of the instance come from different types. Our investigation is motivated partly by practical contexts such as SMT (Satisfiability Modulo Theories) solving, and partly by theoretical issues such as the exact complexity of graph problems and the desire to find a unifying framework for known satisfiability algorithms.We investigate two kinds of mixing: conjunctive mixing, where the mixed instance is formed by taking the conjunction of pure instances of different types, and compositional mixing, where the mixed instance is formed by the composition of different kinds of circuits. For conjunctive mixing, we show that non-trivial savings over brute force search can be obtained for a number of instance types in a generic way using the paradigm of subcube partitioning. We apply this generic result to show a meta-algorithmic result about graph optimisation problems: any optimisation problem that can be formalised in Monadic SNP can be solved exactly with exponential savings over brute-force search. This captures known results about problems such as Clique, Independent Set and Vertex Cover, in a uniform way. For certain kinds of conjunctive mixing, such as mixtures of $k$-CNFs and CNFs of bounded size, and of k-CNFs and Boolean formulas, we obtain improved savings over subcube partitioning by combining existing algorithmic ideas in a more fine-grained way.We use the perspective of compositional mixing to show the first non-trivial algorithm for satisfiability of quantified Boolean formulas, where there is no depth restriction on the formula. We show that there is an algorithm which for any such formula with a constant number of quantifier blocks and of size nc, where c &lt; 5/4, solves satisfiability in time $2^{n-n^{Omega(1)}}$.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {393–402},
numpages = {10},
keywords = {quantified boolean formula, mixed instance, boolean satisfiability algorithm},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840752,
author = {Papadimitriou, Christos H. and Vishnoi, Nisheeth K.},
title = {On the Computational Complexity of Limit Cycles in Dynamical Systems},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840752},
doi = {10.1145/2840728.2840752},
abstract = {We study the Poincare-Bendixson theorem for two-dimensional continuous dynamical systems in compact domains from the point of view of computation, seeking algorithms for finding the limit cycle promised by this classical result. We start by considering a discrete analogue of this theorem and show that both finding a point on a limit cycle, and determining if a given point is on one, are PSPACE-complete. For the continuous version, we show that both problems are uncomputable in the real complexity sense; i.e., their complexity is arbitrarily high. Subsequently, we introduce a notion of an approximate cycle and prove an approximate Poincare-Bendixson theorem guaranteeing that some orbits come very close to forming a cycle in the absence of approximate fixpoints; surprisingly, it holds for all dimensions. The corresponding computational problem defined in terms of arithmetic circuits is PSPACE-complete.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {403},
numpages = {1},
keywords = {poincare-bendixson theorem, dynamical systems, limit cycles, computational complexity},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

@inproceedings{10.1145/2840728.2840755,
author = {Golovnev, Alexander and Kulikov, Alexander S.},
title = {Weighted Gate Elimination: Boolean Dispersers for Quadratic Varieties Imply Improved Circuit Lower Bounds},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840755},
doi = {10.1145/2840728.2840755},
abstract = {In this paper we motivate the study of Boolean dispersers for quadratic varieties by showing that an explicit construction of such objects gives improved circuit lower bounds. An (n,k,s)-quadratic disperser is a function on n variables that is not constant on any subset of Fn/2 of size at least s that can be defined as the set of common roots of at most k quadratic polynomials. We show that if a Boolean function f is a (n, 1.83n, 2g(n)-quadratic disperser for any function g(n)=o(n) then the circuit size of f is at least 3.11n. In order to prove this, we generalize the gate elimination method so that the induction works on the size of the variety rather than on the number of variables as in previously known proofs.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {405–411},
numpages = {7},
keywords = {lower bounds, boolean circuits, dispersers},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}

