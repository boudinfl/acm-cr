@inproceedings{10.1145/2688073.2688109,
author = {Jain, Abhishek and Kalai, Yael Tauman and Lewko, Allison Bishop},
title = {Interactive Coding for Multiparty Protocols},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688109},
doi = {10.1145/2688073.2688109},
abstract = {The problem of constructing error-resilient interactive protocols was introduced in the seminal works of Schulman (FOCS 1992, STOC 1993). These works show how to convert any two-party interactive protocol into one that is resilient to constant-fraction of adversarial error, while blowing up the communication by only a constant factor.In this work we extend the work of Schulman to the multi-party setting. We show how to convert any (non-adaptive) $n$-party protocol into one that is resilient to Θ(1/n)-fraction of adversarial error, while blowing up the communication by only a constant factor.One might hope to get resilience to constant-fraction of errors, by restricting the adversary's error distribution, and allowing him to make at most a constant-fraction of errors per party. We present a black-box lower bound, showing that we cannot be resilient to such adversarial errors, even if we increase the communication by an arbitrary polynomial factor, assuming the error-resilient protocol has a fixed (non-adaptive) speaking order.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {1–10},
numpages = {10},
keywords = {interactive coding, multiparty computation},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688077,
author = {Efremenko, Klim and Gelles, Ran and Haeupler, Bernhard},
title = {Maximal Noise in Interactive Communication over Erasure Channels and Channels with Feedback},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688077},
doi = {10.1145/2688073.2688077},
abstract = {We provide tight upper and lower bounds on the noise resilience of interactive communication over noisy channels with feedback. In this setting, we show that the maximal fraction of noise that any robust protocol can resist is 1/3. Additionally, we provide a simple and efficient robust protocol that succeeds as long as the fraction of noise is at most 1/3--ε. Surprisingly, both bounds hold regardless of whether the parties send bits or symbols from an arbitrarily large alphabet.We also consider interactive communication over erasure channels. We provide a protocol that matches the optimal tolerable erasure rate of 1/2--ε of previous protocols (Franklin et al., CRYPTO '13) but operates in a much simpler and more efficient way. Our protocol works with an alphabet of size 4, in contrast to prior protocols in which the alphabet size grows as ε ╰ 0. Building on the above algorithm with a fixed alphabet size, we are able to devise a protocol for binary erasure channels that tolerates erasure rates of up to 1/3--ε.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {11–20},
numpages = {10},
keywords = {adversarial noise, erasure channels, interactive communication, channels with feedback, coding},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688087,
author = {Braverman, Mark and Mao, Jieming},
title = {Simulating Noisy Channel Interaction},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688087},
doi = {10.1145/2688073.2688087},
abstract = {We show that T rounds of interaction over the binary symmetric channel BSC1/2--ε with feedback can be simulated with O(ε2 T) rounds of interaction over a noiseless channel. We also introduce a more general "energy cost" model of interaction over a noisy channel. We show energy cost to be equivalent to external information complexity, which implies that our simulation results are unlikely to carry over to energy complexity. Our main technical innovation is a self-reduction from simulating a noisy channel to simulating a slightly-less-noisy channel, which may have other applications in the area of interactive compression.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {21–30},
numpages = {10},
keywords = {noisy channel, communication complexity, information complexity},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688117,
author = {Applebaum, Benny and David, Liron and Even, Guy},
title = {Deterministic Rateless Codes for BSC},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688117},
doi = {10.1145/2688073.2688117},
abstract = {A rateless code encodes a finite length information word into an infinitely long codeword such that longer prefixes of the codeword can tolerate a larger fraction of errors. A rateless code achieves capacity for a family of channels if, for every channel in the family, reliable communication is obtained by a prefix of the code whose rate is arbitrarily close to the channel's capacity. As a result, a universal encoder can communicate over all channels in the family while simultaneously achieving optimal communication overhead.In this paper, we construct the first deterministic rateless code for the binary symmetric channel. Our code can be encoded and decoded in O(β) time per bit and in almost logarithmic parallel time of O(β log n), where β is any (arbitrarily slow) super-constant function. Furthermore, the error probability of our code is almost exponentially small exp(--β(n/β)). Previous rateless codes are probabilistic (i.e., based on code ensembles), require polynomial time per bit for decoding, and have inferior asymptotic error probabilities.Our main technical contribution is a constructive proof for the existence of an infinite generating matrix that each of its prefixes induce a weight distribution that approximates the expected weight distribution of a random linear code.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {31–40},
numpages = {10},
keywords = {capacity achieving error correcting code, rateless codes, binary symmetric channel},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688097,
author = {Avin, Chen and Keller, Barbara and Lotker, Zvi and Mathieu, Claire and Peleg, David and Pignolet, Yvonne-Anne},
title = {Homophily and the Glass Ceiling Effect in Social Networks},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688097},
doi = {10.1145/2688073.2688097},
abstract = {The glass ceiling effect has been defined in a recent US Federal Commission report as "the unseen, yet unbreakable barrier that keeps minorities and women from rising to the upper rungs of the corporate ladder, regardless of their qualifications or achievements". It is well documented that many societies and organizations exhibit a glass ceiling. In this paper we formally define and study the glass ceiling effect in social networks and propose a natural mathematical model, called the biased preferential attachment model, that partially explains the causes of the glass ceiling effect. This model consists of a network composed of two types of vertices, representing two sub-populations, and accommodates three well known social phenomena: (i) the "rich get richer" mechanism, (ii) a minority-majority partition, and (iii) homophily. We prove that our model exhibits a strong moment glass ceiling effect and that all three conditions are necessary, i.e., removing any one of them will prevent the appearance of a glass ceiling effect. Additionally, we present empirical evidence taken from a mentor-student network of researchers (derived from the DBLP database) that exhibits both a glass ceiling effect and the above three phenomena.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {41–50},
numpages = {10},
keywords = {glass ceiling, preferential attachment, social networks, homophily},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688091,
author = {Kleinberg, Jon and Oren, Sigal},
title = {Dynamic Models of Reputation and Competition in Job-Market Matching},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688091},
doi = {10.1145/2688073.2688091},
abstract = {A fundamental decision faced by a firm hiring employees --- and a familiar one to anyone who has dealt with the academic job market, for example --- is deciding what caliber of candidates to pursue. Should the firm try to increase its reputation by making offers to higher-quality candidates, despite the risk that the candidates might reject the offers and leave the firm empty-handed? Or is it better to play it safe and go for weaker candidates who are more likely to accept the offer? The question acquires an added level of complexity once we take into account the effect one hiring cycle has on the next: hiring better employees in the current cycle increases the firm's reputation, which in turn increases its attractiveness for higher-quality candidates in the next hiring cycle. These considerations introduce an interesting temporal dynamic aspect to the rich line of research on matching models for job markets, in which long-range planning and evolving reputational effects enter into the strategic decisions made by competing firms.The full set of ingredients in such recruiting decisions is complex, and this has made it difficult to model the fundamental strategic tension at the core of the problem. Here we develop a model based on two competing firms to try capturing as cleanly as possible the elements that we believe constitute this strategic tension: the trade-off between short-term recruiting success and long-range reputation-building; the inefficiency that results from underemployment of people who are not ranked highest; and the influence of earlier accidental outcomes on long-term reputations.Our model exhibits all these phenomena in a stylized setting, governed by a parameter $q$ that captures the difference in strength between the top candidate in each hiring cycle and the next best. Building on an economic model of competition between parties of unequal strength, we show that when $q$ is relatively low, the efficiency of the job market is improved by long-range reputational effects, but when $q$ is relatively high, taking future reputations into account can sometimes reduce the efficiency. While this trade-off arises naturally in the model, the multi-period nature of the strategic reasoning it induces adds new sources of complexity, and our analysis reveals interesting connections between competition with evolving reputations and the the dynamics of urn processes.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {51–60},
numpages = {10},
keywords = {economics, reputation, theory},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688089,
author = {Leung, Samantha and Lui, Edward and Pass, Rafael},
title = {Voting with Coarse Beliefs},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688089},
doi = {10.1145/2688073.2688089},
abstract = {The classic Gibbard-Satterthwaite theorem says that every strategy-proof voting rule with at least three possible candidates must be dictatorial. Similar impossibility results hold even if we consider a weaker notion of strategy-proofness where voters believe that the other voters' preferences are i.i.d. (independent and identically distributed). In this paper, we take a bounded-rationality approach to this problem and consider a setting where voters have "coarse" beliefs (a notion that has gained popularity in the behavioral economics literature). In particular, we construct good voting rules that satisfy a notion of strategy-proofness with respect to coarse i.i.d.~beliefs, thus circumventing the above impossibility results.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {61},
numpages = {1},
keywords = {voting, gibbard-satterthwaite},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688110,
author = {Ebrahimi, Roozbeh and Gao, Jie and Ghasemiesfeh, Golnaz and Schoenebeck, Grant},
title = {Complex Contagions in Kleinberg's Small World Model},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688110},
doi = {10.1145/2688073.2688110},
abstract = {Complex contagions describe diffusion of behaviors in a social network in settings where spreading requires influence by two or more neighbors. In a k-complex contagion, a cluster of nodes are initially infected, and additional nodes become infected in the next round if they have at least k already infected neighbors. It has been argued that complex contagions better model behavioral changes such as adoption of new beliefs, fashion trends or expensive technology innovations. This has motivated rigorous understanding of spreading of complex contagions in social networks. Despite simple contagions (k=1) that spread fast in all small world graphs, how complex contagions spread is much less understood. Previous work [11] analyzes complex contagions in Kleinberg's small world model [14] where edges are randomly added according to a spatial distribution (with exponent γ) on top of a two dimensional grid structure. It has been shown in [11] that the speed of complex contagions differs exponentially when γ=0 compared to when γ=2.In this paper, we fully characterize the entire parameter space of γ except at one point, and provide upper and lower bounds for the speed of k-complex contagions. We study two subtly different variants of Kleinberg's small world model and show that, with respect to complex contagions, they behave differently. For each model and each k ≥ 2, we show that there is an intermediate range of values, such that when γ takes any of these values, a k-complex contagion spreads quickly on the corresponding graph, in a polylogarithmic number of rounds. However, if γ is outside this range, then a k-complex contagion requires a polynomial number of rounds to spread to the entire network.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {63–72},
numpages = {10},
keywords = {kleinberg's small world model, social networks, complex contagion},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688118,
author = {Mehta, Ruta and Panageas, Ioannis and Piliouras, Georgios},
title = {Natural Selection as an Inhibitor of Genetic Diversity: Multiplicative Weights Updates Algorithm and a Conjecture of Haploid Genetics [Working Paper Abstract]},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688118},
doi = {10.1145/2688073.2688118},
abstract = {In a recent series of papers a surprisingly strong connection was discovered between standard evolutionary models of natural selection and Multiplicative Weights Updates Algorithm, a ubiquitous model of online learning and optimization. These papers establish that, under specific assumptions, mathematical models of biological evolution can be reduced to studying discrete replicator dynamics, a close variant of MWUA, in coordination games. This connection allows for introducing insights from game theoretic dynamics into the field of mathematical biology.Using these results as a stepping stone, we show that mathematical models of haploid evolution imply the extinction of genetic diversity in the long term limit, a widely believed conjecture in genetics. In game theoretic terms we show that in the case of coordination games, under minimal genericity assumptions, discrete replicator dynamics converge to pure Nash equilibria for all but a zero measure of initial conditions. This result holds despite the fact that mixed Nash equilibria can be exponentially (or even uncountably) many, completely dominating in number the set of pure Nash equilibria. Thus, in haploid organisms the long term preservation of genetic diversity needs to be safeguarded by other evolutionary mechanisms such as mutations and speciation.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {73},
numpages = {1},
keywords = {stability of equilibria, evolution, replicator dynamics, multiplicative weight updates, algorithmic game theory},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688088,
author = {Panigrahy, Rina and Popat, Preyas},
title = {Fractal Structures in Adversarial Prediction},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688088},
doi = {10.1145/2688073.2688088},
abstract = {Fractals are self-similar recursive structures that have been used in modeling several real world processes. In this work we study how "fractal-like" processes arise in a prediction game where an adversary is generating a sequence of bits and an algorithm is trying to predict them. We will see that under a certain formalization of the predictive payoff for the algorithm it is most optimal for the adversary to produce a fractal-like sequence to minimize the algorithm's ability to predict. Indeed it has been suggested before that financial markets exhibit a fractal-like behavior [FP98, Man05]. We prove that a fractal-like distribution arises naturally out of an optimization from the adversary's perspective.In addition, we give optimal trade-offs between predictability and expected deviation (i.e. sum of bits) for our formalization of predictive payoff. This result is motivated by the observation that several time series data exhibit higher deviations than expected for a completely random walk.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {75–84},
numpages = {10},
keywords = {adversarial prediction, fractal brownian motion},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688111,
author = {Chierichetti, Flavio and Dasgupta, Anirban and Kumar, Ravi and Lattanzi, Silvio},
title = {On Learning Mixture Models for Permutations},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688111},
doi = {10.1145/2688073.2688111},
abstract = {In this paper we consider the problem of learning a mixture of permutations, where each component of the mixture is generated by a stochastic process. Learning permutation mixtures arises in practical settings when a set of items is ranked by different sub-populations and the rankings of users in a sub-population tend to agree with each other. While there is some applied work on learning such mixtures, they have been mostly heuristic in nature.We study the problem where the permutations in a mixture component are generated by the classical Mallows process in which each component is associated with a center and a scalar parameter. We show that even when the centers are arbitrarily separated, with exponentially many samples one can learn the mixture, provided the parameters are all the same and known; we also show that the latter two assumptions are information-theoretically inevitable. We then focus on polynomial-time learnability and show bounds on the performance of two simple algorithms for the case when the centers are well separated.Conceptually, our work suggests that while permutations may not enjoy as nice mathematical properties as Gaussians, certain structural aspects can still be exploited towards analyzing the corresponding mixture learning problem.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {85–92},
numpages = {8},
keywords = {ranking aggregation, mallows models, mixture models, permutations},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688108,
author = {Juba, Brendan},
title = {Restricted Distribution Automatizability in PAC-Semantics},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688108},
doi = {10.1145/2688073.2688108},
abstract = {We consider the proof search (--automatizability--) problem for propositional proof systems in the context of  knowledge discovery (or  data mining and  analytics). Discovered knowledge necessarily features a weaker semantics than usually employed in mathematical logic, and in this work we find that these weaker semantics may result in a proof search problem that seems easier than the classical problem, but that is nevertheless nontrivial. Specifically, if we consider a knowledge discovery task corresponding to the unsupervised learning of parities over the uniform distribution from partial information, then we find the following: Proofs in the system polynomial calculus with resolution (PCR) can be detected in quasipolynomial time, in contrast to the nO(√n)-time best known algorithm for classical proof search for PCR.By contrast, a quasipolynomial time algorithm that distinguishes whether a formula of PCR is satisfied a 1-ε fraction of the time or merely an -ε-fraction of the time (for polynomially small -ε would give a randomized quasipolynomial time algorithm for NP, so the use of the promise of a small PCR proof is essential in the above result.Likewise, if integer factoring requires subexponential time, we find that bounded-depth Frege proofs cannot be detected in quasipolynomial time.The final result essentially shows that negative results based on the hardness of interpolation [31, 13, 11] persist under this new semantics, while the first result suggests, in light of negative results for PCR [22] and resolution [2] under the classical semantics, that there are intriguing new possibilities for proof search in the context of knowledge discovery and data analysis.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {93–102},
numpages = {10},
keywords = {pac-semantics, automatizability, proof complexity, learning},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688094,
author = {Fitzsimons, Joseph and Vidick, Thomas},
title = {A Multiprover Interactive Proof System for the Local Hamiltonian Problem},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688094},
doi = {10.1145/2688073.2688094},
abstract = {We give a quantum interactive proof system for the local Hamiltonian problem on n qubits in which (i) the verifier has a single round of interaction with five entangled provers, (ii) the verifier sends a classical message on O(log n) bits to each prover, who replies with a constant number of qubits, and (iii) completeness and soundness are separated by an inverse polynomial in $n$. As the same class of proof systems, without entanglement between the provers, is included in QCMA, our result provides the first indication that quantum multiprover interactive proof systems with entangled provers may be strictly more powerful than unentangled-prover interactive proof systems. A distinguishing feature of our protocol is that the completeness property requires honest provers to share a large entangled state, obtained as the encoding of the ground state of the local Hamiltonian via an error-correcting code. Our result can be interpreted as a first step towards a multiprover variant of the quantum PCP conjecture.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {103–112},
numpages = {10},
keywords = {quantum interactive proofs, local hamiltonian, entanglement},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688074,
author = {G\"{o}\"{o}s, Mika and Pitassi, Toniann and Watson, Thomas},
title = {Zero-Information Protocols and Unambiguity in Arthur-Merlin Communication},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688074},
doi = {10.1145/2688073.2688074},
abstract = {We study whether information complexity can be used to attack the long-standing open problem of proving lower bounds against Arthur{Merlin (AM) communication protocols. Our starting point is to show that|in contrast to plain randomized communication complexity|every boolean function admits an AM communication protocol where on each yes- input, the distribution of Merlin's proof leaks no information about the input and moreover, this proof is unique for each outcome of Arthur's randomness. We posit that these two properties of zero information leakage and unambiguity on yes-inputs are interesting in their own right and worthy of investigation as new avenues toward AM.Zero-information protocols (ZAM). Our basic ZAM protocol uses exponential communication for some functions, and this raises the question of whether more efficient protocols exist. We prove that all functions in the classical space-bounded complexity classes NL and L have polynomial-communication ZAM protocols. We also prove that ZAM complexity is lower bounded by conondeterministic communication complexity.Unambiguous protocols (UAM). Our most technically substantial result is a (n) lower bound on the UAM complexity of the NP-complete set-intersection function; the proof uses information complexity arguments in a new, indirect way and overcomes the zero-information barrier" described above. We also prove that in general, UAM complexity is lower bounded by the classic discrepancy bound, and we give evidence that it is not generally lower bounded by the classic corruption bound.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {113–122},
numpages = {10},
keywords = {arthur-merlin protocols, communication complexity, information complexity},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688112,
author = {Bavarian, Mohammad and Shor, Peter W.},
title = {Information Causality, Szemer\'{e}Di-Trotter and Algebraic Variants of CHSH},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688112},
doi = {10.1145/2688073.2688112},
abstract = {n this work, we consider the following family of two prover one-round games. In the CHSH_q game, two parties are given x,y in F_q uniformly at random, and each must produce an output a,b in F_q without communicating with the other. The players' objective is to maximize the probability that their outputs satisfy a+b=xy in F_q. This game was introduced by Buhrman and Massar (PRA 2005) as a large alphabet generalization of the celebrated CHSH game---which is one of the most well-studied two-prover games in quantum information theory, and which has a large number of applications to quantum cryptography and quantum complexity.Our main contributions in this paper are the first asymptotic and explicit bounds on the entangled and classical values of CHSH_q, and the realization of a rather surprising connection between CHSH_q and geometric incidence theory. On the way to these results, we also resolve a problem of Pawlowski and Winter about pairwise independent Information Causality, which, beside being interesting on its own, gives as an application a short proof of our upper bound for the entangled value of CHSH_q.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {123–132},
numpages = {10},
keywords = {point-line incidences, two player refereed games, bell inequalities and tsirelson bounds., the chsh game},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688079,
author = {Gur, Tom and Rothblum, Ron D.},
title = {Non-Interactive Proofs of Proximity},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688079},
doi = {10.1145/2688073.2688079},
abstract = {We initiate a study of non-interactive proofs of proximity. These proof-systems consist of a verifier that wishes to ascertain the validity of a given statement, using a short (sublinear length) explicitly given proof, and a sublinear number of queries to its input. Since the verifier cannot even read the entire input, we only require it to reject inputs that are far from being valid. Thus, the verifier is only assured of the proximity of the statement to correct one. Such proof-systems can be viewed as the NP (or more accurately MA) analogue of property testing.We explore both the power and limitations of non interactive proofs of proximity. We show that such proof-systems can be exponentially stronger than property testers, but are exponentially weaker than the interactive proofs of proximity studied by Rothblum, Vadhan and Wigderson (STOC 2013). In addition, we show a natural problem that has a full and (almost) tight multiplicative trade-off between the length of the proof and the verifier's query complexity. On the negative side, we also show that there exist properties for which even a linearly-long (non-interactive) proof of proximity cannot significantly reduce the query complexity.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {133–142},
numpages = {10},
keywords = {property testing, probabilistic proof systems},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688114,
author = {Applebaum, Benny and Avron, Jonathan and Brzuska, Christina},
title = {Arithmetic Cryptography: Extended Abstract},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688114},
doi = {10.1145/2688073.2688114},
abstract = {We study the possibility of computing cryptographic primitives in a fully-black-box arithmetic model over a finite field $F$. In this model, the input to a cryptographic primitive (e.g., encryption scheme) is given as a sequence of field elements, the honest parties are implemented by arithmetic circuits which make only a black-box use of the underlying field, and the adversary has a full (non-black-box) access to the field. This model captures many standard information-theoretic constructions.We prove several positive and negative results in this model for various cryptographic tasks. On the positive side, we show that, under reasonable assumptions, computational primitives like commitment schemes, public-key encryption, oblivious transfer, and general secure two-party computation can be implemented in this model. On the negative side, we prove that garbled circuits, homomorphic encryption, and secure computation with low online complexity cannot be achieved in this model. Our results reveal a qualitative difference between the standard model and the arithmetic model, and explain, in retrospect, some of the limitations of previous constructions.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {143–151},
numpages = {9},
keywords = {arithmetic circuits, cryptography, computational complexity},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688102,
author = {Chandran, Nishanth and Chongchitmate, Wutichai and Garay, Juan A. and Goldwasser, Shafi and Ostrovsky, Rafail and Zikas, Vassilis},
title = {The Hidden Graph Model: Communication Locality and Optimal Resiliency with Adaptive Faults},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688102},
doi = {10.1145/2688073.2688102},
abstract = {The vast majority of works on secure multi-party computation (MPC) assume a full communication pattern: every party exchanges messages with all the network participants over a complete network of point-to-point channels. This can be problematic in modern large scale networks, where the number of parties can be of the order of millions, as for example when computing on large distributed data.Motivated by the above observation, Boyle, Goldwasser, and Tessaro [TCC 2013] recently put forward the notion of  communication locality, namely, the total number of point-to-point channels that each party uses in the protocol, as a quality metric of MPC protocols. They proved that assuming a public-key infrastructure (PKI) and a common reference string (CRS), an MPC protocol can be constructed for computing any n-party function, with communication locality O[logc n] and round complexity O[log\'{c} n], for appropriate constants c and \'{c}. Their protocol tolerates a static (i.e., non-adaptive) adversary corrupting up to t&lt;(1/3-ε)n parties for any given constant 0 &lt; ε &lt; 1/3. These results leave open the following questions:  Can we achieve low communication locality and round complexity while tolerating adaptive adversaries?Can we achieve low communication locality with optimal resiliency t<n 2?in="" this="" work="" we="" answer="" both="" questions="" affirmatively.="" consider="" the="" boyle="" et="" al.="" model,="" where="" replace="" crs="" with="" a="" symmetric-key="" infrastructure="" (ski).="" in="" model="" give="" protocol="" communication="" locality="" and="" round="" complexity="" polylog[n]="" (similarly="" to="" al.)="" which="" tolerates="" up="" t<n="" 2="" adaptive="" corruptions,="" under="" standard="" intractability="" assumption="" for="" adaptively="" secure="" protocols,="" namely,="" existence="" of="" trapdoor="" permutations="" whose="" domain="" has="" invertible="" sampling.="" is="" done="" by="" using="" ski="" derive="" sequence="" random="" graphs="" among="" players.="" central="" new="" technique="" shows="" how="" use="" these="" emulate="" complete="" network="" rounds="" while="" preserving="" locality.="" also="" show="" remove="" setup="" at="" cost,="" however,="" increasing="" (but="" not="" complexity)="" factor="" √n.},="" booktitle="{Proceedings" 2015="" conference="" on="" innovations="" theoretical="" computer="" science},="" pages="{153–162}," numpages="{10}," keywords="{graph" theory,="" security,="" locality,="" multi-party="" computation},="" location="{Rehovot," israel},="" series="{ITCS" '15}="" }<="" div="" hidden="">
  </n>

@inproceedings{10.1145/2688073.2688105,
author = {Hubacek, Pavel and Wichs, Daniel},
title = {On the Communication Complexity of Secure Function Evaluation with Long Output},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688105},
doi = {10.1145/2688073.2688105},
abstract = {We study the communication complexity of secure function evaluation (SFE). Consider a setting where Alice has a short input χA, Bob has an input χB and we want Bob to learn some function y = f(χA, χB) with large output size. For example, Alice has a small secret decryption key, Bob has a large encrypted database and we want Bob to learn the decrypted data without learning anything else about Alice's key. In a trivial insecure protocol, Alice can just send her short input χA to Bob. However, all known SFE protocols have communication complexity that scales with size of the output y, which can potentially be much larger. Is such 'output-size dependence' inherent in SFE'Surprisingly, we show that output-size dependence can be avoided in the honest-but-curious setting. In particular, using indistinguishability obfuscation (iO) and fully homomorphic encryption (FHE), we construct the first honest-but-curious SFE protocol whose communication complexity only scales with that of the best insecure protocol for evaluating the desired function, independent of the output size. Our construction relies on a novel way of using iO via a new tool that we call a 'somewhere statistically binding (SSB) hash', and which may be of independent interest.On the negative side, we show that output-size dependence is inherent in the fully malicious setting, or even already in an honest-but-deterministic setting, where the corrupted party follows the protocol as specified but fixes its random tape to some deterministic value. Moreover, we show that even in an offline/online protocol, the communication of the online phase must have output-size dependence. This negative result uses an incompressibility argument and it generalizes several recent lower bounds for functional encryption and (reusable) garbled circuits, which follow as simple corollaries of our general theorem.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {163–172},
numpages = {10},
keywords = {communication complexity, indistinguishability obfuscation, fully homomorphic encryption, secure function evaluation, merkle hash tree},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688100,
author = {Blum, Avrim and Morgenstern, Jamie and Sharma, Ankit and Smith, Adam},
title = {Privacy-Preserving Public Information for Sequential Games},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688100},
doi = {10.1145/2688073.2688100},
abstract = {In settings with incomplete information, players can find it difficult to coordinate to find states with good social welfare. For instance, one of the main reasons behind the recent financial crisis was found to be the lack of market transparency, which made it difficult for financial firms to accurately measure the risks and returns of their investments. Although regulators may have access to firms' investment decisions, directly reporting all firms' actions raises confiden- tiality concerns for both individuals and institutions. The natural question, therefore, is whether it is possible for the regulatory agencies to publish some information that, on one hand, helps the financial firms understand the risks of their investments better, and, at the same time, preserves the privacy of their investment decisions. More generally, when can the publication of privacy-preserving information about the state of the game improve overall outcomes such as social welfare? In this paper, we explore this question in a sequential resource-sharing game where the value gained by a player on choosing a resource depends on the number of other players who have chosen that resource in the past. Without any knowledge of the actions of the past players, the social welfare attained in this game can be arbitrarily bad. We show, however, that it is possible for the players to achieve good social welfare with the help of privacy-preserving, publicly-announced information. We model the behavior of players in this imperfect information setting in two ways -- greedy and undominated strategic behaviours, and we prove guarantees about the social welfare that certain kinds of privacy-preserving information can help attain. To achieve the social welfare guarantees, we design a counter with improved privacy guarantees under continual observation. In addition to the resource-sharing game, we study the main question for other games including sequential versions of the cut, machine-scheduling and cost-sharing games, and games where the value attained by a player on a particular action is not only a function of the actions of the past players but also of the actions of the future players.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {173–180},
numpages = {8},
keywords = {privacy, game theory},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688113,
author = {Cohen, Michael B. and Lee, Yin Tat and Musco, Cameron and Musco, Christopher and Peng, Richard and Sidford, Aaron},
title = {Uniform Sampling for Matrix Approximation},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688113},
doi = {10.1145/2688073.2688113},
abstract = {Random sampling has become a critical tool in solving massive matrix problems. For linear regression, a small, manageable set of data rows can be randomly selected to approximate a tall, skinny data matrix, improving processing time significantly. For theoretical performance guarantees, each row must be sampled with probability proportional to its statistical leverage score. Unfortunately, leverage scores are difficult to compute. A simple alternative is to sample rows uniformly at random. While this often works, uniform sampling will eliminate critical row information for many natural instances.We take a fresh look at uniform sampling by examining what information it does preserve. Specifically, we show that uniform sampling yields a matrix that, in some sense, well approximates a large fraction of the original. While this weak form of approximation is not enough for solving linear regression directly, it is enough to compute a better approximation.This observation leads to simple iterative row sampling algorithms for matrix approximation that run in input-sparsity time and preserve row structure and sparsity at all intermediate steps. In addition to an improved understanding of uniform sampling, our main proof introduces a structural result of independent interest: we show that every matrix can be made to have low coherence by reweighting a small subset of its rows.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {181–190},
numpages = {10},
keywords = {randomized numerical linear algebra, leverage scores, matrix sampling, regression},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688116,
author = {Awasthi, Pranjal and Bandeira, Afonso S. and Charikar, Moses and Krishnaswamy, Ravishankar and Villar, Soledad and Ward, Rachel},
title = {Relax, No Need to Round: Integrality of Clustering Formulations},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688116},
doi = {10.1145/2688073.2688116},
abstract = {We study exact recovery conditions for convex relaxations of point cloud clustering problems, focusing on two of the most common optimization problems for unsupervised clustering: k-means and k-median clustering. Motivations for focusing on convex relaxations are: (a) they come with a certificate of optimality, and (b) they are generic tools which are relatively parameter-free, not tailored to specific assumptions over the input. More precisely, we consider the distributional setting where there are k clusters in Rm and data from each cluster consists of n points sampled from a symmetric distribution within a ball of unit radius. We ask: what is the minimal separation distance between cluster centers needed for convex relaxations to exactly recover these k clusters as the optimal integral solution? For the k-median linear programming relaxation we show a tight bound: exact recovery is obtained given arbitrarily small pairwise separation ε &gt; O between the balls. In other words, the pairwise center separation is δ &gt; 2+ε. Under the same distributional model, the k-means LP relaxation fails to recover such clusters at separation as large as δ = 4. Yet, if we enforce PSD constraints on the k-means LP, we get exact cluster recovery at separation as low as δ &gt; min{2 + √2k/m}, 2+√2 + 2/m} + ε. In contrast, common heuristics such as Lloyd's algorithm (a.k.a. the k means algorithm) can fail to recover clusters in this setting; even with arbitrarily large cluster separation, k-means++ with overseeding by any constant factor fails with high probability at exact cluster recovery. To complement the theoretical analysis, we provide an experimental study of the recovery guarantees for these various methods, and discuss several open problems which these experiments suggest.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {191–200},
numpages = {10},
keywords = {exact recovery, convex optimization, kmedians, clustering, kmeans},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688086,
author = {Chekuri, Chandra and Jayram, T.S. and Vondrak, Jan},
title = {On Multiplicative Weight Updates for Concave and Submodular Function Maximization},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688086},
doi = {10.1145/2688073.2688086},
abstract = {We develop a continuous-time framework based on multiplicative weight updates to approximately solve continuous optimization problems. The framework allows for a simple and modular analysis for a variety of problems involving convex constraints and concave or submodular objective functions. The continuous-time framework avoids the cumbersome technical details that are typically necessary in actual algorithms. We also show that the continuous-time algorithms can be converted into implementable algorithms via a straightforward discretization process. Using our framework and additional ideas we obtain significantly faster algorithms compared to previously known algorithms to maximize the multilinear relaxation of a monotone or non-monotone submodular set function subject to linear packing constraints.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {201–210},
numpages = {10},
keywords = {multiplicative weight updates, submodular function},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688104,
author = {Lattanzi, Silvio and Leonardi, Stefano and Mirrokni, Vahab and Razenshteyn, Ilya},
title = {Robust Hierarchical K-Center Clustering},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688104},
doi = {10.1145/2688073.2688104},
abstract = {One of the most popular and widely used methods for data clustering is hierarchical clustering. This clustering technique has proved useful to reveal interesting structure in the data in several applications ranging from computational biology to computer vision. Robustness is an important feature of a clustering technique if we require the clustering to be stable against small perturbations in the input data. In most applications, getting a clustering output that is robust against adversarial outliers or stochastic noise is a necessary condition for the applicability and effectiveness of the clustering technique. This is even more critical in hierarchical clustering where a small change at the bottom of the hierarchy may propagate all the way through to the top. Despite all the previous work, our theoretical understanding of robust hierarchical clustering is still limited and several hierarchical clustering algorithms are not known to satisfy such robustness properties. In this paper, we study the limits of robust hierarchical $k$-center clustering by introducing the concept of universal hierarchical clustering and provide (almost) tight lower and upper bounds for the robust hierarchical $k$-center clustering problem with outliers and variants of the stochastic clustering problem. Most importantly we present a constant-factor approximation for optimal hierarchical k-center with at most $z$ outliers using a universal set of at most O(z2) set of outliers and show that this result is tight. Moreover we show the necessity of using a universal set of outliers in order to compute an approximately optimal hierarchical $k$-center with a different set of outliers for each $k$.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {211–218},
numpages = {8},
keywords = {k-center, outliers, hierarchical clustering, robust clustering},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688082,
author = {Dinur, Irit and Goldwasser, Shafi and Lin, Huijia},
title = {The Computational Benefit of Correlated Instances},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688082},
doi = {10.1145/2688073.2688082},
abstract = {The starting point of this paper is that instances of computational problems often do not exist in isolation. Rather, multiple and correlated instances of the same problem arise naturally in the real world. The challenge is how to gain computationally from instance correlations when they exist. We will be interested in settings where significant computational gain can be made in solving a single primary instance by having access to additional auxiliary instances which are correlated to the primary instance via the solution space.We focus on Constraint Satisfaction Problems (CSPs), a very expressive class of computational problems that is well-studied both in terms of approximation algorithms and NP-hardness and in terms of average case hardness and usage for cryptography, e.g. Feige's random 3-SAT hypothesis, Goldreich's one way function proposal, learning-parity-with-noise, and others.To model correlations between instances, we consider generating processes over search problems, where a primary instance I is first selected according to some distribution D (e.g. worst case, uniform, etc); then auxiliary instances I_1,...,I_T are generated so that their underlying solutions S_1,...,S_T each are a "perturbation" of a primary solution S for I. For example, St may be obtained by the probabilistic process of flipping each bit of S with a small constant probability.We consider a variety of naturally occurring worst case and average case CSPs, and show how availability of a small number of auxiliary instances generated through a natural generating process, radically changes the complexity of solving the primary instance, from intractable to expected polynomial time. Indeed, at a high-level, knowing a {logarithmic} number of auxiliary instances enables a close polynomial time approximation of the primary solution, and when in addition the "difference vector" between the primary and the auxiliary solution is known, the primary solution can be exactly found. Furthermore, knowing even a single auxiliary instance already enables finding the exact primary solution for a large class of CSPs.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {219–228},
numpages = {10},
keywords = {correlated instances, complexity, constraint satisfaction problem},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688075,
author = {Feige, Uriel},
title = {Why Are Images Smooth?},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688075},
doi = {10.1145/2688073.2688075},
abstract = {It is a well observed phenomenon that natural images are smooth, in the sense that nearby pixels tend to have similar values. We describe a mathematical model of images that makes no assumptions on the nature of the environment that images depict. It only assumes that images can be taken at different scales (zoom levels). We provide quantitative bounds on the smoothness of a typical image in our model, as a function of the number of available scales. These bounds can serve as a baseline against which to compare the observed smoothness of natural images.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {229–236},
numpages = {8},
keywords = {natural image statistics, local repetition lemma},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688107,
author = {Schaeffer, Luke},
title = {A Physically Universal Cellular Automaton},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688107},
doi = {10.1145/2688073.2688107},
abstract = {Several cellular automata (CA) are known to be universal in the sense that one can simulate arbitrary computations (e.g., circuits or Turing machines) by carefully encoding the computational device and its input into the cells of the CA. In this paper, we consider a different kind of universality proposed by Janzing. A cellular automaton is physically universal if it is possible to implement any transformation on a finite region of the CA by initializing the complement of the region and letting the system evolve. We give the first known example of a physically universal CA, answering an open problem of Janzing and opening the way for further research in this area.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {237–246},
numpages = {10},
keywords = {universality, cellular automata, models of computation},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688096,
author = {Gilmer, Justin and Kouck\'{y}, Michal and Saks, Michael E.},
title = {A New Approach to the Sensitivity Conjecture},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688096},
doi = {10.1145/2688073.2688096},
abstract = {One of the major outstanding foundational problems about boolean functions is the sensitivity conjecture, which (in one of its many forms) asserts that the degree of a boolean function (i.e. the minimum degree of a real polynomial that interpolates the function) is bounded above by some fixed power of its sensitivity (which is the maximum vertex degree of the graph defined on the inputs where two inputs are adjacent if they differ in exactly one coordinate and their function values are different). We propose an attack on the sensitivity conjecture in terms of a novel two-player communication game. A strong enough lower bound on the cost of this game would imply the sensitivity conjecture.To investigate the problem of bounding the cost of the game, three natural (stronger) variants of the question are considered. For two of these variants, protocols are presented that show that the hoped for lower bound does not hold. These protocols satisfy a certain monotonicity property, and (in contrast to the situation for the two variants) we show that the cost of any monotone protocol satisfies a strong lower bound.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {247–254},
numpages = {8},
keywords = {decision trees, sensitivity conjecture, communication complexity, sensitivity, degree of boolean functions},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688076,
author = {Heilman, Steven and Mossel, Elchanan and Neeman, Joe},
title = {Standard Simplices and Pluralities Are Not the Most Noise Stable},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688076},
doi = {10.1145/2688073.2688076},
abstract = {The Standard Simplex Conjecture and the Plurality is Stablest Conjecture are two conjectures stating that certain partitions are optimal with respect to Gaussian and discretenoise stability respectively. These two conjectures are natural generalizations of the Gaussian noise stability result by Borell (1985) and the Majority is Stablest Theorem (2004). Here we show that the standard simplex is not the most stable partition in Gaussian space and that Plurality is not the most stable low inuence partition in discrete space for every number of parts k &gt; 3, for every value ρ ≠ of the noise and for every prescribed measures for the different parts as long as they are not all equal to 1/k. Our results do not contradict the original statements of the Plurality is Stablest and Standard Simplex Conjectures concerning partitions into sets of equal measure. However, they indicate that if these conjectures are true, their veracity and their proofs will crucially rely on assuming that the sets are of equal measures, in stark contrast to Borell's result, the Majority is Stablest Theorem and many other results in isoperimetric theory.In other words, the optimal partitions for noise stability are of a different nature than the ones considered for partitions into three parts in isoperimetric theory. In the latter case, the standard simplex is the partition of the plane into three sets of smallest Gaussian perimeter, where the sets are restricted to have Gaussian measures a1, a2, a3 &gt; 0 respectively, with a1 + a2 + a3 = 1 and |ai --1/3| &lt; .04 for all i ∈ {1, 2, 3}. Thus, we now know that the extension of noise stability theory from two to three or more parts is very much different than the extension of isoperimetric theory from two to three or more parts. Moreover, all existing proofs which optimize noise stability of two sets must fail for more than three sets, since these proofs rely on the fact that a half-space optimizes noise stability with respect to any measure restriction. Given our results it is natural to ask for (conjectured) partitions achieving the optimum noise stability.The main new ingredient in our work shows that the Ornstein-Uhlenbeck operator applied to the indicator function of a simplicial cone becomes holomorphic when restricted to certain lines. This holomorphicity condition, when combined with a first variation argument (i.e. an innite dimensional perturbative argument of the rst order), then shows that any simplicial cone can be perturbed in a volume-preserving manner to improve its noise stability. Such a holomorphicity argument seems unavailable for the is operimetric problem, since this argument uses the inherent non-locality of the Ornstein-Uhlenbeck semigroup.A full version of the paper is available at arXiv:1403.0885},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {255},
numpages = {1},
keywords = {plurality, noise stability, standard simplex},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688099,
author = {Canonne, Clement Louis and Guruswami, Venkatesan and Meka, Raghu and Sudan, Madhu},
title = {Communication with Imperfectly Shared Randomness},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688099},
doi = {10.1145/2688073.2688099},
abstract = {The communication complexity of many fundamental problems reduces greatly when the communicating parties share randomness that is independent of the inputs to the communication task. Natural communication processes (say between humans) however often involve large amounts of shared correlations among the communicating players, but rarely allow for perfect sharing of randomness. Can the communication complexity benefit from shared correlations as well as it does from shared randomness? This question was considered mainly in the context of simultaneous communication by Bavarian et al. [1]. In this work we study this problem in the standard interactive setting and give some general results. In particular, we show that every problem with communication complexity of k bits with perfectly shared randomness has a protocol using imperfectly shared randomness with complexity 2Ω(k) bits. We also show that this is best possible by exhibiting a promise problem with complexity k bits with perfectly shared randomness which requires 2Ω(k) bits when the randomness is imperfectly shared. Along the way we also highlight some other basic problems such as compression, and agreement distillation, where shared randomness plays a central role and analyze the complexity of these problems in the imperfectly shared randomness model.The technical highlight of this work is the lower bound that goes into the result showing the tightness of our general connection. This result builds on the intuition that communication with imperfectly shared randomness needs to be less sensitive to its random inputs than communication with perfectly shared randomness. The formal proof invokes results about the small-set expansion of the noisy hypercube and an invariance principle to convert this intuition to a proof, thus giving a new application domain for these fundamental results.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {257–262},
numpages = {6},
keywords = {invariance principle, randomness, communication complexity},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688115,
author = {Chapman, Brynmor and Williams, Ryan},
title = {The Circuit-Input Game, Natural Proofs, and Testing Circuits With Data},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688115},
doi = {10.1145/2688073.2688115},
abstract = {We revisit a natural zero-sum game from several prior works. A circuit player, armed with a collection of Boolean circuits, wants to compute a function $f$ with one (or some) of its circuits. An input player has a collection of inputs, and wants to find one (or some) inputs on which the circuit player cannot compute f. Several results are known on the existence of small-support strategies for zero-sum games, in particular the above circuit-input game. We give two new applications of these classical results to circuit complexity:Natural properties useful against self-checking circuits are equivalent to circuit lower bounds. We show how the Natural Proofs barrier may be potentially sidestepped, by simply focusing on analyzing circuits that check their answers. Slightly more precisely, we prove NP not concurrent P/poly if and only if there are natural properties that (a) accept the SAT function and (b) are useful against polynomial-size circuits that never err when they report SAT. (Note, via self-reducibility, any small circuit can be turned into one of this kind!) The proof is very general; similar equivalences hold for other lower bound problems. Our message is that one should search for lower bound methods that are designed to succeed (only) against circuits with "one-sided error."Circuit Complexity versus Testing Circuits With Data. We reconsider the problem of program testing, which we formalize as deciding if a given circuit computes a (fixed) function f. We define the "data complexity" of f (as a function of circuit size s) to be the minimum cardinality of a test suite of inputs: a set of input/output pairs necessary and sufficient for deciding if any given circuit of size at most s computes a slice of f. (This is a "gray-box testing" problem, where the value s is side information.) We prove that designing small test suites for f is equivalent to proving circuit lower bounds on f: the data complexity of testing f is "small" if and only if the circuit complexity of f is "large." Therefore, circuit lower bounds may be constructively viewed as data design circuit-testing problems.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {263–270},
numpages = {8},
keywords = {gray-box testing, complexity theory, program testing, circuit-input game, natural proofs, circuit complexity},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688101,
author = {Feige, Uriel and Jozeph, Shlomo},
title = {Separation between Estimation and Approximation},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688101},
doi = {10.1145/2688073.2688101},
abstract = {We show (under standard assumptions) that there are textsf{NP} optimization problems for which estimation is easier than approximation. Namely, one can estimate the value of the optimal solution within a ratio of $rho$, but it is difficult to find a solution whose value is within $rho$ of optimal. As an important special case, we show that there are linear programming relaxations for which no polynomial time rounding technique matches the integrality gap of the linear program.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {271–276},
numpages = {6},
keywords = {tfnp, integrality gap, rounding, linear programming},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688090,
author = {Bhowmick, Abhishek and Gabizon, Ariel and Le, Th\'{a}i Hoang and Zuckerman, David},
title = {Deterministic Extractors for Additive Sources: Extended Abstract},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688090},
doi = {10.1145/2688073.2688090},
abstract = {We propose a new model of a weakly random source that admits randomness extraction. Our model of additive sources includes such natural sources as uniform distributions on arithmetic progressions (APs), generalized arithmetic progressions (GAPs), and Bohr sets, each of which generalizes affine sources. We give an explicit extractor for additive sources with linear min-entropy over both Zp and Zn/p, for large prime p, although our results over Zn/p require that the source further satisfy a list-decodability condition. As a corollary, we obtain explicit extractors for APs, GAPs, and Bohr sources with linear min-entropy, although again our results over Zn/p require the list-decodability condition.We further explore special cases of additive sources. We improve previous constructions of line sources (affine sources of dimension 1), requiring a field of size linear in n, rather than Ω(n2) by Gabizon and Raz. This beats the non-explicit bound of Θ(n log n) obtained by the probabilistic method. We then generalize this result to APs and GAPs.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {277–286},
numpages = {10},
keywords = {small doubling sets, exponential sums, deterministic extractors, additive combinatorics, bohr sets, arithmetic progressions},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688092,
author = {Rudra, Atri and Wootters, Mary},
title = {It'll Probably Work Out: Improved List-Decoding Through Random Operations},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688092},
doi = {10.1145/2688073.2688092},
abstract = {In this work, we introduce a framework to study the effect of random operations on the combinatorial list decodability of a code. The operations we consider correspond to row and column operations on the matrix obtained from the code by stacking the codewords together as columns. This captures many natural transformations on codes, such as puncturing, folding, and taking subcodes; we show that many such operations can improve the list-decoding properties of a code. There are two main points to this. First, our goal is to advance our (combinatorial) understanding of list-decodability, by understanding what structure (or lack thereof) is necessary to obtain it. Second, we use our more general results to obtain a few interesting corollaries for list decoding.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {287–296},
numpages = {10},
keywords = {random codes, list decoding},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688098,
author = {Br\^{a}nzei, Simina and Procaccia, Ariel D.},
title = {Verifiably Truthful Mechanisms},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688098},
doi = {10.1145/2688073.2688098},
abstract = {It is typically expected that if a mechanism is truthful, then the agents would, indeed, truthfully report their private information. But why would an agent believe that the mechanism is truthful? We wish to design truthful mechanisms that are simple, that is whose truthfulness can be verified efficiently (in the computational sense). Our approach involves three steps: (i) specifying the structure of mechanisms, (ii) constructing a verification algorithm, and (iii) measuring the quality of verifiably truthful mechanisms. We demonstrate this approach using a case study: approximate mechanism design without money for facility location.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {297–306},
numpages = {10},
keywords = {mechanism design, decision trees, approximation, verification, facility location},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688081,
author = {Babaioff, Moshe and Feldman, Moran and Tennenholtz, Moshe},
title = {Mechanism Design with Strategic Mediators},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688081},
doi = {10.1145/2688073.2688081},
abstract = {We consider the problem of designing mechanisms that interact with strategic agents through strategic intermediaries (or mediators), and investigate the cost to society due to the mediators' strategic behavior. Selfish agents with private information are each associated with exactly one strategic mediator, and can interact with the mechanism exclusively through that mediator. Each mediator aims to optimize the combined utility of his agents, while the mechanism aims to optimize the combined utility of all agents. We focus on the problem of facility location on a metric induced by a publicly known tree. With non-strategic mediators, there is a dominant strategy mechanism that is optimal. We show that when both agents and mediators act strategically, there is no dominant strategy mechanism that achieves any approximation. We, thus, slightly relax the incentive constraints, and define the notion of a two-sided incentive compatible mechanism. We show that the 3-competitive deterministic mechanism suggested by Procaccia and Tennenholtz (2009) and Dekel et al. (2010) for lines extends naturally to trees, and is still 3-competitive as well as two-sided incentive compatible. This is essentially the best possible. We then show that by allowing randomization one can construct a 2-competitive randomized mechanism that is two-sided incentive compatible, and this is also essentially tight. This result also closes a gap left in the work of Procaccia and Tennenholtz (2009) and Lu et al. (2009) for the simpler problem of designing strategy-proof mechanisms for weighted agents with no mediators on a line, while extending to the more general model of trees. We also investigate a further generalization of the above setting where there are multiple levels of mediators.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {307–316},
numpages = {10},
keywords = {facility location, mechanism design, mediators},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688106,
author = {Cummings, Rachel and Ligett, Katrina and Roth, Aaron and Wu, Zhiwei Steven and Ziani, Juba},
title = {Accuracy for Sale: Aggregating Data with a Variance Constraint},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688106},
doi = {10.1145/2688073.2688106},
abstract = {We consider the problem of a data analyst who may purchase an unbiased estimate of some statistic from multiple data providers. From each provider i, the analyst has a choice: she may purchase an estimate from that provider that has variance chosen from a finite menu of options. Each level of variance has a cost associated with it, reported (possibly strategically) by the data provider. The analyst wants to choose the minimum cost set of variance levels, one from each provider, that will let her combine her purchased estimators into an aggregate estimator that has variance at most some fixed desired level. Moreover, she wants to do so in such a way that incentivizes the data providers to truthfully report their costs to the mechanism. We give a dominant strategy truthful solution to this problem that yields an estimator that has optimal expected cost, and violates the variance constraint by at most an additive term that tends to zero as the number of data providers grows large.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {317–324},
numpages = {8},
keywords = {mechanism design, vcg mechanism, buying data},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688083,
author = {Chen, Jing and Micali, Silvio and Pass, Rafael},
title = {Better Outcomes from More Rationality},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688083},
doi = {10.1145/2688073.2688083},
abstract = {Mechanism design enables a social planner to obtain a desired outcome by leveraging the players' rationality and their beliefs. It is thus a fundamental, yet unproven, intuition that the higher the level of rationality of the players, the better the set of obtainable outcomes.In this paper we prove this fundamental intuition for players with possibilistic beliefs, the traditional model of epistemic game theory. Specifically,We define a sequence of monotonically increasing revenue benchmarks for single-good auctions, G0 &lt; G1 &lt; G2, where each Gi is defined over the players' beliefs and G0 is the second-highest valuation (i.e., the revenue benchmark achieved by the second-price mechanism).We (1) construct a single, interim individually rational, auction mechanism that, without any clue about the rationality level of the players, guarantees revenue Gk if all players have rationality levels &gt; k+1, and (2) prove that no such mechanism can guarantee revenue even close to Gk when at least two players are at most level-k rational.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {325},
numpages = {1},
keywords = {incomplete information, single-good auctions, epistemic game theory},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688078,
author = {David, Roee and Dinur, Irit and Goldenberg, Elazar and Kindler, Guy and Shinkar, Igor},
title = {Direct Sum Testing},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688078},
doi = {10.1145/2688073.2688078},
abstract = {The k-fold direct sum encoding of a string α ∈ --0,1}n is a function fα that takes as input sets S ⊆ [n] of size k and outputs fα (S) = ∑i ∈ S αi (mod 2. In this paper we prove a Direct Sum Testing theorem. We describe a three query test that accepts with probability one any function of the form fα for some α, and rejects with probability Ω(ε) functions f that are ε being a direct sum encoding.This theorem has a couple of additional guises: Linearity testing: By identifying the subsets of [n] with vectors in --0,1}ne natural way, our result can be thought of as a linearity testing theorem for functions whose domain is restricted to the k'th layer of the hypercube (i.e. the set of n-bit strings with Hamming weight k).Tensor power testing: By moving to --1,1 notation, the direct sum encoding is equivalent (up to a difference that is negligible when k « √n) to a tensor power. Thus our theorem implies a three query test for deciding if a given tensor α ∈ --- 1,1}nk is a tensor power of a single dimensional vector α ∈ ----1,1}n, i.e. whether there is some α such that f = αk.We also provide a four query test for checking if a given ±1 matrix has rank 1. Our test naturally extends the linearity test of Blum, Luby, and Rubinfeld (STOC '90). Our analysis proceeds by first handling the k=n/2 case, and then reducing this case to the general k &lt; n/2 case, using a recent direct product testing theorem of Dinur and Steurer (CCC '2014). The k &lt; n/2 case is proven via a new proof for linearity testing on the hypercube, which we extend to the restricted domain of the n/2-th layer of the hypercube.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {327–336},
numpages = {10},
keywords = {linearity testing, direct sum, property testing},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688080,
author = {Goldreich, Oded and Ron, Dana},
title = {On Sample-Based Testers},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688080},
doi = {10.1145/2688073.2688080},
abstract = {The standard definition of property testing endows the tester with the ability to make arbitrary queries to "elements" of the tested object. In contrast, sample-based testers only obtain independently distributed elements (a.k.a. labeled samples) of the tested object. While sample-based testers were defined by Goldreich, Goldwasser, and Ron  JACM 1998), with few exceptions, most research in property testing is focused on query-based testers. In this work, we advance the study of sample-based property testers by providing several general positive results as well as by revealing relations between variants of this testing model. In particular: We show that certain types of query-based testers yield sample-based testers of sublinear sample complexity. For example, this holds for a natural class of proximity oblivious testers.We study the relation between distribution-free sample-based testers and one-sided error sample-based testers w.r.t. the uniform distribution.While most of this work ignores the time complexity of testing, one part of it does focus on this aspect. The main result in this part is a sublinear-time sample-based tester in the dense graphs model for k-Colorability, for any k &gt; 2.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {337–345},
numpages = {9},
keywords = {property testing, sampling, sublinear algorithms},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688095,
author = {Waggoner, Bo},
title = {<i>L<sub>p</sub></i> Testing and Learning of Discrete Distributions},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688095},
doi = {10.1145/2688073.2688095},
abstract = {The classic problems of testing uniformity of and learning a discrete distribution, given access to independent samples from it, are examined under general lp metrics. The intuitions and results often contrast with the classic l1 case. For p &gt; 1, we can learn and test with a number of samples that is independent of the support size of the distribution: For 1 &lt; p &lt; 2, with a lp distance parameter ε, O(\"{u}√1/εq) samples suffice for testing uniformity and O(1/εq) samples suffice for learning, where q=p/(p-1) is the conjugate of p. These bounds are tight precisely when the support size n of the distribution exceeds 1/εq, which seems to act as an upper bound on the "apparent" support size.For some lp metrics, uniformity testing becomes easier over larger supports: a 6-sided die requires fewer trials to test for fairness than a 2-sided coin, and a card-shuffler requires fewer trials than the die. In fact, this inverse dependence on support size holds if and only if p &gt; 4/3. The uniformity testing algorithm simply thresholds the number of "collisions" or "coincidences" and has an optimal sample complexity up to constant factors for all 1 ≤ p ≤ 2. Another algorithm gives order-optimal sample complexity for l∞ uniformity testing. Meanwhile, the most natural learning algorithm is shown to have order-optimal sample complexity for all lp metrics.The author thanks C\'{e}ment Canonne for discussions and contributions to this work.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {347–356},
numpages = {10},
keywords = {property testing, lp norms, uniformity testing, learning, discrete distributions},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688084,
author = {Haviv, Ishay and Xie, Ning},
title = {Sunflowers and Testing Triangle-Freeness of Functions},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688084},
doi = {10.1145/2688073.2688084},
abstract = {A function f : Fn/2 → {0,1} is triangle-free if there are no x1, x2, x3 ∈ Fn/2 satisfying x1 + x2 + x3 --0 and f(x1) -- f(x2) -- f(x3) -- 1. In testing triangle freeness, the goal is to distinguish with high probability triangle-free functions from those which are ε-far from being triangle-free. It was shown by Green that the query complexity of the canonical tester for the problem is upper bounded by a function that depends only on ε (GAFA, 2005), however the best known upper bound is a tower type function of 1/ε. The best known lower bound on the query complexity of the canonical tester is 1/ε13.239 (Fu and Kleinberg, RANDOM, 2014).In this work we introduce a new approach to proving lower bounds on the query complexity of triangle-freeness. We relate the problem to combinatorial questions on collections of vectors in Zn/D and to sunflower conjectures studied by Alon, Shpilka, and Umans (Comput. Complex., 2013). The relations yield that a refutation of the Weak Sunflower Conjecture over Z4 implies a super-polynomial lower bound on the query complexity of the canonical tester for triangle-freeness. Our results are extended to testing k-cycle-freeness of functions with domain Fn/p for every k &gt; 3 and a prime p. In addition, we generalize the lower bound of Fu and Kleinberg to k-cycle-freeness for k &gt; 4 by generalizing the construction of uniquely solvable puzzles due to Coppersmith and Winograd (J. Symbolic Comput., 1990).},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {357–366},
numpages = {10},
keywords = {sunflowers, property testing, lower bounds, triangle-freeness},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688093,
author = {Kogan, Dmitry and Krauthgamer, Robert},
title = {Sketching Cuts in Graphs and Hypergraphs},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688093},
doi = {10.1145/2688073.2688093},
abstract = {Sketching and streaming algorithms are in the forefront of current research directions for cut problems in graphs. In the streaming model, we show that (1--ε)-approximation for Max-Cut must use n{1-O(ε)} space; moreover, beating 4/5-approximation requires polynomial space. For the sketching model, we show that every r-uniform hypergraph admits a (1+ ε)-cut-sparsifier (i.e., a weighted subhypergraph that approximately preserves all the cuts) with O(ε-2n(r+log n)) edges. We also make first steps towards sketching general CSPs (Constraint Satisfaction Problems).},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {367–376},
numpages = {10},
keywords = {sparsifiers, streaming, hypergraphs, max-cut, sketching},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688103,
author = {Bodwin, Gregory and Williams, Virginia Vassilevska},
title = {Very Sparse Additive Spanners and Emulators},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688103},
doi = {10.1145/2688073.2688103},
abstract = {We obtain new upper bounds on the additive distortion for graph emulators and spanners on relatively few edges. We introduce a new subroutine called "strip creation," and we combine this subroutine with several other ideas to obtain the following results: Every graph has a spanner on O(n1+ε) edges with \~{O}(n{1/2 - ε/2) additive distortion, for arbitrary ε ∈ [0,1].Every graph has an emulator on \~{O}(n1+ε) edges with \~{O}(n{1/3 -- 2ε/3) additive distortion whenever ε ∈ [0, 1/5].Every graph has a spanner on \~{O}(n1 + ε) edges with \~{O}(n2/3 -- 5ε/3) additive distortion whenever ε ∈ [0, 1/4].Our first spanner has the new best known asymptotic edge-error tradeoff for additive spanners whenever ε ∈ [0, 1/7]. Our second spanner has the new best tradeoff whenever ε ∈ [1/7, 3/17]. Our emulator has the new best asymptotic edge-error tradeoff whenever ε ∈ [0, 1/5].},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {377–382},
numpages = {6},
keywords = {graph algorithms, graph emulators, graph spanners},
location = {Rehovot, Israel},
series = {ITCS '15}
}

@inproceedings{10.1145/2688073.2688085,
author = {Black, Timothy},
title = {Monotone Properties of K-Uniform Hypergraphs Are Weakly Evasive},
year = {2015},
isbn = {9781450333337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688073.2688085},
doi = {10.1145/2688073.2688085},
abstract = {A boolean function in n variables is weakly evasive if its decision-tree complexity is Ω(n). By k-graphs we mean k-uniform hypergraphs. A k-graph property} on v vertices is a boolean function on n = v/k variables corresponding to the k-subsets of a v-set that is invariant under the v! permutations of the v-set (isomorphisms of k-graphs). Rivest and Vuillemin (1976) proved that all non-constant monotone graph properties (k=2) are weakly evasive, confirming a conjecture of Aanderaa and Rosenberg (1973). Kulkarni, Qiao, and Sun (2013) proved the analogous result for 3-graphs. We extend these results to k-graphs for every fixed k. From this, we show that monotone boolean functions invariant under the action of a large primitive group are weakly evasive. While KQS (2013) employ the powerful topological approach of Kahn, Saks, and Sturtevant (1984) combined with heavy number theory, our argument is elementary and self-contained (modulo some basic group theory). Inspired by the outline of the KQS approach, we formalize the general framework of "orbit augmentation sequences" of sets with group actions. We show that a parameter of such sequences, called the "spacing," is a lower bound on the decision-tree complexity for any nontrivial monotone property that is Γ-invariant for all groups Γ involved in the orbit augmentation sequence, assuming all those groups are p-groups. We develop operations on such sequences such as composition and direct product which will provide helpful machinery for our applications. We apply this general technique to k-graphs via certain liftings of k-graphs with wreath product action of p-groups.},
booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
pages = {383–391},
numpages = {9},
keywords = {monotone hypergraph properties., decision-tree complexity, evasiveness conjecture},
location = {Rehovot, Israel},
series = {ITCS '15}
}

