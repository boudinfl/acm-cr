@inproceedings{10.1145/2090236.2090237,
author = {Drucker, Andrew},
title = {High-Confidence Predictions under Adversarial Uncertainty},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090237},
doi = {10.1145/2090236.2090237},
abstract = {We study the setting in which the bits of an unknown infinite binary sequence x are revealed sequentially to an observer. We show that very limited assumptions about x allow one to make successful predictions about unseen bits of x. First, we study the problem of successfully predicting a single 0 from among the bits of x. In our model we have only one chance to make a prediction, but may do so at a time of our choosing. This model is applicable to a variety of situations in which we want to perform an action of fixed duration, and need to predict a "safe" time-interval to perform it.Letting Nt denote the number of 1s among the first t bits of x, we say that x is "ε-weakly sparse" if liminf (Nt/t) ≤ ε. Our main result is a randomized algorithm that, given any ε-weakly sparse sequence x, predicts a 0 of x with success probability as close as desired to 1 -- ε. Thus we can perform this task with essentially the same success probability as under the much stronger assumption that each bit of x takes the value 1 independently with probability ε.We apply this result to show how to successfully predict a bit (0 or 1) under a broad class of possible assumptions on the sequence x. The assumptions are stated in terms of the behavior of a finite automaton M reading the bits of x. We also propose and solve a variant of the well-studied "ignorant forecasting" problem. For every ε &gt; 0, we give a randomized forecasting algorithm Sε that, given sequential access to a binary sequence x, makes a prediction of the form: "A p fraction of the next N bits will be 1s." (The algorithm gets to choose p, N, and the time of the prediction.) For any fixed sequence x, the forecast fraction p is accurate to within ±ε with probability 1 − ε.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {1–10},
numpages = {10},
keywords = {prediction, ignorant forecasting, binary sequences},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090238,
author = {Kanade, Varun and Steinke, Thomas},
title = {Learning Hurdles for Sleeping Experts},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090238},
doi = {10.1145/2090236.2090238},
abstract = {We study the online decision problem where the set of available actions varies over time, also called the sleeping experts problem. We consider the setting where the performance comparison is made with respect to the best ordering of actions in hindsight. In this paper, both the payoff function and the availability of actions is adversarial. Kleinberg et al. (2008) gave a computationally efficient no-regret algorithm in the setting where payoffs are stochastic. Kanade et al. (2009) gave an efficient no-regret algorithm in the setting where action availability is stochastic.However, the question of whether there exists a computationally efficient no-regret algorithm in the adversarial setting was posed as an open problem by Kleinberg et al. (2008). We show that such an algorithm would imply an algorithm for PAC learning DNF, a long standing important open problem. We also show that a related problem, the gambling problem, posed as an open problem by Abernethy (2010) is related to agnostically learning halfspaces, albeit under restricted distributions.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {11–18},
numpages = {8},
keywords = {online learning, lower bounds, sleeping experts},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090239,
author = {Dvir, Zeev and Rao, Anup and Wigderson, Avi and Yehudayoff, Amir},
title = {Restriction Access},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090239},
doi = {10.1145/2090236.2090239},
abstract = {We introduce a notion of non-black-box access to computational devices (such as circuits, formulas, decision trees, and so forth) that we call restriction access. Restrictions are partial assignments to input variables. Each restriction simplifies the device, and yields a new device for the restricted function on the unassigned variables. On one extreme, full restrictions (assigning all variables) correspond to evaluating the device on a complete input, yielding the result of the computation on that input, which is the same as standard black-box access. On the other extreme, empty restrictions (assigning no variables) yield a full description of the original device. We explore the grey-scale of possibilities in the middle.Focusing on learning theory, we show that restriction access provides a setting in which one can obtain positive results for problems that have resisted attack in the black-box access model. We introduce a PAC-learning version of restriction access, and show that one can efficiently learn both decision trees and DNF formulas in this model. These two classes are not known to be learnable in the PAC model with black-box access.Our DNF learning algorithm is obtained by a reduction to a general learning problem we call population recovery, in which random samples from an unknown distribution become available only after a random part of each is obliterated. Specifically, assume that every member of an unknown population is described by a vector of values. The algorithm has access to random samples, each of which is a random member of the population, whose values are given only on a random subset of the attributes. Analyzing our efficient algorithm to fully recover the unknown population calls for understanding another basic problem of independent interest: "robust local inversion" of matrices. The population recovery algorithm and construction of robust local inverses for some families of matrices are the main technical contributions of the paper.We also discuss other possible variants of restriction access, in which the values to restricted variables, as well as the subset of free (unassigned) variables, are generated deterministically or randomly, in friendly or adversarial fashions. We discuss how these models may naturally suit situations in computational learning, computational biology, automated proofs, cryptography and complexity theory.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {19–33},
numpages = {15},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090240,
author = {Chiesa, Alessandro and Micali, Silvio and Zhu, Zeyuan Allen},
title = {Mechanism Design with Approximate Valuations},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090240},
doi = {10.1145/2090236.2090240},
abstract = {We study single-good auctions when each player knows his own valuation only within a constant multiplicative factor δ ε (0, 1), known to the mechanism designer. The classical notions of implementation in dominant strategies and implementation in undominated strategies are naturally extended to this setting, but their power is vastly different.On the negative side, we prove that no dominant-strategy mechanism can guarantee social welfare that is significantly better than that achievable by assigning the good to a random player.On the positive side, we provide tight upper and lower bounds for the fraction of the maximum social welfare achievable in undominated strategies, whether deterministically or probabilistically.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {34–38},
numpages = {5},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090241,
author = {Zhang, Shengyu},
title = {Quantum Strategic Game Theory},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090241},
doi = {10.1145/2090236.2090241},
abstract = {We propose a simple yet rich model to extend strategic games to the quantum setting, in which we define quantum Nash and correlated equilibria and study the relations between classical and quantum equilibria. Unlike all previous work that focused on qualitative questions on specific games of very small sizes, we quantitatively address the following fundamental question for general games of growing sizes:How much "advantage" can playing quantum strategies provide, if any?Two measures of the advantage are studied.1. Since game mainly is about each player trying to maximize individual payoff, a natural measure is the increase of payoff by playing quantum strategies. We consider natural mappings between classical and quantum states, and study how well those mappings preserve equilibrium properties. Among other results, we exhibit a correlated equilibrium p whose quantum superposition counterpart [EQUATION] is far from being a quantum correlated equilibrium; actually a player can increase her payoff from almost 0 to almost 1 in a [0, 1]-normalized game. We achieve this by a tensor product construction on carefully designed base cases. The result can also be interpreted as in Meyer's comparison [47]: In a state no classical player can gain, one player using quantum computers has an huge advantage than continuing to play classically.2. Another measure is the hardness of generating correlated equilibria, for which we propose to study correlation complexity, a new complexity measure for correlation generation. We show that there are n-bit correlated equilibria which can be generated by only one EPR pair followed by local operation (without communication), but need at least log2(n) classical shared random bits plus communication. The randomized lower bound can be improved to n, the best possible, assuming (even a much weaker version of) a recent conjecture in linear algebra. We believe that the correlation complexity, as a complexity-theoretical counterpart of the celebrated Bell's inequality, has independent interest in both physics and computational complexity theory and deserves more explorations.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {39–59},
numpages = {21},
keywords = {quantum entanglement, strategic game theory, nash equilibrium, quantum computation, correlated equilibrium},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090242,
author = {Leme, Renato Paes and Syrgkanis, Vasilis and Tardos, \'{E}va},
title = {The Curse of Simultaneity},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090242},
doi = {10.1145/2090236.2090242},
abstract = {Typical models of strategic interactions in computer science use simultaneous move games. However, in applications simultaneity is often hard or impossible to achieve. In this paper, we study the robustness of the Nash Equilibrium when the assumption of simultaneity is dropped. In particular we propose studying the sequential price of anarchy: the quality of outcomes of sequential versions of games whose simultaneous counterparts are prototypical in algorithmic game theory. We study different classes of games with high price of anarchy, and show that the subgame perfect equilibrium of their sequential version is a much more natural prediction, ruling out unreasonable equilibria, and leading to much better quality solutions.We consider three examples of such games: Cost Sharing Games, Unrelated Machine Scheduling Games and Consensus Games. For Machine Cost Sharing Games, the sequential price of anarchy is at most O(log(n)), an exponential improvement of the O(n) price of anarchy of their simultaneous counterparts. Further, the subgame perfect equilibrium can be computed by a polynomial time greedy algorithm, and is independent of the order the players arrive. For Unrelated Machine Scheduling Games we show that the sequential price of anarchy is bounded as a function of the number of jobs n and machines m (by at most O(m2n)), while in the simultaneous version the price of anarchy is unbounded even for two players and two machines. For Consensus Games we observe that the optimal outcome for generic weights is the unique equilibrium that arises in the sequential game. We also study the related Cut Games, where we show that the sequential price of anarchy is at most 4. In addition we study the complexity of finding the subgame perfect equilibrium outcome in these games.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {60–67},
numpages = {8},
keywords = {extensive form games, games, subgame perfect equilibrium, price of anarchy},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090243,
author = {Dolev, Danny and Feitelson, Dror G. and Halpern, Joseph Y. and Kupferman, Raz and Linial, Nathan},
title = {No Justified Complaints: On Fair Sharing of Multiple Resources},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090243},
doi = {10.1145/2090236.2090243},
abstract = {Fair allocation has been studied intensively in both economics and computer science. This subject has aroused renewed interest with the advent of virtualization and cloud computing. Prior work has typically focused on mechanisms for fair sharing of a single resource. We consider a variant where each user is entitled to a certain fraction of the system's resources, and has a fixed usage profile describing how much he would want from each resource. We provide a new definition for the simultaneous fair allocation of multiple continuously-divisible resources that we call bottleneck-based fairness (BBF). Roughly speaking, an allocation of resources is considered fair if every user either gets all the resources he wishes for, or else gets at least his entitlement on some bottleneck resource, and therefore cannot complain about not receiving more. We show that BBF has several desirable properties such as providing an incentive for sharing, and also promotes high overall utilization of resources; we also compare BBF carefully to another notion of fairness proposed recently, dominant resource fairness.Our main technical result is that a fair allocation can be found for every combination of user requests and entitlements. The allocation profile of each user is proportionate to the user's profile of requests. The main problem is that the bottleneck resources are not known in advance, and indeed one can find instances that allow different solutions with different sets of bottlenecks. Therefore known techniques such as linear programming do not seem to work. Our proof uses tools from the theory of ordinary differential equations, showing the existence of a sequence of points that converge upon a solution. It is constructive and provides a practical method to compute the allocations numerically.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {68–75},
numpages = {8},
keywords = {bottleneck, fair share, resource allocation},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090244,
author = {Ishai, Yuval and Kushilevitz, Eyal and Paskin-Cherniavsky, Anat},
title = {From Randomizing Polynomials to Parallel Algorithms},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090244},
doi = {10.1145/2090236.2090244},
abstract = {Randomizing polynomials represent a function f(x) by a low-degree randomized mapping p(x, r) over a finite field F such that, for any input x, the output distribution of p(x, r) depends only on the value of f(x). We study the class of functions f which admit an efficient representation by constant-degree randomizing polynomials. It is known that this class contains NC1 as well as log-space classes contained in NC2. Whether it contains all polynomial-time computable functions is a wide open question. A positive answer would have major and unexpected consequences, including the existence of efficient constant-round multiparty protocols with unconditional security, and the equivalence of (polynomial-time) cryptography and cryptography in NC0.We obtain evidence for the limited power of randomizing polynomials by showing that a useful subclass of constant-degree randomizing polynomials cannot efficiently capture functions beyond NC. Concretely, we consider randomizing polynomials over fields F of a small characteristic in which each monomial has degree (at most) 2 in the random inputs r and constant degree in x. This subclass captures most constructions of randomizing polynomials from the literature. Our main result is that all functions f which can be efficiently represented by such randomizing polynomials over fields of a small characteristic are in non-uniform NC. (The same holds over arbitrary fields given a quadratic residuosity oracle.) This result is obtained in two steps: (1) we observe that computing f as above reduces to counting roots of degree-2 multivariate polynomials; (2) we design parallel algorithms for the latter problem. These parallel root counting algorithms may be of independent interest.On the flip side, our main result provides an avenue for obtaining new parallel algorithms via the construction of randomizing polynomials. This gives an unexpected application of cryptography to algorithm design. We provide several examples for the potential usefulness of this approach.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {76–89},
numpages = {14},
keywords = {secure computation, cryptography, randomizing polynomials, parallel algorithms},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090245,
author = {Cormode, Graham and Mitzenmacher, Michael and Thaler, Justin},
title = {Practical Verified Computation with Streaming Interactive Proofs},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090245},
doi = {10.1145/2090236.2090245},
abstract = {When delegating computation to a service provider, as in the cloud computing paradigm, we seek some reassurance that the output is correct and complete. Yet recomputing the output as a check is inefficient and expensive, and it may not even be feasible to store all the data locally. We are therefore interested in what can be validated by a streaming (sublinear space) user, who cannot store the full input, or perform the full computation herself. Our aim in this work is to advance a recent line of work on "proof systems" in which the service provider proves the correctness of its output to a user. The goal is to minimize the time and space costs of both parties in generating and checking the proof. Only very recently have there been attempts to implement such proof systems, and thus far these have been quite limited in functionality.Here, our approach is two-fold. First, we describe a carefully chosen instantiation of one of the most efficient general-purpose constructions for arbitrary computations (streaming or otherwise), due to Goldwasser, Kalai, and Rothblum [19]. This requires several new insights and enhancements to move the methodology from a theoretical result to a practical possibility. Our main contribution is in achieving a prover that runs in time O(S(n) log S(n)), where S(n) is the size of an arithmetic circuit computing the function of interest; this compares favorably to the poly(S(n)) runtime for the prover promised in [19]. Our experimental results demonstrate that a practical general-purpose protocol for verifiable computation may be significantly closer to reality than previously realized.Second, we describe a set of techniques that achieve genuine scalability for protocols fine-tuned for specific important problems in streaming and database processing. Focusing in particular on non-interactive protocols for problems ranging from matrix-vector multiplication to bipartite perfect matching, we build on prior work [8, 5] to achieve a prover that runs in nearly linear-time, while obtaining optimal tradeoffs between communication cost and the user's working memory. Existing techniques required (substantially) superlinear time for the prover. Finally, we develop improved interactive protocols for specific problems based on a linearization technique originally due to Shen [33]. We argue that even if general-purpose methods improve, fine-tuned protocols will remain valuable in real-world settings for key problems, and hence special attention to specific problems is warranted.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {90–112},
numpages = {23},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090246,
author = {L\'{o}pez-Ortiz, Alejandro and Salinger, Alejandro},
title = {Paging for Multi-Core Shared Caches},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090246},
doi = {10.1145/2090236.2090246},
abstract = {Paging for multi-core processors extends the classical paging problem to a setting in which several processes simultaneously share the cache. Recently, Hassidim proposed a model for multi-core paging [25], studying cache eviction policies for multi-cores under the traditional competitive analysis metric and showing that LRU is not competitive against an offline policy that has the power to arbitrarily delay request sequences to its advantage. While Hassidim brought attention to this problem, an effective and realistic model with accompanying competitive caching algorithms remains to be introduced.In this paper we propose a more conventional model in which requests must be served as they arrive. We study the problem of minimizing the number of faults, deriving bounds on the competitive ratios of natural strategies to manage the cache. We show that traditional online paging algorithms are not competitive in our model. We then study the offline paging problem and show that the problem of deciding if a request can be served such that at a given time each sequence has faulted at most a given number of times is NP-complete and that its optimization version is APX-hard (for an unbounded number of sequences). We show as well that although offline algorithms can benefit from properly aligning future requests by means of faults, an algorithm that does so by forcing faults on pages that it has in its cache has no advantage over an honest algorithm that evicts pages only when faults occur. Lastly, we describe offline algorithms for the decision problem and for minimizing the total number of faults that run in polynomial time in the length of the sequences.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {113–127},
numpages = {15},
keywords = {chip multiprocessor, multi-core, online algorithms, paging, cache},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090247,
author = {Braverman, Mark and Grigo, Alexander and Rojas, Cristobal},
title = {Noise vs Computational Intractability in Dynamics},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090247},
doi = {10.1145/2090236.2090247},
abstract = {Computation plays a key role in predicting and analyzing natural phenomena. There are two fundamental barriers to our ability to computationally understand the long-term behavior of a dynamical system that describes a natural process. The first one is unaccounted-for errors, which may make the system unpredictable beyond a very limited time horizon. This is especially true for chaotic systems, where a small change in the initial conditions may cause a dramatic shift in the trajectories. The second one is Turing-completeness. By the undecidability of the Halting Problem, the long-term prospects of a system that can simulate a Turing Machine cannot be determined computationally.We investigate the interplay between these two forces -- unaccounted-for errors and Turing-completeness. We show that the introduction of even a small amount of noise into a dynamical system is sufficient to "destroy" Turing-completeness, and to make the system's long-term behavior computationally predictable. On a more technical level, we deal with long-term statistical properties of dynamical systems, as described by invariant measures. We show that while there are simple dynamical systems for which the invariant measures are non-computable, perturbing such systems makes the invariant measures efficiently computable. Thus, noise that makes the short term behavior of the system harder to predict, may make its long term statistical behavior computationally tractable. We also obtain some insight into the computational complexity of predicting systems affected by random noise.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {128–141},
numpages = {14},
keywords = {predictability, invariant distributions, uncomputability, perturbation, robustness},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090248,
author = {Valiant, Paul},
title = {Distribution Free Evolvability of Polynomial Functions over All Convex Loss Functions},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090248},
doi = {10.1145/2090236.2090248},
abstract = {We formulate a notion of evolvability for functions with domain and range that are real-valued vectors, a compelling way of expressing many natural biological processes. We show that linear and fixed-degree polynomial functions are evolvable in the following dually robust sense: There is a single evolution algorithm that for all convex loss functions converges for all distributions.It is possible that such dually robust results can be achieved by simpler and more natural evolution algorithms. In the second part of the paper we introduce a simple and natural algorithm that we call "wide-scale random noise" and prove a corresponding result for the L2 metric. We conjecture that the algorithm works for more general classes of metrics.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {142–148},
numpages = {7},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090249,
author = {Anagnostopoulos, Aris and Kumar, Ravi and Mahdian, Mohammad and Upfal, Eli and Vandin, Fabio},
title = {Algorithms on Evolving Graphs},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090249},
doi = {10.1145/2090236.2090249},
abstract = {Motivated by applications that concern graphs that are evolving and massive in nature, we define a new general framework for computing with such graphs. In our framework, the graph changes over time and an algorithm can only track these changes by explicitly probing the graph. This framework captures the inherent tradeoff between the complexity of maintaining an up-to-date view of the graph and the quality of results computed with the available view. We apply this framework to two classical graph connectivity problems, namely, path connectivity and minimum spanning trees, and obtain efficient algorithms.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {149–160},
numpages = {12},
keywords = {algorithms, path connectivity, evolving graphs, minimum spanning tree},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090250,
author = {Braverman, Mark},
title = {Towards Deterministic Tree Code Constructions},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090250},
doi = {10.1145/2090236.2090250},
abstract = {We present a deterministic operator on tree codes -- we call tree code product -- that allows one to deterministically combine two tree codes into a larger tree code. Moreover, if the original tree codes are efficiently encodable and decodable, then so is their product. This allows us to give the first deterministic subexponential-time construction of explicit tree codes: we are able to construct a tree code T of size n in time 2nε,. Moreover, T is also encodable and decodable in time 2nε,.We then apply our new construction to obtain a deterministic constant-rate error-correcting scheme for interactive computation over a noisy channel with random errors. If the length of the interactive computation is n, the amount of computation required is deterministically bounded by n1+o(1), and the probability of failure is n-ω(1).},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {161–167},
numpages = {7},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090251,
author = {Viderman, Michael},
title = {Linear Time Decoding of Regular Expander Codes},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090251},
doi = {10.1145/2090236.2090251},
abstract = {Sipser and Spielman (IEEE IT, 1996) showed that any (c, d)-regular expander code with expansion parameter &gt; 3/4 is decodable in linear time from a constant fraction of errors. Feldman et al. (IEEE IT, 2007) proved that expansion parameter &gt; 2/3 + 1/3c is sufficient to correct a constant fraction of errors in polynomial time using LP decoding.In this work we give a simple combinatorial algorithm that achieves even better parameters. In particular, our algorithm runs in linear time and works for any expansion parameter &gt; 2/3−1/6c. We also prove that our decoding algorithm can be executed in logarithmic time on a linear number of parallel processors.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {168–182},
numpages = {15},
keywords = {LDPC codes, error-correcting codes, decoding, expander codes},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090252,
author = {Guruswami, Venkatesan and Narayanan, Srivatsan and Wang, Carol},
title = {List Decoding Subspace Codes from Insertions and Deletions},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090252},
doi = {10.1145/2090236.2090252},
abstract = {We present a construction of subspace codes along with an efficient algorithm for list decoding from both insertions and deletions, handling an information-theoretically maximum fraction of these with polynomially small rate. Our construction is based on a variant of the folded Reed-Solomon codes in the world of linearized polynomials, and the algorithm is inspired by the recent linear-algebraic approach to list decoding [4]. Ours is the first list decoding algorithm for subspace codes that can handle deletions; even one deletion can totally distort the structure of the basis of a subspace and is thus challenging to handle. When there are only insertions, we also present results for list decoding subspace codes that are the linearized analog of Reed-Solomon codes (proposed in [15, 8], and closely related to the Gabidulin codes for rank-metric coding), obtaining some improvements over similar results in [10].},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {183–189},
numpages = {7},
keywords = {codes over grassmanians, network coding, linearized polynomials, folded reed-solomon codes, list error-correction, gabidulin codes},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090253,
author = {Kol, Gillat and Raz, Ran},
title = {Bounds on Locally Testable Codes with Unique Tests},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090253},
doi = {10.1145/2090236.2090253},
abstract = {The Unique Games Conjecture (UGC) is an important open problem in the research of PCPs and hardness of approximation. The conjecture is a strengthening of the PCP Theorem, predicting the existence of a special type of PCP verifiers: 2-query verifiers that only make unique tests. Moreover, the UGC predicts that such PCP verifiers can have almost-perfect completeness and low-soundness.The computational complexity notion of a PCP is closely related to the combinatorial notion of a Locally Testable Code (LTC). LTCs are error-correcting codes with codeword testers that only make a constant number of queries to the tested word. All known PCP constructions use LTCs as building blocks. Furthermore, to obtain PCPs with certain properties, one usually uses LTCs with corresponding properties.In light of the strong connection between PCPs and LTCs, one may conjecture the existence of LTCs with properties similar to the ones required by the UGC. In this work we show limitations on such LTCs: We consider 2-query LTCs with codeword testers that only make unique tests. Roughly speaking, we show that any such LTC with relative distance close to 1, almost-perfect completeness and low-soundness, is of constant size.While our result does not imply anything about the correctness of the UGC, it does show some limitations of unique tests, compared, for example, to projection tests.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {190–202},
numpages = {13},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090254,
author = {Nissim, Kobbi and Smorodinsky, Rann and Tennenholtz, Moshe},
title = {Approximately Optimal Mechanism Design via Differential Privacy},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090254},
doi = {10.1145/2090236.2090254},
abstract = {We study the implementation challenge in an abstract interdependent values model and an arbitrary objective function. We design a generic mechanism that allows for approximate optimal implementation of insensitive objective functions in ex-post Nash equilibrium. If, furthermore, values are private then the same mechanism is strategy proof. We cast our results onto two specific models: pricing and facility location. The mechanism we design is optimal up to an additive factor of the order of magnitude of one over the square root of the number of agents and involves no utility transfers.Underlying our mechanism is a lottery between two auxiliary mechanisms --- with high probability we actuate a mechanism that reduces players influence on the choice of the social alternative, while choosing the optimal outcome with high probability. This is where differential privacy is employed. With the complementary probability we actuate a mechanism that may be typically far from optimal but is incentive compatible. The joint mechanism inherits the desired properties from both.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {203–213},
numpages = {11},
keywords = {mechanism design, differential privacy, monopolist pricing, facility location},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090255,
author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
title = {Fairness through Awareness},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090255},
doi = {10.1145/2090236.2090255},
abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {214–226},
numpages = {13},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090256,
author = {Manshadi, Vahideh H. and Saberi, Amin},
title = {Dynamics of Prisoner's Dilemma and the Evolution of Cooperation on Networks},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090256},
doi = {10.1145/2090236.2090256},
abstract = {We study the evolution of cooperation in populations where individuals play prisoner's dilemma on a network. Every node of the network corresponds to an individual choosing whether to cooperate or defect in a repeated game. The players revise their actions by imitating those neighbors who have higher payoffs.We show that when the interactions take place on graphs with large girth, cooperation is more likely to emerge. On the flip side, in graphs with many cycles of length 3 and 4, defection spreads more rapidly.One of the key ideas of our analysis is that our dynamics can be seen as a perturbation of the voter model. We write the transition kernel of the corresponding Markov chain in terms of the pairwise correlations in the voter model. We analyze the pairwise correlation and show that in graphs with relatively large girth, cooperators cluster and help each other.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {227–235},
numpages = {9},
keywords = {games on networks, Markov chain, game dynamics},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090257,
author = {Azar, Pablo and Chen, Jing and Micali, Silvio},
title = {Crowdsourced Bayesian Auctions},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090257},
doi = {10.1145/2090236.2090257},
abstract = {We investigate the problem of optimal mechanism design, where an auctioneer wants to sell a set of goods to buyers, in order to maximize revenue. In a Bayesian setting the buyers' valuations for the goods are drawn from a prior distribution D, which is often assumed to be known by the seller. In this work, we focus on cases where the seller has no knowledge at all, and "the buyers know each other better than the seller knows them". In our model, D is not necessarily common knowledge. Instead, each buyer individually knows a posterior distribution associated with D. Since the seller relies on the buyers' knowledge to help him set a price, we call these types of auctions crowdsourced Bayesian auctions.For this crowdsourced Bayesian model and many environments of interest, we show that, for arbitrary valuation distributions D (in particular, correlated ones), it is possible to design mechanisms matching to a significant extent the performance of the optimal dominant-strategy-truthful mechanisms where the seller knows D.To obtain our results, we use two techniques: (1) proper scoring rules to elicit information from the players; and (2) a reverse version of the classical Bulow-Klemperer inequality. The first lets us build mechanisms with a unique equilibrium and good revenue guarantees, even when the players' second and higher-order beliefs about each other are wrong. The second allows us to upper bound the revenue of an optimal mechanism with n players by an n/n--1 fraction of the revenue of the optimal mechanism with n -- 1 players. We believe that both techniques are new to Bayesian optimal auctions and of independent interest for future work.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {236–248},
numpages = {13},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090258,
author = {Zhan, Bohua and Kimmel, Shelby and Hassidim, Avinatan},
title = {Super-Polynomial Quantum Speed-Ups for Boolean Evaluation Trees with Hidden Structure},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090258},
doi = {10.1145/2090236.2090258},
abstract = {We give a quantum algorithm for evaluating a class of boolean formulas (such as NAND trees and 3-majority trees) on a restricted set of inputs. Due to the structure of the allowed inputs, our algorithm can evaluate a depth n tree using O(n2+logω) queries, where ω is independent of n and depends only on the type of subformulas within the tree. We also prove a classical lower bound of nΩ(log log n) queries, thus showing a (small) super-polynomial speed-up.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {249–265},
numpages = {17},
keywords = {span programs, NAND tree, super-polynomial quantum speed-up},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090259,
author = {Ito, Tsuyoshi and Kobayashi, Hirotada and Watrous, John},
title = {Quantum Interactive Proofs with Weak Error Bounds},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090259},
doi = {10.1145/2090236.2090259},
abstract = {This paper proves that the computational power of quantum interactive proof systems, with a double-exponentially small gap in acceptance probability between the completeness and soundness cases, is precisely characterized by EXP, the class of problems solvable in exponential time by deterministic Turing machines. This fact, and our proof of it, has implications concerning quantum and classical interactive proof systems in the setting of unbounded error that include the following:• Quantum interactive proof systems are strictly more powerful than their classical counterparts in the unbounded-error setting unless PSPACE = EXP, as even unbounded error classical interactive proof systems can be simulated in PSPACE.• The recent proof of Jain, Ji, Upadhyay, and Watrous (STOC 2010) establishing QIP = PSPACE relies heavily on the fact that the quantum interactive proof systems defining the class QIP have bounded error. Our result implies that some nontrivial assumption on the error bounds for quantum interactive proofs is unavoidable to establish this result (unless PSPACE = EXP).• To prove our result, we give a quantum interactive proof system for EXP with perfect completeness and soundness error 1--2-2poly, for which the soundness error bound is provably tight. This establishes another respect in which quantum and classical interactive proof systems differ, because such a bound cannot hold for any classical interactive proof system: distinct acceptance probabilities for classical interactive proof systems must be separated by a gap that is at least (single-)exponentially small.We also study the computational power of a few other related unbounded-error complexity classes.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {266–275},
numpages = {10},
keywords = {nonlocal correlations, computational complexity, interactive proofs, quantum computing},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090260,
author = {Farhi, Edward and Gosset, David and Hassidim, Avinatan and Lutomirski, Andrew and Shor, Peter},
title = {Quantum Money from Knots},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090260},
doi = {10.1145/2090236.2090260},
abstract = {Quantum money is a cryptographic protocol in which a mint can produce a quantum state, no one else can copy the state, and anyone (with a quantum computer) can verify that the state came from the mint. We present a concrete quantum money scheme based on superpositions of diagrams that encode oriented links with the same Alexander polynomial. We expect our scheme to be secure against computationally bounded adversaries.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {276–289},
numpages = {14},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090261,
author = {Ozols, Maris and Roetteler, Martin and Roland, J\'{e}r\'{e}mie},
title = {Quantum Rejection Sampling},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090261},
doi = {10.1145/2090236.2090261},
abstract = {Rejection sampling is a well-known method to sample from a target distribution, given the ability to sample from a given distribution. The method has been first formalized by von Neumann (1951) and has many applications in classical computing. We define a quantum analogue of rejection sampling: given a black box producing a coherent superposition of (possibly unknown) quantum states with some amplitudes, the problem is to prepare a coherent superposition of the same states, albeit with different target amplitudes. The main result of this paper is a tight characterization of the query complexity of this quantum state generation problem. We exhibit an algorithm, which we call quantum rejection sampling, and analyze its cost using semidefinite programming. Our proof of a matching lower bound is based on the automorphism principle which allows to symmetrize any algorithm over the automorphism group of the problem. Our main technical innovation is an extension of the automorphism principle to continuous groups that arise for quantum state generation problems where the oracle encodes unknown quantum states, instead of just classical data. Furthermore, we illustrate how quantum rejection sampling may be used as a primitive in designing quantum algorithms, by providing three different applications. We first show that it was implicitly used in the quantum algorithm for linear systems of equations by Harrow, Hassidim and Lloyd. Secondly, we show that it can be used to speed up the main step in the quantum Metropolis sampling algorithm by Temme et al.. Finally, we derive a new quantum algorithm for the hidden shift problem of an arbitrary Boolean function and relate its query complexity to "water-filling" of the Fourier spectrum.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {290–308},
numpages = {19},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090262,
author = {Brakerski, Zvika and Gentry, Craig and Vaikuntanathan, Vinod},
title = {(Leveled) Fully Homomorphic Encryption without Bootstrapping},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090262},
doi = {10.1145/2090236.2090262},
abstract = {We present a novel approach to fully homomorphic encryption (FHE) that dramatically improves performance and bases security on weaker assumptions. A central conceptual contribution in our work is a new way of constructing leveled fully homomorphic encryption schemes (capable of evaluating arbitrary polynomial-size circuits), without Gentry's bootstrapping procedure.Specifically, we offer a choice of FHE schemes based on the learning with error (LWE) or ring-LWE (RLWE) problems that have 2λ security against known attacks. For RLWE, we have:• A leveled FHE scheme that can evaluate L-level arithmetic circuits with \~{O}(λ · L3) per-gate computation -- i.e., computation quasi-linear in the security parameter. Security is based on RLWE for an approximation factor exponential in L. This construction does not use the bootstrapping procedure.• A leveled FHE scheme that uses bootstrapping as an optimization, where the per-gate computation (which includes the bootstrapping procedure) is \~{O}(λ2), independent of L. Security is based on the hardness of RLWE for quasi-polynomial factors (as opposed to the sub-exponential factors needed in previous schemes).We obtain similar results to the above for LWE, but with worse performance.Based on the Ring LWE assumption, we introduce a number of further optimizations to our schemes. As an example, for circuits of large width -- e.g., where a constant fraction of levels have width at least λ -- we can reduce the per-gate computation of the bootstrapped version to \~{O}(λ), independent of L, by batching the bootstrapping operation. Previous FHE schemes all required Ω(λ3.5) computation per gate.At the core of our construction is a much more effective approach for managing the noise level of lattice-based ciphertexts as homomorphic operations are performed, using some new techniques recently introduced by Brakerski and Vaikuntanathan (FOCS 2011).},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {309–325},
numpages = {17},
keywords = {modulus reduction, bootstrapping, learning with errors, fully homomorphic encryption},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090263,
author = {Bitansky, Nir and Canetti, Ran and Chiesa, Alessandro and Tromer, Eran},
title = {From Extractable Collision Resistance to Succinct Non-Interactive Arguments of Knowledge, and Back Again},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090263},
doi = {10.1145/2090236.2090263},
abstract = {The existence of succinct non-interactive arguments for NP (i.e., non-interactive computationally-sound proofs where the verifier's work is essentially independent of the complexity of the NP nondeterministic verifier) has been an intriguing question for the past two decades. Other than CS proofs in the random oracle model [Micali, FOCS '94], the only existing candidate construction is based on an elaborate assumption that is tailored to a specific protocol [Di Crescenzo and Lipmaa, CiE '08].We formulate a general and relatively natural notion of an extractable collision-resistant hash function (ECRH) and show that, if ECRHs exist, then a modified version of Di Crescenzo and Lipmaa's protocol is a succinct non-interactive argument for NP. Furthermore, the modified protocol is actually a succinct non-interactive adaptive argument of knowledge (SNARK). We then propose several candidate constructions for ECRHs and relaxations thereof.We demonstrate the applicability of SNARKs to various forms of delegation of computation, to succinct non-interactive zero knowledge arguments, and to succinct two-party secure computation. Finally, we show that SNARKs essentially imply the existence of ECRHs, thus demonstrating the necessity of the assumption.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {326–349},
numpages = {24},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090264,
author = {Boneh, Dan and Segev, Gil and Waters, Brent},
title = {Targeted Malleability: Homomorphic Encryption for Restricted Computations},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090264},
doi = {10.1145/2090236.2090264},
abstract = {We put forward the notion of targeted malleability: given a homomorphic encryption scheme, in various scenarios we would like to restrict the homomorphic computations one can perform on encrypted data. We introduce a precise framework, generalizing the foundational notion of non-malleability introduced by Dolev, Dwork, and Naor (SICOMP '00), ensuring that the malleability of a scheme is targeted only at a specific set of "allowable" functions.In this setting we are mainly interested in the efficiency of such schemes as a function of the number of repeated homomorphic operations. Whereas constructing a scheme whose ciphertext grows linearly with the number of such operations is straightforward, obtaining more realistic (or merely non-trivial) length guarantees is significantly more challenging.We present two constructions that transform any homomorphic encryption scheme into one that offers targeted malleability. Our constructions rely on standard cryptographic tools and on succinct non-interactive arguments, which are currently known to exist in the standard model based on variants of the knowledge-of-exponent assumption. The two constructions offer somewhat different efficiency guarantees, each of which may be preferable depending on the underlying building blocks.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {350–366},
numpages = {17},
keywords = {non-malleable encryption, homomorphic encryption},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090265,
author = {Atserias, Albert and Maneva, Elitza},
title = {Sherali-Adams Relaxations and Indistinguishability in Counting Logics},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090265},
doi = {10.1145/2090236.2090265},
abstract = {Two graphs with adjacency matrices A and B are isomorphic if there exists a permutation matrix P for which the identity PTAP = B holds. Multiplying through by P and relaxing the permutation matrix to a doubly stochastic matrix leads to the linear programming relaxation known as fractional isomorphism. We show that the levels of the Sherali-Adams (SA) hierarchy of linear programming relaxations applied to fractional isomorphism interleave in power with the levels of a well-known color-refinement heuristic for graph isomorphism called the Weisfeiler-Lehman algorithm, or equivalently, with the levels of indistinguishability in a logic with counting quantifiers and a bounded number of variables. This tight connection has quite striking consequences. For example, it follows immediately from a deep result of Grohe in the context of logics with counting quantifiers, that a fixed number of levels of SA suffice to determine isomorphism of planar and minor-free graphs. We also offer applications both in finite model theory and polyhedral combinatorics. First, we show that certain properties of graphs, such as that of having a flow-circulation of a prescribed value, are definable in the infinitary logic with counting with a bounded number of variables. Second, we exploit a lower bound construction due to Cai, F\"{u}rer and Immerman in the context of counting logics to give simple explicit instances that show that the SA relaxations of the vertex-cover and cut polytopes do not reach their integer hulls for up to Ω(n) levels, where n is the number of vertices in the graph.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {367–379},
numpages = {13},
keywords = {graph isomorphism, pebble games, lift-and-project, sherali-adams relaxations, linear programming, counting logics, weisfeiler-lehman algorithm},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090266,
author = {Hardt, Moritz and Srivastava, Nikhil and Tulsiani, Madhur},
title = {Graph Densification},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090266},
doi = {10.1145/2090236.2090266},
abstract = {We initiate a principled study of graph densification. Given a graph G the goal of graph densification is to come up with another graph H that has significantly more edges than G but nevertheless approximates G well with respect to some set of test functions. In this paper we focus on the case of cut and spectral approximations.As it turns out graph densification exhibits rich connections to a set of interesting and sometimes seemingly unrelated questions in graph theory and metric embeddings. In particular we show the following results:• A graph G has a multiplicative cut approximation with an asymptotically increased density if and only if it does not embed into l1 under a weak notion of embeddability. We demonstrate that all planar graphs as well as random geometric graphs possess such embeddings and thus do not have densifiers. On the other hand, expanders do have densifiers (namely, the complete graph) and as a result do not embed into l1 even under our weak notion of embedding.• An analogous characterization is true for multiplicative spectral approximations where the embedding is into l22. Using this characterization we expose a surprisingly close connection between multiplicative spectral and multiplicative cut densifiers.• We also consider additive cut and spectral approximations. We exhibit graphs that do not possess non-trivial additive densifiers.Our results are mainly based on linear and semidefinite programs (and their duals) for computing the maximum weight densifier of a given graph. This also leads to efficient algorithms in the case of spectral densifiers and additive cut densifiers.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {380–392},
numpages = {13},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090267,
author = {Kapralov, Michael and Panigrahy, Rina},
title = {Spectral Sparsification via Random Spanners},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090267},
doi = {10.1145/2090236.2090267},
abstract = {In this paper we introduce a new notion of distance between nodes in a graph that we refer to as robust connectivity. Robust connectivity between a pair of nodes u and v is parameterized by a threshold k and intuitively captures the number of paths between u and v of length at most k. Using this new notion of distances, we show that any black box algorithm for constructing a spanner can be used to construct a spectral sparsifier. We show that given an undirected weighted graph G, simply taking the union of spanners of a few (polylogarithmically many) random subgraphs of G obtained by sampling edges at different probabilities, after appropriate weighting, yields a spectral sparsifier of G. We show how this be done in \~{O}(m) time, producing a sparsifier with \~{O}(n/ε2) edges. While the cut sparsifiers of Benczur and Karger are based on weighting edges according to (inverse) strong connectivity, and the spectral sparsifiers are based on resistance, our method weights edges using the robust connectivity measure. The main property that we use is that this new measure is always greater than the resistance when scaled by a factor of O(k) (k is chosen to be O(log n)), but, just like resistance and connectivity, has a bounded sum, i.e. \~{O}(n), over all the edges of the graph.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {393–398},
numpages = {6},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090268,
author = {Chekuri, Chandra and Kannan, Sreeram and Raja, Adnan and Viswanath, Pramod},
title = {Multicommodity Flows and Cuts in Polymatroidal Networks},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090268},
doi = {10.1145/2090236.2090268},
abstract = {We consider multicommodity flow and cut problems in polymatroidal networks where there are submodular capacity constraints on the edges incident to a node. Polymatroidal networks were introduced by Lawler and Martel [20] and Hassin [15] in the single-commodity setting and are closely related to the submodular flow model of Edmonds and Giles [10]; the well-known maxflow-mincut theorem holds in this more general setting. Polymatroidal networks for the multicommodity case have not, as far as the authors are aware, been previously explored. Our work is primarily motivated by applications to information flow in wireless networks.We also consider the notion of undirected polymatroidal networks and observe that they provide a natural way to generalize flows and cuts in edge and node capacitated undirected networks. We establish poly-logarithmic flow-cut gap results in several scenarios that have been previously considered in the standard network flow models where capacities are on the edges or nodes [21, 22, 13, 19, 12]. Our results from a preliminary version have already found applications in wireless network information flow [16, 7] and we anticipate more in the future. On the technical side our key tools are the formulation and analysis of the dual of the flow relaxations via continuous extensions of submodular functions, in particular the Lov\'{a}sz extension. For directed graphs we rely on a simple yet useful reduction from polymatroidal networks to standard networks. For undirected graphs we rely on the interplay between the Lov\'{a}sz extension of a submodular function and line embeddings with low average distortion introduced by Matou\v{s}ek and Rabinovich [25]; this connection is inspired by, and generalizes, the work of Feige, Hajiaghayi and Lee [12] on node-capacitated multicommodity flows and cuts. The applicability of embeddings to flow-cut gaps in polymatroidal networks is of independent mathematical interest.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {399–408},
numpages = {10},
keywords = {node-capacitated networks, flow-cut gaps, sub-modular flows, polymatroidal networks, line embeddings},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090269,
author = {Cohen, Gil and Shpilka, Amir and Tal, Avishay},
title = {On the Degree of Univariate Polynomials over the Integers},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090269},
doi = {10.1145/2090236.2090269},
abstract = {We study the following problem raised by von zur Gathen and Roche [GR97]:What is the minimal degree of a nonconstant polynomial f: {0,...,n} → {0,...,m}?Clearly, when m = n the function f(x) = x has degree 1. We prove that when m = n - 1 (i.e. the point {n} is not in the range), it must be the case that deg(f) = n - o(n). This shows an interesting threshold phenomenon. In fact, the same bound on the degree holds even when the image of the polynomial is any (strict) subset of {0,...,n}. Going back to the case m = n, as we noted the function f(x) = x is possible, however, we show that if one excludes all degree 1 polynomials then it must be the case that deg(f) = n - o(n). Moreover, the same conclusion holds even if m = O(n1.475--ε). In other words, there are no polynomials of intermediate degrees that map {0,...,n} to {0,...,m}. Furthermore, we give a meaningful answer when m is a large polynomial, or even exponential, in n. Roughly, we show that if m &lt; (n/c d), for some constant c, then either deg(f) ≤ d - 1 (e.g. f(x) = (x-n/2d - 1) is possible) or deg(f) ≥ n/3 - O(d log n). So, again, no polynomial of intermediate degree exists for such m. We achieve this result by studying a discrete version of the problem of giving a lower bound on the minimal L∞, norm that a monic polynomial of degree d obtains on the interval [-1,1].We complement these results by showing that for every d = o(√n/log n) there exists a polynomial f: {0,...,n} → {0,...,O(nd+0.5)} of degree n/3 - O(d log n) ≤ deg(f) ≤ n - O(d log (n)).Our proofs use a variety of techniques that we believe will find other applications as well. One technique shows how to handle a certain set of diophantine equations by working modulo a well chosen set of primes (i.e. a Boolean cube of primes). Another technique shows how to use lattice theory and Minkowski's theorem to prove the existence of a polynomial with certain properties.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {409–427},
numpages = {19},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090270,
author = {Letscher, David},
title = {On Persistent Homotopy, Knotted Complexes and the Alexander Module},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090270},
doi = {10.1145/2090236.2090270},
abstract = {In this paper techniques from persistent homology are generalized to homotopy groups and to algebraic invariants from knot theory. We define the persistent Alexander module, which can be used to detect knotting in a complex and determine when the knotting changes when viewed from different scales. Algorithms that use the persistent Alexander module are also presented and applied to examples including protein structures. While the basic definition of persistent homotopy is known, this is the first work to use it successfully for computations.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {428–441},
numpages = {14},
keywords = {alexander module, persistent homotopy},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090271,
author = {Pagh, Rasmus},
title = {Compressed Matrix Multiplication},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090271},
doi = {10.1145/2090236.2090271},
abstract = {Motivated by the problems of computing sample covariance matrices, and of transforming a collection of vectors to a basis where they are sparse, we present a simple algorithm that computes an approximation of the product of two n-by-n real matrices A and B. Let ||AB||F denote the Frobenius norm of AB, and b be a parameter determining the time/accuracy trade-off. Given 2-wise independent hash functions h1, h2: [n] → [b], and s1, s2: [n] → {-1, +1} the algorithm works by first "compressing" the matrix product into the polynomial[EQUATION]Using FFT for polynomial multiplication, we can compute c0,...,cb-1 such that Σi cixi = (p(x) mod xb) + (p(x) div xb) in time \~{O}(n2 + nb). An unbiased estimator of (AB)ij with variance at most ||AB||2F/b can then be computed as:[EQUATION]Our approach also leads to an algorithm for computing AB exactly, whp., in time \~{O}(N + nb) in the case where A and B have at most N nonzero entries, and AB has at most b nonzero entries. Also, we use error-correcting codes in a novel way to recover significant entries of AB in near-linear time.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {442–451},
numpages = {10},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090272,
author = {Cai, Jin-Yi and Kowalczyk, Michael and Williams, Tyson},
title = {Gadgets and Anti-Gadgets Leading to a Complexity Dichotomy},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090272},
doi = {10.1145/2090236.2090272},
abstract = {We introduce an idea called anti-gadgets in complexity reductions. These combinatorial gadgets have the effect of erasing the presence of some other graph fragment, as if we had managed to include a negative copy of a graph gadget. We use this idea to prove a complexity dichotomy theorem for the partition function Z(G) on 3-regular directed graphs G, where each edge is given a complex-valued binary function f: {0,1}2 → C. We show that[EQUATION]is either computable in polynomial time or #P-hard, depending explicitly on f.To state the dichotomy theorem more explicitly, we show that the partition function Z(G) on 3-regular directed graphs G is computable in polynomial time when f belongs to one of four classes, which can be described as (1) degenerate, (2) generalized disequality, (3) generalized equality, and (4) affine after a holographic transformation. In all other cases it is #P-hard. Here class (4), after a holographic transformation, can also be described as an exponential quadratic polynomial of the form iQ(x, y), where i = √-1 and the cross term xy in the quadratic polynomial Q(x, y) has an even coefficient. If the input graph G is planar, then an additional class of functions becomes computable in polynomial time, and everything else remains #P-hard. This additional class is precisely those which can be computed by holographic algorithms with matchgates, making use of the Fisher-Kasteleyn-Temperley algorithm via Pfaffians.There is a long history in the study of "Exactly Solved Models" in statistical physics. In the language of complexity theory, physicists' notion of an "Exactly Solvable" system corresponds to a system with a polynomial time computable partition function. A central question is to identify which "systems" can be solved "exactly" and which "systems" are "difficult". While in physics, there is no rigorous definition of being "difficult", complexity theory supplies the proper notion---#P-hardness.The main innovation in this paper is the idea of an anti-gadget. It is analogous to the pairing of a particle and its anti-particle in physics. Coupled with the idea of anti-gadgets, we also introduce a general way of proving #P-hardness by two types of gadgets called recursive gadgets and projector gadgets. We prove a Group Lemma which spells out a general condition for the technique to succeed. This Group Lemma states that as long as the group generated by the transition matrices of the constructed gadgets is infinite, then one can interpolate all unary functions---a key step in the proof of #P-hardness. Interpolation is carried out by forming a Vandermonde system and proving that it is of full rank. The anti-gadget concept makes the transition to group theory very natural and seamless.Not only is the idea of anti-gadgets useful in proving a new complexity dichotomy theorem in counting complexity, we also show that anti-gadgets provide a simple explanation for some miraculous cancellations that were observed in previous results. Furthermore, anti-gadgets can also guide the search for gadget sets more by design than by chance.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {452–467},
numpages = {16},
keywords = {spin system, holographic transformation, holographic algorithm, gadget, dichotomy theorem, group},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090273,
author = {Fefferman, Bill and Shaltiel, Ronen and Umans, Christopher and Viola, Emanuele},
title = {On Beating the Hybrid Argument},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090273},
doi = {10.1145/2090236.2090273},
abstract = {The hybrid argument allows one to relate the distinguishability of a distribution (from uniform) to the predictability of individual bits given a prefix. The argument incurs a loss of a factor k equal to the bit-length of the distributions: ε-distinguishability implies ε/k-predictability. This paper studies the consequences of avoiding this loss - what we call "beating the hybrid argument" -- and develops new proof techniques that circumvent the loss in certain natural settings. Specifically, we obtain the following results:1. We give an instantiation of the Nisan-Wigderson generator (JCSS '94) that can be broken by quantum computers, and that is o(1)-unpredictable against AC0. We conjecture that this generator indeed fools AC0. Our conjecture implies the existence of an oracle relative to which BQP is not in the PH, a longstanding open problem.2. We show that the "INW" generator by Impagliazzo, Nisan, and Wigderson (STOC '94) with seed length O(log n log log n) produces a distribution that is 1/log n-unpredictable against poly-logarithmic width (general) read-once oblivious branching programs. Obtaining such generators where the output is indistinguishable from uniform is a longstanding open problem.3. We identify a property of functions f, "resamplability," that allows us to beat the hybrid argument when arguing indistinguishability of[EQUATION]from uniform. This gives new pseudorandom generators for classes such as AC0[p] with a stretch that, despite being sub-linear, is the largest known. We view this as a first step towards beating the hybrid argument in the analysis of the Nisan-Wigderson generator (which applies [EQUATION] on correlated x1,...,xk) and proving the conjecture in the first item.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {468–483},
numpages = {16},
keywords = {quantum computing, pseudorandomness, branching program, constant depth circuits, small space, hybrid argument},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090274,
author = {Kun, Gabor and O'Donnell, Ryan and Tamaki, Suguru and Yoshida, Yuichi and Zhou, Yuan},
title = {Linear Programming, Width-1 CSPs, and Robust Satisfaction},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090274},
doi = {10.1145/2090236.2090274},
abstract = {We say that an algorithm robustly decides a constraint satisfaction problem Π if it distinguishes at-least-(1 -ε)-satisfiable instances from less-than-(1 - r(ε))-satisfiable instances for some function r(ε) with r(ε) → 0 as ε → 0. In this paper we show that the canonical linear programming relaxation robustly decides Π if and only if Π has "width 1" (in the sense of Feder and Vardi).},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {484–495},
numpages = {12},
keywords = {constraint satisfaction problems, approximation algorithm, linear programming, width-1 CSPs, robust satisfaction algorithm},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{10.1145/2090236.2090275,
author = {Jansen, Maurice and Santhanam, Rahul},
title = {Marginal Hitting Sets Imply Super-Polynomial Lower Bounds for Permanent},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090275},
doi = {10.1145/2090236.2090275},
abstract = {Suppose f is a univariate polynomial of degree r = r(n) that is computed by a size n arithmetic circuit. It is a basic fact of algebra that a nonzero univariate polynomial of degree r can vanish on at most r points. This implies that for checking whether f is identically zero, it suffices to query f on an arbitrary test set of r + 1 points. Could this brute-force method be improved upon by a single point? We develop a framework where such a marginal improvement implies that Permanent does not have polynomial size arithmetic circuits.More formally, we formulate the following hypothesis for any field of characteristic zero: There is a fixed depth d and some function s(n) = O(n), such that for arbitrarily small ε &gt; 0, there exists a hitting set Hn ⊂ Z of size at most 2s(nε) against univariate polynomials of degree at most 2s(nε) computable by size n constant-free1 arithmetic circuits, where Hn can be encoded by uniform TC0 circuits of size 2O(nε) and depth d. We prove that the hypothesis implies that Permanent does not have polynomial size constant-free arithmetic circuits.Our hypothesis provides a unifying perspective on several important complexity theoretic conjectures, as it follows from these conjectures for different degree ranges as determined by the function s(n). We will show that it follows for s(n) = n from the widely-believed assumption that poly size Boolean circuits cannot compute the Permanent of a 0,1-matrix over Z. The hypothesis can also be easily derived from the Shub-Smale τ-conjecture [21], for any s(n) with s(n) = ω(log n) and s(n) = O(n). This implies our result strengthens a theorem by B\"{u}rgisser [4], who derives the same lower bound from the τ-conjecture. For s(n) = 0, the hypothesis follows from the statement that (n!) is ultimately hard, a statement that is known to imply P ≠ NP over C [21].We apply our randomness-to-hardness theorem to prove the following unconditional result for Permanent: either Permanent does not have uniform constant-depth threshold circuits of sub-exponential size, or Permanent does not have polynomial-size constant-free arithmetic circuits.Turning to the Boolean world, we give a simplified proof of the following strengthening of Allender's lower bound [2] for the (0,1)-Permanent: either the (0,1)-Permanent is not simultaneously in polynomial time and sub-polynomial space, or logarithmic space does not have uniform constant-depth threshold circuits of polynomial size.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {496–506},
numpages = {11},
keywords = {computational complexity theory, derandomization, permanent, polynomial identity testing, lower bounds, arithmetic circuits},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

