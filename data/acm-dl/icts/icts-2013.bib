@inproceedings{10.1145/2422436.2422438,
author = {Juba, Brendan and Williams, Ryan},
title = {Massive Online Teaching to Bounded Learners},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422438},
doi = {10.1145/2422436.2422438},
abstract = {We consider a model of teaching in which the learners are consistent and have bounded state, but are otherwise arbitrary. The teacher is non-interactive and "massively open": the teacher broadcasts a sequence of examples of an arbitrary target concept, intended for every possible on-line learning algorithm to learn from. We focus on the problem of designing interesting teachers: efficient sequences of examples that lead all capable and consistent learners to learn concepts, regardless of the underlying algorithm used by the learner. We use two measures of teaching efficiency: the number of mistakes made by the worst-case learner, and the maximum length of the example sequence needed for the worst-case learner. Our results are summarized as follows: Given a uniform random sequence of examples of an n-bit concept function, learners (capable of consistently learning the concept) with s(n) bits of state are guaranteed to make only O(n ⋅ s(n)) mistakes and exactly learn the concept, with high probability. This theorem has interesting corollaries; for instance, every concept c has a sequence of examples can teach c to all capable consistent on-line learners implementable with s(n)-size circuits, such that every learner makes only ~O(s(n)^2) mistakes. That is, all resource-bounded algorithms capable of consistently learning a concept can be simultaneously taught that concept with few mistakes, on a single example sequence. We also show how to efficiently generate such a sequence of examples on-line: using Nisan's pseudorandom generator, each example in the sequence can be generated with polynomial-time overhead per example, with an O(n ⋅ s(n))-bit initial seed. To justify our use of randomness, we prove that any non-trivial derandomization of our sequences would imply new circuit lower bounds. For instance, if there is a deterministic 2n O(1) time algorithm that generates a sequence of examples, such that all consistent and capable polynomial-size circuit learners learn the all-zeroes concept with less than 2n mistakes, then EXP ⊄ P. We present examples illustrating that the key differences in our model -- our focus on mistakes rather than the total number of examples, and our use of a state bound -- must be considered together to obtain our results. We show that for every consistent s(n)-state bounded learner A, and every n-bit concept that A is capable of learning, there is a custom "tutoring" sequence of only O(n ⋅ s(n)) examples that teaches A the concept. That is, in principle, there are no slow learners, only bad teachers: if a state-bounded learner is capable of learning a concept at all, then it can always be taught that concept quickly via some short sequence of examples.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {1–10},
numpages = {10},
keywords = {on-line learning, derandomization, teaching, semantic communication},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422439,
author = {Hsu, Daniel and Kakade, Sham M.},
title = {Learning Mixtures of Spherical Gaussians: Moment Methods and Spectral Decompositions},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422439},
doi = {10.1145/2422436.2422439},
abstract = {This work provides a computationally efficient and statistically consistent moment-based estimator for mixtures of spherical Gaussians. Under the condition that component means are in general position, a simple spectral decomposition technique yields consistent parameter estimates from low-order observable moments, without additional minimum separation assumptions needed by previous computationally efficient estimation procedures. Thus computational and information-theoretic barriers to efficient estimation in mixture models are precluded when the mixture components have means in general position and spherical covariances. Some connections are made to estimation problems related to independent component analysis.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {11–20},
numpages = {10},
keywords = {spectral decomposition, mixtures of gaussians, mixture models, method of moments},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422440,
author = {Long, Philip M. and Servedio, Rocco A.},
title = {Low-Weight Halfspaces for Sparse Boolean Vectors},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422440},
doi = {10.1145/2422436.2422440},
abstract = {For S ⊆ {0,1}n, a Boolean function f: S -&gt; {-1,1} is a halfspace over S if there exist w ∈ Rn and θ ∈ R such that f(x)=sign(w ⋅ x - θ) for all x ∈ S. We give bounds on the size of integer weights w1,...,wn ∈ Z that are required to represent halfspaces over Hamming balls centered at 0n, i.e. halfspaces over S ={0,1}n≤ k = {x ∈ {0,1}n : x1 + ⋅⋅⋅ + xn ≤ k}. Such weight bounds for halfspaces over Hamming balls have immediate consequences for the performance of learning algorithms in the increasingly common scenario of learning from very high-dimensional categorical examples which are such that only a small number of features are active in each example.We give upper and lower bounds on weight both for exact representation (when sign(w ⋅ x -θ) must equal f(x) for every x ∈ S) and for ε-approximate representation (when sign(w ⋅ x-θ) may disagree with f(x) for up to an ε fraction of points x ∈ S). Our results show that extremal bounds for exact representation are qualitatively rather similar whether the domain is all of {0,1}n or the Hamming ball {0,1}n≤ k, but extremal bounds for approximate representation are qualitatively very different between these two domains.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {21–36},
numpages = {16},
keywords = {linear separator, halfspace, hamming ball, linear threshold function, hyperplane},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422441,
author = {Yang, Liu and Blum, Avrim and Carbonell, Jaime},
title = {Learnability of DNF with Representation-Specific Queries},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422441},
doi = {10.1145/2422436.2422441},
abstract = {We study the problem of PAC learning the class of DNF formulas with a type of natural pairwise query specific to the DNF representation. Specifically, given a pair of positive examples from a polynomial-sized sample, we consider boolean queries that ask whether the two examples satisfy at least one term in common in the target DNF, and numerical queries that ask how many terms in common the two examples satisfy. We provide both positive and negative results for learning with these queries under both uniform and general distributions.For boolean queries, we show that the problem of learning an arbitrary DNF target under an arbitrary distribution is no easier than in the traditional PAC model. However, on the positive side, we show that under the uniform distribution, we can properly learn any DNF formula with O(log(n)) relevant variables, any DNF formula where each variable appears in at most O(log(n)) terms, and any DNF formula having at most 2O(√log(n)) terms. Under general distributions, we show that 2-term DNFs are efficiently properly learnable as are disjoint DNFs.For numerical queries, we show we can learn arbitrary DNF formulas under the uniform distribution; in the process, we give an algorithm for learning a sum of monotone terms from labeled data only. Numerical-valued queries also allow us to properly learn any DNF with O(log(n)) relevant variables under arbitrary distributions, as well as DNF having O(log(n)) terms, and DNF for which each example can satisfy at most O(1) terms.Other possible generalizations of the query include allowing the algorithm to ask the query for an arbitrary number of examples from the sample at once (rather than just two), or allowing the algorithm to ask the query for examples of its own construction; we show that both of these generalizations allow for efficient proper learnability of arbitrary DNF functions under arbitrary distributions.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {37–46},
numpages = {10},
keywords = {pac learnability, learning DNF, efficient learning, queries},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422443,
author = {Chung, Kai-Min and Lui, Edward and Pass, Rafael},
title = {Can Theories Be Tested? A Cryptographic Treatment of Forecast Testing},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422443},
doi = {10.1145/2422436.2422443},
abstract = {How do we test if a weather forecaster actually knows something about whether it will rain or not? Intuitively, a "good" forecast test should be complete---namely, a forecaster knowing the distribution of Nature should be able to pass the test with high probability, and sound---an uninformed forecaster should only be able to pass the test with small probability. We provide a comprehensive cryptographic study of the feasibility of complete and sound forecast testing, introducing various notions of both completeness and soundness, inspired by the literature on interactive proofs. Our main technical result is an incompleteness theorem for our most basic notion of computationally sound and complete forecast testing: If Nature is implemented by a polynomial-time algorithm, then every complete polynomial-time test can be passed by a completely uninformed polynomial-time forecaster (i.e., a computationally-bounded "charlatan") with high probability. We additionally study alternative notions of soundness and completeness and present both positive and negative results for these notions.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {47–56},
numpages = {10},
keywords = {incompleteness, multiplicative weights, forecast testing},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422444,
author = {Chastain, Erick and Livnat, Adi and Papadimitriou, Christos and Vazirani, Umesh},
title = {Multiplicative Updates in Coordination Games and the Theory of Evolution},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422444},
doi = {10.1145/2422436.2422444},
abstract = {In this paper we point out a new and unexpected connection between three fields: Evolution Theory, Game Theory, and Algorithms.In particular, we study the standard equations of population genetics for Evolution, in the presence of recombination (sex), focusing on the important special case of weak selection [1,2] in which all fitness values are assumed to be close to one another. Weak selection is the mathematical regime capturing the widely accepted Neutral Theory proposed by Kimura in the 1970s [3], hypothesizing that evolution proceeds for the most part not by substantial increases in fitness but by essentially random drift. We show that in this regime evolution through natural selection and sex is tantamount to a game played through the multiplicative weight updates game dynamics [4]. The players of the game are the genes (genetic loci), the strategies available to each player are the alleles of the gene, and the probabilities whereby a player plays a strategy is the strategy's frequency in the population. The utility to each player/gene of each strategy profile is the fitness of the corresponding genotype (organism). That is, the game is a coordination game between genes, in which the players' interests are perfectly aligned. Importantly, the utility maximized in this game, as well as the amount by which each allele is boosted, is precisely the allele's mixability, or average fitness, a quantity recently proposed in [5] as a novel concept that is crucial in understanding natural selection under sex, thus providing a rigorous demonstration of that insight.We also establish a result regarding the maintenance of genetic diversity (multiplicity of alleles per gene). We prove that the equilibria in two-person coordination games are likely to have large supports, and thus genetic diversity need not suffer much at equilibrium. Establishing large supports involves answering through a novel technique the following question: what is the probability that for a random square matrix $A$ (with entries drawn independently from smooth distributions that are symmetric around zero) both systems Ax=1 and ATy=1 have positive solutions? The proof is through a simple potential function argument. Both the question and the technique may be of broader interest.It has often seemed astonishing --- even to experienced students of Evolution, Darwin included --- that the crude mechanism of natural selection is responsible for producing the dazzling variety of Life around us. The present mathematical connection of Evolution with the multiplicative weight updates algorithm --- a technique that has surprised our field time and again with its fantastic effectiveness and versatility --- may carry some explanatory force in this regard.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {57–58},
numpages = {2},
keywords = {algorithmic game theory, multiplicative weight updates, theory of evolution},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422445,
author = {Vishnoi, Nisheeth K.},
title = {Making Evolution Rigorous: The Error Threshold},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422445},
doi = {10.1145/2422436.2422445},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {59–60},
numpages = {2},
keywords = {error threshold, evolution, quasispecies},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422446,
author = {Bhattacharyya, Arnab and Braverman, Mark and Chazelle, Bernard and Nguyen, Huy L.},
title = {On the Convergence of the Hegselmann-Krause System},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422446},
doi = {10.1145/2422436.2422446},
abstract = {We study convergence of the following discrete-time non-linear dynamical system: $n$ agents are located in Rd and at every time step, each moves synchronously to the average location of all agents within a unit distance of it. This popularly studied system was introduced by Krause to model the dynamics of opinion formation and is often referred to as the Hegselmann-Krause model. We prove the first polynomial time bound for the convergence of this system in arbitrary dimensions. This improves on the bound of nO(n) resulting from a more general theorem of Chazelle [4]. Also, we show a quadratic lower bound and improve the upper bound for one-dimensional systems to O(n3).},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {61–66},
numpages = {6},
keywords = {hegselmann-krause system, convergence, opinion dynamics},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422448,
author = {Xiao, David},
title = {Is Privacy Compatible with Truthfulness?},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422448},
doi = {10.1145/2422436.2422448},
abstract = {In the area of privacy-preserving data mining, a differentially private mechanism intuitively encourages people to share their data because they are at little risk of revealing their own information. However, we argue that this interpretation is incomplete because external incentives are necessary for people to participate in databases, and so data release mechanisms should not only be differentially private but also compatible with incentives, otherwise the data collected may be false. We apply the notion of truthfulness from game theory to this problem. In certain settings, it turns out that existing differentially private mechanisms do not encourage participants to report their information truthfully.On the positive side, we exhibit a transformation that takes truthful mechanisms and transforms them into differentially private mechanisms that remain truthful. Our transformation applies to games where the type space is small and the goal is to optimize an insensitive quantity such as social welfare. Our transformation incurs only a small additive loss in optimality, and it is computationally efficient. Combined with the VCG mechanism, our transformation implies that there exists a differentially private, truthful, and approximately efficient mechanism for any social welfare game with small type space.We also study a model where an explicit numerical cost is assigned to the information leaked by a mechanism. We show that in this case, even differential privacy may not be strong enough of a notion to motivate people to participate truthfully. We show that mechanisms that release a perturbed histogram of the database may reveal too much information. We also show that, in general, any mechanism that outputs a synopsis that resembles the original database (such as the mechanism of Blum et al. (STOC '08)) may reveal too much information.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {67–86},
numpages = {20},
keywords = {differential privacy, mechanism design},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422449,
author = {Blocki, Jeremiah and Blum, Avrim and Datta, Anupam and Sheffet, Or},
title = {Differentially Private Data Analysis of Social Networks via Restricted Sensitivity},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422449},
doi = {10.1145/2422436.2422449},
abstract = {We introduce the notion of restricted sensitivity as an alternative to global and smooth sensitivity to improve accuracy in differentially private data analysis. The definition of restricted sensitivity is similar to that of global sensitivity except that instead of quantifying over all possible datasets, we take advantage of any beliefs about the dataset that a querier may have, to quantify over a restricted class of datasets. Specifically, given a query f and a hypothesis HH about the structure of a dataset D, we show generically how to transform f into a new query fHH whose global sensitivity (over all datasets including those that do not satisfy HH) matches the restricted sensitivity of the query f. Moreover, if the belief of the querier is correct (i.e., D ∈ HH) then fHH(D) = f(D). If the belief is incorrect, then fHH(D) may be inaccurate.We demonstrate the usefulness of this notion by considering the task of answering queries regarding social-networks, which we model as a combination of a graph and a labeling of its vertices. In particular, while our generic procedure is computationally inefficient, for the specific definition of H as graphs of bounded degree, we exhibit efficient ways of constructing fH using different projection-based techniques. We then analyze two important query classes: subgraph counting queries (e.g., number of triangles) and local profile queries (e.g., number of people who know a spy and a computer-scientist who know each other). We demonstrate that the restricted sensitivity of such queries can be significantly lower than their smooth sensitivity. Thus, using restricted sensitivity we can maintain privacy whether or not D ∈ HH, while providing more accurate results in the event that HH holds true.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {87–96},
numpages = {10},
keywords = {social networks, differential privacy},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422450,
author = {Beimel, Amos and Nissim, Kobbi and Stemmer, Uri},
title = {Characterizing the Sample Complexity of Private Learners},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422450},
doi = {10.1145/2422436.2422450},
abstract = {In 2008, Kasiviswanathan el al. defined private learning as a combination of PAC learning and differential privacy [16]. Informally, a private learner is applied to a collection of labeled individual information and outputs a hypothesis while preserving the privacy of each individual. Kasiviswanathan et al. gave a generic construction of private learners for (finite) concept classes, with sample complexity logarithmic in the size of the concept class. This sample complexity is higher than what is needed for non-private learners, hence leaving open the possibility that the sample complexity of private learning may be sometimes significantly higher than that of non-private learning. We give a combinatorial characterization of the sample size sufficient and necessary to privately learn a class of concepts. This characterization is analogous to the well known characterization of the sample complexity of non-private learning in terms of the VC dimension of the concept class. We introduce the notion of probabilistic representation of a concept class, and our new complexity measure RepDim corresponds to the size of the smallest probabilistic representation of the concept class. We show that any private learning algorithm for a concept class C with sample complexity m implies RepDim(C) = O(m), and that there exists a private learning algorithm with sample complexity m = O(RepDim(C)).We further demonstrate that a similar characterization holds for the database size needed for privately computing a large class of optimization problems and also for the well studied problem of private data release.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {97–110},
numpages = {14},
keywords = {probabilistic representation, pac learning, sample complexity, differential privacy},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422451,
author = {Wichs, Daniel},
title = {Barriers in Cryptography with Weak, Correlated and Leaky Sources},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422451},
doi = {10.1145/2422436.2422451},
abstract = {There has been much recent progress in constructing cryptosystems that maintain their security without requiring uniform randomness and perfect secrecy. These schemes are motivated by a diverse set of problems such as providing resilience to side-channel leakage, using weak physical sources of randomness as secret keys, and allowing deterministic encryption for high-entropy messages. Nevertheless, despite this progress, some basic and seemingly achievable security properties have eluded our reach. For example, we are unable to prove the security of basic tools for manipulating weak/leaky random sources, such as as pseudo-entropy generators and seed-dependent computational condensers. We also do not know how to prove leakage-resilient security of any cryptosystem with a uniquely determined secret key. In the context of deterministic encryption we do not have a standard-model constructions achieving the strongest notion of security proposed by Bellare, Boldyreva and O'Neill (CRYPTO '07), that would allow us to encrypt arbitrarily correlated messages of sufficiently large individual entropy.We provide broad black-box separation results, showing that the security of such primitives cannot be proven under virtually any standard cryptographic hardness assumption via a reduction that treats the adversary as a black box. We do so by formalizing the intuition that "the only way that a reduction can simulate the correctly distributed view for an attacker is to know all the secrets, in which case it does not learn anything useful from the attack". Such claims are often misleading and clever way of getting around them allow us to achieve a wealth of positive results with imperfect/leaky randomness. However, in this work we show that this intuition can be formalized and that it indeed presents a real barrier for the examples given above.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {111–126},
numpages = {16},
keywords = {cryptography},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422453,
author = {Goldreich, Oded and Goldwasser, Shafi and Ron, Dana},
title = {On the Possibilities and Limitations of Pseudodeterministic Algorithms},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422453},
doi = {10.1145/2422436.2422453},
abstract = {We study the possibilities and limitations of pseudodeterministic algorithms, algorithms, a notion put forward by Gat and Goldwasser (2011). These are probabilistic algorithms that solve search problems such that on each input, with high probability, they output the same solution, which may be thought of as a canonical solution. We consider both the standard setting of (probabilistic) polynomial-time algorithms and the setting of (probabilistic) sublinear-time algorithms. Some of our results are outlined next. In the standard setting, we show that pseudodeterministic algorithms are more powerful than deterministic algorithms if and only if cPneqBPP, but are weaker than general probabilistic algorithms. In the sublinear-time setting, we show that if a search problem has a pseudodeterministic algorithm of query complexity q, then this problem can be solved deterministically making O(q4) queries. This refers to total search problems. In contrast, for several natural promise search problems, we present pseudodeterministic algorithms that are much more efficient than their deterministic counterparts.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {127–138},
numpages = {12},
keywords = {bpp, zpp, search problems, sublinear-time computations, unique solutions},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422454,
author = {Kulkarni, Raghav},
title = {Evasiveness through a Circuit Lens},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422454},
doi = {10.1145/2422436.2422454},
abstract = {A function f : {0, 1}n -&gt; {0, 1} is called evasive if its decision tree complexity is maximal, i.e., D(f) = n. The long-standing Anderaa-Rosenberg-Karp (ARK) Conjecture asserts that every non-trivial monotone graph property is evasive. The Evasiveness Conjecture (EC) is a generalization of ARK Conjecture from monotone graph properties to arbitrary monotone transitive Boolean functions.In this paper we study a weakening of the Evasiveness Conjecture called Weak Evasivenss Conjecture (weak-EC). The weak-EC asserts that every non-trivial monotone transitive Boolean function must have D(f) ≥ n1- ε, for every ε &gt; 0. The purpose of this note is to make some remarks on weak-EC that hint towards a plausible attack on EC.First we observe that weak-EC is equivalent to EC. Further we observe that ruling out only certain simple (monotone-NC1) counter-examples to weak-EC suffices to confirm EC in its whole generality. Finally we rule out some simple counter-examples to weak-EC (AC0 : unconditionally; and monotone-TC0 : under a conjecture of Benjamini, Kalai, and Schramm on their noise stability).We also investigate an analogue of weak-EC for the stronger model of parity decision trees and provide a counter-example to this seemingly stronger version under a conjecture of Montanaro and Osborne.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {139–144},
numpages = {6},
keywords = {decision tree complexity, evasiveness conjecture, monotone graph properties, circuit complexity},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422455,
author = {Buhrman, Harry and Fehr, Serge and Schaffner, Christian and Speelman, Florian},
title = {The Garden-Hose Model},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422455},
doi = {10.1145/2422436.2422455},
abstract = {We define a new model of communication complexity, called the garden-hose model. Informally, the garden-hose complexity of a function f:{0,1}n x {0,1}n -&gt; {0,1} is given by the minimal number of water pipes that need to be shared between two parties, Alice and Bob, in order for them to compute the function f as follows: Alice connects her ends of the pipes in a way that is determined solely by her input x ∈ {0,1}n and, similarly, Bob connects his ends of the pipes in a way that is determined solely by his input y ∈ {0,1}n. Alice turns on the water tap that she also connected to one of the pipes. Then, the water comes out on Alice's or Bob's side depending on the function value f(x,y).We prove almost-linear lower bounds on the garden-hose complexity for concrete functions like inner product, majority, and equality, and we show the existence of functions with exponential garden-hose complexity. Furthermore, we show a connection to classical complexity theory by proving that all functions computable in log-space have polynomial garden-hose complexity.We consider a randomized variant of the garden-hose complexity, where Alice and Bob hold pre-shared randomness, and a quantum variant, where Alice and Bob hold pre-shared quantum entanglement, and we show that the randomized garden-hose complexity is within a polynomial factor of the deterministic garden-hose complexity. Examples of (partial) functions are given where the quantum garden-hose complexity is logarithmic in n while the classical garden-hose complexity can be lower bounded by nc for constant c&gt;0.Finally, we show an interesting connection between the garden-hose model and the (in)security of a certain class of quantum position-verification schemes.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {145–158},
numpages = {14},
keywords = {garden-hose model, communication complexity, position-based quantum cryptography},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422456,
author = {Brody, Joshua E. and Chen, Shiteng and Papakonstantinou, Periklis A. and Song, Hao and Sun, Xiaoming},
title = {Space-Bounded Communication Complexity},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422456},
doi = {10.1145/2422436.2422456},
abstract = {In the past thirty years, Communication Complexity has emerged as a foundational tool to proving lower bounds in many areas of computer science. Its power comes from its generality, but this generality comes at a price---no superlinear communication lower bound is possible, since a player may communicate his entire input. However, what if the players are limited in their ability to recall parts of their interaction?We introduce memory models for 2-party communication complexity. Our general model is as follows: two computationally unrestricted players, Alice and Bob, each have s(n) bits of memory. When a player receives a bit of communication, he "compresses" his state. This compression may be an arbitrary function of his current memory contents, his input, and the bit of communication just received; the only restriction is that the compression must return at most s(n) bits. We obtain memory hierarchy theorems (also comparing this general model with its restricted variants), and show super-linear lower bounds for some explicit (non-boolean) functions.Our main conceptual and technical contribution concerns the following variant. The communication is one-way, from Alice to Bob, where Bob controls two types of memory: (i) a large, oblivious memory, where updates are only a function of the received bit and the current memory content, and (ii) a smaller, non-oblivious/general memory, where updates can be a function of the input given to Bob. We exhibit natural protocols where this semi-obliviousness shows up. For this model we also introduce new techniques through which certain limitations of space-bounded computation are revealed.One of the main motivations of this work is in understanding the difference in the use of space when computing the following functions: Equality (EQ), Inner Product (IP), and connectivity in a directed graph (REACH). When viewed as communication problems, EQ can be decided using 0 non-oblivious bits (and log2 n oblivious bits), IP requires exactly 1 non-oblivious bit, whereas for REACH we obtain the same lower bound as for IP and conjecture that the actual bound is Omega(log2 n). In fact, proving that 1 non-oblivious bit is required becomes technically sophisticated, and the question even for 2 non-oblivious bits for any explicit boolean function remains open.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {159–172},
numpages = {14},
keywords = {space-bounded, communication complexity, memory-bounded},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422458,
author = {Khot, Subhash and Safra, Muli and Tulsiani, Madhur},
title = {Towards an Optimal Query Efficient PCP?},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422458},
doi = {10.1145/2422436.2422458},
abstract = {We construct a PCP based on the hyper-graph linearity test with 3 free queries. It has near-perfect completeness and soundness strictly less than 1/8. Such a PCP was known before only assuming the Unique Games Conjecture, albeit with soundness arbitrarily close to 1/16. At a technical level, our main contribution is constructing a new outer PCP which is "robust" against bounded degree polynomials, and showing that it can be composed with the hyper-graph linearity test with 3 free queries. We believe this outer PCP may be useful in obtaining the optimal query vs. soundness tradeoff for PCPs.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {173–186},
numpages = {14},
keywords = {hardness of approximation, CSPs, PCPs},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422459,
author = {Austrin, Per and Khot, Subhash},
title = {A Characterization of Approximation Resistance for Even K-Partite CSPs},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422459},
doi = {10.1145/2422436.2422459},
abstract = {A constraint satisfaction problem (CSP) is said to be approximation resistant if it is hard to approximate better than the trivial algorithm which picks a uniformly random assignment. Assuming the Unique Games Conjecture, we give a characterization of approximation resistance for k-partite CSPs defined by an even predicate.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {187–196},
numpages = {10},
keywords = {unique games conjecture, approximation resistance},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422460,
author = {Barak, Boaz and Kindler, Guy and Steurer, David},
title = {On the Optimality of Semidefinite Relaxations for Average-Case and Generalized Constraint Satisfaction},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422460},
doi = {10.1145/2422436.2422460},
abstract = {This work studies several questions about the optimality of semidefinite programming (SDP) for constraint satisfaction problems (CSPs). First we propose the hypothesis that the well known Basic SDP relaxation is actually optimal for random instances of constraint satisfaction problems for every predicate. This unifies several conjectures proposed in the past, and suggests a unifying principle for the average-case complexity of CSPs. We provide several types of indirect evidence for the truth of this hypothesis, and also show that it (and its variants) imply several conjectures in hardness of approximation including polynomial factor hardness for the densest k subgraph problem and hard instances for the Sliding Scale Conjecture of Bellare, Goldwasser, Lund and Russell (1993).Second, we observe that for every predicate P, the basic SDP relaxation achieves the same approximation guarantee for the CSP for P and for a more general problem (involving not just Boolean but constrained vector assignments), which we call the Generalized CSP for P. Raghavendra (2008) showed that it is UGC-hard to approximate the CSP for P better than this guarantee. We show that it is NP-hard to approximate the Generalized CSP for P better than this guarantee.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {197–214},
numpages = {18},
keywords = {UGC, complexity, hardness of approximation, semi-definite program, unique games conjecture},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422461,
author = {Austrin, Per and H\r{a}stad, Johan and Pass, Rafael},
title = {On the Power of Many One-Bit Provers},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422461},
doi = {10.1145/2422436.2422461},
abstract = {We study the class of languages, denoted by MIP[k, 1-ε, s], which have k-prover games where each prover just sends a single bit, with completeness 1-ε and soundness error s. For the case that k=1 (i.e., for the case of interactive proofs), Goldreich, Vadhan and Wigderson (Computational Complexity'02) demonstrate that SZK exactly characterizes languages having 1-bit proof systems with "non-trivial" soundness (i.e., 1/2 &lt; s ≤ 1-2ε). We demonstrate that for the case that k ≥ 2, 1-bit k-prover games exhibit a significantly richer structure: (Folklore) When s ≤ 1/2k - ε, MIP[k, 1-ε, s] = BPP; When 1/2k + ε ≤ s &lt; 2/2k -ε, MIP[k, 1-ε, s] = SZK; When s ≥ 2/2k + ε, AM ⊆ MIP[k, 1-ε, s]; For s ≤ 0.62 k/2k and sufficiently large k, MIP[k, 1-ε, s] ⊆ EXP; For s ≥ 2k/2k, MIP[k, 1, 1-ε, s] = NEXP.As such, 1-bit k-prover games yield a natural "quantitative" approach to relating complexity classes such as BPP, SZK, AM, EXP, and NEXP. We leave open the question of whether a more fine-grained hierarchy (between AM and NEXP) can be established for the case when s ≥ 2/2k + ε.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {215–220},
numpages = {6},
keywords = {laconic provers, multi-prover interactive proofs},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422463,
author = {Fiat, Amos and Karlin, Anna and Koutsoupias, Elias and Vidali, Angelina},
title = {Approaching Utopia: Strong Truthfulness and Externality-Resistant Mechanisms},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422463},
doi = {10.1145/2422436.2422463},
abstract = {We introduce and study strongly truthful mechanisms and their applications. We use strongly truthful mechanisms as a tool for implementation in undominated strategies for several problems, including the design of externality resistant auctions and a variant of multi-dimensional scheduling.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {221–230},
numpages = {10},
keywords = {algorithmic game theory, mechanism design},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422464,
author = {Azar, Pablo Daniel and Micali, Silvio},
title = {Parametric Digital Auctions},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422464},
doi = {10.1145/2422436.2422464},
abstract = {We study revenue maximization for digital auctions, where there are infinitely many copies of a good for sale. There are n buyers, each of whom is interested in obtaining one copy of the good. The buyers' private valuations are drawn from a joint distribution vec{F}. The seller does not know this distribution. The only information that she has are the mean ui and variance σi2 of each buyer i's marginal distribution Fi. We call such auctions parametric auctions. We construct a deterministic parametric auction that, for a wide class of distributions, guarantees a constant fraction of the optimal revenue achievable when the seller precisely knows the distribution F. Furthermore, our auction is a posted price mechanism and it is maximin optimal among all such mechanisms. That is, it is the posted price mechanism that maximizes revenue in the worst case over an adversarial choice of the distribution.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {231–232},
numpages = {2},
keywords = {approximately optimal auctions, robust optimization},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422465,
author = {Ghosh, Arpita and Hummel, Patrick},
title = {Learning and Incentives in User-Generated Content: Multi-Armed Bandits with Endogenous Arms},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422465},
doi = {10.1145/2422436.2422465},
abstract = {Motivated by the problem of learning the qualities of user-generated content on the Web, we study a multi-armed bandit problem where the number and success probabilities of the arms of the bandit are endogenously determined by strategic agents in response to the incentives provided by the learning algorithm. We model the contributors of user-generated content as attention-motivated agents who derive benefit when their contribution is displayed, and have a cost to quality, where a contribution's quality is the probability of its receiving a positive viewer vote. Agents strategically choose whether and what quality contribution to produce in response to the algorithm that decides how to display contributions. The algorithm, which would like to eventually only display the highest quality contributions, can only learn a contribution's quality from the viewer votes the contribution receives when displayed. The problem of inferring the relative qualities of contributions using viewer feedback, to optimize for overall viewer satisfaction over time, can then be modeled as the classic multi-armed bandit problem,  except that the arms available to the bandit and therefore the achievable regret are endogenously determined by strategic agents --- a good algorithm for this setting must not only quickly identify the best contributions, but also incentivize high-quality contributions to choose amongst in the first place. We first analyze the well-known UCB algorithm Ma [Auer et al. 2002] as a mechanism in this setting, where the total number of potential contributors or arms, K, can grow with the total number of viewers or available periods, T, and the maximum possible success probability of an arm, γ, may be bounded away from 1 to model malicious or error-prone viewers in the audience. We first show that while Ma can incentivize high-quality arms and achieve strong sublinear equilibrium regret when K(T) does not grow too quickly with T, it incentivizes very low quality contributions when K(T) scales proportionally with T. We then show that modifying the UCB mechanism to explore a randomly chosen restricted subset of √{T} arms provides excellent incentive properties --- this modified mechanism achieves strong sublinear regret, which is the regret measured against the maximum achievable quality γ, in every equilibrium, for all ranges of K(T) ≤ T, for all possible values of the audience parameter $gamma$.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {233–246},
numpages = {14},
keywords = {endogenous arms, attention economics, user-generated content (UGC), quality of online content, multi-armed bandits, game theory},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422466,
author = {Feige, Uriel and Izsak, Rani},
title = {Welfare Maximization and the Supermodular Degree},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422466},
doi = {10.1145/2422436.2422466},
abstract = {Given a set of items and a collection of players, each with a nonnegative monotone valuation set function over the items, the welfare maximization problem requires that every item be allocated to exactly one player, and one wishes to maximize the sum of values obtained by the players, as computed by applying the respective valuation function to the bundle of items allocated to the player. This problem in its full generality is NP-hard, and moreover, at least as hard to approximate as set-packing. Better approximation guarantees are known for restricted classes of valuation functions.In this work we introduce a new parameter, the supermodular degree of a valuation function, which is a measure for the extent to which the function exhibits supermodular behavior. We design an approximation algorithm for the welfare maximization problem whose approximation guarantee is linear in the supermodular degree of the underlying valuation functions.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {247–256},
numpages = {10},
keywords = {submodular functions, combinatorial auctions, approximation algorithms},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422468,
author = {Lacki, Jakub and Sankowski, Piotr},
title = {Reachability in Graph Timelines},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422468},
doi = {10.1145/2422436.2422468},
abstract = {In this paper we consider the problem of maintaining information about graphs with history -- so called graph timeline. A graph timeline is a sequence of graphs G1,..., Gt, in which consecutive graphs are obtained from previous ones by small modifications, e.g., by adding or removing a single edge. We aim to devise algorithms that after some preprocessing are able to efficiently answer queries about the existence of paths in the entire timeline of the graph, or within some time interval. We consider two types of queries: [forall (u,v,a,b)] --- query that checks if there exists a path from u to v in each of Ga,..., Gb; [exists(u,v,a,b)] --- query that checks if there exists a path from u to v in any of Ga,...,Gb. Our study is motivated by the recent intensive study of the evolution of graphs, and the question whether information about history can be efficiently aggregated. We show that for path queries this is, somewhat astonishingly, true. In some cases it is possible to preprocess graph timeline and answer such queries in almost optimal time.First, we consider undirected graphs and timelines in which two consecutive graphs differ by addition or removal of a single edge. We show a randomized algorithm that requires O(t log t log log t log n+m) preprocessing time and answers forall queries in O(log n log log t) time. Here, $n$ is the number of vertices in the graph and m is the number of permanent edges, that is edges common to all graphs in the timeline. This algorithm can be derandomized at some cost in the running time. Second, we give a deterministic algorithm that needs O(nt+m) preprocessing and answers exists queries in O(1) time. Finally, for directed graphs, we consider timelines where two consecutive graphs differ by addition or removal of a set of edges incident to one vertex. In this case we, are able to propose a randomized algorithm that after O(nω-1t) preprocessing time can answer exists(u,v,a,b) queries in O(n) time and forall(u,v,a,b) queries in O(n+b-a) time.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {257–268},
numpages = {12},
keywords = {reachability, transitive closure, dynamic graph algorithms, graph timeline},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422469,
author = {Chin, Hui Han and Madry, Aleksander and Miller, Gary L. and Peng, Richard},
title = {Runtime Guarantees for Regression Problems},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422469},
doi = {10.1145/2422436.2422469},
abstract = {We study theoretical runtime guarantees for a class of optimization problems that occur in a wide variety of inference problems. These problems are motivated by the LASSO framework and have applications in machine learning and computer vision. Our work shows a close connection between these problems and core questions in algorithmic graph theory. While this connection demonstrates the difficulties of obtaining runtime guarantees, it also suggests an approach of using techniques originally developed for graph algorithms.We then show that most of these problems can be formulated as a grouped least squares problem, and give efficient algorithms for this formulation. Our algorithms rely on routines for solving quadratic minimization problems, which in turn are equivalent to solving linear systems. Some preliminary experimental work on image processing tasks are also presented.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {269–282},
numpages = {14},
keywords = {image processing, optimization, regression},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422470,
author = {Roy, Swapnoneel and Rudra, Atri and Verma, Akshat},
title = {An Energy Complexity Model for Algorithms},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422470},
doi = {10.1145/2422436.2422470},
abstract = {Energy consumption has emerged as a first class computing resource for both server systems and personal computing devices. The growing importance of energy has led to rethink in hardware design, hypervisors, operating systems and compilers. Algorithm design is still relatively untouched by the importance of energy and algorithmic complexity models do not capture the energy consumed by an algorithm. In this paper, we propose a new complexity model to account for the energy used by an algorithm. Based on an abstract memory model (which was inspired by the popular DDR3 memory model and is similar to the parallel disk I/O model of Vitter and Shriver), we present a simple energy model that is a (weighted) sum of the time complexity of the algorithm and the number of 'parallel' I/O accesses made by the algorithm. We derive this simple model from a more complicated model that better models the ground truth and present some experimental justification for our model. We believe that the simplicity (and applicability) of this energy model is the main contribution of the paper. We present some sufficient conditions on algorithm behavior that allows us to bound the energy complexity of the algorithm in terms of its time complexity (in the RAM model) and its I/O complexity (in the I/O model). As corollaries, we obtain energy optimal algorithms for sorting (and its special cases like permutation), matrix transpose and (sparse) matrix vector multiplication.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {283–304},
numpages = {22},
keywords = {energy efficient algorithms, parallel disk i/o model},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422471,
author = {Klauck, Hartmut and Prakash, Ved},
title = {Streaming Computations with a Loquacious Prover},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422471},
doi = {10.1145/2422436.2422471},
abstract = {We define a new model of data streaming algorithms that employ a prover/helper to outsource difficult computations in a verifiable way. While for the verifier the usual time (per symbol read) and space constraints of the data streaming model are in place, the prover has unbounded space. Both parties cannot look into the future (i.e., do not know data arriving later). Previous work on such models either severely restricted the total communication between the prover and the verifier, or extended the computation by a long annotation that has to be streamed from the prover to the verifier offline after the original stream has ended, delaying the computation of the result. We argue that restricting the total communication severely is unnatural and investigate a model that only bounds the communication overhead, i.e., the amount of communication sent from the prover to the verifier per symbol of the data stream. This allows for vastly more communication between prover and verifier while maintaining the online nature of the model (in particular long annotations sent after the stream has ended are not allowed). Relaxing the communication requirement allows us to find simple algorithms for problems like the Longest Increasing Subsequence Problem (LIS), finding the Median, and for deciding whether the rank of a matrix is full or not. All our algorithms have a similar structure with phases whose length shrinks geometrically, and phase i being used to verify certain properties of the stream up to phase i-1 using re-streaming of parts of the previous stream. The challenge in each case is to tie the different phases together.Furthermore, while in previous work it was shown that all problems in the class NC can be computed in a streaming model with a prover, this general purpose algorithm tends to be inefficient, and this as well as related algorithms based on arithmetization techniques suffer from the following bottleneck: they employ a final verification phase (taking place after the end of the stream), which uses polylogarithmic communication (essentially the only communication in the whole protocol), yet the prover needs to perform computations that take at least linear time during that phase. While we show that such a verification phase with a large number of communication rounds between the prover and the verifier is unavoidable for certain problems, most algorithms we describe in our model (for problems like Median or LIS or Rank) avoid this bottleneck.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {305–320},
numpages = {16},
keywords = {data streaming, delegating computation, interactive proofs, proof verification},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422473,
author = {Reichardt, Ben W. and Unger, Falk and Vazirani, Umesh},
title = {A Classical Leash for a Quantum System: Command of Quantum Systems via Rigidity of CHSH Games},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422473},
doi = {10.1145/2422436.2422473},
abstract = {Can a classical experimentalist command an untrusted quantum system to realize arbitrary quantum dynamics, aborting if it misbehaves? If so, then we could realize the dream of device-independent quantum cryptography: using untrusted quantum devices to establish a shared random key, with security based on the correctness of quantum mechanics. It would also allow for testing whether a claimed quantum computer is truly quantum. We prove a rigidity theorem for the famous Clauser-Horne-Shimony-Holt (CHSH) game, first formulated to provide a means of experimentally testing the violation of the Bell inequalities. The theorem shows that the only way for the two non-communicating quantum players to win many games played in sequence is if their shared quantum state is close to the tensor product of EPR states (Bell states) and their measurements are the optimal CHSH measurements on successive qubits. This theorem may be viewed as analogous to classical multi-linearity testing, in the sense that the outcome of local checks gives a characterization of a global object.The rigidity theorem provides the basis of a technique by which a classical system can certify the joint, entangled state of a bipartite quantum system, as well as command the application of specific operators on each subsystem. This leads directly to a scheme for device-independent quantum key distribution. Control over the state and operators can also be leveraged to create more elaborate protocols for realizing general quantum circuits. In particular, it allows us to establish that a quantum interactive proof system with a classical verifier is as powerful as one with a quantum verifier, or QMIP = MIP*.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {321–322},
numpages = {2},
keywords = {CHSH game, quantum computing},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422474,
author = {Belovs, Aleksandrs and Spalek, Robert},
title = {Adversary Lower Bound for the K-Sum Problem},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422474},
doi = {10.1145/2422436.2422474},
abstract = {We prove a tight quantum query lower bound Omega(nk/(k+1)) for the problem of deciding whether there exist k numbers among n that sum up to a prescribed number, provided that the alphabet size is sufficiently large.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {323–328},
numpages = {6},
keywords = {orthogonal arrays, knapsack packing problem, quantum query complexity},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422475,
author = {Kobayashi, Hirotada and Le Gall, Fran\c{c}ois and Nishimura, Harumichi},
title = {Stronger Methods of Making Quantum Interactive Proofs Perfectly Complete},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422475},
doi = {10.1145/2422436.2422475},
abstract = {This paper presents stronger methods of achieving perfect completeness in quantum interactive proofs. First, it is proved that any problem in QMA has a two-message quantum interactive proof system of perfect completeness with constant soundness error, where the verifier has only to send a constant number of halves of EPR pairs. This in particular implies that the class QMA is necessarily included by the class QIP1(2) of problems having two-message quantum interactive proofs of perfect completeness, which gives the first nontrivial upper bound for QMA in terms of quantum interactive proofs. It is also proved that any problem having an $m$-message quantum interactive proof system necessarily has an ${(m+1)}$-message quantum interactive proof system of perfect completeness. This improves the previous result due to Kitaev and Watrous, where the resulting system of perfect completeness requires ${m+2}$ messages if not using the parallelization result.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {329–352},
numpages = {24},
keywords = {computational complexity, interactive proof systems, quantum computing},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422476,
author = {Woods, Damien and Chen, Ho-Lin and Goodfriend, Scott and Dabby, Nadine and Winfree, Erik and Yin, Peng},
title = {Active Self-Assembly of Algorithmic Shapes and Patterns in Polylogarithmic Time},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422476},
doi = {10.1145/2422436.2422476},
abstract = {We describe a computational model for studying the complexity of self-assembled structures with active molecular components. Our model captures notions of growth and movement ubiquitous in biological systems. The model is inspired by biology's fantastic ability to assemble biomolecules that form systems with complicated structure and dynamics, from molecular motors that walk on rigid tracks and proteins that dynamically alter the structure of the cell during mitosis, to embryonic development where large scale complicated organisms efficiently grow from a single cell. Using this active self-assembly model, we show how to efficiently self-assemble shapes and patterns from simple monomers. For example we show how to grow a line of monomers in time and number of monomer states that is merely logarithmic in its length. Our main results show how to grow arbitrary connected two-dimensional geometric shapes and patterns in expected time polylogarithmic in the size of the shape plus roughly the time required to run a Turing machine deciding whether or not a given pixel is in the shape. We do this while keeping the number of monomer types logarithmic in shape size, plus monomers required by the Kolmogorov complexity of the shape or pattern. This work thus highlights the fundamental efficiency advantage of active self-assembly over passive self-assembly and motivates experimental effort to construct self-assembly systems with active molecular components.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {353–354},
numpages = {2},
keywords = {reconfigurable robotics, molecular programming, model of computation, self-assembly},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422478,
author = {Micciancio, Daniele and Tessaro, Stefano},
title = {An Equational Approach to Secure Multi-Party Computation},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422478},
doi = {10.1145/2422436.2422478},
abstract = {We present a novel framework for the description and analysis of secure computation protocols that is at the same time mathematically rigorous and notationally lightweight and concise. The distinguishing feature of the framework is that it allows to specify (and analyze) protocols in a manner that is largely independent of time, greatly simplifying the study of cryptographic protocols. At the notational level, protocols are described by systems of mathematical equations (over domains), and can be studied through simple algebraic manipulations like substitutions and variable elimination. We exemplify our framework by analyzing in detail two classic protocols: a protocol for secure broadcast, and a verifiable secret sharing protocol, the second of which illustrates the ability of our framework to deal with probabilistic systems, still in a purely equational way.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {355–372},
numpages = {18},
keywords = {cryptography, multi-party computation, universal composability},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422479,
author = {Mahmoody, Mohammad and Moran, Tal and Vadhan, Salil},
title = {Publicly Verifiable Proofs of Sequential Work},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422479},
doi = {10.1145/2422436.2422479},
abstract = {We construct a publicly verifiable protocol for proving computational work based on collision-resistant hash functions and a new plausible complexity assumption regarding the existence of "inherently sequential" hash functions. Our protocol is based on a novel construction of time-lock puzzles. Given a sampled "puzzle" P getsr Dn, where $n$ is the security parameter and Dn is the distribution of the puzzles, a corresponding "solution" can be generated using N evaluations of the sequential hash function, where N&gt;n is another parameter, while any feasible adversarial strategy for generating valid solutions must take at least as much time as Ω(N) serial evaluations of the hash function after receiving $P$. Thus, valid solutions constitute a "proof" that Ω(N) parallel time elapsed since p was received. Solutions can be publicly and efficiently verified in time poly(n) ⋅ polylog(N). Applications of these "time-lock puzzles" include noninteractive timestamping of documents (where the distribution over the possible documents corresponds to the puzzle distribution Dn) and universally verifiable CPU benchmarks. Our construction is secure in the standard model under complexity assumptions (collision-resistant hash functions and inherently sequential hash functions), and makes black-box use of the underlying primitives. Consequently, the corresponding construction in the random oracle model is secure unconditionally. Moreover, as it is a public-coin protocol, it can be made non-interactive in the random oracle model using the Fiat-Shamir Heuristic.Our construction makes a novel use of "depth-robust" directed acyclic graphs---ones whose depth remains large even after removing a constant fraction of vertices---which were previously studied for the purpose of complexity lower-bounds. The construction bypasses a recent lower-bound of Mahmoody, Moran, and Vadhan (CRYPTO '11) for time-lock puzzles in the random oracle model, which showed that it is impossible to have time-lock puzzles like ours in the random oracle model if the puzzle generator also computes a solution together with the puzzle.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {373–388},
numpages = {16},
keywords = {time-lock puzzles, timestamping, depth robust graphs, proofs of work},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422480,
author = {Chung, Kai-Min and Lin, Huijia and Mahmoody, Mohammad and Pass, Rafael},
title = {On the Power of Nonuniformity in Proofs of Security},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422480},
doi = {10.1145/2422436.2422480},
abstract = {Nonuniform proofs of security are common in cryptography, but traditional black-box separations consider only uniform security reductions. In this paper, we initiate a formal study of the power and limits of nonuniform black-box proofs of security. We first show that a known protocol (based on the existence of one-way permutations) that uses a nonuniform proof of security, and it cannot be proven secure through a uniform security reduction. Therefore, nonuniform proofs of security are indeed provably more powerful than uniform ones. We complement this result by showing that many known black-box separations in the uniform regime actually do extend to the nonuniform regime. We prove our results by providing general techniques for extending certain types of black-box separations to handle nonuniformity.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {389–400},
numpages = {12},
keywords = {black-box separation, nonuniformity, proofs of security},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422481,
author = {Ben-Sasson, Eli and Chiesa, Alessandro and Genkin, Daniel and Tromer, Eran},
title = {Fast Reductions from RAMs to Delegatable Succinct Constraint Satisfaction Problems: Extended Abstract},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422481},
doi = {10.1145/2422436.2422481},
abstract = {Succinct arguments for NP are proof systems that allow a weak verifier to retroactively check computation done by a powerful prover. Constructions of such protocols prove membership in languages consisting of very large yet succinctly-represented constraint satisfaction problems that, alas, are unnatural in the sense that the problems that arise in practice are not in such form. For general computation tasks, the most natural representation is typically as random-access machine (RAM) algorithms, because such a representation can be obtained very efficiently by applying a compiler to code written in a high-level programming language. Thus, understanding the efficiency of reductions from RAM computations to other NP-complete problem representations for which succinct arguments (or proofs) are known is a prerequisite to a more complete understanding of the applicability of these arguments.Existing succinct argument constructions rely either on circuit satisfiability or (in PCP-based constructions) on algebraic constraint satisfaction problems. In this paper, we present new and more efficient reductions from RAM (and parallel RAM) computations to both problems that (a) preserve succinctness (i.e., do not "unroll" the computation of a machine), (b) preserve zero-knowledge and proof-of-knowledge properties, and (c) enjoy fast and highly-parallelizable algorithms for transforming a witness for the RAM computation into a witness for the corresponding problem. These additional properties are typically not considered in "classical" complexity theory but are often required or very desirable in the application of succinct arguments.Fulfilling all these efficiency requirements poses significant technical challenges, and we develop a set of tools (both unconditional and leveraging computational assumptions) for generically and efficiently structuring and arithmetizing RAM computations for use in succinct arguments. More generally, our results can be applied to proof systems for NP relying on the aforementioned problem representations; these include various zero-knowledge proof constructions.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {401–414},
numpages = {14},
keywords = {delegation of computation, random-access machines, probabilistically checkable proofs, zero-knowledge proofs, succinct arguments},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422482,
author = {Garay, Juan and Johnson, David and Kiayias, Aggelos and Yung, Moti},
title = {Resource-Based Corruptions and the Combinatorics of Hidden Diversity},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422482},
doi = {10.1145/2422436.2422482},
abstract = {In the setting of cryptographic protocols, the corruption of a party has traditionally been viewed as a simple, uniform and atomic operation, where the adversary decides to get control over a party and this party immediately gets corrupted. In this paper, motivated by the fact that different players may require different resources to get corrupted, we put forth the notion of resource-based corruptions, where the adversary must invest some resources in order to corrupt a player.If the adversary has full information about the system configuration then resource-based corruptions would provide no fundamental difference from the standard corruption model. However, in a resource "anonymous" setting, in the sense that such configuration is hidden from the adversary, much is to be gained in terms of efficiency and security.We showcase the power of such hidden diversity in the context of secure multiparty computation (MPC) with resource-based corruptions and prove that anonymity it can effectively be used to circumvent known impossibility results. Specifically, if OPT is the corruption budget that violates the completeness of MPC (the case when half or more of the players are corrupted), we show that if hidden diversity is available, the completeness of MPC can be made to hold against an adversary with as much as a B ⋅ OPT budget, for any constant B&gt;1. This result requires a suitable choice of parameters (in terms of number of players and their hardness to corrupt), which we provide and further prove other tight variants of the result when the said choice is not available. Regarding efficiency gains, we show that hidden diversity can be used to force the corruption threshold to drop from 1/2 to 1/3, in turn allowing the use of much more efficient (information-theoretic) MPC protocols.We achieve the above through a series of technical contributions: The modeling of the corruption process in the setting of cryptographic protocols through corruption oracles as well as the introduction of a notion of reduction to relate such oracles; the abstraction of the corruption game as a combinatorial problem and its analysis; and, importantly, the formulation of the notion of inversion effort preserving (IEP) functions which is a type of direct-sum property, and the property of hardness indistinguishability. While hardness indistinguishability enables the dissociation of parties' identities and the resources needed to corrupt them, IEP enables the discretization of adversarial work into corruption tokens, all of which may be of independent interest.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {415–428},
numpages = {14},
keywords = {exact hardness, secure multi-party computation, hardness amplification, combinatorial analysis, cost of corruption},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422484,
author = {Watson, Thomas},
title = {Time Hierarchies for Sampling Distributions},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422484},
doi = {10.1145/2422436.2422484},
abstract = {We show that "a little more time gives a lot more power to sampling algorithms." We prove that for every constant k ≥ 2, every polynomial time bound t, and every polynomially small ε, there exists a family of distributions on k elements that can be sampled exactly in polynomial time but cannot be sampled within statistical distance 1-1/k-ε in time t. This implies the following general time hierarchy for sampling distributions on arbitrary-size domains such as {0,1}n: For every polynomial time bound t and every constant ε&gt;0, there exists a family of distributions that can be sampled exactly in polynomial time but cannot be sampled within statistical distance 1-ε in time t. Our proof involves reducing the problem to a communication problem over a certain type of noisy channel. To solve the latter problem we use a type of list-decodable code for a setting where there is no bound on the number of errors but each error gives more information than an erasure. This type of code can be constructed using certain known traditional list-decodable codes, but we give a new construction that is elementary, self-contained, and tailored to this setting.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {429–440},
numpages = {12},
keywords = {distributions, time hierarchies, sampling},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422485,
author = {Tal, Avishay},
title = {Properties and Applications of Boolean Function Composition},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422485},
doi = {10.1145/2422436.2422485},
abstract = {For Boolean functions f:{0,1}n -&gt; {0,1} and g:{0,1}m -&gt; {0,1}, the function composition of f and g denoted by f O g : {0,1}nm -&gt; {0,1} is the value of f on n inputs, each of them is the calculation of g on a distinct set of m Boolean variables. Motivated by previous works that achieved some of the best separations between complexity measures such as sensitivity, block-sensitivity, degree, certificate complexity and decision tree complexity we show that most of these complexity measures behave multiplicatively under composition. We use this multiplicative behavior to establish several applications. First, we give a negative answer for Adam Kalai's question from [MOS04]: "Is it true that every Boolean function f:{0,1}n -&gt; {0,1} with degree as a polynomial over the reals (denoted by deg(f)) at most n/3, has a restriction fixing 2n/3 of its variables under which f becomes a parity function?" This question was motivated by the problem of learning juntas as it suggests a simple algorithm, faster than that of Mossel et al. We give a counterexample for the question using composition of functions strongly related to the Walsh-Hadamard code. In fact, we show that for every constants ε,δ&gt;0 there are (infinitely many) Boolean functions f: {0,1}n -&gt; {0,1} such that deg(f) ≤ ε ⋅ n and under any restriction fixing less than (1-δ) ⋅ n variables, f does not become a parity function.Second, we show that for composition, the block sensitivity (denoted by bs) property has an unusual behavior - namely that bs(f O g) can be larger than bs (f) ⋅ bs(g). We show that the ratio between these two has a strong connection to the integrality gap of the Set Packing problem. In addition, we obtain the best known separation between block-sensitivity and certificate complexity (denoted by C) giving infinitely many functions f such that C(f) ≥ bs (f)log(26)/log(17) = bs (f)1.149....Last, we present a factor 2 improvement of a result by Nisan and Szegedy [NS94], by showing that for all Boolean functions bs f ≤ deg(f)2.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {441–454},
numpages = {14},
keywords = {certificate complexity, block sensitivity, decision tree complexity, boolean functions},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422486,
author = {Bonacina, Ilario and Galesi, Nicola},
title = {Pseudo-Partitions, Transversality and Locality: A Combinatorial Characterization for the Space Measure in Algebraic Proof Systems},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422486},
doi = {10.1145/2422436.2422486},
abstract = {We devise a new combinatorial framework for proving space lower bounds in algebraic proof systems like Polynomial Calculus (Pc) and Polynomial Calculus with Resolution (Pcr). Our method can be thought as a Spoiler-Duplicator game, which is capturing boolean reasoning on polynomials instead that clauses as in the case of Resolution. Hence, for the first time, we move the problem of studying the space complexity for algebraic proof systems in the range of 2-players games, as is the case for Resolution.A very simple case of our method allows us to obtain all the currently known space lower bounds for Pc/Pcr (CTn, PHPmn, BIT-PHPmn, XOR-PHPmn). The way our method applies to all these examples explains how and why all the known examples of space lower bounds for Pc/Pcr are an application of the method originally given by [Alekhnovich et al 2002] that holds for set of contradictory polynomials having high degree. Our approach unifies in a clear way under a common combinatorial framework and language the proofs of the space lower bounds known so far for Pc/Pcr.More importantly, using our approach in its full potentiality, we answer to the open problem [Alekhnovich et al 2002, Filmus et al 2012] of proving space lower bounds in Polynomial Calculus and Polynomials Calculus with Resolution for the polynomial encoding of randomly chosen k-CNF formulas. Our result holds for k&gt;= 4. Then, as proved for Resolution in [BenSasson and Galesi 2003], also in Pc and in Pcr refuting a random k-CNF over n variables requires high space measure of the order of Omega(n). Our method also applies to the Graph-PHPmn, which is a PHPmn defined over a constant (left) degree bipartite expander graph. We develop a common language for the two examples.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {455–472},
numpages = {18},
keywords = {random k-CNF formulae, polynomial calculus, resolution, proof complexity, space complexity},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422487,
author = {Kol, Gillat and Raz, Ran},
title = {Competing Provers Protocols for Circuit Evaluation},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422487},
doi = {10.1145/2422436.2422487},
abstract = {Let C be a (fan-in 2) Boolean circuit of size s and depth d, and let x be an input for C. Assume that a verifier that knows C but doesn't know x can access the low degree extension of x at one random point. Two competing provers try to convince the verifier that C(x)=0 and C(x)=1, respectively, and assume that one of the provers is honest.For any r≥1, we give an r rounds protocol with communication complexity d1/r poly log(s) that convinces the verifier in the correct value of C(x) (with small probability of error). In particular, when we allow only one round, the protocol exchanges d ⋅ poly log(s) bits, and when we allow r=O(log(d)/log log(s)) rounds, the protocol exchanges only poly log(s) bits.Moreover, the complexity of the verifier and honest provers in this protocol is poly(s), and if in addition the circuit is log(s)-space uniform, the complexity of the verifier is d1/r poly log(s).The protocol is obtained by combining the delegation protocol of Goldwasser, Kalai and Rothblum [5] and the competing provers protocols of Feige and Kilian [3] and some new techniques. We suggest two applications of these results:Delegating computation to competing clouds: The main motivation behind the protocol of [5] was delegating computation to a cloud. Using our new protocol, a verifier can delegate computation to two (or more) competing clouds. If at least one of the clouds is reliable the verifier can trust that the computation is correct (with high probability). The advantage over the protocol of [5] is that the communication complexity and the number of rounds in our protocol are significantly lower.Communication complexity with competing provers, and circuit lower bounds: Aaronson and Wigderson [1] suggested the model of communication complexity with competing provers, where two competing provers try to convince two players that f(x,y)=0 and f(x,y)=1, respectively, where x is an input held by the first player and y is an input held by the second player. By scaling down the competing provers protocols of [3], they showed that strong enough lower bounds for the communication complexity of f, in this model, imply lower bounds for the computational complexity of f.Our results strengthen this connection. More precisely, we show that if f can be computed by a Boolean circuit of size s and depth d then for any r≥1 there is an r rounds protocol for f, in this model, with communication complexity d1/r poly log(s). This can be viewed as a possible direction towards proving circuit lower bounds. For instance, in order to prove f∉NC, it suffices to show that any one round protocol for f, in this model, requires the exchange of ω(poly log(n)) bits. This gives a relatively simple combinatorial property that implies strong circuit lower bounds.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {473–484},
numpages = {12},
keywords = {delegating computation, communication complexity, interactive proofs, competing provers, circuit lower bound},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422489,
author = {Cygan, Marek and Englert, Matthias and Gupta, Anupam and Mucha, Marcin and Sankowski, Piotr},
title = {Catch Them If You Can: How to Serve Impatient Users},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422489},
doi = {10.1145/2422436.2422489},
abstract = {Consider the following problem of serving impatient users: we are given a set of customers we would like to serve. We can serve at most one customer in each time step (getting value vi for serving customer i). At the end of each time step, each as-yet-unserved customer i leaves the system independently with probability qi, never to return. What strategy should we use to serve customers to maximize the expected value collected?The standard model of competitive analysis can be applied to this problem: picking the customer with maximum value gives us half the value obtained by the optimal algorithm, and using a vertex weighted online matching algorithm gives us 1-1/e ~ 0.632 fraction of the optimum. As is usual in competitive analysis, these approximations compare to the best value achievable by an clairvoyant adversary that knows all the coin tosses of the customers. Can we do better?We show an upper bound of ~0.648 if we compare our performance to such an clairvoyant algorithm, suggesting we cannot improve our performance substantially. However, these are pessimistic comparisons to a much stronger adversary: what if we compare ourselves to the optimal strategy for this problem, which does not have an unfair advantage? In this case, we can do much better: in particular, we give an algorithm whose expected value is at least 0.7 of that achievable by the optimal algorithm. This improvement is achieved via a novel rounding algorithm, and a non-local analysis.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {485–494},
numpages = {10},
keywords = {bipartite matching, impatience, online algorithms, stochastic models},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422490,
author = {Megow, Nicole and Mestre, Julian},
title = {Instance-Sensitive Robustness Guarantees for Sequencing with Unknown Packing and Covering Constraints},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422490},
doi = {10.1145/2422436.2422490},
abstract = {Sequencing problems with an unknown covering or packing constraint appear in various applications, e.g., in real-time computing environments with uncertain run-time availability. A sequence is called α-robust when, for any possible constraint, the maximal or minimal prefix of the sequence that satisfies the constraint is at most a factor α from an optimal packing or covering. It is known that the covering problem always admits a 4-robust solution, and there are instances for which this factor is tight. For the packing variant no such constant robustness factor is possible in general. In this work we address the fact that many problem instances may allow for a much better robustness guarantee than the pathological worst case instances. We aim for more meaningful, instance-sensitive performance guarantees. We present an algorithm that constructs for each instance a solution with a robustness factor arbitrarily close to optimal. This implies nearly optimal solutions for previously studied problems such as the universal knapsack problem and for universal scheduling on an unreliable machine. The crucial ingredient and main result is a nearly exact feasibility test for dual-value sequencing with a given target function. We show that deciding exact feasibility is strongly NP-hard, and thus, our test is best possible, unless P=NP.We hope that the idea of instance-sensitive performance guarantees inspires to revisit other optimization problems and design algorithm tailored to perform well for each individual instance.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {495–504},
numpages = {10},
keywords = {sequencing, knapsack, universal solution, instance-sensitive worst-case guarantee, robustness factor},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422491,
author = {Buhmann, Joachim M. and Mihalak, Matus and Sramek, Rastislav and Widmayer, Peter},
title = {Robust Optimization in the Presence of Uncertainty},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422491},
doi = {10.1145/2422436.2422491},
abstract = {We study optimization in the presence of uncertainty such as noise in measurements, and advocate a novel approach of tackling it. The main difference to any existing approach is that we do not assume any knowledge about the nature of the uncertainty (such as for instance a probability distribution). Instead, we are given several instances of the same optimization problem as input, and, assuming they are typical w.r.t. the uncertainty, we make use of it in order to compute a solution that is good for the sample instances as well as for future (unknown) typical instances.We demonstrate our approach for the case of two typical input instances. We first propose a measure of similarity of instances with respect to an objective. This concept allows us to assess whether instances are indeed typical. Based on this concept, we then choose a solution randomly among all solutions that are near-optimum for both instances. We show that the exact notion of near-optimum is intertwined with the proposed measure of similarity. Furthermore, we will show that our measure of similarity also allows us to derive formal statements about the expected quality of the computed solution: If the given instances are not similar, or are too noisy, our approach will detect this. We demonstrate for a few optimization problems and real world data that our approach works well not only in theory, but also in practice.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {505–514},
numpages = {10},
keywords = {optimization, instance similarity, robustness, noise, uncertainty},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422492,
author = {Makarychev, Konstantin and Makarychev, Yury and Vijayaraghavan, Aravindan},
title = {Sorting Noisy Data with Partial Information},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422492},
doi = {10.1145/2422436.2422492},
abstract = {In this paper, we propose two semi-random models for the Minimum Feedback Arc Set Problem and present approximation algorithms for them. In the first model, which we call the Random Edge Flipping model, an instance is generated as follows. We start with an arbitrary acyclic directed graph and then randomly flip its edges (the adversary may later un-flip some of them). In the second model, which we call the Random Backward Edge model, again we start with an arbitrary acyclic graph but now add new random backward edges (the adversary may delete some of them). For the first model, we give an approximation algorithm that finds a solution of cost (1+ δ) OPT + n polylog n, where OPT is the cost of the optimal solution. For the second model, we give an approximation algorithm that finds a solution of cost O(planted) + n polylog n, where planted is the cost of the planted solution. Additionally, we present an approximation algorithm for semi-random instances of Minimum Directed Balanced Cut.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {515–528},
numpages = {14},
keywords = {semi-random model, approximation algorithm, minimum feedback arc set, average-case analysis},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422494,
author = {Guo, Alan and Kopparty, Swastik and Sudan, Madhu},
title = {New Affine-Invariant Codes from Lifting},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422494},
doi = {10.1145/2422436.2422494},
abstract = {In this work we explore error-correcting codes derived from the "lifting" of "affine-invariant" codes. Affine-invariant codes are simply linear codes whose coordinates are a vector space over a field and which are invariant under affine-transformations of the coordinate space. Lifting takes codes defined over a vector space of small dimension and lifts them to higher dimensions by requiring their restriction to every subspace of the original dimension to be a codeword of the code being lifted. While the operation is of interest on its own, this work focusses on new ranges of parameters that can be obtained by such codes, in the context of local correction and testing. In particular we present four interesting ranges of parameters that can be achieved by such lifts, all of which are new in the context of affine-invariance and some may be new even in general. The main highlight is a construction of high-rate codes with sublinear time decoding. The only prior construction of such codes is due to Kopparty, Saraf and Yekhanin [33]. All our codes are extremely simple, being just lifts of various parity check codes (codes with one symbol of redundancy), and in the final case, the lift of a Reed-Solomon code.We also present a simple connection between certain lifted codes and lower bounds on the size of "Nikodym sets". Roughly, a Nikodym set in Fqm is a set S with the property that every point has a line passing through it which is almost entirely contained in S. While previous lower bounds on Nikodym sets were roughly growing as qm/2m, we use our lifted codes to prove a lower bound of (1 - o(1))qm for fields of constant characteristic.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {529–540},
numpages = {12},
keywords = {locally correctible codes, locally testable codes, property testing, lifting, affine-invariance, nikodym sets},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422495,
author = {Haviv, Ishay and Langberg, Michael},
title = {H-Wise Independence},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422495},
doi = {10.1145/2422436.2422495},
abstract = {For a hypergraph H on the vertex set {1,...,n}, a distribution D = (D_1,...,D_n) over {0,1}^n is H-wise independent if every restriction of D to indices which form an edge in H is uniform. This generalizes the notion of k-wise independence obtained by taking H to be the complete n vertex k-uniform hypergraph. This generalization was studied by Schulman (STOC 1992), who presented constructions of H-wise independent distributions that are linear, i.e., the samples are strings of inner products (over F2) of a fixed set of vectors with a uniformly chosen random vector. Let l(H) denote the minimum possible size of a sample space of a uniform H-wise independent distribution. The l parameter is well understood for the special case of k-wise independence. In this work we study the notion of H-wise independence and the l parameter for general graphs and hypergraphs. For graphs, we show how the l parameter relates to standard graph parameters (e.g., clique number, chromatic number, Lovasz theta function, minrank). We derive algorithmic and hardness results for this parameter as well as an explicit construction of graphs G for which l(G) is exponentially smaller than the size of the sample space of any linear G-wise independent distribution. For hypergraphs, we study the problem of testing whether a given distribution is H-wise independent, generalizing results of Alon et al. (STOC 2007).},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {541–552},
numpages = {12},
keywords = {k-wise independence, derandomization, h-wise independence},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422496,
author = {Bogdanov, Andrej and Guo, Siyao},
title = {Sparse Extractor Families for All the Entropy},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422496},
doi = {10.1145/2422436.2422496},
abstract = {We consider the problem of extracting entropy by sparse transformations, namely functions with a small number of overall input-output dependencies. In contrast to previous works, we seek extractors for essentially all the entropy without any assumption on the underlying distribution beyond a min-entropy requirement. We give two simple constructions of sparse extractor families. These are collections of sparse functions such that for any distribution X on inputs of sufficiently high min-entropy, the output of most functions from the collection on input X is statistically close to uniform.For strong extractor families (i.e., functions in the family do not take additional randomness) we give upper and lower bounds on the sparsity that are tight up to a constant factor for a wide range of min-entropies. We then prove that for some min-entropies weak extractor families can achieve better sparsity.We show how this construction can be used towards more efficient parallel transformation of (non-uniform) one-way functions into pseudorandom generators. More generally, sparse extractor families can be used instead of pairwise independence in various randomized or nonuniform settings where sparsity or preserving locality (i.e., parallelism) is of interest.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {553–560},
numpages = {8},
keywords = {randomness extraction, parallel cryptography, random matrices},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

@inproceedings{10.1145/2422436.2422497,
author = {Chakraborty, Sourav and Fischer, Eldar and Goldhirsh, Yonatan and Matsliah, Arie},
title = {On the Power of Conditional Samples in Distribution Testing},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2422436.2422497},
doi = {10.1145/2422436.2422497},
abstract = {In this paper we define and examine the power of the conditional sampling oracle in the context of distribution-property testing. The conditional sampling oracle for a discrete distribution μ takes as input a subset S ⊂ [n] of the domain, and outputs a random sample i ∈ S drawn according to μ, conditioned on S (and independently of all prior samples). The conditional-sampling oracle is a natural generalization of the ordinary sampling oracle in which S always equals [n]. We show that with the conditional-sampling oracle, testing uniformity, testing identity to a known distribution, and testing any label-invariant property of distributions is easier than with the ordinary sampling oracle. On the other hand, we also show that for some distribution properties the sample complexity remains near-maximal even with conditional sampling.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {561–580},
numpages = {20},
keywords = {distribution testing, statistical approximation, conditional samples},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

