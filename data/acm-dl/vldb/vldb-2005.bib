@inproceedings{10.5555/1083592.1083594,
author = {Olstad, Bj\o{}rn},
title = {Why Search Engines Are Used Increasingly to Offload Queries from Databases},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {The development of future search engine technology is no longer limited to free text. Rather, the aim is to build core indexing services that focus on extreme performance and scalability for retrieval and analysis across structured and unstructured data sources alike. In addition, binary query evaluation is being replaced with advanced frameworks that provide both fuzzy matching and ranking schemes, to separate value from noise. As another trend, analytical applications are being enabled by the computation of contextual concept relationships across billions of documents/records on-the-fly.Based on these developments in search engine technology, a set of new information retrieval infrastructure patterns are appearing:1. the mirroring of DB content into a search engine in order to improve query capacity and user experience,2. the use of search engine technology as the default access pattern to both structured and unstructured data in applications such as CRM and storage and document management, and3. a paradigm shift is predicted in business intelligence.The presentation will review key trends from search engine development and relate these to concrete user scenarios.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1},
numpages = {1},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083596,
author = {Konopnicki, David and Shmueli, Oded},
title = {Database-Inspired Search},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {"W3QL: A Query Language for the WWW", published in 1995, presented a language with several distinctive features. Employing existing indexes as access paths, it allowed the selection of documents using conditions on semi-structured documents and maintaining dynamic views of navigational queries. W3QL was capable of automatically filling out forms and navigating through them. Finally, in the SQL tradition, it was a declarative query language, that could be the subject of optimization.Ten years later, we examine some current trends in the domain of search, namely the emergence of system-level search services and of the semantic web. In this context, we explore whether W3QL's ideas are still relevant to help improve information search and retrieval. We identify two main environments for searching, the enterprise and the web at large. Both environments could benefit from database-inspired integration language, and an execution system that implements it.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {2–12},
numpages = {11},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@dataset{10.1145/review-1083592.1083596_R40544,
author = {Hankley, William J.},
title = {Review ID:R40544 for DOI: 10.5555/1083592.1083596},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1083592.1083596_R40544}
}

@inproceedings{10.5555/1083592.1083598,
author = {Cormode, Graham and Garofalakis, Minos},
title = {Sketching Streams through the Net: Distributed Approximate Query Tracking},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Emerging large-scale monitoring applications require continuous tracking of complex data-analysis queries over collections of physically-distributed streams. Effective solutions have to be simultaneously space/time efficient (at each remote monitor site), communication efficient (across the underlying communication network), and provide continuous, guaranteed-quality approximate query answers. In this paper, we propose novel algorithmic solutions for the problem of continuously tracking a broad class of complex aggregate queries in such a distributed-streams setting. Our tracking schemes maintain approximate query answers with provable error guarantees, while simultaneously optimizing the storage space and processing time at each remote site, and the communication cost across the network. They rely on tracking general-purpose randomized sketch summaries of local streams at remote sites along with concise prediction models of local site behavior in order to produce highly communication- and space/time-efficient solutions. The result is a powerful approximate query tracking framework that readily incorporates several complex analysis queries (including distributed join and multi-join aggregates, and approximate wavelet representations), thus giving the first known low-overhead tracking solution for such queries in the distributed-streams model.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {13–24},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083599,
author = {Cormode, Graham and Muthukrishnan, S. and Rozenbaum, Irina},
title = {Summarizing and Mining Inverse Distributions on Data Streams via Dynamic Inverse Sampling},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Emerging data stream management systems approach the challenge of massive data distributions which arrive at high speeds while there is only small storage by summarizing and mining the distributions using samples or sketches. However, data distributions can be "viewed" in different ways. A data stream of integer values can be viewed either as the forward distribution f (x), ie., the number of occurrences of x in the stream, or as its inverse, f-1 (i), which is the number of items that appear i times. While both such "views" are equivalent in stored data systems, over data streams that entail approximations, they may be significantly different. In other words, samples and sketches developed for the forward distribution may be ineffective for summarizing or mining the inverse distribution. Yet, many applications such as IP traffic monitoring naturally rely on mining inverse distributions.We formalize the problems of managing and mining inverse distributions and show provable differences between summarizing the forward distribution vs the inverse distribution. We present methods for summarizing and mining inverse distributions of data streams: they rely on a novel technique to maintain a dynamic sample over the stream with provable guarantees which can be used for variety of summarization tasks (building quantiles or equidepth histograms) and mining (anomaly detection: finding heavy hitters, and measuring the number of rare items), all with provable guarantees on quality of approximations and time/space used by our streaming methods.We also complement our analytical and algorithmic results by presenting an experimental study of the methods over network data streams.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {25–36},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083600,
author = {Cheng, Reynold and Kao, Ben and Prabhakar, Sunil and Kwan, Alan and Tu, Yicheng},
title = {Adaptive Stream Filters for Entity-Based Queries with Non-Value Tolerance},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {We study the problem of applying adaptive filters for approximate query processing in a distributed stream environment. We propose filter bound assignment protocols with the objective of reducing communication cost. Most previous works focus on value-based queries (e.g., average) with numerical error tolerance. In this paper, we cover entity-based queries (e.g., nearest neighbor) with non-value-based error tolerance. We investigate different non-value-based error tolerance definitions and discuss how they are applied to two classes of entity-based queries: non-rank-based and rank-based queries. Extensive experiments show that our protocols achieve significant savings in both communication overhead and server computation.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {37–48},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083602,
author = {Zhou, Jingren and Cieslewicz, John and Ross, Kenneth A. and Shah, Mihir},
title = {Improving Database Performance on Simultaneous Multithreading Processors},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Simultaneous multithreading (SMT) allows multiple threads to supply instructions to the instruction pipeline of a superscalar processor. Because threads share processor resources, an SMT system is inherently different from a multiprocessor system and, therefore, utilizing multiple threads on an SMT processor creates new challenges for database implementers.We investigate three thread-based techniques to exploit SMT architectures on memory-resident data. First, we consider running independent operations in separate threads, a technique applied to conventional multi-processor systems. Second, we describe a novel implementation strategy in which individual operators are implemented in a multi-threaded fashion. Finally, we introduce a new data-structure called a work-ahead set that allows us to use one of the threads to aggressively preload data into the cache.We evaluate each method with respect to its performance, implementation complexity, and other measures. We also provide guidance regarding when and how to best utilize the various threading techniques. Our experimental results show that by taking advantage of SMT technology we achieve a 30% to 70% improvement in throughput over single threaded implementations on in-memory database operations.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {49–60},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083603,
author = {Raman, Vijayshankar and Han, Wei and Narang, Inderpal},
title = {Parallel Querying with Non-Dedicated Computers},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {We present DITN, a new method of parallel querying based on dynamic outsourcing of join processing tasks to non-dedicated, heterogeneous computers. In DITN, partitioning is not the means of parallelism. Data layout decisions are taken outside the scope of the DBMS, and handled within the storage software; query processors see a "Data In The Network" image. This allows gradual scaleout as the workload grows, by using non-dedicated computers.A typical operator in a parallel query plan is Exchange [7]. We argue that Exchange is unsuitable for non-dedicated machines because it poorly addresses node heterogeneity, and is vulnerable to failures or load spikes during query execution. DITN uses an alternate intra-fragment parallelism where each node executes an independent select-project-join-aggregate-group by block, with no tuple exchange between nodes. This method cleanly handles heterogeneous nodes, and well adapts during execution to node failures or load spikes.Initial experiments suggest that DITN performs competitively with a traditional configuration of dedicated machines and well-partitioned data for up to 10 processors at least. At the same time, DITN gives significant flexibility in terms of gradual scaleout and handling of heterogeneity, load bursts, and failures.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {61–72},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083604,
author = {Colohan, Christopher B. and Ailamaki, Anastassia and Steffan, J. Gregory and Mowry, Todd C.},
title = {Optimistic Intra-Transaction Parallelism on Chip Multiprocessors},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {With the advent of chip multiprocessors, exploiting intra-transaction parallelism is an attractive way of improving transaction performance. However, exploiting intra-transaction parallelism in existing database systems is difficult, for two reasons: first, significant changes are required to avoid races or conflicts within the DBMS, and second, adding threads to transactions requires a high level of sophistication from transaction programmers. In this paper we show how dividing a transaction into speculative threads solves both problems---it minimizes the changes required to the DBMS, and the details of parallelization are hidden from the transaction programmer. Our technique requires a limited number of small, localized changes to a subset of the low-level data structures in the DBMS. Through this method of parallelizing transactions we can dramatically improve performance: on a simulated 4-processor chip-multiprocessor, we improve the response time by 36-74% for three of the five TPC-C transactions.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {73–84},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083606,
author = {Bohannon, Philip and Fan, Wenfei and Flaster, Michael and Narayan, P. P. S.},
title = {Information Preserving XML Schema Embedding},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {A fundamental concern of information integration in an XML context is the ability to embed one or more source documents in a target document so that (a) the target document conforms to a target schema and (b) the information in the source document(s) is preserved. In this paper, information preservation for XML is formally studied, and the results of this study guide the definition of a novel notion of schema embedding between two XML DTD schemas represented as graphs. Schema embedding generalizes the conventional notion of graph similarity by allowing an edge in a source DTD schema to be mapped to a path in the target DTD. Instance-level embeddings can be defined from the schema embedding in a straightforward manner, such that conformance to a target schema and information preservation are guaranteed. We show that it is NP-complete to find an embedding between two DTD schemas. We also provide efficient heuristic algorithms to find candidate embeddings, along with experimental results to evaluate and compare the algorithms. These yield the first systematic and effective approach to finding information preserving XML mappings.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {85–96},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083607,
author = {Zhang, Zhen and He, Bin and Chang, Kevin Chen-Chuan},
title = {Light-Weight Domain-Based Form Assistant: Querying Web Databases on the Fly},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {The Web has been rapidly "deepened" by myriad searchable databases online, where data are hidden behind query forms. Helping users query alternative "deep Web" sources in the same domain (e.g., Books, Airfares) is an important task with broad applications. As a core component of those applications, dynamic query translation (i.e., translating a user's query across dynamically selected sources) has not been extensively explored. While existing works focus on isolated subproblems (e.g., schema matching, query rewriting) to study, we target at building a complete query translator and thus face new challenges: 1) To complete the translator, we need to solve the predicate mapping problem (i.e., map a source predicate to target predicates), which is largely unexplored by existing works; 2) To satisfy our application requirements, we need to design a customizable system architecture to assemble various components addressing respective subproblems (i.e., schema matching, predicate mapping, query rewriting). Tackling these challenges, we develop a light-weight domain-based form assistant, which can generally handle alternative sources in the same domain and is easily customizable to new domains. Our experiment shows the effectiveness of our form assistant in translating queries for real Web sources.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {97–108},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083608,
author = {Barbosa, Denilson and Freire, Juliana and Mendelzon, Alberto O.},
title = {Designing Information-Preserving Mapping Schemes for XML},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {An XML-to-relational mapping scheme consists of a procedure for shredding documents into relational databases, a procedure for publishing databases back as documents, and a set of constraints the databases must satisfy. In previous work, we defined two notions of information preservation for mapping schemes: losslessness, which guarantees that any document can be reconstructed from its corresponding database; and validation, which requires every legal database to correspond to a valid document. We also described one information-preserving mapping scheme, called Edge++, and showed that, under reasonable assumptions, losslessness and validation are both undecidable. This leads to the question we study in this paper: how to design mapping schemes that are information-preserving. We propose to do it by starting with a scheme known to be information-preserving and applying to it equivalence-preserving transformations written in weakly recursive ILOG. We study an instance of this framework, the LILO algorithm, and show that it provides significant performance improvements over Edge++ and introduces constraints that are efficiently enforced in practice.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {109–120},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083610,
author = {Xu, Wanhong and \"{O}zsoyoglu, Z. Meral},
title = {Rewriting XPath Queries Using Materialized Views},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {As a simple XML query language but with enough expressive power, XPath has become very popular. To expedite evaluation of XPath queries, we consider the problem of rewriting XPath queries using materialized XPath views. This problem is very important and arises not only from query optimization in server side but also from semantic caching in client side. We consider the problem of deciding whether there exists a rewriting of a query using XPath views and the problem of finding minimal rewritings. We first consider those two problems for a very practical XPath fragment containing the descendent, child, wildcard and branch features. We show that the rewriting existence problem is coNP-hard and the problem of finding minimal rewritings is Σp3. We also consider those two rewriting problems for three subclasses of this XPath fragment, each of which contains child feature and two of descendent, wildcard and branch features, and show that both rewriting problems can be polynomially solved. Finally, we give an algorithm for finding minimal rewritings, which is sound for the XPath fragment, but is also complete and runs in polynomial time for its three subclasses.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {121–132},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083611,
author = {Barta, Attila and Consens, Mariano P. and Mendelzon, Alberto O.},
title = {Benefits of Path Summaries in an XML Query Optimizer Supporting Multiple Access Methods},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {We compare several optimization strategies implemented in an XML query evaluation system. The strategies incorporate the use of path summaries into the query optimizer, and rely on heuristics that exploit data statistics.We present experimental results that demonstrate a wide range of performance improvements for the different strategies supported. In addition, we compare the speedups obtained using path summaries with those reported for index-based methods. The comparison shows that low-cost path summaries combined with optimization strategies achieve essentially the same benefits as more expensive index structures.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {133–144},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083612,
author = {Wang, Wei and Jiang, Haifeng and Wang, Hongzhi and Lin, Xuemin and Lu, Hongjun and Li, Jianzhong},
title = {Efficient Processing of XML Path Queries Using the Disk-Based F&amp;B Index},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {With the proliferation of XML data and applications on the Internet, efficient XML query processing techniques are in great demand. Answering queries using XML indexes is a natural approach. A number of XML indexes have been proposed in the literature: among them, F&amp;B Index is one powerful index as it is the smallest index that answers all twig queries. However, an F&amp;B Index suffers from the following two problems: (1) it was originally proposed as a memory-based index while its size is usually large in practice and (2) answering queries using an F&amp;B Index is not fully optimized. These problems limit the benefits and even applications of F&amp;B Indexes in practice.In this paper, we propose a highly optimized disk organization method for an F&amp;B Index; the result is a disk-based F&amp;B Index with good clustering properties. In addition, novel query processing algorithms exploiting the physical organization of the disk-based F&amp;B Indexes are proposed. Experimental results verify that our disk-based F&amp;B Index can scale up for large data size with good query performance compared with state-of-the-art XML query processing algorithms.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {145–156},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083614,
author = {Ivanova, Milena and Risch, Tore},
title = {Customizable Parallel Execution of Scientific Stream Queries},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Scientific applications require processing high-volume on-line streams of numerical data from instruments and simulations. We present an extensible stream database system that allows scalable and flexible continuous queries on such streams. Application dependent streams and query functions are defined through an object-relational model. Distributed execution plans for continuous queries are described as high-level data flow distribution templates. Using a generic template we define two partitioning strategies for scalable parallel execution of expensive stream queries: window split and window distribute. Window split provides operators for parallel execution of query functions by reducing the size of stream data units using application dependent functions as parameters. By contrast, window distribute provides operators for customized distribution of entire data units without reducing their size. We evaluate these strategies for a typical high volume scientific stream application and show that window split is favorable when expensive queries are executed on limited resources, while window distribution is better otherwise.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {157–168},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083615,
author = {Metwally, Ahmed and Agrawal, Divyakant and Abbadi, Amr El},
title = {Using Association Rules for Fraud Detection in Web Advertising Networks},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Discovering associations between elements occurring in a stream is applicable in numerous applications, including predictive caching and fraud detection. These applications require a new model of association between pairs of elements in streams. We develop an algorithm, Streaming-Rules, to report association rules with tight guarantees on errors, using limited processing per element, and minimal space. The modular design of Streaming-Rules allows for integration with current stream management systems, since it employs existing techniques for finding frequent elements. The presentation emphasizes the applicability of the algorithm to fraud detection in advertising networks. Such fraud instances have not been successfully detected by current techniques. Our experiments on synthetic data demonstrate scalability and efficiency. On real data, potential fraud was discovered.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {169–180},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083616,
author = {Fung, Gabriel Pui Cheong and Yu, Jeffrey Xu and Yu, Philip S. and Lu, Hongjun},
title = {Parameter Free Bursty Events Detection in Text Streams},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Text classification is a major data mining task. An advanced text classification technique is known as partially supervised text classification, which can build a text classifier using a small set of positive examples only. This leads to our curiosity whether it is possible to find a set of features that can be used to describe the positive examples. Therefore, users do not even need to specify a set of positive examples. As the first step, in this paper, we formalize it as a new problem, called hot bursty events detection, to detect bursty events from a text stream which is a sequence of chronologically ordered documents. Here, a bursty event is a set of bursty features, and is considered as a potential category to build a text classifier. It is important to know that the hot bursty events detection problem, we study in this paper, is different from TDT (topic detection and tracking) which attempts to cluster documents as events using clustering techniques. In other words, our focus is on detecting a set of bursty features for a bursty event. In this paper, we propose a new novel parameter free probabilistic approach, called feature-pivot clustering. Our main technique is to fully utilize the time information to determine a set of bursty features which may occur in differenttime windows. We detect bursty events based on the feature distributions. There is no need to tune or estimate any parameters. We conduct experiments using real life data, a major English newspaper in Hong Kong, and show that the parameter free feature-pivot clustering approach can detect the bursty events with a high success rate.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {181–192},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083618,
author = {Lu, Jiaheng and Ling, Tok Wang and Chan, Chee-Yong and Chen, Ting},
title = {From Region Encoding to Extended Dewey: On Efficient Processing of XML Twig Pattern Matching},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Finding all the occurrences of a twig pattern in an XML database is a core operation for efficient evaluation of XML queries. A number of algorithms have been proposed to process a twig query based on region encoding labeling scheme. While region encoding supports efficient determination of structural relationship between two elements, we observe that the information within a single label is very limited. In this paper, we propose a new labeling scheme, called extended Dewey. This is a powerful labeling scheme, since from the label of an element alone, we can derive all the elements names along the path from the root to the element. Based on extended Dewey, we design a novel holistic twig join algorithm, called TJFast. Unlike all previous algorithms based on region encoding, to answer a twig query, TJFast only needs to access the labels of the leaf query nodes. Through this, not only do we reduce disk access, but we also support the efficient evaluation of queries with wildcards in branching nodes, which is very difficult to be answered by algorithms based on region encoding. Finally, we report our experimental results to show that our algorithms are superior to previous approaches in terms of the number of elements scanned, the size of intermediate results and query performance.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {193–204},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083619,
author = {Moro, Mirella M. and Vagena, Zografoula and Tsotras, Vassilis J.},
title = {Tree-Pattern Queries on a Lightweight XML Processor},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Popular XML languages, like XPath, use "tree-pattern" queries to select nodes based on their structural characteristics. While many processing methods have already been proposed for such queries, none of them has found its way to any of the existing "lightweight" XML engines (i.e. engines without optimization modules). The main reason is the lack of a systematic comparison of query methods under a common storage model. In this work, we aim to fill this gap and answer two important questions: what the relative similarities and important differences among the tree-pattern query methods are, and if there is a prominent method among them in terms of effectiveness and robustness that an XML processor should support. For the first question, we propose a novel classification of the methods according to their matching process. We then describe a common storage model and demonstrate that the access pattern of each class conforms or can be adapted to conform to this model. Finally, we perform an experimental evaluation to compare their relative performance. Based on the evaluation results, we conclude that the family of holistic processing methods, which provides performance guarantees, is the most robust alternative for such an environment.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {205–216},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083620,
author = {Kwon, Joonho and Rao, Praveen and Moon, Bongki and Lee, Sukho},
title = {FiST: Scalable XML Document Filtering by Sequencing Twig Patterns},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In recent years, publish-subscribe (pub-sub) systems based on XML document filtering have received much attention. In a typical pub-sub system, subscribed users specify their interest in profiles expressed in the XPath language, and each new content is matched against the user profiles so that the content is delivered to only the interested subscribers. As the number of subscribed users and their profiles can grow very large, the scalability of the system is critical to the success of pub-sub services. In this paper, we propose a novel scalable filtering system called FiST (Filtering by Sequencing Twigs) that transforms twig patterns expressed in XPath and XML documents into sequences using Pr\"{u}fer's method. As a consequence, instead of matching linear paths of twig patterns individually and merging the matches during post-processing, FiST performs holistic matching of twig patterns with incoming documents. FiST organizes the sequences into a dynamic hash based index for efficient filtering. We demonstrate that our holistic matching approach yields lower filtering cost and good scalability under various situations.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {217–228},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083622,
author = {Godfrey, Parke and Shipley, Ryan and Gryz, Jarek},
title = {Maximal Vector Computation in Large Data Sets},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Finding the maximals in a collection of vectors is relevant to many applications. The maximal set is related to the convex hull---and hence, linear optimization---and nearest neighbors. The maximal vector problem has resurfaced with the advent of skyline queries for relational databases and skyline algorithms that are external and relationally well behaved.The initial algorithms proposed for maximals are based on divide-and-conquer. These established good average and worst case asymptotic running times, showing it to be O(n) average-case. where n is the number of vectors. However, they are not amenable to externalizing. We prove, furthermore, that their performance is quite bad with respect to the dimensionality, k, of the problem. We demonstrate that the more recent external skyline algorithms are actually better behaved, although they do not have as good an apparent asymptotic complexity. We introduce a new external algorithm, LESS, that combines the best features of these. experimentally evaluate its effectiveness and improvement over the field, and prove its average-case running time is O(kn).},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {229–240},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083623,
author = {Yuan, Yidong and Lin, Xuemin and Liu, Qing and Wang, Wei and Yu, Jeffrey Xu and Zhang, Qing},
title = {Efficient Computation of the Skyline Cube},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Skyline has been proposed as an important operator for multi-criteria decision making, data mining and visualization, and user-preference queries. In this paper. we consider the problem of efficiently computing a SKYCUBE, which consists of skylines of all possible non-empty subsets of a given set of dimensions. While existing skyline computation algorithms can be immediately extended to computing each skyline query independently, such "shared-nothing" algorithms are inefficient. We develop several computation sharing strategies based on effectively identifying the computation dependencies among multiple related skyline queries. Based on these sharing strategies, two novel algorithms, Bottom-Up and Top-Down algorithms, are proposed to compute SKYCUBE efficiently. Finally, our extensive performance evaluations confirm the effectiveness of the sharing strategies. It is shown that new algorithms significantly outperform the na\"{\i}ve ones.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {241–252},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083624,
author = {Pei, Jian and Jin, Wen and Ester, Martin and Tao, Yufei},
title = {Catching the Best Views of Skyline: A Semantic Approach Based on Decisive Subspaces},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {The skyline operator is important for multi-criteria decision making applications. Although many recent studies developed efficient methods to compute skyline objects in a specific space, the fundamental problem on the semantics of skylines remains open: Why and in which subspaces is (or is not) an object in the skyline? Practically, users may also be interested in the skylines in any subspaces. Then, what is the relationship between the skylines in the subspaces and those in the super-spaces? How can we effectively analyze the subspace skylines? Can we efficiently compute skylines in various subspaces?In this paper, we investigate the semantics of skylines, propose the subspace skyline analysis, and extend the full-space skyline computation to subspace skyline computation. We introduce a novel notion of skyline group which essentially is a group of objects that are coincidentally in the skylines of some subspaces. We identify the decisive subspaces that qualify skyline groups in the subspace skylines. The new notions concisely capture the semantics and the structures of skylines in various subspaces. Multidimensional roll-up and drilldown analysis is introduced. We also develop an efficient algorithm, Skyey, to compute the set of skyline groups and, for each subspace, the set of objects that are in the subspace skyline. A performance study is reported to evaluate our approach.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {253–264},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083626,
author = {Li, Xiaogang and Agrawal, Gagan},
title = {Efficient Evaluation of XQuery over Streaming Data},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {With the growing popularity of XML and emergence of streaming data model, processing queries over streaming XML has become an important topic. This paper presents a new framework and a set of techniques for processing XQuery over streaming data. As compared to the existing work on supporting XPath/XQuery over data streams, we make the following three contributions:1. We propose a series of optimizations which transform XQuery queries so that they can be correctly executed with a single pass on the dataset.2. We present a methodology for determining when an XQuery query, possibly after the transformations we introduce, can be correctly executed with only a single pass on the dataset.3. We describe a code generation approach which can handle XQuery queries with user-defined aggregates, including recursive functions. We aggressively use static analysis and generate executable code, i.e., do not require a query plan to be interpreted at runtime.We have evaluated our implementation using several XMark benchmarks and three other XQuery queries driven by real applications. Our experimental results show that as compared to Qizx/Open, Saxon, and Galax, our system: 1) is at least 25% faster on XMark queries with small datasets, 2) is significantly faster on XMark queries with larger datasets, 3) at least one order of magnitude faster on the queries driven by real applications, as unlike other systems, we can transform them to execute with a single pass, and 4) executes queries efficiently on large datasets when other systems often have memory overflows.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {265–276},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083627,
author = {Su, Hong and Rundensteiner, Elke A. and Mani, Murali},
title = {Semantic Query Optimization for XQuery over XML Streams},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {277–288},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083628,
author = {Zhang, Ning and Haas, Peter J. and Josifovski, Vanja and Lohman, Guy M. and Zhang, Chun},
title = {Statistical Learning Techniques for Costing XML Queries},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Developing cost models for query optimization is significantly harder for XML queries than for traditional relational queries. The reason is that XML query operators are much more complex than relational operators such as table scans and joins. In this paper, we propose a new approach, called COMET, to modeling the cost of XML operators; to our knowledge, COMET is the first method ever proposed for addressing the XML query costing problem. As in relational cost estimation, COMET exploits a set of system catalog statistics that summarizes the XML data; the set of "simple path" statistics that we propose is new, and is well suited to the XML setting. Unlike the traditional approach, COMET uses a new statistical learning technique called "transform regression" instead of detailed analytical models to predict the overall cost. Besides rendering the cost estimation problem tractable for XML queries, COMET has the further advantage of enabling the query optimizer to be self-tuning, automatically adapting to changes over time in the query workload and in the system environment. We demonstrate COMET's feasibility by developing a cost model for the recently proposed XNAV navigational operator. Empirical studies with synthetic, benchmark, and real-world data sets show that COMET can quickly obtain accurate cost estimates for a variety of XML queries and data sets.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {289–300},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@dataset{10.1145/review-1083592.1083628_R40185,
author = {Putnam, Jeffrey B.},
title = {Review ID:R40185 for DOI: 10.5555/1083592.1083628},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1083592.1083628_R40185}
}

@inproceedings{10.5555/1083592.1083630,
author = {Augsten, Nikolaus and B\"{o}hlen, Michael and Gamper, Johann},
title = {Approximate Matching of Hierarchical Data Using <i>Pq</i>-Grams},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {When integrating data from autonomous sources, exact matches of data items that represent the same real world object often fail due to a lack of common keys. Yet in many cases structural information is available and can be used to match such data. As a running example we use residential address information. Addresses are hierarchical structures and are present in many databases. Often they are the best, if not only, relationship between autonomous data sources. Typically the matching has to be approximate since the representations in the sources differ.We propose pq-grams to approximately match hierarchical information from autonomous sources. We define the pq-gram distance between ordered labeled trees as an effective and efficient approximation of the well-known tree edit distance. We analyze the properties of the pq-gram distance and compare it with the edit distance and alternative approximations. Experiments with synthetic and real world data confirm the analytic results and the scalability of our approach.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {301–312},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083631,
author = {Ercegovac, Vuk and DeWitt, David J. and Ramakrishnan, Raghu},
title = {The TEXTURE Benchmark: Measuring Performance of Text Queries on a Relational DBMS},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {We introduce a benchmark called TEXTURE (TEXT Under RElations) to measure the relative strengths and weaknesses of combining text processing with a relational workload in an RDBMS. While the well-known TREC benchmarks focus on quality, we focus on efficiency. TEXTURE is a micro-benchmark for query workloads, and considers two central text support issues that previous benchmarks did not: (1) queries with relevance ranking, rather than those that just compute all answers, and (2) a richer mix of text and relational processing, reflecting the trend toward seamless integration. In developing this benchmark, we had to address the problem of generating large text collections that reflected the (performance) characteristics of a given "seed" collection; this is essential for a controlled study of specific data characteristics and their effects on performance. In addition to presenting the benchmark, with performance numbers for three commercial DBMSs, we present and validate a synthetic generator for populating text fields.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {313–324},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083632,
author = {Kim, Min-Soo and Whang, Kyu-Young and Lee, Jae-Gil and Lee, Min-Jae},
title = {N-Gram/2L: A Space and Time Efficient Two-Level n-Gram Inverted Index Structure},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {The n-gram inverted index has two major advantages: language-neutral and error-tolerant. Due to these advantages, it has been widely used in information retrieval or in similar sequence matching for DNA and protein databases. Nevertheless, the n-gram inverted index also has drawbacks: the size tends to be very large, and the performance of queries tends to be bad. In this paper, we propose the two-level n-gram inverted index (simply, the n-gram/2L index) that significantly reduces the size and improves the query performance while preserving the advantages of the n-gram inverted index. The proposed index eliminates the redundancy of the position information that exists in the n-gram inverted index. The proposed index is constructed in two steps: 1) extracting subsequences of length m from documents and 2) extracting n-grams from those subsequences. We formally prove that this two-step construction is identical to the relational normalization process that removes the redundancy caused by a non-trivial multivalued dependency. The n-gram/2L index has excellent properties: 1) it significantly reduces the size and improves the performance compared with the n-gram inverted index with these improvements becoming more marked as the database size gets larger; 2) the query processing time increases only very slightly as the query length gets longer. Experimental results using databases of 1 GBytes show that the size of the n-gram/2L index is reduced by up to 1.9 ~ 2.7 times and, at the same time, the query performance is improved by up to 13.1 times compared with those of the n-gram inverted index.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {325–336},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083634,
author = {Fan, Wenfei and Yu, Jeffrey Xu and Lu, Hongjun and Lu, Jianhua and Rastogi, Rajeev},
title = {Query Translation from XPATH to SQL in the Presence of Recursive DTDs},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {The interaction between recursion in XPATH and recursion in DTDS makes it challenging to answer XPATH queries on XML data that is stored in an RDBMS via schema-based shredding. We present a new approach to translating XPATH queries into SQL queries with a simple least fixpoint (LFP) operator, which is already supported by most commercial RDBMS. The approach is based on our algorithm for rewriting XPATH queries into regular XPATH expressions, which are capable of capturing both DTD recursion and XPATH queries in a uniform framework. Furthermore, we provide an algorithm for translating regular XPATH queries to SQL queries with LFP, and optimization techniques for minimizing the use of the LFP operator. The novelty of our approach consists in its capability to answer a large class of XPATH queries by means of only low-end RDBMS features already available in most RDBMS. Our experimental results verify the effectiveness of our techniques.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {337–348},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083635,
author = {Paparizos, Stelios and Jagadish, H. V.},
title = {Pattern Tree Algebras: Sets or Sequences?},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {XML and XQuery semantics are very sensitive to the order of the produced output. Although pattern-tree based algebraic approaches are becoming more and more popular for evaluating XML, there is no universally accepted technique which can guarantee both a correct output order and a choice of efficient alternative plans.We address the problem using hybrid collections of trees that can be either sets or sequences or something in between. Each such collection is coupled with an Ordering Specification that describes how the trees are sorted (full, partial or no order). This provides us with a formal basis for developing a query plan having parts that maintain no order and parts with partial or full order.It turns out that duplicate elimination introduces some of the same issues as order maintenance: it is expensive and a single collection type does not always provide all the flexibility required to optimize this properly. To solve this problem we associate with each hybrid collection a Duplicate Specification that describes the presence or absence of duplicate elements in it. We show how to extend an existing bulk tree algebra, TLC [12], to use Ordering and Duplicate specifications and produce correctly ordered results. We also suggest some optimizations enabled by the flexibility of our approach, and experimentally demonstrate the performance increase due to them.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {349–360},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083636,
author = {Amer-Yahia, Sihem and Koudas, Nick and Marian, Am\'{e}lie and Srivastava, Divesh and Toman, David},
title = {Structure and Content Scoring for XML},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {XML repositories are usually queried both on structure and content. Due to structural heterogeneity of XML, queries are often interpreted approximately and their answers are returned ranked by scores. Computing answer scores in XML is an active area of research that oscillates between pure content scoring such as the well-known tf*idf and taking structure into account. However, none of the existing proposals fully accounts for structure and combines it with content to score query answers. We propose novel XML scoring methods that are inspired by tf*idf and that account for both structure and content while considering query relaxations. Twig scoring, accounts for the most structure and content and is thus used as our reference method. Path scoring is an approximation that loosens correlations between query nodes hence reducing the amount of time required to manipulate scores during top-k query processing. We propose efficient data structures in order to speed up ranked query processing. We run extensive experiments that validate our scoring methods and that show that path scoring provides very high precision while improving score computation time.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {361–372},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083638,
author = {Markl, V. and Megiddo, N. and Kutsch, M. and Tran, T. M. and Haas, P. and Srivastava, U.},
title = {Consistently Estimating the Selectivity of Conjuncts of Predicates},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Cost-based query optimizers need to estimate the selectivity of conjunctive predicates when comparing alternative query execution plans. To this end, advanced optimizers use multivariate statistics (MVS) to improve information about the joint distribution of attribute values in a table. The joint distribution for all columns is almost always too large to store completely, and the resulting use of partial distribution information raises the possibility that multiple, non-equivalent selectivity estimates may be available for a given predicate. Current optimizers use ad hoc methods to ensure that selectivities are estimated in a consistent manner. These methods ignore valuable information and tend to bias the optimizer toward query plans for which the least information is available, often yielding poor results. In this paper we present a novel method for consistent selectivity estimation based on the principle of maximum entropy (ME). Our method efficiently exploits all available information and avoids the bias problem. In the absence of detailed knowledge, the ME approach reduces to standard uniformity and independence assumptions. Our implementation using a prototype version of DB2 UDB shows that ME improves the optimizer's cardinality estimates by orders of magnitude, resulting in better plan quality and significantly reduced query execution times.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {373–384},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083639,
author = {Enderle, Jost and Schneider, Nicole and Seidl, Thomas},
title = {Efficiently Processing Queries on Interval-and-Value Tuples in Relational Databases},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {With the increasing occurrence of temporal and spatial data in present-day database applications, the interval data type is adopted by more and more database systems. For an efficient support of queries that contain selections on interval attributes as well as simple-valued attributes (e.g. numbers, strings) at the same time, special index structures are required supporting both types of predicates in combination. Based on the Relational Interval Tree, we present various indexing schemes that support such combined queries and can be integrated in relational database systems with minimum effort. Experiments on different query types show superior performance for the new techniques in comparison to competing access methods.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {385–396},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083640,
author = {Jin, Liang and Li, Chen},
title = {Selectivity Estimation for Fuzzy String Predicates in Large Data Sets},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Many database applications have the emerging need to support fuzzy queries that ask for strings that are similar to a given string, such as "name similar to smith" and "telephone number similar to 412-0964." Query optimization needs the selectivity of such a fuzzy predicate, i.e., the fraction of records in the database that satisfy the condition. In this paper, we study the problem of estimating selectivities of fuzzy string predicates. We develop a novel technique, called SEPIA, to solve the problem. It groups strings into clusters, builds a histogram structure for each cluster, and constructs a global histogram for the database. It is based on the following intuition: given a query string q, a preselected string p in a cluster, and a string s in the cluster, based on the proximity between q and p, and the proximity between p and s, we can obtain a probability distribution from a global histogram about the similarity between q and s. We give a full specification of the technique using the edit distance function. We study challenges in adopting this technique, including how to construct the histogram structures, how to use them to do selectivity estimation, and how to alleviate the effect of non-uniform errors in the estimation. We discuss how to extend the techniques to other similarity functions. Our extensive experiments on real data sets show that this technique can accurately estimate selectivities of fuzzy string predicates.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {397–408},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083642,
author = {Guha, Sudipto},
title = {Space Efficiency in Synopsis Construction Algorithms},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Histograms and Wavelet synopses have been found to be useful in query optimization, approximate query answering and mining. Over the last few years several good synopsis algorithms have been proposed. These have mostly focused on the running time of the synopsis constructions, optimum or approximate, vis-a-vis their quality. However the space complexity of synopsis construction algorithms has not been investigated as thoroughly. Many of the optimum synopsis construction algorithms (as well as few of the approximate ones) are expensive in space. In this paper, we propose a general technique that reduces space complexity. We show that the notion of "working space" proposed in these contexts is redundant. We believe that our algorithm also generalizes to a broader range of dynamic programs beyond synopsis construction. Our modifications can be easily adapted to existing algorithms. We demonstrate the performance benefits through experiments on real-life and synthetic data.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {409–420},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083643,
author = {Karras, Panagiotis and Mamoulis, Nikos},
title = {One-Pass Wavelet Synopses for Maximum-Error Metrics},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {We study the problem of computing wavelet-based synopses for massive data sets in static and streaming environments. A compact representation of a data set is obtained after a thresholding process is applied on the coefficients of its wavelet decomposition. Existing polynomial-time thresholding schemes that minimize maximum error metrics are disadvantaged by impracticable time and space complexities and are not applicable in a data stream context. This is a cardinal issue, as the problem at hand in its most practically interesting form involves the time-efficient approximation of huge amounts of data, potentially in a streaming environment. In this paper we fill this gap by developing efficient and practicable wavelet thresholding algorithms for maximum-error metrics, for both a static and a streaming case. Our algorithms achieve near-optimal accuracy and superior runtime performance, as our experiments show, under frugal space requirements in both contexts.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {421–432},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083644,
author = {Bu, Shaofeng and Lakshmanan, Laks V. S. and Ng, Raymond T.},
title = {MDL Summarization with Holes},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Summarization of query results is an important problem for many OLAP applications. The Minimum Description Length principle has been applied in various studies to provide summaries. In this paper, we consider a new approach of applying the MDL principle. We study the problem of finding summaries of the form S Θ H for k-d cubes with tree hierarchies. The S part generalizes the query results, while the H part describes all the exceptions to the generalizations. The optimization problem is to minimize the combined cardinalities of S and H. We first characterize the problem by showing that solving the 1-d problem can be done in time linear to the size of hierarchy, but solving the 2-d problem is NP-hard. We then develop three different heuristics, based on a greedy approach, a dynamic programming approach and a quadratic programming approach. We conduct a comprehensive experimental evaluation. Both the dynamic programming algorithm and the greedy algorithm can be used for different circumstances. Both produce summaries that are significantly shorter than those generated by state-of-the-art alternatives.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {433–444},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083646,
author = {Larson, Per-\r{A}ke and Zhou, Jingren},
title = {View Matching for Outer-Join Views},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Prior work on computing queries from materialized views has focused on views defined by expressions consisting of selection, projection, and inner joins, with an optional aggregation on top (SPJG views). This paper provides the first view matching algorithm for views that may also contain outer joins (SPOJG views). The algorithm relies on a normal form for SPOJ expressions and does not use bottom-up syntactic matching of expressions. It handles any combination of inner and outer joins, deals correctly with SQL bag semantics and exploits not-null constraints, uniqueness constraints and foreign key constraints.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {445–456},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083647,
author = {Guo, Hongfei and Larson, Per-\r{A}ke and Ramakrishnan, Raghu},
title = {Caching with "Good Enough" Currency, Consistency, and Completeness},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {SQL extensions that allow queries to explicitly specify data quality requirements in terms of currency and consistency were proposed in an earlier paper. This paper develops a data quality-aware, finer grained cache model and studies cache design in terms of four fundamental properties: presence, consistency, completeness and currency. The model provides an abstract view of the cache to the query processing layer, and opens the door for adaptive cache management. We describe an implementation approach that builds on the MTCache framework for partially materialized views. The optimizer checks most consistency constraints and generates a dynamic plan that includes currency checks and inexpensive checks for dynamic consistency constraints that cannot be validated during optimization. Our solution not only supports transparent caching but also provides fine grained data currency and consistency guarantees.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {457–468},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083648,
author = {Mandhani, Bhushan and Suciu, Dan},
title = {Query Caching and View Selection for XML Databases},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In this paper, we propose a method for maintaining a semantic cache of materialized XPath views. The cached views include queries that have been previously asked, and additional selected views. The cache can be stored inside or outside the database. We describe a notion of XPath query/view answerability, which allows us to reduce tree operations to string operations for matching a query/view pair. We show how to store and maintain the cached views in relational tables, so that cache lookup is very efficient. We also describe a technique for view selection, given a warm-up workload. We experimentally demonstrate the efficiency of our caching techniques, and performance gains obtained by employing such a cache.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {469–480},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083650,
author = {Guravannavar, Ravindra and Ramanujam, H. S. and Sudarshan, S.},
title = {Optimizing Nested Queries with Parameter Sort Orders},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Nested iteration is an important technique for query evaluation. It is the default way of executing nested subqueries in SQL. Although decorrelation often results in cheaper non-nested plans, decorrelation is not always applicable for nested subqueries. Nested iteration, if implemented properly, can also win over decorrelation for several classes of queries. Decorrelation is also hard to apply to nested iteration in user-defined SQL procedures and functions. Recent research has proposed evaluation techniques to speed up execution of nested iteration, but does not address the optimization issue. In this paper, we address the issue of exploiting the ordering of nested iteration/procedure calls to speed up nested iteration. We propose state retention of operators as an important technique to exploit the sort order of parameters/correlation variables. We then show how to efficiently extend an optimizer to take parameter sort orders into consideration. We implemented our evaluation techniques on PostgreSQL, and present performance results that demonstrate significant benefits.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {481–492},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083651,
author = {Chen, Li and Gupta, Amarnath and Kurul, M. Erdem},
title = {Stack-Based Algorithms for Pattern Matching on DAGs},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Existing work for query processing over graph data models often relies on pre-computing the transitive closure or path indexes. In this paper, we propose a family of stack-based algorithms to handle path, twig, and dag pattern queries for directed acyclic graphs (DAGs) in particular. Our algorithms do not precompute the transitive closure nor path indexes for a given graph, however they achieve an optimal runtime complexity quadratic in the average size of the query variable bindings. We prove the soundness and completeness of our algorithms and present the experimental results.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {493–504},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083652,
author = {Kacholia, Varun and Pandit, Shashank and Chakrabarti, Soumen and Sudarshan, S. and Desai, Rushi and Karambelkar, Hrishikesh},
title = {Bidirectional Expansion for Keyword Search on Graph Databases},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Relational, XML and HTML data can be represented as graphs with entities as nodes and relationships as edges. Text is associated with nodes and possibly edges. Keyword search on such graphs has received much attention lately. A central problem in this scenario is to efficiently extract from the data graph a small number of the "best" answer trees. A Backward Expanding search, starting at nodes matching keywords and working up toward confluent roots, is commonly used for predominantly text-driven queries. But it can perform poorly if some keywords match many nodes, or some node has very large degree.In this paper we propose a new search algorithm, Bidirectional Search, which improves on Backward Expanding search by allowing forward search from potential roots towards leaves. To exploit this flexibility, we devise a novel search frontier prioritization technique based on spreading activation. We present a performance study on real data, establishing that Bidirectional Search significantly outperforms Backward Expanding search.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {505–516},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083654,
author = {Gy\"{o}ngyi, Zolt\'{a}n and Garcia-Molina, Hector},
title = {Link Spam Alliances},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Link spam is used to increase the ranking of certain target web pages by misleading the connectivity-based ranking algorithms in search engines. In this paper we study how web pages can be interconnected in a spam farm in order to optimize rankings. We also study alliances, that is, interconnections of spam farms. Our results identify the optimal structures and quantify the potential gains. In particular, we show that alliances can be synergistic and improve the rankings of all participants. We believe that the insights we gain will be useful in identifying and combating link spam.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {517–528},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083655,
author = {Graupmann, Jens and Schenkel, Ralf and Weikum, Gerhard},
title = {The SphereSearch Engine for Unified Ranked Retrieval of Heterogeneous XML and Web Documents},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines: concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {529–540},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083656,
author = {Li, Ning and Hui, Joshua and Hsiao, Hui-I and Beyer, Kevin S.},
title = {Hubble: An Advanced Dynamic Folder Technology for XML},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {A significant amount of information is stored in computer systems today, but people are struggling to manage their documents such that the information is easily found. XML is a de-facto standard for content publishing and data exchange. The proliferation of XML documents has created new challenges and opportunities for managing document collections. Existing technologies for automatically organizing document collections are either imprecise or based on only simple criteria. Since XML documents are self describing, it is now possible to automatically categorize XML documents precisely, according to their content. With the availability of the standard XML query languages, e.g. XQuery, much more powerful folder technologies are now feasible. To address this new challenge and exploit this new opportunity, this paper proposes a new and powerful dynamic folder mechanism, called Hubble. Hubble fully exploits the rich data model and semantic information embedded in the XML documents to build folder hierarchies dynamically and to categorize XML collections precisely. Besides supporting basic folder operations, Hubble also provides advanced features such as multi-path navigation and folder traversal across multiple document collections. Our performance study shows that Hubble is both efficient and scalable. Thus, it is an ideal technology for automating the process of organizing and categorizing XML documents.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {541–552},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083658,
author = {Stonebraker, Mike and Abadi, Daniel J. and Batkin, Adam and Chen, Xuedong and Cherniack, Mitch and Ferreira, Miguel and Lau, Edmond and Lin, Amerson and Madden, Sam and O'Neil, Elizabeth and O'Neil, Pat and Rasin, Alex and Tran, Nga and Zdonik, Stan},
title = {C-Store: A Column-Oriented DBMS},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.We present preliminary performance data on a subset of TPC-H and show that the system we are building, C-Store, is substantially faster than popular commercial products. Hence, the architecture looks very encouraging.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {553–564},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083659,
author = {Akal, Fuat and T\"{u}rker, Can and Schek, Hans-J\"{o}rg and Breitbart, Yuri and Grabs, Torsten and Veen, Lourens},
title = {Fine-Grained Replication and Scheduling with Freshness and Correctness Guarantees},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Lazy replication protocols provide good scalability properties by decoupling transaction execution from the propagation of new values to replica sites while guaranteeing a correct and more efficient transaction processing and replica maintenance. However, they impose several restrictions that are often not valid in practical database settings, e.g., they require that each transaction executes at its initiation site and/or are restricted to full replication schemes. Also, the protocols cannot guarantee that the transactions will always see the freshest available replicas. This paper presents a new lazy replication protocol called PDBREP that is free of these restrictions while ensuring one-copy-serializable executions. The protocol exploits the distinction between read-only and update transactions and works with arbitrary physical data organizations such as partitioning and striping as well as different replica granularities. It does not require that each read-only transaction executes entirely at its initiation site. Hence, each read-only site need not contain a fully replicated database. PDBREP moreover generalizes the notion of freshness to finer data granules than entire databases.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {565–576},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083660,
author = {Ghoting, Amol and Buehrer, Gregory and Parthasarathy, Srinivasan and Kim, Daehyun and Nguyen, Anthony and Chen, Yen-Kuang and Dubey, Pradeep},
title = {Cache-Conscious Frequent Pattern Mining on a Modern Processor},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In this paper, we examine the performance of frequent pattern mining algorithms on a modern processor. A detailed performance study reveals that even the best frequent pattern mining implementations, with highly efficient memory managers, still grossly under-utilize a modern processor. The primary performance bottlenecks are poor data locality and low instruction level parallelism (ILP). We propose a cache-conscious prefix tree to address this problem. The resulting tree improves spatial locality and also enhances the benefits from hardware cache line prefetching. Furthermore, the design of this data structure allows the use of a novel tiling strategy to improve temporal locality. The result is an overall speedup of up to 3.2 when compared with state-of-the-art implementations. We then show how these algorithms can be improved further by realizing a non-naive thread-based decomposition that targets simultaneously multi-threaded processors. A key aspect of this decomposition is to ensure cache re-use between threads that are co-scheduled at a fine granularity. This optimization affords an additional speedup of 50%, resulting in an overall speedup of up to 4.8. To the best of our knowledge, this effort is the first to target cache-conscious data mining.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {577–588},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083662,
author = {Haftmann, Florian and Kossmann, Donald and Lo, Eric},
title = {Parallel Execution of Test Runs for Database Application Systems},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In a recent paper [8], it was shown how tests for database application systems can be executed efficiently. The challenge was to control the state of the database during testing and to order the test runs in such a way that expensive reset operations that bring the database into the right state need to be executed as seldom as possible. This work extends that work so that test runs can be executed in parallel. The goal is to achieve linear speed-up and/or exploit the available resources as well as possible. This problem is challenging because parallel testing can involve interference between the execution of concurrent test runs.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {589–600},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083663,
author = {Sion, Radu},
title = {Query Execution Assurance for Outsourced Databases},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In this paper we propose and analyze a method for proofs of actual query execution in an outsourced database framework, in which a client outsources its data management needs to a specialized provider. The solution is not limited to simple selection predicate queries but handles arbitrary query types. While this work focuses mainly on read-only, compute-intensive (e.g. data-mining) queries, it also provides preliminary mechanisms for handling data updates (at additional costs). We introduce query execution proofs; for each executed batch of queries the database service provider is required to provide a strong cryptographic proof that provides assurance that the queries were actually executed correctly over their entire target data set. We implement a proof of concept and present experimental results in a real-world data mining application, proving the deployment feasibility of our solution. We analyze the solution and show that its overheads are reasonable and are far outweighed by the added security benefits. For example an assurance level of over 95% can be achieved with less than 25% execution time overhead.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {601–612},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083664,
author = {Berardi, Daniela and Calvanese, Diego and De Giacomo, Giuseppe and Hull, Richard and Mecella, Massimo},
title = {Automatic Composition of Transition-Based Semantic Web Services with Messaging},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform; (ii) their impact on the "real world" (modeled as a relational database); (iii) their transition-based behavior; and (iv) the messages they can send and receive (from/to other web services and "human" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {613–624},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083666,
author = {Theobald, Martin and Schenkel, Ralf and Weikum, Gerhard},
title = {An Efficient and Versatile Query Engine for TopX Search},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {This paper presents a novel engine, coined TopX, for efficient ranked retrieval of XML documents over semistructured but nonschematic data collections. The algorithm follows the paradigm of threshold algorithms for top-k query processing with a focus on inexpensive sequential accesses to index lists and only a few judiciously scheduled random accesses. The difficulties in applying the existing top-k algorithms to XML data lie in 1) the need to consider scores for XML elements while aggregating them at the document level, 2) the combination of vague content conditions with XML path conditions, 3) the need to relax query conditions if too few results satisfy all conditions, and 4) the selectivity estimation for both content and structure conditions and their impact on evaluation strategies. TopX addresses these issues by precomputing score and path information in an appropriately designed index structure, by largely avoiding or postponing the evaluation of expensive path conditions so as to preserve the sequential access pattern on index lists, and by selectively scheduling random accesses when they are cost-beneficial. In addition, TopX can compute approximate top-k results using probabilistic score estimators, thus speeding up queries with a small and controllable loss in retrieval precision.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {625–636},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083667,
author = {Michel, Sebastian and Triantafillou, Peter and Weikum, Gerhard},
title = {KLEE: A Framework for Distributed Top-k Query Algorithms},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {This paper addresses the efficient processing of top-k queries in wide-area distributed data repositories where the index lists for the attribute values (or text terms) of a query are distributed across a number of data peers and the computational costs include network latency, bandwidth consumption, and local peer work. We present KLEE, a novel algorithmic framework for distributed top-k queries, designed for high performance and flexibility. KLEE makes a strong case for approximate top-k algorithms over widely distributed data sources. It shows how great gains in efficiency can be enjoyed at low result-quality penalties. Further, KLEE affords the query-initiating peer the flexibility to trade-off result quality and expected performance and to trade-off the number of communication phases engaged during query execution versus network bandwidth performance. We have implemented KLEE and related algorithms and conducted a comprehensive performance evaluation. Our evaluation employed real-world and synthetic large, web-data collections, and query benchmarks. Our experimental results show that KLEE can achieve major performance gains in terms of network bandwidth, query response times, and much lighter peer loads, all with small errors in result precision and other result-quality measures.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {637–648},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083668,
author = {Fu, Ada Wai-chee and Keogh, Eamonn and Lau, Leo Yung Hang and Ratanamahatana, Chotirat Ann},
title = {Scaling and Time Warping in Time Series Querying},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {The last few years have seen an increasing understanding that Dynamic Time Warping (DTW), a technique that allows local flexibility in aligning time series, is superior to the ubiquitous Euclidean Distance for time series classification, clustering, and indexing. More recently, it has been shown that for some problems, Uniform Scaling (US), a technique that allows global scaling of time series, may just be as important for some problems. In this work, we note that for many real world problems, it is necessary to combine both DTW and US to achieve meaningful results. This is particularly true in domains where we must account for the natural variability of human action, including biometrics, query by humming, motion-capture/animation, and handwriting recognition. We introduce the first technique which can handle both DTW and US simultaneously, and demonstrate its utility and effectiveness on a wide range of problems in industry, medicine, and entertainment.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {649–660},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083670,
author = {Jagadish, H. V. and Ooi, Beng Chin and Vu, Quang Hieu},
title = {BATON: A Balanced Tree Structure for Peer-to-Peer Networks},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {We propose a balanced tree structure overlay on a peer-to-peer network capable of supporting both exact queries and range queries efficiently. In spite of the tree structure causing distinctions to be made between nodes at different levels in the tree, we show that the load at each node is approximately equal. In spite of the tree structure providing precisely one path between any pair of nodes, we show that sideways routing tables maintained at each node provide sufficient fault tolerance to permit efficient repair. Specifically, in a network with N nodes, we guarantee that both exact queries and range queries can be answered in O(log N) steps and also that update operations (to both data and network) have an amortized cost of O(log N). An experimental assessment validates the practicality of our proposal.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {661–672},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083671,
author = {Shah, Shetal and Ramamritham, Krithi and Ravishankar, Chinya},
title = {Client Assignment in Content Dissemination Networks for Dynamic Data},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Consider a content distribution network consisting of a set of sources, repositories and clients where the sources and the repositories cooperate with each other for efficient dissemination of dynamic data. In this system, necessary changes are pushed from sources to repositories and from repositories to clients so that they are automatically informed about the changes of interest. Clients and repositories associate coherence requirements with a data item d, denoting the maximum permissible deviation of the value of d known to them from the value at the source. Given a list of <data item,="" coherence=""> served by each repository and a set of <client, data="" item,="" coherence=""> requests, we address the following problem: How do we assign clients to the repositories, so that the fidelity, that is, the degree to which client coherence requirements are met, is maximized?In this paper, we first prove that the client assignment problem is NP-Hard. Given the closeness of the client-repository assignment problem and the matching problem in combinatorial optimization, we have tailored and studied two available solutions to the matching problem from the literature: (i) max-flow min-cost and (ii) stable-marriages. Our empirical results using real-world dynamic data show that the presence of coherence requirements adds a new dimension to the client-repository assignment problem. An interesting result is that in update intensive situations a better fidelity can be delivered to the clients by attempting to deliver data to some of the clients at a coherence lower than what they desire. A consequence of this observation is the necessity for quick adaptation of the delivered (vs. desired) data coherence with respect to the changes in the dynamics of the system. We develop techniques for such adaptation and show their impressive performance.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {673–684},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}</client,></data>

@inproceedings{10.5555/1083592.1083672,
author = {Aberer, Karl and Datta, Anwitaman and Hauswirth, Manfred and Schmidt, Roman},
title = {Indexing Data-Oriented Overlay Networks},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {The application of structured overlay networks to implement index structures for data-oriented applications such as peer-to-peer databases or peer-to-peer information retrieval, requires highly efficient approaches for overlay construction, as changing application requirements frequently lead to re-indexing of the data and hence (re)construction of overlay networks. This problem has so far not been addressed in the literature and thus we describe an approach for the efficient construction of data-oriented, structured overlay networks from scratch in a self-organized way. Standard maintenance algorithms for overlay networks cannot accomplish this efficiently, as they are inherently sequential. Our proposed algorithm is completely decentralized, parallel, and can construct a new overlay network with short latency. At the same time it ensures good load-balancing for skewed data key distributions which result from preserving key order relationships as necessitated by data-oriented applications. We provide both a theoretical analysis of the basic algorithms and a complete system implementation that has been tested on PlanetLab. We use this implementation to support peer-to-peer information retrieval and database applications.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {685–696},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083674,
author = {Papadimitriou, Spiros and Sun, Jimeng and Faloutsos, Christos},
title = {Streaming Pattern Discovery in Multiple Time-Series},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In this paper, we introduce SPIRIT (Streaming Pattern dIscoveRy in multIple Time-series). Given n numerical data streams, all of whose values we observe at each time tick t, SPIRIT can incrementally find correlations and hidden variables, which summarise the key trends in the entire stream collection. It can do this quickly, with no buffering of stream values and without comparing pairs of streams. Moreover, it is any-time, single pass, and it dynamically detects changes. The discovered trends can also be used to immediately spot potential anomalies, to do efficient forecasting and, more generally, to dramatically simplify further data processing. Our experimental evaluation and case studies show that SPIRIT can incrementally capture correlations and discover trends, efficiently and effectively.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {697–708},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083675,
author = {Xin, Dong and Han, Jiawei and Yan, Xifeng and Cheng, Hong},
title = {Mining Compressed Frequent-Pattern Sets},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {A major challenge in frequent-pattern mining is the sheer size of its mining results. In many cases, a high min_sup threshold may discover only commonsense patterns but a low one may generate an explosive number of output patterns, which severely restricts its usage.In this paper, we study the problem of compressing frequent-pattern sets. Typically, frequent patterns can be clustered with a tightness measure δ (called δ-cluster), and a representative pattern can be selected for each cluster. Unfortunately, finding a minimum set of representative patterns is NP-Hard. We develop two greedy methods, RPglobal and RPlocal. The former has the guaranteed compression bound but higher computational complexity. The latter sacrifices the theoretical bounds but is far more efficient. Our performance study shows that the compression quality using RPlocal is very close to RPglobal, and both can reduce the number of closed frequent patterns by almost two orders of magnitude. Furthermore, RPlocal mines even faster than FPClose[11], a very fast closed frequent-pattern mining method. We also show that RPglobal and RPlocal can be combined together to balance the quality and efficiency.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {709–720},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083676,
author = {Gibson, David and Kumar, Ravi and Tomkins, Andrew},
title = {Discovering Large Dense Subgraphs in Massive Graphs},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {We present a new algorithm for finding large, dense subgraphs in massive graphs. Our algorithm is based on a recursive application of fingerprinting via shingles, and is extremely efficient, capable of handling graphs with tens of billions of edges on a single machine with modest resources.We apply our algorithm to characterize the large, dense subgraphs of a graph showing connections between hosts on the World Wide Web; this graph contains over 50M hosts and 11B edges, gathered from 2.1B web pages. We measure the distribution of these dense subgraphs and their evolution over time. We show that more than half of these hosts participate in some dense subgraph found by the analysis. There are several hundred giant dense subgraphs of at least ten thousand hosts; two thousand dense subgraphs at least a thousand hosts; and almost 64K dense subgraphs of at least a hundred hosts.Upon examination, many of the dense subgraphs output by our algorithm are link spam, i.e., websites that attempt to manipulate search engine rankings through aggressive interlinking to simulate popular content. We therefore propose dense subgraph extraction as a useful primitive for spam detection, and discuss its incorporation into the workflow of web search engines.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {721–732},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083678,
author = {Saint-Paul, R\'{e}gis and Raschia, Guillaume and Mouaddib, Noureddine},
title = {General Purpose Database Summarization},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In this paper, a message-oriented architecture for large database summarization is presented. The summarization system takes a database table as input and produces a reduced version of this table through both a rewriting and a generalization process. The resulting table provides tuples with less precision than the original but yet are very informative of the actual content of the database. This reduced form can be used as input for advanced data mining processes as well as some specific application presented in other works. We describe the incremental maintenance of the summarized table, the system capability to directly deal with XML database systems, and finally scalability which allows it to handle very large datasets of a million record.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {733–744},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083679,
author = {Jermaine, Christopher and Dobra, Alin and Pol, Abhijit and Joshi, Shantanu},
title = {Online Estimation for Subset-Based SQL Queries},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {The largest databases in use today are so large that answering a query exactly can take minutes, hours, or even days. One way to address this problem is to make use of approximation algorithms. Previous work on online aggregation has considered how to give online estimates with ever-increasing accuracy for aggregate functions over relational join and selection queries. However, no existing work is applicable to online estimation over subset-based SQL queries-those queries with a correlated subquery linked to an outer query via a NOT EXISTS, NOT IN, EXISTS, or IN clause (other queries such as EXCEPT and INTERSECT can also be seen as subset-based queries). In this paper we develop algorithms for online estimation over such queries, and consider the difficult problem of providing probabilistic accuracy guarantees at all times during query execution.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {745–756},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083680,
author = {Bizarro, Pedro and Babu, Shivnath and DeWitt, David and Widom, Jennifer},
title = {Content-Based Routing: Different Plans for Different Data},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Query optimizers in current database systems are designed to pick a single efficient plan for a given query based on current statistical properties of the data. However, different subsets of the data can sometimes have very different statistical properties. In such scenarios it can be more efficient to process different subsets of the data for a query using different plans. We propose a new query processing technique called content-based routing (CBR) that eliminates the single-plan restriction in current systems. We present low-overhead adaptive algorithms that partition input data based on statistical properties relevant to query execution strategies, and efficiently route individual tuples through customized plans based on their partition. We have implemented CBR as an extension to the Eddies query processor in the TelegraphCQ system, and we present an extensive experimental evaluation showing the significant performance benefits of CBR.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {757–768},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083681,
author = {Abadi, Daniel J. and Madden, Samuel and Lindner, Wolfgang},
title = {REED: Robust, Efficient Filtering and Event Detection in Sensor Networks},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {This paper presents a set of algorithms for efficiently evaluating join queries over static data tables in sensor networks. We describe and evaluate three algorithms that take advantage of distributed join techniques. Our algorithms are capable of running in limited amounts of RAM, can distribute the storage burden over groups of nodes, and are tolerant to dropped packets and node failures. REED is thus suitable for a wide range of event-detection applications that traditional sensor network database and data collection systems cannot be used to implement.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {769–780},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083683,
author = {Pandey, Sandeep and Roy, Sourashis and Olston, Christopher and Cho, Junghoo and Chakrabarti, Soumen},
title = {Shuffling a Stacked Deck: The Case for Partially Randomized Ranking of Search Engine Results},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In-degree, PageRank, number of visits and other measures of Web page popularity significantly influence the ranking of search results by modern search engines. The assumption is that popularity is closely correlated with quality, a more elusive concept that is difficult to measure directly. Unfortunately, the correlation between popularity and quality is very weak for newly-created pages that have yet to receive many visits and/or in-links. Worse, since discovery of new content is largely done by querying search engines, and because users usually focus their attention on the top few results, newly-created but high-quality pages are effectively "shut out," and it can take a very long time before they become popular.We propose a simple and elegant solution to this problem: the introduction of a controlled amount of randomness into search result ranking methods. Doing so offers new pages a chance to prove their worth, although clearly using too much randomness will degrade result quality and annul any benefits achieved. Hence there is a tradeoff between exploration to estimate the quality of new pages and exploitation of pages already known to be of high quality. We study this tradeoff both analytically and via simulation, in the context of an economic objective function based on aggregate result quality amortized over time. We show that a modest amount of randomness leads to improved search results.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {781–792},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083684,
author = {Jin, Liang and Li, Chen and Koudas, Nick and Tung, Anthony K. H.},
title = {Indexing Mixed Types for Approximate Retrieval},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In various applications such as data cleansing, being able to retrieve categorical or numerical attributes based on notions of approximate match (e.g., edit distance, numerical distance) is of profound importance. Commonly, approximate match predicates are specified on combinations of attributes in conjunction. Existing database techniques for approximate retrieval, however, limit their applicability to single attribute retrieval through B-trees and their variants. In this paper, we propose a methodology that utilizes known multidimensional indexing structures for the problem of approximate multi-attribute retrieval. Our method enables indexing of a collection of string and/or numeric attributes to facilitate approximate retrieval using edit distance as an approximate match predicate for strings and numeric distance for numeric attributes. The approach presented is based on representing sets of strings at higher levels of the index structure as tries suitably compressed in a way that reasoning about edit distance between a query string and a compressed trie at index nodes is still feasible. We propose and evaluate various techniques to generate the compressed trie representation and fully specify our indexing methodology. Our experimental results show the benefits of our proposal when compared with various alternate strategies for the same problem.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {793–804},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083685,
author = {Dalvi, Nilesh and Suciu, Dan},
title = {Answering Queries from Statistics and Probabilistic Views},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Systems integrating dozens of databases, in the scientific domain or in a large corporation, need to cope with a wide variety of imprecisions, such as: different representations of the same object in different sources; imperfect and noisy schema alignments; contradictory information across sources; constraint violations; or insufficient evidence to answer a given query. If standard query semantics were applied to such data, all but the most trivial queries will return an empty answer.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {805–816},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083687,
author = {Chen, Shimin and Ailamaki, Anastassia and Gibbons, Phillip B. and Mowry, Todd C.},
title = {Inspector Joins},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {The key idea behind Inspector Joins is that during the I/O partitioning phase of a hash-based join, we have the opportunity to look at the actual data itself and then use this knowledge in two ways: (1) to create specialized indexes, specific to the given query on the given data, for optimizing the CPU cache performance of the subsequent join phase of the algorithm, and (2) to decide which join phase algorithm best suits this specific query. We show how inspector joins, employing novel statistics and specialized indexes, match or exceed the performance of state-of-the-art cache-friendly hash join algorithms. For example, when run on eight or more processors, our experiments show that inspector joins offer 1.1-1.4X speedups over these previous algorithms, with the speedup increasing as the number of processors increases.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {817–828},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083688,
author = {Liu, Bin and Rundensteiner, Elke A.},
title = {Revisiting Pipelined Parallelism in Multi-Join Query Processing},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Multi-join queries are the core of any integration service that integrates data from multiple distributed data sources. Due to the large number of data sources and possibly high volumes of data, the evaluation of multi-join queries faces increasing scalability concerns. State-of-the-art parallel multi-join query processing commonly assume that the application of maximal pipelined parallelism leads to superior performance. In this paper, we instead illustrate that this assumption does not generally hold. We investigate how best to combine pipelined parallelism with alternate forms of parallelism to achieve an overall effective processing strategy. A segmented bushy processing strategy is proposed. Experimental studies are conducted on an actual software system over a cluster of high-performance PCs. The experimental results confirm that the proposed solution leads to about 50% improvement in terms of total processing time in comparison to existing state-of-the-art solutions.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {829–840},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083689,
author = {Lawrence, Ramon},
title = {Early Hash Join: A Configurable Algorithm for the Efficient and Early Production of Join Results},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Minimizing both the response time to produce the first few thousand results and the overall execution time is important for interactive querying. Current join algorithms either minimize the execution time at the expense of response time or minimize response time by producing results early without optimizing the total time. We present a hash-based join algorithm, called early hash join, which can be dynamically configured at any point during join processing to tradeoff faster production of results for overall execution time. We demonstrate that varying how inputs are read has a major effect on these two factors and provide formulas that allow an optimizer to calculate the expected rate of join output and the number of I/O operations performed using different input reading strategies. Experimental results show that early hash join performs significantly fewer I/O operations and executes faster than other early join algorithms, especially for one-to-many joins. Its overall execution time is comparable to standard hybrid hash join, but its response time is an order of magnitude faster. Thus, early hash join can replace hybrid hash join in any situation where a fast initial response time is beneficial without the penalty in overall execution time exhibited by other early join algorithms.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {841–852},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083691,
author = {Brakatsoulas, Sotiris and Pfoser, Dieter and Salas, Randall and Wenk, Carola},
title = {On Map-Matching Vehicle Tracking Data},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Vehicle tracking data is an essential "raw" material for a broad range of applications such as traffic management and control, routing, and navigation. An important issue with this data is its accuracy. The method of sampling vehicular movement using GPS is affected by two error sources and consequently produces inaccurate trajectory data. To become useful, the data has to be related to the underlying road network by means of map matching algorithms. We present three such algorithms that consider especially the trajectory nature of the data rather than simply the current position as in the typical map-matching case. An incremental algorithm is proposed that matches consecutive portions of the trajectory to the road network, effectively trading accuracy for speed of computation. In contrast, the two global algorithms compare the entire trajectory to candidate paths in the road network. The algorithms are evaluated in terms of (i) their running time and (ii) the quality of their matching result. Two novel quality measures utilizing the Fr\'{e}chet distance are introduced and subsequently used in an experimental evaluation to assess the quality of matching real tracking data to a road network.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {853–864},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083692,
author = {Cho, Hyung-Ju and Chung, Chin-Wan},
title = {An Efficient and Scalable Approach to CNN Queries in a Road Network},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {A continuous search in a road network retrieves the objects which satisfy a query condition at any point on a path. For example, return the three nearest restaurants from all locations on my route from point s to point e. In this paper, we deal with NN queries as well as continuous NN queries in the context of moving objects databases. The performance of existing approaches based on the network distance such as the shortest path length depends largely on the density of objects of interest. To overcome this problem, we propose UNICONS (a <u>uni</u>que <u>con</u>tinuous <u>s</u>earch algorithm) for NN queries and CNN queries performed on a network. We incorporate the use of precomputed NN lists into Dijkstra's algorithm for NN queries. A mathematical rationale is employed to produce the final results of CNN queries. Experimental results for real-life datasets of various sizes show that UNICONS outperforms its competitors by up to 3.5 times for NN queries and 5 times for CNN queries depending on the density of objects and the number of NNs required.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {865–876},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083693,
author = {Hadjieleftheriou, Marios and Kollios, George and Bakalov, Petko and Tsotras, Vassilis J.},
title = {Complex Spatio-Temporal Pattern Queries},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {This paper introduces a novel type of query, what we name Spatio-temporal Pattern Queries (STP). Such a query specifies a spatiotemporal pattern as a sequence of distinct spatial predicates where the predicate temporal ordering (exact or relative) matters. STP queries can use various types of spatial predicates (range search, nearest neighbor, etc.) where each such predicate is associated (1) with an exact temporal constraint (a time-instant or a time-interval), or (2) more generally, with a relative order among the other query predicates. Using traditional spatiotemporal index structures for these types of queries would be either inefficient or not an applicable solution. Alternatively, we propose specialized query evaluation algorithms for STP queries With Time. We also present a novel index structure, suitable for STP queries With Order. Finally, we conduct a comprehensive experimental evaluation to show the merits of our techniques.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {877–888},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083695,
author = {Zhang, Nan and Zhao, Wei},
title = {Distributed Privacy Preserving Information Sharing},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In this paper, we address issues related to sharing information in a distributed system consisting of autonomous entities, each of which holds a private database. Semi-honest behavior has been widely adopted as the model for adversarial threats. However, it substantially underestimates the capability of adversaries in reality. In this paper, we consider a threat space containing more powerful adversaries that includes not only semi-honest but also those malicious adversaries. In particular, we classify malicious adversaries into two widely existing subclasses, called weakly malicious and strongly malicious adversaries, respectively. We define a measure of privacy leakage for information sharing systems and propose protocols that can effectively and efficiently protect privacy against different kinds of malicious adversaries.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {889–900},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083696,
author = {Aggarwal, Charu C.},
title = {On <i>k</i>-Anonymity and the Curse of Dimensionality},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In recent years, the wide availability of personal data has made the problem of privacy preserving data mining an important one. A number of methods have recently been proposed for privacy preserving data mining of multidimensional data records. One of the methods for privacy preserving data mining is that of anonymization, in which a record is released only if it is indistinguishable from k other entities in the data. We note that methods such as k-anonymity are highly dependent upon spatial locality in order to effectively implement the technique in a statistically robust way. In high dimensional space the data becomes sparse, and the concept of spatial locality is no longer easy to define from an application point of view. In this paper, we view the k-anonymization problem from the perspective of inference attacks over all possible combinations of attributes. We show that when the data contains a large number of attributes which may be considered quasi-identifiers, it becomes difficult to anonymize the data without an unacceptably high amount of information loss. This is because an exponential number of combinations of dimensions can be used to make precise inference attacks, even when individual attributes are partially specified within a range. We provide an analysis of the effect of dimensionality on k-anonymity methods. We conclude that when a data set contains a large number of attributes which are open to inference attacks, we are faced with a choice of either completely suppressing most of the data or losing the desired level of anonymity. Thus, this paper shows that the curse of high dimensionality also applies to the problem of privacy preserving data mining.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {901–909},
numpages = {9},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083697,
author = {Yao, Chao and Wang, X. Sean and Jajodia, Sushil},
title = {Checking for <i>k</i>-Anonymity Violation by Views},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {When a private relational table is published using views, secrecy or privacy may be violated. This paper uses a formally-defined notion of k-anonymity to measure disclosure by views, where k &gt;1 is a positive integer. Intuitively, violation of k-anonymity occurs when a particular attribute value of an entity can be determined to be among less than k possibilities by using the views together with the schema information of the private table. The paper shows that, in general, whether a set of views violates k-anonymity is a computationally hard problem. Subcases are identified and their computational complexities discussed. Especially interesting are those subcases that yield polynomial checking algorithms (in the number of tuples in the views). The paper also provides an efficient conservative algorithm that checks for necessary conditions for k-anonymity violation.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {910–921},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083699,
author = {Tao, Yufei and Cheng, Reynold and Xiao, Xiaokui and Ngai, Wang Kay and Kao, Ben and Prabhakar, Sunil},
title = {Indexing Multi-Dimensional Uncertain Data with Arbitrary Probability Density Functions},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In an "uncertain database", an object o is associated with a multi-dimensional probability density function(pdf), which describes the likelihood that o appears at each position in the data space. A fundamental operation is the "probabilistic range search" which, given a value pq and a rectangular area rq, retrieves the objects that appear in rq with probabilities at least pq. In this paper, we propose the U-tree, an access method designed to optimize both the I/O and CPU time of range retrieval on multi-dimensional imprecise data. The new structure is fully dynamic (i.e., objects can be incrementally inserted/deleted in any order), and does not place any constraints on the data pdfs. We verify the query and update efficiency of U-trees with extensive experiments.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {922–933},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083700,
author = {Rasetic, Slobodan and Sander, J\"{o}rg and Elding, James and Nascimento, Mario A.},
title = {A Trajectory Splitting Model for Efficient Spatio-Temporal Indexing},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {This paper addresses the problem of splitting trajectories optimally for the purpose of efficiently supporting spatio-temporal range queries using index structures (e.g., R-trees) that use minimum bounding hyper-rectangles as trajectory approximations. We derive a formal cost model for estimating the number of I/Os required to evaluate a spatio-temporal range query with respect to a given query size and an arbitrary split of a trajectory. Based on the proposed model, we introduce a dynamic programming algorithm for splitting a set of trajectories that minimizes the number of expected disk I/Os with respect to an average query size. In addition, we develop a linear time, near optimal solution for this problem to be used in a dynamic case where trajectory points are continuously updated. Our experimental evaluation confirms the effectiveness and efficiency of our proposed splitting policies when embedded into an R-tree structure.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {934–945},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083701,
author = {Xia, Tian and Zhang, Donghui and Kanoulas, Evangelos and Du, Yang},
title = {On Computing Top-<i>t</i> Most Influential Spatial Sites},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Given a set O of weighted objects, a set S of sites, and a query site s, the bichromatic RNN query computes the influence set of s, or the set of objects in O that consider s as the nearest site among all sites in S. The influence of a site s can be defined as the total weight of its RNNs. This paper addresses the new and interesting problem of finding the top-t most influential sites from S, inside a given spatial region Q. A straightforward approach is to find the sites in Q, and compute the RNNs of every such site. This approach is not efficient for two reasons. First, all sites in Q need to be identified whatsoever, and the number may be large. Second, both the site R-tree and the object R-tree need to be browsed a large number of times. For each site in Q, the R-tree of sites is browsed to identify the influence region -- a polygonal region that may contain RNNs, and then the R-tree of objects is browsed to find the RNN set. This paper proposes an algorithm called TopInfluential-Sites, which finds the top-t most influential sites by browsing both trees once systematically. Novel pruning techniques are provided, based on a new metric called minExistDNN. There is no need to compute the influence for all sites in Q, or even to visit all sites in Q. Experimental results verify that our proposed method outperforms the straightforward approach.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {946–957},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083703,
author = {Fagin, R. and Kolaitis, Ph. and Kumar, R. and Novak, J. and Sivakumar, D. and Tomkins, A.},
title = {Efficient Implementation of Large-Scale Multi-Structural Databases},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In earlier work, we defined "multi-structural databases," a data model to support efficient analysis of large, complex data sets over multiple numerical and hierarchical dimensions. We defined three types of queries over this data model, each of which required solving an optimization problem. An example is to find the ten most significant non-overlapping regions of geography crossed with time in which coverage of the Olympics was much stronger in newspapers than online sources.In this paper, we present a general query framework capturing the original three queries as part of a much broader family. We then give efficient algorithms for particular subclasses of this family. Finally, we describe an implementation of these algorithms that operates on a collection of several billion web documents. Using our algorithms in conjunction with random sampling techniques, our system can solve these queries in real time.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {958–969},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083704,
author = {Burdick, Doug and Deshpande, Prasad M. and Jayram, T. S. and Ramakrishnan, Raghu and Vaithyanathan, Shivakumar},
title = {OLAP over Uncertain and Imprecise Data},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {We extend the OLAP data model to represent data ambiguity, specifically imprecision and uncertainty, and introduce an allocation-based approach to the semantics of aggregation queries over such data. We identify three natural query properties and use them to shed light on alternative query semantics. While there is much work on representing and querying ambiguous data, to our knowledge this is the first paper to handle both imprecision and uncertainty in an OLAP setting.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {970–981},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083705,
author = {Chen, Bee-Chung and Chen, Lei and Lin, Yi and Ramakrishnan, Raghu},
title = {Prediction Cubes},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In this paper, we introduce a new family of tools for exploratory data analysis, called prediction cubes. As in standard OLAP data cubes, each cell in a prediction cube contains a value that summarizes the data belonging to that cell, and the granularity of cells can be changed via operations such as roll-up and drill-down. In contrast to data cubes, in which each cell value is computed by an aggregate function, e.g., SUM or AVG, each cell value in a prediction cube summarizes a predictive model trained on the data corresponding to that cell, and characterizes its decision behavior or predictiveness. In this paper, we propose and motivate prediction cubes, and show that they can be efficiently computed by exploiting the idea of model decomposition.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {982–993},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083707,
author = {Sayyadian, Mayssam and Lee, Yoonkyong and Doan, AnHai and Rosenthal, Arnon S.},
title = {Tuning Schema Matching Software Using Synthetic Scenarios},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Most recent schema matching systems assemble multiple components, each employing a particular matching technique. The domain user must then tune the system: select the right component to be executed and correctly adjust their numerous "knobs" (e.g., thresholds, formula coefficients). Tuning is skill- and time-intensive, but (as we show) without it the matching accuracy is significantly inferior.We describe eTuner, an approach to automatically tune schema matching systems. Given a schema S, we match S against synthetic schemas, for which the ground truth mapping is known, and find a tuning that demonstrably improves the performance of matching S against real schemas. To efficiently search the huge space of tuning configurations, eTuner works sequentially, starting with tuning the lowest level components. To increase the applicability of eTuner, we develop methods to tune a broad range of matching components. While the tuning process is completely automatic, eTuner can also exploit user assistance (whenever available) to further improve the tuning quality. We employed eTuner to tune four recently developed matching systems on several real-world domains. eTuner produced tuned matching systems that achieve higher accuracy than using the systems with currently possible tuning methods, at virtually no cost to the domain user.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {994–1005},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083708,
author = {Yu, Cong and Popa, Lucian},
title = {Semantic Adaptation of Schema Mappings When Schemas Evolve},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Schemas evolve over time to accommodate the changes in the information they represent. Such evolution causes invalidation of various artifacts depending on the schemas, such as schema mappings. In a heterogenous environment, where cooperation among data sources depends essentially upon them, schema mappings must be adapted to reflect schema evolution. In this study, we explore the mapping composition approach for addressing this mapping adaptation problem. We study the semantics of mapping composition in the context of mapping adaptation and compare our approach with the incremental approach of Velegrakis et al [21]. We show that our method is superior in terms of capturing the semantics of both the original mappings and the evolution. We design and implement a mapping adaptation system based on mapping composition as well as additional mapping pruning techniques that significantly speed up the adaptation. We conduct comprehensive experimental analysis and show that the composition approach is practical in various evolution scenarios. The mapping language that we consider is a nested relational extension of the second-order dependencies of Fagin et al [7]. Our work can also be seen as an implementation of the mapping composition operator of the model management framework.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1006–1017},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083709,
author = {McCann, Robert and AlShebli, Bedoor and Le, Quoc and Nguyen, Hoa and Vu, Long and Doan, AnHai},
title = {Mapping Maintenance for Data Integration Systems},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {To answer user queries, a data integration system employs a set of semantic mappings between the mediated schema and the schemas of data sources. In dynamic environments sources often undergo changes that invalidate the mappings. Hence, once the system is deployed, the administrator must monitor it over time, to detect and repair broken mappings. Today such continuous monitoring is extremely labor intensive, and poses a key bottleneck to the widespread deployment of data integration systems in practice.We describe MAVERIC, an automatic solution to detecting broken mappings. At the heart of MAVERIC is a set of computationally inexpensive modules called sensors, which capture salient characteristics of data sources (e.g., value distributions, HTML layout properties). We describe how MAVERIC trains and deploys the sensors to detect broken mappings. Next we develop three novel improvements: perturbation (i.e., injecting artificial changes into the sources) and multi-source training to improve detection accuracy, and filtering to further reduce the number of false alarms. Experiments over 114 real-world sources in six domains demonstrate the effectiveness of our sensor-based approach over existing solutions, as well as the utility of our improvements.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1018–1029},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083711,
author = {Dittrich, Jens-Peter and Kossmann, Donald and Kreutz, Alexander},
title = {Bridging the Gap between OLAP and SQL},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In the last ten years, database vendors have invested heavily in order to extend their products with new features for decision support. Examples of functionality that has been added are top N [2], ranking [13, 7], spreadsheet computations [19], grouping sets [14], data cube [9], and moving sums [15] in order to name just a few. Unfortunately, many modern OLAP systems do not use that functionality or replicate a great deal of it in addition to other database-related functionality. In fact, the gap between the functionality provided by an OLAP system and the functionality used from the underlying database systems has widened in the past, rather than narrowed. The reasons for this trend are that SQL as a data definition and query language, the relational model, and the client/server architecture of the current generation of database products have fundamental shortcomings for OLAP. This paper lists these deficiencies and presents the BTell OLAP engine as an example on how to bridge these shortcomings. In addition, we discuss how to extend current DBMS to better support OLAP in the future.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1031–1042},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083712,
author = {Folkert, Nathan and Gupta, Abhinav and Witkowski, Andrew and Subramanian, Sankar and Bellamkonda, Srikanth and Shankar, Shrikanth and Bozkaya, Tolga and Sheng, Lei},
title = {Optimizing Refresh of a Set of Materialized Views},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In many data warehousing environments, it is common to have materialized views (MVs) at different levels of aggregation of one or more dimensions. The extreme case of this is relational OLAP environments, where, for performance reasons, nearly all levels of aggregation across all dimensions may be computed and stored in MVs. Furthermore, base tables and MVs are usually partitioned for ease and speed of maintenance. In these scenarios, updates to the base table are done using Bulk or Partition operations like add, exchange, truncate and drop partition. If changes to base tables can be tracked at the partition level, join dependencies. functional dependencies and query rewrite can be used to optimize refresh of an individual MV. The refresh optimizer, in the presence of partitioned tables and MVs, may recognize dependencies between base table and the MV partitions leading to the generation of very efficient refresh expressions. Additionally, in the presence of multiple MVs, the refresh subsytem can come up with an optimal refresh schedule such that MVs can be refreshed using query rewrite against previously refreshed MVs. This makes the database server more manageable and user friendly since a single function call can optimally refresh all the MVs in the system.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1043–1054},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083713,
author = {Poess, Meikel and Nambiar, Raghunath Othayoth},
title = {Large Scale Data Warehouses on Grid: Oracle Database 10<i>g</i> and HP Proliant Servers},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Grid computing has the potential to drastically change enterprise computing as we know it today. The main concept of grid computing is viewing computing as a utility. It should not matter where data resides, or what computer processes a task. This concept has been applied successfully to academic research. It also has many advantages for commercial data warehouse applications such as virtualization, flexible provisioning, reduced cost due to commodity hardware, high availability and high scale-out. In this paper we show how a large-scale, high-performing and scalable grid-based data warehouse can be implemented using commodity hardware (industry-standard x86-based). Oracle Database 10g and the Linux operating system. We further demonstrate this architecture in a recently published TPC-H benchmark.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1055–1066},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083715,
author = {Markowitz, Victor M. and Korzeniewski, Frank and Palaniappan, Krishna and Szeto, Ernest and Ivanova, Natalia and Kyrpides, Nikos C.},
title = {The Integrated Microbial Genomes (IMG) System: A Case Study in Biological Data Management},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Biological data management includes the traditional areas of data generation, acquisition, modelling, integration, and analysis. Although numerous academic biological data management systems are currently available, employing them effectively remains a significant challenge. We discuss how this challenge was addressed in the course of developing the Integrated Microbial Genomes (IMG) system for comparative analysis of microbial genome data.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1067–1078},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083716,
author = {Johnson, Theodore and Muthukrishnan, S. and Shkapenyuk, Vladislav and Spatscheck, Oliver},
title = {A Heartbeat Mechanism and Its Application in Gigascope},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&amp;T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1079–1088},
numpages = {10},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083717,
author = {Meier, Andreas and Werro, Nicolas and Albrecht, Martin and Sarakinos, Miltiadis},
title = {Using a Fuzzy Classification Query Language for Customer Relationship Management},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {A key challenge for companies is to manage customer relationships as an asset. To create an effective toolkit for the analysis of customer relationships, a combination of relational databases and fuzzy logic is proposed. The fuzzy Classification Query Language allows marketers to improve customer equity, launch loyalty programs, automate mass customization, and refine marketing campaigns.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1089–1096},
numpages = {8},
keywords = {query language, relational database, fuzzy classification, customer equity, customer relationship management},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083719,
author = {Bruno, Nicolas and Chaudhuri, Surajit},
title = {Flexible Database Generators},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Evaluation and applicability of many database techniques, ranging from access methods, histograms, and optimization strategies to data normalization and mining, crucially depend on their ability to cope with varying data distributions in a robust way. However, comprehensive real data is often hard to come by, and there is no flexible data generation framework capable of modelling varying rich data distributions. This has led individual researchers to develop their own ad-hoc data generators for specific tasks. As a consequence, the resulting data distributions and query workloads are often hard to reproduce, analyze, and modify, thus preventing their wider usage. In this paper we present a flexible, easy to use, and scalable framework for database generation. We then discuss how to map several proposed synthetic distributions to our framework and report preliminary results.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1097–1107},
numpages = {11},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083720,
author = {Ronstr\"{o}m, Mikael and Oreland, Jonas},
title = {Recovery Principles of MySQL Cluster 5.1},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {MySQL Cluster is a parallel main memory database. It is using the normal MySQL software with a new storage engine NDB Cluster. MySQL Cluster 5.1 has been adapted to also handle fields on disk. In this work a number of recovery principles of MySQL Cluster had to be adapted to handle very large data sizes. The article presents an efficient algorithm for synchronizing a starting node with very large data sets. It provides reasons for the unorthodox choice of a no-steal algorithm in the buffer manager. It also presents the algorithm to change the data.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1108–1115},
numpages = {8},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083721,
author = {Hall, Christoffer and Bonnet, Philippe},
title = {Getting Priorities Straight: Improving Linux Support for Database I/O},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {The Linux 2.6 kernel supports asynchronous I/O as a result of propositions from the database industry. This is a positive evolution but is it a panacea? In the context of the Badger project, a collaboration between MySQL AB and University of Copenhagen, we evaluate how MySQL/InnoDB can best take advantage of Linux asynchronous I/O and how Linux can help MySQL/InnoDB best take advantage of the underlying I/O bandwidth. This is a crucial problem for the increasing number of MySQL servers deployed for very large database applications. In this paper, we first show that the conservative I/O submission policy used by InnoDB (as well as Oracle 9.2) leads to an under-utilization of the available I/O bandwidth. We then show that introducing prioritized asynchronous I/O in Linux will allow MySQL/InnoDB and the other Linux databases to fully utilize the available I/O bandwith using a more aggressive I/O submission policy.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1116–1127},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083723,
author = {Wang, Fusheng and Liu, Peiya},
title = {Temporal Management of RFID Data},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {RFID technology can be used to significantly improve the efficiency of business processes by providing the capability of automatic identification and data capture. This technology poses many new challenges on current data management systems. RFID data are time-dependent, dynamically changing, in large volumes, and carry implicit semantics. RFID data management systems need to effectively support such large scale temporal data created by RFID applications. These systems need to have an explicit temporal data model for RFID data to support tracking and monitoring queries. In addition, they need to have an automatic method to transform the primitive observations from RFID readers into derived data used in RFID-enabled applications. In this paper, we present an integrated RFID data management system -- Siemens RFID Middleware -- based on an expressive temporal data model for RFID data. Our system enables semantic RFID data filtering and automatic data transformation based on declarative rules, provides powerful query support of RFID object tracking and monitoring, and can be adapted to different RFID-enabled applications.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1128–1139},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083724,
author = {Hu, Ying and Sundara, Seema and Chorma, Timothy and Srinivasan, Jagannathan},
title = {Supporting RFID-Based Item Tracking Applications in Oracle DBMS Using a Bitmap Datatype},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Radio Frequency Identification (RFID) based item-level tracking holds the promise of revolutionizing supply-chain, retail store, and asset management applications. However, the high volume of data generated by item-level tracking poses challenges to the applications as well as to backend databases. This paper addresses the problem of efficiently modeling identifier collections occurring in RFID-based item-tracking applications and databases. Specifically, 1) a bitmap datatype is introduced to compactly represent a collection of identifiers, and 2) a set of bitmap access and manipulation routines is provided. The proposed bitmap datatype can model a collection of generic identifiers, including 64-bit, 96-bit, and 256-bit Electronic Product Codes™ (EPCs), and it can be used to represent both transient and persistent identifier collections. Persistent identifier collections can be stored in a table as a column of bitmap datatype. An efficient primary B+- tree-based storage scheme is proposed for such columns. The bitmap datatype can be easily implemented by leveraging the DBMS bitmap index implementation, which typically manages bitmaps of table row identifiers. This paper presents the bitmap datatype and related functionality, illustrates its usage in supporting RFID-based item-tracking applications, describes its prototype implementation in Oracle DBMS, and gives a performance study that characterizes the benefits of the bitmap datatype.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1140–1151},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083725,
author = {Milenova, Boriana L. and Yarmus, Joseph S. and Campos, Marcos M.},
title = {SVM in Oracle Database 10g: Removing the Barriers to Widespread Adoption of Support Vector Machines},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Contemporary commercial databases are placing an increased emphasis on analytic capabilities. Data mining technology has become crucial in enabling the analysis of large volumes of data. Modern data mining techniques have been shown to have high accuracy and good generalization to novel data. However, achieving results of good quality often requires high levels of user expertise. Support Vector Machines (SVM) is a powerful state-of-the-art data mining algorithm that can address problems not amenable to traditional statistical analysis. Nevertheless, its adoption remains limited due to methodological complexities, scalability challenges, and scarcity of production quality SVM implementations. This paper describes Oracle's implementation of SVM where the primary focus lies on ease of use and scalability while maintaining high performance accuracy. SVM is fully integrated within the Oracle database framework and thus can be easily leveraged in a variety of deployment scenarios.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1152–1163},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083727,
author = {Nicola, Matthias and van der Linden, Bert},
title = {Native XML Support in DB2 Universal Database},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB2 Universal Database® is enhanced with comprehensive native XML support. "Native" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQL/XML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB2 a true hybrid database system which places equal weight on XML and relational data management.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1164–1174},
numpages = {11},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083728,
author = {Pal, Shankar and Cseri, Istvan and Seeliger, Oliver and Rys, Michael and Schaller, Gideon and Yu, Wei and Tomic, Dragan and Baras, Adrian and Berg, Brandon and Churin, Denis and Kogan, Eugene},
title = {XQuery Implementation in a Relational Database System},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging W3C recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging W3C recommendation XPath 2.0 for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server 2005. XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1175–1186},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083729,
author = {Lim, Lipyeow and Wang, Min and Vitter, Jeffrey Scott},
title = {CXHist: An on-Line Classification-Based Histogram for XML String Selectivity Estimation},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Query optimization in IBM's System RX, the first truly relational-XML hybrid data management system, requires accurate selectivity estimation of path-value pairs, i.e., the number of nodes in the XML tree reachable by a given path with the given text value. Previous techniques have been inadequate, because they have focused mainly on the tag-labeled paths (tree structure) of the XML data. For most real XML data, the number of distinct string values at the leaf nodes is orders of magnitude larger than the set of distinct rooted tag paths. Hence, the real challenge lies in accurate selectivity estimation of the string predicates on the leaf values reachable via a given path.In this paper, we present CXHist, a novel workload-aware histogram technique that provides accurate selectivity estimation on a broad class of XML string-based queries. CXHist builds a histogram in an on-line manner by grouping queries into buckets using their true selectivity obtained from query feedback. The set of queries associated with each bucket is summarized into feature distributions. These feature distributions mimic a Bayesian classifier that is used to route a query to its associated bucket during selectivity estimation. We show how CXHist can be used for two general types of <path, string=""> queries: exact match queries and substring match queries. Experiments using a prototype show that CXHist provides accurate selectivity estimation for both exact match queries and substring match queries.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1187–1198},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}</path,>

@inproceedings{10.5555/1083592.1083731,
author = {Greenfield, Paul and Kuo, Dean and Nepal, Surya and Fekete, Alan},
title = {Consistency for Web Services Applications},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {A key challenge facing the designers of service-oriented applications is ensuring that the autonomous services that make up these distributed applications always finish in consistent states despite application-level failures and other exceptional events. This paper addresses this problem by first describing the relationship between internal service states, messages and application protocols and then shows how this relationship transforms the problem of ensuring consistent outcomes into a correctness problem that can be addressed with established protocol verification tools.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1199–1203},
numpages = {5},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083733,
author = {Witkowski, Andrew and Bellamkonda, Srikanth and Bozkaya, Tolga and Naimat, Aman and Sheng, Lei and Subramanian, Sankar and Waingold, Allison},
title = {Query by Excel},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Spreadsheets, and MS Excel in particular, are established analysis tools. They offer an attractive user interface, provide an easy to use computational model, and offer substantial interactivity for what-if analysis. However, as opposed to RDBMS, spreadsheets do not provide a central repository hence they do not provide shareability of models built in Excel and lead to proliferation of multiple copies of the same spreadsheet. Furthermore, spreadsheets do not offer scalable computation, for example, they lack parallelization. To address the shareability, and scalability problems, we propose to automatically translate Excel computation into SQL. An analyst can import the data from a relational system, define computation over it using familiar Excel formulas and then translate and store it as a relational SQL view over the imported data. The Excel computation is then performed by the relational system. To edit the model, the analyst can bring the model back to Excel, modify it in Excel and store it back as an SQL View. We refer to this system as Query by Excel, QBX in short.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1204–1215},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083734,
author = {Chong, Eugene Inseok and Das, Souripriya and Eadon, George and Srinivasan, Jagannathan},
title = {An Efficient SQL-Based RDF Querying Scheme},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Devising a scheme for efficient and scalable querying of Resource Description Framework (RDF) data has been an active area of current research. However, most approaches define new languages for querying RDF data, which has the following shortcomings: 1) They are difficult to integrate with SQL queries used in database applications, and 2) They incur inefficiency as data has to be transformed from SQL to the corresponding language data format. This paper proposes a SQL based scheme that avoids these problems. Specifically, it introduces a SQL table function RDF_MATCH to query RDF data. The results of RDF_MATCH table function can be further processed by SQL's rich querying capabilities and seamlessly combined with queries on traditional relational data. Furthermore, the RDF_MATCH table function invocation is rewritten as a SQL query, thereby avoiding run-time table function procedural overheads. It also enables optimization of rewritten query in conjunction with the rest of the query. The resulting query is executed efficiently by making use of B-tree indexes as well as specialized subject-property materialized views. This paper describes the functionality of the RDF_MATCH table function for querying RDF data, which can optionally include user-defined rulebases, and discusses its implementation in Oracle RDBMS. It also presents an experimental study characterizing the overhead eliminated by avoiding procedural code at runtime, characterizing performance under various input conditions, and demonstrating scalability using 80 million RDF triples from UniProt protein and annotation data.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1216–1227},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083735,
author = {Reddy, Naveen and Haritsa, Jayant R.},
title = {Analyzing Plan Diagrams of Database Query Optimizers},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {A "plan diagram" is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. In this paper, we present and analyze representative plan diagrams on a suite of popular commercial query optimizers for queries based on the TPC-H benchmark. These diagrams, which often appear similar to cubist paintings, provide a variety of interesting insights, including that current optimizers make extremely fine-grained plan choices, which may often be supplanted by less efficient options without substantively affecting the quality; that the plan optimality regions may have highly intricate patterns and irregular boundaries, indicating strongly non-linear cost models; that non-monotonic cost behavior exists where increasing result cardinalities decrease the estimated cost; and, that the basic assumptions underlying the research literature on parametric query optimization often do not hold in practice.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1228–1239},
numpages = {12},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083737,
author = {Bernstein, Philip A. and DeWitt, David and Heuer, Andreas and Ives, Zachary and Jensen, Christian S. and Meyer, Holger and \"{O}zsu, M. Tamer and Snodgrass, Richard T. and Whang, Kyu-Young and Widom, Jennifer},
title = {Database Publication Practices},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {There has been a growing interest in improving the publication processes for database research papers. This panel reports on recent changes in those processes and presents an initial cut at historical data for the VLDB Journal and ACM Transactions on Database Systems.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1241–1245},
numpages = {5},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083739,
author = {Kassoff, Michael and Zen, Lee-Ming and Garg, Ankit and Genesereth, Michael},
title = {PrediCalc: A Logical Spreadsheet Management System},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Computerized spreadsheets are a great success. They are often touted in newspapers and magazine articles as the first "killer app" for personal computers. Over the years, they have proven their worth time and again. Today, they are used for managing enterprises of all sorts - from one-person projects to multi-institutional conglomerates.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1247–1250},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083740,
author = {Bilke, Alexander and Bleiholder, Jens and Naumann, Felix and B\"{o}hm, Christoph and Draba, Karsten and Weis, Melanie},
title = {Automatic Data Fusion with HumMer},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Heterogeneous and dirty data is abundant. It is stored under different, often opaque schemata, it represents identical real-world objects multiple times, causing duplicates, and it has missing values and conflicting values. The Humboldt Merger (HumMer) is a tool that allows ad-hoc, declarative fusion of such data using a simple extension to SQL.Guided by a query against multiple tables, HumMer proceeds in three fully automated steps: First, instance-based schema matching bridges schematic heterogeneity of the tables by aligning corresponding attributes. Next, duplicate detection techniques find multiple representations of identical real-world objects. Finally, data fusion and conflict resolution merges duplicates into a single, consistent, and clean representation.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1251–1254},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083741,
author = {Beeri, Catriel and Eyal, Anat and Kamenkovich, Simon and Milo, Tova},
title = {Querying Business Processes with BP-QL},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {A business process consists of a group of business activities undertaken by one or more organizations in pursuit of some particular goal. It usually depends upon various business functions for support, e.g. personnel, accounting, inventory, and interacts with other business processes/activities carried by the same or other organizations. Consequently, the software implementing a business processes typically operates in a cross-organization, distributed environment.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1255–1258},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@dataset{10.1145/review-1083592.1083741_R40284,
author = {Strnadl, Christoph F.},
title = {Review ID:R40284 for DOI: 10.5555/1083592.1083741},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1083592.1083741_R40284}
}

@inproceedings{10.5555/1083592.1083743,
author = {Kuntschke, Richard and Stegmaier, Bernhard and Kemper, Alfons and Reiser, Angelika},
title = {<i>StreamGlobe</i>: Processing and Sharing Data Streams in Grid-Based P2P Infrastructures},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Data stream processing is currently gaining importance due to the developments in novel application areas like e-science, e-health, and e-business (considering RFID, for example). Focusing on e-science, it can be observed that scientific experiments and observations in many fields, e. g., in physics and astronomy, create huge volumes of data which have to be interchanged and processed. With experimental and observational data coming in particular from sensors, online simulations, etc., the data has an inherently streaming nature. Furthermore, continuing advances will result in even higher data volumes, rendering storing all of the delivered data prior to processing increasingly impractical. Hence, in such e-science scenarios, processing and sharing of data streams will play a decisive role. It will enable new possibilities for researchers, since they will be able to subscribe to interesting data streams of other scientists without having to set up their own devices or experiments. This results in much better utilization of expensive equipment such as telescopes, satellites, etc. Further, processing and sharing data streams on-the-fly in the network helps to reduce network traffic and to avoid network congestion. Thus, even huge streams of data can be handled efficiently by removing unnecessary parts early on, e. g., by early filtering and aggregation, and by sharing previously generated data streams and processing results.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1259–1262},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083744,
author = {Bender, Matthias and Michel, Sebastian and Triantafillou, Peter and Weikum, Gerhard and Zimmer, Christian},
title = {MINERVA: Collaborative P2P Search},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {This paper proposes the live demonstration of a prototype of MINERVA, a novel P2P Web search engine. The search engine is layered on top of a DHT-based overlay network that connects an a-priori unlimited number of peers, each of which maintains a personal local database and a local search facility. Each peer posts a small amount of metadata to a physically distributed directory that is used to efficiently select promising peers from across the peer population that can best locally execute a query. The proposed demonstration serves as a proof of concept for P2P Web search by deploying the project on standard notebook PCs and also invites everybody to join the network by instantly installing a small piece of software from a USB memory stick.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1263–1266},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083745,
author = {Bonifati, Angela and Chang, Elaine Qing and Lakshmanan, Laks V. S. and Ho, Terence and Pottinger, Rachel},
title = {HePToX: Marrying XML and Heterogeneity in Your P2P Databases},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {We present HePToX, a full-fledged peer-to-peer database system that efficiently handles XML data heterogeneity. In a highly dynamic P2P network, it is unrealistic for a peer entering the network to be forced to agree on a global mediated schema, or to perform heavy-weight operations for mapping its schema to neighboring schemas. In our demo, we show that to enter the HePToX network a peer user is only asked to draw a simple set of visual annotations to a few other schemas. We show how the mapping rules are then automatically generated and how efficient query translation is performed on top of these mappings.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1267–1270},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083747,
author = {Cheng, Reynold and Singh, Sarvjeet and Prabhakar, Sunil},
title = {U-DBMS: A Database System for Managing Constantly-Evolving Data},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In many systems, sensors are used to acquire information from external environments such as temperature, pressure and locations. Due to continuous changes in these values, and limited resources (e.g., network bandwidth and battery power), it is often infeasible for the database to store the exact values at all times. Queries that uses these old values can produce invalid results. In order to manage the uncertainty between the actual sensor value and the database value, we propose a system called U-DBMS. U-DBMS extends the database system with uncertainty management functionalities. In particular, each data value is represented as an interval and a probability distribution function, and it can be processed with probabilistic query operators to produce imprecise (but correct) answers. This demonstration presents a PostgreSQL-based system that handles uncertainty and probabilistic queries for constantly-evolving data.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1271–1274},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083748,
author = {Galindo-Legaria, Cesar and Grabs, Torsten and Kleinerman, Christian and Waas, Florian},
title = {Database Change Notifications: Primitives for Efficient Database Query Result Caching},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Many database applications implement caching of data from a back-end database server to avoid repeated round trips to the back-end and to improve response times for end-user requests. For example, consider a web application that caches dynamic web content in the mid-tier [3, 2]. The content of dynamic web pages is usually assembled from data stored in the underlying database system and subject to modification whenever the data sources are modified. The workload is ideal for caching query results: most queries are read-only (browsing sessions) and only a small portion of the queries are actually modifying data. Caching at the mid-tier helps off-load the back-end database servers and can increase scalability of a distributed system drastically.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1275–1278},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083749,
author = {Petrovic, Milenko and Liu, Haifeng and Jacobsen, Hans-Arno},
title = {CMS-ToPSS: Efficient Dissemination of RSS Documents},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Recent years have seen a rise in the number of unconventional publishing tools on the Internet. Tools such as wikis, blogs, discussion forums, and web-based content management systems have experienced tremendous rise in popularity and use; primarily because they provide something traditional tools do not: easy of use for non computer-oriented users and they are based on the idea of "collaboration." It is estimated, by pewinternet.org, that 32 million people in the US read blogs (which represents 27% of the estimated 120 million US Internet users) while 8 million people have said that they have created blogs.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1279–1282},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083751,
author = {Bernstein, Philip A. and Melnik, Sergey and Mork, Peter},
title = {Interactive Schema Translation with Instance-Level Mappings},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {We demonstrate a prototype that translates schemas from a source metamodel (e.g., OO, relational, XML) to a target metamodel. The prototype is integrated with Microsoft Visual Studio 2005 to generate relational schemas from an object-oriented design. It has four novel features. First, it produces instance mappings to round-trip the data between the source schema and the generated target schema. It compiles the instance mappings into SQL views to reassemble the objects stored in relational tables. Second, it offers interactive editing, i.e., incremental modifications of the source schema yield incremental modifications of the target schema. Third, it incorporates a novel mechanism for mapping inheritance hierarchies to relations, which supports all known strategies and their combinations. Fourth, it is integrated with a commercial product featuring a high-quality user interface. The schema translation process is driven by high-level rules that eliminate constructs that are absent from the target metamodel.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1283–1286},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083752,
author = {Zadorozhny, Vladimir and Gal, Avigdor and Raschid, Louiqa and Ye, Qiang},
title = {AReNA: Adaptive Distributed Catalog Infrastructure Based on Relevance Networks},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Wide area applications (WAAs) utilize a WAN infrastructure (e.g., the Internet) to connect a federation of hundreds of servers with tens of thousands of clients. Earlier generations of WAA relied on Web accessible sources and the http protocol for data delivery. Recent developments such as the PlanetLab [8] testbed is now demonstrating an emerging class of data- and compute- intensive wide area applications.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1287–1290},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083753,
author = {Rodr\'{\i}guez-Gianolli, Patricia and Kementsietsidis, Anastasios and Garzetti, Maddalena and Kiringa, Iluju and Jiang, Lei and Masud, Mehedi and Miller, Ren\'{e}e J. and Mylopoulos, John},
title = {Data Sharing in the Hyperion Peer Database System},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {This demo presents Hyperion, a prototype system that supports data sharing for a network of independent Peer Relational Database Management Systems (PDBMSs). The nodes of such a network are assumed to be autonomous PDBMSs that form acquaintances at run-time, and manage mapping tables to define value correspondences among different databases. They also use distributed Event-Condition-Action (ECA) rules to enable and coordinate data sharing. Peers perform local querying and update processing, and also propagate queries and updates to their acquainted peers. The demo illustrates the following key functionalities of Hyperion: (1) the use of (data level) mapping tables to infer new metadata as peers dynamically join the network, (2) the ability to answer queries using data in acquaintances, and (3) the ability to coordinate peers through update propagation.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1291–1294},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083755,
author = {Ali, M. H. and Aref, W. G. and Bose, R. and Elmagarmid, A. K. and Helal, A. and Kamel, I. and Mokbel, M. F.},
title = {Nile-PDT: A Phenomenon Detection and Tracking Framework for Data Stream Management Systems},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In this demo, we present Nile-PDT, a Phenomenon Detection and Tracking framework using the Nile data stream management system. A phenomenon is characterized by a group of streams showing similar behavior over a period of time. The functionalities of Nile-PDT is split between the Nile server and the Nile-PDT application client. At the server side, Nile detects phenomenon candidate members and tracks their propagation incrementally through specific sensor network operators. Phenomenon candidate members are processed at the client side to detect phenomena of interest to a particular application. Nile-PDT is scalable in the number of sensors, the sensor data rates, and the number of phenomena. Guided by the detected phenomena, Nile-PDT tunes query processing towards sensors that heavily affect the monitoring of phenomenon propagation.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1295–1298},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083756,
author = {Schmidt, Sven and Legler, Thomas and Sch\"{a}r, Sebastian and Lehner, Wolfgang},
title = {Robust Real-Time Query Processing with QStream},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Processing data streams with Quality-of-Service (QoS) guarantees is an emerging area in existing streaming applications. Although it is possible to negotiate the result quality and to reserve the required processing resources in advance, it remains a challenge to adapt the DSMS to data stream characteristics which are not known in advance or are difficult to obtain. Within this paper we present the second generation of our QStream DSMS which addresses the above challenge by using a real-time capable operating system environment for resource reservation and by applying an adaptation mechanism if the data stream characteristics change spontaneously.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1299–1301},
numpages = {3},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083757,
author = {Chi, Yun and Wang, Haixun and Yu, Philip S.},
title = {Loadstar: Load Shedding in Data Stream Mining},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In this demo, we show that intelligent load shedding is essential in achieving optimum results in mining data streams under various resource constraints. The Loadstar system introduces load shedding techniques to classifying multiple data streams of large volume and high speed. Loadstar uses a novel metric known as the quality of decision (QoD) to measure the level of uncertainty in classification. Resources are then allocated to sources where uncertainty is high. To make optimum classification decisions and accurate QoD measurement, Loadstar relies on feature prediction to model the data dropped by the load shedding mechanism. Furthermore, Loadstar is able to adapt to the changing data characteristics in data streams. The system thus offers a nice solution to data mining with resource constraints.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1302–1305},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083759,
author = {Dittrich, Jens-Peter and Salles, Marcos Antonio Vaz and Kossmann, Donald and Blunschi, Lukas},
title = {IMeMex: Escapes from the Personal Information Jungle},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Modern computer work stations provide thousands of applications that store data in &gt; 100.000 files on the file system of the underlying OS. To handle these files data processing logic is reinvented inside each application. This results in a jungle of data processing solutions and a jungle of data and file formats. For a user, it is extremely hard to manage information in this jungle. Most of all it is impossible to use data distributed among different files and formats for combined queries, e.g., join and union operations. To solve the problems arising from file based data management, we present a software system called iMeMex as a unified solution to personal information management and integration. iMeMex is designed to integrate seamlessly into existing operating systems like Windows, Linux and Mac OS X. Our system enables existing applications to gradually dispose file based storage. By using iMeMex modern operating systems are enabled to make use of sophisticated DBMS, IR and data integration technologies. The seamless integration of iMeMex into existing operating systems enables new applications that provide concepts of data storage and analysis unseen before.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1306–1309},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083760,
author = {Amer-Yahia, Sihem and Fundulaki, Irini and Jain, Prateek and Lakshmanan, Laks},
title = {Personalizing XML Text Search in PIMENT},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {A growing number of text-rich XML repositories are being made available. As a result, more efforts have been deployed to provide XML full-text search that combines querying structure with complex conditions on text ranging from simple keyword search to sophisticated proximity search composed with stemming and thesaurus. However, one of the key challenges in full-text search is to match users' expectations and determine the most relevant answers to a full-text query. In this context, we propose query personalization as a way to take user profiles into account in order to customize query answers based on individual users' needs.We present PIMENT, a system that enables query personalization by query rewriting and answer ranking. PIMENT is composed of a profile repository that stores user profiles, a query customizer that rewrites user queries based on user profiles and, a ranking module to rank query answers.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1310–1313},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083761,
author = {He, Hai and Meng, Weiyi and Yu, Clement and Wu, Zonghuan},
title = {WISE-Integrator: A System for Extracting and Integrating Complex Web Search Interfaces of the Deep Web},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {We demonstrate WISE-Integrator - an automatic search interface extraction and integration tool. The basic research issues behind this tool will also be explained.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1314–1317},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083763,
author = {Zhou, Xuan and Pang, HweeHwa and Tan, Kian-Lee and Mangla, Dhruv},
title = {WmXML: A System for Watermarking XML Data},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {As increasing amount of data is published in the form of XML, copyright protection of XML data is becoming an important requirement for many applications. While digital watermarking is a widely used measure to protect digital data from copyright offences, the complex and flexible construction of XML data poses a number of challenges to digital watermarking, such as re-organization and alteration attacks. To overcome these challenges, the watermarking scheme has to be based on the usability of data and the underlying semantics like key attributes and functional dependencies. In this paper, we describe WmXML, a system for watermarking XML documents. It generates queries from essential semantics to identify the available watermarking bandwidth in XML documents, and integrates query rewriting technique to overcome the threats from data re-organization and alteration. In the demonstration, we will showcase the use of WmXML and its effectiveness in countering various attacks.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1318–1321},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083764,
author = {Boncz, Peter and Grust, Torsten and van Keulen, Maurice and Manegold, Stefan and Rittinger, Jan and Teubner, Jens},
title = {Pathfinder: XQuery---the Relational Way},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Relational query processors are probably the best understood (as well as the best engineered) query engines available today. Although carefully tuned to process instances of the relational model (tables of tuples), these processors can also provide a foundation for the evaluation of "alien" (non-relational) query languages: if a relational encoding of the alien data model and its associated query language is given, the RDBMS may act like a special-purpose processor for the new language.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1322–1325},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083765,
author = {Cho, SungRan and Koudas, Nick and Srivastava, Divesh},
title = {MIX: A Meta-Data Indexing System for XML},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1326–1329},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083766,
author = {Arion, Andrei and Benzaken, V\'{e}ronique and Manolescu, Ioana and Vijay, Ravi},
title = {ULoad: Choosing the Right Storage for Your XML Application},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {A key factor for the outstanding success of database management systems is physical data independence: queries, and application programs, are able to refer to the data at the logical level, ignoring the details on how the data is physically stored and accessed by the system. The corner stone of implementing physical data independence is an access path selection algorithm: whenever a disk-resident data item can be accessed in several ways, the access path selection algorithm, which is part of the query optimizer, will identify the possible alternatives, and choose the one likely to provide the best performance for a given query [13].},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1330–1333},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083768,
author = {Ross, Kenneth A. and Janevski, Angel and Stoyanovich, Julia},
title = {A Faceted Query Engine Applied to Archaeology},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In this demonstration, we describe a system for storing and querying faceted hierarchies. We have developed a general faceted domain model and a query language for hierarchically classified data. We present here the use of our system on two real archaeological datasets containing thousands of artifacts. Our system is a sharable, evolvable resource that can provide global access to sizeable datasets in queriable format, and can serve as a valuable tool for data analysis and research in many application domains.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1334–1337},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083769,
author = {Liu, Bin and Zhu, Yali and Jbantova, Mariana and Momberger, Bradley and Rundensteiner, Elke A.},
title = {A Dynamically Adaptive Distributed System for Processing Complex Continuous Queries},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Recent years have witnessed rapidly growing research attention on continuous query processing over streams [2, 3]. A continuous query system can easily run out of resources in case of large amount of input stream data. Distributed continuous query processing over a shared nothing architecture, i.e., a cluster of machines, has been recognized as a scalable method to solve this problem [2, 8, 9]. Due to the lack of initial cost information and the fluctuating nature of the streaming data, uneven workload among machines may occur and this may impair the benefits of distributed processing. Thus dynamic adaptation techniques are crucial for a distributed continuous query system.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1338–1341},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083770,
author = {Li, Chengkai and Soliman, Mohamed A. and Chang, Kevin Chen-Chuan and Ilyas, Ihab F.},
title = {RankSQL: Supporting Ranking Queries in Relational Database Management Systems},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Ranking queries (or top-k queries) are dominant in many emerging applications, e.g., similarity queries in multimedia databases, searching Web databases, middleware, and data mining. The increasing importance of top-k queries warrants an efficient support of ranking in the relational database management system (RDBMS) and has recently gained the attention of the research community. Top-k queries aim at providing only the top k query results, according to a user-specified ranking function, which in many cases is an aggregate of multiple criteria. The following is an example top-k query.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1342–1345},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083771,
author = {Catania, Barbara and Maddalena, Anna and Mazza, Maurizio},
title = {PSYCHO: A Prototype System for Pattern Management},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Patterns represent in a compact and rich in semantics way huge quantity of heterogeneous data. Due to their characteristics, specific systems are required for pattern management, in order to model and manipulate patterns, with a possibly user-defined structure, in an efficient and effective way. In this demonstration we present PSYCHO, a pattern based management system prototype. PSYCHO allows the user to: (i) use standard pattern types or define new ones; (ii) generate or import patterns, represented according to existing standards; (iii) manipulate possibly heterogeneous patterns under an integrated environment.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1346–1349},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083773,
author = {Nambiar, Ullas and Kambhampati, Subbarao},
title = {Answering Imprecise Queries over Web Databases},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {The rapid expansion of the World Wide Web has made a large number of databases like bibliographies, scientific databases etc. to become accessible to lay users demanding "instant gratification". Often, these users may not know how to precisely express their needs and may formulate queries that lead to unsatisfactory results.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1350–1353},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083774,
author = {Fuxman, Ariel and Fuxman, Diego and Miller, Ren\'{e}e J.},
title = {ConQuer: A System for Efficient Querying over Inconsistent Databases},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Although integrity constraints have long been used to maintain data consistency, there are situations in which they may not be enforced or satisfied. In this demo, we showcase ConQuer, a system for efficient and scalable answering of SQL queries on databases that may violate a set of constraints. ConQuer permits users to postulate a set of key constraints together with their queries. The system rewrites the queries to retrieve all (and only) data that is consistent with respect to the constraints. The rewriting is into SQL, so the rewritten queries can be efficiently optimized and executed by commercial database systems.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1354–1357},
numpages = {4},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083775,
author = {Li, Wen-Syan and Batra, Vishal S. and Raman, Vijayshankar and Han, Wei and Narang, Inderpal},
title = {QoS-Based Data Access and Placement for Federated Systems},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {A wide variety of applications require access to multiple heterogeneous, distributed data sources. By transparently integrating such diverse data sources, underlying differences in DBMSs, languages, and data models can be hidden and users can use a single data model and a single high-level query language to access the unified data through a global schema.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1358–1362},
numpages = {5},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083777,
author = {Koudas, Nick and Srivastava, Divesh},
title = {Approximate Joins: Concepts and Techniques},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {The quality of the data residing in information repositories and databases gets degraded due to a multitude of reasons. Such reasons include typing mistakes during insertion (e.g., character transpositions), lack of standards for recording database fields (e.g., addresses), and various errors introduced by poor database design (e.g., missing integrity constraints). Data of poor quality can result in significant impediments to popular business practices: sending products or bills to incorrect addresses, inability to locate customer records during service calls, inability to correlate customers across multiple services, etc.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1363},
numpages = {1},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083778,
author = {Guha, Sudipto and Shim, Kyuseok},
title = {Offline and Data Stream Algorithms for Efficient Computation of Synopsis Structures},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {Synopsis and small space representations are important data analysis tools and have long been used OLAP/DSS systems, approximate query answering, query optimization and data mining. These techniques represent the input in terms broader characteristics and improve efficiency of various applications, e.g., learning, classification, event detection, among many others. In a recent past, the synopsis techniques have gained more currency due to the emerging areas like data stream management.In this tutorial, we propose to revisit algorithms for Wavelet and Histogram synopsis construction. In the recent years, a significant number of papers have appeared which has advanced the state-of-the-art in synopsis construction considerably. In particular, we have seen the development of a large number of efficient algorithms which are also guaranteed to be near optimal. Furthermore, these synopsis construction problems have found deep roots in theory and database systems, and have influenced a wide range of problems. In a different level, a large number of the synopsis construction algorithms use a similar set of techniques. It is extremely valuable to discuss and analyze these techniques, and we expect broader pictures and paradigms to emerge. This would allow us to develop algorithms for newer problems with greater ease. Understanding these recurrent themes and intuition behind the development of these algorithms is one of the main thrusts of the tutorial.Our goal will be to cover a wide spectrum of these topics and make the researchers in VLDB community aware of the new algorithms, optimum or approximate, offline or streaming. The tutorial will be self contained and develop most of the mathematical and database backgrounds needed.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1364},
numpages = {1},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083779,
author = {Ioannidis, Yannis and Koutrika, Georgia},
title = {Personalized Systems: Models and Methods from an IR and DB Perspective},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In today's knowledge-driven society, information abundance and personal electronic device ubiquity have made it difficult for users to find the right information at the right time and at the right level of detail. To solve this problem, researchers have developed systems that adapt their behavior to the goals, tasks, interests, and other characteristics of their users. Based on models that capture important user characteristics, these personalized systems maintain their users' profiles and take them into account to customize the content generated or its presentation to the different individuals.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1365},
numpages = {1},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083780,
author = {\O{}hrn, Aleksander},
title = {Contextual Insight in Search: Enabling Technologies and Applications},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {When the fields of efficient and scalable search, data aggregation, XML search and information extraction come together, we get a powerful and exciting mix. Taken together, these are enabling technologies that make it possible to search for information in new ways and that make new types of search applications possible. Using a state-of-the-art enterprise search platform as an example, this tutorial outlines the workings of these technologies and presents some applications of their intersection.Search engines differ from traditional databases in several ways, yet they both address the issue of organizing information and making it retrievable. The anatomy of a modern search engine will be presented. Although search engines are typically associated with web search, some are now equipped with features usually associated with databases and structured data, e.g., the ability to do aggregation of meta data across a full result set.Most search engines are built around a flat document model: A document is seen as a collection of typed fields, but the fields themselves have no particular structure. As such, queries tend to focus on document content, with little or no constraints on document structure. Recently, however, scalable and efficient search engines have appeared that support indexing and retrieval of complex XML. Queries that are posed to a search engine can thus combine content and structure in ways that enable extreme search precision. Furthermore, data aggregation can be restricted to take place on the level of the matching document fragments instead of on the document level, thus providing more contextually relevant statistics.Given XML capabilities, applying text mining and information extraction techniques to the content becomes particularly interesting: By automatically detecting semantic entities, and possibly also relations between these, this information can be searched for in different ways and, e.g., enable applications that have a strong element of discovery to them.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1366},
numpages = {1},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083781,
author = {Aberer, Karl and Cudr\'{e}-Mauroux, Philippe},
title = {Semantic Overlay Networks},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {In a handful of years only, Peer-to-Peer (P2P) systems have become an integral part of the Internet. After a few key successes related to music-sharing (e.g., Napster or Gnutella), they rapidly developed and are nowadays firmly established in various contexts, ranging from large-scale content distribution (Bit Torrent) to Internet telephony(Skype) or networking platforms (JXTA). The main idea behind P2P is to leverage on the power of end-computers: Instead of relying on central components (e.g., servers), services are powered by decentralized overlay architectures where end-computers connect to each other dynamically.},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1367},
numpages = {1},
location = {Trondheim, Norway},
series = {VLDB '05}
}

@inproceedings{10.5555/1083592.1083782,
author = {Amer-Yahia, Sihem and Shanmugasundaram, Jayavel},
title = {XML Full-Text Search: Challenges and Opportunities},
year = {2005},
isbn = {1595931546},
publisher = {VLDB Endowment},
abstract = {An ever growing number of XML repositories are being made available for search. A lot of activity has been deployed in the past few years to query such repositories. In particular, full-text querying of text-rich XML documents has generated a wealth of issues that are being addressed by both the database (DB) and information retrieval (IR) communities. The DB community has traditionally focused on developing query languages and efficient evaluation algorithms for highly structured data. In contrast, the IR community has focused on searching unstructured data, and has developed various techniques for ranking query results and evaluating their effectiveness. Fortunately, recent trends in DB and IR research demonstrate a growing interest in adopting IR techniques in DBs and vice versa [1, 2, 3, 4, 5, 6, 7, 9].},
booktitle = {Proceedings of the 31st International Conference on Very Large Data Bases},
pages = {1368},
numpages = {1},
location = {Trondheim, Norway},
series = {VLDB '05}
}

