@inproceedings{10.5555/1325851.1325853,
author = {Vogels, Werner},
title = {Data Access Patterns in the Amazon.Com Technology Platform},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The Amazon.com technology platform provides a set of highly advanced business and infrastructure services implemented using ultra-scalable distributed systems technologies. Within this environment we can identify a number of specific data access patterns, each with their own availability, consistency, performance and operational requirements in order to serve a collection of highly diverse business processes. In this presentation we will reviews these different patterns in detail and discuss which technologies are required to support them in an always-on environment.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1},
numpages = {1},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325854,
author = {Brewer, Eric},
title = {Technology for Developing Regions},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Moore's Law and the wave of technologies it enabled have led to tremendous improvements in productivity and the quality of life in industrialized nations. Yet, technology has had almost no effect on the other five billion people on our planet. In this talk I argue that decreasing costs of computing and wireless networking make this the right time to spread the benefits of technology, and that the biggest missing piece is a lack of focus on the problems that matter. After covering some example applications that have shown very high impact, I present some our own preliminary results, including the use of novel low-cost telemedicine to improve the vision of real people, with over 20,000 patients examined so far. I conclude with some discussion on the role of database researchers in this new area.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {2},
numpages = {1},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325856,
author = {Chaudhuri, Surajit and Narasayya, Vivek},
title = {Self-Tuning Database Systems: A Decade of Progress},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In this paper we discuss advances in self-tuning database systems over the past decade, based on our experience in the AutoAdmin project at Microsoft Research. This paper primarily focuses on the problem of automated physical database design. We also highlight other areas where research on self-tuning database technology has made significant progress. We conclude with our thoughts on opportunities and open issues.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {3–14},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325858,
author = {Pei, Jian and Jiang, Bin and Lin, Xuemin and Yuan, Yidong},
title = {Probabilistic Skylines on Uncertain Data},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Uncertain data are inherent in some important applications. Although a considerable amount of research has been dedicated to modeling uncertain data and answering some types of queries on uncertain data, how to conduct advanced analysis on uncertain data remains an open problem at large. In this paper, we tackle the problem of skyline analysis on uncertain data. We propose a novel probabilistic skyline model where an uncertain object may take a probability to be in the skyline, and a p-skyline contains all the objects whose skyline probabilities are at least p. Computing probabilistic skylines on large uncertain data sets is challenging. We develop two efficient algorithms. The bottom-up algorithm computes the skyline probabilities of some selected instances of uncertain objects, and uses those instances to prune other instances and uncertain objects effectively. The top-down algorithm recursively partitions the instances of uncertain objects into subsets, and prunes subsets and objects aggressively. Our experimental results on both the real NBA player data set and the benchmark synthetic data sets show that probabilistic skylines are interesting and useful, and our two algorithms are efficient on large data sets, and complementary to each other in performance.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {15–26},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325859,
author = {Kimelfeld, Benny and Sagiv, Yehoshua},
title = {Matching Twigs in Probabilistic XML},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Evaluation of twig queries over probabilistic XML is investigated. Projection is allowed and, in particular, a query may be Boolean. It is shown that for a well-known model of probabilistic XML, the evaluation of twigs with projection is tractable under data complexity (whereas in other probabilistic data models, projection is intractable). Under query-and-data complexity, the problem becomes intractable even without projection (and for rather simple twigs and data).In earlier work on probabilistic XML, answers are always complete. However, there is often a need to produce partial answers because XML data may have missing sub-elements and, furthermore, complete answers may be deemed irrelevant if their probabilities are too low. It is shown how to define a semantics that provides partial answers that are maximal with respect to a probability threshold, which is specified by the user. For this semantics, it is shown how to efficiently evaluate twigs, even under query-and-data complexity if there is no projection.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {27–38},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325860,
author = {Burdick, Doug and Doan, AnHai and Ramakrishnan, Raghu and Vaithyanathan, Shivakumar},
title = {OLAP over Imprecise Data with Domain Constraints},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Several recent papers have focused on OLAP over imprecise data, where each fact can be a region, instead of a point, in a multi-dimensional space. They have provided a multiple-world semantics for such data, and developed efficient ways to answer OLAP aggregation queries over the imprecise facts. These solutions, however, assume that the imprecise facts can be interpreted independently of one another, a key assumption that is often violated in practice. Indeed, imprecise facts in real-world applications are often correlated, and such correlations can be captured as domain integrity constraints (e.g., repairs with the same customer names and models took place in the same city, or a text span can refer to a person or a city, but not both).In this paper we provide a framework for answering OLAP aggregation queries over imprecise data in the presence of such domain constraints. We first describe a relatively simple yet powerful constraint language, and formalize what it means to take into account such constraints in query answering. Next, we prove that OLAP queries can be answered efficiently given a database D* of fact marginals. We then exploit the regularities in the constraint space (captured in a constraint hypergraph) and the fact space to efficiently construct D*. We present extensive experiments over real-world and synthetic data to demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {39–50},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325861,
author = {R\'{e}, Christopher and Suciu, Dan},
title = {Materialized Views in Probabilistic Databases: For Information Exchange and Query Optimization},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Views over probabilistic data contain correlations between tuples, and the current approach is to capture these correlations using explicit lineage. In this paper we propose an alternative approach to materializing probabilistic views, by giving conditions under which a view can be represented by a block-independent disjoint (BID) table. Not all views can be represented as BID tables and so we propose a novel partial representation that can represent all views but may not define a unique probability distribution. We then give conditions on when a query's value on a partial representation will be uniquely defined. We apply our theory to two applications: query processing using views and information exchange using views. In query processing on probabilistic data, we can ignore the lineage and use materialized views to more efficiently answer queries. By contrast, if the view has explicit lineage, the query evaluation must reprocess the lineage to compute the query resulting in dramatically slower execution. The second application is information exchange when we do not wish to disclose the entire lineage, which otherwise may result in shipping the entire database. The paper contains several theoretical results that completely solve the problem of deciding whether a conjunctive view can be represented as a BID and whether a query on a partial representation is uniquely determined. We validate our approach experimentally showing that representable views exist in real and synthetic workloads and show over three magnitudes of improvement in query processing versus a lineage based approach.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {51–62},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325863,
author = {Tatikonda, Shirish and Parthasarathy, Srinivasan and Goyder, Matthew},
title = {LCS-TRIM: Dynamic Programming Meets XML Indexing and Querying},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In this article, we propose a new approach for querying and indexing a database of trees with specific applications to XML datasets. Our approach relies on representing both the queries and the data using a sequential encoding and then subsequently employing an innovative variant of the longest common subsequence (LCS) matching algorithm to retrieve the desired results. A key innovation here is the use of a series of inter-linked early pruning steps, coupled with a simple index structure that enable us to reduce the search space and eliminate a large number of false positive matches prior to applying the more expensive LCS matching algorithm. Additionally, we also present mechanisms that enable the user to specify constraints on the retrieved output and show how such constraints can be pushed deep into the retrieval process, leading to improved response times. Mechanisms supporting the retrieval of approximate matches are also supported. When compared with state-of-the-art approaches, the query processing time of our algorithms is shown to be up to two to three orders of magnitude faster on several real datasets on realistic query workloads. Finally, we show that our approach is suitable for emerging multi-core server architectures when retrieving data for more expensive queries.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {63–74},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325864,
author = {Botan, Irina and Kossmann, Donald and Fischer, Peter M. and Kraska, Tim and Florescu, Dana and Tamosevicius, Rokas},
title = {Extending XQuery with Window Functions},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {This paper presents two extensions for XQuery. The first extension allows the definition and processing of different kinds of windows over an input sequence; i.e., tumbling, sliding, and landmark windows. The second extension extends the XQuery data model (XDM) to support infinite sequences. This extension makes it possible to use XQuery as a language for continuous queries. Both extensions have been integrated into a Java-based open source XQuery engine. This paper gives details of this implementation and presents the results of running the Linear Road benchmark on the extended XQuery engine.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {75–86},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325865,
author = {Arion, Andrei and Benzaken, V\'{e}ronique and Manolescu, Ioana and Papakonstantinou, Yannis},
title = {Structured Materialized Views for XML Queries},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The performance of XML database queries can be greatly enhanced by rewriting them using materialized views. We study the problem of rewriting a query using materialized views, where both the query and the views are described by a tree pattern language, appropriately extended to capture a large XQuery subset. The pattern language features optional nodes and nesting, allowing to capture the data needs of nested XQueries. The language also allows describing storage features such as structural identifiers, which enlarge the space of rewritings. We study pattern containment and equivalent rewriting under the constraints expressed in a structural summary, whose enhanced form also entails integrity constraints. Our approach is implemented in the ULoad [7] prototype and we present a performance analysis.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {87–98},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325866,
author = {Zhang, Ying and Boncz, Peter},
title = {XRPC: Interoperable and Efficient Distributed XQuery},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {We propose XRPC, a minimal XQuery extension that enables distributed yet efficient querying of heterogeneous XQuery data sources. XRPC enhances the existing concept of XQuery functions with the Remote Procedure Call (RPC) paradigm. By calling out of an XQuery for-loop to multiple destinations, and by calling functions that themselves perform XRPC calls, complex P2P communication patterns can be achieved. The XRPC extension is orthogonal to all XQuery features, including the XQuery Update Facility (XQUF). We provide formal semantics for XRPC that encompasses execution of both read-only and update queries.XRPC is also a network SOAP sub-protocol, that integrates seamlessly with web services and Service Oriented Architectures (SOA), and AJAX-based GUIs. A crucial feature of the protocol is bulk RPC, that allows remote execution of many different calls to the same procedure, using possibly a single network round-trip. The efficiency potential of XRPC is demonstrated via an open-source implementation in MonetDB/XQuery. We show, however, that XRPC is not system-specific: every XQuery data source can service XRPC calls using a wrapper.Since XQuery is a pure functional language, we can leverage techniques developed for functional query decomposition to rewrite data shipping queries into XRPC-based function shipping queries. Powerful distributed database techniques (such as semi-join optimizations) directly map on bulk RPC, opening up interesting future work opportunities.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {99–110},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325868,
author = {Wong, W. K. and Cheung, David W. and Hung, Edward and Kao, Ben and Mamoulis, Nikos},
title = {Security in Outsourcing of Association Rule Mining},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Outsourcing association rule mining to an outside service provider brings several important benefits to the data owner. These include (i) relief from the high mining cost, (ii) minimization of demands in resources, and (iii) effective centralized mining for multiple distributed owners. On the other hand, security is an issue; the service provider should be prevented from accessing the actual data since (i) the data may be associated with private information, (ii) the frequency analysis is meant to be used solely by the owner. This paper proposes substitution cipher techniques in the encryption of transactional data for outsourcing association rule mining. After identifying the non-trivial threats to a straightforward one-to-one item mapping substitution cipher, we propose a more secure encryption scheme based on a one-to-n item mapping that transforms transactions non-deterministically, yet guarantees correct decryption. We develop an effective and efficient encryption algorithm based on this method. Our algorithm performs a single pass over the database and thus is suitable for applications in which data owners send streams of transactions to the service provider. A comprehensive cryptanalysis study is carried out. The results show that our technique is highly secure with a low data transformation cost.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {111–122},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325869,
author = {di Vimercati, Sabrina De Capitani and Foresti, Sara and Jajodia, Sushil and Paraboschi, Stefano and Samarati, Pierangela},
title = {Over-Encryption: Management of Access Control Evolution on Outsourced Data},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Data outsourcing is emerging today as a successful paradigm allowing users and organizations to exploit external services for the distribution of resources. A crucial problem to be addressed in this context concerns the enforcement of selective authorization policies and the support of policy updates in dynamic scenarios.In this paper, we present a novel solution to the enforcement of access control and the management of its evolution. Our proposal is based on the application of selective encryption as a means to enforce authorizations. Two layers of encryption are imposed on data: the inner layer is imposed by the owner for providing initial protection, the outer layer is imposed by the server to reflect policy modifications. The combination of the two layers provides an efficient and robust solution. The paper presents a model, an algorithm for the management of the two layers, and an analysis to identify and therefore counteract possible information exposure risks.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {123–134},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325870,
author = {Papadopoulos, Stavros and Yang, Yin and Papadias, Dimitris},
title = {CADS: Continuous Authentication on Data Streams},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {We study processing and authentication of long-running queries on outsourced data streams. In this scenario, a data owner (DO) constantly transmits its data to a service provider (SP), together with additional authentication information. Clients register continuous range queries to the SP. Whenever the data change, the SP must update the results of all affected queries and inform the clients accordingly. The clients can verify the correctness of the results using the authentication information provided by the DO. Compared to conventional databases, stream environments pose new challenges such as the need for fast structure updating, support for continuous query processing and authentication, and provision for temporal completeness. Specifically, in addition to the correctness of individual results, the client must be able to verify that there are no missing results in between updates.We face these challenges through several contributions. Since there is no previous work, we first present a technique, called REF, that achieves correctness and temporal completeness but incurs false transmissions, i.e., the SP has to inform clients whenever there is a data update, even if their results are not affected. Then, we propose CADS, which minimizes the processing and transmission overhead through an elaborate indexing scheme and a virtual caching mechanism. Finally, we extend CADS to the case where multiple owners outsource their data to the same SP. The SP integrates all data in a single authentication process, independently of the number of DOs.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {135–146},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325871,
author = {Li, Feifei and Yi, Ke and Hadjieleftheriou, Marios and Kollios, George},
title = {Proof-Infused Streams: Enabling Authentication of Sliding Window Queries on Streams},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {As computer systems are essential components of many critical commercial services, the need for secure online transactions is now becoming evident. The demand for such applications, as the market grows, exceeds the capacity of individual businesses to provide fast and reliable services, making outsourcing technologies a key player in alleviating issues of scale. Consider a stock broker that needs to provide a real-time stock trading monitoring service to clients. Since the cost of multicasting this information to a large audience might become prohibitive, the broker could outsource the stock feed to third-party providers, who are in turn responsible for forwarding the appropriate sub-feed to clients. Evidently, in critical applications the integrity of the third-party should not be taken for granted. In this work we study a variety of authentication algorithms for selection and aggregation queries over sliding windows. Our algorithms enable the end-users to prove that the results provided by the third-party are correct, i.e., equal to the results that would have been computed by the original provider. Our solutions are based on Merkle hash trees over a forest of space partitioning data structures, and try to leverage key features, like update, query, signing, and authentication costs. We present detailed theoretical analysis for our solutions and empirically evaluate the proposed techniques.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {147–158},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325873,
author = {Tatbul, Nesime and \c{C}etintemel, Uundefinedur and Zdonik, Stan},
title = {Staying FIT: Efficient Load Shedding Techniques for Distributed Stream Processing},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In distributed stream processing environments, large numbers of continuous queries are distributed onto multiple servers. When one or more of these servers become overloaded due to bursty data arrival, excessive load needs to be shed in order to preserve low latency for the query results. Because of the load dependencies among the servers, load shedding decisions on these servers must be well-coordinated to achieve end-to-end control on the output quality. In this paper, we model the distributed load shedding problem as a linear optimization problem, for which we propose two alternative solution approaches: a solver-based centralized approach, and a distributed approach based on metadata aggregation and propagation, whose centralized implementation is also available. Both of our solutions are based on generating a series of load shedding plans in advance, to be used under certain input load conditions. We have implemented our techniques as part of the Borealis distributed stream processing system. We present experimental results from our prototype implementation showing the performance of these techniques under different input and query workloads.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {159–170},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325874,
author = {Chen, Aiyou and Cao, Jin and Bu, Tian},
title = {A Simple and Efficient Estimation Method for Stream Expression Cardinalities},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Estimating the cardinality (i.e. number of distinct elements) of an arbitrary set expression defined over multiple distributed streams is one of the most fundamental queries of interest. Earlier methods based on probabilistic sketches have focused mostly on the sketching algorithms. However, the estimators do not fully utilize the information in the sketches and thus are not statistically efficient. In this paper, we develop a novel statistical model and an efficient yet simple estimator for the cardinalities based on a continuous variant of the well known Flajolet-Martin sketches. Specifically, we show that, for two streams, our estimator has almost the same statistical efficiency as the Maximum Likelihood Estimator (MLE), which is known to be optimal in the sense of Cramer-Rao lower bounds under regular conditions. Moreover, as the number of streams gets larger, our estimator is still computationally simple, but the MLE becomes intractable due to the complexity of the likelihood. Let N be the cardinality of the union of all streams, and |S| be the cardinality of a set expression S to be estimated. For a given relative standard error δ, the memory requirement of our estimator is O(δ-2|S|-1 N log log N), which is superior to state-of-the-art algorithms, especially for large N and small |S|/N where the estimation is most challenging.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {171–182},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325875,
author = {Das, Gautam and Gunopulos, Dimitrios and Koudas, Nick and Sarkas, Nikos},
title = {Ad-Hoc Top-k Query Answering for Data Streams},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {A top-k query retrieves the k highest scoring tuples from a data set with respect to a scoring function defined on the attributes of a tuple. The efficient evaluation of top-k queries has been an active research topic and many different instantiations of the problem, in a variety of settings, have been studied. However, techniques developed for conventional, centralized or distributed databases are not directly applicable to highly dynamic environments and on-line applications, like data streams.Recently, techniques supporting top-k queries on data streams have been introduced. Such techniques are restrictive however, as they can only efficiently report top-k answers with respect to a pre-specified (as opposed to ad-hoc) set of queries. In this paper we introduce a novel geometric representation for the top-k query problem that allows us to raise this restriction. Utilizing notions of geometric arrangements, we design and analyze algorithms for incrementally maintaining a data set organized in an arrangement representation under streaming updates. We introduce query evaluation strategies that operate on top of an arrangement data structure that are able to guarantee efficient evaluation for ad-hoc queries. The performance of our core technique is augmented by incorporating tuple pruning strategies, minimizing the number of tuples that need to be stored and manipulated. This results in a main memory indexing technique supporting both efficient incremental updates and the evaluation of ad-hoc top-k queries. A thorough experimental study evaluates the efficiency of the proposed technique.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {183–194},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325877,
author = {Lee, Hongrae and Ng, Raymond T. and Shim, Kyuseok},
title = {Extending Q-Grams to Estimate Selectivity of String Matching with Low Edit Distance},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {There are many emerging database applications that require accurate selectivity estimation of approximate string matching queries. Edit distance is one of the most commonly used string similarity measures. In this paper, we study the problem of estimating selectivity of string matching with low edit distance. Our framework is based on extending q-grams with wildcards. Based on the concepts of replacement semi-lattice, string hierarchy and a combinatorial analysis, we develop the formulas for selectivity estimation and provide the algorithm BasicEQ. We next develop the algorithm Opt EQ by enhancing BasicEQ with two novel improvements. Finally we show a comprehensive set of experiments using three benchmarks comparing Opt EQ with the state-of-the-art method SEPIA. Our experimental results show that Opt EQ delivers more accurate selectivity estimations.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {195–206},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325878,
author = {Litwin, Witold and Mokadem, Riad and Rigaux, Philippe and Schwarz, Thomas},
title = {Fast NGram-Based String Search over Data Encoded Using Algebraic Signatures},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {We propose a novel string search algorithm for data stored once and read many times. Our search method combines the sublinear traversal of the record (as in Boyer Moore or Knuth-Morris-Pratt) with the agglomeration of parts of the record and search pattern into a single character -- the algebraic signature -- in the manner of Karp-Rabin. Our experiments show that our algorithm is up to seventy times faster for DNA data, up to eleven times faster for ASCII, and up to a six times faster for XML documents compared with an implementation of Boyer-Moore. To obtain this speed-up, we store records in encoded form, where each original character is replaced with an algebraic signature.Our method applies to records stored in databases in general and to distributed implementations of a Database As Service (DAS) in particular. Clients send records for insertion and search patterns already in encoded form and servers never operate on records in clear text. No one at a node can involuntarily discover the content of the stored data.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {207–218},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325879,
author = {Nandi, Arnab and Jagadish, H. V.},
title = {Effective Phrase Prediction},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Autocompletion is a widely deployed facility in systems that require user input. Having the system complete a partially typed "word" can save user time and effort.In this paper, we study the problem of autocompletion not just at the level of a single "word", but at the level of a multi-word "phrase". There are two main challenges: one is that the number of phrases (both the number possible and the number actually observed in a corpus) is combinatorially larger than the number of words; the second is that a "phrase", unlike a "word", does not have a well-defined boundary, so that the autocompletion system has to decide not just what to predict, but also how far.We introduce a FussyTree structure to address the first challenge and the concept of a significant phrase to address the second. We develop a probabilistically driven multiple completion choice model, and exploit features such as frequency distributions to improve the quality of our suffix completions. We experimentally demonstrate the practicability and value of our technique for an email composition application and show that we can save approximately a fifth of the keystrokes typed.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {219–230},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325881,
author = {Zhou, Jingren and Larson, Per-Ake and Elmongui, Hicham G.},
title = {Lazy Maintenance of Materialized Views},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Materialized views can speed up query processing greatly but they have to be kept up to date to be useful. Today, database systems typically maintain views eagerly in the same transaction as the base table updates. This has the effect that updates pay for view maintenance while beneficiaries (queries) get a free ride! View maintenance overhead can be significant and it seems unfair to have updates bear the cost.We present a novel way to lazily maintain materialized views that relieves updates of this overhead. Maintenance of a view is postponed until the system has free cycles or the view is referenced by a query. View maintenance is fully or partly hidden from queries depending on the system load. Ideally, views are maintained entirely on system time at no cost to updates and queries. The efficiency of lazy maintenance is improved by combining updates from several transactions into a single maintenance operation, by condensing multiple updates of the same row into a single update, and by exploiting row versioning. Experiments using a prototype implementation in Microsoft SQL Server show much faster response times for updates and also significant reduction in maintenance cost when combining updates.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {231–242},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325882,
author = {Bravo, Loreto and Fan, Wenfei and Ma, Shuai},
title = {Extending Dependencies with Conditions},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {This paper introduces a class of conditional inclusion dependencies (CINDs), which extends traditional inclusion dependencies (INDs) by enforcing bindings of semantically related data values. We show that CINDs are useful not only in data cleaning, but are also in contextual schema matching [7]. To make effective use of CINDs in practice, it is often necessary to reason about them. The most important static analysis issue concerns consistency, to determine whether or not a given set of CINDs has conflicts. Another issue concerns implication, i.e., deciding whether a set of CINDs entails another CIND. We give a full treatment of the static analyses of CINDs, and show that CINDs retain most nice properties of traditional INDs: (a) CINDs are always consistent; (b) CINDs are finitely axiomatizable, i.e., there exists a sound and complete inference system for implication of CINDs; and (c) the implication problem for CINDs has the same complexity as its traditional counterpart, namely, PSPACE-complete, in the absence of attributes with a finite domain; but it is EXPTIME-complete in the general setting. In addition, we investigate the interaction between CINDs and conditional functional dependencies (CFDs), an extension of functional dependencies proposed in [9]. We show that the consistency problem for the combination of CINDs and CFDs becomes undecidable. In light of the undecidability, we provide heuristic algorithms for the consistency analysis of CFDs and CINDs, and experimentally verify the effectiveness and efficiency of our algorithms.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {243–254},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325883,
author = {Lim, Lipyeow and Wang, Haixun and Wang, Min},
title = {Unifying Data and Domain Knowledge Using Virtual Views},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The database community is on a constant quest for better integration of data management and knowledge management. Recently, with the increasing use of ontology in various applications, the quest has become more concrete and urgent. However, manipulating knowledge along with relational data in DBMSs is not a trivial undertaking. In this paper, we introduce a novel, unified framework for managing data and domain knowledge. We provide the user with a virtual view that unifies the data, the domain knowledge and the knowledge inferable from the data using the domain knowledge. Because the virtual view is in the relational format, users can query the data and the knowledge in a seamlessly integrated manner. To facilitate knowledge representation and inferencing within the database engine, our approach leverages XML support in hybrid relational-XML DBMSs (e.g., Microsoft SQL Server &amp; IBM DB2 9 PureXML). We provide a query rewriting mechanism to bridge the difference between logical and physical data modeling, so that queries on the virtual view can be automatically transformed to components that execute on the hybrid relational-XML engine in a way that is transparent to the user.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {255–266},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325885,
author = {Morse, Michael and Patel, Jignesh M. and Jagadish, H. V.},
title = {Efficient Skyline Computation over Low-Cardinality Domains},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Current skyline evaluation techniques follow a common paradigm that eliminates data elements from skyline consideration by finding other elements in the dataset that dominate them. The performance of such techniques is heavily influenced by the underlying data distribution (i.e. whether the dataset attributes are correlated, independent, or anti-correlated).In this paper, we propose the Lattice Skyline Algorithm (LS) that is built around a new paradigm for skyline evaluation on datasets with attributes that are drawn from low-cardinality domains. LS continues to apply even if one attribute has high cardinality. Many skyline applications naturally have such data characteristics, and previous skyline methods have not exploited this property. We show that for typical dimensionalities, the complexity of LS is linear in the number of input tuples. Furthermore, we show that the performance of LS is independent of the input data distribution. Finally, we demonstrate through extensive experimentation on both real and synthetic databsets that LS can results in a significant performance advantage over existing technqiues.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {267–278},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325886,
author = {Lee, Ken C. K. and Zheng, Baihua and Li, Huajing and Lee, Wang-Chien},
title = {Approaching the Skyline in Z Order},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Given a set of multidimensional data points, skyline query retrieves a set of data points that are not dominated by any other points. This query is useful for multi-preference analysis and decision making. By analyzing the skyline query, we observe a close connection between Z-order curve and skyline processing strategies and propose to use a new index structure called ZBtree, to index and store data points based on Z-order curve. We develop a suite of novel and efficient skyline algorithms, which scale very well to data dimensionality and cardinality, including (1) ZSearch, which processes skyline queries and supports progressive result delivery; (2) ZUpdate, which facilitates incremental skyline result maintenance; and (3) k-ZSearch, which answers k-dominant skyline query (a skyline variant that retrieves a representative subset of skyline results). Extensive experiments have been conducted to evaluate our proposed algorithms and compare them against the best available algorithms designed for skyline search, skyline result update, and k-dominant skyline search, respectively. The result shows that our algorithms, developed coherently based on the same ideas and concepts, soundly outperforms the state-of-the-art skyline algorithms in their specialized domains.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {279–290},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325887,
author = {Dellis, Evangelos and Seeger, Bernhard},
title = {Efficient Computation of Reverse Skyline Queries},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In this paper, for the first time, we introduce the concept of Reverse Skyline Queries. At first, we consider for a multidimensional data set P the problem of dynamic skyline queries according to a query point q. This kind of dynamic skyline corresponds to the skyline of a transformed data space where point q becomes the origin and all points of P are represented by their distance vector to q. The reverse skyline query returns the objects whose dynamic skyline contains the query object q. In order to compute the reverse skyline of an arbitrary query point, we first propose a Branch and Bound algorithm (called BBRS), which is an improved customization of the original BBS algorithm. Furthermore, we identify a super set of the reverse skyline that is used to bound the search space while computing the reverse skyline. To further reduce the computational cost of determining if a point belongs to the reverse skyline, we propose an enhanced algorithm (called RSSA) that is based on accurate pre-computed approximations of the skylines. These approximations are used to identify whether a point belongs to the reverse skyline or not. Through extensive experiments with both real-world and synthetic datasets, we show that our algorithms can efficiently support reverse skyline queries. Our enhanced approach improves reversed skyline processing by up to an order of magnitude compared to the algorithm without the usage of pre-computed approximations.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {291–302},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325889,
author = {Li, Chen and Wang, Bin and Yang, Xiaochun},
title = {VGRAM: Improving Performance of Approximate Queries on String Collections Using Variable-Length Grams},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Many applications need to solve the following problem of approximate string matching: from a collection of strings, how to find those similar to a given string, or the strings in another (possibly the same) collection of strings? Many algorithms are developed using fixed-length grams, which are substrings of a string used as signatures to identify similar strings. In this paper we develop a novel technique, called VGRAM, to improve the performance of these algorithms. Its main idea is to judiciously choose high-quality grams of variable lengths from a collection of strings to support queries on the collection. We give a full specification of this technique, including how to select high-quality grams from the collection, how to generate variable-length grams for a string based on the preselected grams, and what is the relationship between the similarity of the gram sets of two strings and their edit distance. A primary advantage of the technique is that it can be adopted by a plethora of approximate string algorithms without the need to modify them substantially. We present our extensive experiments on real data sets to evaluate the technique, and show the significant performance improvements on three existing algorithms.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {303–314},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325890,
author = {Cong, Gao and Fan, Wenfei and Geerts, Floris and Jia, Xibei and Ma, Shuai},
title = {Improving Data Quality: Consistency and Accuracy},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Two central criteria for data quality are consistency and accuracy. Inconsistencies and errors in a database often emerge as violations of integrity constraints. Given a dirty database D, one needs automated methods to make it consistent, i.e., find a repair D' that satisfies the constraints and "minimally" differs from D. Equally important is to ensure that the automatically-generated repair D' is accurate, or makes sense, i.e., D' differs from the "correct" data within a predefined bound. This paper studies effective methods for improving both data consistency and accuracy. We employ a class of conditional functional dependencies (CFDs) proposed in [6] to specify the consistency of the data, which are able to capture inconsistencies and errors beyond what their traditional counterparts can catch. To improve the consistency of the data, we propose two algorithms: one for automatically computing a repair D' that satisfies a given set of CFDs, and the other for incrementally finding a repair in response to updates to a clean database. We show that both problems are intractable. Although our algorithms are necessarily heuristic, we experimentally verify that the methods are effective and efficient. Moreover, we develop a statistical method that guarantees that the repairs found by the algorithms are accurate above a predefined rate without incurring excessive user interaction.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {315–326},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325891,
author = {Chaudhuri, Surajit and Chen, Bee-Chung and Ganti, Venkatesh and Kaushik, Raghav},
title = {Example-Driven Design of Efficient Record Matching Queries},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Record matching is the task of identifying records that match the same real world entity. This is a problem of great significance for a variety of business intelligence applications. Implementations of record matching rely on exact as well as approximate string matching (e.g., edit distances) and use of external reference data sources. Record matching can be viewed as a query composed of a small set of primitive operators. However, formulating such record matching queries is difficult and depends on the specific application scenario. Specifically, the number of options both in terms of string matching operations as well as the choice of external sources can be daunting. In this paper, we exploit the availability of positive and negative examples to search through this space and suggest an initial record matching query. Such queries can be subsequently modified by the programmer as needed. We ensure that the record matching queries our approach produces are (1) efficient: these queries can be run on large datasets by leveraging operations that are well-supported by RDBMSs, and (2) explainable: the queries are easy to understand so that they may be modified by the programmer with relative ease. We demonstrate the effectiveness of our approach on several real-world datasets.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {327–338},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325893,
author = {Cieslewicz, John and Ross, Kenneth A.},
title = {Adaptive Aggregation on Chip Multiprocessors},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The recent introduction of commodity chip multiprocessors requires that the design of core database operations be carefully examined to take full advantage of on-chip parallelism. In this paper we examine aggregation in a multi-core environment, the Sun UltraSPARC T1, a chip multiprocessor with eight cores and a shared L2 cache. Aggregation is an important aspect of query processing that is seemingly easy to understand and implement. Our research, however, demonstrates that a chip multiprocessor adds new dimensions to understanding hash-based aggregation performance---concurrent sharing of aggregation data structures and contentious accesses to frequently used values. We also identify a trade off between private data structures assigned to each thread versus shared data structures for aggregation. Depending on input characteristics, different aggregation strategies are optimal and choosing the wrong strategy can result in a performance penalty of over an order of magnitude. We provide a thorough explanation of the factors affecting aggregation performance on chip multiprocessors and identify three key input characteristics that dictate performance: (1) average run length of identical group-by values, (2) locality of references to the aggregation hash table, and (3) frequency of repeated accesses to the same hash table location. We then introduce an adaptive aggregation operator that performs lightweight sampling of the input to choose the correct aggregation strategy with high accuracy. Our experiments verify that our adaptive algorithm chooses the highest performing aggregation strategy on a number of common input distributions.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {339–350},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325894,
author = {Johnson, Ryan and Harizopoulos, Stavros and Hardavellas, Nikos and Sabirli, Kivanc and Pandis, Ippokratis and Ailamaki, Anastasia and Mancheril, Naju G. and Falsafi, Babak},
title = {To Share or Not to Share?},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Intuitively, aggressive work sharing among concurrent queries in a database system should always improve performance by eliminating redundant computation or data accesses. We show that, contrary to common intuition, this is not always the case in practice, especially in the highly parallel world of chip multiprocessors. As the number of cores in the system increases, a trade-off appears between exploiting work sharing opportunities and the available parallelism. To resolve the trade-off, we develop an analytical approach that predicts the effect of work sharing in multi-core systems. Database systems can use the model to determine, statically or at runtime, whether work sharing is beneficial and apply it only when appropriate.The contributions of this paper are as follows. First, we introduce and analyze the effects of the trade-off between work sharing and parallelism on database systems running complex decision-support queries. Second, we propose an intuitive and simple model that can evaluate the trade-off using real-world measurement approximations of the query execution processes. Furthermore, we integrate the model into a prototype database execution engine, and demonstrate that selective work sharing according to the model outperforms never-share static schemes by 20% on average and always-share ones by 2.5x.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {351–362},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325895,
author = {Gedik, Buundefinedra and Yu, Philip S. and Bordawekar, Rajesh R.},
title = {Executing Stream Joins on the Cell Processor},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Low-latency and high-throughput processing are key requirements of data stream management systems (DSMSs). Hence, multi-core processors that provide high aggregate processing capacity are ideal matches for executing costly DSMS operators. The recently developed Cell processor is a good example of a heterogeneous multi-core architecture and provides a powerful platform for executing data stream operators with high-performance. On the down side, exploiting the full potential of a multi-core processor like Cell is often challenging, mainly due to the heterogeneous nature of the processing elements, the software managed local memory at the co-processor side, and the unconventional programming model in general.In this paper, we study the problem of scalable execution of windowed stream join operators on multi-core processors, and specifically on the Cell processor. By examining various aspects of join execution flow, we determine the right set of techniques to apply in order to minimize the sequential segments and maximize parallelism. Concretely, we show that basic windows coupled with low-overhead pointer-shifting techniques can be used to achieve efficient join window partitioning, column-oriented join window organization can be used to minimize scattered data transfers, delay-optimized double buffering can be used for effective pipelining, rate-aware batching can be used to balance join throughput and tuple delay, and finally SIMD (single-instruction multiple-data) optimized operator code can be used to exploit data parallelism. Our experimental results show that, following the design guidelines and implementation techniques outlined in this paper, windowed stream joins can achieve high scalability (linear in the number of co-processors) by making efficient use of the extensive hardware parallelism provided by the Cell processor (reaching data processing rates of ≈ 13 GB/sec) and significantly surpass the performance obtained form conventional high-end processors (supporting a combined input stream rate of 2000 tuples/sec using 15 minutes windows and without dropping any tuples, resulting in ≈ 8.3 times higher output rate compared to an SSE implementation on dual 3.2Ghz Intel Xeon).},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {363–374},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325897,
author = {Cho, Junghoo and Schonfeld, Uri},
title = {RankMass Crawler: A Crawler with High Personalized Pagerank Coverage Guarantee},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Crawling algorithms have been the subject of extensive research and optimizations, but some important questions remain open. In particular, given the unbounded number of pages available on the Web, search-engine operators constantly struggle with the following vexing questions: When can I stop downloading the Web? How many pages should I download to cover "most" of the Web? How can I know I am not missing an important part when I stop? In this paper we provide an answer to these questions by developing, in the context of a system that is given a set of trusted pages, a family of crawling algorithms that (1) provide a theoretical guarantee on how much of the "important" part of the Web it will download after crawling a certain number of pages and (2) give a high priority to important pages during a crawl, so that the search engine can index the most important part of the Web first. We prove the correctness of our algorithms by theoretical analysis and evaluate their performance experimentally based on 141 million URLs obtained from the Web. Our experiments demonstrate that even our simple algorithm is effective in downloading important pages early on and provides high "coverage" of the Web with a relatively small number of pages.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {375–386},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325898,
author = {Cheng, Tao and Yan, Xifeng and Chang, Kevin Chen-Chuan},
title = {EntityRank: Searching Entities Directly and Holistically},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {As the Web has evolved into a data-rich repository, with the standard "page view," current search engines are becoming increasingly inadequate for a wide range of query tasks. While we often search for various data "entities" (e.g., phone number, paper PDF, date), today's engines only take us indirectly to pages. While entities appear in many pages, current engines only find each page individually. Toward searching directly and holistically for finding information of finer granularity, we study the problem of entity search, a significant departure from traditional document retrieval. We focus on the core challenge of ranking entities, by distilling its underlying conceptual model Impression Model and developing a probabilistic ranking framework, EntityRank, that is able to seamlessly integrate both local and global information in ranking. We evaluate our online prototype over a 2TB Web corpus, and show that EntityRank performs effectively.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {387–398},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325899,
author = {DeRose, Pedro and Shen, Warren and Chen, Fei and Doan, AnHai and Ramakrishnan, Raghu},
title = {Building Structured Web Community Portals: A Top-down, Compositional, and Incremental Approach},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Structured community portals extract and integrate information from raw Web pages to present a unified view of entities and relationships in the community. In this paper we argue that to build such portals, a top-down, compositional, and incremental approach is a good way to proceed. Compared to current approaches that employ complex monolithic techniques, this approach is easier to develop, understand, debug, and optimize. In this approach, we first select a small set of important community sources. Next, we compose plans that extract and integrate data from these sources, using a set of extraction/integration operators. Executing these plans yields an initial structured portal. We then incrementally expand this portal by monitoring the evolution of current data sources, to detect and add new data sources. We describe our initial solutions to the above steps, and a case study of employing these solutions to build DBLife, a portal for the database community. We found that DBLife could be built quickly and achieve high accuracy using simple extraction/integration operators, and that it can be maintained and expanded with little human effort. The initial solutions together with the case study demonstrate the feasibility and potential of our approach.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {399–410},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325900,
author = {Abadi, Daniel J. and Marcus, Adam and Madden, Samuel R. and Hollenbach, Kate},
title = {Scalable Semantic Web Data Management Using Vertical Partitioning},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Efficient management of RDF data is an important factor in realizing the Semantic Web vision. Performance and scalability issues are becoming increasingly pressing as Semantic Web technology is applied to real-world applications. In this paper, we examine the reasons why current data management solutions for RDF data scale poorly, and explore the fundamental scalability limitations of these approaches. We review the state of the art for improving performance for RDF databases and consider a recent suggestion, "property tables." We then discuss practically and empirically why this solution has undesirable features. As an improvement, we propose an alternative solution: vertically partitioning the RDF data. We compare the performance of vertical partitioning with prior art on queries generated by a Web-based RDF browser over a large-scale (more than 50 million triples) catalog of library data. Our results show that a vertical partitioned schema achieves similar performance to the property table technique while being much simpler to design. Further, if a column-oriented DBMS (a database architected specially for the vertically partitioned case) is used instead of a row-oriented DBMS, another order of magnitude performance improvement is observed, with query times dropping from minutes to several seconds.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {411–422},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325902,
author = {Han, Wook-Shin and Lee, Jinsoo and Moon, Yang-Sae and Jiang, Haifeng},
title = {Ranked Subsequence Matching in Time-Series Databases},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Existing work on similar sequence matching has focused on either whole matching or range subsequence matching. In this paper, we present novel methods for ranked subsequence matching under time warping, which finds top-k subsequences most similar to a query sequence from data sequences. To the best of our knowledge, this is the first and most sophisticated subsequence matching solution mentioned in the literature. Specifically, we first provide a new notion of the minimum-distance matching-window pair (MDMWP) and formally define the mdmwp-distance, a lower bound between a data subsequence and a query sequence. The mdmwp-distance can be computed prior to accessing the actual subsequence. Based on the mdmwp-distance, we then develop a ranked subsequence matching algorithm to prune unnecessary subsequence accesses. Next, to reduce random disk I/Os and bad buffer utilization, we develop a method of deferred group subsequence retrieval. We then derive another lower bound, the window-group distance, that can be used to effectively prune unnecessary subsequence accesses during deferred group-subsequence retrieval. Through extensive experiments with many data sets, we showcase the superiority of the proposed methods.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {423–434},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325903,
author = {Chen, Qiuxia and Chen, Lei and Lian, Xiang and Liu, Yunhao and Yu, Jeffrey Xu},
title = {Indexable PLA for Efficient Similarity Search},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Similarity-based search over time-series databases has been a hot research topic for a long history, which is widely used in many applications, including multimedia retrieval, data mining, web search and retrieval, and so on. However, due to high dimensionality (i.e. length) of the time series, the similarity search over directly indexed time series usually encounters a serious problem, known as the "dimensionality curse". Thus, many dimensionality reduction techniques are proposed to break such curse by reducing the dimensionality of time series. Among all the proposed methods, only Piecewise Linear Approximation (PLA) does not have indexing mechanisms to support similarity queries, which prevents it from efficiently searching over very large time-series databases. Our initial studies on the effectiveness of different reduction methods, however, show that PLA performs no worse than others. Motivated by this, in this paper, we re-investigate PLA for approximating and indexing time series. Specifically, we propose a novel distance function in the reduced PLA-space, and prove that this function indeed results in a lower bound of the Euclidean distance between the original time series, which can lead to no false dismissals during the similarity search. As a second step, we develop an effective approach to index these lower bounds to improve the search efficiency. Our extensive experiments over a wide spectrum of real and synthetic data sets have demonstrated the efficiency and effectiveness of PLA together with the newly proposed lower bound distance, in terms of both pruning power and wall clock time, compared with two state-of-the-art reduction methods, Adaptive Piecewise Constant Approximation (APCA) and Chebyshev Polynomials (CP).},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {435–446},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325904,
author = {Li, Xiaolei and Han, Jiawei},
title = {Mining Approximate Top-k Subspace Anomalies in Multi-Dimensional Time-Series Data},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Market analysis is a representative data analysis process with many applications. In such an analysis, critical numerical measures, such as profit and sales, fluctuate over time and form time-series data. Moreover, the time series data correspond to market segments, which are described by a set of attributes, such as age, gender, education, income level, and product-category, that form a multi-dimensional structure. To better understand market dynamics and predict future trends, it is crucial to study the dynamics of time-series in multi-dimensional market segments. This is a topic that has been largely ignored in time series and data cube research.In this study, we examine the issues of anomaly detection in multi-dimensional time-series data. We propose time-series data cube to capture the multi-dimensional space formed by the attribute structure. This facilitates the detection of anomalies based on expected values derived from higher level, "more general" time-series. Anomaly detection in a time-series data cube poses computational challenges, especially for high-dimensional, large data sets. To this end, we also propose an efficient search algorithm to iteratively select subspaces in the original high-dimensional space and detect anomalies within each one. Our experiments with both synthetic and real-world data demonstrate the effectiveness and efficiency of the proposed solution.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {447–458},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325905,
author = {Papadimitriou, Spiros and Li, Feifei and Kollios, George and Yu, Philip S.},
title = {Time Series Compressibility and Privacy},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In this paper we study the trade-offs between time series compressibility and partial information hiding and their fundamental implications on how we should introduce uncertainty about individual values by perturbing them. More specifically, if the perturbation does not have the same compressibility properties as the original data, then it can be detected and filtered out, reducing uncertainty. Thus, by making the perturbation "similar" to the original data, we can both preserve the structure of the data better, while simultaneously making breaches harder. However, as data become more compressible, a fraction of the uncertainty can be removed if true values are leaked, revealing how they were perturbed. We formalize these notions, study the above trade-offs on real data and develop practical schemes which strike a good balance and can also be extended for on-the-fly data hiding in a streaming environment.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {459–470},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325907,
author = {Wu, Mingxi and Jermaine, Christopher},
title = {A Bayesian Method for Guessing the Extreme Values in a Data Set?},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {For a large number of data management problems, it would be very useful to be able to obtain a few samples from a data set, and to use the samples to guess the largest (or smallest) value in the entire data set. Min/max online aggregation, top-k query processing, outlier detection, and distance join are just a few possible applications. This paper details a statistically rigorous, Bayesian approach to attacking this problem. Just as importantly, we demonstrate the utility of our approach by showing how it can be applied to two specific problems that arise in the context of data management.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {471–482},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325908,
author = {Yiu, Man Lung and Mamoulis, Nikos},
title = {Efficient Processing of Top-<i>k</i> Dominating Queries on Multi-Dimensional Data},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The top-k dominating query returns k data objects which dominate the highest number of objects in a dataset. This query is an important tool for decision support since it provides data analysts an intuitive way for finding significant objects. In addition, it combines the advantages of top-k and skyline queries without sharing their disadvantages: (i) the output size can be controlled, (ii) no ranking functions need to be specified by users, and (iii) the result is independent of the scales at different dimensions. Despite their importance, top-k dominating queries have not received adequate attention from the research community. In this paper, we design specialized algorithms that apply on indexed multi-dimensional data and fully exploit the characteristics of the problem. Experiments on synthetic datasets demonstrate that our algorithms significantly outperform a previous skyline-based approach, while our results on real datasets show the meaningfulness of top-k dominating queries.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {483–494},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325909,
author = {Akbarinia, Reza and Pacitti, Esther and Valduriez, Patrick},
title = {Best Position Algorithms for Top-k Queries},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The general problem of answering top-k queries can be modeled using lists of data items sorted by their local scores. The most efficient algorithm proposed so far for answering top-k queries over sorted lists is the Threshold Algorithm (TA). However, TA may still incur a lot of useless accesses to the lists. In this paper, we propose two new algorithms which stop much sooner. First, we propose the best position algorithm (BPA) which executes top-k queries more efficiently than TA. For any database instance (i.e. set of sorted lists), we prove that BPA stops as early as TA, and that its execution cost is never higher than TA. We show that the position at which BPA stops can be (m-1) times lower than that of TA, where m is the number of lists. We also show that the execution cost of our algorithm can be (m-1) times lower than that of TA. Second, we propose the BPA2 algorithm which is much more efficient than BPA. We show that the number of accesses to the lists done by BPA2 can be about (m-1) times lower than that of BPA. Our performance evaluation shows that over our test databases, BPA and BPA2 achieve significant performance gains in comparison with TA.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {495–506},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325910,
author = {Qi, Yan and Candan, K. Sel\c{c}uk and Sapino, Maria Luisa},
title = {Sum-Max Monotonic Ranked Joins for Evaluating Top-k Twig Queries on Weighted Data Graphs},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In many applications, the underlying data (the web, an XML document, or a relational database) can be seen as a graph. These graphs may be enriched with weights, associated with the nodes and edges of the graph, denoting application specific desirability/penalty assessments, such as popularity, trust, or cost. A particular challenge when considering such weights in query processing is that results need to be ranked accordingly. Answering keyword-based queries on weighted graphs is shown to be computationally expensive. In this paper, we first show that answering queries with further structure imposed on them remains NP-hard. We next show that, while the query evaluation task can be viewed in terms of ranked structural-joins along query axes, the monotonicity property, necessary for ranked join algorithms, is violated. Consequently, traditional ranked join algorithms are not directly applicable. Thus, we establish an alternative, sum-max monotonicity property and show how to leverage this for developing a self-punctuating, horizon-based ranked join (HR-Join) operator for ranked twig-query execution on data graphs. We experimentally show the effectiveness of the proposed evaluation schemes and the HR-join operator for merging ranked sub-results under sum-max monotonicity.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {507–518},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325912,
author = {Ge, Tingjian and Zdonik, Stan},
title = {Answering Aggregation Queries in a Secure System Model},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {As more sensitive data is captured in electronic form, security becomes more and more important. Data encryption is the main technique for achieving security. While in the past enterprises were hesitant to implement database encryption because of the very high cost, complexity, and performance degradation, they now have to face the ever-growing risk of data theft as well as emerging legislative requirements. Data encryption can be done at multiple tiers within the enterprise. Different choices on where to encrypt the data offer different security features that protect against different attacks. One class of attack that needs to be taken seriously is the compromise of the database server, its software or administrator. A secure way to address this threat is for a DBMS to directly process queries on the ciphertext, without decryption. We conduct a comprehensive study on answering SUM and AVG aggregation queries in such a system model by using a secure homomorphic encryption scheme in a novel way. We demonstrate that the performance of such a solution is comparable to a traditional symmetric encryption scheme (e.g., DES) in which each value is decrypted and the computation is performed on the plaintext. Clearly this traditional encryption scheme is not a viable solution to the problem because the server must have access to the secret key and the plaintext, which violates our system model and security requirements. We study the problem in the setting of a read-optimized DBMS for data warehousing applications, in which SUM and AVG are frequent and crucial.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {519–530},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325913,
author = {Rastogi, Vibhor and Suciu, Dan and Hong, Sungho},
title = {The Boundary between Privacy and Utility in Data Publishing},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {We consider the privacy problem in data publishing: given a database instance containing sensitive information "anonymize" it to obtain a view such that, on one hand attackers cannot learn any sensitive information from the view, and on the other hand legitimate users can use it to compute useful statistics. These are conflicting goals. In this paper we prove an almost crisp separation of the case when a useful anonymization algorithm is possible from when it is not, based on the attacker's prior knowledge. Our definition of privacy is derived from existing literature and relates the attacker's prior belief for a given tuple t, with the posterior belief for the same tuple. Our definition of utility is based on the error bound on the estimates of counting queries. The main result has two parts. First we show that if the prior beliefs for some tuples are large then there exists no useful anonymization algorithm. Second, we show that when the prior is bounded for all tuples then there exists an anonymization algorithm that is both private and useful. The anonymization algorithm that forms our positive result is novel, and improves the privacy/utility tradeoff of previously known algorithms with privacy/utility guarantees such as FRAPP.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {531–542},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325914,
author = {Wong, Raymond Chi-Wing and Fu, Ada Wai-Chee and Wang, Ke and Pei, Jian},
title = {Minimality Attack in Privacy Preserving Data Publishing},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Data publishing generates much concern over the protection of individual privacy. Recent studies consider cases where the adversary may possess different kinds of knowledge about the data. In this paper, we show that knowledge of the mechanism or algorithm of anonymization for data publication can also lead to extra information that assists the adversary and jeopardizes individual privacy. In particular, all known mechanisms try to minimize information loss and such an attempt provides a loophole for attacks. We call such an attack a minimality attack. In this paper, we introduce a model called m-confidentiality which deals with minimality attacks, and propose a feasible solution. Our experiments show that minimality attacks are practical concerns on real datasets and that our algorithm can prevent such attacks with very little overhead and information loss.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {543–554},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325915,
author = {Wang, Qihua and Yu, Ting and Li, Ninghui and Lobo, Jorge and Bertino, Elisa and Irwin, Keith and Byun, Ji-Won},
title = {On the Correctness Criteria of Fine-Grained Access Control in Relational Databases},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Databases are increasingly being used to store information covered by heterogeneous policies, which require support for access control with great flexibility. This has led to increasing interest in using fine-grained access control, where different cells in a relation may be governed by different access control rules. Although several proposals have been made to support fine-grained access control, there currently does not exist a formal notion of correctness regarding the query answering procedure. In this paper, we propose such a formal notion of correctness in fine-grained database access control, and discuss why existing approaches fall short in some circumstances. We then propose a labeling approach for masking unauthorized information and a query evaluation algorithm which better supports fine-grained access control. Finally, we implement our algorithm using query modification and evaluate its performance.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {555–566},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325917,
author = {Zinn, Daniel and Bosch, Jim and Gertz, Michael},
title = {Modeling and Querying Vague Spatial Objects Using Shapelets},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Research in modeling and querying spatial data has primarily focused on traditional "crisp" spatial objects with exact location and spatial extent. More recent work, however, has begun to address the need for spatial data types describing spatial phenomena that cannot be modeled by objects having sharp boundaries. Other work has focused on point objects whose location is not precisely known and is typically described using a probability distribution.In this paper, we present a new technique for modeling and querying vague spatial objects. Using shapelets, an image decomposition technique developed in astronomy, as base data type, we introduce a comprehensive set of low-level operations that provide building blocks for versatile high-level operations on vague spatial objects. In addition, we describe an implementation of this data model as an extension to PostgreSQL, including an indexing technique for shapelet objects. Unlike existing techniques for modeling and querying vague or fuzzy data, our approach is optimized for localized, smoothly varying spatial objects, and as such is more suitable for many real-world datasets.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {567–578},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325918,
author = {Wong, Raymond Chi-Wing and Tao, Yufei and Fu, Ada Wai-Chee and Xiao, Xiaokui},
title = {On Efficient Spatial Matching},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {This paper proposes and solves a new problem called spatial matching (SPM). Let P and O be two sets of objects in an arbitrary metric space, where object distances are defined according to a norm satisfying the triangle inequality. Each object in O represents a customer, and each object in P indicates a service provider, which has a capacity corresponding to the maximum number of customers that can be supported by the provider. SPM assigns each customer to her/his nearest provider, among all the providers whose capacities have not been exhausted in serving other closer customers. We elaborate the applications where SPM is useful, and develop algorithms that settle this problem with a linear number O(|P| + |O|) of nearest neighbor queries. We verify our theoretical findings with extensive experiments, and show that the proposed solutions outperform alternative methods by a factor of orders of magnitude.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {579–590},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325919,
author = {Biveinis, Laurynas and \v{S}altenis, Simonas and Jensen, Christian S.},
title = {Main-Memory Operation Buffering for Efficient R-Tree Update},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Emerging communication and sensor technologies enable new applications of database technology that require database systems to efficiently support very high rates of spatial-index updates. Previous works in this area require the availability of large amounts of main memory, do not exploit all the main memory that is indeed available, or do not support some of the standard index operations.Assuming a setting where the index updates need not be written to disk immediately, we propose an R-tree-based indexing technique that does not exhibit any of these drawbacks. This technique exploits the buffering of update operations in main memory as well as the grouping of operations to reduce disk I/O. In particular, operations are performed in bulk so that multiple operations are able to share I/O. The paper presents an analytical cost model that is shown to be accurate by empirical studies. The studies also show that, in terms of update I/O performance, the proposed technique improves on state of the art in settings with frequent updates.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {591–602},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325921,
author = {Beeri, Catriel and Eyal, Anat and Milo, Tova and Pilberg, Alon},
title = {Monitoring Business Processes with Queries},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Many enterprises nowadays use business processes, based on the BPEL standard, to achieve their goals. These are complex, often distributed, processes. Monitoring the execution of such processes for interesting patterns is critical for enforcing business policies and meeting efficiency and reliability goals. BP-Mon (Business Processes Monitoring) is a novel query language for monitoring business processes, that allows users to visually define monitoring tasks and associated reports, using a simple intuitive interface, similar to those used for designing BPEL processes. We describe here the BP-Mon language and its underlying formal model. We also present the language implementation and describe our novel optimization techniques. An important feature of the implementation is that BP-Mon queries are translated to BPEL processes that run on the same execution engine as the monitored processes. Our experiments indicate that this approach incurs very minimal overhead, hence is a practical and efficient approach to monitoring.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {603–614},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325922,
author = {Vrhovnik, Marko and Schwarz, Holger and Suhre, Oliver and Mitschang, Bernhard and Markl, Volker and Maier, Albert and Kraft, Tobias},
title = {An Approach to Optimize Data Processing in Business Processes},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In order to optimize their revenues and profits, an increasing number of businesses organize their business activities in terms of business processes. Typically, they automate important business tasks by orchestrating a number of applications and data stores. Obviously, the performance of a business process is directly dependent on the efficiency of data access, data processing, and data management.In this paper, we propose a framework for the optimization of data processing in business processes. We introduce a set of rewrite rules that transform a business process in such a way that an improved execution with respect to data management can be achieved without changing the semantics of the original process. These rewrite rules are based on a semi-procedural process graph model that externalizes data dependencies as well as control flow dependencies of a business process. Furthermore, we present a multi-stage control strategy for the optimization process. We illustrate the benefits and opportunities of our approach through a prototype implementation. Our experimental results demonstrate that independent of the underlying database system performance gains of orders of magnitude are achievable by reasoning about data and control in a unified framework.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {615–626},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325923,
author = {Roman, Dumitru and Kifer, Michael},
title = {Reasoning about the Behavior of Semantic Web Services with Concurrent Transaction Logic},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The recent upsurge in the interest in Semantic Web services and the high-profile projects such as the WSMO, OWLS, and SWSL, have drawn attention to the importance of logic-based modeling of the behavior of Web services. In the context of Semantic Web services, the logic-based approach has many applications, including service discovery, service choreography, enactment, and contracting for services. In this paper we propose logic-based methods for reasoning about service behavior, including the aforementioned choreography, contracting, and enactment. The formalism underlying our framework is Concurrent Transaction Logic---a logic for declarative specification, analysis, and execution of database transactions. The new results include reasoning about service behavior under more general sets of constraints and extension of the framework towards conditional control and data flow---two crucial aspect that were missing in previous logical formalizations.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {627–638},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325925,
author = {Xu, Fei and Jermaine, Christopher},
title = {Randomized Algorithms for Data Reconciliation in Wide Area Aggregate Query Processing},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Many aspects of the data integration problem have been considered in the literature: how to match schemas across different data sources, how to decide when different records refer to the same entity, how to efficiently perform the required entity resolution in a batch fashion, and so on. However, what has largely been ignored is a way to efficiently deploy these existing methods in a realistic, distributed enterprise integration environment. The straightforward use of existing methods often requires that all data be shipped to a coordinator for cleaning, which is often unacceptable. We develop a set of randomized algorithms that allow efficient application of existing entity resolution methods to the answering of aggregate queries over data that have been distributed across multiple sites. Using our methods, it is possible to efficiently generate aggregate query results that account for duplicate and inconsistent values scattered across a federated system.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {639–650},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325926,
author = {Wolf, Garrett and Khatri, Hemal and Chokshi, Bhaumik and Fan, Jianchun and Chen, Yi and Kambhampati, Subbarao},
title = {Query Processing over Incomplete Autonomous Databases},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Incompleteness due to missing attribute values (aka "null values") is very common in autonomous web databases, on which user accesses are usually supported through mediators. Traditional query processing techniques that focus on the strict soundness of answer tuples often ignore tuples with critical missing attributes, even if they wind up being relevant to a user query. Ideally we would like the mediator to retrieve such possible answers and gauge their relevance by accessing their likelihood of being pertinent answers to the query. The autonomous nature of web databases poses several challenges in realizing this objective. Such challenges include the restricted access privileges imposed on the data, the limited support for query patterns, and the bounded pool of database and network resources in the web environment. We introduce a novel query rewriting and optimization framework QPIAD that tackles these challenges. Our technique involves reformulating the user query based on mined correlations among the database attributes. The reformulated queries are aimed at retrieving the relevant possible answers in addition to the certain answers. QPIAD is able to gauge the relevance of such queries allowing tradeoffs in reducing the costs of database query processing and answer transmission. To support this framework, we develop methods for mining attribute correlations (in terms of Approximate Functional Dependencies), value distributions (in the form of Na\"{\i}ve Bayes Classifiers), and selectivity estimates. We present empirical studies to demonstrate that our approach is able to effectively retrieve relevant possible answers with high precision, high recall, and manageable cost.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {651–662},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325927,
author = {Vaz Salles, Marcos Antonio and Dittrich, Jens-Peter and Karakashian, Shant Kirakos and Girard, Olivier Ren\'{e} and Blunschi, Lukas},
title = {ITrails: Pay-as-You-Go Information Integration in Dataspaces},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Dataspace management has been recently identified as a new agenda for information management [17, 22] and information integration [23]. In sharp contrast to standard information integration architectures, a dataspace management system is a data-coexistence approach: it does not require any investments in semantic integration before querying services on the data are provided. Rather, a dataspace can be gradually enhanced over time by defining relationships among the data. Defining those integration semantics gradually is termed pay-as-you-go information integration [17], as time and effort (pay) are needed over time (go) to provide integration semantics. The benefits are better query results (gain). This paper is the first to explore pay-as-you-go information integration in dataspaces. We provide a technique for declarative pay-as-you-go information integration named iTrails. The core idea of our approach is to declaratively add lightweight 'hints' (trails) to a search engine thus allowing gradual enrichment of loosely integrated data sources. Our experiments confirm that iTrails can be efficiently implemented introducing only little overhead during query execution. At the same time iTrails strongly improves the quality of query results. Furthermore, we present rewriting and pruning techniques that allow us to scale iTrails to tens of thousands of trail definitions with minimal growth in the rewritten query size.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {663–674},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325929,
author = {Green, Todd J. and Karvounarakis, Grigoris and Ives, Zachary G. and Tannen, Val},
title = {Update Exchange with Mappings and Provenance},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {We consider systems for data sharing among heterogeneous peers related by a network of schema mappings. Each peer has a locally controlled and edited database instance, but wants to ask queries over related data from other peers as well. To achieve this, every peer's updates propagate along the mappings to the other peers. However, this update exchange is filtered by trust conditions --- expressing what data and sources a peer judges to be authoritative --- which may cause a peer to reject another's updates. In order to support such filtering, updates carry provenance information. These systems target scientific data sharing applications, and their general principles and architecture have been described in [20].In this paper we present methods for realizing such systems. Specifically, we extend techniques from data integration, data exchange, and incremental view maintenance to propagate updates along mappings; we integrate a novel model for tracking data provenance, such that curators may filter updates based on trust conditions over this provenance; we discuss strategies for implementing our techniques in conjunction with an RDBMS; and we experimentally demonstrate the viability of our techniques in the ORCHESTRA prototype system.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {675–686},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325930,
author = {Dong, Xin and Halevy, Alon Y. and Yu, Cong},
title = {Data Integration with Uncertainty},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {This paper reports our first set of results on managing uncertainty in data integration. We posit that data-integration systems need to handle uncertainty at three levels, and do so in a principled fashion. First, the semantic mappings between the data sources and the mediated schema may be approximate because there may be too many of them to be created and maintained or because in some domains (e.g., bioinformatics) it is not clear what the mappings should be. Second, queries to the system may be posed with keywords rather than in a structured form. Third, the data from the sources may be extracted using information extraction techniques and so may yield imprecise data.As a first step to building such a system, we introduce the concept of probabilistic schema mappings and analyze their formal foundations. We show that there are two possible semantics for such mappings: by-table semantics assumes that there exists a correct mapping but we don't know what it is; by-tuple semantics assumes that the correct mapping may depend on the particular tuple in the source data. We present the query complexity and algorithms for answering queries in the presence of approximate schema mappings, and we describe an algorithm for efficiently computing the top-k answers to queries in such a setting.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {687–698},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325931,
author = {Chuang, Shui-Lung and Chang, Kevin Chen-Chuan and Zhai, ChengXiang},
title = {Context-Aware Wrapping: Synchronized Data Extraction},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The deep Web presents a pressing need for integrating large numbers of dynamically evolving data sources. To be more automatic yet accurate in building an integration system, we observe two problems: First, across sequential tasks in integration, how can a wrapper (as an extraction task) consider the peer sources to facilitate the subsequent matching task? Second, across parallel sources, how can a wrapper leverage the peer wrappers or domain rules to enhance extraction accuracy? These issues, while seemingly unrelated, both boil down to the lack of "context awareness": Current automatic wrapper induction approaches generate a wrapper for one source at a time, in isolation, and thus inherently lack the awareness of the peer sources or domain knowledge in the context of integration. We propose the concept of context-aware wrappers that are amenable to matching and that can leverage peer wrappers or prior domain knowledge. Such context awareness inspires a synchronization framework to construct wrappers consistently and collaboratively across their mutual context. We draw the insight from turbo codes and develop the turbo syncer to interconnect extraction with matching, which together achieve context awareness in wrapping. Our experiments show that the turbo syncer can, on the one hand, enhance extraction consistency and thus increase matching accuracy (from 17--83% to 78--94% in F-measure) and, on the other hand, incorporate peer wrappers and domain knowledge seamlessly to reduce extraction errors (from 09--60% to 01--11%).},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {699–710},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325933,
author = {Duan, Songyun and Babu, Shivanath},
title = {Processing Forecasting Queries},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Forecasting future events based on historic data is useful in many domains like system management, adaptive query processing, environmental monitoring, and financial planning. We describe the Fa system where users and applications can pose declarative forecasting queries---both one-time queries and continuous queries---and get forecasts in real-time along with accuracy estimates. Fa supports efficient algorithms to generate execution plans automatically for forecasting queries from a novel plan space comprising operators for transforming data, learning statistical models from data, and doing inference using the learned models. In addition, Fa supports adaptive query-processing algorithms that adapt plans for continuous forecasting queries to the time-varying properties of input data streams. We report an extensive experimental evaluation of Fa using synthetic datasets, datasets collected on a testbed, and two real datasets from production settings. Our experiments give interesting insights on plans for forecasting queries, and demonstrate the effectiveness and scalability of our plan-selection algorithms.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {711–722},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325934,
author = {Zukowski, Marcin and H\'{e}man, S\'{a}ndor and Nes, Niels and Boncz, Peter},
title = {Cooperative Scans: Dynamic Bandwidth Sharing in a DBMS},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {This paper analyzes the performance of concurrent (index) scan operations in both record (NSM/PAX) and column (DSM) disk storage models and shows that existing scheduling policies do not fully exploit data-sharing opportunities and therefore result in poor disk bandwidth utilization. We propose the Cooperative Scans framework that enhances performance in such scenarios by improving data-sharing between concurrent scans. It performs dynamic scheduling of queries and their data requests, taking into account the current system situation. We first present results on top of an NSM/PAX storage layout, showing that it achieves significant performance improvements over traditional policies in terms of both the number of I/Os and overall execution time, as well as latency of individual queries. We provide benchmarks with varying system parameters, data sizes and query loads to confirm the improvement occurs in a wide range of scenarios. Then we extend our proposal to a more complicated DSM scenario, discussing numerous problems related to the two-dimensional nature of disk scheduling in column stores.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {723–734},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325935,
author = {Chaudhuri, Surajit and Kaushik, Raghav and Pol, Abhijit and Ramamurthy, Ravi},
title = {Stop-and-Restart Style Execution for Long Running Decision Support Queries},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Long running decision support queries can be resource intensive and often lead to resource contention in data warehousing systems. Today, the only real option available to the DBAs when faced with such contention is to carefully select one or more queries and terminate them. However, the work done by such terminated queries is entirely lost even if they were very close to completion and these queries will need to be run in their entirety at a later time. In this paper, we show how instead we can support a Stop-and-Restart style query execution that can leverage partially the work done in the initial query execution. In order to re-execute only the remaining work of the query, a Stop-and-Restart execution would need to save all the previous work. But this approach would clearly incur high overheads which is undesirable. In contrast, we present a technique that can be used to save information selectively from the past execution so that the overhead can be bounded. Despite saving only limited information, our technique is able to reduce the running time of the restarted queries substantially. We show the effectiveness of our approach using real and benchmark data.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {735–745},
numpages = {11},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325937,
author = {Iwuchukwu, Tochukwu and Naughton, Jeffrey F.},
title = {K-Anonymization as Spatial Indexing: Toward Scalable and Incremental Anonymization},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In this paper we observe that k-anonymizing a data set is strikingly similar to building a spatial index over the data set, so similar in fact that classical spatial indexing techniques can be used to anonymize data sets. We use this observation to leverage over 20 years of work on database indexing to provide efficient and dynamic anonymization techniques. Experiments with our implementation show that the R-tree index-based approach yields a batch anonymization algorithm that is orders of magnitude more efficient than previously proposed algorithms and has the advantage of supporting incremental updates. Finally, we show that the anonymizations generated by the R-tree approach do not sacrifice quality in their search for efficiency; in fact, by several previously proposed quality metrics, the compact partitioning properties of R-trees generate anonymizations superior to those generated by previously proposed anonymization algorithms.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {746–757},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325938,
author = {Ghinita, Gabriel and Karras, Panagiotis and Kalnis, Panos and Mamoulis, Nikos},
title = {Fast Data Anonymization with Low Information Loss},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Recent research studied the problem of publishing microdata without revealing sensitive information, leading to the privacy preserving paradigms of k-anonymity and l-diversity. k-anonymity protects against the identification of an individual's record. l-diversity, in addition, safeguards against the association of an individual with specific sensitive information. However, existing approaches suffer from at least one of the following drawbacks: (i) The information loss metrics are counter-intuitive and fail to capture data inaccuracies inflicted for the sake of privacy. (ii) l-diversity is solved by techniques developed for the simpler k-anonymity problem, which introduces unnecessary inaccuracies. (iii) The anonymization process is inefficient in terms of computation and I/O cost.In this paper we propose a framework for efficient privacy preservation that addresses these deficiencies. First, we focus on one-dimensional (i.e., single attribute) quasi-identifiers, and study the properties of optimal solutions for k-anonymity and l-diversity, based on meaningful information loss metrics. Guided by these properties, we develop efficient heuristics to solve the one-dimensional problems in linear time. Finally, we generalize our solutions to multi-dimensional quasi-identifiers using space-mapping techniques. Extensive experimental evaluation shows that our techniques clearly outperform the state-of-the-art, in terms of execution time and information loss.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {758–769},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325939,
author = {Chen, Bee-Chung and LeFevre, Kristen and Ramakrishnan, Raghu},
title = {Privacy Skyline: Privacy with Multidimensional Adversarial Knowledge},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Privacy is an important issue in data publishing. Many organizations distribute non-aggregate personal data for research, and they must take steps to ensure that an adversary cannot predict sensitive information pertaining to individuals with high confidence. This problem is further complicated by the fact that, in addition to the published data, the adversary may also have access to other resources (e.g., public records and social networks relating individuals), which we call external knowledge. A robust privacy criterion should take this external knowledge into consideration.In this paper, we first describe a general framework for reasoning about privacy in the presence of external knowledge. Within this framework, we propose a novel multidimensional approach to quantifying an adversary's external knowledge. This approach allows the publishing organization to investigate privacy threats and enforce privacy requirements in the presence of various types and amounts of external knowledge. Our main technical contributions include a multidimensional privacy criterion that is more intuitive and flexible than previous approaches to modeling background knowledge. In addition, we provide algorithms for measuring disclosure and sanitizing data that improve computational efficiency several orders of magnitude over the best known techniques.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {770–781},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325940,
author = {Xie, Min and Wang, Haixun and Yin, Jian and Meng, Xiaofeng},
title = {Integrity Auditing of Outsourced Data},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {An increasing number of enterprises outsource their IT services to third parties who can offer these services for a much lower cost due to economy of scale. Quality of service is a major concern in outsourcing. In particular, query integrity, which means that query results returned by the service provider are both correct and complete, must be assured. Previous work requires clients to manage data locally to audit the results sent back by the server, or database engine to be modified for generating authenticated results. In this paper, we introduce a novel integrity audit mechanism that eliminating these costly requirements. In our approach, we insert a small amount of records into an outsourced database so that the integrity of the system can be effectively audited by analyzing the inserted records in the query results. We study both randomized and deterministic approaches for generating the inserted records, as how these records are generated has significant implications for storage and performance. Furthermore, we show that our method is provable secure, which means it can withstand any attacks by an adversary whose computation power is bounded. Our analytical and empirical results demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {782–793},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325942,
author = {Gonzalez, Hector and Han, Jiawei and Li, Xiaolei and Myslinska, Margaret and Sondag, John Paul},
title = {Adaptive Fastest Path Computation on a Road Network: A Traffic Mining Approach},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Efficient fastest path computation in the presence of varying speed conditions on a large scale road network is an essential problem in modern navigation systems. Factors affecting road speed, such as weather, time of day, and vehicle type, need to be considered in order to select fast routes that match current driving conditions. Most existing systems compute fastest paths based on road Euclidean distance and a small set of predefined road speeds. However, "History is often the best teacher". Historical traffic data or driving patterns are often more useful than the simple Euclidean distance-based computation because people must have good reasons to choose these routes, e.g., they may want to avoid those that pass through high crime areas at night or that likely encounter accidents, road construction, or traffic jams.In this paper, we present an adaptive fastest path algorithm capable of efficiently accounting for important driving and speed patterns mined from a large set of traffic data. The algorithm is based on the following observations: (1) The hierarchy of roads can be used to partition the road network into areas, and different path pre-computation strategies can be used at the area level, (2) we can limit our route search strategy to edges and path segments that are actually frequently traveled in the data, and (3) drivers usually traverse the road network through the largest roads available given the distance of the trip, except if there are small roads with a significant speed advantage over the large ones. Through an extensive experimental evaluation on real road networks we show that our algorithm provides desirable (short and well-supported) routes, and that it is significantly faster than competing methods.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {794–805},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325943,
author = {Bansal, Nilesh and Chiang, Fei and Koudas, Nick and Tompa, Frank Wm.},
title = {Seeking Stable Clusters in the Blogosphere},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The popularity of blogs has been increasing dramatically over the last couple of years. As topics evolve in the blogosphere, keywords align together and form the heart of various stories. Intuitively we expect that in certain contexts, when there is a lot of discussion on a specific topic or event, a set of keywords will be correlated: the keywords in the set will frequently appear together (pair-wise or in conjunction) forming a cluster. Note that such keyword clusters are temporal (associated with specific time periods) and transient. As topics recede, associated keyword clusters dissolve, because their keywords no longer appear frequently together.In this paper, we formalize this intuition and present efficient algorithms to identify keyword clusters in large collections of blog posts for specific temporal intervals. We then formalize problems related to the temporal properties of such clusters. In particular, we present efficient algorithms to identify clusters that persist over time. Given the vast amounts of data involved, we present algorithms that are fast (can efficiently process millions of blogs with multiple millions of posts) and take special care to make them efficiently realizable in secondary storage. Although we instantiate our techniques in the context of blogs, our methodology is generic enough to apply equally well to any temporally ordered text source.We present the results of an experimental study using both real and synthetic data sets, demonstrating the efficiency of our algorithms, both in terms of performance and in terms of the quality of the keyword clusters and associated temporal properties we identify.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {806–817},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325944,
author = {Li, Cuiping and Tung, Anthony K. H. and Jin, Wen and Ester, Martin},
title = {On Dominating Your Neighborhood Profitably},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Recent research on skyline queries has attracted much interest in the database and data mining community. Given a database, an object belongs to the skyline if it cannot be dominated with respect to the given attributes by any other database object. Current methods have only considered so-called min/max attributes like price and quality which a user wants to minimize or maximize. However, objects can also have spatial attributes like x, y coordinates which can be used to represent relevant constraints on the query results. In this paper, we introduce novel skyline query types taking into account not only min/max attributes but also spatial attributes and the relationships between these different attribute types. Such queries support a micro-economic approach to decision making, considering not only the quality but also the cost of solutions. We investigate two alternative approaches for efficient query processing, a symmetrical one based on off-the-shelf index structures, and an asymmetrical one based on index structures with special purpose extensions. Our experimental evaluation using a real dataset and various synthetic datasets demonstrates that the new query types are indeed meaningful and the proposed algorithms are efficient and scalable.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {818–829},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325945,
author = {Haas, Peter J. and Hueske, Fabian and Markl, Volker},
title = {Detecting Attribute Dependencies from Query Feedback},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Real-world datasets exhibit a complex dependency structure among the data attributes. Learning this structure is a key task in automatic statistics configuration for query optimizers, as well as in data mining, metadata discovery, and system management. In this paper, we provide a new method for discovering dependent attribute pairs based on query feedback. Our approach avoids the problem of searching through a combinatorially large space of candidate attribute pairs, automatically focusing system resources on those pairs of demonstrable interest to users. Unlike previous methods, our technique combines all of the pertinent feedback for a specified pair of attributes in a principled and robust manner, while being simple and fast enough to be incorporated into current commercial products. The method is similar in spirit to the CORDS algorithm, which proactively collects frequencies of data values and computes a chi-squared statistic from the resulting contingency table. In the reactive query-feedback setting, many entries of the contingency table are missing, and a key contribution of this paper is a variant of classical chi-squared theory that handles this situation. Because we typically discover a large number of dependent attribute pairs, we provide novel methods for ranking the pairs based on degree of dependency. Such ranking information, e.g., enables a database system to avoid exceeding the space budget for the system catalog by storing only the currently most important multivariate statistics. Experiments indicate that our dependency rankings are stable even in the presence of relatively few feedback records.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {830–841},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325947,
author = {Silberstein, Adam and Puggioni, Gavino and Gelfand, Alan and Munagala, Kamesh and Yang, Jun},
title = {Suppression and Failures in Sensor Networks: A Bayesian Approach},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Sensor networks allow continuous data collection on unprecedented scales. The primary limiting factor of such networks is energy, of which communication is the dominant consumer. The default strategy of nodes continually reporting their data to the root results in too much messaging. Suppression stands to greatly alleviate this problem. The simplest such scheme is temporal suppression, in which a node transmits its reading only when it has changed beyond some e since last transmitted. In the absence of a report, the root can infer that the value remains within ±ε hence, it is still able to derive the history of readings produced at the node.The critical weakness of suppression is message failure, to which sensor networks are particularly vulnerable. Failure creates ambiguity: a non-report may either be a suppression or a failure. Inferring the correct values for missing data and learning the parameters of the underlying process model become quite challenging. We propose a novel solution, BaySail, that incorporates the knowledge of the suppression scheme and application-level redundancy in Bayesian inference. We investigate several redundancy schemes and evaluate them in terms of in-network transmission costs and out-of-network inference efficacy, and the trade-off between these. Our experimental evaluation shows application-level redundancy outperforms retransmissions and basic sampling in both cost and accuracy of inference. The BaySail framework shows suppression schemes are generally effective for data collection, despite the presence of failures.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {842–853},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325948,
author = {Bhattacharya, Arnab and Meka, Anand and Singh, Ambuj K.},
title = {MIST: Distributed Indexing and Querying in Sensor Networks Using Statistical Models},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The modeling of high level semantic events from low level sensor signals is important in order to understand distributed phenomena. For such content-modeling purposes, transformation of numeric data into symbols and the modeling of resulting symbolic sequences can be achieved using statistical models---Markov Chains (MCs) and Hidden Markov Models (HMMs). We consider the problem of distributed indexing and semantic querying over such sensor models. Specifically, we are interested in efficiently answering (i) range queries: return all sensors that have observed an unusual sequence of symbols with a high likelihood, (ii) top-1 queries: return the sensor that has the maximum probability of observing a given sequence, and (iii) 1-NN queries: return the sensor (model) which is most similar to a query model. All the above queries can be answered at the centralized base station, if each sensor transmits its model to the base station. However, this is communication-intensive. We present a much more efficient alternative---a distributed index structure, MIST (Model-based Index STructure), and accompanying algorithms for answering the above queries. MIST aggregates two or more constituent models into a single composite model, and constructs an in-network hierarchy over such composite models. We develop two kinds of composite models: the first kind captures the average behavior of the underlying models and the second kind captures the extreme behaviors of the underlying models. Using the index parameters maintained at the root of a subtree, we bound the probability of observation of a query sequence from a sensor in the subtree. We also bound the distance of a query model to a sensor model using these parameters. Extensive experimental evaluation on both real-world and synthetic data sets show that the MIST schemes scale well in terms of network size and number of model states. We also show its superior performance over the centralized schemes in terms of update, query, and total communication costs.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {854–865},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325949,
author = {Moro, Mirella M. and Bakalov, Petko and Tsotras, Vassilis J.},
title = {Early Profile Pruning on XML-Aware Publish-Subscribe Systems},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Publish-subscribe applications are an important class of content-based dissemination systems where the message transmission is defined by the message content, rather than its destination IP address. With the increasing use of XML as the standard format on many Internet-based applications, XML aware pub-sub applications become necessary. In such systems, the messages (generated by publishers) are encoded as XML documents, and the profiles (defined by subscribers) as XML query statements. As the number of documents and query requests grow, the performance and scalability of the matching phase (i.e. matching of queries to incoming documents) become vital. Current solutions have limited or no flexibility to prune out queries in advance. In this paper, we overcome such limitation by proposing a novel early pruning approach called Bounding-based XML Filtering or BoXFilter. The BoXFilter is based on a new tree-like indexing structure that organizes the queries based on their similarity and provides lower and upper bound estimations needed to prune queries not related to the incoming documents. Our experimental evaluation shows that the early profile pruning approach offers drastic performance improvements over the current state-of-the-art in XML filtering.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {866–877},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325950,
author = {Chandramouli, Badrish and Phillips, Jeff M. and Yang, Jun},
title = {Value-Based Notification Conditions in Large-Scale Publish/Subscribe Systems?},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {We address the problem of providing scalable support for subscriptions with personalized value-based notification conditions in wide-area publish/subscribe systems. Notification conditions can be fine-tuned by subscribers, allowing precise and flexible control of when events are delivered to the subscribers. For example, a user may specify that she should be notified if and only if the price of a particular stock moves outside a "radius" around her last notified value. Naive techniques for handling notification conditions are not scalable. It is challenging to share subscription processing and notification dissemination of subscriptions with personalized value-based notification conditions, because two subscriptions may see two completely different sequences of notifications even if they specify the same radius. We develop and experimentally evaluate scalable processing and dissemination techniques for these subscriptions. Our approach uses standard network substrates for notification dissemination, and avoids pushing complex application processing into the network. Compared with other alternatives, our approach generates orders of magnitude lower network traffic, and incurs lower server processing cost.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {878–889},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325952,
author = {Hua, Ming and Pei, Jian and Fu, Ada W. C. and Lin, Xuemin and Leung, Ho-Fung},
title = {Efficiently Answering Top-k Typicality Queries on Large Databases},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Finding typical instances is an effective approach to understand and analyze large data sets. In this paper, we apply the idea of typicality analysis from psychology and cognition science to database query answering, and study the novel problem of answering top-k typicality queries. We model typicality in large data sets systematically. To answer questions like "Who are the top-k most typical NBA players?", the measure of simple typicality is developed. To answer questions like "Who are the top-k most typical guards distinguishing guards from other players?", the notion of discriminative typicality is proposed.Computing the exact answer to a top-k typicality query requires quadratic time which is often too costly for online query answering on large databases. We develop a series of approximation methods for various situations. (1) The randomized tournament algorithm has linear complexity though it does not provide a theoretical guarantee on the quality of the answers. (2) The direct local typicality approximation using VP-trees provides an approximation quality guarantee. (3) A VP-tree can be exploited to index a large set of objects. Then, typicality queries can be answered efficiently with quality guarantees by a tournament method based on a Local Typicality Tree data structure. An extensive performance study using two real data sets and a series of synthetic data sets clearly show that top-k typicality queries are meaningful and our methods are practical.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {890–901},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325953,
author = {Schnaitter, Karl and Spiegel, Joshua and Polyzotis, Neoklis},
title = {Depth Estimation for Ranking Query Optimization},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {A relational ranking query uses a scoring function to limit the results of a conventional query to a small number of the most relevant answers. The increasing popularity of this query paradigm has led to the introduction of specialized rank join operators that integrate the selection of top tuples with join processing. These operators access just "enough" of the input in order to generate just "enough" output and can offer significant speed-ups for query evaluation. The number of input tuples that an operator accesses is called the input depth of the operator, and this is the driving cost factor in rank join processing. This introduces the important problem of depth estimation, which is crucial for the costing of rank join operators during query compilation and thus for their integration in optimized physical plans.We introduce an estimation methodology, termed Deep, for approximating the input depths of rank join operators in a physical execution plan. At the core of Deep lies a general, principled framework that formalizes depth computation in terms of the joint distribution of scores in the base tables. This framework results in a systematic estimation methodology that takes the characteristics of the data directly into account and thus enables more accurate estimates. We develop novel estimation algorithms that provide an efficient realization of the formal Deep framework, and describe their integration on top of the statistics module of an existing query optimizer. We validate the performance of Deep with an extensive experimental study on data sets of varying characteristics. The results verify the effectiveness of Deep as an estimation method and demonstrate its advantages over previously proposed techniques.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {902–913},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325954,
author = {Arai, Benjamin and Das, Gautam and Gunopulos, Dimitrios and Koudas, Nick},
title = {Anytime Measures for Top-k Algorithms},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Top-k queries on large multi-attribute data sets are fundamental operations in information retrieval and ranking applications. In this paper, we initiate research on the anytime behavior of top-k algorithms. In particular, given specific top-k algorithms (TA and TA-Sorted) we are interested in studying their progress toward identification of the correct result at any point during the algorithms' execution. We adopt a probabilistic approach where we seek to report at any point of operation of the algorithm the confidence that the top-k result has been identified. Such a functionality can be a valuable asset when one is interested in reducing the runtime cost of top-k computations. We present a thorough experimental evaluation to validate our techniques using both synthetic and real data sets.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {914–925},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325956,
author = {Chen, Chen and Yan, Xifeng and Yu, Philip S. and Han, Jiawei and Zhang, Dong-Qing and Gu, Xiaohui},
title = {Towards Graph Containment Search and Indexing},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Given a set of model graphs D and a query graph q, containment search aims to find all model graphs g ε D such that q contains g (q ⊇ g). Due to the wide adoption of graph models, fast containment search of graph data finds many applications in various domains. In comparison to traditional graph search that retrieves all the graphs containing q (q ⊆ g), containment search has its own indexing characteristics that have not yet been examined. In this paper, we perform a systematic study on these characteristics and propose a contrast subgraph-based indexing model, called cIndex. Contrast subgraphs capture the structure differences between model graphs and query graphs, and are thus perfect for indexing due to their high selectivity. Using a redundancy-aware feature selection process, cIndex can sort out a set of significant and distinctive contrast subgraphs and maximize its indexing capability. We show that it is NP-complete to choose the best set of indexing features, and our greedy algorithm can approximate the one-level optimal index within a ratio of 1-- 1/e. Taking this solution as a base indexing model, we further extend it to accommodate hierarchical indexing methodologies and apply data space clustering and sampling techniques to reduce the index construction time. The proposed methodology provides a general solution to containment search and indexing, not only for graphs, but also for any data with transitive relations as well. Experimental results on real test data show that cIndex achieves near-optimal pruning power on various containment search workloads, and confirms its obvious advantage over indices built for traditional graph search in this new scenario.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {926–937},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325957,
author = {Zhao, Peixiang and Yu, Jeffrey Xu and Yu, Philip S.},
title = {Graph Indexing: Tree + Delta &lt;= Graph},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Recent scientific and technological advances have witnessed an abundance of structural patterns modeled as graphs. As a result, it is of special interest to process graph containment queries effectively on large graph databases. Given a graph database G, and a query raph q, the graph containment query is to retrieve all graphs in G which contain q as subgraph(s). Due to the vast number of graphs in G and the nature of complexity for subgraph isomorphism testing, it is desirable to make use of high-quality graph indexing mechanisms to reduce the overall query processing cost. In this paper, we propose a new cost-effective graph indexing method based on frequent tree-features of the graph database. We analyze the effectiveness and efficiency of tree as indexing feature from three critical aspects: feature size, feature selection cost, and pruning power. In order to achieve better pruning ability than existing graph-based indexing methods, we select, in addition to frequent tree-features (Tree), a small number of discriminative graphs (Δ) on demand, without a costly graph mining process beforehand. Our study verifies that (Tree+Δ) is a better choice than graph for indexing purpose, denoted (Tree+Δ ≥Graph), to address the graph containment query problem. It has two implications: (1) the index construction by (Tree+Δ) is efficient, and (2) the graph containment query processing by (Tree+Δ) is efficient. Our experimental studies demonstrate that (Tree+Δ) has a compact index structure, achieves an order of magnitude better performance in index construction, and most importantly, outperforms up-to-date graph-based indexing methods: gIndex and C-Tree, in graph containment query processing.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {938–949},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325958,
author = {Lv, Qin and Josephson, William and Wang, Zhe and Charikar, Moses and Li, Kai},
title = {Multi-Probe LSH: Efficient Indexing for High-Dimensional Similarity Search},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Similarity indices for high-dimensional data are very desirable for building content-based search systems for feature-rich data such as audio, images, videos, and other sensor data. Recently, locality sensitive hashing (LSH) and its variations have been proposed as indexing techniques for approximate similarity search. A significant drawback of these approaches is the requirement for a large number of hash tables in order to achieve good search quality. This paper proposes a new indexing scheme called multi-probe LSH that overcomes this drawback. Multi-probe LSH is built on the well-known LSH technique, but it intelligently probes multiple buckets that are likely to contain query results in a hash table. Our method is inspired by and improves upon recent theoretical work on entropy-based LSH designed to reduce the space requirement of the basic LSH method. We have implemented the multi-probe LSH method and evaluated the implementation with two different high-dimensional datasets. Our evaluation shows that the multi-probe LSH method substantially improves upon previously proposed methods in both space and time efficiency. To achieve the same search quality, multi-probe LSH has a similar time-efficiency as the basic LSH method while reducing the number of hash tables by an order of magnitude. In comparison with the entropy-based LSH method, to achieve the same search quality, multi-probe LSH uses less query time and 5 to 8 times fewer number of hash tables.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {950–961},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325960,
author = {Jain, Navendu and Kit, Dmitry and Mahajan, Prince and Yalagandula, Praveen and Dahlin, Mike and Zhang, Yin},
title = {STAR: Self-Tuning Aggregation for Scalable Monitoring},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {We present STAR, a self-tuning algorithm that adaptively sets numeric precision constraints to accurately and efficiently answer continuous aggregate queries over distributed data streams. Adaptivity and approximation are essential for both robustness to varying workload characteristics and for scalability to large systems. In contrast to previous studies, we treat the problem as a workload-aware optimization problem whose goal is to minimize the total communication load for a multi-level aggregation tree under a fixed error budget. STAR's hierarchical algorithm takes into account the update rate and variance in the input data distribution in a principled manner to compute an optimal error distribution, and it performs cost-benefit throttling to direct error slack to where it yields the largest benefits. Our prototype implementation of STAR in a large-scale monitoring system provides (1) a new distribution mechanism that enables self-tuning error distribution and (2) an optimization to reduce communication overhead in a practical setting by carefully distributing the initial, default error budgets. Through extensive simulations and experiments on a real network monitoring implementation, we show that STAR achieves significant performance benefits compared to existing approaches while still providing high accuracy and incurring low overheads.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {962–973},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325961,
author = {Quian\'{e}-Ruiz, J.-A. and Lamarre, Philippe and Valduriez, Patrick},
title = {SQLB: A Query Allocation Framework for Autonomous Consumers and Providers},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In large-scale distributed information systems, where participants are autonomous and have special interests for some queries, query allocation is a challenge. Much work in this context has focused on distributing queries among providers in a way that maximizes overall performance (typically throughput and response time). However, preserving the participants" interests is also important. In this paper, we make two main contributions. First, we provide a model to define participants' perception of the system w.r.t. their interests and propose metrics to evaluate the quality of query allocation methods. This model facilitates the design and evaluation of new query allocation methods that take into account the participants' interests. Second, we propose a framework for query allocation called Satisfaction-based Query Load Balancing (SQLB). To be fair, SQLB dynamically trades consumers' interests for providers' interests. And it continuously adapts to changes in participants' interests and to the workload. We implemented SQLB and compared it, through experimentation, to two important baseline query allocation methods, namely Capacity based and Mariposa-like. The results demonstrate that SQLB yields high efficiency while satisfying the participants' interests and significantly outperforms the baseline methods.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {974–985},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325962,
author = {Doulkeridis, Christos and Vlachou, Akrivi and Kotidis, Yannis and Vazirgiannis, Michalis},
title = {Peer-to-Peer Similarity Search in Metric Spaces},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {This paper addresses the efficient processing of similarity queries in metric spaces, where data is horizontally distributed across a P2P network. The proposed approach does not rely on arbitrary data movement, hence each peer joining the network autonomously stores its own data. We present SIMPEER, a novel framework that dynamically clusters peer data, in order to build distributed routing information at super-peer level. SIMPEER allows the evaluation of range and nearest neighbor queries in a distributed manner that reduces communication cost, network latency, bandwidth consumption and computational overhead at each individual peer. SIMPEER utilizes a set of distributed statistics and guarantees that all similar objects to the query are retrieved, without necessarily flooding the network during query processing. The statistics are employed for estimating an adequate query radius for k-nearest neighbor queries, and transform the query to a range query. Our experimental evaluation employs both real-world and synthetic data collections, and our results show that SIMPEER performs efficiently, even in the case of high degree of distribution.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {986–997},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325964,
author = {Bex, Geert Jan and Neven, Frank and Vansummeren, Stijn},
title = {Inferring XML Schema Definitions from XML Data},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Although the presence of a schema enables many optimizations for operations on XML documents, recent studies have shown that many XML documents in practice either do not refer to a schema, or refer to a syntactically incorrect one. It is therefore of utmost importance to provide tools and techniques that can automatically generate schemas from sets of sample documents. While previous work in this area has mostly focused on the inference of Document Type Definitions (DTDs for short), we will consider the inference of XML Schema Definitions (XSDs for short) --- the increasingly popular schema formalism that is turning DTDs obsolete. In contrast to DTDs where the content model of an element depends only on the element's name, the content model in an XSD can also depend on the context in which the element is used. Hence, while the inference of DTDs basically reduces to the inference of regular expressions from sets of sample strings, the inference of XSDs also entails identifying from a corpus of sample documents the contexts in which elements bear different content models. Since a seminal result by Gold implies that no inference algorithm can learn the complete class of XSDs from positive examples only, we focus on a class of XSDs that captures most XSDs occurring in practice. For this class, we provide a theoretically complete algorithm that always infers the correct XSD when a sufficiently large corpus of XML documents is available. In addition, we present a variant of this algorithm that works well on real-world (and therefore incomplete) data sets.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {998–1009},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325965,
author = {Yu, Cong and Jagadish, H. V.},
title = {Querying Complex Structured Databases},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Correctly generating a structured query (e.g., an XQuery or a SQL query) requires the user to have a full understanding of the database schema, which can be a daunting task. Alternative query models have been proposed to give users the ability to query the database without schema knowledge. Those models, including simple keyword search and labeled keyword search, aim to extract meaningful data fragments that match the structure-free query conditions (e.g., keywords) based on various matching semantics. Typically, the matching semantics are content-based: they are defined on data node inter-relationships and incur significant query evaluation cost. Our first contribution is a novel matching semantics based on analyzing the database schema. We show that query models employing a schema-based matching semantics can reduce query evaluation cost significantly while maintaining or even improving result quality.The adoption of schema-based matching semantics does not change the nature of those query models: they are still schema-ignorant, i.e., users express no schema knowledge (except the labels in labeled keyword search) in the query. While those models work well for some queries on some databases, they often encounter problems when applied to complex queries on databases with complex schemas. Our second contribution is a novel query model that incorporates partial schema knowledge through the use of schema summary. This new summary-aware query model, called Meaningful Summary Query (MSQ), seamlessly integrates summary-based structural conditions and structure-free conditions, and enables ordinary users to query complex databases. We design algorithms for evaluating MSQ queries, and demonstrate that MSQ queries can produce better results against complex databases when compared with previous approaches, and that they can be efficiently evaluated.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1010–1021},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325966,
author = {Helmer, Sven},
title = {Measuring the Structural Similarity of Semistructured Documents Using Entropy},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {We propose a technique for measuring the structural similarity of semistructured documents based on entropy. After extracting the structural information from two documents we use either Ziv-Lempel encoding or Ziv-Merhav crossparsing to determine the entropy and consequently the similarity between the documents. To the best of our knowledge, this is the first true linear-time approach for evaluating structural similarity. In an experimental evaluation we demonstrate that the results of our algorithm in terms of clustering quality are on a par with or even better than existing approaches.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1022–1032},
numpages = {11},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325968,
author = {Shen, Warren and Doan, AnHai and Naughton, Jeffrey F. and Ramakrishnan, Raghu},
title = {Declarative Information Extraction Using Datalog with Embedded Extraction Predicates},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In this paper we argue that developing information extraction (IE) programs using Datalog with embedded procedural extraction predicates is a good way to proceed. First, compared to current ad-hoc composition using, e.g., Perl or C++, Datalog provides a cleaner and more powerful way to compose small extraction modules into larger programs. Thus, writing IE programs this way retains and enhances the important advantages of current approaches: programs are easy to understand, debug, and modify. Second, once we write IE programs in this framework, we can apply query optimization techniques to them. This gives programs that, when run over a variety of data sets, are more efficient than any monolithic program because they are optimized based on the statistics of the data on which they are invoked. We show how optimizing such programs raises challenges specific to text data that cannot be accommodated in the current relational optimization framework, then provide initial solutions. Extensive experiments over real-world data demonstrate that optimization is indeed vital for IE programs and that we can effectively optimize IE programs written in this proposed framework.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1033–1044},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325969,
author = {Chu, Eric and Baid, Akanksha and Chen, Ting and Doan, AnHai and Naughton, Jeffrey},
title = {A Relational Approach to Incrementally Extracting and Querying Structure in Unstructured Data},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {There is a growing consensus that it is desirable to query over the structure implicit in unstructured documents, and that ideally this capability should be provided incrementally. However, there is no consensus about what kind of system should be used to support this kind of incremental capability. We explore using a relational system as the basis for a workbench for extracting and querying structure from unstructured data. As a proof of concept, we applied our relational approach to support structured queries over Wikipedia. We show that the data set is always available for some form of querying, and that as it is processed, users can pose a richer set of structured queries. We also provide examples of how we can incrementally evolve our understanding of the data in the context of the relational workbench.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1045–1056},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325970,
author = {Shao, Feng and Guo, Lin and Botev, Chavdar and Bhaskar, Anand and Chettiar, Muthiah and Yang, Fan and Shanmugasundaram, Jayavel},
title = {Efficient Keyword Search over Virtual XML Views},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark [5] open-source XML database system indicates that the proposed approach is scalable and efficient.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1057–1068},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325972,
author = {Gibas, Michael and Zheng, Ning and Ferhatosmanoglu, Hakan},
title = {A General Framework for Modeling and Processing Optimization Queries},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {An optimization query asks for one or more data objects that maximize or minimize some function over the data set. We propose a general class of queries, model-based optimization queries, in which a generic model is used to define a wide variety of queries involving an optimization objective function and/or a set of constraints on the attributes. This model can be used to define optimization of linear and nonlinear expressions over object attributes as well as many existing query types studied in database research literature. A significant and important subset of this general model relevant to real-world applications include queries where the optimization function and constraints are convex. We cast such queries as members of the convex optimization (CP) model and provide a unified query processing framework for CP queries that I/O optimally accesses data and space partitioning index structures without changing the underlying structures. We perform experiments to show the generality of the technique and where possible, compare to techniques developed for specialized optimization queries. We find that we achieve nearly identical performance to the limited optimization query types with optimal solutions, while providing generic modeling and processing for a much broader class of queries, and while effectively handling problem constraints.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1069–1080},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325973,
author = {D, Harish and Darera, Pooja N. and Haritsa, Jayant R.},
title = {On the Production of Anorexic Plan Diagrams},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {A "plan diagram" is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. We have shown recently that, for industrial-strength database engines, these diagrams are often remarkably complex and dense, with a large number of plans covering the space. However, they can often be reduced to much simpler pictures, featuring significantly fewer plans, without materially affecting the query processing quality. Plan reduction has useful implications for the design and usage of query optimizers, including quantifying redundancy in the plan search space, enhancing useability of parametric query optimization, identifying error-resistant and least-expected-cost plans, and minimizing the overheads of multi-plan approaches.We investigate here the plan reduction issue from theoretical, statistical and empirical perspectives. Our analysis shows that optimal plan reduction, w.r.t. minimizing the number of plans, is an NP-hard problem in general, and remains so even for a storage-constrained variant. We then present a greedy reduction algorithm with tight and optimal performance guarantees, whose complexity scales linearly with the number of plans in the diagram for a given resolution. Next, we devise fast estimators for locating the best tradeoff between the reduction in plan cardinality and the impact on query processing quality. Finally, extensive experimentation with a suite of multi-dimensional TPCH-based query templates on industrial-strength optimizers demonstrates that complex plan diagrams easily reduce to "anorexic" (small absolute number of plans) levels incurring only marginal increases in the estimated query processing costs.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1081–1092},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325974,
author = {Papadomanolakis, Stratos and Dash, Debabrata and Ailamaki, Anastasia},
title = {Efficient Use of the Query Optimizer for Automated Physical Design},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {State-of-the-art database design tools rely on the query optimizer for comparing between physical design alternatives. Although it provides an appropriate cost model for physical design, query optimization is a computationally expensive process. The significant time consumed by optimizer invocations poses serious performance limitations for physical design tools, causing long running times, especially for large problem instances. So far it has been impossible to remove query optimization overhead without sacrificing cost estimation precision. Inaccuracies in query cost estimation are detrimental to the quality of physical design algorithms, as they increase the chances of "missing" good designs and consequently selecting sub-optimal ones. Precision loss and the resulting reduction in solution quality is particularly undesirable and it is the reason the query optimizer is used in the first place.In this paper we eliminate the tradeoff between query cost estimation accuracy and performance. We introduce the INdex Usage Model (INUM), a cost estimation technique that returns the same values that would have been returned by the optimizer, while being three orders of magnitude faster. Integrating INUM with existing index selection algorithms dramatically improves their running times without precision compromises.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1093–1104},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325976,
author = {Krompass, Stefan and Kuno, Harumi and Dayal, Umeshwar and Kemper, Alfons},
title = {Dynamic Workload Management for Very Large Data Warehouses: Juggling Feathers and Bowling Balls},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Workload management for business intelligence (BI) queries poses different challenges than those addressed in the online transaction processing (OLTP) context. The fundamental problem is that the execution times of BI queries can range from milliseconds to hours, and it is difficult to estimate these times accurately. Key challenges raised by this problem are how to identify queries that are not performing properly and what to do about them.We propose here a workload management system for controlling the execution of individual queries based on realistic customer service level objectives. In order to validate our proposal, we have implemented an experimental system that includes a dynamic execution controller that leverages fuzzy logic. We present results from a number of experiments that we ran using workloads based on actual industrial workloads and customer objectives that we gathered by interviewing industry practitioners.Our experiments show that even a handful of moderately mis-behaving problem queries can have a significant impact on a workload consisting of thousands of queries. We were surprised when our experiments also demonstrated that false positives -- incorrectly identifying a normal query as a problem -- can also have significant consequences. For those reasons, it is very important that an execution controller be as accurate as possible -- avoiding both false positives and false negatives. Our experiments also validate that our execution controller can markedly improve the execution of a workload that includes problem queries.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1105–1115},
numpages = {11},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325977,
author = {Zhang, Mingwu and Zhang, Xiangyu and Zhang, Xiang and Prabhakar, Sunil},
title = {Tracing Lineage beyond Relational Operators},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Tracing the lineage of data is an important requirement for establishing the quality and validity of data. Recently, the problem of data provenance has been increasingly addressed in database research. Earlier work has been limited to the lineage of data as it is manipulated using relational operations within an RDBMS. While this captures a very important aspect of scientific data processing, the existing work is incapable of handling the equally important, and prevalent, cases where the data is processed by non-relational operations. This is particularly common in scientific data where sophisticated processing is achieved by programs that are not part of a DBMS. The problem of tracking lineage when non-relational operators are used to process the data is particularly challenging since there is potentially no constraint on the nature of the processing. In this paper we propose a novel technique that overcomes this significant barrier and enables the tracing of lineage of data generated by an arbitrary function. Our technique works directly with the executable code of the function and does not require any high-level description of the function or even the source code. We establish the feasibility of our approach on a typical application and demonstrate that the technique is able to discern the correct lineage. Furthermore, it is shown that the method can help identify limitations in the function itself.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1116–1127},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325978,
author = {Casati, Fabio and Castellanos, Malu and Dayal, Umeshwar and Salazar, Norman},
title = {A Generic Solution for Warehousing Business Process Data},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Improving business processes is critical to any corporation. Process improvement requires analysis as its first basic step. Process analysis has many unique challenges: i) companies execute many business processes, and devising ad hoc solutions for each of them is too costly. Hence, generic approaches must be sought; ii) the abstraction level at which processes need to be analyzed is much higher with respect to the information available in the process execution environment; iii) the rapidly increasing need of co-developing the process analysis and the process automation solution and the scale of the problem makes it hard to cope with frequent changes in the sources of process data. To address these problems, we have developed a process warehousing solution, used by HP and its customers. In this paper we describe the solution, the challenges we had to face, and the lessons we learned in implementing and deploying it.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1128–1137},
numpages = {10},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325979,
author = {Poess, Meikel and Nambiar, Raghunath Othayoth and Walrath, David},
title = {Why You Should Run TPC-DS: A Workload Analysis},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The Transaction Processing Performance Council (TPC) is completing development of TPC-DS, a new generation industry standard decision support benchmark. The TPC-DS benchmark, first introduced in the "The Making of TPC-DS" [9] paper at the 32nd International Conference on Very Large Data Bases (VLDB), has now entered the TPC's "Formal Review" phase for new benchmarks; companies and researchers alike can now download the draft benchmark specification and tools for evaluation. The first paper [9] gave an overview of the TPC-DS data model, workload model, and execution rules. This paper details the characteristics of different phases of the workload, namely: database load, query workload and data maintenance; and also their impact to the benchmark's performance metric. As with prior TPC benchmarks, this workload will be widely used by vendors to demonstrate their capabilities to support complex decision support systems, by customers as a key factor in purchasing servers and software, and by the database community for research and development of optimization techniques.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1138–1149},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325981,
author = {Stonebraker, Michael and Madden, Samuel and Abadi, Daniel J. and Harizopoulos, Stavros and Hachem, Nabil and Helland, Pat},
title = {The End of an Architectural Era: (It's Time for a Complete Rewrite)},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In previous papers [SC05, SBC+07], some of us predicted the end of "one size fits all" as a commercial relational DBMS paradigm. These papers presented reasons and experimental evidence that showed that the major RDBMS vendors can be outperformed by 1--2 orders of magnitude by specialized engines in the data warehouse, stream processing, text, and scientific database markets.Assuming that specialized engines dominate these markets over time, the current relational DBMS code lines will be left with the business data processing (OLTP) market and hybrid markets where more than one kind of capability is required. In this paper we show that current RDBMSs can be beaten by nearly two orders of magnitude in the OLTP market as well. The experimental evidence comes from comparing a new OLTP prototype, H-Store, which we have built at M.I.T. to a popular RDBMS on the standard transactional benchmark, TPC-C.We conclude that the current RDBMS code lines, while attempting to be a "one size fits all" solution, in fact, excel at nothing. Hence, they are 25 year old legacy code lines that should be retired in favor of a collection of "from scratch" specialized engines. The DBMS vendors (and the research community) should start with a clean sheet of paper and design systems for tomorrow's requirements, not continue to push code lines and architectures designed for yesterday's needs.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1150–1160},
numpages = {11},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325982,
author = {Brodie, Michael L.},
title = {Computer Science 2.0: A New World of Data Management},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $15 billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 1990's data under DBMS management reached 10% of the world's data. The six-fold growth of non-relational data in the period 2006--2010 will reduce that number to well below 5%.We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science 2.0 will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management.The data management world should embrace these opportunities and provide leadership for data management in Computer Science 2.0. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of 40 billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think? Semantic web services will constitute a programming model of Computer Science 2.0. How does data management fit into this semantically rich environment?Computer Science 2.0 offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1161},
numpages = {1},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325984,
author = {Vo, Binh Dao and Manku, Gurmeet Singh},
title = {RadixZip: Linear Time Compression of Token Streams},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {RadixZip is a block compression technique for token streams. It introduces RadixZip Transform, a linear time algorithm that rearranges bytes using a technique inspired by radix sorting. For appropriate data, RadixZip Transform is analogous to the Burrows-Wheeler Transform used in bzip2, but is both simpler in operation and more effective in compression. In addition, RadixZip Transform can take advantage of correlations between token streams with no computational overhead. Experiments over practical data show that for common token streams, RadixZip is superior to bzip2.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1162–1172},
numpages = {11},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325985,
author = {Witkowski, Andrew and Bellamkonda, Srikanth and Li, Hua-Gang and Liang, Vince and Sheng, Lei and Smith, Wayne and Subramanian, Sankar and Terry, James and Yu, Tsae-Feng},
title = {Continuous Queries in Oracle},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {This paper describes Continuous Queries (CQ) in Oracle RDBMS, a feature that incorporates stream and complex event processing into an RDBMS, the first such attempt in commercial databases. The feature is based on the concept of query difference and allows us to monitor real time changes to the query as the result of changes to its underlying tables. The result of a continuous query can be deposited into historical tables or queues for further asynchronous de-queuing, or can invoke a synchronous trigger for procedural processing. The main contribution of our CQ engine is that it allows us to react to complex scenarios of changes to data such as mixed INSERT, DELETE and UPDATE changes, unlike the existing stream processing systems that deal with INSERTS only. We support a wide range of query shapes including inner, semi and anti-joins, aggregates and window functions. More details are given to the efficient computation of query difference for general cases and their optimizations based on semantic constraints. They are shown to improve the response time for practical cases by more than an order of magnitude. We also show how delaying CQ re-computation can improve its performance by batch processing the changes to the base tables.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1173–1184},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325986,
author = {Wu, Kun-Lung and Hildrum, Kirsten W. and Fan, Wei and Yu, Philip S. and Aggarwal, Charu C. and George, David A. and Gedik, Buundefinedra and Bouillet, Eric and Gu, Xiaohui and Luo, Gang and Wang, Haixun},
title = {Challenges and Experience in Prototyping a Multi-Modal Stream Analytic and Monitoring Application on System S},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In this paper, we describe the challenges of prototyping a reference application on System S, a distributed stream processing middleware under development at IBM Research. With a large number of stream PEs (Processing Elements) implementing various stream analytic algorithms, running on a large-scale, distributed cluster of nodes, and collaboratively digesting several multi-modal source streams with vastly differing rates, prototyping a reference application on System S faces many challenges. Specifically, we focus on our experience in prototyping DAC (Disaster Assistance Claim monitoring), a reference application dealing with multi-modal stream analytic and monitoring. We describe three critical challenges: (1) How do we generate correlated, multi-modal source streams for DAC? (2) How do we design and implement a comprehensive stream application, like DAC, from many divergent stream analytic PEs? (3) How do we deploy DAC in light of source streams with extremely different rates? We report our experience in addressing these challenges, including modeling a disaster claim processing center to generate correlated source streams, constructing the PE flow graph, utilizing programming supports from System S, adopting parallelism, and exploiting resource-adaptive computation.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1185–1196},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325988,
author = {Bhattacharjee, Bishwaranjan and Malkemus, Timothy and Lau, Sherman and McKeough, Sean and Kirton, Jo-anne and Von Boeschoten, Robin and Kennedy, John P},
title = {Efficient Bulk Deletes for Multi Dimensional Clustered Tables in DB2},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In data warehousing applications, the ability to efficiently delete large chunks of data from a table is very important. This feature is also known as Rollout or Bulk Deletes. Rollout is generally carried out periodically and is often done on more than one dimension or attribute. The ability to efficiently handle the updates of RID indexes while doing Rollouts is a well known problem for database engines and its solution is very important for data warehousing applications. DB2 UDB V8.1 introduced a new physical clustering scheme called Multi Dimensional Clustering (MDC) which allows users to cluster data in a table on multiple attributes or dimensions. This is very useful for query processing and maintenance activities including deletes. Subsequently, an enhancement was incorporated in DB2 UDB Viper 2 which allows for very efficient online rollout of data on dimensional boundaries even when there are a lot of secondary RID indexes defined on the table. This is done by the asynchronous updates of these RID indexes in the background while allowing the delete to commit and the table to be accessed. This paper details the design of MDC Rollout and the challenges that were encountered. It discusses some performance results which show order of magnitude improvements using it and the lessons learnt.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1197–1206},
numpages = {10},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325989,
author = {Hu, Ying and Sundara, Seema and Srinivasan, Jagannathan},
title = {Supporting Time-Constrained SQL Queries in Oracle},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The growing nature of databases, and the flexibility inherent in the SQL query language that allows arbitrarily complex formulations, can result in queries that take inordinate amount of time to complete. To mitigate this problem, strategies that are optimized to return the 'first-few rows' or 'top-k rows' (in case of sorted results) are usually employed. However, both these strategies can lead to unpredictable query processing times. Thus, in this paper we propose supporting time-constrained SQL queries. Specifically, a user issues a SQL query as before but additionally provides nature of constraint (soft or hard), an upper bound for query processing time, and acceptable nature of results (partial or approximate). The DBMS takes the criteria (constraint type, time limit, quality of result) into account in generating the query execution plan, which is expected (guaranteed) to complete in the allocated time for soft (hard) time constraint. If partial results are acceptable then the technique of reducing result set cardinality (i.e. returning first few or top-k rows) is used, whereas if approximate results are acceptable then sampling is used, to compute query results within the specified time limit. For the latter case, we argue that trading off quality of results for predictable response time is quite useful. However, for this case, we provide additional aggregate functions to estimate the aggregate values and to compute the associated confidence interval. This paper presents the notion of time-constrained SQL queries, discusses the challenges in supporting such a construct, describes a framework for supporting such queries, and outlines its implementation in Oracle Database by exploiting Oracle's cost-based optimizer and extensibility capabilities.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1207–1218},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325990,
author = {Lee, Rubao and Zhou, Minghong and Liao, Huaming},
title = {Request Window: An Approach to Improve Throughput of RDBMS-Based Data Integration System by Utilizing Data Sharing across Concurrent Distributed Queries},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {This paper focuses on the problem of improving distributed query throughput of the RDBMS-based data integration system that has to inherit the query execution model of the underlying RDBMS: execute each query independently and utilize a global buffer pool mechanism to provide disk page sharing across concurrent query execution processes. However, this model is not suitable for processing concurrent distributed queries because the foundation, the memory-disk hierarchy, does not exist for data provided by remote sources. Therefore, the query engine cannot exploit any data sharing so that each process will have to interact with data sources independently: issue data requests and fetch data over the network.This paper presents Request Window, a novel DQP mechanism that can detect and employ data sharing opportunities across concurrent distributed queries. By combining multiple similar data requests issued to the same data source to a common data request, Request Window allows concurrent query executing processes to share the common result data. With the benefits of reduced source burdens and data transfers, the throughput of query engine can be significantly improved. This paper also introduces the IGNITE system, an extended PostgreSQL with DQP support. Our experimental results show that Request Window makes IGNITE achieve a 1.7x speedup over a commercial data integration system when running a workload of distributed TPC-H queries.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1219–1230},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325991,
author = {Onose, Nicola and Borkar, Vinayak and Carey, Michael J.},
title = {Inverse Functions in the AquaLogic Data Services Platform},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {When integrating data from heterogeneous sources, it is often necessary to transform both the schemas and the data from the underlying sources in order to present the integrated data in the form desired by its consuming applications. Unfortunately, these transformations---particularly if implemented by custom code---can block query optimization and updates, leading to potentially severe performance and functionality limitations. To circumvent these problems, the BEA AquaLogic Data Services Platform provides support for user-defined inverse functions. This paper describes the motivation, design, user experience, and implementation associated with inverse functions in ALDSP. This functionality debuted in version 2.1 of ALDSP in March 2006.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1231–1242},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325993,
author = {Bati, Hardik and Giakoumakis, Leo and Herbert, Steve and Surna, Aleksandras},
title = {A Genetic Approach for Random Testing of Database Systems},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Testing a database engine has been and continues to be a challenging task. The space of possible SQL queries along with their possible access paths is practically unbounded. Moreover, this space is continuously increasing in size as the feature set of modern DBMS systems expands with every product release. To tackle these problems, random query generator tools have been used to create large numbers of test cases. While such test case generators enable the creation of complex and syntactically correct SQL queries, they do not guarantee that the queries produced return results or exercise desired DBMS components. Very often the generated queries contain logical contradictions, which cause "short-circuits" at the query optimization layer, failing to exercise the lower layers of the database engine (query optimization, query execution, access methods, etc.)In this paper we present a random test case generation technique, which provides solutions to the above problems. Our technique utilizes execution feedback, obtained from the DBMS under test, in order to guide the test generation process toward specific DBMS subcomponents and rarely exercised code paths. Test cases are created incrementally using a genetic approach, which synthesizes query characteristics that are of interest for the purposes of test coverage. Our experiments indicate that our technique can outperform other methods of random testing in terms of efficiency and code coverage. We also provide experimental results which show that the use of execution feedback improves code coverage of specific DBMS components. Finally, we share our experiences gained from using this testing approach during the development cycles of Microsoft SQL Server.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1243–1251},
numpages = {9},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325994,
author = {Chaudhuri, Surajit and Narasayya, Vivek and Syamala, Manoj},
title = {Bridging the Application and DBMS Profiling Divide for Database Application Developers},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In today's world, tools for profiling and tuning application code remain disconnected from the profiling and tuning tools for relational DBMSs. This makes it challenging for developers of database applications to profile, tune and debug their applications, for example, identifying application code that causes deadlocks in the server. We have developed an infrastructure that simultaneously captures both the application context as well as the database context, thereby enabling a rich class of tuning, profiling and debugging tasks that is not possible today. We have built a tool using this infrastructure that enables developers to seamlessly profile, tune and debug ADO.NET applications over Microsoft SQL Server by taking advantage of information across the application and database contexts. We describe and evaluate several tasks that can be accomplished using this tool.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1252–1262},
numpages = {11},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325995,
author = {Jorwekar, Sudhir and Fekete, Alan and Ramamritham, Krithi and Sudarshan, S.},
title = {Automating the Detection of Snapshot Isolation Anomalies},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Snapshot isolation (SI) provides significantly improved concurrency over 2PL, allowing reads to be non-blocking. Unfortunately, it can also lead to non-serializable executions in general. Despite this, it is widely used, supported in many commercial databases, and is in fact the highest available level of consistency in Oracle and Post-greSQL. Sufficient conditions for detecting whether SI anomalies could occur in a given set of transactions were presented recently, and extended to necessary conditions for transactions without predicate reads.In this paper we address several issues in extending the earlier theory to practical detection/correction of anomalies. We first show how to mechanically find a set of programs which is large enough so that we ensure that all executions will be free of SI anomalies, by modifying these programs appropriately. We then address the problem of false positives, i.e., transaction programs wrongly identified as possibly leading to anomalies, and present techniques that can significantly reduce such false positives. Unlike earlier work, our techniques are designed to be automated, rather than manually carried out. We describe a tool which we are developing to carry out this task. The tool operates on descriptions of the programs either taken from the application code itself, or taken from SQL query traces. It can be used with any database system. We have used our tool on two real world applications in production use at IIT Bombay, and detected several anomalies, some of which have caused real world problems. We believe such a tool will be invaluable for ensuring safe execution of the large number of applications which are already running under SI.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1263–1274},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325997,
author = {Liu, Li and Li, Eric and Zhang, Yimin and Tang, Zhizhong},
title = {Optimization of Frequent Itemset Mining on Multiple-Core Processor},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Multi-core processors are proliferated across different domains in recent years. In this paper, we study the performance of frequent pattern mining on a modern multi-core machine. A detailed study shows that, even with the best implementation, current FP-tree based algorithms still under-utilize a multi-core system due to poor data locality and insufficient parallelism expression. We propose two techniques: a cache-conscious FP-array (frequent pattern array) and a lock-free dataset tiling parallelization mechanism to address this problem. The FP-array efficiently improves the data locality performance, and makes use of the benefits from hardware and software prefetching. The result yields an overall 4.0 speedup compared with the state-of-the-art implementation. Furthermore, to unlock the power of multi-core processor, a lock-free parallelization approach is proposed to restructure the FP-tree building algorithm. It not only eliminates the locks in building a single FP-tree with fine-grained threads, but also improves the temporal data locality performance. To summarize, with the proposed cache-conscious FP-array and lock-free parallelization enhancements, the overall FP-tree algorithm achieves a 24 fold speedup on an 8-core machine. Finally, we believe the presented techniques can be applied to other data mining tasks as well with the prevalence of multi-core processor.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1275–1285},
numpages = {11},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325998,
author = {Gedik, Buundefinedra and Bordawekar, Rajesh R. and Yu, Philip S.},
title = {CellSort: High Performance Sorting on the Cell Processor},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In this paper we describe the design and implementation of CellSort - a high performance distributed sort algorithm for the Cell processor. We design CellSort as a distributed bitonic merge with a data-parallel bitonic sorting kernel. In order to best exploit the architecture of the Cell processor and make use of all available forms of parallelism to achieve good scalability, we structure CellSort as a three-tiered sort. The first tier is a SIMD (single-instruction multiple data) optimized bitonic sort, which sorts up to 128KB of items that cat fit into one SPE's (a co-processor on Cell) local store. We design a comprehensive SIMDization scheme that employs data parallelism even for the most fine-grained steps of the bitonic sorting kernel. Our results show that, SIMDized bitonic sorting kernel is vastly superior to other alternatives on the SPE and performs up to 1.7 times faster compared to quick sort on 3.2GHz Intel Xeon. The second tier is an in-core bitonic merge optimized for cross-SPE data transfers via asynchronous DMAs, and sorts enough number of items that can fit into the cumulative space available on the local stores of the participating SPEs. We design data transfer and synchronization patters that minimize serial sections of the code by taking advantage of the high aggregate cross-SPE bandwidth available on Cell. Results show that, in-core bitonic sort scales well on the Cell processor with increasing number of SPEs, and performs up to 10 times faster with 16 SPEs compared to parallel quick sort on dual-3.2 GHz Intel Xeon. The third tier is an out-of-core bitonic merge which sorts large number of items stored in the main memory. Results show that, when properly implemented, distributed out-of-core bitonic sort on Cell can significantly outperform the asymptotically (average case) superior quick sort for large number of memory resident items (up to 4 times faster when sorting 0.5GB of data with 16 SPEs, compared to dual-3.2GHz Intel Xeon).},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1286–1297},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1325999,
author = {Lang, Christian A. and Bhattacharjee, Bishwaranjan and Malkemus, Tim and Wong, Kwai},
title = {Increasing Buffer-Locality for Multiple Index Based Scans through Intelligent Placement and Index Scan Speed Control},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Decision support systems are characterized by large concurrent scan operations. A significant percentage of these scans are executed as index based scans of the data. This is especially true when the data is physically clustered on the index columns using the various clustering schemes employed by database engines. Common database management systems have only limited ability to reuse buffer content across multiple running queries due to their treatment of queries in isolation. Previous attempts to coordinate scans for better buffer reuse were limited to table scans only. Attempts for index based scan sharing were non existent or were less than satisfactory due to drifting between scans.In this paper, we describe a mechanism to keep scans using the same index closer together on scan position during scanning. This is achieved via intelligent placement of index scans at scan start time based on their scan ranges and speeds. This is then augmented by adaptive throttling of scan speeds based on the index scans runtime behavior during scan execution. We discuss the challenges in doing it for index scans in comparison to the more common table scan sharing. We show that this can be done with minimal changes to an existing database management system as demonstrated in our DB2 UDB prototype. Our experiments show significant gains in end-to-end response times and disk I/O for TPC-H workloads.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1298–1309},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326001,
author = {Wang, Fusheng and Bourgu\'{e}, Pierre-Emmanuel and Hackenberg, Georg and Wang, Mo and Kaltschmidt, David and Liu, Peiya},
title = {SciPort: An Adaptable Scientific Data Integration Platform for Collaborative Scientific Research},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Scientific data are posing new challenges to data management due to the large volume, complexity and heterogeneity of the data. Meanwhile, scientific collaboration becomes increasingly important, which relies on integrating and sharing data from distributed institutions. In this demo, we present SciPort, a Web-based platform on supporting scientific data management and integration based on peer-to-peer architectures, where researchers can easily collect, publish, and share their complex scientific data across multi-institutions. SciPort provides a general metadata based data model to capture the context description of experiments and link experiment data into comprehensive metadata documents, and supports a hierarchical organization of the overall data space for data browsing. SciPort takes two alternative "peer"-to-"peer" (or peer-database-to-peer-database) based approaches to integrate scientific data: pure peer-to-peer architecture and central server based peer-to-peer architecture. The later provides a virtual view of all published data from multiple local sites and supports complex queries with XQuery. The system provides a unified framework for adaptable architectures and customizable schemas, and supplies comprehensive tool set to manage and share scientific data. SciPort was first prototyped in Siemens Corporate Research, and now becomes a mature product and has been successfully used in both biomedical research and clinical trials for scientific research communities.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1310–1313},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326002,
author = {Wu, Tianyi and Li, Xiaolei and Xin, Dong and Han, Jiawei and Lee, Jacob and Redder, Ricardo},
title = {DataScope: Viewing Database Contents in Google Maps' Way},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {People have been relying on Google Maps, MapQuest, or other similar services to find desired locations on maps, browse surrounding businesses, get driving directions, etc.. Navigation by clicking and dragging the mouse to browse maps at multiple levels of resolution is one of the most attractive features in Web-based map exploration. Most database systems, though with some graphical user interfaces, are still lack of data-content browsing-based interfaces. Motivated by Google Maps, we develop DataScope, a Web-based data content visualization system, for people to view the desired data easily, interactively, and at multi-resolution.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1314–1317},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326003,
author = {Duchateau, Fabien and Bellahs\`{e}ne, Zohra and Hunt, Ela},
title = {XBenchMatch: A Benchmark for XML Schema Matching Tools},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {We present XBenchMatch, a benchmark which uses as input the result of a schema matching algorithm (set of mappings and/or an integrated schema) and generates statistics about the quality of this input and the performance of the matching tool.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1318–1321},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326004,
author = {Kensche, David and Quix, Christoph and Li, Xiang and Li, Yong},
title = {GeRoMeSuite: A System for Holistic Generic Model Management},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Manipulation of models and mappings is a common task in the design and development of information systems. Research in Model Management aims at supporting these tasks by providing a set of operators to manipulate models and mappings. As a framework, GeRoMeSuite provides an environment to simplify the implementation of model management operators. GeRoMeSuite is based on the generic role based metamodel GeRoMe [10], which represents models from different modeling languages (such as XML Schema, OWL, SQL) in a generic way. Thereby, the management of models in a polymorphic fashion is enabled, i.e. the same operator implementations are used regardless of the original modeling language of the schemas. In addition to providing a framework for model management, GeRoMeSuite implements several fundamental operators such as Match, Merge, and Compose.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1322–1325},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326005,
author = {Chiticariu, Laura and Hern\'{a}ndez, Mauricio A. and Kolaitis, Phokion G. and Popa, Lucian},
title = {Semi-Automatic Schema Integration in Clio},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Schema integration is the problem of finding a unified representation, called the integrated schema, from a set of source schemas that are related to each other. The relationships between the source schemas can be represented via correspondences between schema elements or via some other forms of schema mappings such as constraints or views. The integrated schema can be viewed as a means for dealing with the heterogeneity in the source schemas, by providing a standard representation of the data. Schema integration has received much of attention in the research literature [1, 2, 6, 8, 10] and still remains a challenge in practice. Existing approaches require substantial amount of human feedback during the integration process and moreover, the outcome of these approaches is a single integrated schema. In general, however, there can be multiple possible schemas that integrate data in different ways and each may be valuable in a given scenario.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1326–1329},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326006,
author = {Liu, Ziyang and Walker, Jeffrey and Chen, Yi},
title = {XSeek: A Semantic XML Search Engine Using Keywords},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {We present XSeek, a keyword search engine that enables users to easily access XML data without the need of learning XPath or XQuery and studying possibly complex data schemas. XSeek addresses a challenge in XML keyword search that has been neglected in the literature: how to determine the desired return information, analogous to inferring a "return" clause in XQuery. To infer the search semantics, XSeek recognizes possible entities and attributes in the data, differentiates search predicates and return specifications in the keywords, and generates meaningful search results based on the analysis.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1330–1333},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326007,
author = {Cudr\'{e}-Mauroux, Philippe and Agarwal, Suchit and Budura, Adriana and Haghani, Parisa and Aberer, Karl},
title = {Self-Organizing Schema Mappings in the GridVine Peer Data Management System},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {GridVine is a Peer Data Management System based on a decentralized access structure. Built following the principle of data independence, it separates a logical layer -- where data, schemas and mappings are managed -- from a physical layer consisting of a structured Peer-to-Peer network supporting efficient routing of messages and index load-balancing. Our system is totally decentralized, yet it fosters semantic interoperability through pairwise schema mappings and query reformulation. In this demonstration, we present a set of algorithms to automatically organize the network of schema mappings. We concentrate on three key functionalities: (1) the sharing of data, schemas and schema mappings in the network, (2) the dynamic creation and deprecation of mappings to foster global interoperability, and (3) the propagation of queries using the mappings. We illustrate these functionalities using bioinformatic schemas and data in a network running on several hundreds of peers simultaneously.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1334–1337},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326008,
author = {Nambiar, Ullas and Gupta, Himanshu and Mohania, Mukesh},
title = {CallAssist: Helping Call Center Agents in Preference Elicitation},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The increasing complexity of products and services being offered by businesses has made providing customers with easy access to technical assistance an important business function. Therefore, most businesses operate call centers to respond to product related queries from consumers. An emerging model is to let a third-party to run the contact center for a business. Preference elicitation - the process of asking queries to determine preferences is a key function performed by call-center agents. In this paper, our focus is on helping call-center agents to efficiently elicit customer's preference.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1338–1341},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326009,
author = {Sion, Radu and Bajaj, Sumeet and Carbunar, Bogdan and Katzenbeisser, Stefan},
title = {NS2: Networked Searchable Store with Correctness},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In an outsourced data framework, we introduce and demonstrate mechanisms for securely storing a set of data items (documents) on an un-trusted server, while allowing for subsequent conjunctive keyword searches for matching documents. The protocols provide full computational privacy, query correctness assurances and no leaks: the server either correctly executes client queries or (if it behaves maliciously) is immediately detected. The client is then provided with strong assurances proving the authenticity and completeness of results. This is different from existing secure keyword search research efforts where a cooperating, non-malicious server behavior is assumed. Additionally, not only does the oblivious search protocol conceal the outsourced data (from the un-trusted server) but it also does not leak client access patterns, the queries themselves, the association between different queries or between newly added documents and their corresponding keywords (not even in encrypted form). These assurances come at the expense of additional computation costs which we analyze in the context of today's hardware.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1342–1345},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326010,
author = {Salperwyck, Christophe and Anciaux, Nicolas and Benzine, Mehdi and Bouganim, Luc and Pucheral, Philippe and Shasha, Dennis},
title = {GhostDB: Hiding Data from Prying Eyes},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Imagine that you have been entrusted with private data, such as corporate product information, sensitive government information, or symptom and treatment information about hospital patients. You may want to issue queries whose result will combine private and public data, but private data must not be revealed, say, to the prying eyes of some insurance fraudster. GhostDB is an architecture and system to achieve this.You carry private data in a smart USB device (a large Flash persistent store combined with a tamper and snoop-resistant CPU and small RAM). When the key is plugged in, you can issue queries that link private and public data and be sure that the only information revealed to a potential spy is which queries you pose and the public data you access. Queries linking public and private data entail novel distributed processing techniques on extremely unequal devices (standard computer and smart USB device) in which data flows in only one direction: from public to private. This demonstration shows GhostDB's query processing in action.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1346–1349},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326012,
author = {Bleiholder, Jens and Draba, Karsten and Naumann, Felix},
title = {FuSem: Exploring Different Semantics of Data Fusion},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Data fusion is the final step of a typical data integration process, after schematic conflicts have been overcome and after duplicates have been correctly identified. We present the relational data fusion system FuSem, which uses schema mappings and information about duplicates to decide what to fuse, i.e., which tuples to merge into one. The aspect emphasized by the demo is how to fuse the duplicates with FuSem. First, it offers several conflict resolution functions to handle data conflicts among duplicates. Furthermore, different fusion semantics proposed in the literature, such as MatchJoin or ConQuer, can be compared and visually explored. Optimized execution allows interactive access to the data and thus to explore the different data fusion procedures.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1350–1353},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326013,
author = {Brochhaus, Christoph and Seidl, Thomas},
title = {<i>IndeGS</i>: Index Supported Graphics Data Server for CFD Data Postprocessing},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Virtual reality techniques particularly in the field of CFD (computational fluid dynamics) are of growing importance due to their ability to offer comfortable means to interactively explore 3D data sets. The growing accuracy of the simulations brings modern main memory based visualization frameworks to their limits, inducing a limitation on CFD data sizes and an increase in query response times, which are obliged to be very low for efficient interactive exploration. We therefore developed "IndeGS", the index supported graphics data server, to offer efficient dynamic view dependent query processing on secondary storage indexes organized by "IndeGS" offering a high degree of interactivity and mobility in VR environments in the context of CFD postprocessing on arbitrarily sized data sets.Our demonstration setup presents "IndeGS" as an independent network component which can be addressed by arbitrary VR visualization hardware ranging from complex setups (e.g. CAVE, HoloBench) over standard PCs to mobile devices (e.g. PDAs). Our demonstration includes a 2D visualization prototype and a comfortable user interface to simulate view dependent CFD postprocessing performed by an interactive user freely roaming a fully immersive VR environment. Hereby, the effects of the use of different distance functions and query strategies integrated into "IndeGS" are visualized in a comprehensible way.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1354–1357},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326014,
author = {Minock, Michael J.},
title = {A <i>STEP</i> towards Realizing Codd's Vision of Rendezvous with the Casual User},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {This demonstration showcases the STEP system for natural language access to relational databases. In STEP an administrator authors a highly structured semantic grammar through coupling phrasal patterns to elementary expressions within a decidable fragment of tuple relational calculus. The resulting phrasal lexicon serves as a bi-directional grammar, enabling the generation of natural language from tuple relational calculus and the inverse parsing of natural language to tuple calculus. This ability to both understand and generate natural language enables STEP to engage the user in clarification dialogs when the parse of their query is of questionable quality. The STEP system is nearing completion and will soon be field tested in several domains.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1358–1361},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326015,
author = {Jensen, Christian S. and Pakalnis, Stardas},
title = {TRAX: Real-World Tracking of Moving Objects},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {A range of mobile services rely on knowing the current positions of populations of so-called moving objects. In the ideal setting, the positions of all objects are known always and exactly. While this is not possible in practice, it is possible to know each object's position with a certain guaranteed accuracy.This paper presents the TRAX tracking system that supports several techniques capable of tracking the current positions of moving objects with guaranteed accuracies at low update and communication costs in real-world settings. The techniques are readily relevant for practical applications, but they also have implications for continued research. The tracking techniques offer a realistic setting for existing query processing techniques that assume that it is possible to always know the exact positions of moving objects. The techniques enable studies of trade-offs between querying and update, and the accuracy guarantees they offer may be exploited by query processing techniques to offer perfect recall.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1362–1365},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326016,
author = {Biton, Olivier and Cohen-Boulakia, Sarah and Davidson, Susan B.},
title = {Zoom*UserViews: Querying Relevant Provenance in Workflow Systems},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In this demonstration, we present the ZOOM*UserView system, and focus on the module which generates a "user view" based on what tasks the user perceives to be relevant in the workflow specification. We will show how user views can be used to reduce the amount of information returned by provenance queries, while focusing on information the user finds relevant. User views are based on the notion of composite tasks, and induce a higher-level specification of a workflow.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1366–1369},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326017,
author = {Altinel, Mehmet and Brown, Paul and Cline, Susan and Kartha, Rajesh and Louie, Eric and Markl, Volker and Mau, Louis and Ng, Yip-Hing and Simmen, David and Singh, Ashutosh},
title = {Damia: A Data Mashup Fabric for Intranet Applications},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Damia is a lightweight enterprise data integration service where line of business users can create and catalog high value data feeds for consumption by situational applications. Damia is inspired by the Web 2.0 mashup phenomenon. It consists of (1) a browser-based user-interface that allows for the specification of data mashups as data flowfgraphs using a set of operators, (2) a server with an execution engine, as well as (3) APIs for searching, debugging, executing and managing mashups. Damia offers a framework and functionality for dynamic entity resolution, streaming and other higher value features particularly important in the enterprise domain. Damia is currently in perpetual beta in the IBM Intranet.In this demonstration, we showcase the creation and execution of several enterprise data mashups, thereby illustrating the architecture and features of the overall Damia system.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1370–1373},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326018,
author = {Shen, Heng Tao and Zhou, Xiaofang and Huang, Zi and Shao, Jie and Zhou, Xiangmin},
title = {UQLIPS: A Real-Time near-Duplicate Video Clip Detection System},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Near-duplicate video clip (NDVC) detection is an important problem with a wide range of applications such as TV broadcast monitoring, video copyright enforcement, content-based video clustering and annotation, etc. For a large database with tens of thousands of video clips, each with thousands of frames, can NDVC search be performed in real-time? In addition to considering inter-frame similarity (i.e., spatial information), what is the impact of frame sequence similarity (i.e., temporal information) on search speed and accuracy? UQLIPS is a prototype system for online NDVC detection. The core of UQLIPS comprises two novel complementary schemes for detecting NDVCs. Bounded Coordinate System (BCS), a compact representation model ignoring temporal information, globally summarizes each video to a single vector which captures the dominating content and content changing trends of each clip. The other proposal, named FRAme Symbolization (FRAS), maps each clip to a sequence of symbols, and takes temporal order and sequence context information into consideration. Using a large collection of TV commercials, UQLIPS clearly demonstrates that it is feasible to perform real-time NDVC detection with high accuracy.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1374–1377},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326019,
author = {Koch, Christoph and Scherzinger, Stefanie and Schmidt, Michael},
title = {The GCX System: Dynamic Buffer Minimization in Streaming XQuery Evaluation},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {In this demonstration, we present the main-memory based streaming XQuery engine GCX which implements novel buffer management strategies that combine static and dynamic analysis to keep main memory consumption low. Depending on the progress made in query evaluation, memory buffers are dynamically purged and minimized. In this demo, we show the various stages in evaluating a practical fragment of XQuery with GCX. We present the major steps in static analysis and demonstrate the mechanisms of dynamic buffer minimization. We apply our system to XML streams and demonstrate the significant impact of our approach on reducing main memory consumption and running time.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1378–1381},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326020,
author = {Kraft, Tobias},
title = {A Cost-Estimation Component for Statement Sequences},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Query generators producing sequences of SQL statements are embedded in many applications. As the execution time of such sequences is often far from optimal, their optimization is an important issue. Therefore, in [5] we proposed a rule-based optimization approach, which we called CGO (Coarse-Grained Optimization). Our first prototype used a heuristic, priority-based control strategy to choose the rewrite rules that should be applied to a given statement sequence. This worked well but there is still potential for improvements. Thus, in [4] we have introduced an approach to provide cost estimates for statement sequences which is the basis for a cost-based CGO optimizer. It exploits histogram propagation and the optimizer of the underlying database system for this purpose. In this demonstration, we want to showcase the functionality and the effectiveness of our approach. Thereto, we present a prototype of a cost-estimation component for statement sequences which implements this approach. It includes a graphical user interface to explain the histogram-propagation process and to report the results of the cost-estimation process. In the setup for this demonstration, we use a TPC-H benchmark database with an appropriate set of sequences as sample scenario.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1382–1385},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326021,
author = {Gil, Joseph (Yossi) and Lenz, Keren},
title = {Eliminating Impedance Mismatch in C++},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Recently, the C# and the VISUAL BASIC communities were tantalized by the advent of LINQ [18]---the Language INtegrated Query technology from Microsoft. LINQ represents a set of language extensions relying on advanced (some say hard to understand) techniques drawn from functional languages such as type inference, λ-expressions and most importantly, monads. The 3rd edition of C# just as the 9th of VISUAL BASIC allow programmer to directly access relational and XML-based databases from within the programming language. We show that very similar capabilities can be achieved in the C++ programming language without relying on any language extensions, compiler modifications, external processing tools, or any other vendor specific machinery: ARATAT is a C++ template library whose objective is type safe generation of SQL statements for access relational database systems. Learning curve is minimal since ARARAT resembles relational algebra, which is at the core of SQL.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1386–1389},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326023,
author = {Abiteboul, Serge and Dar, Itay and Pop, Radu and Vasile, Gabriel and Vodislav, Dan and Preda, Nicoleta},
title = {Large Scale P2P Distribution of Open-Source Software},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Open-source software communities currently face an increasing complexity in managing and distributing software content among their developers and contributors. This is mainly due to the continuously growing size of the software, of the community, the high frequency of updates, and the heterogeneity of the participants. We propose a large scale P2P distribution system that tackles two main issues in software content management: efficient content dissemination and advanced information system capabilities.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1390–1393},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326024,
author = {Scholl, Tobias and Bauer, Bernhard and Gufler, Benjamin and Kuntschke, Richard and Weber, Daniel and Reiser, Angelika and Kemper, Alfons},
title = {HiSbase: Histogram-Based P2P Main Memory Data Management},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Many e-science communities, e. g., medicine, climatology, and astrophysics, are overwhelmed by the exponentially growing data volumes that need to be accessible by collaborating researchers. Nowadays, new scientific results are often obtained by exploring and cross-correlating data from different distributed sources [3]. However, neither centralized data processing by shipping the data to the processing site on demand nor a centralized data warehouse approach scale sufficiently to handle the huge data volumes and processing demands of future e-science communities and applications. The former suffers from high transmission costs while the latter cannot scale to the large amounts of data in combination with the growing number of queries.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1394–1397},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326025,
author = {Parreira, Josiane Xavier and Michel, Sebastian and Bender, Matthias and Crecelius, Tom and Weikum, Gerhard},
title = {P2P Authority Analysis for Social Communities},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {PageRank-style authority analyses of Web graphs are of great importance for Web mining. Such authority analyses also apply to hot "Web 2.0" applications that exhibit a natural graph structure, such as social networks (e.g., MySpace, Facebook) or tagging communities (e.g., Flickr, Del.icio.us). Finding the most trustworthy or most important authorities in such a community is a pressing need, given the huge scale and also the anonymity of social networks.Computing global authority measures in a Peer-to-Peer (P2P) collaboration of autonomous peers is a hot research topic, in particular because of the incomplete local knowledge of the peers, which typically only know about (arbitrarily overlapping) sub-graphs of the complete graph. We demonstrate a self-organizing P2P collaboration that, based on the local sub-graphs, efficiently computes global authority scores. In hand with the loosely-coupled spirit of a P2P system, the computation is carried out in a completely asynchronous manner without any central knowledge or coordinating instance. We demonstrate the applicability of authority analyses to large-scale distributed systems.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1398–1401},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326026,
author = {Lu, Jing and Ma, Li and Zhang, Lei and Brunner, Jean-S\'{e}bastien and Wang, Chen and Pan, Yue and Yu, Yong},
title = {SOR: A Practical System for Ontology Storage, Reasoning and Search},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Ontology, an explicit specification of shared conceptualization, has been increasingly used to define formal data semantics and improve data reusability and interoperability in enterprise information systems. In this paper, we present and demonstrate SOR (Scalable Ontology Repository), a practical system for ontology storage, reasoning, and search. SOR uses Relational DBMS to store ontologies, performs inference over them, and supports SPARQL language for query. Furthermore, a faceted search with relationship navigation is designed and implemented for ontology search. This demonstration shows how to efficiently solve three key problems in practical ontology management in RDBMS, namely storage, reasoning, and search. Moreover, we show how the SOR system is used for semantic master data management.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1402–1405},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326027,
author = {Tata, Sandeep and Lang, Willis and Patel, Jignesh M.},
title = {Periscope/SQ: Interactive Exploration of Biological Sequence Databases},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Life science laboratories today have to rely on procedural techniques to store and manage large sequence datasets. Procedural techniques are cumbersome to use and are often very inefficient compared to optimized declarative techniques. We have designed and implemented a system called Periscope/SQ that makes it possible to rapidly express complex queries within a declarative framework and take advantage of database-style query optimization. As a result, queries in Periscope/SQ run orders of magnitude faster than typical procedural implementations. We demonstrate the power of Persicope/SQ through an application called Gene-Locator which allows biologists to rapidly explore large genomic sequence databases.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1406–1409},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326028,
author = {Bansal, Nilesh and Koudas, Nick},
title = {BlogScope: A System for Online Analysis of High Volume Text Streams},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {We present BlogScope (www.blogscope.net), a system for online analysis of temporally ordered streaming text, currently applied to the analysis of the Blogosphere. The system currently tracks over ten million blogs and handles hundreds of thousands of updates daily. BlogScope is an information discovery and text analysis system that offers a set of unique features. Such features include, spatio-temporal analysis of blogs, flexible navigation of the Blogosphere through information bursts, keyword correlations and burst synopsis, as well as enhanced ranking functions for improved query answer relevance. We describe the system, its design and the features of the current version of BlogScope.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1410–1413},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326029,
author = {Berberich, Klaus and Bedathur, Srikanta and Neumann, Thomas and Weikum, Gerhard},
title = {FluxCapacitor: Efficient Time-Travel Text Search},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {An increasing number of temporally versioned text collections is available today with Web archives being a prime example. Search on such collections, however, is often not satisfactory and ignores their temporal dimension completely. Time-travel text search solves this problem by evaluating a keyword query on the state of the text collection as of a user-specified time point. This work demonstrates our approach to efficient time-travel text search and its implementation in the FLUXCAPACITOR prototype.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1414–1417},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326030,
author = {Li, Wen-Syan and Gao, Dengfeng and Bhatti, Rafae and Narang, Inderpal and Matsuzawa, Hirofumi and Numao, Masayuki and Ohkawa, Masahiro and Fukuda, Takeshi},
title = {Deadline and QoS Aware Data Warehouse},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {A data warehouse infrastructure needs to support the requirement of (day time) ad hoc query response time and (night time) batch workload completion time. The following tasks need to be finished in a batch window: (1) Apply one day's delta data to the base tables; (2) refresh MQTs (Materialized Query Tables) for ad hoc queries and batch workloads; (3) run batch queries. Tools are available to optimize each step; however, many factors need to be considered for improving the overall performance of a data warehouse (i.e. meeting batch window deadline and ad hoc query response time). We have prototyped a Data Warehouse Operation Advisor to systematically study each component contributing to the batch window problem, and then perform global optimization to achieve desired results!},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1418–1421},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326031,
author = {Antova, Lyublena and Koch, Christoph and Olteanu, Dan},
title = {Query Language Support for Incomplete Information in the MayBMS System},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {MayBMS [4, 1, 3, 2] is a data management system for incomplete information developed at Saarland University. Its main features are a simple and compact representation system for incomplete information and a language called I-SQL with explicit operations for handling uncertainty. MayBMS is currently an extension of PostgreSQL and manages both complete and incomplete data and evaluates I-SQL queries.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1422–1425},
numpages = {4},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326033,
author = {Deshpande, Amol and Ives, Zachary and Raman, Vijayshankar},
title = {Adaptive Query Processing: Why, How, When, What Next?},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Adaptive query processing has been the subject of a great deal of recent work, particularly in emerging data management environments such as data integration and data streams. We provide an overview of the work in this area, identifying its common themes, laying out the space of query plans, and discussing open research problems. We discuss why adaptive query processing is needed, how it is being implemented, where it is most appropriately used, and finally, what next, i.e., open research problems.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1426–1427},
numpages = {2},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326034,
author = {Baumann, Peter},
title = {Raster Databases},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Since the launch of Google Earth at the latest it is clear that online services for multi-Terabyte satellite imagery are becoming integral part of our Internet experience. Actually, 2-D imagery is but the tip of the iceberg - the general concept of multi-dimensional spatio-temporal raster data covers 1-D sensor time series, 2-D imagery, 3-D image time series (x/y/t) and exploration data (x/y/z), 4-D climate models (x/y/z/t), and many more.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1428},
numpages = {1},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326035,
author = {Liu, Ling},
title = {From Data Privacy to Location Privacy: Models and Algorithms},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {This tutorial presents the definition, the models and the techniques of location privacy from the data privacy perspective. By reviewing and revising the state of art research in data privacy area, the presenter describes the essential concepts, the alternative models, and the suite of techniques for providing location privacy in mobile and ubiquitous data management systems. The tutorial consists of two main components. First, we will introduce location privacy threats and give an overview of the state of art research in data privacy and analyze the applicability of the existing data privacy techniques to location privacy problems. Second, we will present the various location privacy models and techniques effective in either the privacy policy based framework or the location anonymization based framework. The discussion will address a number of important issues in both data privacy and location privacy research, including the location utility and location privacy trade-offs, the need for a careful combination of policy-based location privacy mechanisms and location anonymization based privacy schemes, as well as the set of safeguards for secure transmission, use and storage of location information, reducing the risks of unauthorized disclosure of location information.The tutorial is designed to be self-contained, and gives the essential background for anyone interested in learning about the concept and models of location privacy, and the principles and techniques for design and development of a secure and customizable architecture for privacy-preserving mobile data management in mobile and pervasive information systems. This tutorial is accessible to data management administrators, mobile location based service developers, and graduate students and researchers who are interested in data management in mobile information syhhhstems, pervasive computing, and data privacy.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1429–1430},
numpages = {2},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326036,
author = {Sion, Radu},
title = {Secure Data Outsourcing},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The networked and increasingly ubiquitous nature of today's data management services mandates assurances to detect and deter malicious or faulty behavior. This is particularly relevant for outsourced data frameworks in which clients place data management with specialized service providers. Clients are reluctant to place sensitive data under the control of a foreign party without assurances of confidentiality. Additionally, once outsourced, privacy and data access correctness (data integrity and query completeness) become paramount. Today's solutions are fundamentally insecure and vulnerable to illicit behavior, because they do not handle these dimensions.In this tutorial we will explore how to design and build robust, efficient, and scalable data outsourcing mechanisms providing strong security assurances of (1) correctness, (2) confidentiality, and (3) data access privacy.There exists a strong relationship between such assurances; for example, the lack of access pattern privacy usually allows for statistical attacks compromising data confidentiality. Confidentiality can be achieved by data encryption. However, to be practical, outsourced data services should allow expressive client queries (e.g., relational joins with arbitrary predicates) without compromising confidentiality. This is a hard problem because decryption keys cannot be directly provided to potentially untrusted servers. Moreover, if the remote server cannot be fully trusted, protocol correctness become essential. Therefore, solutions that do not address all three dimensions are incomplete and insecure.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1431–1432},
numpages = {2},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326037,
author = {Sion, Radu and Winslett, Marianne},
title = {Regulatory-Compliant Data Management},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Digital societies and markets increasingly mandate consistent procedures for the access, processing and storage of information. In the United States alone, over 10,000 such regulations can be found in financial, life sciences, health - care and government sectors, including the Gramm - Leach - Bliley Act, Health Insurance Portability and Accountability Act, and Sarbanes - Oxley Act. A recurrent theme in these regulations is the need for regulatory - compliant data management as an underpinning to ensure data confidentiality, access integrity and authentication; provide audit trails, guaranteed deletion, and data migration; and deliver Write Once Read Many (WORM) assurances, essential for enforcing long - term data retention and life - cycle policies.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1433–1434},
numpages = {2},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326038,
author = {Deshpande, Amol and Sarawagi, Sunita},
title = {Probabilistic Graphical Models and Their Role in Databases},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Probabilistic graphical models provide a framework for compact representation and efficient reasoning about the joint probability distribution of several interdependent variables. This is a classical topic with roots in statistical physics. In recent years, spurred by several applications in unstructured data integration, sensor networks, image processing, bio-informatics, and code design, the topic has received renewed interest in the machine learning, data mining, and database communities. Techniques from graphical models have also been applied to many topics directly of interest to the database community including information extraction, sensor data analysis, imprecise data representation and querying, selectivity estimation for query optimization, and data privacy. As database research continues to expand beyond the confines of traditional enterprise domains, we expect both the need and applicability of probabilistic graphical models to increase dramatically over the next few years. With this tutorial, we are aiming to provide a foundational overview of probabilistic graphical models to the database community, accompanied by a brief overview of some of the recent research literature on the role of graphical models in databases.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1435–1436},
numpages = {2},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326039,
author = {Amer-Yahia, Sihem and Baeza-Yates, Ricardo and Consens, Mariano P. and Lalmas, Mounia},
title = {XML Retrieval: DB/IR in Theory, Web in Practice},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {The world of data has been developed from two main points of view: the structured relational data model and the unstructured text model. The two distinct cultures of databases and information retrieval now have a natural meeting place in the Web with its semi-structured XML model. Data in Digital Libraries and in Enterprise Environments also shares many of the semi-structured characteristics of web data. As web-style searching becomes an ubiquitous tool, the need for integrating these two viewpoints becomes even more important.In particular, we consider the application of DB and IR research to querying Web data in the context of online communities. With Web 2.0, the question arises: how can search interfaces remain simple when users are allowed to contribute content (Wikipedia), share it (Flickr), and rate it (YouTube)? When they can decide who their friends are (del.icio.us), what they like to see, and how they want it to look like (MySpace)? While we want to keep the user interface simple (keyword search), we would like to study the applicability of querying structure and content to a context where new forms of data-driven dynamic web content (e.g. user feed-back, tags, contributed multimedia) are provided.This tutorial will provide an overview of the different issues and approaches put forward by the IR and DB communities and survey the DB-IR integration efforts as they focus in the problem of retrieval from XML content. In particular, the context of querying content in online communities is an excellent example of such an application. Both earlier proposals as well as recent ones will be discussed. A variety of application scenarios for XML Retrieval will be covered, including examples of current tools and techniques.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1437–1438},
numpages = {2},
keywords = {social tagging, web 2.0, XML retrieval, database and information retrieval techniques, indexing &amp; query processing, online communities, retrieval evaluation, semi-structured data and data models},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326040,
author = {Bernstein, Philip A. and Ho, Howard},
title = {Model Management and Schema Mappings: Theory and Practice},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {We present an overview of a tutorial on model management---an approach to solving data integration problems, such as data warehousing, e-commerce, object-to-relational mapping, schema evolution and enterprise information integration. Model management defines a small set of operations for manipulating schemas and mappings, such as Match, Compose, Inverse, and Merge. The long-term goal is to build generic implementations of the operations that can be applied to a wide variety of data integration problems.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1439–1440},
numpages = {2},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326042,
author = {Manolescu, Ioana and Manegold, Stefan},
title = {Performance Evaluation and Experimental Assessment: Conscience or Curse of Database Research?},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Performance, performance and performance used to be the three things that really mattered in database research. Most of our published works indeed include an experimental evaluation of the proposed techniques. However, such evaluations are sometimes seen as a "must-have" eating up the valuable space where one could describe new ideas. The experimental evaluations end up being short, lacking important information to interpret and/or reproduce the results, and often end without clear conclusion.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1441–1442},
numpages = {2},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.5555/1325851.1326043,
author = {Amer-Yahia, Sihem and Halevy, Alon},
title = {What Does Web 2.0 Have to Do with Databases?},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Web 2.0 is a buzzword we have been hearing for over 2 years. According to Wikipedia, it hints at an improved form of the World Wide Web where technologies such as weblogs, social bookmarking, RSS feeds, photo and video sharing, based on an architecture of participation and democracy that encourages users to add value to the application as they use it. Web 2.0 enables social networking on the Web by allowing users to contribute content, share it, rate it, create a network of friends, and decide what they like to see and how they want it to look like.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {1443},
numpages = {1},
location = {Vienna, Austria},
series = {VLDB '07}
}

