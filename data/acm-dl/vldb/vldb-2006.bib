@inproceedings{10.5555/1182635.1164128,
author = {Jhingran, Anant},
title = {Enterprise Information Mashups: Integrating Information, <i>Simply</i>},
year = {2006},
publisher = {VLDB Endowment},
abstract = {There is a fundamental transformation that is taking place on the web around information composition through mashups. We first describe this transformation and then assert that this will also affect enterprise architectures. Currently the state-of-the-art in enterprises around information composition is federation and other integration technologies. These scale well, and are well worth the upfront investment for enterprise class, long-lived applications. However, there are many information composition tasks that are not currently well served by these architectures. The needs of Situational Applications (i.e. applications that come together for solving some immediate business problems) are one such set of tasks. Augmenting structured data with unstructured information is another such task. Our hypothesis is that a new class of integration technologies will emerge to serve these tasks, and we call it an enterprise information mashup fabric. In the talk, we discuss the information management primitives that are needed for this fabric, the various options that exist for implementation, and pose several, currently unanswered, research questions.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {3–4},
numpages = {2},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164129,
author = {Sikka, Vishal},
title = {Next Generation Data Management in Enterprise Application Platforms},
year = {2006},
publisher = {VLDB Endowment},
abstract = {As a leading provider of applications and application infrastructure software, SAP has been always interested in the entire spectrum of enterprise data management from transactional to analytical, structured and unstructured, as well as high-volatility event data streams. The underlying architecture for enterprise applications has fundamentally changed in the last decade, with the adoption of service-oriented architectures representing the latest shift. However, DBMS architecture has not evolved sufficiently to meet the challenges that these new application characteristics pose. As a result, at SAP we have been rethinking the way enterprise applications manage their data. In this talk, we will present some key aspects of this rethinking. We will start with a description of the shift in application architecture and the challenges that this shift poses on data management. We will then describe the failings of a single overarching DBMS architecture against these needs, and then describe some examples of usage-specific data management in enterprise application platforms. In particular we will focus on our approach to managing analytical, transactional and master data. We will present some results that describe how, with a combination of better utilization of main-memory based data management techniques, addressing the needs of the next generation application infrastructure and advances in the underlying computing and storage infrastructure, we can do significantly more efficient data management.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {5},
numpages = {1},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164130,
author = {Halevy, Alon and Rajaraman, Anand and Ordille, Joann},
title = {Data Integration: The Teenage Years},
year = {2006},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {9–16},
numpages = {8},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164131,
author = {Li, Hua-Gang and Chen, Songting and Tatemura, Junichi and Agrawal, Divyakant and Candan, K. Sel\c{c}uk and Hsiung, Wang-Pin},
title = {Safety Guarantee of Continuous Join Queries over Punctuated Data Streams},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Continuous join queries (CJQ) are needed for correlating data from multiple streams. One fundamental problem for processing such queries is that since the data streams are infinite, this would require the join operator to store infinite states and eventually run out of space. Punctuation semantics has been proposed to specifically address this problem. In particular, punctuations explicitly mark the end of a subset of data and, hence, enable purging of the stored data which will not contribute to any new query results. Given a set of available punctuation schemes, if one can identify that a CJQ still requires unbounded storage, then this query can be flagged as unsafe and can be prevented from running. Unfortunately, while punctuation semantics is clearly useful, the mechanisms to identify if and how a particular CJQ could benefit from a given set of punctuation schemes are not yet known. In this paper, we provide sufficient and necessary conditions for checking whether a CJQ can be safely executed under a given set of punctuation schemes or not. In particular, we introduce a novel punctuation graph to aid the analysis of the safety for a given query. We show that the safety checking problem can be done in polynomial time based on this punctuation graph construct. In addition, various issues and challenges related to the safety checking of CJQs are highlighted.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {19–30},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164132,
author = {Agarwal, Pankaj K. and Xie, Junyi and Yang, Jun and Yu, Hai},
title = {Scalable Continuous Query Processing by Tracking Hotspots},
year = {2006},
publisher = {VLDB Endowment},
abstract = {This paper considers the problem of scalably processing a large number of continuous queries. We propose a flexible framework with novel data structures and algorithms for group-processing and indexing continuous queries by exploiting potential overlaps in query predicates. Our approach partitions the collection of continuous queries into groups based on the clustering patterns of the query ranges, and then applies specialized processing strategies to those heavily-clustered groups (or hotspots). To maintain the partition dynamically, we present efficient algorithms that maintain a nearly optimal partition in nearly amortized logarithmic time. We show how to use the hotspots to scalably process large numbers of continuous select-join and band-join queries, which are much more challenging than simple range selection queries. Experiments demonstrate that this approach can improve the processing throughput by orders of magnitude. As another application of hotspots, we show how to use them to build a high-quality histogram for intervals in linear time.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {31–42},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164133,
author = {Mouratidis, Kyriakos and Yiu, Man Lung and Papadias, Dimitris and Mamoulis, Nikos},
title = {Continuous Nearest Neighbor Monitoring in Road Networks},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Recent research has focused on continuous monitoring of nearest neighbors (NN) in highly dynamic scenarios, where the queries and the data objects move frequently and arbitrarily. All existing methods, however, assume the Euclidean distance metric. In this paper we study k-NN monitoring in road networks, where the distance between a query and a data object is determined by the length of the shortest path connecting them. We propose two methods that can handle arbitrary object and query moving patterns, as well as fluctuations of edge weights. The first one maintains the query results by processing only updates that may invalidate the current NN sets. The second method follows the shared execution paradigm to reduce the processing time. In particular, it groups together the queries that fall in the path between two consecutive intersections in the network, and produces their results by monitoring the NN sets of these intersections. We experimentally verify the applicability of the proposed techniques to continuous monitoring of large data and query sets.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {43–54},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164134,
author = {Bernstein, Philip A. and Green, Todd J. and Melnik, Sergey and Nash, Alan},
title = {Implementing Mapping Composition},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Mapping composition is a fundamental operation in metadata driven applications. Given a mapping over schemas σ1 and σ2 and a mapping over schemas σ2 and σ3, the composition problem is to compute an equivalent mapping over σ1 and σ3. We describe a new composition algorithm that targets practical applications. It incorporates view unfolding. It eliminates as many σ2 symbols as possible, even if not all can be eliminated. It covers constraints expressed using arbitrary monotone relational operators and, to a lesser extent, non-monotone operators. And it introduces the new technique of left composition. We describe our implementation, explain how to extend it to support user-defined operators, and present experimental results which validate its effectiveness.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {55–66},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164135,
author = {Fuxman, Ariel and Hernandez, Mauricio A. and Ho, Howard and Miller, Renee J. and Papotti, Paolo and Popa, Lucian},
title = {Nested Mappings: Schema Mapping Reloaded},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Many problems in information integration rely on specifications, called schema mappings, that model the relationships between schemas. Schema mappings for both relational and nested data are well-known. In this work, we present a new formalism for schema mapping that extends these existing formalisms in two significant ways. First, our nested mappings allow for nesting and correlation of mappings. This results in a natural programming paradigm that often yields more accurate specifications. In particular, we show that nested mappings can naturally preserve correlations among data that existing mapping formalisms cannot. We also show that using nested mappings for purposes of exchanging data from a source to a target will result in less redundancy in the target data. The second extension to the mapping formalism is the ability to express, in a declarative way, grouping and data merging semantics. This semantics can be easily changed and customized to the integration task at hand. We present a new algorithm for the automatic generation of nested mappings from schema matchings (that is, simple element-to-element correspondences between schemas). We have implemented this algorithm, along with algorithms for the generation of transformation queries (e.g., XQuery) based on the nested mapping specification. We show that the generation algorithms scale well to large, highly nested schemas. We also show that using nested mappings in data exchange can drastically reduce the execution cost of producing a target instance, particularly over large data sources, and can also dramatically improve the quality of the generated data.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {67–78},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164136,
author = {Chiticariu, Laura and Tan, Wang-Chiew},
title = {Debugging Schema Mappings with Routes},
year = {2006},
publisher = {VLDB Endowment},
abstract = {A schema mapping is a high-level declarative specification of the relationship between two schemas; it specifies how data structured under one schema, called the source schema, is to be converted into data structured under a possibly different schema, called the target schema. Schema mappings are fundamental components for both data exchange and data integration. To date, a language for specifying (or programming) schema mappings exists. However, developmental support for programming schema mappings is still lacking. In particular, a tool for debugging schema mappings has not yet been developed. In this paper, we propose to build a debugger for understanding and exploring schema mappings. We present a primary feature of our debugger, called routes, that describes the relationship between source and target data with the schema mapping. We present two algorithms for computing all routes or one route for selected target data. Both algorithms execute in polynomial time in the size of the input. In computing all routes, our algorithm produces a concise representation that factors common steps in the routes. Furthermore, every minimal route for the selected data can, essentially, be found in this representation. Our second algorithm is able to produce one route fast, if there is one, and alternative routes as needed. We demonstrate the feasibility of our route algorithms through a set of experimental results on both synthetic and real datasets.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {79–90},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164140,
author = {Wang, Hui and Lakshmanan, Laks V. S.},
title = {Efficient Secure Query Evaluation over Encrypted XML Databases},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Motivated by the "database-as-service" paradigm wherein data owned by a client is hosted on a third-party server, there is significant interest in secure query evaluation over encrypted databases. We consider this problem for XML databases. We consider an attack model where the attacker may possess exact knowledge about the domain values and their occurrence frequencies, and we wish to protect sensitive structural information as well as value associations. We capture such security requirements using a novel notion of security constraints. For security reasons, sensitive parts of the hosted database are encrypted. There is a tension between data security and efficiency of query evaluation for different granularities of encryption. We show that finding an optimal, secure encryption scheme is NP-hard. For speeding up query processing, we propose to keep metadata, consisting of structure and value indices, on the server. We want to prevent the server, or an attacker who gains access to the server, from learning sensitive information in the database. We propose security properties for such a hosted XML database system to satisfy and prove that our proposal satisfies these properties. Intuitively, this means the attacker cannot improve his prior belief probability distribution about which candidate database led to the given encrypted database, by looking at the encrypted database as well as the metadata. We also prove that by observing a series of queries and their answers, the attacker cannot improve his prior belief probability distribution over which sensitive queries (structural or value associations) hold in the hosted database. Finally, we demonstrate with a detailed set of experiments that our techniques enable efficient query processing while satisfying the security properties defined in the paper.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {127–138},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164141,
author = {Xiao, Xiaokui and Tao, Yufei},
title = {Anatomy: Simple and Effective Privacy Preservation},
year = {2006},
publisher = {VLDB Endowment},
abstract = {This paper presents a novel technique, anatomy, for publishing sensitive data. Anatomy releases all the quasi-identifier and sensitive values directly in two separate tables. Combined with a grouping mechanism, this approach protects privacy, and captures a large amount of correlation in the microdata. We develop a linear-time algorithm for computing anatomized tables that obey the l-diversity privacy requirement, and minimize the error of reconstructing the microdata. Extensive experiments confirm that our technique allows significantly more effective data analysis than the conventional publication method based on generalization. Specifically, anatomy permits aggregate reasoning with average error below 10%, which is lower than the error obtained from a generalized table by orders of magnitude.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {139–150},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164142,
author = {Nabar, Shubha U. and Marthi, Bhaskara and Kenthapadi, Krishnaram and Mishra, Nina and Motwani, Rajeev},
title = {Towards Robustness in Query Auditing},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We consider the online query auditing problem for statistical databases. Given a stream of aggregate queries posed over sensitive data, when should queries be denied in order to protect the privacy of individuals? We construct efficient auditors for max queries and bags of max and min queries in both the partial and full disclosure settings. Our algorithm for the partial disclosure setting involves a novel application of probabilistic inference techniques that may be of independent interest. We also study for the first time, a particular dimension of the utility of an auditing scheme and obtain initial results for the utility of sum auditing when guarding against full disclosure.The result is positive for large databases, indicating that answers to queries will not be riddled with denials.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {151–162},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164137,
author = {Kanne, Carl-Christian and Moerkotte, Guido},
title = {A Linear Time Algorithm for Optimal Tree Sibling Partitioning and Approximation Algorithms in Natix},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Document insertion into a native XML Data Store (XDS) requires to partition the document tree into a number of storage units with limited capacity, such as records on disk pages. As intra partition navigation is much faster than navigation between partitions, minimizing the number of partitions has a beneficial effect on query performance.We present a linear time algorithm to optimally partition an ordered, labeled, weighted tree such that each partition does not exceed a fixed weight limit. Whereas traditionally tree partitioning algorithms only allow child nodes to share a partition with their parent node (i.e. a partition corresponds to a subtree), our algorithm also considers partitions containing several subtrees as long as their roots are adjacent siblings. We call this sibling partitioning.Based on our study of the optimal algorithm, we further introduce two novel, near-optimal heuristics. They are easier to implement, do not need to hold the whole document instance in memory, and require much less runtime than the optimal algorithm.Finally, we provide an experimental study comparing our novel and existing algorithms. One important finding is that compared to partitioning that exclusively considers parent-child partitions, including sibling partitioning as well can decrease the total number of partitions by more than 90%, and improve query performance by more than a factor of two.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {91–102},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164138,
author = {Yu, Cong and Jagadish, H. V.},
title = {Efficient Discovery of XML Data Redundancies},
year = {2006},
publisher = {VLDB Endowment},
abstract = {As XML becomes widely used, dealing with redundancies in XML data has become an increasingly important issue. Redundantly stored information can lead not just to a higher data storage cost, but also to increased costs for data transfer and data manipulation. Furthermore, such data redundancies can lead to potential update anomalies, rendering the database inconsistent.One way to avoid data redundancies is to employ good schema design based on known functional dependencies. In fact, several recent studies have focused on defining the notion of XML Functional Dependencies (XML FDs) to capture XML data redundancies. We observe further that XML databases are often "casually designed" and XML FDs may not be determined in advance. Under such circumstances, discovering XML data redundancies (in terms of FDs) from the data itself becomes necessary and is an integral part of the schema refinement process.In this paper, we present the design and implementation of the first system, DiscoverXFD, for effcient discovery of XML data redundancies. It employs a novel XML data structure and introduces a new class of partition based algorithms. DiscoverXFD can not only be used for the previous definitions of XML functional dependencies, but also for a more comprehensive notion we develop in this paper, capable of detecting redundancies involving set elements while maintaining clear semantics. Experimental evaluations using real life and benchmark datasets demonstrate that our system is practical and scales well with increasing data size.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {103–114},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164139,
author = {Bex, Geert Jan and Neven, Frank and Schwentick, Thomas and Tuyls, Karl},
title = {Inference of Concise DTDs from XML Data},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We consider the problem to infer a concise Document Type Definition (DTD) for a given set of XML-documents, a problem which basically reduces to learning of concise regular expressions from positive example strings. We identify two such classes: single occurrence regular expressions (SOREs) and chain regular expressions (CHAREs). Both classes capture the far majority of the regular expressions occurring in practical DTDs and are succinct by definition. We present the algorithm iDTD (infer DTD) that learns SOREs from strings by first inferring an automaton by known techniques and then translating that automaton to a corresponding SORE, possibly by repairing the automaton when no equivalent SORE can be found. In the process, we introduce a novel automaton to regular expression rewrite technique which is of independent interest. We show that iDTD outperforms existing systems in accuracy, conciseness and speed. In a scenario where only a very small amount of XML data is available, for instance when generated by Web service requests or by answers to queries, iDTD produces regular expressions which are too specific. Therefore, we introduce a novel learning algorithm CRX that directly infers CHAREs (which form a subclass of SOREs) without going through an automaton representation. We show that CRX performs very well within its target class on very small data sets. Finally, we discuss incremental computation, noise, numerical predicates, and the generation of XML Schemas.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {115–126},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164164,
author = {Parreira, Josiane Xavier and Donato, Debora and Michel, Sebastian and Weikum, Gerhard},
title = {Efficient and Decentralized PageRank Approximation in a Peer-to-Peer Web Search Network},
year = {2006},
publisher = {VLDB Endowment},
abstract = {PageRank-style (PR) link analyses are a cornerstone of Web search engines and Web mining, but they are computationally expensive. Recently, various techniques have been proposed for speeding up these analyses by distributing the link graph among multiple sites. However, none of these advanced methods is suitable for a fully decentralized PR computation in a peer-to-peer (P2P) network with autonomous peers, where each peer can independently crawl Web fragments according to the user's thematic interests. In such a setting the graph fragments that different peers have locally available or know about may arbitrarily overlap among peers, creating additional complexity for the PR computation.This paper presents the JXP algorithm for dynamically and collaboratively computing PR scores of Web pages that are arbitrarily distributed in a P2P network. The algorithm runs at every peer, and it works by combining locally computed PR scores with random meetings among the peers in the network. It is scalable as the number of peers on the network grows, and experiments as well as theoretical arguments show that JXP scores converge to the true PR scores that one would obtain by a centralized computation.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {415–426},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164165,
author = {Yin, Xiaoxin and Han, Jiawei and Yu, Philip S.},
title = {LinkClus: Efficient Clustering via Heterogeneous Semantic Links},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Data objects in a relational database are cross-linked with each other via multi-typed links. Links contain rich semantic information that may indicate important relationships among objects. Most current clustering methods rely only on the properties that belong to the objects per se. However, the similarities between objects are often indicated by the links, and desirable clusters cannot be generated using only the properties of objects.In this paper we explore linkage-based clustering, in which the similarity between two objects is measured based on the similarities between the objects linked with them. In comparison with a previous study (SimRank) that computes links recursively on all pairs of objects, we take advantage of the power law distribution of links, and develop a hierarchical structure called SimTree to represent similarities in multi-granularity manner. This method avoids the high cost of computing and storing pairwise similarities but still thoroughly explore relationships among objects. An efficient algorithm is proposed to compute similarities between objects by avoiding pairwise similarity computations through merging computations that go through the same branches in the SimTree. Experiments show the proposed approach achieves high efficiency, scalability, and accuracy in clustering multi-typed linked objects.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {427–438},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164166,
author = {Gyongyi, Zoltan and Berkhin, Pavel and Garcia-Molina, Hector and Pedersen, Jan},
title = {Link Spam Detection Based on Mass Estimation},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Link spamming intends to mislead search engines and trigger an artificially high link-based ranking of specific target web pages. This paper introduces the concept of spam mass, a measure of the impact of link spamming on a page's ranking. We discuss how to estimate spam mass and how the estimates can help identifying pages that benefit significantly from link spamming. In our experiments on the host-level Yahoo! web graph we use spam mass estimates to successfully identify tens of thousands of instances of heavyweight link spamming.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {439–450},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164170,
author = {Harizopoulos, Stavros and Liang, Velen and Abadi, Daniel J. and Madden, Samuel},
title = {Performance Tradeoffs in Read-Optimized Databases},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Database systems have traditionally optimized performance for write-intensive workloads. Recently, there has been renewed interest in architectures that optimize read performance by using column-oriented data representation and light-weight compression. This previous work has shown that under certain broad classes of workloads, column-based systems can outperform row-based systems. Previous work, however, has not characterized the precise conditions under which a particular query workload can be expected to perform better on a column-oriented database.In this paper we first identify the distinctive components of a read-optimized DBMS and describe our implementation of a high-performance query engine that can operate on both row and column-oriented data. We then use our prototype to perform an in-depth analysis of the tradeoffs between column and row-oriented architectures. We explore these tradeoffs in terms of disk bandwidth, CPU cache latency, and CPU cycles. We show that for most database workloads, a carefully designed column system can outperform a carefully designed row system, sometimes by an order of magnitude. We also present an analytical model to predict whether a given workload on a particular hardware configuration is likely to perform better on a row or column-based system.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {487–498},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164171,
author = {Bruno, Nicolas and Chaudhuri, Surajit},
title = {To Tune or Not to Tune? A Lightweight Physical Design Alerter},
year = {2006},
publisher = {VLDB Endowment},
abstract = {In recent years there has been considerable research on automating the physical design in database systems. Current techniques provide good recommendations, but are resource intensive. This makes DBAs somewhat conservative when deciding to launch a resource-intensive tuning session. In this paper, we introduce an alerter that helps determining when a physical design tool should be invoked. The alerter is a lightweight mechanism that provides guaranteed lower (and upper bounds) on the improvement that a DBA could expect by invoking a comprehensive physical design tool. Moreover, it produces an accompanying recommendation that serves as a "proof" for the lower bound. We show experimentally that the alerter handles large workloads with little overhead, and help judiciously decide on launching subsequent tuning sessions.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {499–510},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164172,
author = {Sharaf, Mohamed A. and Chrysanthis, Panos K. and Labrinidis, Alexandros and Pruhs, Kirk},
title = {Efficient Scheduling of Heterogeneous Continuous Queries},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Data Stream Management Systems (DSMS) typically host multiple Continuous Queries (CQ) that process streams of data. In this paper, we examine the problem of how to schedule CQs in a DSMS to optimize for average QoS. We show that unlike standard on-line systems, scheduling policies in DSMSs that optimize for average response time will be different than policies that optimize for average slowdown which is more appropriate metric to use in the presence of a heterogeneous workload. We also propose a hybrid scheduling policy based on slowdown that strikes a fine balance between performance and fairness. We further discuss how our policies can be efficiently implemented and extended to exploit sharing in optimized multi-query plans and multi-stream CQs. Finally, we experimentally show using real data that our policies outperform currently used ones.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {511–522},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164143,
author = {Jeffery, Shawn R. and Garofalakis, Minos and Franklin, Michael J.},
title = {Adaptive Cleaning for RFID Data Streams},
year = {2006},
publisher = {VLDB Endowment},
abstract = {To compensate for the inherent unreliability of RFID data streams, most RFID middleware systems employ a "smoothing filter", a sliding-window aggregate that interpolates for lost readings. In this paper, we propose SMURF, the first declarative, adaptive smoothing filter for RFID data cleaning. SMURF models the unreliability of RFID readings by viewing RFID streams as a statistical sample of tags in the physical world, and exploits techniques grounded in sampling theory to drive its cleaning processes. Through the use of tools such as binomial sampling and π-estimators, SMURF continuously adapts the smoothing window size in a principled manner to provide accurate RFID data to applications.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {163–174},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164144,
author = {Rao, Jun and Doraiswamy, Sangeeta and Thakkar, Hetal and Colby, Latha S.},
title = {A Deferred Cleansing Method for RFID Data Analytics},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Radio Frequency Identification is gaining broader adoption in many areas. One of the challenges in implementing an RFID-based system is dealing with anomalies in RFID reads. A small number of anomalies can translate into large errors in analytical results. Conventional "eager" approaches cleanse all data upfront and then apply queries on cleaned data. However, this approach is not feasible when several applications define anomalies and corrections on the same data set differently and not all anomalies can be defined beforehand. This necessitates anomaly handling at query time. We introduce a deferred approach for detecting and correcting RFID data anomalies. Each application specifies the detection and the correction of relevant anomalies using declarative sequence-based rules. An application query is then automatically rewritten based on the cleansing rules that the application has specified, to provide answers over cleaned data. We show that a naive approach to deferred cleansing that applies rules without leveraging query information can be prohibitive. We develop two novel rewrite methods, both of which reduce the amount of data to be cleaned, by exploiting predicates in application queries while guaranteeing correct answers. We leverage standardized SQL/OLAP functionality to implement rules specified in a declarative sequence-based language. This allows efficient evaluation of cleansing rules using existing query processing capabilities of a DBMS. Our experimental results show that deferred cleansing is affordable for typical analytic queries over RFID data.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {175–186},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164145,
author = {Subramaniam, S. and Palpanas, T. and Papadopoulos, D. and Kalogeraki, V. and Gunopulos, D.},
title = {Online Outlier Detection in Sensor Data Using Non-Parametric Models},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Sensor networks have recently found many popular applications in a number of different settings. Sensors at different locations can generate streaming data, which can be analyzed in real-time to identify events of interest. In this paper, we propose a framework that computes in a distributed fashion an approximation of multi-dimensional data distributions in order to enable complex applications in resource-constrained sensor networks.We motivate our technique in the context of the problem of outlier detection. We demonstrate how our framework can be extended in order to identify either distance- or density-based outliers in a single pass over the data, and with limited memory requirements. Experiments with synthetic and real data show that our method is efficient and accurate, and compares favorably to other proposed techniques. We also demonstrate the applicability of our technique to other related problems in sensor networks.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {187–198},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164176,
author = {Candan, K. Sel\c{c}uk and Hsiung, Wang-Pin and Chen, Songting and Tatemura, Junichi and Agrawal, Divyakant},
title = {AFilter: Adaptable XML Filtering with Prefix-Caching Suffix-Clustering},
year = {2006},
publisher = {VLDB Endowment},
abstract = {XML message filtering problem involves searching for instances of a given, potentially large, set of patterns in a continuous stream of XML messages. Since the messages arrive continuously, it is essential that the filtering rate matches the data arrival rate. Therefore, the given set of filter patterns needs to be indexed appropriately to enable real-time processing of the streaming XML data. In this paper, we propose AFilter, an adaptable, and thus scalable, path expression filtering approach. AFilter has a base memory requirement linear in filter expression and data size. Furthermore, when additional memory is available, AFilter can exploit prefix commonalities in the set of filter expressions using a loosely-coupled prefix caching mechanism as opposed to tightly-coupled active state representation of alternative approaches. Unlike existing systems, AFilter can also exploit suffix-commonalities across filter expressions, while simultaneously leveraging the prefix-commonalities through the cache. Finally, AFilter uses a triggering mechanism to prevent excessive consumption of resources by delaying processing until a trigger condition is observed. Experiment results show that AFilter provides significantly better scalability and runtime performance when compared to state of the art filtering systems.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {559–570},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164177,
author = {Lakshmanan, Laks V. S. and Wang, Hui and Zhao, Zheng},
title = {Answering Tree Pattern Queries Using Views},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We study the query answering using views (QAV) problem for tree pattern queries. Given a query and a view, the QAV problem is traditionally formulated in two ways: (i) find an equivalent rewriting of the query using only the view, or (ii) find a maximal contained rewriting using only the view. The former is appropriate for classical query optimization and was recently studied by Xu and Ozsoyoglu for tree pattern queries (TP). However, for information integration, we cannot rely on equivalent rewriting and must instead use maximal contained rewriting as shown by Halevy. Motivated by this, we study maximal contained rewriting for TP, a core subset of XPath, both in the absence and presence of a schema. In the absence of a schema, we show there are queries whose maximal contained rewriting (MCR) can only be expressed as the union of exponentially many TPs. We characterize the existence of a maximal contained rewriting and give a polynomial time algorithm for testing the existence of an MCR. We also give an algorithm for generating the MCR when one exists. We then consider QAV in the presence of a schema. We characterize the existence of a maximal contained rewriting when the schema contains no recursion or union types, and show that it consists of at most one TP. We give an efficient polynomial time algorithm for generating the maximal contained rewriting whenever it exists. Finally, we discuss QAV in the presence of recursive schemas.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {571–582},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164178,
author = {Sawires, Arsany and Tatemura, Junichi and Po, Oliver and Agrawal, Divyakant and El Abbadi, Amr and Candan, K. Sel\c{c}uk},
title = {Maintaining XPath Views in Loosely Coupled Systems},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We address the problem of maintaining materialized XPath views in environments where the view maintenance system and the base data system are loosely-coupled. We show that the recently proposed XPath view maintenance techniques require tight coupling, and thus are not practical for loosely-coupled systems. Our solution adapts to loose-coupling by using information that is fully available through standard XPath interfaces. This information consists of the view definition, the update statement, and the current materialized view result. Under this model, incremental maintenance is not always possible; thus, maintaining the consistency of the views requires frequent view recomputations. Our goal is to reduce the frequency of view recomputation by detecting cases where a base update is irrelevant to a view, and cases where a view is self maintainable given a base update. We develop an approach that reduces the irrelevance and self maintainability tests, respectively, to checking the intersection and containment of XPath expressions. We present experimental results showing the effectiveness of the proposed approach in reducing view recomputations.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {583–594},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164179,
author = {Gemulla, Rainer and Lehner, Wolfgang and Haas, Peter J.},
title = {A Dip in the Reservoir: Maintaining Sample Synopses of Evolving Datasets},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Perhaps the most flexible synopsis of a database is a random sample of the data; such samples are widely used to speed up processing of analytic queries and data-mining tasks, enhance query optimization, and facilitate information integration. In this paper, we study methods for incrementally maintaining a uniform random sample of the items in a dataset in the presence of an arbitrary sequence of insertions and deletions. For "stable" datasets whose size remains roughly constant over time, we provide a novel sampling scheme, called "random pairing" (RP) which maintains a bounded-size uniform sample by using newly inserted data items to compensate for previous deletions. The RP algorithm is the first extension of the almost 40-year-old reservoir sampling algorithm to handle deletions. Experiments show that, when dataset-size fluctuations over time are not too extreme, RP is the algorithm of choice with respect to speed and sample-size stability. For "growing" datasets, we consider algorithms for periodically "resizing" a bounded-size random sample upwards. We prove that any such algorithm cannot avoid accessing the base data, and provide a novel resizing algorithm that minimizes the time needed to increase the sample size.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {595–606},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164180,
author = {Aggarwal, Charu C.},
title = {On Biased Reservoir Sampling in the Presence of Stream Evolution},
year = {2006},
publisher = {VLDB Endowment},
abstract = {The method of reservoir based sampling is often used to pick an unbiased sample from a data stream. A large portion of the unbiased sample may become less relevant over time because of evolution. An analytical or mining task (eg. query estimation) which is specific to only the sample points from a recent time-horizon may provide a very inaccurate result. This is because the size of the relevant sample reduces with the horizon itself. On the other hand, this is precisely the most important case for data stream algorithms, since recent history is frequently analyzed. In such cases, we show that an effective solution is to bias the sample with the use of temporal bias functions. The maintenance of such a sample is non-trivial, since it needs to be dynamically maintained, without knowing the total number of points in advance. We prove some interesting theoretical properties of a large class of memory-less bias functions, which allow for an efficient implementation of the sampling algorithm. We also show that the inclusion of bias in the sampling process introduces a maximum requirement on the reservoir size. This is a nice property since it shows that it may often be possible to maintain the maximum relevant sample with limited storage requirements. We not only illustrate the advantages of the method for the problem of query estimation, but also show that the approach has applicability to broader data mining problems such as evolution analysis and classification.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {607–618},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164181,
author = {Wang, Song and Rundensteiner, Elke and Ganguly, Samrat and Bhatnagar, Sudeept},
title = {State-Slice: New Paradigm of Multi-Query Optimization of Window-Based Stream Queries},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Modern stream applications such as sensor monitoring systems and publish/subscription services necessitate the handling of large numbers of continuous queries specified over high volume data streams. Efficient sharing of computations among multiple continuous queries, especially for the memory- and CPU-intensive window-based operations, is critical. A novel challenge in this scenario is to allow resource sharing among similar queries, even if they employ windows of different lengths. This paper first reviews the existing sharing methods in the literature, and then illustrates the significant performance shortcomings of these methods.This paper then presents a novel paradigm for the sharing of window join queries. Namely we slice window states of a join operator into fine-grained window slices and form a chain of sliced window joins. By using an elaborate pipelining methodology, the number of joins after state slicing is reduced from quadratic to linear. This novel sharing paradigm enables us to push selections down into the chain and flexibly select subsequences of such sliced window joins for computation sharing among queries with different window sizes. Based on the state-slice sharing paradigm, two algorithms are proposed for the chain buildup. One minimizes the memory consumption while the other minimizes the CPU usage. The algorithms are proven to find the optimal chain with respect to memory or CPU usage for a given query workload. We have implemented the slice-share paradigm within the data stream management system CAPE. The experimental results show that our strategy provides the best performance over a diverse range of workload settings among all alternate solutions in the literature.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {619–630},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164146,
author = {Koudas, Nick and Li, Chen and Tung, Anthony K. H. and Vernica, Rares},
title = {Relaxing Join and Selection Queries},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Database users can be frustrated by having an empty answer to a query. In this paper, we propose a framework to systematically relax queries involving joins and selections. When considering relaxing a query condition, intuitively one seeks the 'minimal' amount of relaxation that yields an answer. We first characterize the types of answers that we return to relaxed queries. We then propose a lattice based framework in order to aid query relaxation. Nodes in the lattice correspond to different ways to relax queries. We characterize the properties of relaxation at each node and present algorithms to compute the corresponding answer. We then discuss how to traverse this lattice in a way that a non-empty query answer is obtained with the minimum amount of query condition relaxation. We implemented this framework and we present our results of a thorough performance evaluation using real and synthetic data. Our results indicate the practical utility of our framework.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {199–210},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164147,
author = {Buneman, Peter and Cong, Gao and Fan, Wenfei and Kementsietsidis, Anastasios},
title = {Using Partial Evaluation in Distributed Query Evaluation},
year = {2006},
publisher = {VLDB Endowment},
abstract = {A basic idea in parallel query processing is that one is prepared to do more computation than strictly necessary at individual sites in order to reduce the elapsed time, the network traffic, or both in the evaluation of the query. We develop this idea for the evaluation of boolean XPath queries over a tree that is fragmented, both horizontally and vertically over a number of sites. The key idea is to send the whole query to each site which partially evaluates, in parallel, the query and sends the results as compact boolean functions to a coordinator which combines these to obtain the result. This approach has several advantages. First, each site is visited only once, even if several fragments of the tree are stored at that site. Second, no prior constraints on how the tree is decomposed are needed, nor is any structural information about the tree required, such as a DTD. Third, there is a satisfactory bound on the total computation performed on all sites and on the total network traffic. We also develop a simple incremental maintenance algorithm that requires communication only with the sites at which changes have taken place; moreover the network traffic depends neither on the data nor on the update. These results, we believe, illustrate the usefulness and potential of partial evaluation in distributed systems as well as centralized xml stores for evaluating XPath queries and beyond.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {211–222},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164148,
author = {Huang, Jiansheng and Naughton, Jeffrey F. and Livny, Miron},
title = {TRAC: Toward Recency and Consistency Reporting in a Database with Distributed Data Sources},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Distributed computing environments, including workflows in computational grids, present challenges for monitoring, as the state of the system may be captured only in logs distributed throughout the system. One approach to monitoring such systems is to "sniff" these distributed logs and to store their transformed content in a DBMS. This centralizes the state and exposes it for querying; unfortunately, it also creates uncertainty with respect to the recency and consistency of the data. Previous related work has focused on allowing queries to express currency and consistency constraints, which are then enforced by "pulling" data from the distributed sources on demand, or by requiring synchronous updates of a centralized data store. In some instances this is impossible due to legacy system issues or inefficient as the system scales to large numbers of processors. Accordingly, we propose that instead of enforcing consistency and recency, such monitoring systems should report these properties along with query results, with the hope that this will allow the data to be appropriately interpreted. We present techniques for reporting consistency and recency for queries and evaluate them with respect to efficiency and precision. Finally, we describe our prototype implementation and present experimental results of our techniques.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {223–234},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164182,
author = {Tung, Anthony K. H. and Zhang, Rui and Koudas, Nick and Ooi, Beng Chin},
title = {Similarity Search: A Matching Based Approach},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Similarity search is a crucial task in multimedia retrieval and data mining. Most existing work has modelled this problem as the nearest neighbor (NN) problem, which considers the distance between the query object and the data objects over a fixed set of features. Such an approach has two drawbacks: 1) it leaves many partial similarities uncovered; 2) the distance is often affected by a few dimensions with high dissimilarity. To overcome these drawbacks, we propose the k-n-match problem in this paper.The k-n-match problem models similarity search as matching between the query object and the data objects in n dimensions, where n is a given integer smaller than dimensionality d and these n dimensions are determined dynamically to make the query object and the data objects returned in the answer set match best. The k-n-match query is expected to be superior to the kNN query in discovering partial similarities, however, it may not be as good in identifying full similarity since a single value of n may only correspond to a particular aspect of an object instead of the entirety. To address this problem, we further introduce the frequent k-n-match problem, which finds a set of objects that appears in the k-n-match answers most frequently for a range of n values. Moreover, we propose search algorithms for both problems. We prove that our proposed algorithm is optimal in terms of the number of individual attributes retrieved, which is especially useful for information retrieval from multiple systems. We can also apply the proposed algorithmic strategy to achieve a disk based algorithm for the (frequent) k-n-match query. By a thorough experimental study using both real and synthetic data sets, we show that: 1) the k-n-match query yields better result than the kNN query in identifying similar objects by partial similarities; 2) our proposed method (for processing the frequent k-n-match query) outperforms existing techniques for similarity search in terms of both effectiveness and efficiency.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {631–642},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164183,
author = {Zhang, Donghui and Du, Yang and Xia, Tian and Tao, Yufei},
title = {Progressive Computation of the Min-Dist Optimal-Location Query},
year = {2006},
publisher = {VLDB Endowment},
abstract = {This paper proposes and solves the min-dist optimal-location query in spatial databases. Given a set S of sites, a set O of weighted objects, and a spatial region Q, the min-dist optimal-location query returns a location in Q which, if a new site is built there, minimizes the average distance from each object to its closest site. This query can help a franchise (e.g. McDonald's) decide where to put a new store in order to maximize the benefit to its customers. To solve this problem is challenging, for there are theoretically infinite number of locations in Q, all of which could be candidates. This paper first provides a theorem that limits the number of candidate locations without losing the power to find exact answers. Then it provides a progressive algorithm that quickly suggests a location, tells the maximum error it may have, and keeps refining the result. When the algorithm finishes, the exact answer can be found. The intermediate result of early runs can be used to prune the search space for later runs. Crucial to the pruning technique are novel lower-bound estimators. The proposed algorithm, the effect of several optimizations, and the progressiveness are experimentally evaluated.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {643–654},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164184,
author = {Chen, Bee-Chung and Ramakrishnan, Raghu and Shavlik, Jude W. and Tamma, Pradeep},
title = {Bellwether Analysis: Predicting Global Aggregates from Local Regions},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Massive datasets are becoming commonplace in a wide range of domains, and mining them is recognized as a challenging problem with great potential value. Motivated by this challenge, much effort has been concentrated on developing scalable versions of machine learning algorithms. An often overlooked issue is that large datasets are rarely labeled with the outputs that we wish to learn to predict, due to the human labor required. We make the key observation that analysts can often use queries to define labels for cases, which leads to the problem of learning to predict such query-produced labels. Of course, if a dataset is available in its entirety, we can simply run the query again to compute labels. The interesting scenarios are those where, after the predictive model is trained, new data is gathered at significant incremental cost and, perhaps, over time. The challenge is to accurately predict the query-labels for the projected completion of new datasets, based only on certain cost-effective subsets, which we call bellwethers.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {655–666},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164149,
author = {Xin, Dong and Chen, Chen and Han, Jiawei},
title = {Towards Robust Indexing for Ranked Queries},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Top-k query asks for k tuples ordered according to a specific ranking function that combines the values from multiple participating attributes. The combined score function is usually linear. To efficiently answer top-k queries, preprocessing and indexing the data have been used to speed up the run time performance. Many indexing methods allow the online query algorithms progressively retrieve the data and stop at a certain point. However, in many cases, the number of data accesses is sensitive to the query parameters (i.e., linear weights in the score functions).In this paper, we study the sequentially layered indexing problem where tuples are put into multiple consecutive layers and any top-k query can be answered by at most k layers of tuples. We propose a new criterion for building the layered index. A layered index is robust if for any k, the number of tuples in the top k layers is minimal in comparison with all the other alternatives. The robust index guarantees the worst case performance for arbitrary query parameters. We derive a necessary and sufficient condition for robust index. The problem is shown solvable within O(ndlog n) (where d is the number of dimensions, and n is the number of tuples). To reduce the high complexity of the exact solution, we develop an approximate approach, which has time complexity O(2d n(log n)r(d)-1), where r(d) = ⌈d/2⌉ + ⌊d/2⌋ ⌈d/2⌉. Our experimental results show that our proposed method outperforms the best known previous methods.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {235–246},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164150,
author = {Augsten, Nikolaus and B\"{o}hlen, Michael and Gamper, Johann},
title = {An Incrementally Maintainable Index for Approximate Lookups in Hierarchical Data},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Several recent papers argue for approximate lookups in hierarchical data and propose index structures that support approximate searches in large sets of hierarchical data. These index structures must be updated if the underlying data changes. Since the performance of a full index reconstruction is prohibitive, the index must be updated incrementally.We propose a persistent and incrementally maintainable index for approximate lookups in hierarchical data. The index is based on small tree patterns, called pq-grams. It supports efficient updates in response to structure and value changes in hierarchical data and is based on the log of tree edit operations. We prove the correctness of the incremental maintenance for sequences of edit operations. Our algorithms identify a small set of pq-grams that must be updated to maintain the index. The experimental results with synthetic and real data confirm the scalability of our approach.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {247–258},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164151,
author = {Zhang, Ning and \"{O}zsu, M. Tamer and Ilyas, Ihab F. and Aboulnaga, Ashraf},
title = {FIX: Feature-Based Indexing Technique for XML Documents},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Indexing large XML databases is crucial for efficient evaluation of XML twig queries. In this paper, we propose a feature-based indexing technique, called FIX, based on spectral graph theory. The basic idea is that for each twig pattern in a collection of XML documents, we calculate a vector of features based on its structural properties. These features are used as keys for the patterns and stored in a B+tree. Given an XPath query, its feature vector is first calculated and looked up in the index. Then a further refinement phase is performed to fetch the final results. We experimentally study the indexing technique over both synthetic and real data sets. Our experiments show that FIX provides great pruning power and could gain an order of magnitude performance improvement for many XPath queries over existing evaluation techniques.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {259–270},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164200,
author = {Apaydin, Tan and Canahuate, Guadalupe and Ferhatosmanoglu, Hakan and Tosun, Ali Saman},
title = {Approximate Encoding for Direct Access and Query Processing over Compressed Bitmaps},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Bitmap indices have been widely and successfully used in scientific and commercial databases. Compression techniques based on run-length encoding are used to improve the storage performance. However, these techniques introduce significant overheads in query processing even when only a few rows are queried. We propose a new bitmap encoding scheme based on multiple hashing, where the bitmap is kept in a compressed form, and can be directly accessed without decompression. Any subset of rows and/or columns can be retrieved efficiently by reconstructing and processing only the necessary subset of the bitmap. The proposed scheme provides approximate results with a trade-off between the amount of space and the accuracy. False misses are guaranteed not to occur, and the false positive rate can be estimated and controlled. We show that query execution is significantly faster than WAH-compressed bitmaps, which have been previously shown to achieve the fastest query response times. The proposed scheme achieves accurate results (90%-100%) and improves the speed of query processing from 1 to 3 orders of magnitude compared to WAH.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {846–857},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164201,
author = {Raman, Vijayshankar and Swart, Garret},
title = {How to Wring a Table Dry: Entropy Compression of Relations and Querying of Compressed Relations},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We present a method to compress relations close to their entropy while still allowing efficient queries. Column values are encoded into variable length codes to exploit skew in their frequencies. The codes in each tuple are concatenated and the resulting tuplecodes are sorted and delta-coded to exploit the lack of ordering in a relation. Correlation is exploited either by co-coding correlated columns, or by using a sort order that leverages the correlation. We prove that this method leads to near-optimal compression (within 4.3 bits/tuple of entropy), and in practice, we obtain up to a 40 fold compression ratio on vertical partitions tuned for TPC-H queries.We also describe initial investigations into efficient querying over compressed data. We present a novel Huffman coding scheme, called segregated coding, that allows range and equality predicates on compressed data, without accessing the full dictionary. We also exploit the delta coding to speed up scans, by reusing computations performed on nearly identical records. Initial results from a prototype suggest that with these optimizations, we can efficiently scan, tokenize and apply predicates on compressed relations.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {858–869},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164202,
author = {Reiss, Frederick and Garofalakis, Minos and Hellerstein, Joseph M.},
title = {Compact Histograms for Hierarchical Identifiers},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Distributed monitoring applications often involve streams of unique identifiers (UIDs) such as IP addresses or RFID tag IDs. An important class of query for such applications involves partitioning the UIDs into groups using a large lookup table; the query then performs aggregation over the groups. We propose using histograms to reduce bandwidth utilization in such settings, using a histogram partitioning function as a compact representation of the lookup table. We investigate methods for constructing histogram partitioning functions for lookup tables over unique identifiers that form a hierarchy of contiguous groups, as is the case with network addresses and several other types of UID. Each bucket in our histograms corresponds to a subtree of the hierarchy. We develop three novel classes of partitioning functions for this domain, which vary in their structure, construction time, and estimation accuracy.Our approach provides several advantages over previous work. We show that optimal instances of our partitioning functions can be constructed efficiently from large lookup tables. The partitioning functions are also compact, with each partition represented by a single identifier. Finally, our algorithms support minimizing any error metric that can be expressed as a distributive aggregate; and they extend naturally to multiple hierarchical dimensions. In experiments on real-world network monitoring data, we show that our histograms provide significantly higher accuracy per bit than existing techniques.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {870–881},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164203,
author = {Keogh, Eamonn and Wei, Li and Xi, Xiaopeng and Lee, Sang-Hee and Vlachos, Michail},
title = {LB_Keogh Supports Exact Indexing of Shapes under Rotation Invariance with Arbitrary Representations and Distance Measures},
year = {2006},
publisher = {VLDB Endowment},
abstract = {The matching of two-dimensional shapes is an important problem with applications in domains as diverse as biometrics, industry, medicine and anthropology. The distance measure used must be invariant to many distortions, including scale, offset, noise, partial occlusion, etc. Most of these distortions are relatively easy to handle, either in the representation of the data or in the similarity measure used. However rotation invariance seems to be uniquely difficult. Current approaches typically try to achieve rotation invariance in the representation of the data, at the expense of discrimination ability, or in the distance measure, at the expense of efficiency. In this work we show that we can take the slow but accurate approaches and dramatically speed them up. On real world problems our technique can take current approaches and make them four orders of magnitude faster, without false dismissals. Moreover, our technique can be used with any of the dozens of existing shape representations and with all the most popular distance measures including Euclidean distance, Dynamic Time Warping and Longest Common Subsequence.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {882–893},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164204,
author = {Hu, Haibo and Lee, Dik Lun and Lee, Victor C. S.},
title = {Distance Indexing on Road Networks},
year = {2006},
publisher = {VLDB Endowment},
abstract = {The processing of kNN and continuous kNN queries on spatial network databases (SNDB) has been intensively studied recently. However, there is a lack of systematic study on the computation of network distances, which is the most fundamental difference between a road network and a Euclidean space. Since the online Dijkstra's algorithm has been shown to be efficient only for short distances, we propose an efficient index, called distance signature, for distance computation and query processing over long distances. Distance signature discretizes the distances between objects and network nodes into categories and then encodes these categories. To minimize the storage and search costs, we present the optimal category partition, and the encoding and compression algorithms for the signatures, based on a simplified network topology. By mathematical analysis and experimental study, we showed that the signature index is efficient and robust for various data distributions, query workloads, parameter settings and network updates.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {894–905},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164205,
author = {Venkateswaran, Jayendra and Lachwani, Deepak and Kahveci, Tamer and Jermaine, Christopher},
title = {Reference-Based Indexing of Sequence Databases},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We consider the problem of similarity search in a very large sequence database with edit distance as the similarity measure. Given limited main memory, our goal is to develop a reference-based index that reduces the number of costly edit distance computations in order to answer a query. The idea in reference-based indexing is to select a small set of reference sequences that serve as a surrogate for the other sequences in the database. We consider two novel strategies for selecting references as well as a new strategy for assigning references to database sequences. Our experimental results show that our selection and assignment methods far outperform competitive methods. For example, our methods prune up to 20 times as many sequences as the Omni method, and as many as 30 times as many sequences as frequency vectors. Our methods also scale nicely for databases containing many and/or very long sequences.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {906–917},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164152,
author = {Benzaken, V\'{e}ronique and Castagna, Giuseppe and Colazzo, Dario and Nguy\^{e}n, Kim},
title = {Type-Based XML Projection},
year = {2006},
publisher = {VLDB Endowment},
abstract = {XML data projection (or pruning) is one of the main optimization techniques recently adopted in the context of main-memory XML query-engines. The underlying idea is quite simple: given a query Q over a document D, the subtrees of D not necessary to evaluate Q are pruned, thus obtaining a smaller document D'. Then Q is executed over D', hence avoiding to allocate and process nodes that will never be reached by navigational specifications in Q.In this article, we propose a new approach, based on types, that greatly improves current solutions. Besides providing comparable or greater precision and far lesser pruning overhead our solution, unlike current approaches, takes into account backward axes, predicates, and can be applied to multiple queries rather than just to single ones. A side contribution is a new type system for XPath able to handle backward axes, which we devise in order to apply our solution.The soundness of our approach is formally proved. Furthermore, we prove that the approach is also complete (i.e., yields the best possible type-driven pruning) for a relevant class of queries and DTDs, which include nearly all the queries used in the XMark and XPathMark benchmarks. These benchmarks are also used to test our implementation and show and gauge the practical benefits of our solution.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {271–282},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164153,
author = {Chen, Songting and Li, Hua-Gang and Tatemura, Junichi and Hsiung, Wang-Pin and Agrawal, Divyakant and Candan, K. Sel\c{c}uk},
title = {Twig<sup>2</sup>Stack: Bottom-up Processing of Generalized-Tree-Pattern Queries over XML Documents},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Tree pattern matching is one of the most fundamental tasks for XML query processing. Holistic twig query processing techniques [4, 16] have been developed to minimize the intermediate results, namely, those root-to-leaf path matches that are not in the final twig results. However, useless path matches cannot be completely avoided, especially when there is a parent-child relationship in the twig query. Furthermore, existing approaches do not consider the fact that in practice, in order to process XPath or XQuery statements, a more powerful form of twig queries, namely, Generalized-Tree-Pattern (GTP) [8] queries, is required. Most existing works on processing GTP queries generally calls for costly post-processing for eliminating redundant data and/or grouping of the matching results.In this paper, we first propose a novel hierarchical stack encoding scheme to compactly represent the twig results. We introduce Twig2Stack, a bottom-up algorithm for processing twig queries based on this encoding scheme. Then we show how to efficiently enumerate the query results from the encodings for a given GTP query. To our knowledge, this is the first GTP matching solution that avoids any post path-join, sort, duplicate elimination and grouping operations. Extensive performance studies on various data sets and queries show that the proposed Twig2Stack algorithm not only has better twig query processing performance than state-of-the-art algorithms, but is also capable of efficiently processing the more complex GTP queries.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {283–294},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164154,
author = {Pradhan, Sujeet},
title = {An Algebraic Query Model for Effective and Efficient Retrieval of XML Fragments},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Finding a suitable fragment of interest in a nonschematic XML document with a simple keyword search is a complex task. To deal with this problem, this paper proposes a theoretical framework with a focus on an algebraic query model having a novel query semantics. Based on this semantics, XML fragments that look meaningful to a keyword-based query are effectively retrieved by the operations defined in the model. In contrast to earlier work, our model supports filters for restricting the size of a query result, which otherwise may contain a large number of potentially irrelevant fragments. We introduce a class of filters having a special property that enables significant reduction in query processing cost. Many practically useful filters fall in this class and hence, the proposed model can be efficiently applied to real-world XML documents. Several other issues regarding algebraic manipulation of the operations defined in our query model are also formally discussed.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {295–306},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164209,
author = {Benjelloun, Omar and Sarma, Anish Das and Halevy, Alon and Widom, Jennifer},
title = {ULDBs: Databases with Uncertainty and Lineage},
year = {2006},
publisher = {VLDB Endowment},
abstract = {This paper introduces ULDBs, an extension of relational databases with simple yet expressive constructs for representing and manipulating both lineage and uncertainty. Uncertain data and data lineage are two important areas of data management that have been considered extensively in isolation, however many applications require the features in tandem. Fundamentally, lineage enables simple and consistent representation of uncertain data, it correlates uncertainty in query results with uncertainty in the input data, and query processing with lineage and uncertainty together presents computational benefits over treating them separately.We show that the ULDB representation is complete, and that it permits straightforward implementation of many relational operations. We define two notions of ULDB minimality--data-minimal and lineage-minimal--and study minimization of ULDB representations under both notions. With lineage, derived relations are no longer self-contained: their uncertainty depends on uncertainty in the base data. We provide an algorithm for the new operation of extracting a database subset in the presence of interconnected uncertainty. Finally, we show how ULDBs enable a new approach to query processing in probabilistic databases.ULDBs form the basis of the Trio system under development at Stanford.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {953–964},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164210,
author = {Gupta, Rahul and Sarawagi, Sunita},
title = {Creating Probabilistic Databases from Information Extraction Models},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Many real-life applications depend on databases automatically curated from unstructured sources through imperfect structure extraction tools. Such databases are best treated as imprecise representations of multiple extraction possibli-ties. State-of-the-art statistical models of extraction provide a sound probability distribution over extractions but are not easy to represent and query in a relational framework. In this paper we address the challenge of approximating such distributions as imprecise data models. In particular, we investigate a model that captures both row-level and column-level uncertainty and show that this representation provides significantly better approximation compared to models that use only row or only column level uncertainty. We present efficient algorithms for finding the best approximating parameters for such a model: our algorithm exploits the structure of the model to avoid enumerating the exponential number of extraction possibilities.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {965–976},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164211,
author = {Missier, Paolo and Embury, Suzanne and Greenwood, Mark and Preece, Alun and Jin, Binling},
title = {Quality Views: Capturing and Exploiting the User Perspective on Data Quality},
year = {2006},
publisher = {VLDB Endowment},
abstract = {There is a growing awareness among life scientists of the variability in quality of the data in public repositories, and of the threat that poor data quality poses to the validity of experimental results. No standards are available, however, for computing quality levels in this data domain. We argue that data processing environments used by life scientists should feature facilities for expressing and applying quality-based, personal data acceptability criteria.We propose a framework for the specification of users' quality processing requirements, called quality views. These views are compiled and semi-automatically embedded within the data processing environment. The result is a quality management toolkit that promotes rapid prototyping and reuse of quality components. We illustrate the utility of the framework by showing how it can be deployed within Taverna, a scientific workflow management tool, and applied to actual workflows for data analysis in proteomics.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {977–988},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164155,
author = {Bohannon, Philip and Elnahrawy, Eiman and Fan, Wenfei and Flaster, Michael},
title = {Putting Context into Schema Matching},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Attribute-level schema matching has proven to be an important first step in developing mappings for data exchange, integration, restructuring and schema evolution. In this paper we investigate contextual schema matching, in which selection conditions are associated with matches by the schema matching process in order to improve overall match quality. We define a general space of matching techniques, and within this framework we identify a variety of novel, concrete algorithms for contextual schema matching. Furthermore, we show how common schema mapping techniques can be generalized to take more effective advantage of contextual matches, enabling automatic construction of mappings across certain forms of schema heterogeneity. An experimental study examines a wide variety of quality and performance issues. In addition, it demonstrates that contextual schema matching is an effective and practical technique to further automate the definition of complex data transformations.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {307–318},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164156,
author = {Yu, Cong and Jagadish, H. V.},
title = {Schema Summarization},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Real database systems can often be very complex. A person wishing to access data from an unfamiliar database has the daunting task of understanding its schema before being able to pose a correct query against it. A schema summary can be of great help, providing a succinct overview of the entire schema, and making it possible to explore in depth only the relevant schema components.In this paper we formally define a schema summary and two desirable properties (in addition to minimizing size) of a summary: presenting important schema elements and achieving broad information coverage. We develop algorithms that allow us to automatically generate schema summaries based on these two goals. We further develop an objective metric for assessing the quality of a schema summary using query information. Experimental evaluation using this metric demonstrates that the summaries produced by our algorithms can significantly reduce the amount of user effort required to formulate a query through schema exploration.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {319–330},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164157,
author = {Warren, Robert H. and Tompa, Frank Wm.},
title = {Multi-Column Substring Matching for Database Schema Translation},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We describe a method for discovering complex schema translations involving substrings from multiple database columns. The method does not require a training set of instances linked across databases and it is capable of dealing with both fixed-and variable-length field columns. We propose an iterative algorithm that deduces the correct sequence of concatenations of column substrings in order to translate from one database to another. We introduce the algorithm along with examples on common database data values and examine its performance on real-world and synthetic datasets.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {331–342},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164222,
author = {Song, Jung-Hee},
title = {IT839 Policy Leading to U-Korea},
year = {2006},
publisher = {VLDB Endowment},
abstract = {With the strategic early adoption of IT infrastructure technologies such as the broadband and the CDMA wireless communication in Korea, IT industry has been the major contributor to the recent Korean economic growth, accounting for 15.8% of the real GDP in 2005.In 2004, Korean Ministry of Information and Communication has established the so-called IT839 strategy as its new IT initiative. IT839 means 8 new IT services which will be deployed within the next three to four years so that service operators invest on 3 new wireless broadband and secure communication infrastructures to offer high-quality ubiquitous service. For rich user experience, 9 hardware and software component industries are defined as the growth engine. The total twenty industry sectors of the IT839 strategy form the IT industry value chain.In 2005, Korean Ministry of Information and Communication updated its IT839 structure by replacing some of its components to explicitly address the u-Korea framework.This IT839 strategy will contribute not only to IT industry but bring qualitative changes to the economic and social paradigm. It ultimately aims to realize a ubiquitous world by forming a virtuous circle of developing new services, infrastructure, and growth engines.To sustain the current momentum, Korea must become proactive in the global collaboration. Korean Ministry of Information and Communication invests in drawing leading global IT companies into Korea for cooperation with Korea R&amp;D partners. During the past two years, twelve global companies established local R&amp;D laboratories in Korea with partial funding from Korean government.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1103},
numpages = {1},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164223,
author = {Ryu, In},
title = {Home Network: Road to Ubiquitous World},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Home Network is considered to be a stepping stone for ubiquitous world. The current home network solution in Korea is highly focused on delivering security, home control, and remote management to high-rise multi-dwelling units. Initial deployment case shows that individual users are fairly satisfied with the current solution sets but still looking for high-end entertainment solution as well. In this talk, we will cover the current status of LG HomNet, LG Electronics' home network solution, deployment cases, and future roadmap.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1104},
numpages = {1},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164224,
author = {Kim, Changhyun},
title = {Advances in Memory Technology},
year = {2006},
publisher = {VLDB Endowment},
abstract = {The continuous growth of the memory market, whose early beginnings in the 70's and 80's were marked by PC and server DRAMs, has experienced a new boost since the beginning of the 21st century due to the emergence of digital consumer &amp; mobile markets such as cellular phone, DSC, and MP3. The join of the nonvolatile and low-power Flash memory has led to a further explosive growth. Ever increasing density and decreasing costs have evoked a tremendous rise in consumer demand.Innovations in memory technology are reflected in the continuous advance in high density, high speed and low power technologies, in the course of which the design rule has shown a transition from micrometer to nanometer scale. Additionally, the development of new materials has given birth to new high-performance nonvolatile memory types (PRAM, RRAM, MRAM, FRAM, etc.), which open even more opportunities for growth of the semiconductor market.The steep increase in technology of today's memories shows itself in the capacity and speed of storing information of everybody's use: a 1cm2 memory chip can store 10Gbit information now, which corresponds to either 80K pages of newspaper, 20 hours of music or 2.5 movie hours. Today's DRAM shows a random access time of 25-50ns and I/O bandwidth of 3-4GHz. Technical innovations will continue to drive the increase of memory density and speed in the future. Higher storage density is expected to be achieved by breakthroughs such as 3D memory stacking technology (cell/chip/package), the use of 3-dimensional transistors or the shrinkage of memory storage nodes to the atomic scale. Memory system performance will possibly be enhanced by the fusion of conventional commodity memories and new memories: several memories like Flash, SRAM, DRAM, new memories will be merged together with logic and software. Thus we expect that semiconductor products will show a larger variety of high performance systems with much higher robustness and persistence.In the 21st century, which has just begun, memory technology will combine with various other fields (IT, BT, NT) and thus open new markets such as massive data &amp; information processing, bio &amp; health care, and humanoid &amp; aerospace. It will contribute to a world with more comfort and stability, where everywhere and anytime people can exchange and share their thoughts, sensations and emotions.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1105},
numpages = {1},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164158,
author = {Beeri, Catriel and Eyal, Anat and Kamenkovich, Simon and Milo, Tova},
title = {Querying Business Processes},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We present in this paper BP-QL, a novel query language for querying business processes. The BP-QL language is based on an intuitive model of business processes, an abstraction of the emerging BPEL (Business Process Execution Language) standard. It allows users to query business processes visually, in a manner very analogous to how such processes are typically specified, and can be employed in a distributed setting, where process components may be provided by distinct providers(peers).We describe here the query language as well as its underlying formal model. We consider the properties of the various language components and explain how they influenced the language design. In particular we distinguish features that can be efficiently supported, and those that incur a prohibitively high cost, or cannot be computed at all. We also present our implementation which complies with real life standards for business process specifications, XML, and Web services, and is used in the BP-QL system.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {343–354},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164159,
author = {Srivastava, Utkarsh and Munagala, Kamesh and Widom, Jennifer and Motwani, Rajeev},
title = {Query Optimization over Web Services},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Web services are becoming a standard method of sharing data and functionality among loosely-coupled systems. We propose a general-purpose Web Service Management System (WSMS) that enables querying multiple web services in a transparent and integrated fashion. This paper tackles a first basic WSMS problem: query optimization for Select-Project-Join queries spanning multiple web services. Our main result is an algorithm for arranging a query's web service calls into a pipelined execution plan that optimally exploits parallelism among web services to minimize the query's total running time. Surprisingly, the optimal plan can be found in polynomial time even in the presence of arbitrary precedence constraints among web services, in contrast to traditional query optimization where the analogous problem is NP-hard. We also give an algorithm for determining the optimal granularity of data "chunks" to be used for each web service call. Experiments with an initial prototype indicate that our algorithms can lead to significant performance improvement over more straightforward techniques.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {355–366},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164160,
author = {Dittrich, Jens-Peter and Salles, Marcos Antonio Vaz},
title = {IDM: A Unified and Versatile Data Model for Personal Dataspace Management},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Personal Information Management Systems require a powerful and versatile data model that is able to represent a highly heterogeneous mix of data such as relational data, XML, file content, folder hierarchies, emails and email attachments, data streams, RSS feeds and dynamically computed documents, e.g. ActiveXML [3]. Interestingly, until now no approach was proposed that is able to represent all of the above data in a single, powerful yet simple data model. This paper fills this gap. We present the iMeMex Data Model (iDM) for personal information management. iDM is able to represent unstructured, semi-structured and structured data inside a single model. Moreover, iDM is powerful enough to represent graph-structured data, intensional data as well as infinite data streams. Further, our model enables to represent the structural information available inside files. As a consequence, the artifical boundary between inside and outside a file is removed to enable a new class of queries. As iDM allows the representation of the whole personal dataspace [20] of a user in a single model, it is the foundation of the iMeMex Personal Dataspace Management System (PDSMS) [16, 14, 47]. This paper also presents results of an evaluation of an initial iDM implementation in iMeMex that show that iDM can be efficiently supported in a real PDSMS.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {367–378},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164161,
author = {Morfonios, Konstantinos and Ioannidis, Yannis},
title = {CURE for Cubes: Cubing Using a ROLAP Engine},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Data cube construction has been the focus of much research due to its importance in improving efficiency of OLAP. A significant fraction of this work has been on ROLAP techniques, which are based on relational technology. Existing ROLAP cubing solutions mainly focus on "flat" datasets, which do not include hierarchies in their dimensions. Nevertheless, the nature of hierarchies introduces several complications into cube construction, making existing techniques essentially inapplicable in a significant number of real-world applications. In particular, hierarchies raise three main challenges: (a) The number of nodes in a cube lattice increases dramatically and its shape is more involved. These require new forms of lattice traversal for efficient execution. (b) The number of unique values in the higher levels of a dimension hierarchy may be very small; hence, partitioning data into fragments that fit in memory and include all entries of a particular value may often be impossible. This requires new partitioning schemes. (c) The number of tuples that need to be materialized in the final cube increases dramatically. This requires new storage schemes that remove all forms of redundancy for efficient space utilization. In this paper, we propose CURE, a novel ROLAP cubing method that addresses these issues and constructs complete data cubes over very large datasets with arbitrary hierarchies. CURE contributes a novel lattice traversal scheme, an optimized partitioning method, and a suite of relational storage schemes for all forms of redundancy. We demonstrate the effectiveness of CURE through experiments on both real-world and synthetic datasets. Among the experimental results, we distinguish those that have made CURE the first ROLAP technique to complete the construction of the cube of the highest-density dataset in the APB-1 benchmark (12 GB). CURE was in fact quite efficient on this, showing great promise with respect to the potential of the technique overall.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {379–390},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164162,
author = {Burdick, Doug and Deshpande, Prasad M. and Jayram, T. S. and Ramakrishnan, Raghu and Vaithyanathan, Shivakumar},
title = {Efficient Allocation Algorithms for OLAP over Imprecise Data},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Recent work proposed extending the OLAP data model to support data ambiguity, specifically imprecision and uncertainty. A process called allocation was proposed to transform a given imprecise fact table into a form, called the Extended Database, that can be readily used to answer OLAP aggregation queries.In this work, we present scalable, efficient algorithms for creating the Extended Database (i.e., performing allocation) for a given imprecise fact table. Many allocation policies require multiple iterations over the imprecise fact table, and the straightforward evaluation approaches introduced earlier can be highly inefficient. Optimizing iterative allocation policies for large datasets presents novel challenges, and has not been considered previously to the best of our knowledge. In addition to developing scalable allocation algorithms, we present a performance evaluation that demonstrates their efficiency and compares their performance with respect to straight-foward approaches.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {391–402},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164163,
author = {Chen, Lei and Ramakrishnan, Raghu and Barford, Paul and Chen, Bee-Chung and Yegneswaran, Vinod},
title = {Composite Subset Measures},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Measures are numeric summaries of a collection of data records produced by applying aggregation functions. Summarizing a collection of subsets of a large dataset, by computing a measure for each subset in the (typically, user-specified) collection is a fundamental problem. The multidimensional data model, which treats records as points in a space defined by dimension attributes, offers a natural space of data subsets to be considered as summarization candidates, and traditional SQL and OLAP constructs, such as GROUP BY and CUBE, allow us to compute measures for subsets drawn from this space. However, GROUP BY only allows us to summarize a limited collection of subsets, and CUBE summarizes all subsets in this space. Further, they restrict the measure used to summarize a data subset to be a one-step aggregation, using functions such as SUM, of field-values in the data records.In this paper, we introduce composite subset measures, computed by aggregating not only data records but also the measures of other related subsets. We allow summarization of naturally related regions in the multidimensional space, offering more flexibility than either GROUP BY or CUBE in the choice of what data subsets to summarize. Thus, our framework allows more meaningful summaries to be computed for a targeted collection of data subsets.We propose an algebra called AW-RA and an equivalent pictorial language called aggregation workflows. Aggregation workflows allow for intuitive expression of composite measure queries, and the underlying algebra is designed to facilitate efficient multiscan execution. We describe an evaluation framework based on multiple passes of sorting and scanning over the original dataset. In each pass, several measures are evaluated simultaneously, and dependencies between these measures and containment relationships between the underlying subsets of data are orchestrated to reduce the memory footprint of the computation. We present a performance evaluation that demonstrates the benefits of our approach.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {403–414},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164241,
author = {Mazeika, Arturas and B\"{o}hlen, Michael H. and Taliun, Andrej},
title = {Adaptive Density Estimation},
year = {2006},
publisher = {VLDB Endowment},
abstract = {This demonstration illustrates the APDF tree: an adaptive tree that supports the effective and effcient computation of continuous density information. The APDF tree allocates more partition points in non-linear areas of the density function and fewer points in linear areas of the density function. This yields not only a bounded, but a tight control of the error. The demonstration explains the core steps of the computation of the APDF tree (split, kernel additions, tree optimization, kernel additions, unsplit) and demos the implementation for different datasets.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1191–1194},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164242,
author = {Rodrigues, Jos\'{e} F. and Tong, Hanghang and Traina, Agma J. M. and Faloutsos, Christos and Leskovec, Jure},
title = {GMine: A System for Scalable, Interactive Graph Visualization and Mining},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Several graph visualization tools exist. However, they are not able to handle large graphs, and/or they do not allow interaction. We are interested on large graphs, with hundreds of thousands of nodes. Such graphs bring two challenges: the first one is that any straightforward interactive manipulation will be prohibitively slow. The second one is sensory overload: even if we could plot and replot the graph quickly, the user would be overwhelmed with the vast volume of information because the screen would be too cluttered as nodes and edges overlap each other.Our GMine system addresses both these issues, by using summarization and multi-resolution. GMine offers multi-resolution graph exploration by partitioning a given graph into a hierarchy of communities-within-communities and storing it into a novel R-treelike structure which we name G-Tree. GMine offers summarization by implementing an innovative subgraph extraction algorithm and then visualizing its output.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1195–1198},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164243,
author = {Aberer, Karl and Hauswirth, Manfred and Salehi, Ali},
title = {A Middleware for Fast and Flexible Sensor Network Deployment},
year = {2006},
publisher = {VLDB Endowment},
abstract = {A key problem in current sensor network technology is the heterogeneity of the available software and hardware platforms which makes deployment and application development a tedious and time consuming task. To minimize the unnecessary and repetitive implementation of identical functionalities for different platforms, we present our Global Sensor Networks (GSN) middleware which supports the flexible integration and discovery of sensor networks and sensor data, enables fast deployment and addition of new platforms, provides distributed querying, filtering, and combination of sensor data, and supports the dynamic adaption of the system configuration during operation. GSN's central concept is the virtual sensor abstraction which enables the user to declaratively specify XML-based deployment descriptors in combination with the possibility to integrate sensor network data through plain SQL queries over local and remote sensor data sources. In this demonstration, we specifically focus on the deployment aspects and allow users to dynamically reconfigure the running system, to add new sensor networks on the fly, and to monitor the effects of the changes via a graphical interface. The GSN implementation is available from http://globalsn.sourceforge.net/.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1199–1202},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164244,
author = {Chu, David and Tavakoli, Arsalan and Popa, Lucian and Hellerstein, Joseph},
title = {Entirely Declarative Sensor Network Systems},
year = {2006},
publisher = {VLDB Endowment},
abstract = {The database and sensor network community have both recognized the utility of SQL for interfacing with sensor network systems. Recently there have been proposals to construct Internet protocols declaratively in variants of Datalog. We take these ideas to their logical extreme, and demonstrate entire distributed sensor network systems built declaratively. Our demo exposes the rapidity, flexibility, and efficiency of our approach by building several fully-functional yet widely-varying sensor network applications and services declaratively. As a result of our declarative construction, we are able to highlight a wealth of previously underexposed similarities between sensor networks and database concepts. In addition, we tackle many database systems challenges in building multiple layers of a declarative database for an embedded, distributed system.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1203–1206},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164245,
author = {Wang, Song and Su, Hong and Li, Ming and Wei, Mingzhu and Yang, Shoushen and Ditto, Drew and Rundensteiner, Elke A. and Mani, Murali},
title = {R-SOX: Runtime Semantic Query Optimization over XML Streams},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Optimizing queries over XML streams has been an important and non-trivial issue with the emergence of complex XML stream applications such as monitoring sensor networks and online transaction processing. Our system, R-SOX, provides a platform for runtime query optimization based on dynamic schema knowledge embedded in the XML streams. Such information provides refined runtime schema knowledge thus dramatically enlarged the opportunity for schema-based query optimizations. In this demonstration, we focus on the following three aspects: (1) annotation of runtime schema knowledge; (2) incremental maintenance of run-time schema knowledge; (3) dynamic semantic query optimization techniques. The overall framework for runtime semantic query optimization, including several classes of dynamic optimization techniques, will be shown in this demonstration.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1207–1210},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164246,
author = {Zhang, Junqi and Zhou, Xiangdong and Wang, Wei and Shi, Baile and Pei, Jian},
title = {Using High Dimensional Indexes to Support Relevance Feedback Based Interactive Images Retrieval},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Image retrieval has found more and more applications. Due to the well recognized semantic gap problem, the accuracy and the recall of image similarity search are often still low. As an effective method to improve the quality of image retrieval, the relevance feedback approach actively applies users' feedback to refine the search. As searching a large image database is often costly, to improve the efficiency, high dimensional indexes may help. However, many existing database indexes are not adaptive to updates of distance measures caused by users' feedback. In this paper, we propose a demo to illustrate the relevance feedback based interactive images retrieval procedure, and examine the effectiveness and the efficiency of various indexes. Particularly, audience can interactively investigate the effect of updated distance measures on the data space where the images are supposed to be indexed, and on the distributions of the similar images in the indexes. We also introduce our new B+-tree-like index method based on cluster splitting and iDistance.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1211–1214},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164247,
author = {Huang, Chia-Hsin and Chuang, Tyng-Ruey and Lu, James J. and Lee, Hahn-Ming},
title = {XML Evolution: A Two-Phase XML Processing Model Using XML Prefiltering Techniques},
year = {2006},
publisher = {VLDB Endowment},
abstract = {An implementation based on the two-phase XML processing model introduced in [3] is presented in this paper. The model employs a prefilter to remove uninteresting fragments of an input XML document by approximately executing a user's queries. The refined candidate-set XML document is then returned to the user's DOM- or SAX-based applications for further processing. In this demonstration, it is shown that the technique significantly enhances the performance of existing DOM- and SAX-based XML applications and tools (e.g., XPath/XQuery processors and XML parsers), while reducing computational resource needs. Moreover, the prefilter can be easily integrated into existing applications by adding only one instruction. We also present an enhancement to the indexing scheme of the prefiltering technique to speed up the evaluation of certain axes.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1215–1218},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164248,
author = {Kowalkiewicz, Marek and Kaczmarek, Tomasz and Abramowicz, Witold},
title = {MyPortal: Robust Extraction and Aggregation of Web Content},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We demonstrate myPortal - an application for web content block extraction and aggregation. The research issues behind the tool are also explained, with an emphasis on robustness of web content extraction.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1219–1222},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164249,
author = {Shegalov, German and Weikum, Gerhard},
title = {EOS<sup>2</sup>: Unstoppable Stateful PHP},
year = {2006},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1223–1226},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164250,
author = {Fan, Wenfei and Geerts, Floris and Jia, Xibei and Kementsietsidis, Anastasios},
title = {SMOQE: A System for Providing Secure Access to XML},
year = {2006},
publisher = {VLDB Endowment},
abstract = {XML views have been widely used to enforce access control, support data integration, and speed up query answering. In many applications, e.g., XML security enforcement, it is prohibitively expensive to materialize and maintain a large number of views. Therefore, views are necessarily virtual. An immediate question then is how to answer queries on XML virtual views. A common approach is to rewrite a query on the view to an equivalent one on the underlying document, and evaluate the rewritten query. This is the approach used in the Secure MOdular Query Engine (SMOQE). The demo presents SMOQE, the first system to provide efficient support for answering queries over virtual and possibly recursively defined XML views. We demonstrate a set of novel techniques for the specification of views, the rewriting, evaluation and optimization of XML queries. Moreover, we provide insights into the internals of the engine by a set of visual tools.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1227–1230},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164251,
author = {Zheng, Yifeng and Fisher, Stephen and Cohen, Shirley and Guo, Sheng and Kim, Junhyong and Davidson, Susan B.},
title = {Crimson: A Data Management System to Support Evaluating Phylogenetic Tree Reconstruction Algorithms},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Evolutionary and systems biology increasingly rely on the construction of large phylogenetic trees which represent the relationships between species of interest. As the number and size of such trees increases, so does the need for efficient data storage and query capabilities. Although much attention has been focused on XML as a tree data model, phylogenetic trees differ from document-oriented applications in their size and depth, and their need for structure-based queries rather than path-based queries.This paper focuses on Crimson, a tree storage system for phylogenetic trees used to evaluate phylogenetic tree reconstruction algorithms within the context of the NSF CIPRes project. A goal of the modeling component of the CIPRes project is to construct a huge simulation tree representing a "gold standard" of evolutionary history against which phylogenetic tree reconstruction algorithms can be tested.In this demonstration, we highlight our storage and indexing strategies and show how Crimson is used for benchmarking phylogenetic tree reconstruction algorithms. We also show how our design can be used to support more general queries over phylogenetic trees.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1231–1234},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164252,
author = {Wang, Ling and Rundensteiner, Elke A. and Mani, Murali and Jiang, Ming},
title = {HUX: Handling Updates in XML},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We demonstrate HUX (Handling Updates in XML) which provides a reliable and efficient solution for the XML view update problem. Given an update over an XML view, our U-Filter subsytem first determines whether the update is translatable or not by examining potential conflicts in both schema and data. If an update is determined to be translatable, our U-Translator subsystem searches potential translations and finds a "good" one. Our demonstration illustrates the working, as well as the performance, of the two sub-systems within HUX for different application scenarios.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1235–1238},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164167,
author = {Das, Gautam and Gunopulos, Dimitrios and Koudas, Nick and Tsirogiannis, Dimitris},
title = {Answering Top-k Queries Using Views},
year = {2006},
publisher = {VLDB Endowment},
abstract = {The problem of obtaining efficient answers to top-k queries has attracted a lot of research attention. Several algorithms and numerous variants of the top-k retrieval problem have been introduced in recent years. The general form of this problem requests the k highest ranked values from a relation, using monotone combining functions on (a subset of) its attributes.In this paper we explore space performance tradeoffs related to this problem. In particular we study the problem of answering top-k queries using views. A view in this context is a materialized version of a previously posed query, requesting a number of highest ranked values according to some monotone combining function defined on a subset of the attributes of a relation. Several problems of interest arise in the presence of such views. We start by presenting a new algorithm capable of combining the information from a number of views to answer ad hoc top-k queries. We then address the problem of identifying the most promising (in terms of performance) views to use for query answering in the presence of a collection of views. We formalize both problems and present efficient algorithms for their solution. We also discuss several extensions of the basic problems in this setting.We present the results of a thorough experimental study that deploys our techniques on real and synthetic data sets. Our results indicate that the techniques proposed herein comprise a robust solution to the problem of top-k query answering using views, gracefully exploring the space versus performance tradeoffs in the context of top-k query answering.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {451–462},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164168,
author = {Xin, Dong and Han, Jiawei and Cheng, Hong and Li, Xiaolei},
title = {Answering Top-k Queries with Multi-Dimensional Selections: The Ranking Cube Approach},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Observed in many real applications, a top-k query often consists of two components to reflect a user's preference: a selection condition and a ranking function. A user may not only propose ad hoc ranking functions, but also use different interesting subsets of the data. In many cases, a user may want to have a thorough study of the data by initiating a multi-dimensional analysis of the top-k query results. Previous work on top-k query processing mainly focuses on optimizing data access according to the ranking function only. The problem of efficient answering top-k queries with multi-dimensional selections has not been well addressed yet.This paper proposes a new computational model, called ranking cube, for efficient answering top-k queries with multi-dimensional selections. We define a rank-aware measure for the cube, capturing our goal of responding to multi-dimensional ranking analysis. Based on the ranking cube, an efficient query algorithm is developed which progressively retrieves data blocks until the top-k results are found. The curse of dimensionality is a well-known challenge for the data cube and we cope with this difficulty by introducing a new technique of ranking fragments. Our experiments on Microsoft's SQL Server 2005 show that our proposed approaches have significant improvement over the previous methods.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {463–474},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164169,
author = {Bast, Holger and Majumdar, Debapriyo and Schenkel, Ralf and Theobald, Martin and Weikum, Gerhard},
title = {IO-Top-k: Index-Access Optimized Top-k Query Processing},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Top-k query processing is an important building block for ranked retrieval, with applications ranging from text and data integration to distributed aggregation of network logs and sensor data. Top-k queries operate on index lists for a query's elementary conditions and aggregate scores for result candidates. One of the best implementation methods in this setting is the family of threshold algorithms, which aim to terminate the index scans as early as possible based on lower and upper bounds for the final scores of result candidates. This procedure performs sequential disk accesses for sorted index scans, but also has the option of performing random accesses to resolve score uncertainty. This entails scheduling for the two kinds of accesses: 1) the prioritization of different index lists in the sequential accesses, and 2) the decision on when to perform random accesses and for which candidates.The prior literature has studied some of these scheduling issues, but only for each of the two access types in isolation. The current paper takes an integrated view of the scheduling issues and develops novel strategies that outperform prior proposals by a large margin. Our main contributions are new, principled, scheduling methods based on a Knapsack-related optimization for sequential accesses and a cost model for random accesses. The methods can be further boosted by harnessing probabilistic estimators for scores, selectivities, and index list correlations. In performance experiments with three different datasets (TREC Terabyte, HTTP server logs, and IMDB), our methods achieved significant performance gains compared to the best previously known methods.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {475–486},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164173,
author = {Panda, Biswanath and Riedewald, Mirek and Pope, Stephen B. and Gehrke, Johannes and Chew, L. Paul},
title = {Indexing for Function Approximation},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Simulation is one of the most powerful tools that scientists have at their disposal for studying and understanding real-world physical phenomena. In order to be realistic, the mathematical models which drive simulations are often very complex and run for a very large number of simulation steps. The required computational resources often make it infeasible to evaluate simulation models exactly at each step, and thus scientists trade accuracy for reduced simulation cost.In this paper, we explore function approximation for a combustion simulation. In particular, we model high-dimensional function approximation (HFA) as a storage and retrieval problem, and we show that HFA defines a novel class of applications for high dimensional index structures. The interesting property of HFA is that it imposes a mixed query/update workload on the index which leads to novel tradeoffs between the efficiency of search versus updates. We investigate in detail one specific approach to HFA based on Taylor Series expansions and we analyze tradeoffs in index structure design through a thorough experimental study.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {523–534},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.11641734,
author = {Shivam, Piyush and Babu, Shivnath and Chase, Jeff},
title = {Active and Accelerated Learning of Cost Models for Optimizing Scientific Applications},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We present the NIMO system that automatically learns cost models for predicting the execution time of computational-science applications running on large-scale networked utilities such as computational grids. Accurate cost models are important for selecting efficient plans for executing these applications on the utility. Computational-science applications are often scripts (written, e.g., in languages like Perl or Matlab) connected using a workflow-description language, and therefore, pose different challenges compared to modeling the execution of plans for declarative queries with well-understood semantics. NIMO generates appropriate training samples for these applications to learn fairly-accurate cost models quickly using statistical learning techniques. NIMO's approach is active and noninvasive: it actively deploys and monitors the application under varying conditions, and obtains its training data from passive instrumentation streams that require no changes to the operating system or applications. Our experiments with real scientific applications demonstrate that NIMO significantly reduces the number of training samples and the time to learn fairly-accurate cost models.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {535–546},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164175,
author = {Denny, Matthew and Franklin, Michael J.},
title = {Adaptive Execution of Variable-Accuracy Functions},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Many analysis applications require the ability to repeatedly execute sophisticated modeling functions, which can each take minutes or even hours to produce a single answer. Because of this expense, such applications have largely been unable to directly use such models in queries, with either on-demand or continuous query processing technology. Query processors are hindered in their ability to optimize expensive modeling functions due to the "black box" nature of existing user-defined function (UDF) interfaces. In this paper, we address the problem of querying over sophisticated models with the development of VAOs (Variable-Accuracy Operators). VAOs use a new function interface that exposes the trade-off between compute time and accuracy that exists in many modeling functions. Using this interface, VAOs adaptively run each function call in a query only to an accuracy needed to answer the query, thus eliminating unneeded work. In this paper, we present the design of VAOs for a set of common query operations. We show the effectiveness of VAOs using a prototype implementation running financial queries over real bond market data.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {547–558},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164185,
author = {Chakaravarthy, Venkatesan T. and Gupta, Himanshu and Roy, Prasan and Mohania, Mukesh},
title = {Efficiently Linking Text Documents with Relevant Structured Information},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Faced with growing knowledge management needs, enterprises are increasingly realizing the importance of interlinking critical business information distributed across structured and unstructured data sources. We present a novel system, called EROCS, for linking a given text document with relevant structured data. EROCS views the structured data as a predefined set of "entities" and identifies the entities that best match the given document. EROCS also embeds the identified entities in the document, effectively creating links between the structured data and segments within the document. Unlike prior approaches, EROCS identifies such links even when the relevant entity is not explicitly mentioned in the document. EROCS uses an efficient algorithm that performs this task keeping the amount of information retrieved from the database at a minimum. Our evaluation shows that EROCS achieves high accuracy with reasonable overheads.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {667–678},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164186,
author = {Dragut, Eduard C. and Yu, Clement and Meng, Weiyi},
title = {Meaningful Labeling of Integrated Query Interfaces},
year = {2006},
publisher = {VLDB Endowment},
abstract = {The contents of Web databases are accessed through queries formulated on complex user interfaces. In many domains of interest (e.g. Auto) users are interested in obtaining information from alternative sources. Thus, they have to access many individual Web databases via query interfaces. We aim to construct automatically a well-designed query interface that integrates a set of interfaces in the same domain. This will permit users to access information uniformly from multiple sources. Earlier research in this area includes matching attributes across multiple query interfaces in the same domain and grouping related attributes. In this paper, we investigate the naming of the attributes in the integrated query interface. We provide a set of properties which are required in order to have consistent labels for the attributes within an integrated interface so that users have no difficulty in understanding it. Based on these properties, we design algorithms to systematically label the attributes. Experimental results on seven domains validate our theoretical study. In the process of naming attributes, a set of logical inference rules among the textual labels is discovered. These inferences are also likely to be applicable to other integration problems sensitive to naming: e.g., HTML forms, HTML tables or concept hierarchies in the semantic Web.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {679–690},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164187,
author = {Sismanis, Yannis and Brown, Paul and Haas, Peter J. and Reinwald, Berthold},
title = {GORDIAN: Efficient and Scalable Discovery of Composite Keys},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Identification of (composite) key attributes is of fundamental importance for many different data management tasks such as data modeling, data integration, anomaly detection, query formulation, query optimization, and indexing. However, information about keys is often missing or incomplete in many real-world database scenarios. Surprisingly, the fundamental problem of automatic key discovery has received little attention in the existing literature. Existing solutions ignore composite keys, due to the complexity associated with their discovery. Even for simple keys, current algorithms take a brute-force approach; the resulting exponential CPU and memory requirements limit the applicability of these methods to small datasets. In this paper, we describe GORDIAN, a scalable algorithm for automatic discovery of keys in large datasets, including composite keys. GORDIAN can provide exact results very efficiently for both real-world and synthetic datasets. GORDIAN can be used to find (composite) key attributes in any collection of entities, e.g., key column-groups in relational data, or key leaf-node sets in a collection of XML documents with a common schema. We show empirically that GORDIAN can be combined with sampling to efficiently obtain high quality sets of approximate keys even in very large datasets.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {691–702},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164188,
author = {Lau, Edmond and Madden, Samuel},
title = {An Integrated Approach to Recovery and High Availability in an Updatable, Distributed Data Warehouse},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Any highly available data warehouse will use some form of data replication to tolerate machine failures. In this paper, we demonstrate that we can leverage this data redundancy to build an integrated approach to recovery and high availability. Our approach, called HARBOR, revives a crashed site by querying remote, online sites for missing updates and uses timestamps to determine which tuples need to be copied or updated. HARBOR does not require a stable log, recovers without quiescing the system, allows replicated data to be stored non-identically, and is simpler than a log-based recovery algorithm.We compare the runtime overhead and recovery performance of HARBOR to those of two-phase commit and ARIES, the gold standard for log-based recovery, on a three-node distributed database system. Our experiments demonstrate that HARBOR suffers lower runtime overhead, has recovery performance comparable to ARIES's, and can tolerate the fault of a worker and efficiently bring it back online.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {703–714},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164189,
author = {Daudjee, Khuzaima and Salem, Kenneth},
title = {Lazy Database Replication with Snapshot Isolation},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Snapshot isolation is a popular transactional isolation level in database systems. Several replication techniques based on snapshot isolation have recently been proposed. These proposals, however, do not fully leverage the local concurrency controls that provide snapshot isolation. Furthermore, guaranteeing snapshot isolation in lazy replicated systems may result in transaction inversions, which happen when transactions see stale data. Strong snapshot isolation, which is provided in centralized database servers, avoids transaction inversions but is expensive to provide in a lazy replicated system. In this paper, we show how snapshot isolation can be maintained in lazy replicated systems while taking full advantage of the local concurrency controls. We propose strong session snapshot isolation, a correctness criterion that prevents transaction inversions. We show how strong session snapshot isolation can be implemented efficiently in a lazy replicated database system. Through performance studies, we quantify the cost of implementing our techniques in lazy replicated systems.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {715–726},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164190,
author = {Narayanan, Dushyanth and Donnelly, Austin and Mortier, Richard and Rowstron, Antony},
title = {Delay Aware Querying with Seaweed},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Large highly distributed data sets are poorly supported by current query technologies. Applications such as endsystem-based network management are characterized by data stored on large numbers of endsystems, with frequent local updates and relatively infrequent global one-shot queries.The challenges are scale (103 to 109 endsystems)and endsystem unavailability. In such large systems, a significant fraction of endsystems and their data will be unavailable at any given time. Existing methods to provide high data availability despite endsystem unavailability involve centralizing, redistributing or replicating the data. At large scale these methods are not scalable.We advocate a design that trades query delay for completeness, incrementally returning results as endsystems become available. We also introduce the idea of completeness prediction, which provides the user with explicit feedback about this delay/completeness trade-off. Completeness prediction is based on replication of compact data summaries and availability models. This metadata is orders of magnitude smaller than the data.Seaweed is a scalable query infrastructure supporting incremental results, online in-network aggregation and completeness prediction. It is built on a distributed hash table (DHT) but unlike previous DHT based approaches it does not redistribute data across the network. It exploits the DHT infrastructure for failure resilient metadata replication, query dissemination, and result aggregation. We analytically compare Seaweed's scalability against other approaches and also evaluate the Seaweed prototype running on a large-scale network simulator driven by real-world traces.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {727–738},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164191,
author = {Cohen, Sara and Fadida, Itzhak and Kanza, Yaron and Kimelfeld, Benny and Sagiv, Yehoshua},
title = {Full Disjunctions: Polynomial-Delay Iterators in Action},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Full disjunctions are an associative extension of the outer-join operator to an arbitrary number of relations. Their main advantage is the ability to maximally combine data from different relations while preserving all the original information. An algorithm for efficiently computing full disjunctions is presented. This algorithm is superior to previous ones in three ways. First, it is the first algorithm that computes a full disjunction with a polynomial delay between tuples. Hence, it can be implemented as an iterator that produces a stream of tuples, which is important in many cases (e.g., pipelined query processing and Web applications). Second, the total runtime is linear in the size of the output. Third, the algorithm employs a novel optimization that divides the relation schemes into biconnected components, uses a separate iterator for each component and applies outerjoins whenever possible. Combining efficiently full disjunctions with standard SQL operators is discussed. Experiments show the superiority of our algorithm over the state of the art.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {739–750},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164192,
author = {Sharifzadeh, Mehdi and Shahabi, Cyrus},
title = {The Spatial Skyline Queries},
year = {2006},
publisher = {VLDB Endowment},
abstract = {In this paper, for the first time, we introduce the concept of Spatial Skyline Queries (SSQ). Given a set of data points P and a set of query points Q each data point has a number of derived spatial attributes each of which is the point's distance to a query point. An SSQ retrieves those points of P which are not dominated by any other point in P considering their derived spatial attributes. The main difference with the regular skyline query is that this spatial domination depends on the location of the query points Q SSQ has application in several domains such as emergency response and online maps. The main intuition and novelty behind our approaches is that we exploit the geometric properties of the SSQ problem space to avoid the exhaustive examination of all the point pairs in P and Q. Consequently, we reduce the complexity of SSQ search from O(|P|2|Q|) to O(|S|2|C|+√|P|), where |S| and |C| are the solution size and the number of vertices of the convex hull of Q, respectively.We propose two algorithms, B2S2 and VS2, for static query points and one algorithm, VCS2, for streaming Q whose points change location over time (e.g., are mobile). VCS2 exploits the pattern of change in Q to avoid unnecessary re-computation of the skyline and hence efficiently perform updates. Our extensive experiments using real-world datasets verify that both R-tree-based B2S2 and Voronoi-based VS2 out perform the best competitor approach in terms of processing time by a wide margin (4-6 times better in most cases).},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {751–762},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164193,
author = {Mokbel, Mohamed F. and Chow, Chi-Yin and Aref, Walid G.},
title = {The New Casper: Query Processing for Location Services without Compromising Privacy},
year = {2006},
publisher = {VLDB Endowment},
abstract = {This paper tackles a major privacy concern in current location-based services where users have to continuously report their locations to the database server in order to obtain the service. For example, a user asking about the nearest gas station has to report her exact location. With untrusted servers, reporting the location information may lead to several privacy threats. In this paper, we present Casper1; a new framework in which mobile and stationary users can entertain location-based services without revealing their location information. Casper consists of two main components, the location anonymizer and the privacy-aware query processor. The location anonymizer blurs the users' exact location information into cloaked spatial regions based on user-specified privacy requirements. The privacy-aware query processor is embedded inside the location-based database server in order to deal with the cloaked spatial areas rather than the exact location information. Experimental results show that Casper achieves high quality location-based services while providing anonymity for both data and queries.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {763–774},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164194,
author = {Xing, Ying and Hwang, Jeong-Hyon and \c{C}etintemel, Uundefinedur and Zdonik, Stan},
title = {Providing Resiliency to Load Variations in Distributed Stream Processing},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Scalability in stream processing systems can be achieved by using a cluster of computing devices. The processing burden can, thus, be distributed among the nodes by partitioning the query graph. The specific operator placement plan can have a huge impact on performance. Previous work has focused on how to move query operators dynamically in reaction to load changes in order to keep the load balanced. Operator movement is too expensive to alleviate short-term bursts; moreover, some systems do not support the ability to move operators dynamically. In this paper, we develop algorithms for selecting an operator placement plan that is resilient to changes in load. In other words, we assume that operators cannot move, therefore, we try to place them in such a way that the resulting system will be able to withstand the largest set of input rate combinations. We call this a resilient placement.This paper first formalizes the problem for operators that exhibit linear load characteristics (e.g., filter, aggregate), and introduces a resilient placement algorithm. We then show how we can extend our algorithm to take advantage of additional workload information (such as known minimum input stream rates). We further show how this approach can be extended to operators that exhibit non-linear load characteristics (e.g., join). Finally, we present prototype- and simulation-based experiments that quantify the benefits of our approach over existing techniques using real network traffic traces.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {775–786},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164195,
author = {Tu, Yi-Cheng and Liu, Song and Prabhakar, Sunil and Yao, Bin},
title = {Load Shedding in Stream Databases: A Control-Based Approach},
year = {2006},
publisher = {VLDB Endowment},
abstract = {In Data Stream Management Systems (DSMSs), query processing has to meet various Quality-of-Service (QoS) requirements. In many data stream applications, processing delay is the most critical quality requirement since the value of query results decreases dramatically over time. The ability to remain within a desired level of delay is significantly hampered under situations of overloading, which are common in data stream systems. When overloaded, DSMSs employ load shedding in order to meet quality requirements and keep pace with the high rate of data arrivals. Data stream applications are extremely dynamic due to bursty data arrivals and time-varying data processing costs. Current approaches ignore system status information in decision-making and consequently are unable to achieve desired control of quality under dynamic load. In this paper, we present a quality management framework that leverages well studied feedback control techniques. We discuss the design and implementation of such a framework in a real DSMS - the Borealis stream manager. Our data management framework is built on the advantages of system identification and rigorous controller analysis. Experimental results show that our solution achieves significantly fewer QoS (delay) violations with the same or lower level of data loss, as compared to current strategies utilized in DSMSs. It is also robust and bears negligible computational overhead.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {787–798},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164196,
author = {Tatbul, Nesime and Zdonik, Stan},
title = {Window-Aware Load Shedding for Aggregation Queries over Data Streams},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Data stream management systems may be subject to higher input rates than their resources can handle. When overloaded, the system must shed load in order to maintain low-latency query results. In this paper, we describe a load shedding technique for queries consisting of one or more aggregate operators with sliding windows. We introduce a new type of drop operator, called a "Window Drop". This operator is aware of the window properties (i.e., window size and window slide) of its downstream aggregate operators in the query plan. Accordingly, it logically divides the input stream into windows and probabilistically decides which windows to drop. This decision is further encoded into tuples by marking the ones that are disallowed from starting new windows. Unlike earlier approaches, our approach preserves integrity of windows throughout a query plan, and always delivers subsets of original query answers with minimal degradation in result quality.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {799–810},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164197,
author = {Ji, Liping and Tan, Kian-Lee and Tung, Anthony K. H.},
title = {Mining Frequent Closed Cubes in 3D Datasets},
year = {2006},
publisher = {VLDB Endowment},
abstract = {In this paper, we introduce the concept of frequent closed cube (FCC), which generalizes the notion of 2D frequent closed pattern to 3D context. We propose two novel algorithms to mine FCCs from 3D datasets. The first scheme is a Representative Slice Mining (RSM) framework that can be used to extend existing 2D FCP mining algorithms for FCC mining. The second technique, called CubeMiner, is a novel algorithm that operates on the 3D space directly. We have implemented both schemes, and evaluated their performance on both real and synthetic datasets. The experimental results show that the RSM-based scheme is efficient when one of the dimensions is small, while CubeMiner is superior otherwise.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {811–822},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164198,
author = {Lee, Ki Yong and Kim, Myoung Ho},
title = {Efficient Incremental Maintenance of Data Cubes},
year = {2006},
publisher = {VLDB Endowment},
abstract = {The data cube provides users with aggregated results that are group-bys for all possible combinations of dimension attributes. When the number of dimension attributes is n, the data cube computes 2n group-bys, each of which is called a cuboid. A data cube is often precomputed and stored as materialized views in data warehouses. The data cube needs to be updated when source relations change. The incremental maintenance of a data cube is to compute and propagate only changes of source relations rather than recompute the entire data cube from the source relations.To maintain a data cube incrementally, previous methods compute a delta cube which represents the change of the data cube. We call a cuboid in a delta cube a delta cuboid. For a data cube with 2n cuboids, a delta cube consists of 2n delta cuboids. Thus, as the number of dimension attributes increases, the cost of computing the delta cube increases significantly. In this paper, we propose an incremental maintenance method for data cubes that can maintain a data cube by using only (n ⌈n/2⌉) delta cuboids. As a result, the cost of computing delta cuboids is substantially reduced. Through various experiments, we show the performance advantages of our method over the previous methods.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {823–833},
numpages = {11},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164199,
author = {Gonzalez, Hector and Han, Jiawei and Li, Xiaolei},
title = {Flowcube: Constructing RFID Flowcubes for Multi-Dimensional Analysis of Commodity Flows},
year = {2006},
publisher = {VLDB Endowment},
abstract = {With the advent of RFID (Radio Frequency Identication) technology, manufacturers, distributors, and retailers will be able to track the movement of individual objects throughout the supply chain. The volume of data generated by a typical RFID application will be enormous as each item will generate a complete history of all the individual locations that it occupied at every point in time, possibly from a specific production line at a given factory, passing through multiple warehouses, and all the way to a particular checkout counter in a store. The movement trails of such RFID data form gigantic commodity flowgraph representing the locations and durations of the path stages traversed by each item. This commodity flow contains rich multi-dimensional information on the characteristics, trends, changes and outliers of commodity movements.In this paper, we propose a method to construct a warehouse of commodity flows, called flowcube. As in standard OLAP, the model will be composed of cuboids that aggregate item flows at a given abstraction level. The flowcube differs from the traditional data cube in two major ways. First, the measure of each cell will not be a scalar aggregate but a commodity flowgraph that captures the major movement trends and significant deviations of the items aggregated in the cell. Second, each flowgraph itself can be viewed at multiple levels by changing the level of abstraction of path stages. In this paper, we motivate the importance of the model, and present an efficient method to compute it by (1) performing simultaneous aggregation of paths to all interesting abstraction levels, (2) pruning low support path segments along the item and path stage abstraction lattices, and (3) compressing the cube by removing rarely occurring cells, and cells whose commodity flows can be inferred from higher level cells.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {834–845},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164206,
author = {Arasu, Arvind and Ganti, Venkatesh and Kaushik, Raghav},
title = {Efficient Exact Set-Similarity Joins},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Given two input collections of sets, a set-similarity join (SSJoin) identifies all pairs of sets, one from each collection, that have high similarity. Recent work has identified SSJoin as a useful primitive operator in data cleaning. In this paper, we propose new algorithms for SSJoin. Our algorithms have two important features: They are exact, i.e., they always produce the correct answer, and they carry precise performance guarantees. We believe our algorithms are the first to have both features; previous algorithms with performance guarantees are only probabilistically approximate. We demonstrate the effectiveness of our algorithms using a thorough experimental evaluation over real-life and synthetic data sets.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {918–929},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164207,
author = {Moerkotte, Guido and Neumann, Thomas},
title = {Analysis of Two Existing and One New Dynamic Programming Algorithm for the Generation of Optimal Bushy Join Trees without Cross Products},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Two approaches to derive dynamic programming algorithms for constructing join trees are described in the literature. We show analytically and experimentally that these two variants exhibit vastly diverging runtime behaviors for different query graphs. More specifically, each variant is superior to the other for one kind of query graph (chain or clique), but fails for the other. Moreover, neither of them handles star queries well. This motivates us to derive an algorithm that is superior to the two existing algorithms because it adapts to the search space implied by the query graph.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {930–941},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164208,
author = {Cali, Andrea and Kifer, Michael},
title = {Containment of Conjunctive Object Meta-Queries},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We consider the problem of query containment over an object data model derived from F-logic. F-logic has generated considerable interest commercially, in the academia, and within various standardization efforts as a means for building ontologies and for reasoning on the Semantic Web. Solution to the containment problem for F-logic queries can help with query optimization as well as the classification problem in information integration systems. An important property of F-logic queries, which sets them apart from database queries, is that they can mix the data-level and the meta-level in simple and useful ways. This means that such queries may refer not only to data but also schema information. To the best of our knowledge, the containment problem for such queries has not been considered in the literature. We show that, even for queries over meta-information together with data, this problem is decidable for non-recursive conjunctive queries. We also provide relevant complexity results.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {942–952},
numpages = {11},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164212,
author = {Zhao, Hongkun and Meng, Weiyi and Yu, Clement},
title = {Automatic Extraction of Dynamic Record Sections from Search Engine Result Pages},
year = {2006},
publisher = {VLDB Endowment},
abstract = {A search engine returned result page may contain search results that are organized into multiple dynamically generated sections in response to a user query. Furthermore, such a result page often also contains information irrelevant to the query, such as information related to the hosting site of the search engine. In this paper, we present a method to automatically generate wrappers for extracting search result records from all dynamic sections on result pages returned by search engines. This method has the following novel features: (1) it aims to explicitly identify all dynamic sections, including those that are not seen on sample result pages used to generate the wrapper, and (2) it addresses the issue of correctly differentiating sections and records. Experimental results indicate that this method is very promising. Automatic search result record extraction is critical for applications that need to interact with search engines such as automatic construction and maintenance of metasearch engines and deep Web crawling.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {989–1000},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164213,
author = {Mitra, Soumyadeb and Hsu, Windsor W. and Winslett, Marianne},
title = {Trustworthy Keyword Search for Regulatory-Compliant Records Retention},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Recent litigation and intense regulatory focus on secure retention of electronic records have spurred a rush to introduce Write-Once-Read-Many (WORM) storage devices for retaining business records such as electronic mail. However, simply storing records in WORM storage is insuffcient to ensure that the records are trustworthy, i.e., able to provide irrefutable proof and accurate details of past events. Specifically, some form of index is needed for timely access to the records, but unless the index is maintained securely, the records can in effect be hidden or altered, even if stored in WORM storage. In this paper, we systematically analyze the requirements for establishing a trustworthy inverted index to enable keyword-based search queries. We propose a novel scheme for effcient creation of such an index and demonstrate, through extensive simulations and experiments with an enterprise keyword search engine, that the scheme can achieve online update speeds while maintaining good query performance. In addition, we present a secure index structure for multi-keyword queries that supports insert, lookup and range queries in time logarithmic in the number of documents.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1001–1012},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164214,
author = {Luo, Gang},
title = {Efficient Detection of Empty-Result Queries},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Frequently encountered in query processing, empty query results usually do not provide users with much useful information. Yet, users might still have to wait for a long time before they disappointingly realize that their results are empty. To significantly reduce such unfavorable delays, in this paper, we propose a novel method to quickly detect, without actual execution, those queries that will return empty results. Our key idea is to remember and reuse the results from previously-executed, empty-result queries. These results are stored in the form of so-called atomic query parts so that the (partial) results from multiple queries can be combined together to handle a new query without incurring much overhead. To increase our chance of detecting empty-result queries with only a limited storage, our method (1) stores the most "valuable" information about empty-result queries, (2) removes redundant information among different empty-result queries, (3) continuously updates the stored information to adapt to the current query pattern, and (4) utilizes a set of special properties of empty results. We evaluate the efficiency of our method through a theoretical analysis and an initial implementation in PostgreSQL. The results show that our method has low overhead and can often successfully avoid executing empty-result queries.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1015–1025},
numpages = {11},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164215,
author = {Ahmed, Rafi and Lee, Allison and Witkowski, Andrew and Das, Dinesh and Su, Hong and Zait, Mohamed and Cruanes, Thierry},
title = {Cost-Based Query Transformation in Oracle},
year = {2006},
publisher = {VLDB Endowment},
abstract = {This paper describes cost-based query transformation in Oracle relational database system, which is a novel phase in query optimization. It discusses a suite of heuristic- and cost-based transformations performed by Oracle. It presents the framework for cost-based query transformation, the need for such a framework, possible interactions among some of the transformation, and efficient algorithms for enumerating the search space of cost-based transformations. It describes a practical technique to combine cost-based transformations with a traditional physical optimizer. Some of the challenges of cost-based transformation are highlighted. Our experience shows that some transformations when performed in a cost-based manner lead to significant execution time improvements.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1026–1036},
numpages = {11},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164216,
author = {Borkar, Vinayak and Carey, Michael and Lychagin, Dmitry and Westmann, Till and Engovatov, Daniel and Onose, Nicola},
title = {Query Processing in the Aqualogic Data Services Platform},
year = {2006},
publisher = {VLDB Endowment},
abstract = {BEA recently introduced a new middleware product called the Aqua-Logic Data Services Platform (ALDSP). The purpose of ALDSP is to make it easy to design, develop, deploy, and maintain a data services layer in the world of service-oriented architecture (SOA). ALDSP provides a declarative foundation for building SOA applications and services that need to access and compose information from a range of enterprise data sources; this foundation is based on XML, XML Schema, and XQuery. This paper focuses on query processing in ALDSP, describing its overall query processing architecture, its query compiler and runtime system, its distributed query processing techniques, the translation of XQuery plan fragments into SQL when relational data sources are involved, and the production of lineage information to support updates. Several XQuery extensions that were added in support of requirements related to data services are also covered.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1037–1048},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164217,
author = {Nambiar, Raghunath Othayoth and Poess, Meikel},
title = {The Making of TPC-DS},
year = {2006},
publisher = {VLDB Endowment},
abstract = {For the last decade, the research community and the industry have used TPC-D and its successor TPC-H to evaluate performance of decision support technology. Recognizing a paradigm shift in the industry the Transaction Processing Performance Council has developed a new Decision Support benchmark, TPC-DS, expected to be released this year. From an ease of benchmarking perspective it is similar to past benchmarks. However, it adjusts for new technology and new approaches the industry has embarked on in recent years. This paper describes the main characteristics of TPC-DS, explains why some of the key decisions were made and which performance aspects of decision support system it measures.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1049–1058},
numpages = {10},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164218,
author = {Legler, Thomas and Lehner, Wolfgang and Ross, Andrew},
title = {Data Mining with the SAP NetWeaver BI Accelerator},
year = {2006},
publisher = {VLDB Endowment},
abstract = {The new SAP NetWeaver Business Intelligence accelerator is an engine that supports online analytical processing. It performs aggregation in memory and in query runtime over large volumes of structured data. This paper first briefly describes the accelerator and its main architectural features, and cites test results that indicate its power. Then it describes in detail how the accelerator may be used for data mining. The accelerator can perform data mining in the same large repositories of data and using the same compact index structures that it uses for analytical processing. A first such implementation of data mining is described and the results of a performance evaluation are presented. Association rule mining in a distributed architecture was implemented with a variant of the BUC iceberg cubing algorithm. Test results suggest that useful online mining should be possible with wait times of less than 60 seconds on business data that has not been preprocessed.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1059–1068},
numpages = {10},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164219,
author = {Haustein, Michael and H\"{a}rder, Theo and Luttenberger, Konstantin},
title = {Contest of XML Lock Protocols},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We explore and compare the performance behavior of lock protocols to be used in XML DBMSs (XDBMSs, for short) supporting typical XML document processing interfaces. In this paper, we outline 11 protocols proposed in the literature, highlight essential implementation concepts of our XDBMS and realize all of them in the same DBMS environment using so-called meta-synchronization. We design a framework for XML benchmarks including read and update transactions, run extensive empirical experiments which focus on the locking performance, and compare the results using various performance metrics. As a consequence, we can propose a group of protocols which won this practical contest under identical conditions.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1069–1080},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164220,
author = {Storm, Adam J. and Garcia-Arellano, Christian and Lightstone, Sam S. and Diao, Yixin and Surendra, M.},
title = {Adaptive Self-Tuning Memory in DB2},
year = {2006},
publisher = {VLDB Endowment},
abstract = {DB2 for Linux, UNIX, and Windows Version 9.1 introduces the Self-Tuning Memory Manager (STMM), which provides adaptive self tuning of both database memory heaps and cumulative database memory allocation. This technology provides state-of-the-art memory tuning combining control theory, runtime simulation modeling, cost-benefit analysis, and operating system resource analysis. In particular, the nove use of cost-benefit analysis and control theory techniques makes STMM a breakthrough technology in database memory management. The cost-benefit analysis allows STMM to tune memory between radically different memory consumers such as compiled statement cache, sort, and buffer pools. These methods allow for the fast convergence of memory settings while also providing stability in the presence of system noise. The tuning mode has been found in numerous experiments to tune memory allocation as well as expert human administrators, including OLTP, DSS, and mixed environments. We believe this is the first known use of cost-benefit analysis and control theory in database memory tuning across heterogeneous memory consumers.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1081–1092},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164221,
author = {Steinle, Mirko and Aberer, Karl and Girdzijauskas, Sarunas and Lovis, Christian},
title = {Mapping Moving Landscapes by Mining Mountains of Logs: Novel Techniques for Dependency Model Generation},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Problem diagnosis for distributed systems is usually difficult. Thus, an automated support is needed to identify root causes of encountered problems such as performance lags or inadequate functioning quickly. The many tools and techniques existing today that perform this task rely usually on some dependency model of the system. However, in complex and fast evolving environments it is practically unfeasible to keep such a model up-to-date manually and it has to be created in an automatic manner. For high level objects this is in itself a challenging and less studied task. In this paper, we propose three different approaches to discover dependencies by mining system logs. Our work is inspired by a recently developed data mining algorithm and techniques for collocation extraction from the natural language processing field. We evaluate the techniques in a case study for Geneva University Hospitals (HUG) and perform large-scale experiments on production data. Results show that all techniques are capable of finding useful dependency information with reasonable precision in a real-world environment.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1093–1102},
numpages = {10},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164225,
author = {Liu, Zhen Hua and Novoselsky, Agnuel},
title = {Efficient XSLT Processing in Relational Database System},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Efficient processing of XQuery, XPath and SQL/XML on XML documents stored and managed in RDBMS has been widely studied. However, much less of such type of work has been done for efficient XSLT processing of XML documents stored and managed by the database. This is partially due to the observation that the rule based template driven XSLT execution model does not fit nicely with the traditional declarative query language processing model which leverages index probing and iterator based pull mode that can be scaled to handle large size data. In this paper, we share our experience of efficient processing of XSLT in Oracle XML DB. We present the technique of processing XSLT efficiently in database by rewriting XSLT stylesheets into highly efficient XQuery through partially evaluating XSLT over the XML documents structural information. Consequently, we can leverage all the work done for efficient XQuery/XPath processing in database to achieve combined optimisations of XSLT with XQuery/XPath and SQL/XML in Oracle XMLDB. This effectively makes XSLT processing scale to large size XML documents using classical declarative query processing techniques in DBMS.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1106–1116},
numpages = {11},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164226,
author = {Balmin, Andrey and Beyer, Kevin S. and \"{O}zcan, Fatma and Nicola, Matthias},
title = {On the Path to Efficient XML Queries},
year = {2006},
publisher = {VLDB Endowment},
abstract = {XQuery and SQL/XML are powerful new languages for querying XML data. However, they contain a number of stumbling blocks that users need to be aware of to get the expected results and performance. For example, certain language features make it hard if not impossible to exploit XML indexes.The major database vendors provide XQuery and SQL/XML support in their current or upcoming product releases. In this paper, we identify common pitfalls gleaned from the experiences of early adopters of this functionality. We illustrate these pitfalls through concrete examples, explain the unexpected query behavior, and show alternative formulations of the queries that behave and perform as anticipated. As results we provide guidelines for XQuery and SQL/XML users, feedback on the language standards, and food for thought for emerging languages and APIs.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1117–1128},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164227,
author = {M\"{u}lle, Jutta A. and B\"{o}hm, Klemens and R\"{o}per, Nicolas and S\"{u}nder, Tobias},
title = {Building Conference Proceedings Requires Adaptable Workflow and Content Management},
year = {2006},
publisher = {VLDB Endowment},
abstract = {ProceedingsBuilder is a system that helps the proceedings chair of a scientific conference to carry out his chores. It has features of both workflow management systems (WFMS) and content management systems (CMS), in order to collect the material for the printed proceedings and other products. ProceedingsBuilder has been operational at several conferences, including VLDB 2005. When using Proceedings-Builder, we had a very intense lesson which kinds of work-flow adaptations may become necessary. Existing WFMS do not offer support for most of them. The concern of this article is to describe and classify these various requirements regarding adaptation. ProceedingsBuilder is an example of a broad class of systems, namely editorial systems that collect content in order to publish it. Our findings are of interest to a broader audience, not only to conference organizers.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1129–1139},
numpages = {11},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164228,
author = {Cha, Sang K. and Anandan, P. and Hsu, Meichun and Mohan, C. and Rastogi, Rajeev and Sikka, Vishal and Young, Honesty},
title = {Globalization: Challenges to Database Community},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Globalization is flattening the world. As database researcher, we are proud that information technology is a critical enabler of globalization. At the same time, we are seeing that research and development of information technology is also being globalized.In recent years, many R&amp;D labs were established in Asia, especially, in India and China, by global IT companies. Some of our colleagues have moved with globalization to establish new labs or to lead R&amp;D in newly established labs. For those who have not moved physically, it is common to work with colleagues at remote labs with time difference.The objective of this panel is to invite pioneers leading R&amp;D globalization and to share their vision and challenges, and to discuss how globalization impacts the future of database and information management research, education, and industry.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1140},
numpages = {1},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164229,
author = {Wang, Shan and Peng, Zhaohui and Zhang, Jun and Qin, Lu and Wang, Sheng and Yu, Jeffrey Xu and Ding, Bolin},
title = {NUITS: A Novel User Interface for Efficient Keyword Search over Databases},
year = {2006},
publisher = {VLDB Endowment},
abstract = {The integration of database and information retrieval techniques provides users with a wide range of high quality services. We present a prototype system, called NUITS, for efficiently processing keyword queries on top of a relational database. Our NUITS allows users to issue simple keyword queries as well as advanced keyword queries with conditions. The efficiency of keyword query processing and the user-friendly result display will also be addressed in this paper.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1143–1146},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164230,
author = {Mohan, Sriram and Wu, Yuqing},
title = {IPAC: An Interactive Approach to Access Control for Semi-Structured Data},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We propose IPAC(Interactive aPproach to Access Control for semi-structured data), a framework for XML access constraint specification and security view selection. IPAC clearly demarcates access constraint specification, access control strategy and security mechanism (implementation). It features a declarative access constraint specification language, a global access control strategy configuration unit, and an automatic security view generation and ranking tool. IPAC is the first system that assists the DBA in specifying access control strategies and access constraints on XML data, and helps the DBA in choosing the optimal plan that implements the specified strategy and access constraints accurately and efficiently.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1147–1150},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164231,
author = {Agrawal, Parag and Benjelloun, Omar and Sarma, Anish Das and Hayworth, Chris and Nabar, Shubha and Sugihara, Tomoe and Widom, Jennifer},
title = {Trio: A System for Data, Uncertainty, and Lineage},
year = {2006},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1151–1154},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164232,
author = {Barioni, Maria Camila N. and Razente, Humberto and Traina, Agma and Traina, Caetano},
title = {SIREN: A Similarity Retrieval Engine for Complex Data},
year = {2006},
publisher = {VLDB Endowment},
abstract = {This paper presents a similarity retrieval engine - SIREN-that allows posing similarity queries in a relational DBMS using an extended syntax that adds the support for such type of queries in the SQL language. It discusses the main architecture of SIREN, describes some key features and provides a description of the demo.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1155–1158},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164233,
author = {Spiegel, Joshua and Pontikakis, Emmanuel and Budalakoti, Suratna and Polyzotis, Neoklis},
title = {AQAX: A System for Approximate XML Query Answers},
year = {2006},
publisher = {VLDB Endowment},
abstract = {On-line, interactive exploration of large databases becomes prohibitively expensive as the size of the database grows. Approximate query answering offers a cost-effective solution to this problem, by enabling the fast generation of approximate results based on concise data summaries. We apply this paradigm in the context of XML databases, where the increased complexity of data and queries amplifies the challenges behind interactive exploration. We have developed an on-line XML exploration system, termed AQAX that relies on accurate XML summaries in order to enable the rapid exploration of large data sets. To effectively support the exploration of semi-structured query answers, our system employs a tight coupling between the main query processor and the graphical clients that visualize the results. This demonstration will showcase the functionality of our system and the effectiveness of approximate query answering in the context of XML databases.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1159–1162},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164234,
author = {Joshi, Dhiraj and Datta, Ritendra and Zhuang, Ziming and Weiss, W. P. and Friedenberg, Marc and Li, Jia and Wang, James Z.},
title = {PARAgrab: A Comprehensive Architecture for Web Image Management and Multimodal Querying},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We demonstrate PARAgrab - a scalable Web image archival, retrieval, and annotation system that supports multiple querying modalities. The underlying architecture of our large-scale Web image database is described. Querying and visualization techniques used in the system are explained.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1163–1166},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164235,
author = {Bernstein, Philip A. and Melnik, Sergey and Churchill, John E.},
title = {Incremental Schema Matching},
year = {2006},
publisher = {VLDB Endowment},
abstract = {The goal of schema matching is to identify correspondences between the elements of two schemas. Most schema matching systems calculate and display the entire set of correspondences in a single shot. Invariably, the result presented to the engineer includes many false positives, especially for large schemas. The user is often overwhelmed by all of the edges, annoyed by the false positives, and frustrated at the inability to see second- and third-best choices. We demonstrate a tool that circumvents these problems by doing the matching interactively. The tool suggests candidate matches for a selected schema element and allows convenient navigation between the candidates. The ranking of match candidates is based on lexical similarity, schema structure, element types, and the history of prior matching actions. The technical challenges are to make the match algorithm fast enough for incremental matching in large schemas and to devise a user interface that avoids overwhelming the user. The tool has been integrated with a prototype version of Microsoft BizTalk Mapper, a visual programming tool for generating XML-to-XML mappings.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1167–1170},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164236,
author = {Yuan, Jun and Bahrami, Ali and Wang, Changzhou and Murray, Marie and Hunt, Anne},
title = {A Semantic Information Integration Tool Suite},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We describe a prototype software tool suite for semantic information integration; it has the following features. First, it can import local metadata as well as a domain ontology. Imported metadata is stored persistently in an ontological format. Second, it provides a semantic query facility that allows users to retrieve information across multiple data sources using the domain ontology directly. Third, it has a GUI for users to define mappings between the local metadata and the domain ontology. Fourth, it incorporates a novel mechanism to improve system reliability by dynamically adapting query execution upon detecting various types of environmental changes. In addition, this tool suite is compatible with W3C Semantic Web specifications such as RDF and OWL. It also uses the query engine of Commercial EII products for low level query processing.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1171–1174},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164237,
author = {Kache, Holger and Han, Wook-Shin and Markl, Volker and Raman, Vijayshankar and Ewen, Stephan},
title = {POP/FED: Progressive Query Optimization for Federated Queries in DB2},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Federated queries are regular relational queries accessing data on one or more remote relational or non-relational data sources, possibly combining them with tables stored in the federated DBMS server. Their execution is typically divided between the federated server and the remote data sources. Outdated and incomplete statistics have a bigger impact on federated DBMS than on regular DBMS, as maintenance of federated statistics is unequally more complicated and expensive than the maintenance of the local statistics; consequently bad performance commonly occurs for federated queries due to the selection of a suboptimal query plan. To solve this problem we propose a progressive optimization technique for federated queries called POP/FED by extending the state of the art for progressive reoptimization for local source queries, POP [4]. POP/FED uses (a) an opportunistic, but risk controlled reoptimization technique for federated DBMS, (b) a technique for multiple reoptimizations during federated query processing with a strategy to discover redundant and eliminate partial results, and (c) a mechanism to eagerly procure statistics in a federated environment. In this demonstration we showcase POP/FED implemented in a prototype version of WebSphere Information Integrator for DB2 using the TPC-H benchmark database and its workload. For selected queries of the workload we show unique features including multi-round reoptimizations using both a new graphical reoptimization progress monitor POPMonitor and the DB2 graphical plan explain tool.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1175–1178},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164238,
author = {Alexe, Bogdan and Chiticariu, Laura and Tan, Wang-Chiew},
title = {SPIDER: A Schema MapPIng DEbuggeR},
year = {2006},
publisher = {VLDB Endowment},
abstract = {A schema mapping is a high-level declarative specification of how data structured under one schema, called the source schema, is to be transformed into data structured under a possibly different schema, called the target schema. We demonstrate SPIDER, a prototype tool for debugging schema mappings, where the language for specifying schema mappings is based on a widely adopted formalism. We have built SPIDER on top of a data exchange system, Clio, from IBM Almaden Research Center. At the heart of SPIDER is a data-driven facility for understanding a schema mapping through the display of routes. A route essentially describes the relationship between source and target data with the schema mapping. In this demonstration, we showcase our route engine, where we can display one or all routes starting from either source or target data, as well as the intermediary data and schema elements involved. In addition, we demonstrate "standard" debugging features for schema mappings that we have also built, such as computing and exploring routes step-by-step, stopping or pausing the computation with breakpoints, performing "guided" computation of routes by taking human input into account, as well as tracking the state of the target instance during the process of computing routes.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1179–1182},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164239,
author = {Chen, Li and Martone, Maryann and Gupta, Amarnath and Fong, Lisa and Wong-Barnum, Mona},
title = {OntoQuest: Exploring Ontological Data Made Easy},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Recently, there is a large demand by many scientific applications for managing, querying and reasoning ontology concepts and instances. We demonstrate OntoQuest, a system that provides powerful yet easy-to-use query and reasoning utilities by which the ontological data exploration experience is made easy. Even without any knowledge of ontology query languages, one can easily get a hands-on ontological data exploration experience using OntoQuest. The method is to categorize commonly asked queries based on their usage contexts so to prompt the user with context-aware guidance throughout the exploration process. OntoQuest is also designed to offer extended mapping schemes for storing OWL ontologies into back-end databases. Most existing ontology storage systems support mappings only for RDF data. Lastly, OntoQuest supports bulk insertion and updating of instances.In this demonstration, we show how OntoQuest guides a user through the inquiry (querying and reasoning) process and also have a peek at the underlying handling of storing, querying, reasoning, and interacting with the user.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1183–1186},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164240,
author = {Chen, Gang and Li, Xiaoyan and Shou, Lidan and Dong, Jinxiang and Chen, Chun},
title = {HISA: A Query System Bridging the Semantic Gap for Large Image Databases},
year = {2006},
publisher = {VLDB Endowment},
abstract = {We propose a novel system called HISA for organizing very large image databases. HISA implements the first known data structure to capture both the ontological knowledge and visual features for effective and effcient retrieval of images by either keywords, image examples, or both. HISA employs automatic image annotation technique, ontology analysis and statistical analysis of domain knowledge to precompute the data structure. Using these techniques, HISA is able to bridge the gap between the image semantics and the visual features, therefore providing more user-friendly and high-performance queries. We demonstrate the novel data structure employed by HISA, the query algorithms, and the pre-computation process.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1187–1190},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164253,
author = {Hoke, Evan and Sun, Jimeng and Faloutsos, Christos},
title = {InteMon: Intelligent System Monitoring on Large Clusters},
year = {2006},
publisher = {VLDB Endowment},
abstract = {InteMon is a prototype monitoring and mining system for large clusters. Currently, it monitors over 100 hosts of a prototype data center at CMU. It uses the SNMP protocol and it stores the monitoring data in an mySQL database. Then, it allows for visualization of the time-series data using a JSP web-based frontend interface for users.What sets it apart from other cluster monitoring systems is its ability to automatically analyze the monitoring data in real time and alert the users for potential anomalies. It uses state of the art stream mining methods, it has a sophisticated definition of anomalies (broken correlations among input streams), and it can also pinpoint the reason of the anomaly. InteMon has a user-friendly GUI, it allows the users to perform interactive mining tasks, and it is fully operational.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1239–1242},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164254,
author = {Houkj\ae{}r, Kenneth and Torp, Kristian and Wind, Rico},
title = {Simple and Realistic Data Generation},
year = {2006},
publisher = {VLDB Endowment},
abstract = {This paper presents a generic, DBMS independent, and highly extensible relational data generation tool. The tool can efficiently generate realistic test data for OLTP, OLAP, and data streaming applications. The tool uses a graph model to direct the data generation. This model makes it very simple to generate data even for large database schemas with complex inter- and intra table relationships. The model also makes it possible to generate data with very accurate characteristics.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1243–1246},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164255,
author = {Afanasiev, Loredana and Franceschet, Massimo and Marx, Maarten},
title = {XCheck: A Platform for Benchmarking XQuery Engines},
year = {2006},
publisher = {VLDB Endowment},
abstract = {XCheck is a tool for assessing the relative performance of different XQuery engines by means of benchmarks consisting of a set of XQuery queries and a set of XML documents. Given a benchmark and a set of engines, XCheck runs the benchmark on these engines and produces highly informative performance output. The current version of XCheck contains all available XQuery benchmarks which are run against four XQuery engines: Galax, Qizx/open, Saxon and MonetDB/XQuery. XCheck's design makes it easy to include new engines and new benchmarks.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1247–1250},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164256,
author = {Habich, Dirk and Richly, Sebastian and Lehner, Wolfgang},
title = {GignoMDA: Exploiting Cross-Layer Optimization for Complex Database Applications},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Database Systems are often used as persistent layer for applications. This implies that database schemas are generated out of transient programming class descriptions. The basic idea of the MDA approach generalizes this principle by providing a framework to generate applications (and database schemas) for different programming platforms. Within our GignoMDA project [3]--which is subject of this demo proposal--we have extended classic concepts for code generation. That means, our approach provides a single point of truth describing all aspects of database applications (e.g. database schema, project documentation,...) with great potential for cross-layer optimization. These new cross-layer optimization hints are a novel way for the challenging global optimization issue of multi-tier database applications. The demo at VLDB comprises an in-depth explanation of our concepts and the prototypical implementation by directly demonstrating the modeling and the automatic generation of database applications.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1251–1254},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164257,
author = {Kim, Gye-Jeong and Baek, Seung-Cheon and Lee, Hyun-Sook and Lee, Han-Deok and Joe, Moon Jeung},
title = {<i>LGeDBMS</i>: A Small DBMS for Embedded System with Flash Memory},
year = {2006},
publisher = {VLDB Endowment},
abstract = {The ever-increasing requirement of high performance and huge capacity memories of emerging consumer electronics appliances, such as mobile phone, digital camera, MP3, PMP, PDA, etc., has led to the widespread adaptation of flash memory as main data storages, respectively. As a result, managing the data on ash memory has been gaining in significant to satisfy the requirement of mobile embedded applications. However, the read/write/erase behaviors of flash memory are radically different than that of magnetic disks which make traditional database technology irrelevant. In this paper, we introduce LGeDBMS, a scale-downed DBMS engine designed for ash memory and its application. Finally, we demonstrate a PIM(Personal Information Management) application on a mobile phone using LGeDBMS.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1255–1258},
numpages = {4},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164258,
author = {Bitton, Dina and Faerber, Franz and Haas, Laura and Shanmugasundaram, Jayavel},
title = {One Platform for Mining Structured and Unstructured Data: Dream or Reality?},
year = {2006},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1261–1262},
numpages = {2},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164259,
author = {Chaudhuri, Surajit and Weikum, Gerhard},
title = {Foundations of Automated Database Tuning},
year = {2006},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1265},
numpages = {1},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164260,
author = {Cormode, Graham and Garofalakis, Minos},
title = {Streaming in a Connected World: Querying and Tracking Distributed Data Streams},
year = {2006},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1266},
numpages = {1},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164261,
author = {Ailamaki, Anastassia and Govindaraju, Naga K. and Harizopoulos, Stavros and Manocha, Dinesh},
title = {Query Co-Processing on Commodity Processors},
year = {2006},
publisher = {VLDB Endowment},
abstract = {The rapid increase in the data volumes for the past few decades has intensified the need for high processing power for database and data mining applications. Researchers have actively sought to design and develop new architectures for improving the performance. Recent research shows that the performance can be significantly improved using either (a) effective utilization of architectural features and memory hierarchies used by the conventional processors, or (b) the high computational power and memory bandwidth in commodity hardware such as network processing units (NPUs), Cell processors and graphics processing units (GPUs). This tutorial will survey the micro-architectural and architectural differences across these processors with data management in mind, and will present previous work and future opportunities for expanding query processing algorithms to other hardware than general-purpose processors. In addition to the database community, we intend to increase awareness in the computer architecture scene about opportunities to construct heterogeneous chips.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1267},
numpages = {1},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164262,
author = {Keogh, Eamonn},
title = {A Decade of Progress in Indexing and Mining Large Time Series Databases},
year = {2006},
publisher = {VLDB Endowment},
abstract = {Time series data is ubiquitous; large volumes of time series data are routinely created in scientific, industrial, entertainment, medical and biological domains. Examples include gene expression data, electrocardiograms, electroencephalograms, gait analysis, stock market quotes, space telemetry etc. Although statisticians have worked with time series for more than a century, many of their techniques hold little utility for researchers working with massive time series databases.A decade ago, a seminal paper by Faloutsos, Ranganathan, Manolopoulos appeared in SIGMOD. The paper, Fast Subsequence Matching in Time-Series Databases, has spawned at least a thousand references and extensions in the database/ data mining and information retrieval communities. This tutorial will summarize the decade of progress since this influential paper appeared.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1268},
numpages = {1},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{10.5555/1182635.1164263,
author = {Drineas, Petros and Mahoney, Michael W.},
title = {Randomized Algorithms for Matrices and Massive Data Sets},
year = {2006},
publisher = {VLDB Endowment},
abstract = {The tutorial will cover randomized sampling algorithms that extract structure from very large data sets modeled as matrices or tensors. Both provable algorithmic results and recent work on applying these methods to large biological and internet data sets will be discussed.},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1269},
numpages = {1},
location = {Seoul, Korea},
series = {VLDB '06}
}

