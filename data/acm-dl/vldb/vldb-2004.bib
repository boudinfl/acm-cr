@inproceedings{10.5555/1316689.1316690,
author = {Yach, David},
title = {Databases in a Wireless World},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The traditional view of distributed databases is based on a number of database servers with regular communication. Today information is stored not only in these central databases, but on a myriad of computers and computer-based devices in addition to the central storage. These range from desktop and laptop computers to PDA's and wireless devices such as cellular phones and BlackBerry's. The combination of large centralized databases with a large number and variety of associated edge databases effectively form a large distributed database, but one where many of the traditional rules and assumptions for distributed databases are no longer true.This keynote will discuss some of the new and challenging attributes of this new environment, particularly focusing on the challenges of wireless and occasionally connected devices. It will look at the new constraints, how these impact the traditional distributed database model, the techniques and heuristics being used to work within these constraints, and identify the potential areas where future research might help tackle these difficult issues.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {3},
numpages = {1},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316691,
author = {Halevy, Alon Y.},
title = {Structures, Semantics and Statistics},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {At a fundamental level, the key challenge in data integration is to reconcile the semantics of disparate data sets, each expressed with a different database structure. I argue that computing statistics over a large number of structures offers a powerful methodology for producing semantic mappings, the expressions that specify such reconciliation. In essence, the statistics offer hints about the semantics of the symbols in the structures, thereby enabling the detection of semantically similar concepts. The same methodology can be applied to several other data management tasks that involve search in a space of complex structures and in enabling the next-generation on-the-fly data integration systems.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {4–6},
numpages = {3},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316692,
author = {Agrawal, Rakesh and Srikant, Ramakrishnan},
title = {Whither Data Mining?},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The last decade has witnessed tremendous advances in data mining. We take a retrospective look at these developments, focusing on association rules discovery, and discuss the challenges and opportunities ahead.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {9},
numpages = {1},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316693,
author = {Johnson, David and Krishnan, Shankar and Chhugani, Jatin and Kumar, Subodh and Venkatasubramanian, Suresh},
title = {Compressing Large Boolean Matrices Using Reordering Techniques},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Large boolean matrices are a basic representational unit in a variety of applications, with some notable examples being interactive visualization systems, mining large graph structures, and association rule mining. Designing space and time efficient scalable storage and query mechanisms for such large matrices is a challenging problem.We present a lossless compression strategy to store and access such large matrices efficiently on disk. Our approach is based on viewing the columns of the matrix as points in a very high dimensional Hamming space, and then formulating an appropriate optimization problem that reduces to solving an instance of the Traveling Salesman Problem on this space.Finding good solutions to large TSP's in high dimensional Hamming spaces is itself a challenging and little-explored problem -- we cannot readily exploit geometry to avoid the need to examine all N2 inter-city distances and instances can be too large for standard TSP codes to run in main memory. Our multi-faceted approach adapts classical TSP heuristics by means of instance-partitioning and sampling, and may be of independent interest. For instances derived from interactive visualization and telephone call data we obtain significant improvement in access time over standard techniques, and for the visualization application we also make significant improvements in compression.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {13–23},
numpages = {11},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316694,
author = {Wu, Kesheng and Otoo, Ekow and Shoshani, Arie},
title = {On the Performance of Bitmap Indices for High Cardinality Attributes},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {It is well established that bitmap indices are efficient for read-only attributes with low attribute cardinalities. For an attribute with a high cardinality, the size of the bitmap index can be very large. To overcome this size problem, specialized compression schemes are used. Even though there are empirical evidences that some of these compression schemes work well, there has not been any systematic analysis of their effectiveness. In this paper, we systematically analyze the two most efficient bitmap compression techniques, the Byte-aligned Bitmap Code (BBC) and the Word-Aligned Hybrid (WAH) code. Our analyses show that both compression schemes can be optimal. We propose a novel strategy to select the appropriate algorithms so that this optimality is achieved in practice. In addition, our analyses and tests show that the compressed indices are relatively small compared with commonly used indices such as B-trees. Given these facts, we conclude that bitmap index is efficient on attributes of low cardinalities as well as on those of high cardinalities.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {24–35},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316695,
author = {Tata, Sandeep and Hankins, Richard A. and Patel, Jignesh M.},
title = {Practical Suffix Tree Construction},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Large string datasets are common in a number of emerging text and biological database applications. Common queries over such datasets include both exact and approximate string matches. These queries can be evaluated very efficiently by using a suffix tree index on the string dataset. Although suffix trees can be constructed quickly in memory for small input datasets, constructing persistent trees for large datasets has been challenging. In this paper, we explore suffix tree construction algorithms over a wide spectrum of data sources and sizes. First, we show that on modern processors, a cache-efficient algorithm with O(n2) complexity outperforms the popular O(n) Ukkonen algorithm, even for in-memory construction. For larger datasets, the disk I/O requirement quickly becomes the bottleneck in each algorithm's performance. To address this problem, we present a buffer management strategy for the O(n2) algorithm, creating a new disk-based construction algorithm that scales to sizes much larger than have been previously described in the literature. Our approach far outperforms the best known disk-based construction algorithms.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {36–47},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316696,
author = {Tajima, Keishi and Fukui, Yoshiki},
title = {Answering Xpath Queries over Networks by Sending Minimal Views},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {When a client submits a set of XPath queries to a XML database on a network, the set of answer sets sent back by the database may include redundancy in two ways: some elements may appear in more than one answer set, and some elements in some answer sets may be subelements of other elements in other (or the same) answer sets. Even when a client submits a single query, the answer can be self-redundant because some elements may be subelements of other elements in that answer. Therefore, sending those answers as they are is not optimal with respect to communication costs. In this paper, we propose a method of minimizing communication costs in XPath processing over networks. Given a single or a set of queries, we compute a minimal-size view set that can answer all the original queries. The database sends this view set to the client, and the client produces answers from it. We show algorithms for computing such a minimal view set for given queries. This view set is optimal; it only includes elements that appear in some of the final answers, and each element appears only once.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {48–59},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316697,
author = {Balmin, Andrey and \"{O}zcan, Fatma and Beyer, Kevin S. and Cochrane, Roberta J. and Pirahesh, Hamid},
title = {A Framework for Using Materialized XPath Views in XML Query Processing},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {XML languages, such as XQuery, XSLT and SQL/XML, employ XPath as the search and extraction language. XPath expressions often define complicated navigation, resulting in expensive query processing, especially when executed over large collections of documents. In this paper, we propose a framework for exploiting materialized XPath views to expedite processing of XML queries. We explore a class of materialized XPath views, which may contain XML fragments, typed data values, full paths, node references or any combination thereof. We develop an XPath matching algorithm to determine when such views can be used to answer a user query containing XPath expressions. We use the match information to identify the portion of an XPath expression in the user query which is not covered by the XPath view. Finally, we construct, possibly multiple, compensation expressions which need to be applied to the view to produce the query result. Experimental evaluation, using our prototype implementation, shows that the matching algorithm is very efficient and usually accounts for a small fraction of the total query compilation time.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {60–71},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316698,
author = {Li, Yunyao and Yu, Cong and Jagadish, H. V.},
title = {Schema-Free XQuery},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The widespread adoption of XML holds out the promise that document structure can be exploited to specify precise database queries. However, the user may have only a limited knowledge of the XML structure, and hence may be unable to produce a correct XQuery, especially in the context of a heterogeneous information collection. The default is to use keyword-based search and we are all too familiar with how difficult it is to obtain precise answers by these means. We seek to address these problems by introducing the notion of Meaningful Lowest Common Ancestor Structure (MLCAS) for finding related nodes within an XML document. By automatically computing MLCAS and expanding ambiguous tag names, we add new functionality to XQuery and enable users to take full advantage of XQuery in querying XML data precisely and efficiently without requiring (perfect) knowledge of the document structure. Such a Schema-Free XQuery is potentially of value not just to casual users with partial knowledge of schema, but also to experts working in a data integration or data evolution context. In such a context, a schema-free query, once written, can be applied universally to multiple data sources that supply similar content under different schemas, and applied "forever" as these schemas evolve. Our experimental evaluation found that it was possible to express a wide variety of queries in a schema-free manner and have them return correct results over a broad diversity of schemas. Furthermore, the evaluation of a schema-free query is not expensive using a novel stack-based algorithm we develop for computing MLCAS: from 1 to 4 times the execution time of an equivalent schema-aware query.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {72–83},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316699,
author = {Bouganim, Luc and Ngoc, Fran\c{c}ois Dang and Pucheral, Philippe},
title = {Client-Based Access Control Management for XML Documents},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The erosion of trust put in traditional database servers and in Database Service Providers, the growing interest for different forms of data dissemination and the concern for protecting children from suspicious Internet content are different factors that lead to move the access control from servers to clients. Several encryption schemes can be used to serve this purpose but all suffer from a static way of sharing data. With the emergence of hardware and software security elements on client devices, more dynamic client-based access control schemes can be devised. This paper proposes an efficient client-based evaluator of access control rules for regulating access to XML documents. This evaluator takes benefit from a dedicated index to quickly converge towards the authorized parts of a - potentially streaming - document. Additional security mecanisms guarantee that prohibited data can never be disclosed during the processing and that the input document is protected from any form of tampering. Experiments on synthetic and real datasets demonstrate the effectiveness of the approach.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {84–95},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316700,
author = {Yang, Xiaochun and Li, Chen},
title = {Secure XML Publishing without Information Leakage in the Presence of Data Inference},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Recent applications are seeing an increasing need that publishing XML documents should meet precise security requirements. In this paper, we consider data-publishing applications where the publisher specifies what information is sensitive and should be protected. We show that if a partial document is published carelessly, users can use common knowledge (e.g., "all patients in the same ward have the same disease") to infer more data, which can cause leakage of sensitive information. The goal is to protect such information in the presence of data inference with common knowledge. We consider common knowledge represented as semantic XML constraints. We formulate the process how users can infer data using three types of common XML constraints. Interestingly, no matter what sequences users follow to infer data, there is a unique, maximal document that contains all possible inferred documents. We develop algorithms for finding a partial document of a given XML document without causing information leakage, while allowing publishing as much data as possible. Our experiments on real data sets show that effect of inference on data security, and how the proposed techniques can prevent such leakage from happening.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {96–107},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316701,
author = {LeFevre, Kristen and Agrawal, Rakesh and Ercegovac, Vuk and Ramakrishnan, Raghu and Xu, Yirong and DeWitt, David},
title = {Limiting Disclosure in Hippocratic Databases},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We present a practical and efficient approach to incorporating privacy policy enforcement into an existing application and database environment, and we explore some of the semantic tradeoffs introduced by enforcing these privacy policy rules at cell-level granularity. Through a comprehensive set of performance experiments, we show that the cost of privacy enforcement is small, and scalable to large databases.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {108–119},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316702,
author = {Lakshmanan, Laks V. S. and Ramesh, Ganesh and Wang, Hui and Zhao, Zheng},
title = {On Testing Satisfiability of Tree Pattern Queries},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {XPath and XQuery (which includes XPath as a sublanguage) are the major query languages for XML. An important issue arising in efficient evaluation of queries expressed in these languages is satisfiability, i.e., whether there exists a database, consistent with the schema if one is available, on which the query has a non-empty answer. Our experience shows satisfiability check can effect substantial savings in query evaluation.We systematically study satisfiability of tree pattern queries (which capture a useful fragment of XPath) together with additional constraints, with or without a schema. We identify cases in which this problem can be solved in polynomial time and develop novel efficient algorithms for this purpose. We also show that in several cases, the problem is NP-complete. We ran a comprehensive set of experiments to verify the utility of satisfiability check as a preprocessing step in query processing. Our results show that this check takes a negligible fraction of the time needed for processing the query while often yielding substantial savings.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {120–131},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316703,
author = {Dong, Xin and Halevy, Alon Y. and Tatarinov, Igor},
title = {Containment of Nested XML Queries},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Query containment is the most fundamental relationship between a pair of database queries: a query Q is said to be contained in a query Q′ if the answer for Q is always a subset of the answer for Q′, independent of the current state of the database. Query containment is an important problem in a wide variety of data management applications, including verification of integrity constraints, reasoning about contents of data sources in data integration, semantic caching, verification of knowledge bases, determining queries independent of updates, and most recently, in query reformulation for peer data management systems. Query containment has been studied extensively in the relational context and for XPath queries, but not for XML queries with nesting.We consider the theoretical aspects of the problem of query containment for XML queries with nesting. We begin by considering conjunctive XML queries (c-XQueries), and show that containment is in polynomial time if we restrict the fanout (number of sibling sub-blocks) to be 1. We prove that for arbitrary fanout, containment is coNP-hard already for queries with nesting depth 2, even if the query does not include variables in the return clauses. We then show that for queries with fixed nesting depth, containment is coNP-complete.Next, we establish the computational complexity of query containment for several practical extensions of c-XQueries, including queries with union and arithmetic comparisons, and queries where the XPath expressions may include descendant edges and negation. Finally, we describe a few heuristics for speeding up query containment checking in practice by exploiting properties of the queries and the underlying schema.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {132–143},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316704,
author = {Krishnamurthy, Rajasekar and Kaushik, Raghav and Naughton, Jeffrey F.},
title = {Efficient XML-to-SQL Query Translation: Where to Add the Intelligence?},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We consider the efficiency of queries generated by XML to SQL translation. We first show that published XML-to-SQL query translation algorithms are suboptimal in that they often translate simple path expressions into complex SQL queries even when much simpler equivalent SQL queries exist. There are two logical ways to deal with this problem. One could generate suboptimal SQL queries using a fairly naive translation algorithm, and then attempt to optimize the resulting SQL; or one could use a more intelligent translation algorithm with the hopes of generating efficient SQL directly. We show that optimizing the SQL after it is generated is problematic, becoming intractable even in simple scenarios; by contrast, designing a translation algorithm that exploits information readily available at translation time is a promising alternative. To support this claim, we present a translation algorithm that exploits translation time information to generate efficient SQL for path expression queries over tree schemas.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {144–155},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316705,
author = {Chan, Chee-Yong and Fan, Wenfei and Zeng, Yiming},
title = {Taming XPath Queries by Minimizing Wildcard Steps},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {This paper presents a novel and complementary technique to optimize an XPath query by minimizing its wildcard steps. Our approach is based on using a general composite axis called the layer axis, to rewrite a sequence of XPath steps (all of which are wildcard steps except for possibly the last) into a single layer-axis step. We describe an efficient implementation of the layer axis and present a novel and efficient rewriting algorithm to minimize both non-branching as well as branching wildcard steps in XPath queries. We also demonstrate the usefulness of wildcard-step elimination by proposing an optimized evaluation strategy for wildcard-free XPath queries that enables selective loading of only the relevant input XML data for query evaluation. Our experimental results not only validate the scalability and efficiency of our optimized evaluation strategy, but also demonstrate the effectiveness of our rewriting algorithm for minimizing wildcard steps in XPath queries. To the best of our knowledge, this is the first effort that addresses this new optimization problem.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {156–167},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316706,
author = {Deutsch, Alin and Papakonstantinou, Yannis and Xu, Yu},
title = {The NEXT Framework for Logical XQuery Optimization},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Classical logical optimization techniques rely on a logical semantics of the query language. The adaptation of these techniques to XQuery is precluded by its definition as a functional language with operational semantics. We introduce Nested XML Tableaux which enable a logical foundation for XQuery semantics and provide the logical plan optimization framework of our XQuery processor. As a proof of concept, we develop and evaluate a minimization algorithm for removing redundant navigation within and across nested subqueries. The rich XQuery features create key challenges that fundamentally extend the prior work on the problems of minimizing conjunctive and tree pattern queries.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {168–179},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316707,
author = {Kifer, Daniel and Ben-David, Shai and Gehrke, Johannes},
title = {Detecting Change in Data Streams},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Detecting changes in a data stream is an important area of research with many applications. In this paper, we present a novel method for the detection and estimation of change. In addition to providing statistical guarantees on the reliability of detected changes, our method also provides meaningful descriptions and quantification of these changes. Our approach assumes that the points in the stream are independently generated, but otherwise makes no assumptions on the nature of the generating distribution. Thus our techniques work for both continuous and discrete data. In an experimental study we demonstrate the power of our techniques.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {180–191},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316708,
author = {Zhu, Shanzhong and Ravishankar, Chinya V.},
title = {Stochastic Consistency, and Scalable Pull-Based Caching for Erratic Data Stream Sources},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We introduce the notion of stochastic consistency, and propose a novel approach to achieving it for caches of highly erratic data. Erratic data sources, such as stock prices, sensor data, are common and important in practice. However, their erratic patterns of change make caching hard. Stochastic consistency guarantees that errors in cached values of erratic data remain within a user-specified bound, with a user-specified probability. We use a Brownian motion model to capture the behavior of data changes, and use its underlying theory to predict when caches should initiate pulls to refresh cached copies to maintain stochastic consistency. Our approach allows servers to remain totally stateless, thus achieving excellent scalability and reliability. We also discuss a new real-time scheduling approach for servicing pull requests at the server. Our scheduler delivers prompt response whenever possible, and minimizes the aggregate cache-source deviation due to delays during server overload. We conduct extensive experiments to validate our model on real-life datasets, and show that our scheme outperforms current schemes.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {192–203},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316709,
author = {Yu, Jeffery Xu and Chong, Zhihong and Lu, Hongjun and Zhou, Aoying},
title = {False Positive or False Negative: Mining Frequent Itemsets from High Speed Transactional Data Streams},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The problem of finding frequent items has been recently studied over high speed data streams. However, mining frequent itemsets from transactional data streams has not been well addressed yet in terms of its bounds of memory consumption. The main difficulty is due to the nature of the exponential explosion of itemsets. Given a domain of I unique items, the possible number of itemsets can be up to 2I - 1. When the length of data streams approaches to a very large number N, the possibility of an itemset to be frequent becomes larger and difficult to track with limited memory. However, the real killer of effective frequent itemset mining is that most of existing algorithms are false-positive oriented. That is, they control memory consumption in the counting processes by an error parameter ε, and allow items with support below the specified minimum support s but above s-ε counted as frequent ones. Such false-positive items increase the number of false-positive frequent itemsets exponentially, which may make the problem computationally intractable with bounded memory consumption. In this paper, we developed algorithms that can effectively mine frequent item(set)s from high speed transactional data streams with a bound of memory consumption. While our algorithms are false-negative oriented, that is, certain frequent itemsets may not appear in the results, the number of false-negative itemsets can be controlled by a predefined parameter so that desired recall rate of frequent itemsets can be guaranteed. We developed algorithms based on Chernoff bound. Our extensive experimental studies show that the proposed algorithms have high accuracy, require less memory, and consume less CPU time. They significantly outperform the existing false-positive algorithms.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {204–215},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316710,
author = {Mendelzon, Alberto O. and Rizzolo, Flavio and Vaisman, Alejandro},
title = {Indexing Temporal XML Documents},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Different models have been proposed recently for representing temporal data, tracking historical information, and recovering the state of the document as of any given time, in XML documents. We address the problem of indexing temporal XML documents. In particular we show that by indexing continuous paths, i.e. paths that are valid continuously during a certain interval in a temporal XML graph, we can dramatically increase query performance. We describe in detail the indexing scheme, denoted TempIndex, and compare its performance against both a system based on a nontemporal path index, and one based on DOM.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {216–227},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316711,
author = {Koch, Christoph and Scherzinger, Stefanie and Schweikardt, Nicole and Stegmaier, Bernhard},
title = {Schema-Based Scheduling of Event Processors and Buffer Minimization for Queries on Structured Data Streams},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We introduce an extension of the XQuery language, FluX, that supports event-based query processing and the conscious handling of main memory buffers. Purely event-based queries of this language can be executed on streaming XML data in a very direct way. We then develop an algorithm that allows to efficiently rewrite XQueries into the event-based FluX language. This algorithm uses order constraints from a DTD to schedule event handlers and to thus minimize the amount of buffering required for evaluating a query. We discuss the various technical aspects of query optimization and query evaluation within our framework. This is complemented with an experimental evaluation of our approach.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {228–239},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316712,
author = {Wang, Wei and Jiang, Haifeng and Lu, Hongjun and Yu, Jeffrey Xu},
title = {Bloom Histogram: Path Selectivity Estimation for XML Data with Updates},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Cost-based XML query optimization calls for accurate estimation of the selectivity of path expressions. Some other interactive and internet applications can also benefit from such estimations. While there are a number of estimation techniques proposed in the literature, almost none of them has any guarantee on the estimation accuracy within a given space limit. In addition, most of them assume that the XML data are more or less static, i.e., with few updates. In this paper, we present a framework for XML path selectivity estimation in a dynamic context. Specifically, we propose a novel data structure, bloom histogram, to approximate XML path frequency distribution within a small space budget and to estimate the path selectivity accurately with the bloom histogram. We obtain the upper bound of its estimation error and discuss the trade-offs between the accuracy and the space limit. To support updates of bloom histograms efficiently when underlying XML data change, a dynamic summary layer is used to keep exact or more detailed XML path information. We demonstrate through our extensive experiments that the new solution can achieve significantly higher accuracy with an even smaller space than the previous methods in both static and dynamic environments.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {240–251},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316713,
author = {Grust, Torsten and Sakr, Sherif and Teubner, Jens},
title = {XQuery on SQL Hosts},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Relational database systems may be turned into efficient XML and XPath processors if the system is provided with a suitable relational tree encoding. This paper extends this relational XML processing stack and shows that an RDBMS can also serve as a highly efficient XQuery runtime environment. Our approach is purely relational: XQuery expressions are compiled into SQL code which operates on the tree encoding. The core of the compilation procedure trades XQuery's notions of variable scopes and nested iteration (FLWOR blocks) for equi-joins.The resulting relational XQuery processor closely adheres to the language semantics, e.g., it obeys node identity as well as document and sequence order, and can support XQuery's full axis feature. The system exhibits quite promising performance figures in experiments. Somewhat unexpectedly, we will also see that the XQuery compiler can make good use of SQL's OLAP functionality.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {252–263},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316714,
author = {Halverson, Alan and Josifovski, Vanja and Lohman, Guy and Pirahesh, Hamid and M\"{o}rschel, Mathias},
title = {ROX: Relational over XML},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {An increasing percentage of the data needed by business applications is being generated in XML format. Storing the XML in its native format will facilitate new applications that exchange business objects in XML format and query portions of XML documents using XQuery. This paper explores the feasibility of accessing natively-stored XML data through traditional SQL interfaces, called Relational Over XML (ROX), in order to avoid the costly conversion of legacy applications to XQuery. It describes the forces that are driving the industry to evolve toward the ROX scenario as well as some of the issues raised by ROX. The impact of denormalization of data in XML documents is discussed both from a semantic and performance perspective. We also weigh the implications of ROX for manageability and query optimization. We experimentally compared the performance of a prototype of the ROX scenario to today's SQL engines, and found that good performance can be achieved through a combination of utilizing XML's hierarchical storage to store relations "pre-joined" as well as creating indices over the remaining join columns. We have developed an experimental framework using DB2 8.1 for Linux, Unix and Windows, and have gathered initial performance results that validate this approach.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {264–275},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316715,
author = {Braganholo, Vanessa P. and Davidson, Susan B. and Heuser, Carlos A.},
title = {From XML View Updates to Relational View Updates: Old Solutions to a New Problem},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {This paper addresses the question of updating relational databases through XML views. Using query trees to capture the notions of selection, projection, nesting, grouping, and heterogeneous sets found throughout most XML query languages, we show how XML views expressed using query trees can be mapped to a set of corresponding relational views. We then show how updates on the XML view are mapped to updates on the corresponding relational views. Existing work on updating relational views can then be leveraged to determine whether or not the relational views are updatable with respect to the relational updates, and if so, to translate the updates to the underlying relational database.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {276–287},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316716,
author = {Guha, Sudipto and Kim, Chulyun and Shim, Kyuseok},
title = {XWAVE: Optimal and Approximate Extended Wavelets},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Wavelet synopses have been found to be of interest in query optimization and approximate query answering. Recently, extended wavelets were proposed by Deligiannakis and Roussopoulos for data sets containing multiple measures. Extended wavelets optimize the storage utilization by attempting to store the same wavelet coefficient across different measures. This reduces the bookkeeping overhead and more coefficients can be stored. An optimal algorithm for minimizing the error in representation and an approximation algorithm for the complementary problem was provided. However, both their algorithms take linear space. Synopsis structures are often used in environments where space is at a premium and the data arrives as a continuous stream which is too expensive to store. In this paper, we give algorithms for extended wavelets which are space sensitive, i.e., use space which is dependent on the size of the synopsis (and at most on the logarithm of the total data) and operates in a streaming fashion. We present better optimal algorithms based on dynamic programming and a near optimal approximate greedy algorithm. We also demonstrate the performance benefits of our algorithms compared to previous ones through experiments on real-life and synthetic data sets.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {288–299},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316717,
author = {Guha, Sudipto and Shim, Kyuseok and Woo, Jungchul},
title = {REHIST: Relative Error Histogram Construction Algorithms},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Histograms and Wavelet synopses provide useful tools in query optimization and approximate query answering. Traditional histogram construction algorithms, such as V-Optimal, optimize absolute error measures for which the error in estimating a true value of 10 by 20 has the same effect of estimating a true value of 1000 by 1010. However, several researchers have recently pointed out the drawbacks of such schemes and proposed wavelet based schemes to minimize relative error measures. None of these schemes provide satisfactory guarantees - and we provide evidence that the difficulty may lie in the choice of wavelets as the representation scheme.In this paper, we consider histogram construction for the known relative error measures. We develop optimal as well as fast approximation algorithms. We provide a comprehensive theoretical analysis and demonstrate the effectiveness of these algorithms in providing significantly more accurate answers through synthetic and real life data sets.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {300–311},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316718,
author = {Das, Abhinandan and Ganguly, Sumit and Garofalakis, Minos and Rastogi, Rajeev},
title = {Distributed Set-Expression Cardinality Estimation},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We consider the problem of estimating set-expression cardinality in a distributed streaming environment where rapid update streams originating at remote sites are continually transmitted to a central processing system. At the core of our algorithmic solutions for answering set-expression cardinality queries are two novel techniques for lowering data communication costs without sacrificing answer precision. Our first technique exploits global knowledge of the distribution of certain frequently occurring stream elements to significantly reduce the transmission of element state information to the central site. Our second technical contribution involves a novel way of capturing the semantics of the input set expression in a boolean logic formula, and using models (of the formula) to determine whether an element state change at a remote site can affect the set expression result. Results of our experimental study with real-life as well as synthetic data sets indicate that our distributed set-expression cardinality estimation algorithms achieve substantial reductions in message traffic compared to naive approaches that provide the same accuracy guarantees.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {312–323},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316719,
author = {Srivastava, Utkarsh and Widom, Jennifer},
title = {Memory-Limited Execution of Windowed Stream Joins},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We address the problem of computing approximate answers to continuous sliding-window joins over data streams when the available memory may be insufficient to keep the entire join state. One approximation scenario is to provide a maximum subset of the result, with the objective of losing as few result tuples as possible. An alternative scenario is to provide a random sample of the join result, e.g., if the output of the join is being aggregated. We show formally that neither approximation can be addressed effectively for a sliding-window join of arbitrary input streams. Previous work has addressed only the maximum-subset problem, and has implicitly used a frequency-based model of stream arrival. We address the sampling problem for this model. More importantly, we point out a broad class of applications for which an age-based model of stream arrival is more appropriate, and we address both approximation scenarios under this new model. Finally, for the case of multiple joins being executed with an overall memory constraint, we provide an algorithm for memory allocation across the joins that optimizes a combined measure of approximation in all scenarios considered. All of our algorithms are implemented and experimental results demonstrate their effectiveness.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {324–335},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316720,
author = {Arasu, Arvind and Widom, Jennifer},
title = {Resource Sharing in Continuous Sliding-Window Aggregates},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We consider the problem of resource sharing when processing large numbers of continuous queries. We specifically address sliding-window aggregates over data streams, an important class of continuous operators for which sharing has not been addressed. We present a suite of sharing techniques that cover a wide range of possible scenarios: different classes of aggregation functions (algebraic, distributive, holistic), different window types (time-based, tuple-based, suffix, historical), and different input models (single stream, multiple substreams). We provide precise theoretical performance guarantees for our techniques, and show their practical effectiveness through experimental study.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {336–347},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316721,
author = {Chandrasekaran, Sirish and Franklin, Michael},
title = {Remembrance of Streams Past: Overload-Sensitive Management of Archived Streams},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {This paper studies Data Stream Management Systems that combine real-time data streams with historical data, and hence access incoming streams and archived data simultaneously. A significant problem for these systems is the I/O cost of fetching historical data which inhibits processing of the live data streams. Our solution is to reduce the I/O cost for accessing the archive by retrieving only a reduced (summarized or sampled) version of the historical data. This paper does not propose new summarization or sampling techniques, but rather a framework in which multiple resolutions of summarization/sampling can be generated efficiently. The query engine can select the appropriate level of summarization to use depending on the resources currently available. The central research problem studied is whether to generate the multiple representations of archived data eagerly upon data-arrival, lazily at query-time, or in a hybrid fashion. Concrete techniques for each approach are presented, which are tied to a specific data reduction technique (random sampling). The tradeoffs among the three approaches are studied both analytically and experimentally.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {348–359},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316722,
author = {Pandey, Sandeep and Dhamdhere, Kedar and Olston, Christopher},
title = {WIC: A General-Purpose Algorithm for Monitoring Web Information Sources},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The Web is becoming a universal information dissemination medium, due to a number of factors including its support for content dynamicity. A growing number of Web information providers post near real-time updates in domains such as auctions, stock markets, bulletin boards, news, weather, roadway conditions, sports scores, etc. External parties often wish to capture this information for a wide variety of purposes ranging from online data mining to automated synthesis of information from multiple sources. There has been a great deal of work on the design of systems that can process streams of data from Web sources, but little attention has been paid to how to produce these data streams, given that Web pages generally require "pull-based" access.In this paper we introduce a new general-purpose algorithm for monitoring Web information sources, effectively converting pull-based sources into push-based ones. Our algorithm can be used in conjunction with continuous query systems that assume information is fed into the query engine in a push-based fashion. Ideally, a Web monitoring algorithm for this purpose should achieve two objectives: (1) timeliness and (2) completeness of information captured. However, we demonstrate both analytically and empirically using real-world data that these objectives are fundamentally at odds. When resources available for Web monitoring are limited, and the number of sources to monitor is large, it may be necessary to sacrifice some timeliness to achieve better completeness, or vice versa. To take this fact into account, our algorithm is highly parameterized and targets an application-specified balance between timeliness and completeness. In this paper we formalize the problem of optimizing for a flexible combination of timeliness and completeness, and prove that our parameterized algorithm is a 2- approximation in all cases, and in certain cases is optimal.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {360–371},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316723,
author = {Dong, Xin and Halevy, Alon and Madhavan, Jayant and Nemes, Ema and Zhang, Jun},
title = {Similarity Search for Web Services},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Web services are loosely coupled software components, published, located, and invoked across the web. The growing number of web services available within an organization and on the Web raises a new and challenging search problem: locating desired web services. Traditional keyword search is insufficient in this context: the specific types of queries users require are not captured, the very small text fragments in web services are unsuitable for keyword search, and the underlying structure and semantics of the web services are not exploited.We describe the algorithms underlying the Woogle search engine for web services. Woogle supports similarity search for web services, such as finding similar web-service operations and finding operations that compose with a given one. We describe novel techniques to support these types of searches, and an experimental study on a collection of over 1500 web-service operations that shows the high recall and precision of our algorithms.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {372–383},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316724,
author = {Thor, Andreas and Rahm, Erhard},
title = {AWESOME: A Data Warehouse-Based System for Adaptive Website Recommendations},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Recommendations are crucial for the success of large websites. While there are many ways to determine recommendations, the relative quality of these recommenders depends on many factors and is largely unknown. We propose a new classification of recommenders and comparatively evaluate their relative quality for a sample web-site. The evaluation is performed with AWESOME (Adaptive website recommendations), a new data warehouse-based recommendation system capturing and evaluating user feedback on presented recommendations. Moreover, we show how AWESOME performs an automatic and adaptive closed-loop website optimization by dynamically selecting the most promising recommenders based on continuously measured recommendation feedback. We propose and evaluate several alternatives for dynamic recommender selection including a powerful machine learning approach.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {384–395},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316725,
author = {Ester, Martin and Kriegel, Hans-Peter and Schubert, Matthias},
title = {Accurate and Efficient Crawling for Relevant Websites},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Focused web crawlers have recently emerged as an alternative to the well-established web search engines. While the well-known focused crawlers retrieve relevant webpages, there are various applications which target whole websites instead of single webpages. For example, companies are represented by websites, not by individual webpages. To answer queries targeted at websites, web directories are an established solution. In this paper, we introduce a novel focused website crawler to employ the paradigm of focused crawling for the search of relevant websites. The proposed crawler is based on a two-level architecture and corresponding crawl strategies with an explicit concept of websites. The external crawler views the web as a graph of linked websites, selects the websites to be examined next and invokes internal crawlers. Each internal crawler views the webpages of a single given website and performs focused (page) crawling within that website. Our experimental evaluation demonstrates that the proposed focused website crawler clearly outperforms previous methods of focused crawling which were adapted to retrieve websites instead of single webpages.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {396–407},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316726,
author = {Wang, Jiying and Wen, Ji-Rong and Lochovsky, Fred and Ma, Wei-Ying},
title = {Instance-Based Schema Matching for Web Databases by Domain-Specific Query Probing},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {In a Web database that dynamically provides information in response to user queries, two distinct schemas, interface schema (the schema users can query) and result schema (the schema users can browse), are presented to users. Each partially reflects the actual schema of the Web database. Most previous work only studied the problem of schema matching across query interfaces of Web databases. In this paper, we propose a novel schema model that distinguishes the interface and the result schema of a Web database in a specific domain. In this model, we address two significant Web database schema-matching problems: intra-site and inter-site. The first problem is crucial in automatically extracting data from Web databases, while the second problem plays a significant role in meta-retrieving and integrating data from different Web databases. We also investigate a unified solution to the two problems based on query probing and instance-based schema matching techniques. Using the model, a cross validation technique is also proposed to improve the accuracy of the schema matching. Our experiments on real Web databases demonstrate that the two problems can be solved simultaneously with high precision and recall.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {408–419},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316727,
author = {Wang, Yuan and DeWitt, David J.},
title = {Computing Pagerank in a Distributed Internet Search System},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Existing Internet search engines use web crawlers to download data from the Web. Page quality is measured on central servers, where user queries are also processed. This paper argues that using crawlers has a list of disadvantages. Most importantly, crawlers do not scale. Even Google, the leading search engine, indexes less than 1% of the entire Web. This paper proposes a distributed search engine framework, in which every web server answers queries over its own data. Results from multiple web servers will be merged to generate a ranked hyperlink list on the submitting server. This paper presents a series of algorithms that compute PageRank in such framework. The preliminary experiments on a real data set demonstrate that the system achieves comparable accuracy on PageRank vectors to Google's well-known PageRank algorithm and, therefore, high quality of query results.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {420–431},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316728,
author = {Loo, Boon Thau and Hellerstein, Joseph M. and Huebsch, Ryan and Shenker, Scott and Stoica, Ion},
title = {Enhancing P2P File-Sharing with an Internet-Scale Query Processor},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {In this paper, we address the problem of designing a scalable, accurate query processor for peer-to-peer filesharing and similar distributed keyword search systems. Using a globally-distributed monitoring infrastructure, we perform an extensive study of the Gnutella filesharing network, characterizing its topology, data and query workloads. We observe that Gnutella's query processing approach performs well for popular content, but quite poorly for rare items with few replicas. We then consider an alternate approach based on Distributed Hash Tables (DHTs). We describe our implementation of PIERSearch, a DHT-based system, and propose a hybrid system where Gnutella is used to locate popular items, and PIERSearch for handling rare items. We develop an analytical model of the two approaches, and use it in concert with our Gnutella traces to study the trade-off between query recall and system overhead of the hybrid system. We evaluate a variety of localized schemes for identifying items that are rare and worth handling via the DHT. Lastly, we show in a live deployment on fifty nodes on two continents that it nicely complements Gnutella in its ability to handle rare items.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {432–443},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316729,
author = {Ganesan, Prasanna and Bawa, Mayank and Garcia-Molina, Hector},
title = {Online Balancing of Range-Partitioned Data with Applications to Peer-to-Peer Systems},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We consider the problem of horizontally partitioning a dynamic relation across a large number of disks/nodes by the use of range partitioning. Such partitioning is often desirable in large-scale parallel databases, as well as in peer-to-peer (P2P) systems. As tuples are inserted and deleted, the partitions may need to be adjusted, and data moved, in order to achieve storage balance across the participant disks/nodes. We propose efficient, asymptotically optimal algorithms that ensure storage balance at all times, even against an adversarial insertion and deletion of tuples. We combine the above algorithms with distributed routing structures to architect a P2P system that supports efficient range queries, while simultaneously guaranteeing storage balance.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {444–455},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316730,
author = {Ahmad, Yanif and \c{C}etintemel, U\u{g}ur},
title = {Network-Aware Query Processing for Stream-Based Applications},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {This paper investigates the benefits of network awareness when processing queries in widely-distributed environments such as the Internet. We present algorithms that leverage knowledge of network characteristics (e.g., topology, bandwidth, etc.) when deciding on the network locations where the query operators are executed. Using a detailed emulation study based on realistic network models, we analyse and experimentally evaluate the proposed approaches for distributed stream processing. Our results quantify the significant benefits of the network-aware approaches and reveal the fundamental trade-off between bandwidth efficiency and result latency that arises in networked query processing.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {456–467},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316731,
author = {Kementsietsidis, Anastasios and Arenas, Marcelo},
title = {Data Sharing through Query Translation in Autonomous Sources},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We consider the problem of data sharing between autonomous data sources in an environment where constraints cannot be placed on the shared contents of sources. Our solutions rely on the use of mapping tables which define how data from different sources are associated. In this setting, the answer to a local query, that is, a query posed against the schema of a single source, is augmented by retrieving related data from associated sources. This retrieval of data is achieved by translating, through mapping tables, the local query into a set of queries that are executed against the associated sources. We consider both sound translations (which only retrieve correct answers) and complete translations (which retrieve all correct answers, and no incorrect answers) and we present algorithms to compute such translations. Our solutions are implemented and tested experimentally and we describe here our key findings.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {468–479},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316732,
author = {Arasu, Arvind and Cherniack, Mitch and Galvez, Eduardo and Maier, David and Maskey, Anurag S. and Ryvkina, Esther and Stonebraker, Michael and Tibbetts, Richard},
title = {Linear Road: A Stream Data Management Benchmark},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {This paper specifies the Linear Road Benchmark for Stream Data Management Systems (SDMS). Stream Data Management Systems process streaming data by executing continuous and historical queries while producing query results in real-time. This benchmark makes it possible to compare the performance characteristics of SDMS' relative to each other and to alternative (e.g., Relational Database) systems. Linear Road has been endorsed as an SDMS benchmark by the developers of both the Aurora [1] (out of Brandeis University, Brown University and MIT) and STREAM [8] (out of Stanford University) stream systems.Linear Road simulates a toll system for the motor vehicle expressways of a large metropolitan area. The tolling system uses "variable tolling" [6, 11, 9]: an increasingly prevalent tolling technique that uses such dynamic factors as traffic congestion and accident proximity to calculate toll charges. Linear Road specifies a variable tolling system for a fictional urban area including such features as accident detection and alerts, traffic congestion measurements, toll calculations and historical queries. After specifying the benchmark, we describe experimental results involving two implementations: one using a commercially available Relational Database and the other using Aurora. Our results show that a dedicated Stream Data Management System can outperform a Relational Database by at least a factor of 5 on streaming data applications.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {480–491},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316733,
author = {Law, Yan-Nei and Wang, Haixun and Zaniolo, Carlo},
title = {Query Languages and Data Models for Database Sequences and Data Streams},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We study the fundamental limitations of relational algebra (RA) and SQL in supporting sequence and stream queries, and present effective query language and data model enrichments to deal with them. We begin by observing the well-known limitations of SQL in application domains which are important for data streams, such as sequence queries and data mining. Then we present a formal proof that, for continuous queries on data streams, SQL suffers from additional expressive power problems. We begin by focusing on the notion of nonblocking (NB) queries that are the only continuous queries that can be supported on data streams. We characterize the notion of nonblocking queries by showing that they are equivalent to monotonic queries. Therefore the notion of NB-completeness for RA can be formalized as its ability to express all monotonic queries expressible in RA using only the monotonic operators of RA. We show that RA is not NB-complete, and SQL is not more powerful than RA for monotonic queries.To solve these problems, we propose extensions that allow SQL to support all the monotonic queries expressible by a Turing machine using only monotonic operators. We show that these extensions are (i) user-defined aggregates (UDAs) natively coded in SQL (rather than in an external language), and (ii) a generalization of the union operator to support the merging of multiple streams according to their timestamps. These query language extensions require matching extensions to basic relational data model to support sequences explicitly ordered by times-tamps. Along with the formulation of very powerful queries, the proposed extensions entail more efficient expressions for many simple queries. In particular, we show that nonblocking queries are simple to characterize according to their syntactic structure.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {492–503},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316734,
author = {Snodgrass, Richard T. and Yao, Shilong Stanley and Collberg, Christian},
title = {Tamper Detection in Audit Logs},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Audit logs are considered good practice for business systems, and are required by federal regulations for secure systems, drug approval data, medical information disclosure, financial records, and electronic voting. Given the central role of audit logs, it is critical that they are correct and inalterable. It is not sufficient to say, "our data is correct, because we store all interactions in a separate audit log." The integrity of the audit log itself must also be guaranteed. This paper proposes mechanisms within a database management system (DBMS), based on cryptographically strong one-way hash functions, that prevent an intruder, including an auditor or an employee or even an unknown bug within the DBMS itself, from silently corrupting the audit log. We propose that the DBMS store additional information in the database to enable a separate audit log validator to examine the database along with this extra information and state conclusively whether the audit log has been compromised. We show with an implementation on a high-performance storage engine that the overhead for auditing is low and that the validator can efficiently and correctly determine if the audit log has been compromised.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {504–515},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316735,
author = {Agrawal, Rakesh and Bayardo, Roberto and Faloutsos, Christos and Kiernan, Jerry and Rantzau, Ralf and Srikant, Ramakrishnan},
title = {Auditing Compliance with a Hippocratic Database},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We introduce an auditing framework for determining whether a database system is adhering to its data disclosure policies. Users formulate audit expressions to specify the (sensitive) data subject to disclosure review. An audit component accepts audit expressions and returns all queries (deemed "suspicious") that accessed the specified data during their execution.The overhead of our approach on query processing is small, involving primarily the logging of each query string along with other minor annotations. Database triggers are used to capture updates in a backlog database. At the time of audit, a static analysis phase selects a subset of logged queries for further analysis. These queries are combined and transformed into an SQL audit query, which when run against the backlog database, identifies the suspicious queries efficiently and precisely.We describe the algorithms and data structures used in a DB2-based implementation of this framework. Experimental results reinforce our design choices and show the practicality of the approach.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {516–527},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316736,
author = {Li, Xiaolei and Han, Jiawei and Gonzalez, Hector},
title = {High-Dimensional OLAP: A Minimal Cubing Approach},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Data cube has been playing an essential role in fast OLAP (online analytical processing) in many multi-dimensional data warehouses. However, there exist data sets in applications like bioinformatics, statistics, and text processing that are characterized by high dimensionality, e.g., over 100 dimensions, and moderate size, e.g., around 106 tuples. No feasible data cube can be constructed with such data sets. In this paper we will address the problem of developing an efficient algorithm to perform OLAP on such data sets.Experience tells us that although data analysis tasks may involve a high dimensional space, most OLAP operations are performed only on a small number of dimensions at a time. Based on this observation, we propose a novel method that computes a thin layer of the data cube together with associated value-list indices. This layer, while being manageable in size, will be capable of supporting flexible and fast OLAP operations in the original high dimensional space. Through experiments we will show that the method has I/O costs that scale nicely with dimensionality. Furthermore, the costs are comparable to that of accessing an existing data cube when full materialization is possible.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {528–539},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316737,
author = {Sismanis, Yannis and Roussopoulos, Nick},
title = {The Polynomial Complexity of Fully Materialized Coalesced Cubes},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The data cube operator encapsulates all possible groupings of a data set and has proved to be an invaluable tool in analyzing vast amounts of data. However its apparent exponential complexity has significantly limited its applicability to low dimensional datasets. Recently the idea of the coalesced cube was introduced, and showed that high-dimensional coalesced cubes are orders of magnitudes smaller in size than the original data cubes even when they calculate and store every possible aggregation with 100% precision.In this paper we present an analytical framework for estimating the size of coalesced cubes. By using this framework on uniform coalesced cubes we show that their size and the required computation time scales polynomially with the dimensionality of the data set and, therefore, a full data cube at 100% precision is not inherently cursed by high dimensionality. Additionally, we show that such coalesced cubes scale polynomially (and close to linearly) with the number of tuples on the dataset. We were also able to develop an efficient algorithm for estimating the size of coalesced cubes before actually computing them, based only on metadata about the cubes. Finally, we complement our analytical approach with an extensive experimental evaluation using real and synthetic data sets, and demonstrate that not only uniform but also zipfian and real coalesced cubes scale polynomially.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {540–551},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316738,
author = {Geerts, Floris and Mannila, Heikki and Terzi, Evimaria},
title = {Relational Link-Based Ranking},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Link analysis methods show that the interconnections between web pages have lots of valuable information. The link analysis methods are, however, inherently oriented towards analyzing binary relations.We consider the question of generalizing link analysis methods for analyzing relational databases. To this aim, we provide a generalized ranking framework and address its practical implications.More specically, we associate with each relational database and set of queries a unique weighted directed graph, which we call the database graph. We explore the properties of database graphs. In analogy to link analysis algorithms, which use the Web graph to rank web pages, we use the database graph to rank partial tuples. In this way we can, e.g., extend the PageRank link analysis algorithm to relational databases and give this extension a random querier interpretation.Similarly, we extend the HITS link analysis algorithm to relational databases. We conclude with some preliminary experimental results.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {552–563},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316739,
author = {Balmin, Andrey and Hristidis, Vagelis and Papakonstantinou, Yannis},
title = {Objectrank: Authority-Based Keyword Search in Databases},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The ObjectRank system applies authority-based ranking to keyword search in databases modeled as labeled graphs. Conceptually, authority originates at the nodes (objects) containing the keywords and flows to objects according to their semantic connections. Each node is ranked according to its authority with respect to the particular keywords. One can adjust the weight of global importance, the weight of each keyword of the query, the importance of a result actually containing the keywords versus being referenced by nodes containing them, and the volume of authority flow via each type of semantic connection. Novel performance challenges and opportunities are addressed. First, schemas impose constraints on the graph, which are exploited for performance purposes. Second, in order to address the issue of authority ranking with respect to the given keywords (as opposed to Google's global PageRank) we precompute single keyword ObjectRanks and combine them during run time. We conducted user surveys and a set of performance experiments on multiple real and synthetic datasets, to assess the semantic meaningfulness and performance of ObjectRank.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {564–575},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316740,
author = {Gy\"{o}ngyi, Zolt\'{a}n and Garcia-Molina, Hector and Pedersen, Jan},
title = {Combating Web Spam with Trustrank},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Web spam pages use various techniques to achieve higher-than-deserved rankings in a search engine's results. While human experts can identify spam, it is too expensive to manually evaluate a large number of pages. Instead, we propose techniques to semi-automatically separate reputable, good pages from spam. We first select a small set of seed pages to be evaluated by an expert. Once we manually identify the reputable seed pages, we use the link structure of the web to discover other pages that are likely to be good. In this paper we discuss possible ways to implement the seed selection and the discovery of good pages. We present results of experiments run on the World Wide Web indexed by AltaVista and evaluate the performance of our techniques. Our results show that we can effectively filter out spam from a significant fraction of the web, based on a good seed set of less than 200 sites.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {576–587},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316741,
author = {Deshpande, Amol and Guestrin, Carlos and Madden, Samuel R. and Hellerstein, Joseph M. and Hong, Wei},
title = {Model-Driven Data Acquisition in Sensor Networks},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Declarative queries are proving to be an attractive paradigm for ineracting with networks of wireless sensors. The metaphor that "the sensornet is a database" is problematic, however, because sensors do not exhaustively represent the data in the real world. In order to map the raw sensor readings onto physical reality, a model of that reality is required to complement the readings. In this paper, we enrich interactive sensor querying with statistical modeling techniques. We demonstrate that such models can help provide answers that are both more meaningful, and, by introducing approximations with probabilistic confidences, significantly more efficient to compute in both time and energy. Utilizing the combination of a model and live data acquisition raises the challenging optimization problem of selecting the best sensor readings to acquire, balancing the increase in the confidence of our answer against the communication and data acquisition costs in the network. We describe an exponential time algorithm for finding the optimal solution to this optimization problem, and a polynomial-time heuristic for identifying solutions that perform well in practice. We evaluate our approach on several real-world sensor-network data sets, taking into account the real measured data and communication quality, demonstrating that our model-based approach provides a high-fidelity representation of the real phenomena and leads to significant performance gains versus traditional data acquisition techniques.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {588–599},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316742,
author = {Liu, David T. and Franklin, Michael J.},
title = {GridDB: A Data-Centric Overlay for Scientific Grids},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We present GridDB, a data-centric overlay for scientific grid data analysis. In contrast to currently deployed process-centric middleware, GridDB manages data entities rather than processes. GridDB provides a suite of services important to data analysis: a declarative interface, type-checking, interactive query processing, and memoization. We discuss several elements of GridDB: workflow/data model, query language, software architecture and query processing; and a prototype implementation. We validate GridDB by showing its modeling of real-world physics and astronomy analyses, and measurements on our prototype.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {600–611},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316743,
author = {Diao, Yanlei and Rizvi, Shariq and Franklin, Michael J.},
title = {Towards an Internet-Scale XML Dissemination Service},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Publish/subscribe systems have demonstrated the ability to scale to large numbers of users and high data rates when providing content-based data dissemination services on the Internet. However, their services are limited by the data semantics and query expressiveness that they support. On the other hand, the recent work on selective dissemination of XML data has made significant progress in moving from XML filtering to the richer functionality of transformation for result customization, but in general has ignored the challenges of deploying such XML-based services on an Internet-scale. In this paper, we address these challenges in the context of incorporating the rich functionality of XML data dissemination in a highly scalable system. We present the architectural design of ONYX, a system based on an overlay network. We identify the salient technical challenges in supporting XML filtering and transformation in this environment and propose techniques for solving them.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {612–623},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316744,
author = {Singitham, Pavan Kumar C. and Mahabhashyam, Mahathi S. and Raghavan, Prabhakar},
title = {Efficiency-Quality Tradeoffs for Vector Score Aggregation},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Finding the l nearest neighbors to a query in a vector space is an important primitive in text and image retrieval. Here we study an extension of this problem with applications to XML and image retrieval: we have multiple vector spaces, and the query places a weight on each space. Match scores from the spaces are weighted by these weights to determine the overall match between each record and the query; this is a case of score aggregation. We study approximation algorithms that use a small fraction of the computation of exhaustive search through all records, while returning nearly the best matches. We focus on the tradeoff between the computation and the quality of the results. We develop two approaches to retrieval from such multiple vector spaces. The first is inspired by resource allocation. The second, inspired by computational geometry, combines the multiple vector spaces together with all possible query weights into a single larger space. While mathematically elegant, this abstraction is intractable for implementation. We therefore devise an approximation of this combined space. Experiments show that all our approaches (to varying extents) enable retrieval quality comparable to exhaustive search, while avoiding its heavy computational cost.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {624–635},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316745,
author = {Guha, Sudipto and Koudas, Nick and Marathe, Amit and Srivastava, Divesh},
title = {Merging the Results of Approximate Match Operations},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Data Cleaning is an important process that has been at the center of research interest in recent years. An important end goal of effective data cleaning is to identify the relational tuple or tuples that are "most related" to a given query tuple. Various techniques have been proposed in the literature for efficiently identifying approximate matches to a query string against a single attribute of a relation. In addition to constructing a ranking (i.e., ordering) of these matches, the techniques often associate, with each match, scores that quantify the extent of the match. Since multiple attributes could exist in the query tuple, issuing approximate match operations for each of them separately will effectively create a number of ranked lists of the relation tuples. Merging these lists to identify a final ranking and scoring, and returning the top-K tuples, is a challenging task.In this paper, we adapt the well-known footrule distance (for merging ranked lists) to effectively deal with scores. We study efficient algorithms to merge rankings, and produce the top-K tuples, in a declarative way. Since techniques for approximately matching a query string against a single attribute in a relation are typically best deployed in a database, we introduce and describe two novel algorithms for this problem and we provide SQL specifications for them. Our experimental case study, using real application data along with a realization of our proposed techniques on a commercial data base system, highlights the benefits of the proposed algorithms and attests to the overall effectiveness and practicality of our approach.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {636–647},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316746,
author = {Theobald, Martin and Weikum, Gerhard and Schenkel, Ralf},
title = {Top-k Query Evaluation with Probabilistic Guarantees},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Top-k queries based on ranking elements of multidimensional datasets are a fundamental building block for many kinds of information discovery. The best known general-purpose algorithm for evaluating top-k queries is Fagin's threshold algorithm (TA). Since the user's goal behind top-k queries is to identify one or a few relevant and novel data items, it is intriguing to use approximate variants of TA to reduce run-time costs. This paper introduces a family of approximate top-k algorithms based on probabilistic arguments. When scanning index lists of the underlying multidimensional data space in descending order of local scores, various forms of convolution and derived bounds are employed to predict when it is safe, with high probability, to drop candidate items and to prune the index scans. The precision and the efficiency of the developed methods are experimentally evaluated based on a large Web corpus and a structured data collection.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {648–659},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316747,
author = {Harizopoulos, Stavros and Ailamaki, Anastassia},
title = {Steps towards Cache-Resident Transaction Processing},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Online transaction processing (OLTP) is a multibillion dollar industry with high-end database servers employing state-of-the-art processors to maximize performance. Unfortunately, recent studies show that CPUs are far from realizing their maximum intended throughput because of delays in the processor caches. When running OLTP, instruction-related delays in the memory subsystem account for 25 to 40% of the total execution time. In contrast to data, instruction misses cannot be overlapped with out-of-order execution, and instruction caches cannot grow as the slower access time directly affects the processor speed. The challenge is to alleviate the instruction-related delays without increasing the cache size.We propose Steps, a technique that minimizes instruction cache misses in OLTP workloads by multiplexing concurrent transactions and exploiting common code paths. One transaction paves the cache with instructions, while close followers enjoy a nearly miss-free execution. Steps yields up to 96.7% reduction in instruction cache misses for each additional concurrent transaction, and at the same time eliminates up to 64% of mispredicted branches by loading a repeating execution pattern into the CPU. This paper (a) describes the design and implementation of Steps, (b) analyzes Steps using microbenchmarks, and (c) shows Steps performance when running TPC-C on top of the Shore storage manager.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {660–671},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316748,
author = {Graefe, Goetz},
title = {Write-Optimized B-Trees},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Large writes are beneficial both on individual disks and on disk arrays, e.g., RAID-5. The presented design enables large writes of internal B-tree nodes and leaves. It supports both in-place updates and large append-only ("log-structured") write operations within the same storage volume, within the same B-tree, and even at the same time. The essence of the proposal is to make page migration inexpensive, to migrate pages while writing them, and to make such migration optional rather than mandatory as in log-structured file systems. The inexpensive page migration also aids traditional defragmentation as well as consolidation of free space needed for future large writes. These advantages are achieved with a very limited modification to conventional B-trees that also simplifies other B-tree operations, e.g., key range locking and compression.Prior proposals and prototypes implemented transacted B-tree on top of log-structured file systems and added transaction support to log-structured file systems. Instead, the presented design adds techniques and performance characteristics of log-structured file systems to traditional B-trees and their standard transaction support, notably without adding a layer of indirection for locating B-tree nodes on disk. The result retains fine-granularity locking, full transactional ACID guarantees, fast search performance, etc. expected of a modern B-tree implementation, yet adds efficient transacted page relocation and large, high-bandwidth writes.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {672–683},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316749,
author = {Manegold, Stefan and Boncz, Peter and Nes, Niels and Kersten, Martin},
title = {Cache-Conscious Radix-Decluster Projections},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {As CPUs become more powerful with Moore's law and memory latencies stay constant, the impact of the memory access performance bottleneck continues to grow on relational operators like join, which can exhibit random access on a memory region larger than the hardware caches. While cache-conscious variants for various relational algorithms have been described, previous work has mostly ignored (the cost of) projection columns. However, real-life joins almost always come with projections, such that proper projection column manipulation should be an integral part of any generic join algorithm. In this paper, we analyze cache-conscious hash-join algorithms including projections on two storage schemes: N-ary Storage Model (NSM) and Decomposition Storage Model (DSM). It turns out, that the strategy of first executing the join and only afterwards dealing with the projection columns (i.e., post-projection) on DSM, in combination with a new finely tunable algorithm called Radix-Decluster, outperforms all previously reported projection strategies. To make this result generally applicable, we also outline how DSM Radix-Decluster can be integrated in a NSM-based RDBMS using projection indices.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {684–695},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316750,
author = {Shao, Minglong and Schindler, Jiri and Schlosser, Steven W. and Ailamaki, Anastassia and Ganger, Gregory R.},
title = {Clotho: Decoupling Memory Page Layout from Storage Organization},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {As database application performance depends on the utilization of the memory hierarchy, smart data placement plays a central role in increasing locality and in improving memory utilization. Existing techniques, however, do not optimize accesses to all levels of the memory hierarchy and for all the different workloads, because each storage level uses different technology (cache, memory, disks) and each application accesses data using different patterns. Clotho is a new buffer pool and storage management architecture that decouples in-memory page layout from data organization on non-volatile storage devices to enable independent data layout design at each level of the storage hierarchy. Clotho can maximize cache and memory utilization by (a) transparently using appropriate data layouts in memory and non-volatile storage, and (b) dynamically synthesizing data pages to follow application access patterns at each level as needed. Clotho creates in-memory pages individually tailored for compound and dynamically changing workloads, and enables efficient use of different storage technologies (e.g., disk arrays or MEMS-based storage devices). This paper describes the Clotho design and prototype implementation and evaluates its performance under a variety of workloads using both disk arrays and simulated MEMS-based storage devices.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {696–707},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316751,
author = {Aggarwal, G. and Bawa, M. and Ganesan, P. and Garcia-Molina, H. and Kenthapadi, K. and Mishra, N. and Motwani, R. and Srivastava, U. and Thomas, D. and Widom, J. and Xu, Y.},
title = {Vision Paper: Enabling Privacy for the Paranoids},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {P3P [23, 24] is a set of standards that allow corporations to declare their privacy policies. Hippocratic Databases [6] have been proposed to implement such policies within a corporation's datastore. From an end-user individual's point of view, both of these rest on an uncomfortable philosophy of trusting corporations to protect his/her privacy. Recent history chronicles several episodes when such trust has been willingly or accidentally violated by corporations facing bankruptcy courts, civil subpoenas or lucrative mergers. We contend that data management solutions for information privacy must restore controls in the individual's hands. We suggest that enabling such control will require a radical re-think on modeling, release, and management of personal data.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {708–719},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316752,
author = {Hore, Bijit and Mehrotra, Sharad and Tsudik, Gene},
title = {A Privacy-Preserving Index for Range Queries},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Database outsourcing is an emerging data management paradigm which has the potential to transform the IT operations of corporations. In this paper we address privacy threats in database outsourcing scenarios where trust in the service provider is limited. Specifically, we analyze the data partitioning (bucketization) technique and algorithmically develop this technique to build privacy-preserving indices on sensitive attributes of a relational table. Such indices enable an untrusted server to evaluate obfuscated range queries with minimal information leakage. We analyze the worst-case scenario of inference attacks that can potentially lead to breach of privacy (e.g., estimating the value of a data element within a small error margin) and identify statistical measures of data privacy in the context of these attacks. We also investigate precise privacy guarantees of data partitioning which form the basic building blocks of our index. We then develop a model for the fundamental privacy-utility tradeoff and design a novel algorithm for achieving the desired balance between privacy and utility (accuracy of range query evaluation) of the index.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {720–731},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316753,
author = {Sion, Radu and Atallah, Mikhail and Prabhakar, Sunil},
title = {Resilient Rights Protection for Sensor Streams},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Today's world of increasingly dynamic computing environments naturally results in more and more data being available as fast streams. Applications such as stock market analysis, environmental sensing, web clicks and intrusion detection are just a few of the examples where valuable data is streamed. Often, streaming information is offered on the basis of a non-exclusive, single-use customer license. One major concern, especially given the digital nature of the valuable stream, is the ability to easily record and potentially "re-play" parts of it in the future. If there is value associated with such future re-plays, it could constitute enough incentive for a malicious customer (Mallory) to duplicate segments of such recorded data, subsequently re-selling them for profit. Being able to protect against such infringements becomes a necessity.In this paper we introduce the issue of rights protection for discrete streaming data through watermarking. This is a novel problem with many associated challenges including: operating in a finite window, single-pass, (possibly) high-speed streaming model, surviving natural domain specific transforms and attacks (e.g.extreme sparse sampling and summarizations), while at the same time keeping data alterations within allowable bounds. We propose a solution and analyze its resilience to various types of attacks as well as some of the important expected domain-specific transforms, such as sampling and summarization. We implement a proof of concept software (wms.*) and perform experiments on real sensor data from the NASA Infrared Telescope Facility at the University of Hawaii, to assess encoding resilience levels in practice. Our solution proves to be well suited for this new domain. For example, we can recover an over 97% confidence watermark from a highly down-sampled (e.g. less than 8%) stream or survive stream summarization (e.g. 20%) and random alteration attacks with very high confidence levels, often above 99%.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {732–743},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316754,
author = {Tao, Yufei and Papadias, Dimitris and Lian, Xiang},
title = {Reverse KNN Search in Arbitrary Dimensionality},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Given a point q, a reverse k nearest neighbor (RkNN) query retrieves all the data points that have q as one of their k nearest neighbors. Existing methods for processing such queries have at least one of the following deficiencies: (i) they do not support arbitrary values of k (ii) they cannot deal efficiently with database updates, (iii) they are applicable only to 2D data (but not to higher dimensionality), and (iv) they retrieve only approximate results. Motivated by these shortcomings, we develop algorithms for exact processing of RkNN with arbitrary values of k on dynamic multidimensional datasets. Our methods utilize a conventional data-partitioning index on the dataset and do not require any pre-computation. In addition to their flexibility, we experimentally verify that the proposed algorithms outperform the existing ones even in their restricted focus.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {744–755},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316755,
author = {Xia, Chenyi and Lu, Hongjun and Ooi, Beng Chin and Hu, Jing},
title = {Gorder: An Efficient Method for KNN Join Processing},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {An important but very expensive primitive operation of high-dimensional databases is the K-Nearest Neighbor (KNN) similarity join. The operation combines each point of one dataset with its KNNs in the other dataset and it provides more meaningful query results than the range similarity join. Such an operation is useful for data mining and similarity search.In this paper, we propose a novel KNN-join algorithm, called the Gorder (or the G-ordering KNN) join method. Gorder is a block nested loop join method that exploits sorting, join scheduling and distance computation filtering and reduction to reduce both I/O and CPU costs. It sorts input datasets into the G-order and applied the scheduled block nested loop join on the G-ordered data. The distance computation reduction is employed to further reduce CPU cost. It is simple and yet efficient, and handles high-dimensional data efficiently. Extensive experiments on both synthetic cluster and real life datasets were conducted, and the results illustrate that Gorder is an efficient KNN-join method and outperforms existing methods by a wide margin.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {756–767},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316756,
author = {Jensen, Christian S. and Lin, Dan and Ooi, Beng Chin},
title = {Query and Update Efficient B<sup>+</sup>-Tree Based Indexing of Moving Objects},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {A number of emerging applications of data management technology involve the monitoring and querying of large quantities of continuous variables, e.g., the positions of mobile service users, termed moving objects. In such applications, large quantities of state samples obtained via sensors are streamed to a database. Indexes for moving objects must support queries efficiently, but must also support frequent updates. Indexes based on minimum bounding regions (MBRs) such as the R-tree exhibit high concurrency overheads during node splitting, and each individual update is known to be quite costly. This motivates the design of a solution that enables the B+ -tree to manage moving objects. We represent moving-object locations as vectors that are timestamped based on their update time. By applying a novel linearization technique to these values, it is possible to index the resulting values using a single B+-tree that partitions values according to their timestamp and otherwise preserves spatial proximity. We develop algorithms for range and k nearest neighbor queries, as well as continuous queries. The proposal can be grafted into existing database systems cost effectively. An extensive experimental study explores the performance characteristics of the proposal and also shows that it is capable of substantially outperforming the R-tree based TPR-tree for both single and concurrent access scenarios.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {768–779},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316757,
author = {Keogh, Eamonn and Palpanas, Themistoklis and Zordan, Victor B. and Gunopulos, Dimitrios and Cardle, Marc},
title = {Indexing Large Human-Motion Databases},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Data-driven animation has become the industry standard for computer games and many animated movies and special effects. In particular, motion capture data recorded from live actors, is the most promising approach offered thus far for animating realistic human characters. However, the manipulation of such data for general use and re-use is not yet a solved problem. Many of the existing techniques dealing with editing motion rely on indexing for annotation, segmentation, and re-ordering of the data. Euclidean distance is inappropriate for solving these indexing problems because of the inherent variability found in human motion. The limitations of Euclidean distance stems from the fact that it is very sensitive to distortions in the time axis. A partial solution to this problem, Dynamic Time Warping (DTW), aligns the time axis before calculating the Euclidean distance. However, DTW can only address the problem of local scaling. As we demonstrate in this paper, global or uniform scaling is just as important in the indexing of human motion. We propose a novel technique to speed up similarity search under uniform scaling, based on bounding envelopes. Our technique is intuitive and simple to implement. We describe algorithms that make use of this technique, we perform an experimental analysis with real datasets, and we evaluate it in the context of a motion capture processing system. The results demonstrate the utility of our approach, and show that we can achieve orders of magnitude of speedup over the brute force approach, the only alternative solution currently available.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {780–791},
numpages = {12},
keywords = {time series, motion capture, indexing, animation},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316758,
author = {Chen, Lei and Ng, Raymond},
title = {On the Marriage of Lp-Norms and Edit Distance},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Existing studies on time series are based on two categories of distance functions. The first category consists of the Lp-norms. They are metric distance functions but cannot support local time shifting. The second category consists of distance functions which are capable of handling local time shifting but are nonmetric. The first contribution of this paper is the proposal of a new distance function, which we call ERP ("Edit distance with Real Penalty"). Representing a marriage of L1- norm and the edit distance, ERP can support local time shifting, and is a metric.The second contribution of the paper is the development of pruning strategies for large time series databases. Given that ERP is a metric, one way to prune is to apply the triangle inequality. Another way to prune is to develop a lower bound on the ERP distance. We propose such a lower bound, which has the nice computational property that it can be efficiently indexed with a standard B+- tree. Moreover, we show that these two ways of pruning can be used simultaneously for ERP distances. Specifically, the false positives obtained from the B+-tree can be further minimized by applying the triangle inequality. Based on extensive experimentation with existing benchmarks and techniques, we show that this combination delivers superb pruning power and search time performance, and dominates all existing strategies.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {792–803},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316759,
author = {Koudas, Nick and Ooi, Beng Chin and Tan, Kian-Lee and Zhang, Rui},
title = {Approximate NN Queries on Streams with Guaranteed Error/Performance Bounds},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {In data stream applications, data arrive continuously and can only be scanned once as the query processor has very limited memory (relative to the size of the stream) to work with. Hence, queries on data streams do not have access to the entire data set and query answers are typically approximate. While there have been many studies on the k Nearest Neighbors (kNN) problem in conventional multi-dimensional databases, the solutions cannot be directly applied to data streams for the above reasons. In this paper, we investigate the kNN problem over data streams. We first introduce the e-approximate kNN (ekNN) problem that finds the approximate kNN answers of a query point Q such that the absolute error of the k-th nearest neighbor distance is bounded by e. To support ekNN queries over streams, we propose a technique called DISC (aDaptive Indexing on Streams by space-filling Curves). DISC can adapt to different data distributions to either (a) optimize memory utilization to answer ekNN queries under certain accuracy requirements or (b) achieve the best accuracy under a given memory constraint. At the same time, DISC provide efficient updates and query processing which are important requirements in data stream applications. Extensive experiments were conducted using both synthetic and real data sets and the results confirm the effectiveness and efficiency of DISC.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {804–815},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316760,
author = {Beeri, Catriel and Kanza, Yaron and Safra, Eliyahu and Sagiv, Yehoshua},
title = {Object Fusion in Geographic Information Systems},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Given two geographic databases, a fusion algorithm should produce all pairs of corresponding objects (i.e., objects that represent the same real-world entity). Four fusion algorithms, which only use locations of objects, are described and their performance is measured in terms of recall and precision. These algorithms are designed to work even when locations are imprecise and each database represents only some of the real-world entities. Results of extensive experimentation are presented and discussed. The tests show that the performance depends on the density of the data sources and the degree of overlap among them. All four algorithms are much better than the current state of the art (i.e., the one-sided nearest-neighbor join). One of these four algorithms is best in all cases, at a cost of a small increase in the running time compared to the other algorithms.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {816–827},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316761,
author = {Iwerks, Glenn S. and Samet, Hanan and Smith, Kenneth P.},
title = {Maintenance of Spatial Semijoin Queries on Moving Points},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {In this paper, we address the maintenance of spatial semijoin queries over continuously moving points, where points are modeled as linear functions of time. This is analogous to the maintenance of a materialized view except, as time advances, the query result may change independently of updates. As in a materialized view, we assume there is no prior knowledge of updates before they occur. We present a new approach, continuous fuzzy sets (CFS), to maintain continuous spatial semijoins efficiently. CFS is compared experimentally to a simple scaling of previous work. The result is significantly better performance of CFS compared to previous work by up to an order of magnitude in some cases.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {828–839},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316762,
author = {Kolahdouzan, Mohammad and Shahabi, Cyrus},
title = {Voronoi-Based K Nearest Neighbor Search for Spatial Network Databases},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {A frequent type of query in spatial networks (e.g., road networks) is to find the K nearest neighbors (KNN) of a given query object. With these networks, the distances between objects depend on their network connectivity and it is computationally expensive to compute the distances (e.g., shortest paths) between objects. In this paper, we propose a novel approach to efficiently and accurately evaluate KNN queries in spatial network databases using first order Voronoi diagram. This approach is based on partitioning a large network to small Voronoi regions, and then pre-computing distances both within and across the regions. By localizing the precomputation within the regions, we save on both storage and computation and by performing across-the-network computation for only the border points of the neighboring regions, we avoid global pre-computation between every node-pair. Our empirical experiments with several real-world data sets show that our proposed solution outperforms approaches that are based on on-line distance computation by up to one order of magnitude, and provides a factor of four improvement in the selectivity of the filter step as compared to the index-based approaches.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {840–851},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316763,
author = {Aggarwal, Charu C. and Han, Jiawei and Wang, Jianyong and Yu, Philip S.},
title = {A Framework for Projected Clustering of High Dimensional Data Streams},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The data stream problem has been studied extensively in recent years, because of the great ease in collection of stream data. The nature of stream data makes it essential to use algorithms which require only one pass over the data. Recently, single-scan, stream analysis methods have been proposed in this context. However, a lot of stream data is high-dimensional in nature. High-dimensional data is inherently more complex in clustering, classification, and similarity search. Recent research discusses methods for projected clustering over high-dimensional data sets. This method is however difficult to generalize to data streams because of the complexity of the method and the large volume of the data streams.In this paper, we propose a new, high-dimensional, projected data stream clustering method, called HPStream. The method incorporates a fading cluster structure, and the projection based clustering methodology. It is incrementally updatable and is highly scalable on both the number of dimensions and the size of the data streams, and it achieves better clustering quality in comparison with the previous stream clustering methods. Our performance study with both real and synthetic data sets demonstrates the efficiency and effectiveness of our proposed framework and implementation methods.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {852–863},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316764,
author = {Dalvi, Nilesh and Suciu, Dan},
title = {Efficient Query Evaluation on Probabilistic Databases},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We describe a system that supports arbitrarily complex SQL queries on probabilistic databases. The query semantics is based on a probabilistic model and the results are ranked, much like in Information Retrieval. Our main focus is efficient query evaluation, a problem that has not received attention in the past. We describe an optimization algorithm that can compute efficiently most queries. We show, however, that the data complexity of some queries is #P-complete, which implies that these queries do not admit any efficient evaluation methods. For these queries we describe both an approximation algorithm and a Monte-Carlo simulation algorithm.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {864–875},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316765,
author = {Cheng, Reynold and Xia, Yuni and Prabhakar, Sunil and Shah, Rahul and Vitter, Jeffrey Scott},
title = {Efficient Indexing Methods for Probabilistic Threshold Queries over Uncertain Data},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {It is infeasible for a sensor database to contain the exact value of each sensor at all points in time. This uncertainty is inherent in these systems due to measurement and sampling errors, and resource limitations. In order to avoid drawing erroneous conclusions based upon stale data, the use of uncertainty intervals that model each data item as a range and associated probability density function (pdf) rather than a single value has recently been proposed. Querying these uncertain data introduces imprecision into answers, in the form of probability values that specify the likeliness the answer satisfies the query. These queries are more expensive to evaluate than their traditional counterparts but are guaranteed to be correct and more informative due to the probabilities accompanying the answers. Although the answer probabilities are useful, for many applications, it is only necessary to know whether the probability exceeds a given threshold - we term these Probabilistic Threshold Queries (PTQ). In this paper we address the efficient computation of these types of queries.In particular, we develop two index structures and associated algorithms to efficiently answer PTQs. The first index scheme is based on the idea of augmenting uncertainty information to an R-tree. We establish the difficulty of this problem by mapping one-dimensional intervals to a two-dimensional space, and show that the problem of interval indexing with probabilities is significantly harder than interval indexing which is considered a well-studied problem. To overcome the limitations of this R-tree based structure, we apply a technique we call variance-based clustering, where data points with similar degrees of uncertainty are clustered together. Our extensive index structure can answer the queries for various kinds of uncertainty pdfs, in an almost optimal sense. We conduct experiments to validate the superior performance of both indexing schemes.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {876–887},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316766,
author = {Chaudhuri, Surajit and Das, Gautam and Hristidis, Vagelis and Weikum, Gerhard},
title = {Probabilistic Ranking of Database Query Results},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We investigate the problem of ranking answers to a database query when many tuples are returned. We adapt and apply principles of probabilistic models from Information Retrieval for structured data. Our proposed solution is domain independent. It leverages data and workload statistics and correlations. Our ranking functions can be further customized for different applications. We present results of preliminary experiments which demonstrate the efficiency as well as the quality of our ranking system.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {888–899},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316767,
author = {Bhagwat, Deepavali and Chiticariu, Laura and Tan, Wang-Chiew and Vijayvargiya, Gaurav},
title = {An Annotation Management System for Relational Databases},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We present an annotation management system for relational databases. In this system, every piece of data in a relation is assumed to have zero or more annotations associated with it and annotations are propagated along, from the source to the output, as data is being transformed through a query. Such an annotation management system is important for understanding the provenance and quality of data, especially in applications that deal with integration of scientific and biological data.We present an extension, pSQL, of a fragment of SQL that has three different types of annotation propagation schemes, each useful for different purposes. The default scheme propagates annotations according to where data is copied from. The default-all scheme propagates annotations according to where data is copied from among all equivalent formulations of a given query. The custom scheme allows a user to specify how annotations should propagate. We present a storage scheme for the annotations and describe algorithms for translating a pSQL query under each propagation scheme into one or more SQL queries that would correctly retrieve the relevant annotations according to the specified propagation scheme. For the default-all scheme, we also show how we generate finitely many queries that can simulate the annotation propagation behavior of the set of all equivalent queries, which is possibly infinite. The algorithms are implemented and the feasibility of the system is demonstrated by a set of experiments that we have conducted.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {900–911},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316768,
author = {Ross, Kenneth A. and Stoyanovich, Julia},
title = {Symmetric Relations and Cardinality-Bounded Multisets in Database Systems},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {In a binary symmetric relationship, A is related to B if and only if B is related to A. Symmetric relationships between k participating entities can be represented as multisets of cardinality k. Cardinality-bounded multisets are natural in several real-world applications. Conventional representations in relational databases suffer from several consistency and performance problems. We argue that the database system itself should provide native support for cardinality-bounded multisets. We provide techniques to be implemented by the database engine that avoid the drawbacks, and allow a schema designer to simply declare a table to be symmetric in certain attributes. We describe a compact data structure, and update methods for the structure. We describe an algebraic symmetric closure operator, and show how it can be moved around in a query plan during query optimization in order to improve performance. We describe indexing methods that allow efficient lookups on the symmetric columns. We show how to perform database normalization in the presence of symmetric relations. We provide techniques for inferring that a view is symmetric. We also describe a syntactic SQL extension that allows the succinct formulation of queries over symmetric relations.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {912–923},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316769,
author = {Howe, Bill and Maier, David},
title = {Algebraic Manipulation of Scientific Datasets},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We investigate algebraic processing strategies for large numeric datasets equipped with a possibly irregular grid structure. Such datasets arise, for example, in computational simulations, observation networks, medical imaging, and 2-D and 3-D rendering. Existing approaches for manipulating these datasets are incomplete: The performance of SQL queries for manipulating large numeric datasets is not competitive with specialized tools. Database extensions for processing multidimensional discrete data can only model regular, rectilinear grids. Visualization software libraries are designed to process gridded datasets efficiently, but no algebra has been developed to simplify their use and afford optimization. Further, these libraries are data dependent - physical changes to data representation or organization break user programs. In this paper, we present an algebra of grid-fields for manipulating both regular and irregular gridded datasets, algebraic optimization techniques, and an implementation backed by experimental results. We compare our techniques to those of spatial databases and visualization software libraries, using real examples from an Environmental Observation and Forecasting System. We find that our approach can express optimized plans inaccessible to other techniques, resulting in improved performance with reduced programming effort.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {924–935},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316770,
author = {Balke, Wolf-Tilo and G\"{u}ntzer, Ulrich},
title = {Multi-Objective Query Processing for Database Systems},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Query processing in database systems has developed beyond mere exact matching of attribute values. Scoring database objects and retrieving only the top k matches or Pareto-optimal result sets (skyline queries) are already common for a variety of applications. Specialized algorithms using either paradigm can avoid na\"{\i}ve linear database scans and thus improve scalability. However, these paradigms are only two extreme cases of exploring viable compromises for each user's objectives. To find the correct result set for arbitrary cases of multi-objective query processing in databases we will present a novel algorithm for computing sets of objects that are nondominated with respect to a set of monotonic objective functions. Naturally containing top k and skyline retrieval paradigms as special cases, this algorithm maintains scalability also for all cases in between. Moreover, we will show the algorithm's correctness and instance-optimality in terms of necessary object accesses and how the response behavior can be improved by progressively producing result objects as quickly as possible, while the algorithm is still running.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {936–947},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316771,
author = {Deshpande, Amol and Hellerstein, Joseph M.},
title = {Lifting the Burden of History from Adaptive Query Processing},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Adaptive query processing schemes attempt to re-optimize query plans during the course of query execution. A variety of techniques for adaptive query processing have been proposed, varying in the granularity at which they can make decisions [8]. The eddy [1] is the most aggressive of these techniques, with the flexibility to choose tuple-by-tuple how to order the application of operators. In this paper we identify and address a fundamental limitation of the original eddies proposal: the burden of history in routing. We observe that routing decisions have long-term effects on the state of operators in the query, and can severely constrain the ability of the eddy to adapt over time. We then propose a mechanism we call STAIRs that allows the query engine to manipulate the state stored inside the operators and undo the effects of past routing decisions. We demonstrate that eddies with STAIRs achieve both high adaptivity and good performance in the face of uncertainty, outperforming prior eddy proposals by orders of magnitude.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {948–959},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316772,
author = {Neumann, Thomas and Moerkotte, Guido},
title = {A Combined Framework for Grouping and Order Optimization},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Since the introduction of cost-based query optimization by Selinger et al. in their seminal paper, the performance-critical role of interesting orders has been recognized. Some algebraic operators change interesting orders (e.g. sort and select), while others exploit them (e.g. merge join). Likewise, Wang and Cherniack (VLDB 2003) showed that existing groupings should be exploited to avoid redundant grouping operations. Ideally, the reasoning about interesting orderings and groupings should be integrated into one framework.So far, no complete, correct, and efficient algorithm for ordering and grouping inference has been proposed. We fill this gap by proposing a general two-phase approach that efficiently integrates the reasoning about orderings and groupings. Our experimental results show that with a modest increase of the time and space requirements of the preprocessing phase both orderings and groupings can be handled at the same time. More importantly, there is no additional cost for the second phase during which the plan generator changes and exploits orderings and groupings by adding operators to subplans.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {960–971},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316773,
author = {Krishnamurthy, Sailesh and Franklin, Michael J. and Hellerstein, Joseph M. and Jacobson, Garrett},
title = {The Case for Precision Sharing},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Sharing has emerged as a key idea of static and adaptive stream query processing systems. Inherent in these systems is a tension between sharing common work and avoiding unnecessary work. Increased sharing has generally led to more unnecessary work.Our approach of precision sharing aims to share aggressively without unnecessary work. We show why "adaptive" tuple lineage is more generally applicable and use it for precisely shared static dataflows. We also show how "static" ordering constraints can be used for precision sharing in adaptive systems. Finally, we report an experimental study of precision sharing.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {972–984},
numpages = {13},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316774,
author = {Behm, Andreas and Rielau, Serge and Swagerman, Richard},
title = {Returning Modified Rows - Select Statements with Side Effects},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {SQL in the IBM® DB2® Universal DatabaseTM for Linux®, UNIX®, and Windows® (DB2 UDB) database management product has been extended to support nested INSERT, UPDATE, and DELETE operations in SELECT statements. This allows database applications additional processing on modified rows. Within a single unit of work, applications can retrieve a result set containing the modified rows from a table or view modified by an SQL data-change operation. This eliminates the need to select the row after an INSERT or UPDATE, or before a DELETE statement. As a result, fewer network round trips, less server CPU time, fewer cursors, and less server memory are required. In addition, deadlocks can be avoided. The proposed approach is integrated with the set semantics of SQL, and does not require any procedural logic or modifications on the underlying relational data model. Pipelining multiple update, insert and delete operations using the same source data provides a very efficient way for multitable data-change statements typically found in ETL (extraction, transformation, load) applications. We demonstrate significant performance benefit with our experiences in the TPC-C benchmark. Experimental results show that the new SQL is more efficient in query execution compared to classic SQL.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {987–997},
numpages = {11},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316775,
author = {Cunningham, Conor and Galindo-Legaria, C\'{e}sar A. and Graefe, Goetz},
title = {PIVOT and UNPIVOT: Optimization and Execution Strategies in an RDBMS},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {PIVOT and UNPIVOT, two operators on tabular data that exchange rows and columns, enable data transformations useful in data modeling, data analysis, and data presentation. They can quite easily be implemented inside a query processor, much like select, project, and join. Such a design provides opportunities for better performance, both during query optimization and query execution. We discuss query optimization and execution implications of this integrated design and evaluate the performance of this approach using a prototype implementation in Microsoft SQL Server.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {998–1009},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316776,
author = {Rjaibi, Walid and Bird, Paul},
title = {A Multi-Purpose Implementation of Mandatory Access Control in Relational Database Management Systems},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Mandatory Access Control (MAC) implementations in Relational Database Management Systems (RDBMS) have focused solely on Multilevel Security (MLS). MLS has posed a number of challenging problems to the database research community, and there has been an abundance of research work to address those problems. Unfortunately, the use of MLS RDBMS has been restricted to a few government organizations where MLS is of paramount importance such as the intelligence community and the Department of Defense. The implication of this is that the investment of building an MLS RDBMS cannot be leveraged to serve the needs of application domains where there is a desire to control access to objects based on the label associated with that object and the label associated with the subject accessing that object, but where the label access rules and the label structure do not necessarily match the MLS two security rules and the MLS label structure. This paper introduces a flexible and generic implementation of MAC in RDBMS that can be used to address the requirements from a variety of application domains, as well as to allow an RDBMS to efficiently take part in an end-to-end MAC enterprise solution. The paper also discusses the extensions made to the SQL compiler component of an RDBMS to incorporate the label access rules in the access plan it generates for an SQL query, and to prevent unauthorized leakage of data that could occur as a result of traditional optimization techniques performed by SQL compilers.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1010–1020},
numpages = {11},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316777,
author = {Bandi, Nagender and Sun, Chengyu and Agrawal, Divyakant and El Abbadi, Amr},
title = {Hardware Acceleration in Commercial Databases: A Case Study of Spatial Operations},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Traditional databases have focused on the issue of reducing I/O cost as it is the bottleneck in many operations. As databases become increasingly accepted in areas such as Geographic Information Systems (GIS) and Bioinformatics, commercial DBMS need to support data types for complex data such as spatial geometries and protein structures. These non-conventional data types and their associated operations present new challenges. In particular, the computational cost of some spatial operations can be orders of magnitude higher than the I/O cost. In order to improve the performance of spatial query processing, innovative solutions for reducing this computational cost are beginning to emerge. Recently, it has been proposed that hard-ware acceleration of an off-the-shelf graphics card can be used to reduce the computational cost of spatial operations. However, this proposal is preliminary in that it establishes the feasibility of the hardware assisted approach in a stand-alone setting but not in a real-world commercial database. In this paper we present an architecture to show how hardware acceleration of an off-the-shelf graphics card can be integrated into a popular commercial database to speed up spatial queries. Extensive experimentation with real-world datasets shows that significant improvement in the performance of spatial operations can be achieved with this integration. The viability of this approach underscores the significance of a tighter integration of hardware acceleration into commercial databases for spatial applications.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1021–1032},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316778,
author = {Cha, Sang K. and Song, Changbin},
title = {P<sup>*</sup>TIME: Highly Scalable OLTP DBMS for Managing Update-Intensive Stream Workload},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Over the past thirty years since the system R and Ingres projects started to lay the foundation for today's RDBMS implementations, the underlying hardware and software platforms have changed dramatically. However, the fundamental RDBMS architecture, especially, the storage engine architecture, largely remains unchanged. While this conventional architecture may suffices for satisfying most of today's applications, its deliverable performance range is far from meeting the so-called growing "real-time enterprise" demand of acquiring and querying high-volume update data streams cost-effectively.P*TIME is a new, memory-centric light-weight OLTP RDBMS designed and built from scratch to deliver orders of magnitude higher scalability on commodity SMP hardware than existing RDBMS implementations, not only in search but also in update performance. Its storage engine layer incorporates our previous innovations for exploiting engine-level microparallelism such as differential logging and optimistic latch-free index traversal concurrency control protocol. This paper presents the architecture and performance of P*TIME and reports our experience of deploying P*TIME as the stock market database server at one of the largest on-line brokerage firms.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1033–1044},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316779,
author = {Poess, Meikel and Stephens, John M.},
title = {Generating Thousand Benchmark Queries in Seconds},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The combination of an exponential growth in the amount of data managed by a typical business intelligence system and the increased competitiveness of a global economy has propelled decision support systems (DSS) from the role of exploratory tools employed by a few visionary companies to become a core requirement for a competitive enterprise. That same maturation has often resulted in a selection process that requires an ever more critical system evaluation and selection to be completed in an increasingly short period of time. While there have been some advances in the generation of data sets for system evaluation (see [3]), the quantification of query performance has often relied on models and methodologies that were developed for systems that were more simplistic, less dynamic, and less central to a successful business. In this paper we present QGEN, a flexible, high-level query generator optimized for decision support system evaluation. QGEN is able to generate arbitrary query sets, which conform to a selected statistical profile without requiring that the queries be statically defined or disclosed prior to testing. Its novel design links query syntax with abstracted data distributions, enabling users to parameterize their query workload to match an emerging access pattern or data set modification. This results in query sets that retain comparability for system comparisons while reflecting the inherent dynamism of operational systems, and which provide a broad range of syntactic and semantic coverage, while remaining focused on appropriate commonalities within a particular evaluation process or business segment.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1045–1053},
numpages = {9},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316780,
author = {Das, Souripriya and Chong, Eugene Inseok and Eadon, George and Srinivasan, Jaannathan},
title = {Supporting Ontology-Based Semantic Matching in RDBMS},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Ontologies are increasingly being used to build applications that utilize domain-specific knowledge. This paper addresses the problem of supporting ontology-based semantic matching in RDBMS. Specifically, 1) A set of SQL operators, namely ONT_RELATED, ONT_EXPAND, ONT_DISTANCE, and ONT_PATH, are introduced to perform ontology-based semantic matching, 2) A new indexing scheme ONT_INDEXTYPE is introduced to speed up ontology-based semantic matching operations, and 3) System-defined tables are provided for storing ontologies specified in OWL. Our approach enables users to reference ontology data directly from SQL using the semantic match operators, thereby opening up possibilities of combining with other operations such as joins as well as making the ontology-driven applications easy to develop and efficient. In contrast, other approaches use RDBMS only for storage of ontologies and querying of ontology data is typically done via APIs. This paper presents the ontology-related functionality including inferencing, discusses how it is implemented on top of Oracle RDBMS, and illustrates the usage with several database applications.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1054–1065},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316781,
author = {Mukherjea, Sougata and Bamba, Bhuvan},
title = {BioPatentMiner: An Information Retrieval System for Biomedical Patents},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Before undertaking new biomedical research, identifying concepts that have already been patented is essential. Traditional keyword based search on patent databases may not be sufficient to retrieve all the relevant information, especially for the biomedical domain. More sophisticated retrieval techniques are required. This paper presents BioPatentMiner, a system that facilitates information retrieval from biomedical patents. It integrates information from the patents with knowledge from biomedical ontologies to create a Semantic Web. Besides keyword search and queries linking the properties specified by one or more RDF triples, the system can discover Semantic Associations between the resources. The system also determines the importance of the resources to rank the results of a search and prevent information overload while determining the Semantic Associations.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1066–1077},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316782,
author = {Koudas, Nick and Marathe, Amit and Srivastava, Divesh},
title = {Flexible String Matching against Large Databases in Practice},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Data Cleaning is an important process that has been at the center of research interest in recent years. Poor data quality is the result of a variety of reasons, including data entry errors and multiple conventions for recording database fields, and has a significant impact on a variety of business issues. Hence, there is a pressing need for technologies that enable flexible (fuzzy) matching of string information in a database. Cosine similarity with tf-idf is a well-established metric for comparing text, and recent proposals have adapted this similarity measure for flexibly matching a query string with values in a single attribute of a relation.In deploying tf-idf based flexible string matching against real AT&amp;T databases, we observed that this technique needed to be enhanced in many ways. First, along the functionality dimension, where there was a need to flexibly match along multiple string-valued attributes, and also take advantage of known semantic equivalences. Second, we identified various performance enhancements to speed up the matching process, potentially trading off a small degree of accuracy for substantial performance gains. In this paper, we report on our techniques and experience in dealing with flexible string matching against real AT&amp;T databases.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1078–1086},
numpages = {9},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316783,
author = {Zilio, Daniel C. and Rao, Jun and Lightstone, Sam and Lohman, Guy and Storm, Adam and Garcia-Arellano, Christian and Fadden, Scott},
title = {DB2 Design Advisor: Integrated Automatic Physical Database Design},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The DB2 Design Advisor in IBM® DB2® Universal DatabaseTM (DB2 UDB) Version 8.2 for Linux®, UNIX® and Windows® is a tool that, for a given workload, automatically recommends physical design features that are any subset of indexes, materialized query tables (also called materialized views), shared-nothing database partitionings, and multidimensional clustering of tables. Our work is the very first industrial-strength tool that covers the design of as many as four different features, a significant advance to existing tools, which support no more than just indexes and materialized views. Building such a tool is challenging, because of not only the large search space introduced by the interactions among features, but also the extensibility needed by the tool to support additional features in the future. We adopt a novel "hybrid" approach in the Design Advisor that allows us to take important interdependencies into account as well as to encapsulate design features as separate components to lower the reengineering cost. The Design Advisor also features a built-in module that automatically reduces the given workload, and therefore provides great scalability for the tool. Our experimental results demonstrate that our tool can quickly provide good physical design recommendations that satisfy users' requirements.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1087–1097},
numpages = {11},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316784,
author = {Dageville, Benoit and Das, Dinesh and Dias, Karl and Yagoub, Khaled and Zait, Mohamed and Ziauddin, Mohamed},
title = {Automatic SQL Tuning in Oracle 10g},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {SQL tuning is a very critical aspect of database performance tuning. It is an inherently complex activity requiring a high level of expertise in several domains: query optimization, to improve the execution plan selected by the query optimizer; access design, to identify missing access structures; and SQL design, to restructure and simplify the text of a badly written SQL statement. Furthermore, SQL tuning is a time consuming task due to the large volume and evolving nature of the SQL workload and its underlying data.In this paper we present the new Automatic SQL Tuning feature of Oracle 10g. This technology is implemented as a core enhancement of the Oracle query optimizer and offers a comprehensive solution to the SQL tuning challenges mentioned above. Automatic SQL Tuning introduces the concept of SQL profiling to transparently improve execution plans. It also generates SQL tuning recommendations by performing cost-based access path and SQL structure "what-if" analyses.This feature is exposed to the user through both graphical and command line interfaces. The Automatic SQL Tuning is an integral part of the Oracle's framework for self-managing databases. The superiority of this new technology is demonstrated by comparing the results of Automatic SQL Tuning to manual tuning using a real customer workload.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1098–1109},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316785,
author = {Fontoura, Marcus and Shekita, Engene and Zien, Jason Y. and Rajagopalan, Sridhar and Neumann, Andreas},
title = {High Performance Index Build Algorithms for Intranet Search Engines},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {There has been a substantial amount of research on high-performance algorithms for constructing an inverted text index. However, constructing the inverted index in a intranet search engine is only the final step in a more complicated index build process. Among other things, this process requires an analysis of all the data being indexed to compute measures like PageRank. The time to perform this global analysis step is significant compared to the time to construct the inverted index, yet it has not received much attention in the research literature. In this paper, we describe how the use of slightly outdated information from global analysis and a fast index construction algorithm based on radix sorting can be combined in a novel way to significantly speed up the index build process without sacrificing search quality.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1122–1133},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316786,
author = {Krishnaprasad, Muralidhar and Liu, Zhen Hua and Manikutty, Anand and Warner, James W. and Arora, Vikas and Kotsovolos, Susan},
title = {Query Rewrite for XML in Oracle XML DB},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Oracle XML DB integrates XML storage and querying using the Oracle relational and object relational framework. It has the capability to physically store XML documents by shredding them as relational or object relational data, and creating logical XML documents using SQL/XML publishing functions. However, querying XML in a relational or object relational database poses several challenges. The biggest challenge is to efficiently process queries against XML in a database whose fundamental storage is table-based and whose fundamental query engine is tuple-oriented. In this paper, we present the 'XML Query Rewrite' technique used in Oracle XML DB. This technique integrates querying XML using XPath embedded inside SQL operators and SQL/XML publishing functions with the object relational and relational algebra. A common set of algebraic rules is used to reduce both XML and object queries into their relational equivalent. This enables a large class of XML queries over XML type tables and views to be transformed into their semantically equivalent relational or object relational queries. These queries are then amenable to classical relational optimisations yielding XML query performance comparable to relational. Furthermore, this rewrite technique lays out a foundation to enable rewrite of XQuery [1] over XML.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1134–1145},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316787,
author = {Pal, Shankar and Cseri, Istvan and Seeliger, Oliver and Schaller, Gideon and Giakoumakis, Leo and Zolotov, Vasili},
title = {Indexing XML Data Stored in a Relational Database},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {As XML usage grows for both data-centric and document-centric applications, introducing native support for XML data in relational databases brings significant benefits. It provides a more mature platform for the XML data model and serves as the basis for interoperability between relational and XML data. Whereas query processing on XML data shredded into one or more relational tables is well understood, it provides limited support for the XML data model. XML data can be persisted as a byte sequence (BLOB) in columns of tables to support the XML model more faithfully. This introduces new challenges for query processing such as the ability to index the XML blob for good query performance. This paper reports novel techniques for indexing XML data in the upcoming version of Microsoft® SQL ServerTM, and how it ties into the relational framework for query processing.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1146–1157},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316788,
author = {Aboulnaga, A. and Haas, P. and Kandil, M. and Lightstone, S. and Lohman, G. and Markl, V. and Popivanov, I. and Raman, V.},
title = {Automated Statistics Collection in DB2 UDB},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The use of inaccurate or outdated database statistics by the query optimizer in a relational DBMS often results in a poor choice of query execution plans and hence unacceptably long query processing times. Configuration and maintenance of these statistics has traditionally been a time-consuming manual operation, requiring that the database administrator (DBA) continually monitor query performance and data changes in order to determine when to refresh the statistics values and when and how to adjust the set of statistics that the DBMS maintains. In this paper we describe the new Automated Statistics Collection (ASC) component of IBM® DB2® Universal DatabaseTM (DB2 UDB). This autonomic technology frees the DBA from the tedious task of manually supervising the collection and maintenance of database statistics. ASC monitors both the update-delete-insert (UDI) activities on the data as well as query feedback (QF), i.e., the results of the queries that are executed on the data. ASC uses these two sources of information to automatically decide which statistics to collect and when to collect them. This combination of UDI-driven and QF-driven autonomic processes ensures that the system can handle unforeseen queries while also ensuring good performance for frequent and important queries. We present the basic concepts, architecture, and key implementation details of ASC in DB2 UDB, and present a case study showing how the use of ASC can speed up a query workload by orders of magnitude without requiring any DBA intervention.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1158–1169},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316789,
author = {Lightstone, Sam S. and Bhattacharjee, Bishwaranjan},
title = {Automated Design of Multidimensional Clustering Tables for Relational Databases},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The ability to physically cluster a database table on multiple dimensions is a powerful technique that offers significant performance benefits in many OLAP, warehousing, and decision-support systems. An industrial implementation of this technique for the DB2® Universal DatabaseTM (DB2 UDB) product, called multidimensional clustering (MDC), which co-exists with other classical forms of data storage and indexing methods, was described in VLDB 2003. This paper describes the first published model for automating the selection of clustering keys in single-dimensional and multidimensional relational databases that use a cell/block storage structure for MDC. For any significant dimensionality (3 or more), the possible solution space is combinatorially complex. The automated MDC design model is based on what-if query cost modeling, data sampling, and a search algorithm for evaluating a large constellation of possible combinations. The model is effective at trading the benefits of potential combinations of clustering keys against data sparsity and performance. It also effectively selects the granularity at which dimensions should be used for clustering (such as week of year versus month of year). We show results from experiments indicating that the model provides design recommendations of comparable quality to those made by human experts. The model has been implemented in the IBM® DB2 UDB for Linux®, UNIX® and Windows® Version 8.2 release.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1170–1181},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316790,
author = {Bornh\"{o}vd, Christof and Lin, Tao and Haller, Stephan and Schaper, Joachim},
title = {Integrating Automatic Data Acquisition with Business Processes Experiences with SAP's Auto-ID Infrastructure},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Smart item technologies, like RFID and sensor networks, are considered to be the next big step in business process automation [1]. Through automatic and real-time data acquisition, these technologies can benefit a great variety of industries by improving the efficiency of their operations. SAP's Auto-ID infrastructure enables the integration of RFID and sensor technologies with existing business processes. In this paper we give an overview of the existing infrastructure, discuss lessons learned from successful customer pilots, and point out some of the open research issues.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1182–1188},
numpages = {7},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316791,
author = {Chawathe, Sudarshan S. and Krishnamurthy, Venkat and Ramachandran, Sridhar and Sarma, Sanjay},
title = {Managing RFID Data},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Radio-Frequency Identification (RFID) technology enables sensors to efficiently and inexpensively track merchandise and other objects. The vast amount of data resulting from the proliferation of RFID readers and tags poses some interesting challenges for data management. We present a brief introduction to RFID technology and highlight a few of the data management challenges.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1189–1195},
numpages = {7},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316792,
author = {Campbell, David},
title = {Production Database Systems: Making Them Easy is Hard Work},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Enterprise capable database products have evolved into incredibly complex systems, some of which present hundreds of configuration parameters to the system administrator. So, while the processing and storage costs for maintaining large volumes of data have plummeted, the human costs associated with maintaining the data have continued to rise. In this presentation, we discuss the framework and approach used by the team who took Microsoft SQL Server from a state where it had several hundred configuration parameters to a system that can configure itself and respond to changes in workload and environment with little human intervention.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1196–1197},
numpages = {2},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316793,
author = {Bloom, Toby and Sharpe, Ted},
title = {Managing Data from High-Throughput Genomic Processing: A Case Study},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Genomic data has become the canonical example of very large, very complex data sets. As such, there has been significant interest in ways to provide targeted database support to address issues that arise in genomic processing. Whether genomic data is truly a special case, or just another application area exhibiting problems common to other domains, is an as yet unanswered question. In this abstract, we explore the structure and processing requirements of a large-scale genome sequencing center, as a case study of the issues that arise in genomic data managements, and as a means to compare those issues with those that arise in other domains.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1198–1201},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316794,
author = {Nagarajan, Rakesh and Ahmed, Mushtaq and Phatak, Aditya},
title = {Database Challenges in the Integration of Biomedical Data Sets},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The clinical and basic science research domains present exciting and difficult data integration issues. Solving these problems is crucial as current research efforts in the field of biomedicine heavily depend upon integrated storage, querying, analysis, and visualization of clinicopathology information, genomic annotation, and large scale functional genomic research data sets. Such large scale experimental analyses are essential to decipher the pathophysiological processes occurring in most human diseases so that they may be effectively treated. In this paper, we discuss the challenges of integration of multiple biomedical data sets not only at the university level but also at the national level and present the data warehousing based solution we have employed at Washington University School of Medicine. We also describe the tools we have developed to store, query, analyze, and visualize these data sets together.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1202–1213},
numpages = {12},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316795,
author = {Stata, Raymie and Hunt, Patrick and Thiruvalluvan, M. G.},
title = {The Bloomba Personal Content Database},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We believe continued growth in the volume of personal content, together with a shift to a multi-device personal computing environment, will inevitably lead to the development of Personal Content Databases (PCDBs). These databases will make it easier for users to find, use, and replicate large, heterogeneous repositories of personal content.In this paper, we describe the PCDB used to power Bloomba, a commercial personal information manager in broad use. We highlight areas where the special requirements of personal content and personal platforms have influenced the design and implementation of our PCDB. We also discuss what we have and have not been able to leverage from the database community and suggest a few lines of research that would be useful to builders of PCDBs.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1214–1223},
numpages = {10},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316796,
author = {O'Connell, William},
title = {Trends in Data Warehousing: A Practitioner's View},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {This talk will present emerging data warehousing reference architectures, and focus on trends and directions that are shaping these enterprise installations. Implications will be highlighted, including both of new and old technology. Stack seamless integration is also pivotal to success, which also has significant implications on things such as Metadata.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1224},
numpages = {1},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316797,
author = {Bhashyam, Ramesh},
title = {Technology Challenges in a Data Warehouse},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {This presentation will discuss several database technology challenges that are faced when building a data warehouse. It will touch on the challenges posed by high capacity drives and the mechanisms in Teradata DBMS to address that. It will consider the features and capabilities required of a database in a mixed application environment of a warehouse and some solutions to address that.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1225–1226},
numpages = {2},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316798,
author = {MacNicol, Roger and French, Blaine},
title = {Sybase IQ Multiplex - Designed for Analytics},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The internal design of database systems has traditionally given primacy to the needs of transactional data. A radical re-evaluation of the internal design giving primacy to the needs of complex analytics shows clear benefits in large databases for both single servers and in multinode shared-disk grid computing. This design supports the trend to keep more years of more finely grained data online by ameliorating the data explosion problem.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1227–1230},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316799,
author = {Topaloglou, Thodoros and Davidson, Susan B. and Jagadish, H. V. and Markowitz, Victor M. and Steeg, Evan W. and Tyers, Mike},
title = {Biological Data Management: Research, Practice and Opportunities},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1233–1236},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316800,
author = {O'Connell, William and Witkowski, Andy and Bhashyam, Ramesh and Chauduri, Surajit},
title = {Where is Business Intelligence Taking Today's Database Systems?},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1237–1238},
numpages = {2},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316801,
author = {Ailamaki, Anastassia},
title = {Database Architectures for New Hardware},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Thirty years ago, DBMS stored data on disks and cached recently used data in main memory buffer pools, while designers worried about improving I/O performance and maximizing main memory utilization. Today, however, databases live in multi-level memory hierarchies that include disks, main memories, and several levels of processor caches. Four (often correlated) factors have shifted the performance bottleneck of data-intensive commercial workloads from I/O to the processor and memory subsystem. First, storage systems are becoming faster and more intelligent (now disks come complete with their own processors and caches). Second, modern database storage managers aggressively improve locality through clustering, hide I/O latencies using prefetching, and parallelize disk accesses using data striping. Third, main memories have become much larger and often hold the application's working set. Finally, the increasing memory/processor speed gap has pronounced the importance of processor caches to database performance.This tutorial will first survey the computer architecture and database literature on understanding and evaluating database application performance on modern hardware. We will present approaches and methodologies used to produce time breakdowns when executing database workloads on modern processors. We will contrast traditional methods that use system simulation to the more realistic, yet challenging use of hardware event counters. Then, we will survey techniques proposed in the literature to alleviate the problem and their evaluation. We will emphasize the importance and explain the challenges when determining the optimal data placement on all levels of memory hierarchy, and contrast to other approaches such as prefetching data and instructions. Finally, we will discuss open problems and future directions: Is it only the memory subsystem database software architects should worry about? How important are other decisions processors make to database workload behavior? Given the emerging multi-threaded, multi-processor computers with modular, deep cache hierarchies, how feasible is it to create database systems that will adapt to their environment and will automatically take full advantage of the underlying hierarchy?},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1241},
numpages = {1},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316802,
author = {Rosenthal, Arnon and Winslett, Marianne},
title = {Security of Shared Data in Large Systems: State of the Art and Research Directions},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The goals of this tutorial are to enlighten the VLDB research community about the state of the art in data security, especially for enterprise or larger systems, and to engage the community's interest in improving the state of the art. The tutorial includes numerous suggested topics for research and development projects in data security.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1242},
numpages = {1},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316803,
author = {Chaudhuri, Surajit and Dageville, Benoit and Lohman, Guy},
title = {Self-Managing Technology in Database Management Systems},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1243},
numpages = {1},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316804,
author = {Hellerstein, Joseph M.},
title = {Architectures and Algorithms for Internet-Scale (P2p) Data Management},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1244},
numpages = {1},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316805,
author = {Baeza-Yates, Ricardo and Consens, Mariano},
title = {The Continued Saga of DB-IR Integration},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1245–1246},
numpages = {2},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316806,
author = {Jiang, Daxin and Pei, Jian and Zhang, Aidong},
title = {GPX: Interactive Mining of Gene Expression Data},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Discovering co-expressed genes and coherent expression patterns in gene expression data is an important data analysis task in bioinformatics research and biomedical applications. Although various clustering methods have been proposed, two tough challenges still remain on how to integrate the users' domain knowledge and how to handle the high connectivity in the data. Recently, we have systematically studied the problem and proposed an effective approach [3]. In this paper, we describe a demonstration of GPX (for Gene Pattern eXplorer), an integrated environment for interactive exploration of coherent expression patterns and co-expressed genes in gene expression data. GPX integrates several novel techniques, including the coherent pattern index graph, a gene annotation panel, and a graphical interface, to adopt users' domain knowledge and support explorative operations in the clustering procedure. The GPX system as well as its techniques will be showcased, and the progress of GPX will be exemplified using several real-world gene expression data sets.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1249–1252},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316807,
author = {Li, Wei and Mozes, Ari},
title = {Computing Frequent Itemsets inside Oracle 10G},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Frequent itemset counting is the first step for most association rule algorithms and some classification algorithms. It is the process of counting the number of occurrences of a set of items that happen across many transactions. The goal is to find those items which occur together most often. Expressing this functionality in RDBMS engines is difficult for two reasons. First, it leads to extremely inefficient execution when using existing RDBMS operations since they are not designed to handle this type of workload. Second, it is difficult to express the special output type of itemsets.In Oracle 10G, we introduce a new SQL table function which encapsulates the work of frequent itemset counting. It accepts the input dataset along with some user-configurable information, and it directly produces the frequent itemset results. We present examples of typical computations with frequent itemset counting inside Oracle 10G. We also describe how Oracle dynamically adapts during frequent itemset execution as a result of changes in the nature of the data as well as changes in the available system resources.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1253–1256},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316808,
author = {Fan, Wei},
title = {StreamMiner: A Classifier Ensemble-Based Engine to Mine Concept-Drifting Data Streams},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We demonstrate StreamMiner, a random decision-tree ensemble based engine to mine data streams. A fundamental challenge in data stream mining applications (e.g., credit card transaction authorization, security buy-sell transaction, and phone call records, etc) is concept-drift or the discrepancy between the previously learned model and the true model in the new data. The basic problem is the ability to judiciously select data and adapt the old model to accurately match the changed concept of the data stream. StreamMiner uses several techniques to support mining over data streams with possible concept-drifts. We demonstrate the following two key functionalities of StreamMiner: 1. Detecting possible concept-drift on the fly when the trained streaming model is used to classify incoming data streams without knowing the ground truth. 2. Systematic data selection of old data and new data chunks to compute the optimal model that best fits on the changing data streams.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1257–1260},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316809,
author = {Xu, Xin and Cong, Gao and Ooi, Beng Chin and Tan, Kian-Lee and Tung, Anthony K. H.},
title = {Semantic Mining and Analysis of Gene Expression Data},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Association rules can reveal biological relevant relationship between genes and environments / categories. However, most existing association rule mining algorithms are rendered impractical on gene expression data, which typically contains thousands or tens of thousands of columns (gene expression levels), but only tens of rows (samples). The main problem is that these algorithms have an exponential dependence on the number of columns. Another shortcoming is evident that too many associations are generated from such kind of data. To this end, we have developed a novel depth-first row-wise algorithm FARMER [2] that is specially designed to efficiently discover and cluster association rules into interesting rule groups (IRGs) that satisfy user-specified minimum support, confidence and chi-square value thresholds on biological datasets as opposed to finding association rules individually. Based on FARMER, we have developed a prototype system that integrates semantic mining and visual analysis of IRGs mined from gene expression data.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1261–1264},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316810,
author = {Zhang, Ji and Lou, Meng and Ling, Tok Wang and Wang, Hai},
title = {Hos-Miner: A System for Detecting Outlyting Subspaces of High-Dimensional Data},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We identify a new and interesting high-dimensional outlier detection problem in this paper, that is, detecting the subspaces in which given data points are outliers. We call the subspaces in which a data point is an outlier as its Outlying Subspaces. In this paper, we will propose the prototype of a dynamic subspace search system, called HOS-Miner (HOS stands for High-dimensional Outlying Subspaces), that utilizes a sample-based learning process to effectively identify the outlying subspaces of a given point.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1265–1268},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316811,
author = {Lin, Jessica and Keogh, Eamonn and Lonardi, Stefano and Lankford, Jeffrey P. and Nystrom, Daonna M.},
title = {VizTree: A Tool for Visually Mining and Monitoring Massive Time Series Databases},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Moments before the launch of every space vehicle, engineering discipline specialists must make a critical go/no-go decision. The cost of a false positive, allowing a launch in spite of a fault, or a false negative, stopping a potentially successful launch, can be measured in the tens of millions of dollars, not including the cost in morale and other more intangible detriments. The Aerospace Corporation is responsible for providing engineering assessments critical to the go/no-go decision for every Department of Defense (DoD) launch vehicle. These assessments are made by constantly monitoring streaming telemetry data in the hours before launch. For this demonstration, we will introduce VizTree, a novel time-series visualization tool to aid the Aerospace analysts who must make these engineering assessments. VizTree was developed at the University of California, Riverside and is unique in that the same tool is used for mining archival data and monitoring incoming live telemetry. Unlike other time series visualization tools, VizTree can scale to very large databases, giving it the potential to be a generally useful data mining and database tool.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1269–1272},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316812,
author = {Abiteboul, Serge and Alexe, Bogdan and Benjelloun, Omar and Cautis, Bogdan and Fundulaki, Irini and Milo, Tova and Sahuguet, Arnaud},
title = {An Electronic Patient Record "on Steroids": Distributed, Peer-to-Peer, Secure and Privacy-Conscious},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1273–1276},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316813,
author = {Franconi, Enrico and Kuper, Gabriel and Lopatenko, Andrei and Zaihrayeu, Ilya},
title = {Queries and Updates in the CoDB Peer to Peer Database System},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {In this short paper we present the coDB P2P DB system. A network of databases, possibly with different schemas, are interconnected by means of GLAV coordination rules, which are inclusions of conjunctive queries, with possibly existential variables in the head; coordination rules may be cyclic. Each node can be queried in its schema for data, which the node can fetch from its neighbours, if a coordination rule is involved.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1277–1280},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316814,
author = {Liu, Haifeng and Jacobsen, Hans-Arno},
title = {A-ToPSS: A Publish/Subscribe System Supporting Imperfect Information Processing},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1281–1284},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316815,
author = {Xu, Zhengdao and Jacobsen, Hans-Arno},
title = {Efficient Constraint Processing for Highly Personalized Location Based Services},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1285–1288},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316816,
author = {Litwin, Witold and Moussa, Rim and Schwarz, Thomas J. E.},
title = {LH<sup>*</sup><sub>RS</sub>: A Highly Available Distributed Data Storage},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The ideal storage system is always available and incrementally expandable. Existing storage systems fall far from this ideal. Affordable computers and high-speed networks allow us to investigate storage architectures closer to the ideal. Our demo, present a prototype implementation of LH*RS: a highly available scalable and distributed data structure.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1289–1292},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316817,
author = {Su, Hong and Rundensteiner, Elke A. and Mani, Murali},
title = {Semantic Query Optimization in an Automata-Algebra Combined XQuery Engine over XML Streams},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1293–1296},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316818,
author = {Du, Fang and Amer-Yahia, Sihem and Freire, Juliana},
title = {ShreX: Managing XML Documents in Relational Databases},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We describe ShreX, a freely-available system for shredding, loading and querying XML documents in relational databases. ShreX supports all mapping strategies proposed in the literature as well as strategies available in commercial RDBMSs. It provides generic (mapping-independent) functions for loading shredded documents into relations and for translating XML queries into SQL. ShreX is portable and can be used with any relational database backend.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1297–1300},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316819,
author = {Choi, Byron and Fan, Wenfei and Jia, Xibei and Kasprzyk, Arek},
title = {A Uniform System for Publishing and Maintaining XML Data},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1301–1304},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316820,
author = {Mayer, Sabine and Grust, Torsten and van Keulen, Maurice and Teubner, Jens},
title = {An Injection with Tree Awareness: Adding Staircase Join to PostgreSQL},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1305–1308},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316821,
author = {Koch, Christoph and Scherzinger, Stefanie and Schweikardt, Nicole and Stegmaier, Bernhard},
title = {FluXQuery: An Optimizing XQuery Processor for Streaming XML Data},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1309–1312},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316822,
author = {Graupmann, Jens and Biwer, Michael and Zimmer, Christian and Zimmer, Patrick and Bender, Matthias and Theobald, Martin and Weikum, Gerhard},
title = {COMPASS: A Concept-Based Web Search Engine for HTML, XML, and Deep Web Data},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1313–1316},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316823,
author = {Halaschek, Chris and Aleman-Meza, Boanerges and Arpinar, I. Budak and Sheth, Amit P.},
title = {Discovering and Ranking Semantic Associations over a Large RDF Metabase},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Information retrieval over semantic metadata has recently received a great amount of interest in both industry and academia. In particular, discovering complex and meaningful relationships among this data is becoming an active research topic. Just as ranking of documents is a critical component of today's search engines, the ranking of relationships will be essential in tomorrow's semantic analytics engines. Building upon our recent work on specifying these semantic relationships, which we refer to as Semantic Associations, we demonstrate a system where these associations are discovered among a large semantic metabase represented in RDF. Additionally we employ ranking techniques to provide users with the most interesting and relevant results.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1317–1320},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316824,
author = {Crescenzi, Valter and Mecca, Giansalvatore and Merialdo, Paolo and Missier, Paolo},
title = {An Automatic Data Grabber for Large Web Sites},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We demonstrate a system to automatically grab data from data intensive web sites. The system first infers a model that describes at the intensional level the web site as a collection of classes; each class represents a set of structurally homogeneous pages, and it is associated with a small set of representative pages. Based on the model a library of wrappers, one per class, is then inferred, with the help an external wrapper generator. The model, together with the library of wrappers, can thus be used to navigate the site and extract the data.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1321–1324},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316825,
author = {Ba\"{\i}na, Karim and Benatallah, Boualem and Paik, Hye-young and Toumani, Farouk and Rey, Christophe and Rutkowska, Agnieszka and Harianto, Bryan},
title = {WS-CatalogNet: An Infrastructure for Creating, Peering, and Querying e-Catalog Communities},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1325–1328},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316826,
author = {Skogsrud, Halvard and Benatallah, Boualem and Casati, Fabio and Dinh, Manh Q.},
title = {Trust-Serv: A Lightweight Trust Negotiation Service},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1329–1332},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316827,
author = {Sarda, Parag and Haritsa, Jayant R.},
title = {Green Query Optimization: Taming Query Optimization Overheads through Plan Recycling},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {PLASTIC [1] is a recently-proposed tool to help query optimizers significantly amortize optimization overheads through a technique of plan recycling. The tool groups similar queries into clusters and uses the optimizer-generated plan for the cluster representative to execute all future queries assigned to the cluster. An earlier demo [2] had presented a basic prototype implementation of PLASTIC. We have now significantly extended the scope, useability, and efficiency of PLASTIC, by incorporating a variety of new features, including an enhanced query feature vector, variable-sized clustering and a decision-tree-based query classifier. The demo of the upgraded PLASTIC tool is shown on commercial database platforms (IBM DB2 and Oracle 9i).},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1333–1336},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316828,
author = {Raman, Vijayshankar and Markl, Volker and Simmen, David and Lohman, Guy and Pirahesh, Hamid},
title = {Progressive Optimization in Action},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Progressive Optimization (POP) is a technique to make query plans robust, and minimize need for DBA intervention, by repeatedly re-optimizing a query during runtime if the cardinalities estimated during optimization prove to be significantly incorrect. POP works by carefully calculating validity ranges for each plan operator under which the overall plan can be optimal. POP then instruments the query plan with checkpoints that validate at runtime that cardinalities do lie within validity ranges, and re-optimizes the query otherwise. In this demonstration we showcase POP implemented for a research prototype version of IBM's DB2 DBMS, using a mix of real-world and synthetic benchmark databases and workloads. For selected queries of the workload we display the query plans with validity ranges as well as the placement of the various kinds of CHECK operators using the DB2 graphical plan explain tool. We also execute the queries, showing how and where re-optimization is triggered through the CHECK operators, the new plan generated upon re-optimization, and the extent to which previously computed intermediate results are reused.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1337–1340},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316829,
author = {Ilyas, Ihab F. and Markl, Volker and Haas, Peter J. and Brown, Paul G. and Aboulnaga, Ashraf},
title = {CORDS: Automatic Generation of Correlation Statistics in DB2},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {When query optimizers erroneously assume that database columns are statistically independent, they can underestimate the selectivities of conjunctive predicates by orders of magnitude. Such underestimation often leads to drastically suboptimal query execution plans. We demonstrate cords, an efficient and scalable tool for automatic discovery of correlations and soft functional dependencies between column pairs. We apply cords to real, synthetic, and TPC-H benchmark data, and show that cords discovers correlations in an efficient and scalable manner. The output of cords can be visualized graphically, making cords a useful mining and analysis tool for database administrators. cords ranks the discovered correlated column pairs and recommends to the optimizer a set of statistics to collect for the "most important" of the pairs. Use of these statistics speeds up processing times by orders of magnitude for a wide range of queries.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1341–1344},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316830,
author = {Kraft, Tobias and Schwarz, Holger},
title = {CHICAGO: A Test and Evaluation Environment for Coarse-Grained Optimization},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Relational OLAP tools and other database applications generate sequences of SQL statements that are sent to the database server as result of a single information request issued by a user. Coarse-Grained Optimization is a practical approach for the optimization of such statement sequences based on rewrite rules. In this demonstration we present the CHICAGO test and evaluation environment that allows to assess the effectiveness of rewrite rules and control strategies. It includes a lightweight heuristic optimizer that modifies a given statement sequence using a small and variable set of rewrite rules.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1345–1348},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316831,
author = {Teniente, Ernest and Farr\'{e}, Carles and Urp\'{\i}, Toni and Beltr\'{a}n, Carlos and Ga\~{n}\'{a}n, David},
title = {SVT: Schema Validation Tool for Microsoft SQL-Server},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {We present SVT, a tool for validating database schemas in SQL Server. This is done by means of testing desirable properties that a database schema should satisfy. To our knowledge, no commercial relational DBMS provides yet a tool able to perform such kind of validation.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1349–1352},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316832,
author = {Rundensteiner, Elke A. and Ding, Luping and Sutherland, Timothy and Zhu, Yali and Pielech, Brad and Mehta, Nishant},
title = {CAPE: Continuous Query Engine with Heterogeneous-Grained Adaptivity},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1353–1356},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316833,
author = {Cooper, Owen and Edakkunni, Anil and Franklin, Michael J. and Hong, Wei and Jeffery, Shawn R. and Krishnamurthy, Sailesh and Reiss, Fredrick and Rizvi, Shariq and Wu, Eugene},
title = {HiFi: A Unified Architecture for High Fan-in Systems},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Advances in data acquisition and sensor technologies are leading towards the development of "High Fan-in" architectures: widely distributed systems whose edges consist of numerous receptors such as sensor networks and RFID readers and whose interior nodes consist of traditional host computers organized using the principle of successive aggregation. Such architectures pose significant new data management challenges. The HiFi system, under development at UC Berkeley, is aimed at addressing these challenges. We demonstrate an initial prototype of HiFi that uses data stream query processing to acquire, filter, and aggregate data from multiple devices including sensor motes, RFID readers, and low power gateways organized as a High Fan-in system.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1357–1360},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316834,
author = {Abadi, Daniel J. and Lindner, Wolfgang and Madden, Samuel and Schuler, J\"{o}rg},
title = {An Integration Framework for Sensor Networks and Data Stream Management Systems},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {This demonstration shows an integrated query processing environment where users can seamlessly query both a data stream management system and a sensor network with one query expression. By integrating the two query processing systems, the optimization goals of the sensor network (primarily power) and server network (primarily latency and quality) can be unified into one quality of service metric. The demo shows various steps of the unified optimization process for a sample query where the effects of each step that the optimizer takes can be directly viewed using a quality of service monitor. Our demo includes sensors deployed in the demo area in a tiny mockup of a factory application.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1361–1364},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316835,
author = {Schmidt, Sven and Berthold, Henrike and Lehner, Wolfgang},
title = {QStream: Deterministic Querying of Data Streams},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {Current developments in processing data streams are based on the best-effort principle and therefore not adequate for many application areas. When sensor data is gathered by interface hardware and is used for triggering data-dependent actions, the data has to be queried and processed not only in an efficient but also in a deterministic way. Our streaming system prototype embodies novel data processing techniques. It is based on an operator component model and runs on top of a real-time capable environment. This enables us to provide real Quality-of-Service for data stream queries.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1365–1368},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316836,
author = {Sharifzadeh, Mehdi and Shahabi, Cyrus and Navai, Bahareh and Parvini, Farid and Rizzo, Albert A.},
title = {AIDA: An Adaptive Immersive Data Analyzer},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {In this demonstration, we show various querying capabilities of an application called AIDA. AIDA is developed to help the study of attention disorder in kids. In a different study [1], we collected several immresive sensory data streams from kids monitored in an immersive application called the virtual classroom. This dataset, termed immersidata is used to analyze the behavior of kids in the virtual classroom environment. AIDA's database stores all the geometry of the objects in the virtual classroom environment and their spatio-temporal behavior. In addition, it stores all the immersidata collected from the kids experimenting with the application. AIDA's graphical user interface then supports various spatio-temporal queries on these datasets. Moreover, AIDA replays the immersidata streams as if they are collected in real-time and on which supports various continuous queries. This demonstration is a proof-of-concept prototype of a typical design and development of a domain-specific query and analysis application on the users' interaction data with immersive environments.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1369–1372},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316837,
author = {Ulusoy, \"{O}zg\"{u}r and G\"{u}d\"{u}kbay, U\u{g}ur and D\"{o}nderler, Mehmet Emin and Saykol, Ediz and Alper, Cemil},
title = {BilVideo Video Database Management System},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {A prototype video database management system, which we call BilVideo, is presented. BilVideo provides an integrated support for queries on spatio-temporal, semantic and low-level features (color, shape, and texture) on video data. BilVideo does not target a specific application, and thus, it can be used to support any application with video data. An example application, news archives search system, is presented with some sample queries.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1371–1376},
numpages = {6},
location = {Toronto, Canada},
series = {VLDB '04}
}

@inproceedings{10.5555/1316689.1316838,
author = {Mokbel, Mohamed F. and Xiong, Xiaopeng and Aref, Walid G. and Hambrusch, Susanne E. and Prabhakar, Sunil and Hammad, Moustafa A.},
title = {PLACE: A Query Processor for Handling Real-Time Spatio-Temporal Data Streams},
year = {2004},
isbn = {0120884690},
publisher = {VLDB Endowment},
abstract = {The emergence of location-aware services calls for new real-time spatio-temporal query processing algorithms that deal with large numbers of mobile objects and queries. In this demo, we present PLACE (Pervasive Location-Aware Computing Environments); a scalable location-aware database server developed at Purdue University. The PLACE server addresses scalability by adopting an incremental evaluation mechanism for answering concurrently executing continuous spatio-temporal queries. The PLACE server supports a wide variety of stationery and moving continuous spatio-temporal queries through a set of pipelined spatio-temporal operators. The large numbers of moving objects generate real-time spatio-temporal data streams.},
booktitle = {Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30},
pages = {1377–1380},
numpages = {4},
location = {Toronto, Canada},
series = {VLDB '04}
}

