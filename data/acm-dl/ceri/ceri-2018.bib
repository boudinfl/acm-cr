@inproceedings{10.1145/3230599.3230601,
author = {Buscaldi, Davide},
title = {Improving Access to Scientific Literature: A Semantic IR Perspective},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230601},
doi = {10.1145/3230599.3230601},
abstract = {Nowadays, the flow of data and publications in almost every field of research is continuously growing. Some estimates place the growth rate in the number of scientific publications between 2.2% and 14% per year, depending on the type and the domain of the publication [6]. This data deluge presents a bottleneck for scientific progress and a challenge for existing search engines. The problems to be solved are some old ones: the ambiguity of a concept, especially among different research fields (for instance, "lattice" in computer science vs. physics), and the synonymy (or quasi-synonymy) of concepts that are expressed in different ways: for instance, "opinion mining" and "sentiment analysis". These issues may affect various tasks: a researcher building a state of the art for a specific topic, an editor finding reviewers for a given paper, or a government official studying a project proposal, among others.The need to go beyond the mere document retrieval in the context of scientific literature is corroborated by the proliferation of related projects and works, and the organization of new shared tasks, in particular the ScienceIE task at SemEval-2017, focused on the identification of keyphrases representing topics, methods, data and tools [1], and task-7 at Semeval-2018 about semantic relation extraction and classification in scientific papers [3].Some recent works address the problem with the help of structured lists of known keywords, such as Rexplore [7], which integrates statistical analysis with semantic technologies, or by analyzing the citation network among various papers, such as in CiteSpace [2]. In most cases, the relevance, or impact, of a paper is assessed by the number of citations it receives. However, Oren Etzioni1 observed that "Academics may cite papers for non-essential reasons - out of courtesy, for completeness or to promote their own publications. These superfluous citations can impede literature searches and exaggerate a paper's importance" and therefore it is necessary to use Artificial Intelligence to discover the meaning and the importance of a specific citation.Recently, at LIPN we started working on the access to scientific information from a semantic information retrieval perspective, therefore leveraging the use of ontologies and similar semantic resources for this task. The first step has been to build a typology of semantic relations [4] that are often used in state of the art sections of scientific paper. Some of these relations link methods and the problems they solve, others link a resource and a system that used it. This typology can evolve or be integrated into more complex ontologies. The next step was to verify whether it is possible to detect these relations automatically. We focused on unsupervised methods that exploit the information coming from keywords and patterns around the entities that are connected by the relations, and tested the possibility to improve these results using semantic embeddings [5]. We produced a set of annotated documents that were used for task-7 at SemEval 2018, where various participants showed the effectiveness of Deep Neural Networks (DNN) methods to detect and classify the relations [3]. The results show that these methods are usually able to predict with a high accuracy (85 - 90%) the type of a relation, if they are fed the information about the linked entities, but there is still a lot of work to be done for the detection of the relations (~ 50% for the best system).},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {1},
numpages = {1},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230602,
author = {de Bernardo, Guillermo and Ladra, Susana},
title = {About BIRDS Project (Bioinformatics and Information Retrieval Data Structures Analysis and Design)},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230602},
doi = {10.1145/3230599.3230602},
abstract = {BIRDS stands for "Bioinformatics and Information Retrieval Data Structures analysis and design" and is a 4-year project (2016--2019) that has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 690941.The overall goal of BIRDS is to establish a long term international network involving leading researchers in the development of efficient data structures in the fields of Bioinformatics and Information Retrieval, to strengthen the partnership through the exchange of knowledge and expertise, and to develop integrated approaches to improve current approaches in both fields. The research will address challenges in storing, processing, indexing, searching and navigating genome-scale data by designing new algorithms and data structures for sequence analysis, networks representation or compressing and indexing repetitive data.BIRDS project is carried out by 7 research institutions from Australia (University of Melbourne), Chile (University of Chile and University of Concepti\'{o}n), Finland (University of Helsinki), Japan (Kyushu University), Portugal (Instituto de Engenharia de Sistemas e Computadores, Investiga\c{c}\~{a}o e Desenvolvimento em Lisboa, INESC-ID), and Spain (University of A Coru\~{n}a), and a Spanish SME (Enxenio S.L.). It is coordinated by the University of A Coru\~{n}a (Spain).},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {2},
numpages = {4},
keywords = {data structures, bioinformatics, algorithms, MSCA RISE, H2020, information retrieval},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230603,
author = {Valcarce, Daniel and Parapar, Javier and Barreiro, \'{A}lvaro},
title = {Query Expansion as a Matrix Factorization Problem: Extended Abstract},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230603},
doi = {10.1145/3230599.3230603},
abstract = {Pseudo-relevance feedback (PRF) provides an automatic method for query expansion in Information Retrieval. These techniques find relevant expansion terms using the top retrieved documents with the original query. In this paper, we present an approach based on linear methods called LiMe that formulates the PRF task as a matrix factorization problem. LiMe learns an inter-term similarity matrix from the pseudo-relevant set and the query that uses for computing expansion terms. The experiments on five datasets show that LiMe outperforms state-of-the-art baselines in most cases.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {3},
numpages = {4},
keywords = {Linear methods, query expansion, linear least squares, pseudo-relevance feedback},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230604,
author = {Aznar-Gimeno, Roc\'{\i}o and del Carmen Rodr\'{\i}guez-Hern\'{a}ndez, Mar\'{\i}a and del-Hoyo-Alonso, Rafael and Ilarri, Sergio},
title = {Towards a Structured Representation of Results in an Information Retrieval System for Public Examination Calls},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230604},
doi = {10.1145/3230599.3230604},
abstract = {Nowadays, the huge amount of information available may easily overwhelm users. Information Retrieval techniques can help the user to find what he/she needs, but there are still challenges to solve within this research area. An example is the problem of minimizing the user's search time to find specific information in unstructured texts within the retrieved documents, in different application domains. The use of supervised learning-based information extraction techniques can be a solution to this problem. However, a supervised learning model requires as input a large labeled dataset, generated manually by experts. Moreover, there are currently very few information extraction frameworks that allow to reduce or avoid the human effort needed to label such training datasets.In this paper, we present our work in progress towards the development of an information retrieval system that will display structured, centralized and updated information extracted from documents corresponding to calls for public examinations. In this scenario, the search engine should be able not only to display the documents relevant to the user's query, but also specific data contained in the documents. In addition, we present a study of frameworks that can be used in this context as well as our preliminary experience with the use of the Snorkel framework. In the future, we plan to complete our proposal and also extend it for other types of documents published in Spanish official bulletins.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {4},
numpages = {8},
keywords = {Snorkel, supervised machine learning, Information Extraction, representation of structured information},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230605,
author = {Bellog\'{\i}n, Alejandro and S\'{a}nchez, Pablo},
title = {Applying Subsequence Matching to Collaborative Filtering: Extended Abstract},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230605},
doi = {10.1145/3230599.3230605},
abstract = {Neighbourhood-based approaches, although they are one of the most popular strategies in the recommender systems area, continue using classic similarities that leave aside the sequential information of the users interactions. In this extended abstract, we summarise the main contributions of our previous work where we proposed to use the Longest Common Subsequence algorithm as a similarity measure between users, by adapting it to the recommender systems context and proposing a mechanism to transform users interactions into sequences. Furthermore, we also introduced some modifications on the original LCS algorithm to allow non-exact matchings between users and to bound the similarities obtained in the [0,1] interval. Our reported results showed that our LCS-based similarity was able to outperform different state-of-the-art recommenders in two datasets in both ranking and novelty and diversity metrics.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {5},
numpages = {2},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230606,
author = {Su\'{a}rez-Garc\'{\i}a, Eva and Landin, Alfonso and Valcarce, Daniel and Barreiro, \'{A}lvaro},
title = {Term Association Measures for Memory-Based Recommender Systems},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230606},
doi = {10.1145/3230599.3230606},
abstract = {The adaptation of Information Retrieval techniques for the item recommendation task has become a fertile research area. Previous works have established the correspondence between these two fields that allowed to adapt several retrieval techniques successfully. One line of study aims to model the item recommendation problem as a profile expansion task following the methods for query expansion in pseudo-relevance feedback. To solve the query expansion task in ad-hoc retrieval, several term association measures have been proposed in the past. In this paper, we adapt several of these measures to the top-N recommendation problem, specifically to the collaborative filtering scenario. Moreover, we perform experiments to study their effectiveness regarding accuracy, diversity and novelty. Our results show that some of the proposed measures can improve these aspects over well-known and commonly used recommendation similarity metrics (cosine similarity and Pearson's correlation coefficient).},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {6},
numpages = {8},
keywords = {recommender systems, Term association measures, collaborative filtering},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230607,
author = {Albusac, C\'{e}sar and de Campos, Luis M. and Fern\'{a}ndez-Luna, Juan M. and Huete, Juan F.},
title = {Content-Based Recommendation for Academic Expert Finding},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230607},
doi = {10.1145/3230599.3230607},
abstract = {Nowadays it is more and more frequent that Web users search for professionals in order to find people who can help solve any problem in a given field. This is call expert finding. A particular case is when users are interested in scientific researchers. The associated problem is to get, given a query that expresses a topic of interest for a user, a set of researchers who are expert on it. One of the difficulties to tackle the problem is to indentify the topics in which a professional is expert. In this paper, we face this problem from a content-based recommendatation perspective and we present a method where, starting from the articles published by each researcher, and a query, the expert researchers are obtained. We also present a new document collection, called PMSC-UGR, specifically designed for the evaluation in the field of expert finding and document filtering},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {7},
numpages = {8},
keywords = {Sistema de recomendaci\'{o}n, Recomendaci\'{o}n basada en contenido, B\'{u}squeda de expertos},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230608,
author = {Alkharashi, Abdulwhab and Jose, Joemon},
title = {Vandalism on Collaborative Web Communities: An Exploration of Editorial Behaviour in Wikipedia},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230608},
doi = {10.1145/3230599.3230608},
abstract = {Modern online discussion communities allow people to contribute, sometimes anonymously. Such flexibility sometimes threatens the reputation and reliability of community-owned resources. Such flexibility is understandable, however, they engender threats to the reputation and reliability in collective goods. Since not a lot of previous work addressed these issues it is important to study the aforementioned issues to build an innate understanding of recent ongoing vandalism of Wikipedia pages and ways to preventing those.In this study, we consider the type of activity that the anonymous users carry out on Wikipedia and also contemplate how others react to their activities. In particular, we want to study vandalism of Wikipedia pages and ways of preventing this kind of activity. Our preliminary analysis reveals (~ 90%) of the vandalism or foul edits are done by unregistered users in Wikipedia due to nature of openness. The community reaction seemed to be immediate: most vandalisms were reverted within five minutes on an average. Further analysis shed light on the tolerance of Wikipedia community, reliability of anonymous users revisions and feasibility of early prediction of vandalism.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {8},
numpages = {4},
keywords = {Crowdsourcing, Encyclopedia, Vandalism},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230609,
author = {Cacheda, Fidel and Barbieri, Nicola},
title = {Click-through Prediction When Searching Local Businesses},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230609},
doi = {10.1145/3230599.3230609},
abstract = {Local search engines allow users to issue queries with a geographical connotation, named local searches, against a business database. Local search differs from traditional search in that, in order to capture adequately the user behaviour, the relevance estimation must integrate geographical signals, such as distance.In this work we investigate the problem of estimating the click-through in local searches using standard search methods along with a set of geographical features and business related. Our approach is validated using the logs of a local search engine. The evaluation shows how the non-linear combination of features of business, geo-local and textual allow a significant improvement over state-of-the-art alternatives based on text relevance, distance and business reputation.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {9},
numpages = {2},
keywords = {an\'{a}lisis comportamiento usuario, B\'{u}squeda local, predicci\'{o}n clickthrough},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230610,
author = {Ilarri, Sergio and Az\'{o}n, Guillermo},
title = {Towards the Development of a Tool to Keep Track of Interesting Information in a Sea of Digital Documents: Short Paper (Work in Progress)},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230610},
doi = {10.1145/3230599.3230610},
abstract = {Managing the current data deluge is a great challenge for users. Emails are constantly arriving, notifications of tweets and RSS feeds keep popping out, newspapers and blogs of different types publish potentially-relevant news every day, etc. If a user wants to keep track of certain topics in an efficient way, a careful filtering is needed in order to keep the number of items to review manageable, as otherwise the user may finally give up or just perform some random or casual reading. Automated tools can help the user to perform this initial selection, and thus to minimize the feeling of being overwhelmed that the user may experience.In this short paper, we present our ongoing work for the development of DodoAid, a recommender of digital objects that attempts to alleviate the current user's overload when he/she wants to follow information about certain topics. Beyond the application of information retrieval and text mining techniques, it can also apply techniques from the field of recommender systems to suggest items that not only fit topics of interest for the user but are also expected to be valuable according to the individual user's preferences, which can be learnt automatically in an implicit way.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {10},
numpages = {4},
keywords = {Information Retrieval, text mining, recommender systems},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230611,
author = {De la Pe\~{n}a Sarrac\'{e}n, Gretel Liz and Rosso, Paolo},
title = {Automatic Text Summarization Based on Betweenness Centrality},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230611},
doi = {10.1145/3230599.3230611},
abstract = {Automatic text summary plays an important role in information retrieval. With a large volume of information, presenting the user only a summary greatly facilitates the search work of the most relevant. Therefore, this task can provide a solution to the problem of information overload. Automatic text summary is a process of automatically creating a compressed version of a certain text that provides useful information for users. This article presents an unsupervised extractive approach based on graphs. The method constructs an indirected weighted graph from the original text by adding a vertex for each sentence, and calculates a weighted edge between each pair of sentences that is based on a similarity/dissimilarity criterion. The main contribution of the work is that we do a study of the impact of a known algorithm for the social network analysis, which allows to analyze large graphs efficiently. As a measure to select the most relevant sentences, we use betweenness centrality. The method was evaluated in an open reference data set of DUC2002 with Rouge scores.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {11},
numpages = {4},
keywords = {Extractive Summary, Automatic Text Summarization, Betweenness Centrality},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230612,
author = {Losada, David E. and Parapar, Javier and Barreiro, Alvaro},
title = {Cost-Effective Construction of Information Retrieval Test Collections},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230612},
doi = {10.1145/3230599.3230612},
abstract = {In this paper we describe our recent research on effective construction of Information Retrieval collections. Relevance assessments are a core component of test collections, but they are expensive to produce. For each test query, only a sample of documents in the corpus can be assessed for relevance. We discuss here a class of document adjudication methods that iteratively choose documents based on reinforcement learning. Given a pool of candidate documents supplied by multiple retrieval systems, the production of relevance assessments is modeled as a multi-armed bandit problem. These bandit-based algorithms identify relevant documents with minimal effort. One instance of these models has been adopted by NIST to build the test collection of the TREC 2017 common core track.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {12},
numpages = {2},
keywords = {pooling, Information Retrieval evaluation, relevance assessments, multi-armed bandits},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230613,
author = {Tapia, Joofre Honores and Iglesias, Diego Fern\'{a}ndez and Seijo, Fidel Cacheda},
title = {Trends in the Creation of Spanish Web Sites and Their Active Service},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230613},
doi = {10.1145/3230599.3230613},
abstract = {In this research we present statistical information about the domainses that have been created since January 2007 until February 2018, with the objective of providing a clear vision of the different errors they present and how they have changed throughout the years. Moreover, we also provide relevant data about the response times from the web servers when replying to a service request for a web domain, the web page size or the number of input and output links.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {13},
numpages = {6},
keywords = {Internet, Dominios.es, Web Espa\~{n}ola},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230614,
author = {S\'{a}nchez, Pablo and Mesas, Rus M. and Bellog\'{\i}n, Alejandro},
title = {New Approaches for Evaluation: Correctness and Freshness: Extended Abstract},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230614},
doi = {10.1145/3230599.3230614},
abstract = {The main goal of a Recommender System is to suggest relevant items to users, although other utility dimensions -- such as diversity, novelty, confidence, possibility of providing explanations -- are often considered. In this work, we study two dimensions that have been neglected so far in the literature: coverage and temporal novelty. On the one hand, we present a family of metrics that combine precision and coverage in a principled manner (correctness); on the other hand, we provide a measure to account for how much a system is promoting fresh items in its recommendations (freshness). Empirical results show the usefulness of these new metrics to capture more nuances of the recommendation quality.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {14},
numpages = {2},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230615,
author = {D\'{\i}az-Corona, Dayany and Lacasta, Javier and Nogueras-Iso, Javier},
title = {Barriers for the Access to Knowledge Models in Linked Data Cultural Heritage Collections},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230615},
doi = {10.1145/3230599.3230615},
abstract = {During last years governments have promoted the digitisation and on-line access to cultural heritage due to its importance for supporting the knowledge economy. Initiatives like Europeana or UNESCO Memory of the World Programme have encouraged the creation of cultural heritage repositories, which currently contain millions of digitized items. Many of these repositories have adopted semantic web technologies and Linked Data approaches for their publication. However, the way in which they are described does not follow in many cases the best practices in the field. This work details the problems identified when analyzing the way the cultural heritage resources are classified in these semantic repositories. It specifically focuses on the content of the thematic annotation of these resources, but many of the identified problems can be extrapolated to other descriptors.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {15},
numpages = {3},
keywords = {Knowledge organization models, Cultural heritage repositories, Linked Data},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230616,
author = {Berlanga, Rafael and Yepes, Antonio Jimeno and P\'{e}rez, Mar\'{\i}a and Lanza-Cruz, Indira},
title = {Coarse-Grained Semantic Characterization of Large Knowledge Resources},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230616},
doi = {10.1145/3230599.3230616},
abstract = {This work presents an experimental study about the automatic assignment of semantic groups to concepts of large knowledge resources (KR) such as DBpedia1 or BabelNet2. Our proposal combines a simple lexico-statistical method for hypernym extraction combined with document and word embeddings extracted from Wikipedia. Results are encouraging and open new directions for improving other tasks related to large KR management like debugging and semantic annotation.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {16},
numpages = {4},
keywords = {Semantic Embeddings, Automatic Semantic Indexing, Knowledge Resources},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230617,
author = {Tramullas, Jes\'{u}s and Garrido-Picazo, Piedad and S\'{a}nchez-Casab\'{o}n, Ana I.},
title = {Use of Wikipedia Categories on Information Retrieval Research: A Brief Review},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230617},
doi = {10.1145/3230599.3230617},
abstract = {Wikipedia categories, a classification scheme built for organizing and describing Wikpedia articles, are being applied in computer science research. This paper adopts a systematic literature review approach, in order to identify different approaches and uses of Wikipedia categories in information retrieval research. Several types of work are identified, depending on the intrinsic study of the categories structure, or its use as a tool for the processing and analysis of other documentary corpus different to Wikipedia. Information retrieval is identified as one of the major areas of use, in particular its application in the refinement and improvement of search expressions, and the construction of textual corpus. However, the set of available works shows that in many cases research approaches applied and results obtained can be integrated into a comprehensive and inclusive concept of information retrieval.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {17},
numpages = {4},
keywords = {information retrieval, categories, classification, Wikipedia},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230618,
author = {Mart\'{\i}nez-Casta\~{n}o, Rodrigo and Pichel, Juan C. and Losada, David E.},
title = {Building Python-Based Topologies for Massive Processing of Social Media Data in Real Time},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230618},
doi = {10.1145/3230599.3230618},
abstract = {In this paper we propose a streaming approach for real-time processing of huge amounts of data. CATENAE is a library for easy building and execution of Python topologies (e.g., web crawler, classifier). Topologies are designed for their deployment inside Docker containers and, thus, horizontal scaling, granular resource assignment and isolation can be achieved easily. Furthermore, micromodules can have its own dependencies (including the Python version), allowing the user to limit resources such as CPU or memory by instance. We describe an implementation of a use case composed of two topologies: (1) a crawler for tracking users in social media and (2) an early risk detector of depression. We also explain how CATENAE topologies can be connected to non-Python systems.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {18},
numpages = {8},
keywords = {Social Media, Depression, Python, Stream Processing, Real-Time Processing, Text Mining, Docker},
location = {Zaragoza, Spain},
series = {CERI '18}
}

@inproceedings{10.1145/3230599.3230619,
author = {Sanz-Cruzado, Javier and Pepa, Sof\'{\i}a M. and Castells, Pablo},
title = {Recommending Contacts in Social Networks Using Information Retrieval Models},
year = {2018},
isbn = {9781450365437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230599.3230619},
doi = {10.1145/3230599.3230619},
abstract = {The fast expansion of online social networks has given rise to new challenges and opportunities for information retrieval and, as a particular area, recommender systems. A particularly compelling problem in this context is recommending contacts, that is, automatically predicting people that a given user may wish or benefit from connecting to in the network. This task has interesting particularities compared to more traditional recommendation domains, a salient one being that recommended items belong to the same space as the users they are recommended to. In this paper, we explore the connection between the contact recommendation and the information retrieval (IR) tasks. Specifically, we research the adaptation of IR models for recommending contacts in social networks. We report experiments over data downloaded from Twitter where we observe that IR models are competitive compared to state-of-the art contact recommendation methods.},
booktitle = {Proceedings of the 5th Spanish Conference on Information Retrieval},
articleno = {19},
numpages = {8},
keywords = {information retrieval, recommender systems, IR models, social networks, contact recommendation},
location = {Zaragoza, Spain},
series = {CERI '18}
}

