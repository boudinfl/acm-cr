@inproceedings{10.1007/978-3-540-30211-7_1,
author = {Denecke, Matthias and Dohsaka, Kohji and Nakano, Mikio},
title = {Fast Reinforcement Learning of Dialogue Policies Using Stable Function Approximation},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_1},
doi = {10.1007/978-3-540-30211-7_1},
abstract = {We propose a method to speed up reinforcement learning of policies for spoken dialogue systems. This is achieved by combining a coarse grained abstract representation of states and actions with learning only in frequently visited states. The value of unsampled states is approximated by a linear interpolation of known states. Experiments show that the proposed method effectively optimizes dialogue strategies for frequently visited dialogue states.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {1–11},
numpages = {11},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_2,
author = {Kawahara, Daisuke and Kurohashi, Sadao},
title = {Zero Pronoun Resolution Based on Automatically Constructed Case Frames and Structural Preference of Antecedents},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_2},
doi = {10.1007/978-3-540-30211-7_2},
abstract = {This paper describes a method to detect and resolve zero pronouns in Japanese text. We detect zero pronouns by case analysis based on automatically constructed case frames, and select their appropriate antecedents based on similarity to examples in the case frames. We also introduce structural preference of antecedents to precisely capture the tendency that a zero pronoun has its antecedent in its close position. Experimental results on 100 articles indicated that the precision and recall of zero pronoun detection is 87.1% and 74.8% respectively and the accuracy of antecedent estimation is 61.8%.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {12–21},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_3,
author = {Yang, Xiaofeng and Zhou, Guodong and Su, Jian and Tan, Chew Lim},
title = {Improving Noun Phrase Coreference Resolution by Matching Strings},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_3},
doi = {10.1007/978-3-540-30211-7_3},
abstract = {In this paper we present a noun phrase coreference resolution system which aims to enhance the identification of the coreference realized by string matching. For this purpose, we make two extensions to the standard learn-ing-based resolution framework. First, to improve the recall rate, we introduce an additional set of features to capture the different matching patterns between noun phrases. Second, to improve the precision, we modify the instance selection strategy to allow non-anaphors to be included during training instance generation. The evaluation done on MEDLINE data set shows that the combination of the two extensions provides significant gains in the F-measure.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {22–31},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_4,
author = {Zhang, Zhu and Radev, Dragomir},
title = {Combining Labeled and Unlabeled Data for Learning Cross-Document Structural Relationships},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_4},
doi = {10.1007/978-3-540-30211-7_4},
abstract = {Multi-document discourse analysis has emerged with the potential of improving various NLP applications. Based on the newly proposed Cross-document Structure Theory (CST), this paper describes an empirical study that classifies CST relationships between sentence pairs extracted from topically related documents, exploiting both labeled and unlabeled data. We investigate a binary classifier for determining existence of structural relationships and a full classifier using the full taxonomy of relationships. We show that in both cases the exploitation of unlabeled data helps improve the performance of learned classifiers.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {32–41},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_5,
author = {Kim, Jong-Bok and Yang, Jaehyung},
title = {Parsing Mixed Constructions in a Type Feature Structure Grammar},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_5},
doi = {10.1007/978-3-540-30211-7_5},
abstract = {Because of the mixed properties of nominal and verbal properties, Korean gerundive phrases (GPs) posit intriguing issues to both theoretical as well as computational analyses. Various theoretical approaches have been proposed to solve this puzzle, but they all have ended up abandoning or modifying fundamental theory-neutral desiderata such as endocentricity (every phrase has a head), lexicalism (no syntactic rule refers to the word-internal structure), and null licensing (abstract entities are avoided if possible) (cf. Pullum 1991, Malouf 1998). This paper shows that it is possible to analyze and efficiently parse the mixed properties of Korean GPs in a way that maintains the desiderata while avoiding abstract entities. This has been achieved through Korean Phrase Structure Grammar, an extension of HPSG that models human languages as systems of constraints on typed feature structures. The feasibility of the grammar is tested by implementing it into the LKB (Linguistics Knowledge Building) system (cf. Copestake 2002).},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {42–51},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_6,
author = {Tsuruoka, Yoshimasa and Tsujii, Jun’ichi},
title = {Iterative CKY Parsing for Probabilistic Context-Free Grammars},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_6},
doi = {10.1007/978-3-540-30211-7_6},
abstract = {This paper presents an iterative CKY parsing algorithm for probabilistic context-free grammars (PCFG). This algorithm enables us to prune unnecessary edges produced during parsing, which results in more efficient parsing. Since pruning is done by using the edge’s inside Viterbi probability and the upper-bound of the outside Viterbi probability, this algorithm guarantees to output the exact Viterbi parse, unlike beam-search or best-first strategies. Experimental results using the Penn Treebank II corpus show that the iterative CKY achieved more than 60% reduction of edges compared with the conventional CKY algorithm and the run-time overhead is very small. Our algorithm is general enough to incorporate a more sophisticated estimation function, which should lead to more efficient parsing.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {52–60},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_7,
author = {Chang, Du-Seong and Choi, Key-Sun},
title = {Causal Relation Extraction Using Cue Phrase and Lexical Pair Probabilities},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_7},
doi = {10.1007/978-3-540-30211-7_7},
abstract = {This work aims to extract causal relations that exist between two events expressed by noun phrases or sentences. The previous works for the causality made use of causal patterns such as causal verbs. We concentrate on the information obtained from other causal event pairs. If two event pairs share some lexical pairs and one of them is revealed to be causally related, the causal probability of another event pair tends to increase. We introduce the lexical pair probability and the cue phrase probability. These probabilities are learned from raw corpus in unsupervised manner. With these probabilities and the Naive Bayes classifier, we try to resolve the causal relation extraction problem. Our inter-NP causal relation extraction shows the precision of 81.29%, that is 7.05% improvement over the baseline model. The proposed models are also applied to inter-sentence causal relation extraction.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {61–70},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_8,
author = {Chang, Yi and Xu, Hongbo and Bai, Shuo},
title = {A Re-Examination of IR Techniques in QA System},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_8},
doi = {10.1007/978-3-540-30211-7_8},
abstract = {The performance of Information Retrieval in the Question Answering system is not satisfactory from our experiences in TREC QA Track. In this article, we take a comparative study to re-examine IR techniques on document retrieval and sentence level retrieval respectively. Our study shows: 1) query reformulation should be a necessary step to achieve a better retrieval performance; 2) The techniques for document retrieval are also effective in sentence level retrieval, and single sentence will be the appropriate retrieval granularity.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {71–80},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_9,
author = {Du, Yongping and Huang, Xuanjing and Li, Xin and Wu, Lide},
title = {A Novel Pattern Learning Method for Open Domain Question Answering},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_9},
doi = {10.1007/978-3-540-30211-7_9},
abstract = {Open Domain Question Answering (QA) represents an advanced application of natural language processing. We develop a novel pattern based method for implementing answer extraction in QA. For each type of question, the corresponding answer patterns can be learned from the Web automatically. Given a new question, these answer patterns can be applied to find the answer. Although many other QA systems have used pattern based method, however, it is noteworthy that our method has been implemented automatically and it can handle the problem other system failed, and satisfactory results have been achieved. Finally, we give a performance analysis of this approach using the TREC-11 question set.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {81–89},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_10,
author = {Guo, Honglei and Jiang, Jianmin and Hu, Gang and Zhang, Tong},
title = {Chinese Named Entity Recognition Based on Multilevel Linguistic Features},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_10},
doi = {10.1007/978-3-540-30211-7_10},
abstract = {This paper presents a Chinese named entity recognition system that employs the Robust Risk Minimization (RRM) classification method and incorporates the advantages of character-based and word-based models. From experiments on a large-scale corpus, we show that significant performance enhancements can be obtained by integrating various linguistic information (such as Chinese word segmentation, semantic types, part of speech, and named entity triggers) into a basic Chinese character based model. A novel feature weighting mechanism is also employed to obtain more useful cues from most important linguistic features. Moreover, to overcome the limitation of computational resources in building a high-quality named entity recognition system from a large-scale corpus, informative samples are selected by an active learning approach.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {90–99},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_11,
author = {Cheong, Paulo and Song, Dawei and Bruza, Peter and Wong, Kam-Fai},
title = {Information Flow Analysis with Chinese Text},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_11},
doi = {10.1007/978-3-540-30211-7_11},
abstract = {This article investigates the effectiveness of an information inference mechanism on Chinese text. The information inference derives implicit associations via computation of information flow on a high dimensional conceptual space, which is approximated by a cognitively motivated lexical semantic space model, namely Hyperspace Analogue to Language (HAL). A dictionary-based Chinese word segmentation system was used to segment words. To evaluate the Chinese-based information flow model, it is applied to query expansion, in which a set of test queries are expanded automatically via information flow computations and documents are retrieved. Standard recall-precision measures are used to measure performance. Experimental results for TREC-5 Chinese queries and People Daily’s corpus suggest that the Chinese information flow model significantly increases average precision, though the increase is not as high as those achieved using English corpus. Nevertheless, there is justification to believe that the HAL-based information flow model, and in turn our psychologistic stance on the next generation of information processing systems, have a promising degree of language independence.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {100–109},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_12,
author = {Gao, Wei and Wong, Kam-Fai and Lam, Wai},
title = {Phoneme-Based Transliteration of Foreign Names for OOV Problem},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_12},
doi = {10.1007/978-3-540-30211-7_12},
abstract = {A proper noun dictionary is never complete rendering name translation from English to Chinese ineffective. One way to solve this problem is not to rely on a dictionary alone but to adopt automatic translation according to pronunciation similarities, i.e. to map phonemes comprising an English name to the phonetic representations of the corresponding Chinese name. This process is called transliteration. We present a statistical transliteration method. An efficient algorithm for aligning phoneme chunks is described. Unlike rule-based approaches, our method is data-driven. Compared to source-channel based statistical approaches, we adopt a direct transliteration model, i.e. the direction of probabilistic estimation conforms to the transliteration direction. We demonstrate comparable performance to source-channel based system.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {110–119},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_13,
author = {Jin, Qianli and Zhao, Jun and Xu, Bo},
title = {Window-Based Method for Information Retrieval},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_13},
doi = {10.1007/978-3-540-30211-7_13},
abstract = {In this paper, a series of window-based methods is proposed for information retrieval. Compared with traditional tf-idf model, our approaches are based on two new key notions. The first one is that the closer the query words in a document, the larger the similarity value between the query and the document. And the second one is that some query words, like named entities and baseNP called “Core Words” are much more important than other words, and should have special weights. We implement the above notions by three models. They are Simple Window-based Model, Dynamic Window-based Model and Core Window-based Model. Our models can compute similarities between queries and documents based on the importance and distribution of query words in the documents. TREC data are used to test the algorithms. The experiments indicate that our window-based methods outperform most of the traditional methods, such as tf-idf and Okapi BM25. And the Core Window-based Model is the best and most robust model for various queries.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {120–129},
numpages = {10},
keywords = {information retrieval, named entity, window-based method},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_14,
author = {Na, Seung-Hoon and Kang, In-Su and Lee, Jong-Hyeok},
title = {Improving Relevance Feedback in Language Modeling Approach: Maximum a Posteriori Probability Criterion and Three-Component Mixture Model},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_14},
doi = {10.1007/978-3-540-30211-7_14},
abstract = {Recently, researchers have tried to extend a language modeling approach to apply relevance feedback. Their approaches can be classified into two categories. One typical approach is the expansion-based feedback that sequentially performs ‘term selection’ and ‘term re-weighting’ separately. Another approach is the model-based feedback that focuses on estimating ‘query language model’, which predicts well users’ information need. This paper improves these two approaches of relevance feedback by using a maximum a posteriori probability criterion, and a three-component mixture model. A maximum a posteriori probability criterion is a criterion for selection of good expansion terms from feedback documents. A three-component mixture model is the method that eliminates the noise of the query language model by adding a ‘document specific topic model’. The experimental results show that our methods increase the precision of relevance feedback for a short length query. In addition, we make some comparative study between several relevance feedbacks in three document collections.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {130–138},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_15,
author = {You, Lan and Du, Yongping and Ge, Jiayin and Huang, Xuanjing and Wu, Lide},
title = {BBS Based Hot Topic Retrieval Using Back-Propagation Neural Network},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_15},
doi = {10.1007/978-3-540-30211-7_15},
abstract = {BBS, often referred to as forum, is a system that offers so much information, where people talk about various topics. Some topics are hot while others are unpopular. It’s rather a hard job for a person to find out hot topics in these tons of information. In this paper we introduce a system that automatically retrieves hot topics on BBS. Unlike some topic detection systems, this system not only discovers topics but also judges their hotness. Messages are first clustered into topics based on their lexical similarity. Then a BPNN (Back-Propagation Neural Network) based classification algorithm is used to judge the hotness of topic according to its popularity, its quality as well as its message distribution over time. We have conducted experiments over Yahoo! Message Board (Yahoo BBS) and retrieved satisfactory results.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {139–148},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_16,
author = {Zhang, Min and Lin, Chuan and Ma, Shaoping},
title = {How Effective is Query Expansion for Finding Novel Information?},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_16},
doi = {10.1007/978-3-540-30211-7_16},
abstract = {The task of finding novel information in information retrieval (IR) has been proposed recently and paid more attention to. Compared with techniques in traditional document-level retrieval, query expansion (QE) is dominant in the new task. This paper gives an empirical study on the effectiveness of different QE techniques on finding novel information. The conclusion is drawn according to experiments on two standard test collections of TREC2002 and TREC2003 novelty tracks. Local co-occurrence-based QE approach performs best and makes more than 15% consistent improvement, which enhances both precision and recall in some cases. Proximity-based and dependency-based QE are also effective that both make about 10% progress. Pseudo relevance feedback works better than semantics-based QE and the latter one is not helpful on finding novel information.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {149–157},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_17,
author = {Bond, Francis and Fujita, Sanae and Hashimoto, Chikara and Kasahara, Kaname and Nariyama, Shigeko and Nichols, Eric and Ohtani, Akira and Tanaka, Takaaki and Amano, Shigeaki},
title = {The Hinoki Treebank a Treebank for Text Understanding},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_17},
doi = {10.1007/978-3-540-30211-7_17},
abstract = {In this paper we describe the motivation for and construction of a new Japanese lexical resource: the Hinoki treebank. The treebank is built from dictionary definition sentences, and uses an HPSG grammar to encode the syntactic and semantic information. We then show how this treebank can be used to extract thesaurus information from definition sentences in a language-neutral way using minimal recursion semantics.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {158–167},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_18,
author = {Cu\v{r}\'{\i}n, Jan and \v{C}mejrek, Martin and Havelka, Ji\v{r}\'{\i} and Kubo\v{n}, Vladislav},
title = {Building a Parallel Bilingual Syntactically Annotated Corpus},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_18},
doi = {10.1007/978-3-540-30211-7_18},
abstract = {This paper describes a process of building a bilingual syntactically annotated corpus, the PCEDT (Prague Czech-English Dependency Treebank). The corpus is being created at Charles University, Prague, and the release of this corpus as Linguistic Data Consortium data collection is scheduled for the spring of 2004. The paper discusses important decisions made prior to the start of the project and gives an overview of all kinds of resources included in the PCEDT.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {168–176},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_19,
author = {Kumano, Tadashi and Kashioka, Hideki and Tanaka, Hideki and Fukusima, Takahiro},
title = {Acquiring Bilingual Named Entity Translations from Content-Aligned Corpora},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_19},
doi = {10.1007/978-3-540-30211-7_19},
abstract = {We propose a new method for acquiring bilingual named entity (NE) translations from non-literal, content-aligned corpora. It first recognizes NEs in each of a bilingual document pair using the NE extraction technique, then finds NE groups whose members share the same referent, and finally corresponds between bilingual NE groups. The exhaustive detection of NEs can potentially acquire translation pairs with broad coverage. The correspondences between bilingual NE groups are estimated based on the similarity of the appearance order in each document, and the corresponding performance came up to F(β=1) = 71.0% by using small bilingual dictionary together. The total performance for acquiring bilingual NE pairs through the overall process of extraction, grouping, and corresponding was F(β=1) = 58.8%.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {177–186},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_20,
author = {Ma, Minhua and Kevitt, Paul Mc},
title = {Visual Semantics and Ontology of Eventive Verbs},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_20},
doi = {10.1007/978-3-540-30211-7_20},
abstract = {Various English verb classifications have been analyzed in terms of their syntactic and semantic properties, and conceptual components, such as syntactic valency, lexical semantics, and semantic/syntactic correlations. Here the visual semantics of verbs, particularly their visual roles, somatotopic effectors, and level-of-detail, is studied. We introduce the notion of visual valency and use it as a primary criterion to recategorize eventive verbs for language visualization (animation) in our intelligent multimodal storytelling system, CONFUCIUS. The visual valency approach is a framework for modelling deeper semantics of verbs. In our ontological system we consider both language and visual modalities since CONFUCIUS is a multimodal system.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {187–196},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_21,
author = {Ninomiya, Takashi and Tsujii, Jun’ichi and Miyao, Yusuke},
title = {A Persistent Feature-Object Database for Intelligent Text Archive Systems},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_21},
doi = {10.1007/978-3-540-30211-7_21},
abstract = {This paper describes an intelligent text archive system in which typed feature structures are embedded. The aim of the system is to associate feature structures with regions in text, to make indexes for efficient retrieval, to allow users to specify both structure and proximity, and to enable inference on typed feature structures embedded in text. We propose a persistent mechanism for storing typed feature structures and the architecture of the text archive system.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {197–205},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_22,
author = {Aramaki, Eiji and Kurohashi, Sadao and Kashioka, Hideki and Tanaka, Hideki},
title = {Example-Based Machine Translation without Saying Inferable Predicate},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_22},
doi = {10.1007/978-3-540-30211-7_22},
abstract = {For natural translations, a human being does not express predicates that are inferable from the context in a target language. This paper proposes a method of machine translation which handles these predicates. First, to investigate how to translate them, we build a corpus in which predicate correspondences are annotated manually. Then, we observe the corpus, and find alignment patterns including these predicates. In our experimental results, the machine translation system using the patterns demonstrated the basic feasibility of our approach.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {206–215},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_23,
author = {Bilac, Slaven and Tanaka, Hozumi},
title = {Improving Back-Transliteration by Combining Information Sources},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_23},
doi = {10.1007/978-3-540-30211-7_23},
abstract = {Transliterating words and names from one language to another is a frequent and highly productive phenomenon. Transliteration is information loosing since important distinctions are not preserved in the process. Hence, automatically converting transliterated words back into their original form is a real challenge. However, due to wide applicability in MT and CLIR, it is a computationally interesting problem. Previously proposed back-transliteration methods are based either on phoneme modeling or grapheme modeling across languages. In this paper, we propose a new method, combining the two models in order to enhance the back–transliterations of words transliterated in Japanese. Our experiments show that the resulting system outperforms single-model systems.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {216–223},
numpages = {8},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_24,
author = {Chuang, Thomas C. and Wu, Jian-Cheng and Lin, Tracy and Shei, Wen-Chie and Chang, Jason S.},
title = {Bilingual Sentence Alignment Based on Punctuation Statistics and Lexicon},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_24},
doi = {10.1007/978-3-540-30211-7_24},
abstract = {This paper presents a new method of aligning bilingual parallel texts based on punctuation statistics and lexical information. It is demonstrated that the punctuation statistics prove to be effective means to achieve good results. The task of sentence alignment of bilingual texts written in disparate language pairs like English and Chinese is reportedly more difficult. We examine the feasibility of using punctuations for high accuracy sentence alignment. Encouraging precision rate is demonstrated in aligning sentences in bilingual parallel corpora based solely on punctuation statistics. Improved results were obtained when both punctuation statistics and lexical information were employed. We have experimented with an implementation of the proposed method on the parallel corpora of Sinorama Magazine and Records of the Hong Kong Legislative Council with satisfactory results.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {224–232},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_25,
author = {Ding, Yuan and Palmer, Martha},
title = {Automatic Learning of Parallel Dependency Treelet Pairs},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_25},
doi = {10.1007/978-3-540-30211-7_25},
abstract = {Induction of synchronous grammars from empirical data has long been an unsolved problem; despite generative synchronous grammars theoretically suit the machine translation task very well. This fact is mainly due to pervasive structural divergences between languages. This paper presents a statistical approach that learns dependency structure mappings from parallel corpora. The new algorithm automatically learns parallel dependency treelet pairs from loosely matched non-isomorphic dependency trees while keeping computational complexity polynomial in the length of the sentences. A set of heuristics is introduced and specifically optimized for parallel treelet learning purposes using Minimum Error Rate training.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {233–243},
numpages = {11},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_26,
author = {Kitamura, Mihoko and Matsumoto, Yuji},
title = {Practical Translation Pattern Acquisition from Combined Language Resources},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_26},
doi = {10.1007/978-3-540-30211-7_26},
abstract = {Automatic extraction of translation patterns from parallel corpora is an efficient way to automatically develop translation dictionaries, and therefore various approaches have been proposed. This paper presents a practical translation pattern extraction method that greedily extracts translation patterns based on co-occurrence of English and Japanese word sequences, which can also be effectively combined with manual confirmation and linguistic resources, such as chunking information and translation dictionaries. Use of these extra linguistic resources enables it to acquire results of higher precision and broader coverage regardless of the amount of documents.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {244–253},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_27,
author = {Udupa U., Raghavendra and Faruquie, Tanveer A.},
title = {An English-Hindi Statistical Machine Translation System},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_27},
doi = {10.1007/978-3-540-30211-7_27},
abstract = {Recently statistical methods for natural language translation have become popular and found reasonable success. In this paper we describe an English-Hindi statistical machine translation system. Our machine translation system is based on IBM Models 1, 2, and 3. We present experimental results on an English-Hindi parallel corpus consisting of 150,000 sentence pairs. We propose two new algorithms for the transfer of fertility parameters from Model 2 to Model 3. Our algorithms have a worst case time complexity of O(m3) improving on the exponential time algorithm proposed in the classical paper on IBM Models. When the maximum fertility of a word is small, our algorithms are O(m2) and hence very efficient in practice.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {254–262},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_28,
author = {Chen, Wan-Chen and Hsieh, Ching-Tang and Lai, Eugene},
title = {Robust Speaker Identification System Based on Wavelet Transform and Gaussian Mixture Model},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_28},
doi = {10.1007/978-3-540-30211-7_28},
abstract = {This paper presents an effective method for improving the performance of a speaker identification system. Based on the multiresolution property of the wavelet transform, the input speech signal is decomposed into various frequency bands in order not to spread noise distortions over the entire feature space. The linear predictive cepstral coefficients (LPCCs) of each band are calculated. Furthermore, the cepstral mean normalization technique is applied to all computed features. We use feature recombination and likelihood recombination methods to evaluate the task of the text-independent speaker identification. The feature recombination scheme combines the cepstral coefficients of each band to form a single feature vector used to train the Gaussian mixture model (GMM). The likelihood recombination scheme combines the likelihood scores of independent GMM for each band. Experimental results show that both proposed methods outperform the GMM model using full-band LPCCs and mel-frequency cepstral coefficients (MFCCs) in both clean and noisy environments.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {263–271},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_29,
author = {Dong, Minghui and Lua, Kim-Teng and Xu, Jun},
title = {Selecting Prosody Parameters for Unit Selection Based Chinese TTS},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_29},
doi = {10.1007/978-3-540-30211-7_29},
abstract = {In unit selection text-to-speech approach, each unit is described by a set of parameters. However, which parameters effectively express prosody of speech is a problem. In this paper, we propose an approach to the determination of prosody parameters for unit selection-based speech synthesis. We are concerned about how prosody parameters can correctly describe tones and prosodic breaks in Chinese speech. First, we define and evaluate a set of parameters. Then, we cluster the parameters and select a representative parameter from each cluster. Finally, the parameters are evaluated in a real TTS system. Experiment shows that the selected parameters help to improve speech quality.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {272–279},
numpages = {8},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_30,
author = {Kang, In-Su and Bae, Jae-Hak J. and Lee, Jong-Hyeok},
title = {Natural Language Database Access Using Semi-Automatically Constructed Translation Knowledge},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_30},
doi = {10.1007/978-3-540-30211-7_30},
abstract = {In most natural language database interfaces (NLDBI), translation knowledge acquisition heavily depends on human specialties, consequently undermining domain portability. This paper attempts to semi-automatically construct translation knowledge by introducing a physical Entity-Relationship schema, and by simplifying translation knowledge structures. Based on this semi-automatically produced translation knowledge, a noun translation method is proposed in order to resolve NLDBI translation ambiguities.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {280–289},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_31,
author = {Kang, Mi-young and Choi, Sung-ja and Yoon, Ae-sun and Kwon, Hyuk-chul},
title = {Korean Stochastic Word-Spacing with Dynamic Expansion of Candidate Words List},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_31},
doi = {10.1007/978-3-540-30211-7_31},
abstract = {The main aim of this work is to implement stochastic Korean Word-Spacing System which is equally robust for both inner-data and external-data. Word-spacing in Korean is influential in deciding semantic and syntactic scope. In order to cope with various problem yielded by word-spacing errors while processing Korean text, this study (a) presents a simple stochastic word-spacing system with only two parameters using relative word-unigram frequencies and odds favoring the inner-spacing probability of disyllables located at the boundary of stochastic-based words; (b) endeavors to diminish training-data-dependency by dynamically creating candidate words list with the longest-radix-selecting algorithm and (c) removes noise from the training-data by refining training procedure. The system thus becomes robust against unseen words and offers similar performance for both inner-data and external-data: it obtained 98.35% and 97.47% precision in word-unit correction from the inner test-data and the external test-data, respectively.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {290–298},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_32,
author = {Klatt, Stefan and Bohnet, Bernd},
title = {You Don’t Have to Think Twice If You Carefully Tokenize},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_32},
doi = {10.1007/978-3-540-30211-7_32},
abstract = {Most of the currently used tokenizers only segment a text into tokens and combine them to sentences. But this is not the way, we think a tokenizer should work. We believe that a tokenizer should support the following analysis components in the best way it can.We present a tokenizer with a high focus on transparency. First, the tokenizer decisions are encoded in such a way that the original text can be reconstructed. This supports the identification of typical errors and – as a consequence – a faster creation of better tokenizer versions. Second, all detected relevant information that might be important for subsequent analysis components are made transparent by XML-tags and special information codes for each token. Third, doubtful decisions are also marked by XML-tags. This is helpful for off-line applications like corpora building, where it seems to be more appropriate to check doubtful decisions in a few minutes manually than working with incorrect data over years.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {299–309},
numpages = {11},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_33,
author = {Lim, Chul Su and Lee, Kong Joo and Kim, Gil Chang},
title = {Automatic Genre Detection of Web Documents},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_33},
doi = {10.1007/978-3-540-30211-7_33},
abstract = {A genre or a style is another view of documents different from a subject or a topic. The genre is also a criterion to classify the documents. There have been several studies on detecting a genre of textual documents. However, only a few of them dealt with web documents. In this paper we suggest sets of features to detect genres of web documents. Web documents are different from textual documents in that they contain URL and HTML tags within the pages. We introduce the features specific to web documents, which are extracted from URL and HTML tags. Experimental results enable us to evaluate their characteristics and performances.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {310–319},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_34,
author = {L\"{u}, Xueqiang and Zhang, Le and Hu, Junfeng},
title = {Statistical Substring Reduction in Linear Time},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_34},
doi = {10.1007/978-3-540-30211-7_34},
abstract = {We study the problem of efficiently removing equal frequency n-gram substrings from an n-gram set, formally called Statistical Substring Reduction (SSR). SSR is a useful operation in corpus based multi-word unit research and new word identification task of oriental language processing. We present a new SSR algorithm that has linear time (O(n)) complexity, and prove its equivalence with the traditional O(n2) algorithm. In particular, using experimental results from several corpora with different sizes, we show that it is possible to achieve performance close to that theoretically predicated for this task. Even in a small corpus the new algorithm is several orders of magnitude faster than the O(n2) one. These results show that our algorithm is reliable and efficient, and is therefore an appropriate choice for large scale corpus processing.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {320–327},
numpages = {8},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_35,
author = {Tajima, Sachie and Nanba, Hidetsugu and Okumura, Manabu},
title = {Detecting Sentence Boundaries in Japanese Speech Transcriptions Using a Morphological Analyzer},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_35},
doi = {10.1007/978-3-540-30211-7_35},
abstract = {We present a method to automatically detect sentenceboundaries(SBs) in Japanese speech transcriptions. Our method uses a Japanese morphological analyzer that is based on a cost calculation and selects as the best result the one with the minimum cost. The idea behind using a morphological analyzer to identify candidates for SBs is that the analyzer outputs lower costs for better sequences of morphemes. After the candidate SBs have been identified, the unsuitable candidates are deleted by using lexical information acquired from the training corpus. Our method had a 77.24% precision, 88.00% recall, and 0.8277 F-Measure, for a corpus consisting of lecture speech transcriptions in which the SBs are not given.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {328–337},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_36,
author = {Yoshida, Minoru and Nakagawa, Hiroshi},
title = {Specification Retrieval – How to Find Attribute-Value Information on the Web},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_36},
doi = {10.1007/978-3-540-30211-7_36},
abstract = {This paper proposes a method for retrieving Web pages according to objects described in them. To achieve that goal, ontologies extracted from HTML tables are used as queries. The system retrieves Web pages containing the type of objects described by a given ontology. We propose a simple and efficient algorithm for this task and show its performance on real-world Web sites.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {338–347},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_37,
author = {Chung, You-Jin and Moon, Kyonghi and Lee, Jong-Hyeok},
title = {Conceptual Information-Based Sense Disambiguation},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_37},
doi = {10.1007/978-3-540-30211-7_37},
abstract = {Most previous corpus-based approaches to word-sense disambiguation (WSD) collect salient words from the context of a target word. However, they suffer from the problem of data sparseness. To overcome the problem, this paper proposes a concept-based WSD method that uses an automatically generated sense-tagged corpus. Grammatical similarities between Korean and Japanese enable the construction of a sense-tagged Korean corpus through an existing high-quality Japanese-to-Korean machine translation system. The sense-tagged corpus can serve as a knowledge source to extract useful clues for word sense disambiguation, such as concept co-occurrence information. In an evaluation, a weighted voting model achieved the best average precision of 77.22%, with an improvement over the baseline by 14.47%, which shows that our proposed method is very promising for practical MT systems.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {348–357},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_38,
author = {Kang, In-Su and Na, Seung-Hoon and Lee, Jong-Hyeok},
title = {Influence of WSD on Cross-Language Information Retrieval},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_38},
doi = {10.1007/978-3-540-30211-7_38},
abstract = {Translation ambiguity is a major problem in dictionary-based cross-language information retrieval. This paper proposes a statistical word sense disambiguation (WSD) approach for translation ambiguity resolution. Then, with respect to CLIR effectiveness, the pure effect of a disambiguation module will be explored on the following issues: contribution of disambiguation weight to target term weighting, influences of WSD performance on CLIR retrieval effectiveness. In our investigation, we do not use pre-translation or post-translation methods to exclude any mixing effects on CLIR.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {358–366},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_39,
author = {Kiyota, Yoji and Kurohashi, Sadao and Kido, Fuyuko},
title = {Resolution of Modifier-Head Relation Gaps Using Automatically Extracted Metonymic Expressions},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_39},
doi = {10.1007/978-3-540-30211-7_39},
abstract = {This paper proposes a method of extracting metonymic expressions and their interpretative expressions from corpora and its application for the full-parsing-based matching method of a QA system. An evaluation showed that 79% of the extracted interpretations were correct, and an experiment using testsets indicated that introducing the metonymic expressions significantly improved the performance.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {367–376},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_40,
author = {Shirai, Kiyoaki and Tamagaki, Takayuki},
title = {Word Sense Disambiguation Using Heterogeneous Language Resources},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_40},
doi = {10.1007/978-3-540-30211-7_40},
abstract = {This paper proposes a robust method for word sense disambiguation (WSD) of Japanese. Four classifiers were combined in order to improve recall and applicability: one used example sentences in a machine readable dictionary (MRD), one used grammatical information in an MRD, and two classifiers were obtained by supervised learning from a sense-tagged corpus. In other words, we combined several classifiers using heterogeneous language resources, an MRD and a word sense tagged corpus. According to our experimental results, the proposed method outperformed the best single classifier for recall and applicability.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {377–385},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_41,
author = {Wang, Xiaojie and Matsumoto, Yuji},
title = {Improving Word Sense Disambiguation by Pseudo-Samples},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_41},
doi = {10.1007/978-3-540-30211-7_41},
abstract = {Data sparseness is a major problem in word sense disambiguation. Automatic sample acquisition and smoothing are two ways that have been explored to alleviate the influence of data sparseness. In this paper, we consider a combination of these two methods. Firstly, we propose a pattern-based way to acquire pseudo samples, and then we estimate conditional probabilities for variables by combining pseudo data set with sense tagged data set. By using the combinational estimation, we build an appropriate leverage between the two different data sets, which is vital to achieve the best performance. Experiments show that our approach brings significant improvement for Chinese word sense disambiguation.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {386–395},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_42,
author = {Gao, Jianfeng and Suzuki, Hisami},
title = {Long Distance Dependency in Language Modeling: An Empirical Study},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_42},
doi = {10.1007/978-3-540-30211-7_42},
abstract = {This paper presents an extensive empirical study on two language modeling techniques, linguistically-motivated word skipping and predictive clustering, both of which are used in capturing long distance word dependencies that are beyond the scope of a word trigram model. We compare the techniques to others that were proposed previously for the same purpose. We evaluate the resulting models on the task of Japanese Kana-Kanji conversion. We show that the two techniques, while simple, outperform existing methods studied in this paper, and lead to language models that perform significantly better than a word trigram model. We also investigate how factors such as training corpus size and genre affect the performance of the models.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {396–405},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_43,
author = {Kim, Jin-Dong and Tsujii, Jun’ichi},
title = {Word Folding: Taking the Snapshot of Words Instead of the Whole},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_43},
doi = {10.1007/978-3-540-30211-7_43},
abstract = {The snapshot of a word means the most informative fragment of the word. By taking the snapshot instead of the whole, the value space of lexical features can be significantly reduced. From the perspective of machine learning, a small space of feature values implies a loss of information but less data-spareness and less unseen data. The snapshot of words can be taken by using the word folding technique, the goal of which is to reduce the value space of lexical features while minimizing the loss of information.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {406–415},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_44,
author = {Liu, Feifan and Jin, Qianli and Zhao, Jun and Xu, Bo},
title = {Bilingual Chunk Alignment Based on Interactional Matching and Probabilistic Latent Semantic Indexing},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_44},
doi = {10.1007/978-3-540-30211-7_44},
abstract = {An integrated method for bilingual chunk partition andalignment, called “Interactional Matching”, is proposed in this paper. Different from former works, our method tries to get as necessary information as possible from the bilingual corpora themselves, and through bilingual constraint it can automatically build one-to-one chunk-pairs associated with the chunk-pair confidence coefficients. Also, our method partitions bilingual sentences entirely into chunks with no fragments left, different from collocation extracting methods. Furthermore, with the technology of Probabilistic Latent Semantic Indexing(PLSI), this method can deal with not only compositional chunks, but also non-compositional ones. The experiments show that, for overall process (including partition and alignment), our method can obtain 85% precision with 57% recall for the written language chunk-pairs and 78% precision with 53% recall for the spoken language chunk-pairs.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {416–425},
numpages = {10},
keywords = {interactional matching, bilingual chunking, PLSI, alignment},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_45,
author = {Schneider, Karl-Michael},
title = {Learning to Filter Junk E-Mail from Positive and Unlabeled Examples},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_45},
doi = {10.1007/978-3-540-30211-7_45},
abstract = {We study the applicability of partially supervised text classification to junk mail filtering, where a given set of junk messages serve as positive examples while the messages received by a user are unlabeled examples, but there are no negative examples. Supplying a junk mail filter with a large set of junk mails could result in an algorithm that learns to filter junk mail without user intervention and thus would significantly improve the usability of an e-mail client. We study several learning algorithms that take care of the unlabeled examples in different ways and present experimental results.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {426–435},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_46,
author = {Shen, Dan and Zhang, Jie and Su, Jian and Zhou, Guodong and Tan, Chew-Lim},
title = {A Collaborative Ability Measurement for Co-Training},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_46},
doi = {10.1007/978-3-540-30211-7_46},
abstract = {This paper explores collaborative ability of co-training algorithm. We propose a new measurement (CA) for representing the collaborative ability of co-training classifiers based on the overlapping proportion between certain and uncertain instances. The CA measurement indicates whether two classifiers can co-train effectively. We make theoretical analysis for CA values in co-training with independent feature split, with random feature split and without feature split. The experiments justify our analysis. We also explore two variations of the general co-training algorithm and analyze them using the CA measurement.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {436–445},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_47,
author = {Shen, Libin and Joshi, Aravind K.},
title = {Flexible Margin Selection for Reranking with Full Pairwise Samples},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_47},
doi = {10.1007/978-3-540-30211-7_47},
abstract = {Perceptron like large margin algorithms are introduced for the experiments with various margin selections. Compared to the previous perceptron reranking algorithms, the new algorithms use full pairwise samples and allow us to search for margins in a larger space. Our experimental results on the data set of [1] show that a perceptron like ordinal regression algorithm with uneven margins can achieve Recall/Precision of 89.5/90.0 on section 23 of Penn Treebank. Our result on margin selection can be employed in other large margin machine learning algorithms as well as in other NLP tasks.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {446–455},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_48,
author = {Takamura, Hiroya and Okumura, Manabu},
title = {A Comparative Study on the Use of Labeled and Unlabeled Data for Large Margin Classifiers},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_48},
doi = {10.1007/978-3-540-30211-7_48},
abstract = {We propose to use both labeled and unlabeled data with the Expectation-Maximization (EM) algorithm in order to estimate the generative model and use this model to construct a Fisher kernel. The Naive Bayes generative probability is used to model a document. Through the experiments of text categorization, we empirically show that, (a) the Fisher kernel with labeled and unlabeled data outperforms Naive Bayes classifiers with EM and other methods for a sufficient amount of labeled data, (b) the value of additional unlabeled data diminishes when the labeled data size is large enough for estimating a reliable model, (c) the use of categories as latent variables is effective, and (d) larger unlabeled training datasets yield better results.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {456–465},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_49,
author = {Tsou, Benjamin K. and Lai, Tom B. Y. and Chow, Ka-po},
title = {Comparing Entropies within the Chinese Language},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_49},
doi = {10.1007/978-3-540-30211-7_49},
abstract = {Using a large synchronous Chinese corpus, we show how word and character entropy variations exhibit interesting differences in terms of time and space for different Chinese speech communities. We find that word entropy values are affected by the quality of the segmentation process. We also note that word entropies can be affected by proper nouns, which is the most volatile segment of the stable lexicon of the language. Our word and character entropy results provide interesting comparison with the earlier results and the average joint character entropies (a.k.a. entropy rates) of Chinese up to order 20 provided by us indicate that the limits of the conditional character entropies of Chinese for the different speech communities should be about 1 (or less). This invites questions on whether early convergence of character entropies would also entail word entropy convergence.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {466–475},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_50,
author = {Wu, Dekai and Ngai, Grace and Carpuat, Marine},
title = {NTPC: N-Fold Templated Piped Correction},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_50},
doi = {10.1007/978-3-540-30211-7_50},
abstract = {We describe a broadly-applicable conservative error correcting model, N-fold Templated Piped Correction or NTPC (“nitpick”), that consistently improves the accuracy of existing high-accuracy base models. Under circumstances where most obvious approaches actually reduce accuracy more than they improve it, NTPC nevertheless comes with little risk of accidentally degrading performance. NTPC is particularly well suited for natural language applications involving high-dimensional feature spaces, such as bracketing and disambiguation tasks, since its easily customizable template-driven learner allows efficient search over the kind of complex feature combinations that have typically eluded the base models. We show empirically that NTPC yields small but consistent accuracy gains on top of even high-performing models like boosting. We also give evidence that the various extreme design parameters in NTPC are indeed necessary for the intended operating range, even though they diverge from usual practice.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {476–486},
numpages = {11},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_51,
author = {Zhang, Junlin and Sun, Le and Qu, Weimin and Du, Lin and Sun, Yufang},
title = {A Three Level Cache-Based Adaptive Chinese Language Model},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_51},
doi = {10.1007/978-3-540-30211-7_51},
abstract = {Even if n-grams were proved to be very powerful and robust in various tasks involving language models, they have a certain handicap that the dependency is limited to very short local context because of the Markov assumption. This article presents an improved cache based approach to Chinese statistical language modeling. We extend this model by introducing the Chinese concept lexicon into it. The cache of the extended language model contains not only the words occurred recently but also the semantically related words. Experiments have shown that the performance of the adaptive model has been improved greatly.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {487–492},
numpages = {6},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_52,
author = {Chen, Jinying and Xue, Nianwen and Palmer, Martha},
title = {Using a Smoothing Maximum Entropy Model for Chinese Nominal Entity Tagging},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_52},
doi = {10.1007/978-3-540-30211-7_52},
abstract = {This paper treats nominal entity tagging as a six-way (five categories plus non-entity) classification problem and applies a smoothing maximum entropy (ME) model with a Gaussian prior to a Chinese nominal entity tagging task. The experimental results show that the model performs consistently better than an ME model using a simple count cut-off. The results also suggest that simple semantic features extracted from an electronic dictionary improve the model’s performance, especially when the training data is insufficient.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {493–499},
numpages = {7},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_53,
author = {Cheng, Yuchang and Asahara, Masayuki and Matsumoto, Yuji},
title = {Deterministic Dependency Structure Analyzer for Chinese},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_53},
doi = {10.1007/978-3-540-30211-7_53},
abstract = {We present a method of dependency structure analysis for Chinese. The method is a variant of Yamada’s work (Yamada, 2003) originally proposed for English parsing. Our bottom-up parsing algorithm deterministically constructs a dependency structure for an input sentence. Support Vector Machines (SVMs) are utilized to determine the word dependency relations. Experimental evaluations on the CKIP Corpus show that the method is quite accurate on Chinese documents in several domains.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {500–508},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_54,
author = {Ha, Juhong and Zheng, Yu and Kim, Byeongchang and Lee, Gary Geunbae and Seong, Yoon-Suk},
title = {High Speed Unknown Word Prediction Using Support Vector Machine for Chinese Text-to-Speech Systems},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_54},
doi = {10.1007/978-3-540-30211-7_54},
abstract = {One of the most significant problems in POS (Part-of-Speech) tagging of Chinese texts is an identification of words in a sentence, since there is no blank to delimit the words. Because it is impossible to pre-register all the words in a dictionary, the problem of unknown words inevitably occurs during this process. Therefore, the unknown word problem has remarkable effects on the accuracy of the sound in Chinese TTS (Text-to-Speech) system. In this paper, we present a SVM (support vector machine) based method that predicts the unknown words for the result of word segmentation and tagging. For high speed processing to be used in a TTS, we pre-detect the candidate boundary of the unknown words before starting actual prediction. Therefore we perform a two-phase unknown word prediction in the steps of detection and prediction. Results of the experiments are very promising by showing high precision and high recall with also high speed.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {509–517},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_55,
author = {Kim, Mi-Young and Lee, Jong-Hyeok},
title = {Syntactic Analysis of Long Sentences Based on S-Clauses},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_55},
doi = {10.1007/978-3-540-30211-7_55},
abstract = {In dependency parsing of long sentences with fewer subjects than predicates, it is difficult to recognize which predicate governs which subject. To handle such syntactic ambiguity between subjects and predicates, an “S(ubject)-clause” is defined as a group of words containing several predicates and their common subject, and then an automatic S-clause segmentation method is proposed using semantic features as well as morpheme features. We also propose a new dependency tree to reflect S-clauses. Trace information is used to indicate the omitted subject of each predicate. The S-clause information turned out to be very effective in analyzing long sentences, with an improved parsing performance of 4.5%. The precision in determining the governors of subjects in dependency parsing was improved by 32%.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {518–526},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_56,
author = {Tan, Yongmei and Yao, Tianshun and Chen, Qing and Zhu, Jingbo},
title = {Chinese Chunk Identification Using SVMs plus Sigmoid},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_56},
doi = {10.1007/978-3-540-30211-7_56},
abstract = {The paper presents a method of Chinese chunk recognition based on Support Vector Machines (SVMs) plus Sigmoid. It is well known that SVMs are binary classifiers which achieve the best performance in many tasks. However, directly applying binary classifiers in the task of Chinese chunking will face the dilemmas that either two or more different class labels are given to a single unlabeled constituent, or no class labels are given for some unlabeled constituents. Employing sigmoid functions is a method of extracting probabilities (class/input) from SVMs outputs, which is helpful to post-processing of classification. These probabilities are then used to resolve the dilemmas. We compare our method based on SVMs plus Sigmoid with methods based only on SVMs. The experiments show that significant improvements have been achieved.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {527–536},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_57,
author = {Xiong, Deyi and Yu, Hongkui and Liu, Qun},
title = {Tagging Complex NEs with Maxent Models: Layered Structures versus Extended Tagset},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_57},
doi = {10.1007/978-3-540-30211-7_57},
abstract = {The paper discusses two policies for recognizing NEs with complex structures by maximum entropy models. One policy is to develop cascaded MaxEnt models at different levels. The other is to design more detailed tags with human knowledge in order to represent complex structures. The experiments on Chinese organization names recognition indicate that layered structures result in more accurate models while extended tags can not lead to positive results as expected. We empirically prove that the {start, continue, end, unique, other} tag set is the best tag set for NE recognition with MaxEnt models.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {537–544},
numpages = {8},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_58,
author = {Zhao, Shaojun and Lin, Dekang},
title = {A Nearest-Neighbor Method for Resolving PP-Attachment Ambiguity},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_58},
doi = {10.1007/978-3-540-30211-7_58},
abstract = {We present a nearest-neighbor algorithm for resolving prepositional phrase attachment ambiguities. Its performance is significantly higher than previous corpus-based methods for PP-attachment that do not rely on manually constructed knowledge bases. We will also show that the PP-attachment task provides a way to evaluate methods for computing distributional word similarities. Our experiments indicate that the cosine of pointwise mutual information vector is a significantly better similarity measure than several other commonly used similarity measures.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {545–554},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_59,
author = {Fujita, Atsushi and Inui, Kentaro and Matsumoto, Yuji},
title = {Detection of Incorrect Case Assignments in Paraphrase Generation},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_59},
doi = {10.1007/978-3-540-30211-7_59},
abstract = {This paper addresses the issue of post-transfer process in paraphrasing. Our previous investigation into transfer errors revealed that case assignment tends to be incorrect, irrespective of the types of transfer in lexical and structural paraphrasing of Japanese sentences [3]. Motivated by this observation, we propose an empirical method to detect incorrect case assignments. Our error detection model combines two error detection models that are separately trained on a large collection of positive examples and a small collection of manually labeled negative examples. Experimental results show that our combined model significantly enhances the baseline model which is trained only on positive examples. We also propose a selective sampling scheme to reduce the cost of collecting negative examples, and confirm the effectiveness in the error detection task.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {555–565},
numpages = {11},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_60,
author = {Roh, Ji-Eun and Lee, Jong-Hyeok},
title = {Building a Pronominalization Model by Feature Selection and Machine Learning},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_60},
doi = {10.1007/978-3-540-30211-7_60},
abstract = {Pronominalization is an important component in generating a coherent text. In this paper, we identify features that influence pronominalization, and construct a pronoun generation model by using various machine learning techniques. The old entities, which are the target of pronominalization, are categorized into three types according to their tendency in attentional state: Cb and old-Cp derived from a Centering model, and the remaining old entities. We construct a pronoun generation model for each type. Eighty-seven texts are gathered from three genres for training and testing. Using this, we verify that our proposed features are well defined to explain pronominalization in Korean, and we also show that our model significantly outperforms previous ones with 99% confidence level by t-test. We also identify central features that have a strong influence on pronominalization across genres.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {566–575},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_61,
author = {Huang, Chien-Chung and Chuang, Shui-Lung and Chien, Lee-Feng},
title = {Categorizing Unknown Text Segments for Information Extraction Using a Search Result Mining Approach},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_61},
doi = {10.1007/978-3-540-30211-7_61},
abstract = {An advanced information extraction system requires an effective text categorization technique to categorize extracted facts (text segments) into a hierarchy of domain-specific topic categories. Text segments are often short and their categorization is quite different from conventional document categorization. This paper proposes a Web mining approach that exploits Web resources to categorize unknown text segments with limited manual intervention. The feasibility and wide adaptability of the proposed approach has been shown with extensive experiments on categorizing different kinds of text segments including domain-specific terms, named entities, and even paper titles into Yahoo!’s taxonomy trees.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {576–586},
numpages = {11},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_62,
author = {Jung, Sung-won and Han, Gi-deuk and Kwon, Hyuk-chul},
title = {Mining Table Information on the Internet},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_62},
doi = {10.1007/978-3-540-30211-7_62},
abstract = {Making HTML documents, the authors use various methods for clearly conveying their intension. In those various methods, this paper pays special attention to tables because tables are commonly used within many documents to make the meanings clear, which are well recognized because web documents use tags for additional information. On the Internet, tables are used for the purpose of the knowledge structuring as well as design of documents. Thus, we are firstly interested in classifying tables into two types: meaningful tables and decorative tables. However, this is not easy because HTML does not separate presentation and structure. This paper proposes a method of extracting meaningful tables using a modified k-means and compares it with other methods. The experiment results show that classifying on web documents is promising.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {587–595},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_63,
author = {Kobayashi, Nozomi and Inui, Kentaro and Matsumoto, Yuji and Tateishi, Kenji and Fukushima, Toshikazu},
title = {Collecting Evaluative Expressions for Opinion Extraction},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_63},
doi = {10.1007/978-3-540-30211-7_63},
abstract = {Automatic extraction of human opinions from Web documents has been receiving increasing interest. To automate the process of opinion extraction, having a collection of evaluative expressions such as “something is confortable” would be useful. However, it can be costly to manually create an exhaustive list of such expressions for many domains, because they tend to be domain-dependent. Motivated by this, we explored ways to accelerate the process of collecting evaluative expressions by applying a text mining technique. This paper proposes a semi-automatic method that uses particular cooccurrence patterns of evaluated subjects, focused attributes and value expressions.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {596–605},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_64,
author = {Qiang, Wang and XiaoLong, Wang and Yi, Guan},
title = {A Study of Semi-Discrete Matrix Decomposition for LSI in Automated Text Categorization},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_64},
doi = {10.1007/978-3-540-30211-7_64},
abstract = {This paper proposes the use of Latent Semantic Indexing (LSI) techniques, decomposed with semi-discrete matrix decomposition (SDD) method, for text categorization. The SDD algorithm is a recent solution to LSI, which can achieve similar performance at a much lower storage cost. In this paper, LSI is used for text categorization by constructing new features of category as combinations or transformations of the original features. In the experiments on data set of Chinese Library Classification we compare accuracy to a classifier based on k-Nearest Neighbor (k-NN) and the result shows that k-NN based on LSI is sometimes significantly better. Much future work remains, but the results indicate that LSI is a promising technique for text categorization.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {606–615},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_65,
author = {Yoon, Yongwook and Lee, Changki and Lee, Gary Geunbae},
title = {Systematic Construction of Hierarchical Classifier in SVM-Based Text Categorization},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_65},
doi = {10.1007/978-3-540-30211-7_65},
abstract = {In a text categorization task, classification on some hierarchy of classes shows better results than the case without the hierarchy. In current environments where large amount of documents are divided into several subgroups with a hierarchy between them, it is more natural and appropriate to use a hierarchical classification method. We introduce a new internal node evaluation scheme which is very helpful to the development process of a hierarchical classifier. We also show that the hierarchical classifier construction method using this measure yields a classifier with better classification performance especially when applied to the classification task with large depth of hierarchy.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {616–625},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_66,
author = {Bender, Emily M. and Siegel, Melanie},
title = {Implementing the Syntax of Japanese Numeral Classifiers},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_66},
doi = {10.1007/978-3-540-30211-7_66},
abstract = {While the sortal constraints associated with Japanese numeral classifiers are well-studied, less attention has been paid to the details of their syntax. We describe an analysis implemented within a broad-coverage HPSG that handles an intricate set of numeral classifier construction types and compositionally relates each to an appropriate semantic representation, using Minimal Recursion Semantics.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {626–635},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_67,
author = {Bohnet, Bernd},
title = {A Graph Grammar Approach to Map between Dependency Trees and Topological Models},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_67},
doi = {10.1007/978-3-540-30211-7_67},
abstract = {Determining the word order in free word order languages is deemed as a challenge for NLG. In this paper, we propose a simple approach in order to get the appropriate grammatically correct variants of a sentence using a dependency structure as input. We describe a linearization grammar based on a graph grammar that allows to retrieve a topological model using unordered constituent structures and precedence relations. The graph grammar formalism is totally language independent and only the grammar depends on the language. The grammar rules can be automatically acquired from a corpus that is annotated with phrase structures and dependency structures. The dependency structures annotation is retrieved by structure translation from the phrase structure annotation. We conclude with the description of a grammar and the evaluation of the formalism using a large corpus.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {636–645},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_68,
author = {Carroll, John and Fang, Alex C.},
title = {The Automatic Acquisition of Verb Subcategorisations and Their Impact on the Performance of an HPSG Parser},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_68},
doi = {10.1007/978-3-540-30211-7_68},
abstract = {We describe the automatic acquisition of a lexicon of verb subcategorisations from a domain-specific corpus, and an evaluation of the impact this lexicon has on the performance of a “deep”, HPSG parser of English. We conducted two experiments to determine whether the empirically extracted verb stems would enhance the lexical coverage of the grammar and to see whether the automatically extracted verb subcategorisations would result in enhanced parser coverage. In our experiments, the empirically extracted verbs enhance lexical coverage by 8.5%. The automatically extracted verb subcategorisations enhance the parse success rate by 15% in theoretical terms and by 4.5% in practice. This is a promising approach for improving the robustness of deep parsing.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {646–654},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_69,
author = {Chen, Keh-Jiann and Hsieh, Yu-Ming},
title = {Chinese Treebanks and Grammar Extraction},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_69},
doi = {10.1007/978-3-540-30211-7_69},
abstract = {Preparation of knowledge bank is a very difficult task. In this paper, we discuss the knowledge extraction from the manually examined Sinica Treebank. Categorical information, word-to-word relation, word collocations, new syntactic patterns and sentence structures are obtained. A searching system for Chinese sentence structure was developed in this study. By using pre-extracted data and SQL commands, the system replies the user’s queries efficiently. We also analyze the extracted grammars to study the tradeoffs between the granularity of the grammar rules and their coverage as well as ambiguities. It provides the information of knowing how large a treebank is sufficient for the purpose of grammar extraction. Finally, we also analyze the tradeoffs between grammar coverage and ambiguity by parsing results from the grammar rules of different granularity.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {655–663},
numpages = {9},
keywords = {ambiguities, grammar coverage, parsing, knowledge extraction, treebanks},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_70,
author = {Han, Xiwu and Zhao, Tiejun and Yang, Muyun},
title = {FML-Based SCF Predefinition Learning for Chinese Verbs},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_70},
doi = {10.1007/978-3-540-30211-7_70},
abstract = {This paper describes the first attempt to acquire Chinese SCFs automatically and the application of Flexible Maximum Likelihood (FML), a variational filtering method of the simple maximum likelihood (ML) estimate from observed relative frequencies, to the task of predefining a basic SCF set for Chinese verb subcategorization acquisition. By setting a flexible threshold for SCF probability distributions over 1774 Chinese verbs, we obtained 141 basic SCFs with a reasonably practical coverage of 98.64% over 43,000 Chinese sentences. After complementation of 11 manually observed SCFs, a both linguistically and intuitively acceptable basic SCF set was predefined for future SCF acquisition work.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {664–673},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_71,
author = {Kordoni, Valia and Neu, Julia},
title = {Deep Analysis of Modern Greek},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_71},
doi = {10.1007/978-3-540-30211-7_71},
abstract = {We present a deep computational Modern Greek grammar. The grammar is written in HPSG and is being developed in a multilingual context with MRS semantics, contributing to an open-source collection of software and linguistic resources with wide usage in research, education, and application building.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {674–683},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_72,
author = {Miyao, Yusuke and Ninomiya, Takashi and Tsujii, Jun’ichi},
title = {Corpus-Oriented Grammar Development for Acquiring a Head-Driven Phrase Structure Grammar from the Penn Treebank},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_72},
doi = {10.1007/978-3-540-30211-7_72},
abstract = {This paper describes a method of semi-automatically acquiring an English HPSG grammar from the Penn Treebank. First, heuristic rules are employed to annotate the treebank with partially-specified derivation trees of HPSG. Lexical entries are automatically extracted from the annotated corpus by inversely applying HPSG schemata to partially-specified derivation trees. Predefined HPSG schemata assure the acquired lexicon to conform to the theoretical formulation of HPSG. Experimental results revealed that this approach enabled us to develop an HPSG grammar with significant robustness at small cost.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {684–693},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_73,
author = {Feng, Haodi and Chen, Kang and Kit, Chunyu and Deng, Xiaotie},
title = {Unsupervised Segmentation of Chinese Corpus Using Accessor Variety},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_73},
doi = {10.1007/978-3-540-30211-7_73},
abstract = {The lack of word delimiters such as spaces in Chinese texts makes word segmentation a special issue in Chinese text processing. As the volume of Chinese texts grows rapidly on the Internet, the number of unknown words increases accordingly. However, word segmentation approaches relying solely on existing dictionaries are helpless in handling unknown words. In this paper, we propose a novel unsupervised method to segment large Chinese corpora using contextual information. In particular, the number of characters preceding and following a string, known as the accessors of the string, is used to measure the independence of the string. The greater the independence, the more likely it is that the string is a word. The segmentation problem is then considered an optimization problem to maximize the target function of this number over all word candidates in an utterance. Our purpose here is to explore the best function in terms of segmentation performance. The performance is evaluated with the word token recall measure in addition to word type precision and word type recall. Among the three types of target functions that we have explored, polynomial functions turn out to outperform others. This simple method is effective in unsupervised segmentation of Chinese texts and its performance is highly comparable to other recently reported unsupervised segmentation methods.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {694–703},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_74,
author = {Fu, Guohong and Luke, Kang-Kwong},
title = {Chinese Unknown Word Identification Using Class-Based LM},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_74},
doi = {10.1007/978-3-540-30211-7_74},
abstract = {This paper presents a modified class-based LM approach to Chinese unknown word identification. In this work, Chinese unknown word identification is viewed as a classification problem and the part-of-speech of each unknown word is defined as its class. Furthermore, three types of features, including contextual class feature, word juncture model and word formation patterns, are combined in a framework of class-based LM to perform correct unknown word identification on a sequence of known words. In addition to unknown word identification, the class-based LM approach also provides a solution for unknown word tagging. The results of our experiments show that most unknown words in Chinese texts can be resolved effectively by the proposed approach.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {704–713},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_75,
author = {Hu, Qinan and Pan, Haihua and Kit, Chunyu},
title = {An Example-Based Study on Chinese Word Segmentation Using Critical Fragments},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_75},
doi = {10.1007/978-3-540-30211-7_75},
abstract = {In our study, sentences are represented as sequences of critical fragments, and critical fragments with more than one distinct resolution found in the training corpus are considered as being ambiguous. Different from other studies, the ambiguous critical fragments are disambiguated using an example-based system in our study. The contexts, i.e. the adjacent characters, words and critical fragments, on either side of an ambiguous critical fragment, are used to measure the distance between training and testing examples. Two kinds of measures, overlap metric and chi-squared feature weighting, are employed, and our system achieves a precision of 93.65% and a recall of 96.56% in the open test.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {714–722},
numpages = {9},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_76,
author = {Li, Hongqiao and Huang, Chang-Ning and Gao, Jianfeng and Fan, Xiaozhong},
title = {The Use of SVM for Chinese New Word Identification},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_76},
doi = {10.1007/978-3-540-30211-7_76},
abstract = {We present a study of new word identification (NWI) to improve the performance of a Chinese word segmenter. In this paper the distribution and types of new words are discussed empirically. In particular, we focus on the new words of two surface patterns, which account for more than 80% of new words in our data sets: NW11 (two-character new word) and NW21 (a bi-character word followed with a single character). NWI is defined as a problem of binary classification. A statistical learning approach based on a SVM classifier is used. Different features for NWI are explored, including in-word probability of a character (IWP), the analogy between new words and lexicon words, anti-word list, and frequency in documents. The experiments show that these features are useful for NWI. The F-scores of NWI we achieved are 64.4% and 54.7% for NW11 and NW21, respectively. The overall performance of the Chinese word segmenter could be improved by Roov 24.5% and F-score 6.5% in PK-close test of the 1st SIGHAN bakeoff. This achieves the performance of state-of-the-art word segmenters.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {723–732},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_77,
author = {Meng, Yao and Yu, Hao and Nishino, Fumihito},
title = {Chinese New Word Finding Using Character-Based Parsing Model},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_77},
doi = {10.1007/978-3-540-30211-7_77},
abstract = {The new word finding is a difficult and indispensable task in Chinese segmentation. The traditional methods used the string statistical information to identify the new words in the large-scale corpus. But it is neither convenient nor powerful enough to describe the words’ internal and external structure laws. And it is even the less effective when the occurrence frequency of the new words is very low in the corpus. In this paper, we present a novel method of using parsing information to find the new words. A character level PCFG model is trained by People Daily corpus and Penn Chinese Treebank. The characters are inputted into the character parsing system, and the words are determined by the parsing tree automatically. Our method describes the word-building rules in the full sentences, and takes advantage of rich context to find the new words. This is especially effective in identifying the occasional words or rarely used words, which are usually in low frequency. The preliminary experiments indicate that our method can substantially improve the precision and recall of the new word finding process.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {733–742},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_78,
author = {Kuehn, Michael and Leong, Mun-Kew and Tanaka-Ishii, Kumiko},
title = {Thematic Session: Natural Language Technology in Mobile Information Retrieval and Text Processing User Interfaces},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_78},
doi = {10.1007/978-3-540-30211-7_78},
abstract = {One of the strongest impacts in recent information technology is the way mobility has changed computer applications. The rapid rate of handphone adoption, the ubiquitous PDA, and the low cost of wireless adoption has created new problems, new challenges, and new opportunities to researchers in many disciplines. One common thread through all these applications is the necessity for information retrieval and mobile text processing in one form or another. Another characteristic is the limited size of mobile devices and the consequent need for sophisticated methods of text input and output. Other applications with a similar need of language technology support under challenging conditions include e.g. communication aids and authoring tools.The use of NLP plays an integral part in creating better user interfaces, more efficient text input technologies, authoring aids, summarization and analysis of output for precise display, and greater understanding in the interactive dialogue between user and applications working on text. We invited researchers focusing on text input and output technologies,language modeling,summarization,mobile database, information retrieval and management,speech and dialogue IR applications on mobile devices,and HCI researchers on text retrieval and processing systems to explore user oriented and theoretical limits and characteristics of NLP for front-end and back-end solutions within the context of text retrieval and processing user interfaces.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {743–744},
numpages = {2},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_79,
author = {Du, Heather and Crestani, Fabio},
title = {Spoken versus Written Queries for Mobile Information Access: An Experiment on Mandarin Chinese},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_79},
doi = {10.1007/978-3-540-30211-7_79},
abstract = {As Chinese is not alphabetic and the input of Chinese characters into computer is still a difficult and unsolved problem, voice retrieval of information becomes apparently an important application area of mobile information retrieval (IR). It is intuitive to think that users would speak more words and require less time when issuing queries vocally to an IR system than forming queries in writing. This paper presents some new findings derived from an experimental study on Mandarin Chinese to test this hypothesis and assesses the feasibility of spoken queries for search purposes.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {745–754},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_80,
author = {Iwasaki, Hideya and Tanaka-Ishii, Kumiko},
title = {An Interactive Proofreading System for Inappropriately Selected Words on Using Predictive Text Entry},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_80},
doi = {10.1007/978-3-540-30211-7_80},
abstract = {Predictive text entry systems on computers like kana-to-kanji conversion provide a mechanism that enables users to select among possible words for a given input. Mistakes in selection are relatively common, and they introduce real-word errors. A proofreading system is thus needed to detect and correct real-word errors on a computer without imposing troublesome operations on users. To this end, a practical proofreading system for Japanese text is proposed. The system automatically detects possible real-word homonym errors, and for each detected word, suggests substitution candidates of the same pronunciation. The user can either choose the most appropriate one or leave the original untouched. The system uses an algorithm based on the Na\"{\i}ve Bayesian method. Although the proofreading system was implemented for homonym errors in Japanese text, its design concept and algorithm are also applicable to other languages. The client program of the proofreading system is implemented on the Emacs text editor and works in real time.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {755–764},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_81,
author = {Tanaka-Ishii, Kumiko and Frank, Ian},
title = {Dit4dah: Predictive Pruning for Morse Code Text Entry},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_81},
doi = {10.1007/978-3-540-30211-7_81},
abstract = {We present a new predictive pruning algorithm for text entry and show empirically how it outperforms simple text prediction. Our tests are based on a new application domain for predictive entry: the input of Morse code. Our motiviation for this work was to contribute to the development of efficient entry systems for the seriously disabled, but we found that the constraint of using a single key highlighted features of text prediction not previously closely scrutinised. In particular, our tests show how predictive text entry is affected by two factors: altering the rankings of completion candidates based on the difficulty of entering the remaining text with just the keyboard, and the number of candidates presented to the user.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {765–775},
numpages = {11},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_82,
author = {Ananiadou, Sophia and Park, Jong C.},
title = {Thematic Session: Text Mining in Biomedicine},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_82},
doi = {10.1007/978-3-540-30211-7_82},
abstract = {This thematic session follows a series of workshops and conferences recently dedicated to bio text mining in Biology. This interest is due to the overwhelming amount of biomedical literature, Medline alone contains over 14M abstracts, and the urgent need to discover and organise knowledge extracted from texts. Text mining techniques such as information extraction, named entity recognition etc. have been successfully applied to biomedical texts with varying results. A variety of approaches such as machine learning, SVMs, shallow, deep linguistic analyses have been applied to biomedical texts to extract, manage and organize information. There are over 300 databases containing crucial information on biological data. One of the main challenges is the integration of such heterogeneous information from factual databases to texts. One of the major knowledge bottlenecks in biomedicine is terminology. In such a dynamic domain, new terms are constantly created. In addition there is not always a mapping among terms found in databases, controlled vocabularies, ontologies and “actual” terms which are found in texts. Term variation and term ambiguity have been addressed in the past but more solutions are needed. The confusion of what is a descriptor, a term, an index term accentuates the problem. Solving the terminological problem is paramount to biotext mining, as relationships linking new genes, drugs, proteins (i.e. terms) are important for effective information extraction. Mining for relationships between terms and their automatic extraction is important for the semi-automatic updating and populating of ontologies and other resources needed in biomedicine. Text mining applications such as question-answering, automatic summarization, intelligent information retrieval are based on the existence of shared resources, such as annotated corpora (GENIA) and terminological resources. The field needs more concentrated and integrated efforts to build these shared resources. In addition, evaluation efforts such as BioCreaTive, Genomic Trec are important for biotext mining techniques and applications.The aim of text mining in biology is to provide solutions to biologists, to aid curators in their task. We hope this thematic session addressed techniques and applications which aid the biologists in their research.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {776},
numpages = {1},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_83,
author = {Chun, Hong-woo and Hwang, Young-sook and Rim, Hae-Chang},
title = {Unsupervised Event Extraction from Biomedical Literature Using Co-Occurrence Information and Basic Patterns},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_83},
doi = {10.1007/978-3-540-30211-7_83},
abstract = {In this paper, we propose a new unsupervised method of extracting events from biomedical literature, which uses the score measures of events and patterns having reciprocal effects on each other. We, first, generate candidate events by performing linguistic preprocessing and utilizing basic event pattern information, and then extract reliable events based on the event score which is estimated by using co-occurrence information of candidate event’s arguments and pattern score. Unlike the previous approaches, the proposed approach does not require a huge number of rules and manually constructed training corpora.Experimental results on GENIA corpora show that the proposed method can achieve high recall (69.7%) as well as high precision (90.3%).},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {777–786},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_84,
author = {Kim, Jung-jae and Park, Jong C.},
title = {Annotation of Gene Products in the Literature with Gene Ontology Terms Using Syntactic Dependencies},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_84},
doi = {10.1007/978-3-540-30211-7_84},
abstract = {We present a method for automatically annotating gene products in the literature with the terms of Gene Ontology (GO), which provides a dynamic but controlled vocabulary. Although GO is well-organized with such lexical relations as synonymy, ‘is-a’, and ‘part-of’ relations among its terms, GO terms show quite a high degree of morphological and syntactic variations in the literature. As opposed to the previous approaches that considered only restricted kinds of term variations, our method uncovers the syntactic dependencies between gene product names and ontological terms as well in order to deal with real-world syntactic variations, based on the observation that the component words in an ontological term usually appear in a sentence with established patterns of syntactic dependencies.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {787–796},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_85,
author = {Nenadic, Goran and Spasic, Irena and Ananiadou, Sophia},
title = {Mining Biomedical Abstracts: What’s in a Term?},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_85},
doi = {10.1007/978-3-540-30211-7_85},
abstract = {In this paper we present a study of the usage of terminology in the biomedical literature, with the main aim to indicate phenomena that can be helpful for automatic term recognition in the domain. Our analysis is based on the terminology appearing in the Genia corpus. We analyse the usage of biomedical terms and their variants (namely inflectional and orthographic alternatives, terms with prepositions, coordinated terms, etc.), showing the variability and dynamic nature of terms used in biomedical abstracts. Term coordination and terms containing prepositions are analysed in detail. We also show that there is a discrepancy between terms used in the literature and terms listed in controlled dictionaries. In addition, we briefly evaluate the effectiveness of incorporating treatment of different types of term variation into an automatic term recognition system.},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {797–806},
numpages = {10},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

@inproceedings{10.1007/978-3-540-30211-7_86,
author = {Yi, Eunji and Lee, Gary Geunbae and Song, Yu and Park, Soo-Jun},
title = {SVM-Based Biological Named Entity Recognition Using Minimum Edit-Distance Feature Boosted by Virtual Examples},
year = {2004},
isbn = {3540244751},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30211-7_86},
doi = {10.1007/978-3-540-30211-7_86},
abstract = {In this paper, we propose two independent solutions to the problems of spelling variants and the lack of annotated corpus, which are the main difficulties in SVM(Support-Vector Machine) and other machine-learning based biological named entity recognition. To resolve the problem of spelling variants, we propose the use of edit-distance as a feature for SVM. To resolve the lack-of-corpus problem, we propose the use of virtual examples, by which the annotated corpus can be automatically expanded in a fast, efficient and easy way. The experimental results show that the introduction of edit-distance produces some improvements. And the model, which is trained with the corpus expanded by virtual examples, outperforms the model trained with the original corpus. Finally, we achieved the high performance of 71.46 % in F-measure (64.03 % in precision, 80.84 % in recall) in the experiment of five categories named entity recognition on GENIA corpus (version 3.0).},
booktitle = {Proceedings of the First International Joint Conference on Natural Language Processing},
pages = {807–814},
numpages = {8},
location = {Hainan Island, China},
series = {IJCNLP'04}
}

