@inproceedings{10.5555/3200334.3200336,
author = {Brunelle, Justin F. and Weigle, Michele C. and Nelson, Michael L.},
title = {Archival Crawlers and JavaScript: Discover More Stuff but Crawl More Slowly},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {The web is today's primary publication medium, making web archiving an important activity for historical and analytical purposes. Web pages are increasingly interactive, resulting in pages that are correspondingly difficult to archive. JavaScript enables interactions that can potentially change the client-side state of a representation. We refer to representations that load embedded resources via JavaScript as deferred representations. It is difficult to discover and crawl all of the resources in deferred representations and the result of archiving deferred representations is archived web pages that are either incomplete or erroneously load embedded resources from the live web. We propose a method of discovering and archiving deferred representations and their descendants (representation states) that are only reachable through client-side events. Our approach identified an average of 38.5 descendants per seed URI crawled, 70.9% of which are reached through an onclick event. This approach also added 15.6 times more embedded resources than Heritrix to the crawl frontier, but at a crawl rate that was 38.9 times slower than simply using Heritrix. If our method was applied to the July 2015 Common Crawl dataset, a web-scale archival crawler will discover an additional 7.17 PB (5.12 times more) of information per year. This illustrates the significant increase in resources necessary for more thorough archival crawls.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {1–10},
numpages = {10},
keywords = {memento, digital preservation, web crawling, web archiving},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200337,
author = {Fafalios, Pavlos and Holzmann, Helge and Kasturia, Vaibhav and Nejdl, Wolfgang},
title = {Building and Querying Semantic Layers for Web Archives},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Web archiving is the process of collecting portions of the Web to ensure that the information is preserved for future exploitation. However, despite the increasing number of web archives worldwide, the absence of efficient and meaningful exploration methods still remains a major hurdle in the way of turning them into a usable and useful information source. In this paper, we focus on this problem and propose an RDF/S model and a distributed framework for building semantic profiles ("layers") that describe semantic information about the contents of web archives. A semantic layer allows describing metadata information about the archived documents, annotating them with useful semantic information (like entities, concepts and events), and publishing all this data on the Web as Linked Data. Such structured repositories offer advanced query and integration capabilities and make web archives directly exploitable by other systems and tools. To demonstrate their query capabilities, we build and query semantic layers for three different types of web archives. An experimental evaluation showed that a semantic layer can answer information needs that existing keyword-based systems are not able to sufficiently satisfy.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {11–20},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200338,
author = {Jana, Abhik and Mooriyath, Sruthi and Mukherjee, Animesh and Goyal, Pawan},
title = {WikiM: Metapaths Based Wikification of Scientific Abstracts},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {In order to disseminate the exponential extent of knowledge being produced in the form of scientific publications, it would be best to design mechanisms that connect it with already existing rich repository of concepts - the Wikipedia. Not only does it make scientific reading simple and easy (by connecting the involved concepts used in the scientific articles to their Wikipedia explanations) but also improves the overall quality of the article. In this paper, we present a novel metapath based method, WikiM, to efficiently wikify scientific abstracts - a topic that has been rarely investigated in the literature. One of the prime motivations for this work comes from the observation that, wikified abstracts of scientific documents help a reader to decide better, in comparison to the plain abstracts, whether (s)he would be interested to read the full article. We perform mention extraction mostly through traditional tf-idf measures coupled with a set of smart filters. The entity linking heavily leverages on the rich citation and author publication networks. Our observation is that various metapaths defined over these networks can significantly enhance the overall performance of the system. For mention extraction and entity linking, we outperform most of the competing state-of-the-art techniques by a large margin arriving at precision values of 72.42% and 73.8% respectively over a dataset from the ACL Anthology Network. In order to establish the robustness of our scheme, we wikify three other datasets and get precision values of 63.41%-94.03% and 67.67%-73.29% respectively for the mention extraction and the entity linking phase.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {21–30},
numpages = {10},
keywords = {wikification, scientific article, mention extraction, citation network, metapath, author publication network, entity linking},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200339,
author = {Toepfer, Martin and Seifert, Christin},
title = {Descriptor-Invariant Fusion Architectures for Automatic Subject Indexing: Analysis and Empirical Results on Short Texts},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Documents indexed with controlled vocabularies enable users of libraries to discover relevant documents, even across language barriers. Due to the rapid growth of scientific publications, digital libraries require automatic methods that index documents accurately, especially with regard to explicit or implicit concept drift, that is, with respect to new descriptor terms and new types of documents, respectively. This paper first analyzes architectures of related approaches on automatic indexing. We show that their design determines individual strengths and weaknesses and justify research on their fusion. In particular, systems benefit from statistical associative components as well as from lexical components applying dictionary matching, ranking, and binary classification. The analysis emphasizes the importance of descriptor-invariant learning, that is, learning based on features which can be transferred between different descriptors. Theoretic and experimental results on economic titles and author keywords underline the relevance of the fusion methodology in terms of overall accuracy and adaptability to dynamic domains. Experiments show that fusion strategies combining a binary relevance approach and a thesaurus-based system outperform all other strategies on the tested data set. Our findings can help researchers and practitioners in digital libraries to choose appropriate methods for automatic indexing.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {31–40},
numpages = {10},
keywords = {short text, zero-shot learning, meta-learning, automatic subject indexing, multi-label classification, keyphrase indexing},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200340,
author = {Hassan, Saeed-Ul and Akram, Anam and Haddawy, Peter},
title = {Identifying Important Citations Using Contextual Information from Full Text},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {In this paper we address the problem of classifying cited work into important and non-important to the developments presented in a research publication. This task is vital for the algorithmic techniques that detect and follow emerging research topics and to qualitatively measure the impact of publications in increasingly growing scholarly big data. We consider cited work as important to a publication if that work is used or extended in some way. If a reference is cited as background work or for the purpose of comparing results, the cited work is considered to be non-important. By employing five classification techniques (Support Vector Machine, Na\"{\i}ve Bayes, Decision Tree, K-Nearest Neighbors and Random Forest) on an annotated dataset of 465 citations, we explore the effectiveness of eight previously published features and six novel features (including context based, cue words based and textual based). Within this set, our new features are among the best performing. Using the Random Forest classifier we achieve an overall classification accuracy of 0.91 AUC.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {41–48},
numpages = {8},
keywords = {mining scientific papers, machine learning, full text, citation classification, citation context analysis},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200341,
author = {Weihs, Luca and Etzioni, Oren},
title = {Learning to Predict Citation-Based Impact Measures},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Citations implicitly encode a community's judgment of a paper's importance and thus provide a unique signal by which to study scientific impact. Efforts in understanding and refining this signal are reflected in the probabilistic modeling of citation networks and the proliferation of citation-based impact measures such as Hirsch's h-index. While these efforts focus on understanding the past and present, they leave open the question of whether scientific impact can be predicted into the future. Recent work addressing this deficiency has employed linear and simple probabilistic models; we show that these results can be handily outperformed by leveraging non-linear techniques. In particular, we find that these AI methods can predict measures of scientific impact for papers and authors, namely citation rates and h-indices, with surprising accuracy, even 10 years into the future. Moreover, we demonstrate how existing probabilistic models for paper citations can be extended to better incorporate refined prior knowledge. While predictions of "scientific impact" should be approached with healthy skepticism, our results improve upon prior efforts and form a baseline against which future progress can be easily judged.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {49–58},
numpages = {10},
keywords = {citation prediction, reinforced Poisson process, scientific impact, citation network, h-index},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200342,
author = {Singh, Mayank and Jaiswal, Ajay and Shree, Priya and Pal, Arindam and Mukherjee, Animesh and Goyal, Pawan},
title = {Understanding the Impact of Early Citers on Long-Term Scientific Impact},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This paper explores an interesting new dimension to the challenging problem of predicting long-term scientific impact (LTSI) usually measured by the number of citations accumulated by a paper in the long-term. It is well known that early citations (within 1--2 years after publication) acquired by a paper positively affects its LTSI. However, there is no work that investigates if the set of authors who bring in these early citations to a paper also affect its LTSI. In this paper, we demonstrate for the first time, the impact of these authors whom we call early citers (EC) on the LTSI of a paper. Note that this study of the complex dynamics of EC introduces a brand new paradigm in citation behavior analysis. Using a massive computer science bibliographic dataset we identify two distinct categories of EC - we call those authors who have high overall publication/citation count in the dataset as influential and the rest of the authors as non-influential. We investigate three characteristic properties of EC and present an extensive analysis of how each category correlates with LTSI in terms of these properties. In contrast to popular perception, we find that influential EC negatively affects LTSI possibly owing to attention stealing. To motivate this, we present several representative examples from the dataset. A closer inspection of the collaboration network reveals that this stealing effect is more profound if an EC is nearer to the authors of the paper being investigated. As an intuitive use case, we show that incorporating EC properties in the state-of-the-art supervised citation prediction models leads to high performance margins. At the closing, we present an online portal to visualize EC statistics along with the prediction results for a given query paper. We make all the codes and the processed dataset available in the public domain at our portal: http://www.cnergres.iitkgp.ac.in/earlyciters/},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {59–68},
numpages = {10},
keywords = {citation count, early citers, supervised regression models, long-term scientific impact},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200343,
author = {Hamborg, Felix and Meuschke, Norman and Gipp, Bela},
title = {Matrix-Based News Aggregation: Exploring Different News Perspectives},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {News aggregators capably handle the large amount of news that is published nowadays. However, these systems focus on identifying and presenting important, common information in news, but do not reveal different perspectives on the same topic. Differences in the content or presentation of news are referred to as media bias, which can have severe negative effects. Given their analysis approach, current news aggregators cannot effectively reveal media bias. To address this problem, we present matrix-based news analysis (MNA), a novel approach for news exploration that helps users gain a broad and diverse news understanding by presenting various perspectives on the same news topic. Additionally, we present NewsBird, a news aggregator that implements MNA for international news topics. The results of a case study demonstrate that NewsBird broadens the user's news understanding while providing similar news aggregation functionalities as established systems.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {69–78},
numpages = {10},
keywords = {google news, news aggregation, content analysis, media bias, frame analysis},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200344,
author = {Cole, Nicholas and Abdul-Rahman, Alfie and Mallon, Grace},
title = {<i>Quill</i>: A Framework for Constructing Negotiated Texts: With a Case Study on the US Constitutional Convention of 1787},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This paper describes a new approach to the presentation of records relating to formal negotiations and the texts that they create. It describes the architecture of a model, platform, and web-interface (https://www.quillproject.net) that can be used by domain-experts to convert the records typical of formal negotiations in to a model of decision-making (with minimal training). This model has implications for both research and teaching, by allowing for better qualitative and quantitative analysis of negotiations. The platform emphasizes the reconstruction as closely as possible of the context within which proposals and decisions are made. A generic platform, its usability, and benefits are illustrated by a presentation of the records relating to the 1787 Constitutional Convention that wrote the Constitution of the United States.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {79–88},
numpages = {10},
keywords = {data exploration, humanities, negotiated texts, user interfaces},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200345,
author = {Page, Kevin R and Bechhofer, Sean and Fazekas, Gy\"{o}rgy and Weigl, David M and Wilmering, Thomas},
title = {Realising a Layered Digital Library: Exploration and Analysis of the Live Music Archive through Linked Data},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Building upon a collection with functionality for discovery and analysis has been described by Lynch as a 'layered' approach to digital libraries. Meanwhile, as digital corpora have grown in size, their analysis is necessarily supplemented by automated application of computational methods, which can create layers of information as intricate and complex as those within the content itself. This combination of layers - aggregating homogeneous collections, specialised analyses, and new observations - requires a flexible approach to systems implementation which enables pathways through the layers via common points of understanding, while simultaneously accommodating the emergence of previously unforeseen layers.In this paper we follow a Linked Data approach to build a layered digital library based on content from the Internet Archive Live Music Archive. Starting from the recorded audio and basic information in the Archive, we first deploy a layer of catalogue metadata which allows an initial - if imperfect - consolidation of performer, song, and venue information. A processing layer extracts audio features from the original recordings, workflow provenance, and summary feature metadata. A further analysis layer provides tools for the user to combine audio and feature data, discovered and reconciled using interlinked catalogue and feature metadata from layers below.Finally, we demonstrate the feasibility of the system through an investigation of 'key typicality' across performances. This highlights the need to incorporate robustness to inevitable 'imperfections' when undertaking scholarship within the digital library, be that from mislabelling, poor quality audio, or intrinsic limitations of computational methods. We do so not with the assumption that a 'perfect' version can be reached; but that a key benefit of a layered approach is to allow accurate representations of information to be discovered, combined, and investigated for informed interpretation.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {89–98},
numpages = {10},
keywords = {music digital libraries, computational audio analysis, linked data, scholarly investigation using digital libraries, implementation},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200346,
author = {Bast, Hannah and Korzen, Claudius},
title = {A Benchmark and Evaluation for Text Extraction from PDF},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Extracting the body text from a PDF document is an important but surprisingly difficult task. The reason is that PDF is a layout-based format which specifies the fonts and positions of the individual characters rather than the semantic units of the text (e.g., words or paragraphs) and their role in the document (e.g., body text or caption). There is an abundance of extraction tools, but their quality and the range of their functionality are hard to determine.In this paper, we show how to construct a high-quality benchmark of principally arbitrary size from parallel TeX and PDF data. We construct such a benchmark of 12,098 scientific articles from arXiv.org and make it publicly available. We establish a set of criteria for a clean and independent assessment of the semantic abilities of a given extraction tool. We provide an extensive evaluation of 14 state-of-the-art tools for text extraction from PDF on our benchmark according to our criteria. We include our own method, Icecite, which significantly outperforms all other tools, but is still not perfect. We outline the remaining steps necessary to finally make text extraction from PDF a "solved problem".},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {99–108},
numpages = {10},
keywords = {evaluation, benchmark, text extraction, PDF},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200347,
author = {Duretec, Kresimir and Rauber, Andreas and Becker, Christoph},
title = {A Text Extraction Software Benchmark Based on a Synthesized Dataset},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Text extraction plays an important function for data processing workflows in digital libraries. For example, it is a crucial prerequisite for evaluating the quality of migrated textual documents. Complex file formats make the extraction process error-prone and have made it very challenging to verify the correctness of extraction components. Based on digital preservation and information retrieval scenarios, three quality requirements in terms of effectiveness of text extraction tools are identified: 1) is a certain text snippet correctly extracted from a document, 2) does the extracted text appear in the right order relative to other elements and, 3) is the structure of the text preserved. A number of text extraction tools is available fulfilling these three quality requirements to various degrees. However, systematic benchmarks to evaluate those tools are still missing, mainly due to the lack of datasets with accompanying ground truth. The contribution of this paper is two-fold. First we describe a dataset generation method based on model driven engineering principles and use it to synthesize a dataset and its ground truth directly from a model. Second, we define a benchmark for text extraction tools and complete an experiment to calculate performance measures for several tools that cover the three quality requirements. The results demonstrate the benefits of the approach in terms of scalability and effectiveness in generating ground truth for content and structure of text elements.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {109–118},
numpages = {10},
keywords = {ground truth, software testing, digital preservation, dataset, software benchmark, performance measures, text extraction, model driven engineering},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200348,
author = {Suzuki, Tokinori and Fujii, Atsushi},
title = {Mathematical Document Categorization with Structure of Mathematical Expressions},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Mathematical documents are used for mathematical communication such as a math paper and discussion in an online Q&amp;A community. Mathematical document categorization (MDC) is the task of classifying mathematical documents into mathematical categories, e.g., probability theory and set theory. This is an important task for supporting user search in recent widespread digital libraries and archiving services. Although Mathematical expressions (MEs) in the document can provide essential information for categorization, especially in math fields, using MEs for MDC has not been developed. In this paper, we propose a classification method based on text combined with structures of MEs, which are assumed to reflect conventions and rules specific to a category. We also present document collections built for evaluating MDC systems, with investigation of category settings and their statistics. We demonstrate classification results, and our proposed method outperforms existing methods with state-of-the-art ME modeling on F-measures.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {119–128},
numpages = {10},
keywords = {structural kernel, mathematical document, document categorization/classification, SVM},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200349,
author = {Castro, Eduardo P. S. and Chakravarty, Saurabh and Williamson, Eric and Pereira, Denilson Alves and Fox, Edward A.},
title = {Classifying Short Unstructured Data Using the Apache Spark Platform},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {People worldwide use Twitter to post updates about the events that concern them directly or indirectly. Study of these posts can help identify global events and trends of importance. Similarly, E-commerce applications organize their products in a way that can facilitate their management and satisfy the needs and expectations of their customers. However, classifying data such as tweets or product descriptions is still a challenge. These data are described by short texts, containing in their vocabulary abbreviations of sentences, emojis, hashtags, implicit codes, and other non-standard usage of written language. Consequently, traditional text classification techniques are not effective on these data. In this paper, we describe our use of the Spark platform to implement two classification strategies to process large data collections, where each datum is a short textual description. One of our solutions uses an associative classifier, while the other is based on a multiclass Logistic Regression classifier using Word2Vec as a feature selection and transformation technique. Our associative classifier captures the relationships among words that uniquely identify each class, and Word2Vec captures the semantic and syntactic context of the words. In our experimental evaluation, we compared our solutions, as well as Spark MLlib classifiers. We assessed effectiveness, efficiency, and memory requirements. The results indicate that our solutions are able to effectively classify the millions of data instances composed of thousands of distinct features and classes, found in our digital libraries.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {129–138},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200350,
author = {Elekes, \'{A}bel and Sch\"{a}ler, Martin and B\"{o}hm, Klemens},
title = {On the Various Semantics of Similarity in Word Embedding Models},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Finding similar words with the help of word embedding models, such as Google's Word2Vec or Glove, computed on large-scale digital libraries has yielded meaningful results in many cases. However, the underlying notion of similarity has remained ambiguous. In this paper, we examine when exactly similarity values in word embedding models are meaningful. To do so, we analyze the statistical distribution of similarity values systematically, conducting two series of experiments. The first one examines how the distribution of similarity values depends on the different embedding-model algorithms and parameters. The second one starts by showing that intuitive similarity thresholds do not exist. We then propose a method stating which similarity values actually are meaningful for a given embedding model. In more abstract terms, our insights give way to a better understanding of the notion of similarity in embedding models and to more reliable evaluations of such models.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {139–148},
numpages = {10},
keywords = {similarity values, word embedding models, semantic similarity},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200351,
author = {Bamman, David and Carney, Michelle and Gillick, Jon and Hennesy, Cody and Sridhar, Vijitha},
title = {Estimating the Date of First Publication in a Large-Scale Digital Library},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {One prerequisite for cultural analysis in large-scale digital libraries is an accurate estimate of the date of composition of the text---as distinct from the date of publication of an edition---for the works they contain. In this work, we present a manually annotated dataset of first dates of publication of three samples of books from the HathiTrust Digital Library (uniform random, uniform fiction, and stratified by decade), and empirically evaluate the disparity between these gold standard labels and several approximations used in practice (using the date of publication as provided in metadata, several deduplication methods, and automatically predicting the date of composition from the text of the book). We find that a simple heuristic of metadata-based deduplication works best in practice, and text-based composition dating is accurate enough to inform the analysis of "apparent time."},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {149–158},
numpages = {10},
keywords = {bibliographic metadata, digital libraries, publication date prediction},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200352,
author = {Buchanan, George and McKay, Dana},
title = {The Lowest Form of Flattery: Characterising Text Re-Use and Plagiarism Patterns in a Digital Library Corpus},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {The re-use of text---particularly misuse, or plagiarism---is a contentious issue. Technological approaches to identifying student plagiarism are now widespread, but academic publications do not typically come under such scrutiny. It is common knowledge that plagiarism occurs, but we do not know how frequently or extensively, nor where in a document it is likely to be found. This paper offers the first assessment of text re-use in the field of digital libraries. It also characterises text reuse generally (and plagiarism specifically) according to location in the document, author seniority, publication venue and open access. As a secondary contribution, we suggest future routes towards more rigorous plagiarism detection and management.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {159–168},
numpages = {10},
keywords = {misconduct, digital libraries, text re-use, plagiarism, open access},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200353,
author = {Alawini, Abdussalam and Chen, Leshang and Davidson, Susan B. and Da Silva, Natan Portilho and Silvello, Gianmaria},
title = {Automating Data Citation: The Eagle-i Experience},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Data citation is of growing concern for owners of curated databases, who wish to give credit to the contributors and curators responsible for portions of the dataset and enable the data retrieved by a query to be later examined. While several databases specify how data should be cited, they leave it to users to manually construct the citations and do not generate them automatically.We report our experiences in automating data citation for an RDF dataset called eagle-i, and discuss how to generalize this to a citation framework that can work across a variety of different types of databases (e.g., relational or XML).},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {169–178},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200354,
author = {Sikdar, Sandipan and Marsili, Matteo and Ganguly, Niloy and Mukherjee, Animesh},
title = {Influence of Reviewer Interaction Network on Long-Term Citations: A Case Study of the Scientific Peer-Review System of the Journal of High Energy Physics},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {A 'peer-review system' in the context of judging research contributions, is one of the prime steps undertaken to ensure the quality of the submissions received; a significant portion of the publishing budget is spent towards successful completion of the peer-review by the publication houses. Nevertheless, the scientific community is largely reaching a consensus that peer-review system, although indispensable, is nonetheless flawed. A very pertinent question therefore is "could this system be improved?". In this paper, we attempt to present an answer to this question by considering a massive dataset of around 29k papers with roughly 70k distinct review reports together consisting of 12m lines of review text from the Journal of High Energy Physics (JHEP) between 1997 and 2015. In specific, we introduce a novel reviewer-reviewer interaction network (an edge exists between two reviewers if they were assigned by the same editor) and show that surprisingly the simple structural properties of this network such as degree, clustering coefficient, centrality (closeness, betweenness etc.) serve as strong predictors of the long-term citations (i.e., the overall scientific impact) of a submitted paper. These features, when plugged in a regression model, alone achieves a high R2 of 0.79 and a low RMSE of 0.496 in predicting the long-term citations. In addition, we also design a set of supporting features built from the basic characteristics of the submitted papers, the authors and the referees (e.g., the popularity of the submitting author, the acceptance rate history of a referee, the linguistic properties laden in the text of the review reports etc.), which further results in overall improvement with R2 of 0.81 and RMSE of 0.46. Analysis of feature importance shows that the network features constitute the best predictors for this task. Although we do not claim to provide a full-fledged reviewer recommendation system (that could potentially replace an editor), our method could be extremely useful in assisting the editors in deciding the acceptance or rejection of a paper, thereby, improving the effectiveness of the peer-review system.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {179–188},
numpages = {10},
keywords = {prediction, reviewer-reviewer interaction network, citations},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200355,
author = {Klein, Martin and Van de Sompel, Herbert},
title = {Discovering Scholarly Orphans Using ORCID},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Archival efforts such as (C)LOCKSS and Portico are in place to ensure the longevity of traditional scholarly resources like journal articles. At the same time, researchers are depositing a broad variety of other scholarly artifacts into emerging online portals that are designed to support web-based scholarship. These web-native scholarly objects are largely neglected by current archival practices and hence they become scholarly orphans. We therefore argue for a novel paradigm that is tailored towards archiving these scholarly orphans. We are investigating the feasibility of using Open Researcher and Contributor ID (ORCID) as a supporting infrastructure for the process of discovery of web identities and scholarly orphans for active researchers. We analyze ORCID in terms of coverage of researchers, subjects, and location and assess the richness of its profiles in terms of web identities and scholarly artifacts. We find that ORCID currently lacks in all considered aspects and hence can only be considered in conjunction with other discovery sources. However, ORCID is growing fast so there is potential that it could achieve a satisfactory level of coverage and richness in the near future.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {189–198},
numpages = {10},
keywords = {ORCID, archiving, scholarly communication, scholarly orphans},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200356,
author = {Nanni, Federico and Ponzetto, Simone Paolo and Dietz, Laura},
title = {Building Entity-Centric Event Collections},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Web archives preserve an unprecedented abundance of materials regarding major events and transformations in our society. In this paper, we present an approach for building event-centric sub-collections from such large archives, which includes not only the core documents related to the event itself but, even more importantly, documents describing related aspects (e.g., premises and consequences). This is achieved by 1) identifying relevant concepts and entities from a knowledge base, and 2) detecting their mentions in documents, which are interpreted as indicators for relevance. We extensively evaluate our system on two diachronic corpora, the New York Times Corpus and the US Congressional Record, and we test its performance on the TREC KBA Stream corpus, a large and publicly available web archive.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {199–208},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200357,
author = {Benetka, Jan R. and Balog, Krisztian and N\o{}rv\r{a}g, Kjetil},
title = {Towards Building a Knowledge Base of Monetary Transactions from a News Collection},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {We address the problem of extracting structured representations of economic events from a large corpus of news articles, using a combination of natural language processing and machine learning techniques. The developed techniques allow for semi-automatic population of a financial knowledge base, which, in turn, may be used to support a range of data mining and exploration tasks. The key challenge we face in this domain is that the same event is often reported multiple times, with varying correctness of details. We address this challenge by first collecting all information pertinent to a given event from the entire corpus, then considering all possible representations of the event, and finally, using a supervised learning method, to rank these representations by the associated confidence scores. A main innovative element of our approach is that it jointly extracts and stores all attributes of the event as a single representation (quintuple). Using a purpose-built test set we demonstrate that our supervised learning approach can achieve 25% improvement in F1-score over baseline methods that consider the earliest, the latest or the most frequent reporting of the event.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {209–218},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200358,
author = {Nwala, Alexander C. and Weigle, Michele C. and Nelson, Michael L. and Ziegler, Adam B. and Aizman, Anastasia},
title = {Local Memory Project: Providing Tools to Build Collections of Stories for Local Events from Local Sources},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {The national (non-local) news media has different priorities than the local news media. If one seeks to build a collection of stories about local events, the national news media may be insufficient, with the exception of local news which "bubbles" up to the national news media. If we rely exclusively on national media, or build collections exclusively on their reports, we could be late to the important milestones which precipitate major local events, thus, run the risk of losing important stories due to link rot and content drift. Consequently, it is important to consult local sources affected by local events. Our goal is to provide a suite of tools (beginning with two) under the umbrella of the Local Memory Project (LMP) to help users and small communities discover, collect, build, archive, and share collections of stories for important local events by leveraging local news sources. The first service (Geo) returns a list of local news sources (newspaper, TV and radio stations) in order of proximity to a user-supplied zip code. The second service (Local Stories Collection Generator) discovers, collects and archives a collection of news stories about a story or event represented by a user-supplied query and zip code pair. We evaluated 20 pairs of collections, Local (generated by our system) and non-Local, by measuring archival coverage, tweet index rate, temporal range, precision, and sub-collection overlap. Our experimental results showed Local and non-Local collections with archive rates of 0.63 and 0.83, respectively, and tweet index rates of 0.59 and 0.80, respectively. Local collections produced older stories than non-Local collections, at a higher precision (relevance) of 0.84 compared to a non-Local precision of 0.72. These results indicate that Local collections are less exposed, thus less popular than their nonLocal counterpart.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {219–228},
numpages = {10},
keywords = {web archiving, news, digital collections, collections building, journalism, local news},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200359,
author = {Poursardar, Faryaneh and Shipman, Frank},
title = {What is Part of That Resource? User Expectations for Personal Archiving},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Users wish to preserve Internet resources for later use. But what is part of and what is not part of an Internet resource remains an open question. In this paper we examine how specific relationships between web pages affect user perceptions of their being part of the same resource. This study presented participants with pairs of pages and asked about their expectation for having access to the second page after they save the first. The primary-page content in the study comes from multi-page stories, multi-image collections, product pages with reviews and ratings on separate pages, and short single page writings. Participants were asked to agree or disagree with three statements regarding their expectation for later access. Nearly 80% of participants agreed in the case of articles spread across multiple pages, images in the same collection, and additional details or assessments of product information. About 50% agreed for related content on pages linked to by the original page or related items while only about 30% thought advertisements or wish lists linked to were part of the resource. Differences in responses to the same page pairs for the three statements regarding later access indicate some users distinguish between what would be valuable to them and their expectations of systems saving or archiving web content.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {229–238},
numpages = {10},
keywords = {web archiving, personal archiving, digital preservation},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200360,
author = {Xu, Weijia and Esteva, Maria and Beck, Deborah and Hsieh, Yi-Hsuan},
title = {A Portable Strategy for Preserving Web Applications Functionality},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {The value of research data not only resides in its content, but in how it is made available to users. Research data is often presented interactively through a web application, the design of which is often the result of years of work by researchers. Therefore, preserving the data and the application's functionalities becomes equally important. However, preserving web applications, which are commonly deployed within shared and changing technology infrastructures, presents challenges to the reproducibility and portability of the application across technology platforms over time. We propose a functional preservation strategy to decouple web applications and their corresponding data from their hosting environments. The strategy allows re-launching the applications in more portable, simplified environments without compromising their interactive features, and it allows reusing the data in other technical and functional contexts. The strategy fits well with the evolving nature of digital preservation and with the requirements for data reuse. We demonstrate this approach and its evaluation using the Speech Presentation in Homeric Epic digital humanities project.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {233–236},
numpages = {4},
keywords = {web applications preservation, digital libraries, digital humanities, data reuse},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200361,
author = {Alam, Sawood and Kelly, Mat and Weigle, Michele C. and Nelson, Michael L.},
title = {Client-Side Reconstruction of Composite Mementos Using Serviceworker},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {We use the ServiceWorker (SW) API to intercept HTTP requests for embedded resources and reconstruct Composite Mementos without the need for conventional URL rewriting typically performed by web archives. URL rewriting is a problem for archival replay systems, especially for URLs constructed by JavaScript, that frequently results in incorrect URI references. By intercepting requests on the client using SW, we are able to strategically reroute instead of rewrite. Our implementation moves rewriting to clients, saving servers' computing resources and allowing servers to return responses more quickly. In our experiments, retrieving the original instead of rewritten pages from the archive resulted in a one-third reduction in time overhead and a one-fifth reduction in data overhead. Our system, reconstructive.js, prevents the live web from leaking into Composite Mementos while being easy to distribute and maintain.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {237–240},
numpages = {4},
keywords = {memento, composite memento, serviceworker, web archive, archival replay},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200362,
author = {Wu, Jian and Choudhury, Sagnik Ray and Chiatti, Agnese and Liang, Chen and Giles, C. Lee},
title = {HESDK: A Hybrid Approach to Extracting Scientific Domain Knowledge Entities},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {We investigate a variant of the problem of automatic keyphrase extraction from scientific documents which we define as Scientific Domain Knowledge Entity (SDKE) extraction. Keyphrases are noun phrases important to the documents themselves. In contrasxt, an SDKE is text that refers to a concept and can be classified as a process, material, task, dataset etc. A SDKE represents domain knowledge, but is not necessarily important to the document it is in. Supervised keyphrase extraction algorithms using non-sequential classifiers and global measures of informativeness (PMI, tf-idf) have been used for this task. Another approach is to use sequential labeling algorithms with local context from a sentence, as done in the named entity recognition. We show that these two methods can complement each other and a simple merging can improve the extraction accuracy by 5--7 percentiles. We further propose several heuristics to improve the extraction accuracy. Our preliminary experiments suggest that it is possible to improve the accuracy of the sequential learner itself by utilizing the predictions of the non-sequential model.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {241–244},
numpages = {4},
keywords = {conditional random field, digital library, natural language processing, keyphrase extraction, scientific domain knowledge entity},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200363,
author = {Yang, Xiao and He, Dafang and Huang, Wenyi and Ororbia, Alexander and Zhou, Zihan and Kifer, Daniel and Giles, C. Lee},
title = {Smart Library: Identifying Books on Library Shelves Using Supervised Deep Learning for Scene Text Reading},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Physical library collections are valuable and long standing resources for knowledge and learning. However, managing and finding books or other volumes on a large collection of bookshelves often leads to tedious manual work, especially for large collections where books or others might be missing or misplaced. Recently, deep neural-based models have been successful in detecting and recognizing text in images taken from natural scenes. Based on this, we investigate deep learning for facilitating book management. This task introduces further challenges including image distortion and varied lighting conditions. We present a library inventory building and retrieval system based on scene text reading. We specifically design our text recognition model using rich supervision to accelerate training and achieve state-of-the-art performance on several benchmark datasets. Our proposed system has the potential to greatly reduce the amount of manual labor required for managing book inventories.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {245–248},
numpages = {4},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200364,
author = {Chiron, Guillaume and Doucet, Antoine and Coustaty, Micka\"{e}l and Visani, Muriel and Moreux, Jean-Philippe},
title = {Impact of OCR Errors on the Use of Digital Libraries: Towards a Better Access to Information},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Digital collections are increasingly used for a variety of purposes. In Europe only, we can conservatively estimate that tens of thousands of users consult digital libraries daily. The usages are often motivated by qualitative and quantitative research. However, caution must be advised as most digitized documents are indexed through their OCRed version, which is far from perfect, especially for ancient documents. In this paper, we aim to estimate the impact of OCR errors on the use of a major online platform: The Gallica digital library from the National Library of France. It accounts for more than 100M OCRed documents and receives 80M search queries every year. In this context, we introduce two main contributions. First, an original corpus of OCRed documents composed of 12M characters along with the corresponding gold standard is presented and provided, with an equal share of English- and French-written documents. Next, statistics on OCR errors have been computed thanks to a novel alignment method introduced in this paper. Making use of all the user queries submitted to the Gallica portal over 4 months, we take advantage of our error model to propose an indicator for predicting the relative risk that queried terms mismatch targeted resources due to OCR errors, underlining the critical extent to which OCR quality impacts on digital library access.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {249–252},
numpages = {4},
keywords = {search logs, digital libraries, indexation bias, OCR errors},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200365,
author = {Weigl, David M. and Page, Kevin R. and Organisciak, Peter and Downie, J. Stephen},
title = {Information-Seeking in Large-Scale Digital Libraries: Strategies for Scholarly Workset Creation},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Large-scale digital libraries such as the HathiTrust contain massive quantities of content combined from heterogeneous collections, with consequential challenges in providing mechanisms for discovery, unified access, and analysis. The HathiTrust Research Center has proposed 'worksets' as a solution for users to conduct their research into the 15 million volumes of HathiTrust content; however existing models of users' information-seeking behaviour, which might otherwise inform workset development, were established before digital library resources existed at such a scale.We examine whether these information-seeking models can sufficiently articulate the emergent user activities of scholarly investigation as perceived during the creation of worksets. We demonstrate that a combination of established models by Bates, Ellis, and Wilson can accommodate many aspects of information seeking in large-scale digital libraries at a broad, conceptual, level. We go on to identify the supplemental information-seeking strategies necessary to specifically describe several workset creation exemplars.Finally, we propose complementary additions to the existing models: we classify strategies as instances of querying, browsing, and contribution. Similarly we introduce a notion of scope according to the interaction of a strategy with content, content-derived metadata, or contextual metadata. Considering the scope and modality of new and existing strategies within the composite model allows us to better express---and so aid our understanding of---information-seeking behaviour within large-scale digital libraries.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {253–256},
numpages = {4},
keywords = {digital libraries, workset creation, information-seeking behaviour},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200366,
author = {Darch, Peter T. and Sands, Ashley E.},
title = {Uncertainty about the Long-Term: Digital Libraries, Astronomy Data, and Open Source Software},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Digital library developers make critical design and implementation decisions in the face of uncertainties about the future. We present a qualitative case study of the Large Synoptic Survey Telescope (LSST), a major astronomy project that will collect and make available large-scale datasets. LSST developers make decisions now, while facing uncertainties about its period of operations (2022--2032). Uncertainties we identify include topics researchers will seek to address, tools and expertise, and availability of other infrastructures to exploit LSST observations. LSST is using an open source approach to developing and releasing its data management software. We evaluate benefits and burdens of this approach as a strategy for addressing uncertainty. Benefits include: enabling software to adapt to researchers' changing needs; embedding LSST standards and tools in community practices; and promoting interoperability with other infrastructures. Burdens include: open source community management; documentation requirements; and trade-offs between software speed and accessibility.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {257–260},
numpages = {4},
keywords = {data management, open source, knowledge infrastructures, big data, long term, data curation, astronomy, big science, scientific data},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200367,
author = {Murdock, Jaimie and Jett, Jacob and Cole, Tim and Ma, Yu and Downie, J. Stephen and Plale, Beth},
title = {Towards Publishing Secure Capsule-Based Analysis},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Computational engagement with the HathiTrust Digital Library (HTDL) is confounded by the in-copyright status and licensing restrictions on the majority of the content. Because of these limitations, computational analysis on the HTDL must either be carried out in a secure environment or on derivative datasets. The HathiTrust Research Center (HTRC) Data Capsule service provides researchers with a secure environment through which they invoke tools that create, analyze, and export non-consumptive datasets. These derivative datasets, so long as they do not reproduce the full-text of the original work, are a transformative work protected by Fair Use provisions of United States Copyright Law, and can be published for reuse by other researchers, as the HTRC Extracted Features Dataset has been. Secure environments and derivative datasets enable researchers to engage with restricted data from focused studies of a few dozen volumes to large-scale experiments on millions of volumes. This paper describes advances in the Capsule service through a case study of how the HTRC Data Capsule service has advanced our activities on provenance, workflows, worksets, and non-consumptive exports through a topic modeling example. We also discuss the potential applications of this Capsule-based model to other digital libraries wrestling with research access and copyright restrictions.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {261–264},
numpages = {4},
keywords = {semantic web, metadata management, data provenance, research workflows, digital libraries, text processing},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200368,
author = {Kocher, Mirco and Savoy, Jacques},
title = {Author Clustering Using Spatium},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This paper presents the author clustering problem and compares it to related authorship attribution questions. The proposed model is based on a distance measure called Spatium derived from the Canberra measure (weighted version of L1 norm). The selected features consist of the 200 most frequent words and punctuation symbols. An evaluation methodology is presented and the test collections are extracted from the PAN CLEF 2016 evaluation campaign. In addition to those, we also consider two additional corpora reflecting the literature domain more closely. Based on four different languages, the evaluation measures demonstrate a high precision and F1 for all 20 test collections. A more detailed analysis provides reasons explaining some of the failures of the Spatium model.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {265–268},
numpages = {4},
keywords = {authorship attribution, clustering algorithm, stylometry},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200369,
author = {Xu, Shaobin and Smith, David},
title = {Retrieving and Combining Repeated Passages to Improve OCR},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {We present a novel approach to improve the output of optical character recognition (OCR) systems by first detecting duplicate passages in their output and then performing consensus decoding combined with a language model. This approach is orthogonal to, and may be combined with, previously proposed methods for combining the output of different OCR systems on the same image or the output of the same OCR system on differently processed images of the same text. It may also be combined with methods to estimate the parameters of a noisy channel model of OCR errors. Additionally, the current method generalizes previous proposals for a simple majority-vote combination of known duplicated texts. On a corpus of historical newspapers, an annotated set of clusters has a baseline word error rate (WER) of 33%. A majority vote procedure reaches 23% on passages where one or more duplicates were found, and consensus decoding combined with a language model achieves 18% WER. In a separate experiment, newspapers were aligned to very widely reprinted texts such as State of the Union speeches, producing clusters with up to 58 witnesses. Beyond 20 witnesses, majority vote outperforms language model rescoring, though the gap between them is much less in this experiment.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {269–272},
numpages = {4},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200370,
author = {Gipp, Bela and Breitinger, Corinna and Meuschke, Norman and Beel, Joeran},
title = {Cryptsubmit: Introducing Securely Timestamped Manuscript Submission and Peer Review Feedback Using the Blockchain},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Manuscript submission systems are a central fixture in scholarly publishing. However, with existing systems, researchers must trust that their yet unpublished findings will not prematurely be disseminated due to technical weaknesses and that anonymous peer reviewers or committee members will not plagiarize unpublished content. To address this limitation, we present CryptSubmit - a system that automatically creates a decentralized, tamperproof, and publicly verifiable timestamp for each submitted manuscript by utilizing the blockchain of the cryptocurrency Bitcoin. The publicly accessible and tamperproof infrastructure of the blockchain allows researchers to independently verify the validity of the timestamp associated with their manuscript at the time of submission to a conference or journal. Our system supports researchers in protecting their intellectual property even in the face of vulnerable submission platforms or dishonest peer reviewers. Optionally, the system also generates trusted timestamps for the feedback shared by peer reviewers to increase the traceability of ideas. CryptSubmit integrates these features into the open source conference management system OJS. In the future, the method could be integrated at nearly no overhead cost into other manuscript submission systems, such as EasyChair, ConfTool, or Ambra. The introduced method can also improve electronic pre-print services and storage systems for research data.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {273–276},
numpages = {4},
keywords = {blockchain, electronic publishing, scientific data management, manuscript submission, conference management, peer review},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200371,
author = {Singh, Mayank and Bakshi, Nikhil Angad and Niranjan, Abhishek and Mukherjee, Animesh and Gupta, Divyansh and Goyal, Pawan},
title = {Citation Sentence Reuse Behavior of Scientists: A Case Study on Massive Bibliographic Text Dataset of Computer Science},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Our current knowledge of scholarly plagiarism is largely based on the similarity between full text research articles. In this paper, we propose an innovative and novel conceptualization of scholarly plagiarism in the form of reuse of explicit citation sentences in scientific research articles. Note that while full-text plagiarism is an indicator of a gross-level behavior, copying of citation sentences is a more nuanced micro-scale phenomenon observed even for well-known researchers. The current work poses several interesting questions and attempts to answer them by empirically investigating a large bibliographic text dataset from computer science containing millions of lines of citation sentences. In particular, we report evidences of massive copying behavior. We also present several striking real examples throughout the paper to showcase widespread adoption of this undesirable practice. In contrast to the popular perception, we find that copying tendency increases as an author matures. The copying behavior is reported to exist in all fields of computer science; however, the theoretical fields indicate more copying than the applied fields.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {277–280},
numpages = {4},
keywords = {text reuse, citation context, plagiarism},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200373,
author = {Ayala, Brenda Reyes and Chen, Jiangping},
title = {A Machine Learning Approach to Evaluating Translation Quality},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {We explored supervised machine learning (ML) techniques to understand and predict the adequacy and fluency of English-Spanish machine translation. Five experiments were conducted using three classifiers in Weka, an open-source ML tool. We found that the highest performance was achieved by applying a dimensionality reduction approach to the classification task, which included collapsing a numeric scale of quality to two categories: high quality and low quality. Our results showed that the Support Vector Machine classifier performed the best at predicting the adequacy (65.65%) and fluency (65.77%) of the translations. More research is needed to explore the methodologies of applying ML to translation evaluation.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {281–282},
numpages = {2},
keywords = {machine translation evaluation, machine learning, Weka},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200374,
author = {Bainbridge, David and Downie, J. Stephen},
title = {All for One and One for All: Reconciling Research and Production Values at the HathiTrust through User-Scripting},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This article details a practical technique that safely reconciles the production stability and integrity of the HathiTrust Digital Library (HTDL) with the riskier and potentially disruptive experimental functionalities created by the HathiTrust Research Center. Web systems produced by HTRC are necessarily more speculative and, understandably, operate on equipment outside of the HTDL production environment. The key to our approach that brings these two parts closer together is to exploit user-scripting: a web browser add-in technique that allows users to introduce bespoke Javascript code that alters the behavior of specific website(s). We demonstrate how it can be used to provide a mashup of three web sites: HTDL and two web-based offerings operated independently by HTRC. The end result is that the user interacts with the HTDL as usual, and at strategic locations in the interface additionally functionality drawn from the research systems---which takes account of the user's current context---is seamlessly blended in.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {283–284},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200375,
author = {Sterling, Corey and Pierre, Carlin St. and Bainbridge, David},
title = {Big Brother is Watching You: Now in a Doubleplusgood Way},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This paper reports on the progress made with our X Marks The Spot (XMTS) system. In XMTS we make a fundamental change to the compositing operation in a desktop manager, storing all text-display events and their related raster-draw events. This means we can provide the user with a search interface that allows them to go back in time and view how their desktop used to look. Because XMTS is aware of what text was drawn into which window and at what position, the provided snapshot is semi-interactive in that the user can bring individual windows forwards and backwards, and copy text from them. In previous work we demonstrated the approach is technically feasible. In this paper we detail infrastructural advances we have made and showcase its new search interface.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {285–286},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200376,
author = {Eto, Masaki and Agata, Teru and Sugie, Noriko and Otani, Yasuharu and Agata, Mari},
title = {Can Japanese Manga Be Automatically Classified from Public Library Holdings?},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This study addressed the automatic classification of Japanese manga held by public libraries. Holdings of 4,681 public libraries and similar facilities were investigated, and 29,795 manga titles were identified. Hierarchical clustering was applied to 631 titles that were each held by more than one hundred libraries. Five clusters were identified in the upper hierarchy. Principal coordinate analysis and a manual examination of individual titles were performed to identify the common characteristics of the works in each cluster. The results suggest that the proposed method offers a novel approach to large-scale classification of manga titles.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {287–288},
numpages = {2},
keywords = {manga, public libraries, hierarchical clustering},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200377,
author = {Park, Juyeong and Yoshikawa, Masatoshi and Kato, Hiroyuki},
title = {Cell-Based Provenance for Scientific Data},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {We propose a cell-based provenance semiring to calculate the degree of credit given to scientific data creators. We also propose latent provenance, which is a generalization of the cell-based provenance, to assess the contribution of data creators who provided data used in the query calculation but not appearing in the query result.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {289–290},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200378,
author = {Burke, Robin and Lucic, Ana and Shanahan, John},
title = {Circulation Modeling of Library Book Promotions},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Community reading initiatives, in which a book is selected for system-wide reading and discussion, have become common in many library systems. This paper describes the initial findings in a demographic study of the "One Book, One Chicago" initiative by the Chicago Public Library. Using a multilevel linear model, we show that, over six recent offerings of the "One Book" program, the books vary widely in their uptake by library patrons at different branches, and these differences cannot be entirely explained by demographics or the library's promotional strategies. We thus motivate the next stage of our project, which is to incorporate representations of book content and reader response into the model.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {291–292},
numpages = {2},
keywords = {statistical modeling, circulation, demographics},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200379,
author = {Shen, Yi},
title = {Data Discovery, Reuse, and Integration: The Perspectives of Natural Resources and Environmental Scientists},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This paper presents a user community research of natural resources and environmental scientists on their data discovery, reuse, and integration challenges, and provides recommendations on how academic libraries should improve their data repository systems and curation services.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {293–294},
numpages = {2},
keywords = {data discovery, data sharing, data repository system, environmental scientists, natural resources data, user communities},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200380,
author = {Randall, P.},
title = {Expanding Access to Biodiversity Literature},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {The Biodiversity Heritage Library (BHL) is a consortium of major natural history museum libraries, botanical libraries, and research institutions that cooperate to digitize and make accessible the legacy biodiversity literature. Through an Institute of Museum and Library Services (IMLS)-funded grant called Expanding Access to Biodiversity Literature (EABL), BHL has adapted its digitization and metadata workflows to accommodate small organizations outside the consortium that would like to contribute unique content to BHL but lack the resources to do so. This requires innovative approaches to ingesting born digital and already-digitized material; training partners on BHL's metadata creation tool, Macaw; expanded copyright metadata and display fields in the user interface; and definition of articles and other bibliographic segments.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {295–296},
numpages = {2},
keywords = {metadata, natural sciences, collection discovery and development, system design},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200381,
author = {Bailey, Christian and Siravuri, Harish Varma and Kale, Bharat and Alhoori, Hamed and Walker, Jamieson and Papka, Michael E.},
title = {Exploring Features for Predicting Policy Citations},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {In this study we performed an initial investigation and evaluation of altmetrics and their relationship with public policy citation of research papers. We examined methods for using altmetrics and other data to predict whether a research paper is cited in public policy and applied receiver operating characteristic curve on various feature groups in order to evaluate their potential usefulness. From the methods we tested, classifying based on tweet count provided the best results, achieving an area under the ROC curve of 0.91.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {297–298},
numpages = {2},
keywords = {social media, public policy, altmetrics},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200382,
author = {Jones, Bailey and Capra, Robert},
title = {Exploring Salient Thumbnail Generation for Archival Collections Online},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This study evaluated the effectiveness of thumbnails generated using open source image saliency software for representing digitized archival documents. Salient thumbnails were evaluated against a baseline (full images scaled down) in a lab study that assessed the thumbnails' usefulness during finding and re-finding tasks. Results found no significant differences in time or user preference. However, we observed two interesting trends in our data: (1) there were fewer errors for re-finding tasks done with salient thumbnails, suggesting that they may offer some advantages for accuracy of re-finding, and (2) participants spent about 20 seconds longer on average when completing tasks with salient thumbnails. Salient thumbnails may offer a modest advantage over the baseline for re-finding tasks, but their overall suitability for representing a digital archive online depends on the user and task at hand.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {299–300},
numpages = {2},
keywords = {image saliency, graphical user interfaces, digital libraries and archives, thumbnails, digital humanities},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200383,
author = {Wang, Jenq-Haur and Chen, Yen-Sheng and Peng, Jui-Wen and Chen, Ing-Yi},
title = {Heterogeneous Resources Aggregation for Literature Usage Analysis in Academic Libraries},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {In academic libraries, electronic resources usually dominate the expenditure, but their usage is not systematically analyzed. Usage analysis usually requires manual integration of reports from various vendors in different formats via diverse channels. This makes it difficult to evaluate the cost effectiveness. In this poster, we implemented a systematic model to aggregate heterogeneous resources for literature usage analysis in academic libraries. First, usage reports from various vendors were automatically extracted. Then, diverse semantics of usage reports were parsed and transformed into a uniform format extended from COUNTER standard. Finally, objective analysis based on usage and cost statistics was conducted. This helps automatic usage reports aggregation and evaluation of resource development policy.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {301–302},
numpages = {2},
keywords = {cost effectiveness, library expenses, resource aggregation, usage statistics, literature usage analysis},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200384,
author = {Kelly, Mat and Alkwai, Lulwah M. and Alam, Sawood and Nelson, Michael L. and Weigle, Michele C. and Van de Sompel, Herbert},
title = {Impact of URI Canonicalization on Memento Count},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Memento TimeMaps [5] list identifiers for archival web captures (URI-Ms). When some URI-Ms are dereferenced, they redirect to a different URI-M instead of a unique representation at the datetime. This suggests that confidently obtaining an accurate count quantifying the number of non-forwarding captures for an Original Resource URI (URI-R) is not possible using a TimeMap alone and that the magnitude of a TimeMap is not equivalent to the number of representations it identifies. This work represents an abbreviated version of the full technical report describing this phenomena in depth [3]. For google.com we found that 84.9% of the URI-Ms in a TimeMap result in an HTTP redirect when dereferenced. The full study applies this technique to seven other URI-Rs of large Web sites and 13 academic institutions. Using a ratio metric for the number of URI-Ms without redirects to those requiring a redirect when dereferenced, five of the eight large web sites' and two of the thirteen academic institutions' TimeMaps had a ratio of less than one, indicating that more than half of the URI-Ms in these TimeMaps result in redirects when dereferenced.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {303–304},
numpages = {2},
keywords = {memento, HTTP, web archive, redirection},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200385,
author = {Cho, Hyerim and Schmalz, Marc L. and Keating, Stephen A. and Lee, Jin Ha},
title = {Information Needs for Anime Recommendation: Analyzing Anime Users' Online Forum Queries},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Despite the increasing consumption and popularity of audio-visual materials and non-textual information, recommendation-based information retrieval research regarding these materials remains limited. To provide robust recommendation services to users, it is critical to understand how users describe their needs when they seek audio-visual materials. We conducted a content analysis of 396 recommendation threads from Anime News Network online forums to identify 19 common information features used in these requests. Work, Theme, and Genre were the most frequently mentioned features when users described anime they were seeking. Findings also show Audience as an important anime information need. Together, these form a distinct set of interests, vital to understanding the information needs of anime users.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {305–306},
numpages = {2},
keywords = {information seeking behavior, animation, audio-visual materials, multimedia information, information needs, query analysis, anime},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200386,
author = {Saundry, A.},
title = {Institutional Repository Digital Object Metadata Enhancement and Re-Architecting},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {We present work undertaken at our institutional repository to enhance metadata and re-organize digital objects according to new information architecture, in an effort to minimize administrative object management and processing, and improve object discovery and use. This work was partly motivated by the launch of a new discovery platform at our institution, which aggregates metadata and full text from our four open access repositories into a cohesive, consistent, and enhanced searching and browsing experience. The platform provides digital object identifier (DOI) assignment, metadata access via various formats, and an open metadata and full text application program interface (API) for researchers, amongst other features. Functionality of these platform features relies heavily on accurate object representation and metadata. This work facilitates and improves the discovery and engagement of the diverse digital objects available from our institution, so they can be used and analyzed in new, flexible, and innovative ways by a myriad of communities and disciplines.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {307–308},
numpages = {2},
keywords = {information architecture, digital libraries, institutional repository, metadata, digital objects},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200387,
author = {Jett, Jacob and Cole, Timothy W. and Han, Myung-Ja K. and Szylowicz, Caroline},
title = {Linked Open Data (LOD) for Library Special Collections},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {The richness of legacy metadata describing items in library special collections can be an asset when these collections are digitized. Transforming legacy metadata into formats that are Linked Open Data (LOD) friendly, maximizes metadata value and better integrates digital library collections into the emerging Semantic Web. This poster details experience with mapping Dublin-Core-based metadata into the Resource Description Framework (RDF) using Schema.org vocabulary, summarizes challenges encountered and illutrates how such transformations enable additional functionality. Connections into the Semantic Web are enhanced by making item descriptions more accessible to LOD-consuming applications (e.g., Google Structured Data Testing Tool) and enabling the consumption of related LOD resources, thereby enhancing context for users of the collection. The results described here are incomplete but encouraging, suggesting user benefits and illuminating new workflows to explore and develop.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {309–310},
numpages = {2},
keywords = {legacy metadata transformation, schema.org, digital library special collections, RDF metadata, linked open data},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200388,
author = {Griffiths, Olivia and Cole, Nicholas and Abdul-Rahman, Alfie},
title = {Modeling Complex Negotiations: The Quill Project},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This is a poster in support of our long paper, "Quill: A Framework for Constructing Negotiated Texts, with a Case Study on the US Constitutional Convention of 1787" [1]. The paper describes a new approach to the presentation of records relating to formal negotiations and the texts that they create. It describes the architecture of a model, platform, and web-interface (https://www.quillproject.net) that can be used by domain-experts to convert the records typical of formal negotiations in to a model of decision-making (with minimal training). This model has implications for both research and teaching, by allowing for better qualitative and quantitative analysis of negotiations.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {311–312},
numpages = {2},
keywords = {user interfaces, negotiated texts, humanities, data exploration},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200389,
author = {Beel, Joeran and Aizawa, Akiko and Breitinger, Corinna and Gipp, Bela},
title = {Mr. DLib: Recommendations-as-a-Service (RaaS) for Academia},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Only few digital libraries and reference managers offer recommender systems, although such systems could assist users facing information overload. In this paper, we introduce Mr. DLib's recommendations-as-a-service, which allows third parties to easily integrate a recommender system into their products. We explain the recommender approaches implemented in Mr. DLib (content-based filtering among others), and present details on 57 million recommendations, which Mr. DLib delivered to its partner GESIS Sowiport. Finally, we outline our plans for future development, including integration into JabRef, establishing a living lab, and providing personalized recommendations.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {313–314},
numpages = {2},
keywords = {digital library, API, reference management software, recommender system, RaaS, web service, recommendation-as-a-service},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200390,
author = {Yeh, Lo-Yao and Lu, Peggy Joy and Hu, Jen-Wei},
title = {NCHC Blockchain Construction Platform (NBCP): Rapidly Constructing Blockchain Nodes around Taiwan},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Blockchain is an emerging technology that can be utilized to break ground for secure provenance through the use of a centralized-based application. The method of distributed ledger can serve as the trust machine mechanism to prevent the malicious attacks. This paper introduces a novel service to complement some open-source blockchain protocols. A vital point of adopting blockchain technology is to encourage more and more parties to join for higher security and better efficiency. National Center for High-performance Computing (NCHC) has initiated a service, named NBCP, for rapidly constructing multiple blockchain nodes around Taiwan. With several pros, users can easily construct private blockchain nodes, catering to the need of developing various prototypes of blockchain-based applications.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {315–316},
numpages = {2},
keywords = {docker swarm, security, blockchain, smart contract, fintech},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200391,
author = {Rath, Manasa and Wang, Peiling},
title = {Open Peer Review in the Era of Open Science: A Pilot Study of Researchers' Perceptions},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {The quality of Open Access journals is an important issue in developing digital library collections. Traditional criteria, such as peer review, may not be able to identify the quality of peer-reviewed publications. Open peer review (OPR) has emerged as an innovation of open science. OPR is still at an early development stage with various models, from revealing identities of the author and the reviewer to publishing the entire peer review history of the accepted paper. This pilot study utilizes qualitative interviews to investigate researchers' perceptions and attitudes towards the emerging publishing model. Through semi-structured interviews with seven researchers, this pilot study reports on the thoughts and opinions of the interviewees towards OPR and proposes further studies on the potential of OPR in scientific publishing.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {317–318},
numpages = {2},
keywords = {open science, open peer review, OPR, scientific publishing},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200392,
author = {Michels, Christopher and Fayzrakhmanov, Ruslan R. and Ley, Michael and Sallinger, Emanuel and Schenkel, Ralf},
title = {OXpath-Based Data Acquisition for Dblp},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {We demonstrate how the contemporary problems of data acquisition for dblp can be tackled with OXPath. It enables web data extraction and wrapper maintenance for heterogeneous data sources on a simple declarative level. Its features render it a feasible instrument to retrieve the varying and changing web representations of the prototypical substructures in the bibliographic domain.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {319–320},
numpages = {2},
keywords = {OXpath, digital libraries, bibliography, web data extraction, dblp},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200393,
author = {Bangert, Daniel and Frances, Maude},
title = {PIDs to Support Discovery and Citation: Persistent Identifier Service Design and Delivery at UNSW Library},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Persistent identifiers are a key technology in enabling access and interoperability between systems involved in scholarly communication. This paper discusses persistent identifier strategies and services implemented at UNSW Library for institutional and faculty-based digital repositories. In particular, the paper describes the UNSW DOI Service, which is an application that allows researchers to request or mint DOIs. The service has been designed to meet multiple use cases, and reuses descriptive metadata to make the process of getting a DOI as simple and efficient as possible.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {321–322},
numpages = {2},
keywords = {persistent identifiers, repositories, open researcher and contributor ID (ORCID), metadata, digital object identifiers (DOI), scholarly communication},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200394,
author = {Whyte, Jess},
title = {Preservation Planning and Workflows for Digital Holdings at the Thomas Fisher Rare Book Library},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {The goal of this practice-based work is to share experiences and findings with other digital preservation practitioners. The Thomas Fisher Rare Book Library Digital Preservation Pilot is a collaborative project involving the Thomas Fisher Rare Book Library, Information Technology Services at University of Toronto Libraries, and the TALint internship program at the school's Faculty of Information. Guidance was also provided by the Digital Curation Institute at the University of Toronto. The purpose of the project was to evaluate the extent of born-digital content at risk in the Fisher's collections, develop a workflow for accessioning, and establish a baseline level of preservation on the content. The following is an overview of that process, results, challenges, and recommendations for next steps.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {323–325},
numpages = {3},
keywords = {fiwalk, rare books, workflow, manuscripts, hfs2dfxml, digital curation, bagger, bitcurator, md5deep, digital preservation, forensic imaging},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200395,
author = {Kouper, Inna and Suriarachchi, Isuru and Luo, Yu and Plale, Beth},
title = {Provenance Enriched PID Kernel Information as OAI-ORE Map Replacement for SEAD Research Objects},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {PIDs and PID Kernel Information, activities of the Research Data Alliance, have the potential to expand the utility and benefit of data provenance. The poster describes such expansion and outlines a study of the trade-offs of replacing the Research Object (RO) and OAI-ORE map solution of the SEAD publishing services with the PID Kernel Information approach.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {326–327},
numpages = {2},
keywords = {data publishing, persistent identifier (PID), data provenance},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200396,
author = {Wu, Dan and Bi, Renmin},
title = {Query Reformulation Patterns in Cross-Device OPAC Search},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This paper studies the query reformulation patterns in different cross-device conditions by mining the transaction log of an OPAC. The results show that PC-PC switching is the most important form of cross device and the cross-task pattern is the main query reformulation pattern. It can be predicted by time interval and other features between the pre and post switch query.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {328–329},
numpages = {2},
keywords = {library OPAC, query reformulation pattern, cross-device search},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200397,
author = {Cunningham, Sally Jo and Rogers, Bill and Kim, Jane},
title = {Taking a Book off the Shelf in a Virtual Library},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {We present the results of a small-scale study in which participants interacted with a physical book. Their book selection and book opening gestures provide design insights for the interface to a virtual reality library.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {330–331},
numpages = {2},
keywords = {book selection, human computer interaction, digital library, interactive retrieval},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200398,
author = {Sid\`{e}re, Nicolas and Suire, Cyrille and Coustaty, Micka\"{e}l and Chazalon, Joseph and Burie, Jean-Christophe and Ogier, Jean-Marc},
title = {Touchdoc: A Tool to Bridge the Gap between Physical and Digital Libraries},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {In this paper, we explore the concept of augmented document and present a new user experience to digitize a document, modify its layout and edit its content by designing specific interfaces on multi-touch devices and using advanced techniques in document analysis. This framework exploits image processing tools to facilitate manipulations that are natural considering paper documents and complex in their digital versions. In addition, we open discussions on bridging the gap between physical and digital libraries by improving user experience with the use of this platform.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {332–333},
numpages = {2},
keywords = {touch and gestural input, augmented document, user experience, document analysis and recognition},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200399,
author = {Barry, Matthew and Sifton, Daniel},
title = {Towards a Cross-Canadian Digital Library Platform},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {OurDigitalWorld has offered a province-wide digital heritage search portal at OurOntario.ca since 2007, and currently indexes digital objects from over 250 GLAM (galleries, libraries, archives, museums) organizations from across Ontario.The British Columbia Library Associations Provincial Digital Library initiative is laying the foundation for a new provincial digital library in British Columbia.As a collaborative effort between the two organizations we have been exploring Supplejack---a metadata ingestion tool created by Digital New Zealand. Using this tool, we have created a prototype for a metadata discovery layer that can easily ingest metadata from a diverse range of institutions, and provide access to that information through search portal that can potentially be deployed at a national, provincial/territorial, or regional level, allowing for fast and efficient collaboration.Our demonstration will walk users through our functional search engine prototype backed by Supplejack, which has been loaded with sample metadata from a variety of organizations in Ontario and British Columbia with diverse metadata schema.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {334–335},
numpages = {2},
keywords = {digitalnz, ourdigitalworld, metadata, british columbia library association, supplejack, digital library, ingest, bcla, search portal, odw, digital New Zealand},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200400,
author = {Fafalios, Pavlos and Kasturia, Vaibhav and Nejdl, Wolfgang},
title = {Towards a Ranking Model for Semantic Layers over Digital Archives},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Archived collections of documents (like newspaper archives) serve as important information sources for historians, journalists, sociologists and other interested parties. Semantic Layers over such digital archives allow describing and publishing metadata and semantic information about the archived documents in a standard format (RDF), which in turn can be queried through a structured query language (e.g., SPARQL). This enables to run advanced queries by combining metadata of the documents (like publication date) and content-based semantic information (like entities mentioned in the documents). However, the results returned by structured queries can be numerous and also they all equally match the query. Thus, there is the need to rank these results in order to promote the most important ones. In this paper, we focus on this problem and propose a ranking model that considers and combines: i) the relativeness of documents to entities, ii) the timeliness of documents, and iii) the relations among the entities.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {336–337},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200401,
author = {Randles, Bernadette M. and Pasquetto, Irene V. and Golshan, Milena S. and Borgman, Christine L.},
title = {Using the Jupyter Notebook as a Tool for Open Science: An Empirical Study},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {As scientific work becomes more computational and data-intensive, research processes and results become more difficult to interpret and reproduce. In this poster, we show how the Jupyter notebook, a tool originally designed as a free version of Mathematica notebooks, has evolved to become a robust tool for scientists to share code, associated computation, and documentation.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {338–339},
numpages = {2},
keywords = {open science, reproducibility, data sharing and reuse, open data, jupyter notebooks},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200402,
author = {Berlin, John A. and Kelly, Mat and Nelson, Michael L. and Weigle, Michele C.},
title = {WAIL: Collection-Based Personal Web Archiving},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Web Archiving Integration Layer (WAIL) is a desktop application written in Python that integrates Heritrix and OpenWayback. In this work we recreate and extend WAIL from the ground up to facilitate collection-based personal Web archiving. Our new iteration of the software, WAIL-Electron, leverages native Web technologies (e.g., JavaScript, Chromium) using Electron to open new potential for Web archiving by individuals in a stand-alone cross-platform native application. By replacing OpenWayback with PyWb, we provide a novel means for personal Web archivists to curate collections of their captures from their own personal computer rather than relying on an external archival Web service. As extended features we also provide the ability for a user to monitor and automatically archive Twitter users' feeds, even those requiring authentication, as well as provide a reference implementation for integrating a browser-based preservation tool into an OS native application.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {340–341},
numpages = {2},
keywords = {browser-based preservation, web archive collections, personal web archiving},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200404,
author = {Fox, Edward A.},
title = {Introduction to Digital Libraries},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This tutorial is a thorough and deep introduction to the Digital Libraries (DL) field, providing a firm foundation: covering key concepts and terminology, as well as services, systems, technologies, methods, standards, projects, issues, and practices. It introduces and builds upon a firm theoretical foundation (starting with the '5S' set of intuitive aspects: Streams, Structures, Spaces, Scenarios, Societies), giving careful definitions and explanations of all the key parts of a 'minimal digital library', and expanding from that basis to cover key DL issues. Illustrations come from a set of case studies, including from multiple current projects. Attendees will be exposed to four Morgan and Claypool books that elaborate on 5S, published 2012--2014. Complementing the coverage of '5S' will be an overview of key aspects of the DELOS Reference Model and DL.org activities. Further, use of a Hadoop cluster supporting big data DLs will be described.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {342–343},
numpages = {2},
keywords = {scenarios, structures, spaces, big data, societies, streams, 5S},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200405,
author = {Karadkar, Unmil P. and Altman, Audrey and Breedlove, Mark},
title = {Introduction to the Digital Public Library of America API},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {The Digital Public Library of America (DPLA) provides access to over 15 million objects from libraries, museums, and archives. In addition to serving as an open portal for cultural heritage, literature, art, and scientific materials, the DPLA provides access to extensive metadata related to these materials via an openly available, RESTful application programming interface (API). The open API enables third party developers to create targeted applications that enable new and transformative uses of the items indexed by the DPLA. This half day tutorial will introduce participants to the DPLA's data model, describe the API, explain how to retrieve data using the API, and how to work with the retrieved data using freely available software using both interactive and programmatic techniques.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {344–345},
numpages = {2},
keywords = {digital public library of America, DPLA API, RESTful API},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200406,
author = {Saggion, Horacio and Ronzano, Francesco},
title = {Scholarly Data Mining: Making Sense of Scientific Literature},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {During the last decade the amount of scientific information available on-line increased at an unprecedented rate and this situation is unlikely to change. As a consequence, nowadays researchers are overwhelmed by an enormous and continuously growing number of publications to consider when they perform research activities like the exploration of advances in specific topics, peer reviewing, writing and evaluation of proposals. Natural Language Processing technology plays a key role in enabling intelligent access to the content of scientific publications. By mining the contents of scientific papers, for example, rich scientific knowledge bases can be built, thus supporting more effective information discovery and question answering approaches. Moreover, text summarization technology can help condense long papers to their essential contents so as to speed up the selection of scientific articles of interest or to assist in the manual or automatic generation of state of the art reports. Paraphrase and textual entailment techniques can contribute to the identification of relations across different scientific textual sources, thus, for instance, identifying implicit links between publications. This tutorial provides an overview of approaches to the extraction of knowledge from scientific literature, including the in-depth analysis of the structure of the scientific articles, their semantic interpretation, content extraction, summarization, and visualization.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {346–347},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200408,
author = {Allen, Robert B.},
title = {Rich Semantics and Direct Representation for Digital Collections},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {It is now possible to envision the close integration of rich knowledge structures and knowledgebases with digital libraries. Yet, there are many challenges to the implementation of this vision. Chief among these is finding comprehensive and rigorous, but also flexible, representations. Such representations need to go beyond semantics strictly construed to include discourse, the evolution of knowledge, and support for alternate explanations. In this endeavor there are many traditions to draw from such as LIS, linguistics, programming languages, philosophy, jurisprudence, sociology, and systems analysis. While the most obvious application is to develop highly-structured scientific research reports, rich semantic information organization could be applied to areas including law, history, and biography. We propose a community-wide exploration of these issues and the development of a new generation of digital libraries.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {348–349},
numpages = {2},
keywords = {highly-structured repositories, causation, models, discourse, history, events, frames, scholarly resources, science, systems, programming languages, ontologies},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200409,
author = {Trandab\u{a}\c{t}, Diana and G\^{\i}fu, Daniela},
title = {Social Media and the Web of Linked Data},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Written texts have perhaps never been so widely used as they are in today's social media context, with people constantly writing, sharing, commenting, getting involved. At the same time, Linked Data is emerging as an increasingly important topic, and research in this area has resulted in massive amounts of structured linguistic data. In this climate, we intend to analyze how linked data can help to structure and extract meaning from social media's short, informal and context dependent texts, with an emphasis on real-life applications.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {350–351},
numpages = {2},
keywords = {semantic web, linked data, social media},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200410,
author = {Fox, Edward A. and Xie, Zhiwu and Klein, Martin},
title = {Web Archiving and Digital Libraries (WADL)},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This workshop will explore integration of Web archiving and digital libraries, so the complete life cycle involved is covered: creation/authoring, uploading/publishing in the Web (2.0), (focused) crawling, indexing, exploration (searching, browsing), archiving (of events), etc. It will include particular coverage of current topics of interest, like: big data, mobile web archiving, and systems (e.g., Memento, SiteStory, Hadoop processing).},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {352–353},
numpages = {2},
keywords = {web archiving, internet archive},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.5555/3200334.3200412,
author = {(moderator), Deborah Maron and Berry, Dorothy and Payton, Fay Cobb and Lakin, Samantha and White, Erin},
title = {"Can We Really Show This"? Ethics, Representation and Social Justice in Sensitive Digital Space},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This panel addresses ethical issues of curators who work with contentious, uncomfortable, or "sensitive" content relating to marginalized populations and underrepresented histories.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {354–355},
numpages = {2},
keywords = {marginalized populations, digital libraries, digital archives},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

