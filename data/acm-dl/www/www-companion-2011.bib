@inproceedings{10.1145/1963192.1963194,
author = {Abbassi, Zeinab and Heidari, Hoda},
title = {Toward Optimal Vaccination Strategies for Probabilistic Models},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963194},
doi = {10.1145/1963192.1963194},
abstract = {Epidemic outbreaks such as the recent H1N1 influenza show how susceptible large communities are toward the spread of such outbreaks. The occurrence of a widespread disease transmission raises the question of vaccination strategies that are appropriate and close to optimal. The seemingly different problem of viruses disseminating through email networks, shares a common structure with disease epidemics. While it is not possible to vaccinate every individual during a virus outbreak, due to economic and logistical constraints, fortunately, we can leverage the structure and properties of face-to-face social networks to identify individuals whose vaccination would result in a lower number of infected people.The models that have been studied so far [3, 4] assume that once an individual is infected all its adjacent individuals would be infected with probability 1. However, this assumption is not realistic. In reality, if an individual is infected by a virus, the neighboring individuals would get infected with some probability (depending on the type of the disease and the contact). This modification to the model makes the problem more challenging as the simple version is already NP-complete [3].Here we consider the following epidemiological model computationally: A number of individuals in the community get vaccinated which makes them immune to the disease. The disease then outbreaks and a number of nodes that are not vaccinated get infected at random. These nodes can transmit the infection to their friends with some probability. In this work we consider the optimization problem in which the number of nodes that get vaccinated is limited to k and our objective is to minimize the number of infected people overall. We design various algorithms that take into account the properties of social networks to select k nodes for vaccination in order to achieve the goal. We perform experiments on a real dataset of 34,546 vertices and 421,578 edges and assess their effectiveness and scalability.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {1–2},
numpages = {2},
keywords = {SIR, probabilistic model, vaccination, social networks, virus propagation},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963195,
author = {Alici, Sadiye and Altingovde, Ismail Sengor and Ozcan, Rifat and Cambazoglu, B. Barla and Ulusoy, \"{O}zg\"{u}r},
title = {Timestamp-Based Cache Invalidation for Search Engines},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963195},
doi = {10.1145/1963192.1963195},
abstract = {We propose a new mechanism to predict stale queries in the result cache of a search engine. The novelty of our approach is in the use of timestamps in staleness predictions. We show that our approach incurs very little overhead on the system while its prediction accuracy is comparable to earlier works.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {3–4},
numpages = {2},
keywords = {freshness, time-to-live, result cache, cache invalidation},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963196,
author = {Anderka, Maik and Stein, Benno and Lipka, Nedim},
title = {Towards Automatic Quality Assurance in Wikipedia},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963196},
doi = {10.1145/1963192.1963196},
abstract = {Featured articles in Wikipedia stand for high information quality, and it has been found interesting to researchers to analyze whether and how they can be distinguished from "ordinary" articles. Here we point out that article discrimination falls far short of writer support or automatic quality assurance: Featured articles are not identified, but are made. Following this motto we compile a comprehensive list of information quality flaws in Wikipedia, model them according to the latest state of the art, and devise one-class classification technology for their identification.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {5–6},
numpages = {2},
keywords = {information quality, flaw detection, Wikipedia},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963197,
author = {Barajas, Joel and Akella, Ram and Holtan, Marius and Kwon, Jaimie and Null, Brad},
title = {Measuring the Effectiveness of Display Advertising: A Time Series Approach},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963197},
doi = {10.1145/1963192.1963197},
abstract = {We develop an approach for measuring the effectiveness of online display advertising at the campaign level. We present a Kalman filtering approach to deseasonalize and estimate the percentage changes of online sales on a daily basis. For this study, we analyze 3828 campaigns for 961 products on the Advertising.com network.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {7–8},
numpages = {2},
keywords = {marketing, time series, online display advertising},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963198,
author = {Batard, Florent and Boudaoud, Karima and Riveill, Michel},
title = {A Middleware for Securing Mobile Mashups},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963198},
doi = {10.1145/1963192.1963198},
abstract = {Mashups on traditional desktop devices are a well-known source of security risks. In this paper, we examine how these risks translate to mobile mashups and identify new risks caused by mobile-specific characteristics such as access to device features or offline operation. We describe the design of SCCM, a platform independent approach to handle the various mobile mashup security risks in a consistent and systematic manner. Evaluating an SCCM implementation for Android, we find that SCCM successfully protects against common attacks such as inserting a malicious widget from the outside.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {9–10},
numpages = {2},
keywords = {secure communications, mobile widgets, mobile mashups},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963199,
author = {Bharadwaj, Rohit G. and Varma, Vasudeva},
title = {Language Independent Identification of Parallel Sentences Using Wikipedia},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963199},
doi = {10.1145/1963192.1963199},
abstract = {This paper details a novel classification based approach to identify parallel sentences between two languages in a language independent way. We substitute the required language specific resources by the richly structured multilingual content, Wikipedia. Our approach is particularly useful to extract parallel sentences for under-resourced languages like most Indian and African languages, where resources are not readily available with necessary accuracies. We extract various statistics based on the cross lingual links present in Wikipedia and use them to generate feature vectors for each sentence pair. Binary classification of each pair of sentences into parallel or non-parallel has been done using these feature vectors. We achieved a precision upto 78% which is encouraging when compared to other state-of-art approaches.These results support our hypothesis of using Wikipedia to evaluate the parallel coefficient between sentences that can be used to build bilingual dictionaries.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {11–12},
numpages = {2},
keywords = {Wikipedia, language independent, parallel sentences},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963200,
author = {Bollegala, Danushka and Matsuo, Yutaka and Ishizuka, Mitsuru},
title = {From Actors, Politicians, to CEOs: Domain Adaptation of Relational Extractors Using a Latent Relational Mapping},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963200},
doi = {10.1145/1963192.1963200},
abstract = {We propose a method to adapt an existing relation extraction system to extract new relation types with minimum supervision. Our proposed method comprises two stages: learning a lower-dimensional projection between different relations, and learning a relational classifier for the target relation type with instance sampling. We evaluate the proposed method using a dataset that contains 2000 instances for 20 different relation types. Our experimental results show that the proposed method achieves a statistically significant macro-average F-score of 62.77. Moreover, the proposed method outperforms numerous baselines and a previously proposed weakly-supervised relation extraction method.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {13–14},
numpages = {2},
keywords = {web mining, domain adaptation, relation extraction},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963201,
author = {Bonchi, Francesco and Perego, Raffaele and Silvestri, Fabrizio and Vahabi, Hossein and Venturini, Rossano},
title = {Recommendations for the Long Tail by Term-Query Graph},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963201},
doi = {10.1145/1963192.1963201},
abstract = {We define a new approach to the query recommendation problem. In particular, our main goal is to design a model enabling the generation of query suggestions also for rare and previously unseen queries. In other words we are targeting queries in the long tail. The model is based on a graph having two sets of nodes: Term nodes, and Query nodes. The graph induces a Markov chain on which a generic random walker starts from a subset of Term nodes, moves along Query nodes, and restarts (with a given probability) only from the same initial subset of Term nodes. Computing the stationary distribution of such a Markov chain is equivalent to extracting the so-called Center-piece Subgraph from the graph associated with the Markov chain itself. Given a query, we extract its terms and we set the restart subset to this term set. Therefore, we do not require a query to have been previously observed for the recommending model to be able to generate suggestions.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {15–16},
numpages = {2},
keywords = {query recommender systems, web search effectiveness},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963202,
author = {Capannini, Gabriele and Nardini, Franco Maria and Perego, Raffaele and Silvestri, Fabrizio},
title = {Efficient Diversification of Search Results Using Query Logs},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963202},
doi = {10.1145/1963192.1963202},
abstract = {We study the problem of diversifying search results by exploiting the knowledge mined from query logs. Our proposal exploits the presence of different "specializations" of queries in query logs to detect the submission of ambiguous/faceted queries, and manage them by diversifying the search results returned in order to cover the different possible interpretations of the query. We present an original formulation of the results diversification problem in terms of an objective function to be maximized that admits the finding of an optimal solution in linear time.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {17–18},
numpages = {2},
keywords = {query log analysis, search results diversification},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963203,
author = {Chakrabarti, Kaushik and Chaudhuri, Surajit and Cheng, Tao and Xin, Dong},
title = {EntityTagger: Automatically Tagging Entities with Descriptive Phrases},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963203},
doi = {10.1145/1963192.1963203},
abstract = {We consider the problem of entity tagging: given one or more named entities from a specific domain, the goal is to automatically associate descriptive phrases, referred to as etags (entity tags), to each entity. Consider a product catalog containing product names and possibly short descriptions. For a product in the catalog, say Ricoh G600 Digital Camera, we want to associate etags such as "water resistant", "rugged" and "outdoor" to it, even though its name or description does not mention those phrases. Entity tagging can enable more effective search over entities. We propose to leverage signals in web documents to perform such tagging. We develop techniques to perform such tagging in a domain independent manner while ensuring high precision and high recall.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {19–20},
numpages = {2},
keywords = {tag discovery, tag association, entity tagging},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963204,
author = {Chakrabarti, Soumen and Sane, Devshree and Ramakrishnan, Ganesh},
title = {Web-Scale Entity-Relation Search Architecture},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963204},
doi = {10.1145/1963192.1963204},
abstract = {Enabling entity search and ranking at Web-scale is fraught with many challenges: annotating the corpus with entities and types, query language design, index design, query processing logic, and answer consolidation. We describe a Web-scale entity search engine we are building to handle over a billion Web pages, over 200,000 types, over 1,500,000 entities, and hundreds of entity annotations per page. We describe the design of compressed, token span oriented indices for entity and type annotations. Our prototype demonstrates the practicality of Web-scale entity-relation search.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {21–22},
numpages = {2},
keywords = {entity-relation search, web-scale, index design},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963205,
author = {Chen, Bihuan and Peng, Xin and Yu, Yijun and Zhao, Wenyun},
title = {Survivability-Oriented Self-Tuning of Web Systems},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963205},
doi = {10.1145/1963192.1963205},
abstract = {Running in a highly uncertain and changing environment, Web systems cannot always provide full set of services with optim¬al quality, especially when the workload is high or failures in subsys-tems occur frequently. It is thus desirable to continuously maintain a high satisfaction level of the system value proposition, hereafter survivability assurance, while relaxing/sacrificing certain quality/functional requirements that are not crucial to the survival of the Web systems. In this paper, we propose a requirements-driven self-tuning method for survivability assurance of Web systems. Using a value-based feedback controller plus a requirements-oriented reasoner, our method makes both quality and functional requirements tradeoffs decisions at runtime.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {23–24},
numpages = {2},
keywords = {value, self-tuning, reasoning, survivability, requirements},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963206,
author = {Chen, Yan-Ying and Hsu, Winston H. and Liao, Hong-Yuan Mark},
title = {Learning Facial Attributes by Crowdsourcing in Social Media},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963206},
doi = {10.1145/1963192.1963206},
abstract = {Facial attributes such as gender, race, age, hair style, etc., carry rich information for locating designated persons and profiling the communities from image/video collections (e.g., surveillance videos or photo albums). For plentiful facial attributes in photos and videos, collecting costly manual annotations for training detectors is time-consuming. We propose an automatic facial attribute detection method by exploiting the great amount of weakly labelled photos in social media. Our work can (1) automatically extract training images from the semantic-consistent user groups and (2) filter out noisy training photos by multiple mid-level features (by voting). Moreover, we introduce a method to harvest less-biased negative data for preventing uneven distribution of certain attributes. The experiments show that our approach can automatically acquire training photos for facial attributes and is on par with that by manual annotations.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {25–26},
numpages = {2},
keywords = {facial attribute, crowdsourcing, social media},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963207,
author = {Cheng, Gong and Ge, Weiyi and Qu, Yuzhong},
title = {Generating Summaries for Ontology Search},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963207},
doi = {10.1145/1963192.1963207},
abstract = {This poster proposes a novel approach for generating summaries for ontology search. Following previous work, we define ontology summarization as the problem of ranking and selecting RDF sentences, for which we examine three aspects. Firstly, to assess the salience of RDF sentences in an ontology, we devise a bipartite graph model for representing the ontology and analyze random walks on this graph. Secondly, to reflect how an ontology is matched with user needs expressed via keyword queries, we incorporate query relevance into the selection of RDF sentences. Finally, to improve the unity of a summary, we optimize its cohesion in terms of the connections between constituent RDF sentences. We have implemented an online prototype system called Falcons Ontology Search.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {27–28},
numpages = {2},
keywords = {ranking, random walk, query relevance, cohesion, ontology summarization},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963208,
author = {Dai, Na and Qi, Xiaoguang and Davison, Brian D.},
title = {Enhancing Web Search with Entity Intent},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963208},
doi = {10.1145/1963192.1963208},
abstract = {Web entities, such as documents and hyperlinks, are created for different purposes, or intents. Existing intent-based retrieval methods largely focus on information seekers' intent expressed by queries, ignoring the other side of the problem: web content creators' intent. We argue that understanding why the content was created is also important. In this work, we propose to classify such intents into two broad categories: "navigational" and "informational". Then we incorporate such intents into traditional retrieval models, and show their effect on ranking performance.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {29–30},
numpages = {2},
keywords = {retrieval model, web entity intent, query intent},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963209,
author = {Dua, Nitin and Gupta, Kanika and Choudhury, Monojit and Bali, Kalika},
title = {Query Completion without Query Logs for Song Search},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963209},
doi = {10.1145/1963192.1963209},
abstract = {We describe a new method for query completion for Bollywood song search without using query logs. Since song titles in non-English languages (Hindi in our case) are mostly present as Roman transliterations of the native script, both the queries and documents have a large number of valid variations. We address this problem by using a Roman to Hindi transliteration engine coupled with appropriate ranking and implementation strategies. Out of 100 test cases, our system could generate the correct suggestion for 91 queries.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {31–32},
numpages = {2},
keywords = {transliteration, song search, query log, query completion},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963210,
author = {Ermilov, Timofey and Heino, Norman and Auer, S\"{o}ren},
title = {OntoWiki Mobile: Knowledge Management in Your Pocket},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963210},
doi = {10.1145/1963192.1963210},
abstract = {As comparatively powerful mobile computing devices become more common, mobile web applications have started gaining in popularity. Such mobile web applications as Google Mail or Calendar are already in everyday use of millions of people. Some first examples of these applications use Semantic Web technologies and information in the form of RDF (e.g. TripIt). An important feature of these applications is their ability to provide offline functionality with local updates for later synchronization with a web server. The key problem to this is the reconciliation, i.e. the problem of potentially conflicting updates from disconnected clients. In this paper we present an approach for a mobile semantic collaboration platform based on the OntoWiki framework [1]. It allows users to collect instance data and refine the structure knowledge bases on the go. A crucial part of OntoWiki Mobile is the advanced conflict resolution for RDF stores. The approach is based on the EvoPat [2] method for data evolution and ontology refactoring.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {33–34},
numpages = {2},
keywords = {replication, knowledge engineering, mobile web application, RDF},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963211,
author = {Fumarola, Fabio and Weninger, Tim and Barber, Rick and Malerba, Donato and Han, Jiawei},
title = {HyLiEn: A Hybrid Approach to General List Extraction on the Web},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963211},
doi = {10.1145/1963192.1963211},
abstract = {We consider the problem of automatically extracting general lists from the web. Existing approaches are mostly dependent upon either the underlying HTML markup or the visual structure of the Web page. We present HyLiEn an unsupervised, Hybrid approach for automatic List discovery and Extraction on the Web. It employs general assumptions about the visual rendering of lists, and the structural representation of items contained in them. We show that our method significantly outperforms existing methods.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {35–36},
numpages = {2},
keywords = {web information integration, web mining, web lists},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963212,
author = {Gao, Mingyan and Hua, Xian-Sheng and Jain, Ramesh},
title = {WonderWhat: Real-Time Event Determination from Photos},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963212},
doi = {10.1145/1963192.1963212},
abstract = {How often did you feel disappointed in a foreign country, when you had been craving for participating in authentic native events but miserably ended up with being lost in the crowd, due to ignorance of the local culture? Have you ever imagined that with merely a simple click, a tool can identify the events that are right in front of you? As a step in this direction, in this paper, we propose a system that provides users with information of the public events that they are attending by analyzing in real time their photos taken at the event, leveraging both spatio-temporal context and photo content. To fulfill the task, we designed the system to collect event information, maintain dedicated event database, build photo content model for event types, and rank the final results. Extensive experiments were conducted to prove the effectiveness of each component.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {37–38},
numpages = {2},
keywords = {event determination, context, photo, content},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963213,
author = {Ghosh, Saptarshi and Kane, Pushkar and Ganguly, Niloy},
title = {Identifying Overlapping Communities in Folksonomies or Tripartite Hypergraphs},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963213},
doi = {10.1145/1963192.1963213},
abstract = {Online folksonomies are modeled as tripartite hypergraphs, and detecting communities from such networks is a challenging and well-studied problem. However, almost every existing algorithm known to us for community detection in hypergraphs assign unique communities to nodes, whereas in reality, nodes in folksonomies belong to multiple overlapping communities e.g. users have multiple topical interests, and the same resource is often tagged with semantically different tags. In this paper, we propose an algorithm to detect overlapping communities in folksonomies by customizing a recently proposed edge-clustering algorithm (that is originally for traditional graphs) for use on hypergraphs.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {39–40},
numpages = {2},
keywords = {tripartite hypergraph, folksonomy, overlapping communities, community identification},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963214,
author = {Ghosh, Saptarshi and Korlam, Gautam and Ganguly, Niloy},
title = {Spammers' Networks within Online Social Networks: A Case-Study on Twitter},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963214},
doi = {10.1145/1963192.1963214},
abstract = {We analyze the strategies employed by contemporary spammers in Online Social Networks (OSNs) by identifying a set of spam-accounts in Twitter and monitoring their link-creation strategies. Our analysis reveals that spammers adopt intelligent 'collaborative' strategies of link-formation to avoid detection and to increase the reach of their generated spam, such as forming 'spam-farms' and creating large number of links with targeted legitimate users. The observations are verified through the analysis of a giant 'spam-farm' embedded within the Twitter OSN.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {41–42},
numpages = {2},
keywords = {online social networks, spam-farms, Twitter},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963215,
author = {Goharian, Nazli and Mengle, Saket S.R.},
title = {Networked Hierarchies for Web Directories},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963215},
doi = {10.1145/1963192.1963215},
abstract = {The hierarchical nature of existing Web directories, ontologies, and folksonomies, are known to provide meaningful information that guide users and applications. We hypothesize that such hierarchical structures provide richer information if they are further enriched by incorporating additional links besides parents, and siblings, namely, between non-sibling nodes. We call such structure a networked hierarchy. Our empirical results indicate that such a networked hierarchy introduces interesting links between nodes (non-sibling) that otherwise in a hierarchical structure are not evident.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {43–44},
numpages = {2},
keywords = {text classification, hidden information, taxonomy},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963216,
author = {Goswami, Anjan and Chittar, Naren and Sung, Chung H.},
title = {A Study on the Impact of Product Images on User Clicks for Online Shopping},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963216},
doi = {10.1145/1963192.1963216},
abstract = {In this paper we study the importance of image based features on the click-through rate (CTR) in the context of a large scale product search engine. Typically product search engines use text based features in their ranking function. We present a novel idea of using image based features, common in the photography literature, in addition to text based features. We used a stochastic gradient boosting based regression model to learn relationships between features and CTR. Our results indicate statistically significant correlations between the image features and CTR. We also see improvements to NDCG and mean standard regression.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {45–46},
numpages = {2},
keywords = {online shopping, shopping search, product images},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963217,
author = {Goyal, Amit and Lu, Wei and Lakshmanan, Laks V.S.},
title = {CELF++: Optimizing the Greedy Algorithm for Influence Maximization in Social Networks},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963217},
doi = {10.1145/1963192.1963217},
abstract = {Kempe et al. [4] (KKT) showed the problem of influence maximization is NP-hard and a simple greedy algorithm guarantees the best possible approximation factor in PTIME. However, it has two major sources of inefficiency. First, finding the expected spread of a node set is #P-hard. Second, the basic greedy algorithm is quadratic in the number of nodes. The first source is tackled by estimating the spread using Monte Carlo simulation or by using heuristics[4, 6, 2, 5, 1, 3]. Leskovec et al. proposed the CELF algorithm for tackling the second. In this work, we propose CELF++ and empirically show that it is 35-55% faster than CELF.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {47–48},
numpages = {2},
keywords = {greedy algorithm, viral marketing, influence propagation, submodularity, social networks, CELF},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963218,
author = {Graf, Sebastian and Belle, Sebastian Kay and Waldvogel, Marcel},
title = {Rolling Boles, Optimal XML Structure Integrity for Updating Operations},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963218},
doi = {10.1145/1963192.1963218},
abstract = {While multiple techniques exist to utilize the tree structure of the Extensible Markup Language(XML) regarding integrity checks, they all rely on adaptions of the Merkle Tree: All children are acting as one slice regarding the check-sum of one node with the help of an one-way hash concatenation. This results in postorder traversals regarding the (re-)computation of the integrity structure within modification operations. With our approach we perform nearly in-time updates of the entire integrity structure. We therefore equipped an XHash-based approach with an incremental hash function. This replaces postorder traversals by adapting only the incremental modifications to the check-sums of a node and its ancestors. With experimental results we prove that our approach only generates a constant overhead depending on the depth of the tree while native DOMHash implementations produce an overhead based on the depth and the number of all nodes in the tree. Consequently, our approach called Rolling Boles generates sustainable impact since it facilitates instant integrity updates in constant time.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {49–50},
numpages = {2},
keywords = {domhash, XML, xhash, XML data integrity},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963219,
author = {Gummadi, Ravi and Khulbe, Anupam and Kalavagattu, Aravind and Salvi, Sanil and Kambhampati, Subbarao},
title = {SmartInt: Using Mined Attribute Dependencies to Integrate Fragmented Web Databases},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963219},
doi = {10.1145/1963192.1963219},
abstract = {Many web databases can be seen as providing partial and overlapping information about entities in the world. To answer queries effectively, we need to integrate the information about the individual entities that are fragmented over multiple sources. At first blush this is just the inverse of traditional database normalization problem - rather than go from a universal relation to normalized tables, we want to reconstruct the universal relation given the tables (sources). The standard way of reconstructing the entities will involve joining the tables. Unfortunately, because of the autonomous and decentralized way in which the sources are populated, they often do not have Primary Key - Foreign Key relations. While tables do share attributes, direct joins over these shared attributes can result in reconstruction of many spurious entities thus seriously compromising precision. We present a unified approach that supports intelligent retrieval over fragmented web databases by mining and using inter-table dependencies. Experiments with the prototype implementation, SmartInt, show that its retrieval strikes a good balance between precision and recall.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {51–52},
numpages = {2},
keywords = {web databases, entity completion, loss of pk-fk},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963220,
author = {Gupta, Manish and Sun, Yizhou and Han, Jiawei},
title = {Trust Analysis with Clustering},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963220},
doi = {10.1145/1963192.1963220},
abstract = {Web provides rich information about a variety of objects. Trustability is a major concern on the web. Truth establishment is an important task so as to provide the right information to the user from the most trustworthy source. Trustworthiness of information provider and the confidence of the facts it provides are inter-dependent on each other and hence can be expressed iteratively in terms of each other. However, a single information provider may not be the most trustworthy for all kinds of information. Every information provider has its own area of competence where it can perform better than others. We derive a model that can evaluate trustability on objects and information providers based on clusters (groups). We propose a method which groups the set of objects for which similar set of providers provide "good" facts, and provides better accuracy in addition to high quality object clusters.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {53–54},
numpages = {2},
keywords = {clustering, trust, fact finding},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963221,
author = {Heatherly, Raymond D. and Kantarcioglu, Murat},
title = {Automatic Sanitization of Social Network Data to Prevent Inference Attacks},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963221},
doi = {10.1145/1963192.1963221},
abstract = {As the privacy concerns related to information release in social networks become a more mainstream concern, data owners will need to utilize a variety of privacy-preserving methods of examining this data. Here, we propose a method of data generalization that applies to social networks and present some initial findings for the utility/privacy tradeoff required for its use.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {55–56},
numpages = {2},
keywords = {data sanitization, social network privacy},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963222,
author = {Hong, Liangjie and Dan, Ovidiu and Davison, Brian D.},
title = {Predicting Popular Messages in Twitter},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963222},
doi = {10.1145/1963192.1963222},
abstract = {Social network services have become a viable source of information for users. In Twitter, information deemed important by the community propagates through retweets. Studying the characteristics of such popular messages is important for a number of tasks, such as breaking news detection, personalized message recommendation, viral marketing and others. This paper investigates the problem of predicting the popularity of messages as measured by the number of future retweets and sheds some light on what kinds of factors influence information propagation in Twitter. We formulate the task into a classification problem and study two of its variants by investigating a wide spectrum of features based on the content of the messages, temporal information, metadata of messages and users, as well as structural properties of the users' social graph on a large scale dataset. We show that our method can successfully predict messages which will attract thousands of retweets with good performance.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {57–58},
numpages = {2},
keywords = {classification, information diffusion, microblogs, social media},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963223,
author = {Hua, Guichun and Zhang, Min and Liu, Yiqun and Ma, Shaoping and Ru, Liyun},
title = {Automatically Generating Labels Based on Unified Click Model},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963223},
doi = {10.1145/1963192.1963223},
abstract = {Ground truth labels are one of the most important parts in many test collections for information retrieval. Each label, depicting the relevance between a query-document pair, is usually judged by a human, and this process is time-consuming and labor-intensive. Automatically Generating labels from click-through data has attracted increasing attention. In this paper, we propose a Unified Click Model to predict the multi-level labels, which aims at comprehensively considering the advantages of the Position Models and Cascade Models. Experiments show that the proposed click model outperforms the existing click models in predicting the multi-level labels, and could replace the labels judged by humans for test collections.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {59–60},
numpages = {2},
keywords = {learning to rank, ranking SVM, click model},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963224,
author = {Huang, Bojun and Xia, Zenglin},
title = {Allocating Inverted Index into Flash Memory for Search Engines},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963224},
doi = {10.1145/1963192.1963224},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {61–62},
numpages = {2},
keywords = {caching, inverted index, flash memory},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963225,
author = {Jain, Alpa and Pennacchiotti, Marco},
title = {Domain-Independent Entity Extraction from Web Search Query Logs},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963225},
doi = {10.1145/1963192.1963225},
abstract = {Query logs of a Web search engine have been increasingly used as a vital source for data mining. This paper presents a study on large-scale domain-independent entity extraction from search query logs. We present a completely unsupervised method to extract entities by applying pattern-based heuristics and statistical measures. We compare against existing techniques that use Web documents as well as search logs, and show that we improve over the state of the art. We also provide an in-depth qualitative analysis outlining differences and commonalities between these methods.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {63–64},
numpages = {2},
keywords = {entity extraction, data mining, query logs},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963226,
author = {Kahng, Minsuk and Lee, Sangkeun and Lee, Sang-goo},
title = {Ranking in Context-Aware Recommender Systems},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963226},
doi = {10.1145/1963192.1963226},
abstract = {As context is acknowledged as an important factor that can affect users' preferences, many researchers have worked on improving the quality of recommender systems by utilizing users' context. However, incorporating context into recommender systems is not a simple task in that context can influence users' item preferences in various ways depending on the application. In this paper, we propose a novel method for context-aware recommendation, which incorporates several features into the ranking model. By decomposing a query, we propose several types of ranking features that reflect various contextual effects. In addition, we present a retrieval model for using these features, and adopt a learning to rank framework for combining proposed features. We evaluate our approach on two real-world datasets, and the experimental results show that our approach outperforms several baseline methods.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {65–66},
numpages = {2},
keywords = {recommender systems, context-aware recommender systems, usage log, learning to rank, collaborative filtering, ranking in information retrieval, context},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963227,
author = {Kang, Changsung and Vadrevu, Srinivas and Zhang, Ruiqiang and Zwol, Roelof van and Pueyo, Lluis Garcia and Torzec, Nicolas and He, Jianzhang and Chang, Yi},
title = {Ranking Related Entities for Web Search Queries},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963227},
doi = {10.1145/1963192.1963227},
abstract = {Entity ranking is a recent paradigm that refers to retrieving and ranking related objects and entities from different structured sources in various scenarios. Entities typically have associated categories and relationships with other entities. In this work, we present an extensive analysis of Web-scale entity ranking, based on machine learned ranking models using an ensemble of pairwise preference models. Our proposed system for entity ranking uses structured knowledge bases, entity relationship graphs and user data to derive useful features to facilitate semantic search with entities directly within the learning to rank framework. The experimental results are validated on a large-scale graph containing millions of entities and hundreds of millions of entity relationships. We show that our proposed ranking solution clearly improves a simple user behavior based ranking model.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {67–68},
numpages = {2},
keywords = {structured data, entity ranking, semantic search, object ranking},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963228,
author = {Kawakubo, Hidetoshi and Yanai, Keiji},
title = {GeoVisualRank: A Ranking Method of Geotagged Imagesconsidering Visual Similarity and Geo-Location Proximity},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963228},
doi = {10.1145/1963192.1963228},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {69–70},
numpages = {2},
keywords = {pagerank, geotagged images, visualrank},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963229,
author = {Kil, Hyunyoung and Nam, Wonhong},
title = {Anytime Algorithm for QoS Web Service Composition},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963229},
doi = {10.1145/1963192.1963229},
abstract = {The QoS-aware web service composition (WSC) problem aims at the automatic construction of a composite web service with the optimal accumulated QoS value. It is, however, intractable to solve the QoS-aware WSC problem for large scale instances, since the problem corresponds to a global optimization problem. In this paper, we propose a novel anytime algorithm for the QoS-aware WSC problem to identify composite web services with high quality much earlier than an optimal algorithm and the beam stack search [3].},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {71–72},
numpages = {2},
keywords = {web service composition, quality of services (QoS), anytime algorithm},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963230,
author = {Lakkaraju, Himabindu and Rai, Angshu and Merugu, Srujana},
title = {Smart News Feeds for Social Networks Using Scalable Joint Latent Factor Models},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963230},
doi = {10.1145/1963192.1963230},
abstract = {Social networks such as Facebook and Twitter offer a huge opportunity to tap the collective wisdom (both published and yet to be published) of all the participating users in order to address the information needs of individual users in a highly contextualized fashion using rich user-specific information. Realizing this opportunity, however, requires addressing two key limitations of current social networks: (a) difficulty in discovering relevant content beyond the immediate neighborhood, (b) lack of support for information filtering based on semantics, content source and linkage.We propose a scalable framework for constructing smart news feeds based on predicting user-post relevance using multiple signals such as text content and attributes of users and posts, and various user-user, post-post and user-post relations (e.g. friend, comment, author relations). Our solution comprises of two steps where the first step ensures scalability by selecting a small set of user-post dyads with potentially interesting interactions using inverted feature indexes. The second step models the interactions associated with the selected dyads via a joint latent factor model, which assumes that the user/post content and relationships can be effectively captured by a common latent representation of the users and posts. Experiments on a Facebook dataset using the proposed model lead to improved precision/recall on relevant posts indicating potential for constructing superior quality news feeds.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {73–74},
numpages = {2},
keywords = {social media, latent factor models, news feeds},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963231,
author = {Li, Cheng-Te and Lin, Shou-De and Shan, Man-Kwan},
title = {Finding Influential Mediators in Social Networks},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963231},
doi = {10.1145/1963192.1963231},
abstract = {Given a social network, who are the key players controlling the bottlenecks of influence propagation if some persons would like to activate specific individuals? In this paper, we tackle the problem of selecting a set of k mediator nodes as the influential gateways whose existence determines the activation probabilities of targeted nodes from some given seed nodes. We formally define the k-Mediators problem. To have an effective and efficient solution, we propose a three-step greedy method by considering the probabilistic influence and the structural connectivity on the pathways from sources to targets. To the best of our knowledge, this is the first work to consider the k-Mediators problem in networks. Experiments on the DBLP co-authorship graph show the effectiveness and efficiency of the proposed method.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {75–76},
numpages = {2},
keywords = {influential mediator, viral marketing, social networks},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963232,
author = {Li, Decong and Li, Sujian},
title = {Hypergraph-Based Inductive Learning for Generating Implicit Key Phrases},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963232},
doi = {10.1145/1963192.1963232},
abstract = {This paper presents a novel approach to generate implicit key phrases which are ignored in previous researches. Recent researches prefer to extract key phrases with semi-supervised transductive learning methods, which avoid the problem of training data. In this paper, based on a transductive learning method, we formulate the phrases in the document as a hypergraph and expand the hypergraph to include implicit phrases, which are ranked by an inductive learning approach. The highest ranked phrases are seen as implicit key phrases, and experimental results demonstrate the satisfactory performance of this approach.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {77–78},
numpages = {2},
keywords = {inductive semi-supervised learning, implicit key phrase, key phrase generation, transductive learning, hypergraph},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963233,
author = {Liu, Yiming and Yang, Rui and Wilde, Erik},
title = {Open and Decentralized Access across Location-Based Services},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963233},
doi = {10.1145/1963192.1963233},
abstract = {Users now interact with multiple Location-Based Services (LBS) through a myriad set of location-aware devices and interfaces. However, current LBS tend to be centralized silos with ad-hoc APIs, which limits potential for information sharing and reuse. Further, LBS subscriptions and user experiences are not easily portable across devices. We propose a general architecture for providing open and decentralized access to LBS, based on Tiled Feeds - a RESTful protocol for access and interactions with LBS using feeds, and Feed Subscription Management (FSM) - a generalized feed-based service management protocol. We describe two client designs, and demonstrate how they enable standardized access to LBS services, promote information sharing and mashup creation, and offer service management across various types of location-enabled devices.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {79–80},
numpages = {2},
keywords = {atom, location-based services, RSS, feeds},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963234,
author = {Lu, Dongyuan and Li, Qiudan},
title = {Personalized Search on Flickr Based on Searcher's Preference Prediction},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963234},
doi = {10.1145/1963192.1963234},
abstract = {In this paper, we propose a personalized search model to assist users in obtaining interested photos on Flickr, which exploits the favorite marks of the searcher's friends to predict the searcher's preference on the returned photos. The proposed model utilizes a co-clustering method to extract latent interest dimensions from users' implicit interests, and employs a discriminative learning method to predict searcher's preference on the returned photos. Preliminary experiments demonstrate the improvement of the proposed model compared to existing one-fit-all methods and a user-based collaborative filtering method.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {81–82},
numpages = {2},
keywords = {discriminative learning, personalized search, latent interest dimension, preference prediction},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963235,
author = {Mahajan, Dhruv Kumar and Sellamanickam, Sundararajan and Sanyal, Subhajit and Madaan, Amit},
title = {A Classification Based Framework for Concept Summarization},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963235},
doi = {10.1145/1963192.1963235},
abstract = {In this paper we propose a novel classification based framework for finding a small number of images summarizing a concept. Our method exploits metadata information available with the images to get the category information using Latent Dirichlet Allocation. We modify the import vector machine formulation based on kernel logistic regression to solve the underlying classification problem. We show that the import vectors provide a good summary satisfying important properties such as coverage, diversity and balance. Furthermore, the framework allows users to specify desired distributions over category, time etc, that a summary should satisfy. Experimental results show that the proposed method performs better than state-of-the-art summarization methods in terms of satisfying important visual and semantic properties.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {83–84},
numpages = {2},
keywords = {concept and image summarization},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963236,
author = {Mangalampalli, Ashish and Ratnaparkhi, Adwait and Hatch, Andrew O. and Bagherjeiran, Abraham and Parekh, Rajesh and Pudi, Vikram},
title = {A Feature-Pair-Based Associative Classification Approach to Look-Alike Modeling for Conversion-Oriented User-Targeting in Tail Campaigns},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963236},
doi = {10.1145/1963192.1963236},
abstract = {Online advertising offers significantly finer granularity, which has been leveraged in state-of-the-art targeting methods, like Behavioral Targeting (BT). Such methods have been further complemented by recent work in Look-alike Modeling (LAM) which helps in creating models which are customized according to each advertiser's requirements and each campaign's characteristics, and which show ads to users who are most likely to convert on them, not just click them. In Look-alike Modeling given data about converters and nonconverters, obtained from advertisers, we would like to train models automatically for each ad campaign. Such custom models would help target more users who are similar to the set of converters the advertiser provides. The advertisers get more freedom to define their preferred sets of users which should be used as a basis to build custom targeting models.In behavioral data, the number of conversions (positive class) per campaign is very small (conversions per impression for the advertisers in our data set are much less than 10-4), giving rise to a highly skewed training dataset, which has most records pertaining to the negative class. Campaigns with very few conversions are called as tail campaigns, and those with many conversions are called head campaigns. Creation of Look-alike Models for tail campaigns is very challenging and tricky using popular classifiers like Linear SVM and GBDT, because of the very few number of positive class examples such campaigns contain. In this paper, we present an Associative Classification (AC) approach to LAM for tail campaigns. Pairs of features are used to derive rules to build a Rule-based Associative Classifier, with the rules being sorted by frequency-weighted log-likelihood ratio (F-LLR). The top k rules, sorted by F-LLR, are then applied to any test record to score it. Individual features can also form rules by themselves, though the number of such rules in the top k rules and the whole rule-set is very small. Our algorithm is based on Hadoop, and is thus very efficient in terms of speed.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {85–86},
numpages = {2},
keywords = {user targeting, monetization, look-alike modeling, associative classification},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963237,
author = {Maniu, Silviu and Abdessalem, Talel and Cautis, Bogdan},
title = {Casting a Web of Trust over Wikipedia: An Interaction-Based Approach},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963237},
doi = {10.1145/1963192.1963237},
abstract = {We report in this short paper results on inferring a signed network (a "web of trust") from user interactions. On the Wikipedia network of contributors, from a collection of articles in the politics domain and their revision history, we investigate mechanisms by which relationships between contributors - in the form of signed directed links - can be inferred from their interactions. Our preliminary study provides valuable insight into principles underlying a signed network of Wikipedia contributors that is captured by social interaction. We look into whether this network (called hereafter WikiSigned) represents indeed a plausible configuration of link signs. We assess connections to social theories such as structural balance and status, which have already been considered in online communities. We also evaluate on this network the accuracy of a learned predictor for edge signs. Equipped with learning techniques that have been applied before on three explicit signed networks, we obtain good accuracy over the WikiSigned edges. Moreover, by cross training-testing we obtain strong evidence that our network does reveal an implicit signed configuration and that it has similar characteristics to the explicit ones, even though it is inferred from interactions. We also report on an application of the resulting signed network that impacts Wikipedia readers, namely the classification of Wikipedia articles by importance and quality.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {87–88},
numpages = {2},
keywords = {social applications, Wikipedia, online communities, web of trust, signed networks},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963238,
author = {Matsubayashi, Tatsushi and Ishiguro, Katsuhiko},
title = {Mobile Topigraphy: Large-Scale Tag Cloud Visualization for Mobiles},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963238},
doi = {10.1145/1963192.1963238},
abstract = {We introduce a new mobile topigraphy system that uses the contour map metaphor to display large-scale tag clouds. We introduce the technical issues for topigraphy, and recent requirements for and developments in mobile interfaces. We also present some applications for our mobile topigraphy system and describe the assessment on two initial applications.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {89–90},
numpages = {2},
keywords = {tag cloud, Wikipedia, graph drawing, Android OS},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963239,
author = {Mishra, Nikita and Saha Roy, Rishiraj and Ganguly, Niloy and Laxman, Srivatsan and Choudhury, Monojit},
title = {Unsupervised Query Segmentation Using Only Query Logs},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963239},
doi = {10.1145/1963192.1963239},
abstract = {We introduce an unsupervised query segmentation scheme that uses query logs as the only resource and can effectively capture the structural units in queries. We believe that Web search queries have a unique syntactic structure which is distinct from that of English or a bag-of-words model. The segments discovered by our scheme help understand this underlying grammatical structure. We apply a statistical model based on Hoeffding's Inequality to mine significant word n-grams from queries and subsequently use them for segmenting the queries. Evaluation against manually segmented queries shows that this technique can detect rare units that are missed by our Pointwise Mutual Information (PMI) baseline.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {91–92},
numpages = {2},
keywords = {unsupervised query segmentation, query grammar, hoeffding's inequality, query structure},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963240,
author = {Mukherjee, Arjun and Liu, Bing and Wang, Junhui and Glance, Natalie and Jindal, Nitin},
title = {Detecting Group Review Spam},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963240},
doi = {10.1145/1963192.1963240},
abstract = {It is well-known that many online reviews are not written by genuine users of products, but by spammers who write fake reviews to promote or demote some target products. Although some existing works have been done to detect fake reviews and individual spammers, to our knowledge, no work has been done on detecting spammer groups. This paper focuses on this task and proposes an effective technique to detect such groups.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {93–94},
numpages = {2},
keywords = {review spam, adversarial data mining, spammer group detection},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963241,
author = {Nakayama, Kotaro and Matsuo, Yutaka},
title = {A Self Organizing Document Map Algorithm for Large Scale Hyperlinked Data Inspired by Neuronal Migration},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963241},
doi = {10.1145/1963192.1963241},
abstract = {Web document clustering is one of the research topics that is being pursued continuously due to the large variety of applications. Since Web documents usually have variety and diversity in terms of domains, content and quality, one of the technical difficulties is to find a reasonable number and size of clusters. In this research, we pay attention to SOMs (Self Organizing Maps) because of their capability of visualized clustering that helps users to investigate characteristics of data in detail. The SOM is widely known as a "scalable" algorithm because of its capability to handle large numbers of records. However, it is effective only when the vectors are small and dense. Although several research efforts on making the SOM scalable have been conducted, technical issues on scalability and performance for sparse high-dimensional data such as hyperlinked documents still remain. In this paper, we introduce MIGSOM, an SOM algorithm inspired by a recent discovery on neuronal migration. The two major advantages of MIGSOM are its scalability for sparse high-dimensional data and its clustering visualization functionality. In this paper, we describe the algorithm and implementation, and show the practicality of the algorithm by applying MIGSOM to a huge scale real data set: Wikipedia's hyperlink data.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {95–96},
numpages = {2},
keywords = {SOM, clustering, link analysis, visualization, Wikipedia},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963242,
author = {Papapetrou, Odysseas and Siberski, Wolf and Siersdorfer, Stefan},
title = {Collaborative Classification over P2P Networks},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963242},
doi = {10.1145/1963192.1963242},
abstract = {We propose a novel collaborative approach for distributed document classification, combining the knowledge of multiple users for improved organization of data such as individual document repositories or emails. The approach builds on top of a P2P network and outperforms the state of the art approaches in collaborative classification.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {97–98},
numpages = {2},
keywords = {peer-to-peer, distributed classification},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963243,
author = {Pasternack, Jeff and Roth, Dan},
title = {Generalized Fact-Finding},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963243},
doi = {10.1145/1963192.1963243},
abstract = {Once information retrieval has located a document, and information extraction has provided its contents, how do we know whether we should actually believe it? Fact-finders are a state-of-the-art class of algorithms that operate in a manner analogous to Kleinberg's Hubs and Authorities, iteratively computing the trustworthiness of an information source as a function of the believability of the claims it makes, and the believability of a claim as a function of the trustworthiness of those sources asserting it. However, as fact-finders consider only "who claims what", they ignore a great deal of relevant background and contextual information. We present a framework for "lifting" (generalizing) the fact-finding process, allowing us to elegantly incorporate knowledge such as the confidence of the information extractor and the attributes of the information sources. Experiments demonstrate that leveraging this information significantly improves performance over existing, "unlifted" fact-finding algorithms.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {99–100},
numpages = {2},
keywords = {data integration, fact-finders, trust, graph algorithms},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963244,
author = {Pennacchiotti, Marco and Gurumurthy, Siva},
title = {Investigating Topic Models for Social Media User Recommendation},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963244},
doi = {10.1145/1963192.1963244},
abstract = {This paper presents a user recommendation system that recommends to a user new friends having similar interests. We automatically discover users' interests using Latent Dirichlet Allocation (LDA), a linguistic topic model that represents users as mixtures of topics. Our system is able to recommend friends for 4 million users with high recall, outperforming existing strategies based on graph analysis.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {101–102},
numpages = {2},
keywords = {LDA, social media, topic models, user recommendation},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963245,
author = {Phelan, Owen and McCarthy, Kevin and Bennett, Mike and Smyth, Barry},
title = {On Using the Real-Time Web for News Recommendation &amp; Discovery},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963245},
doi = {10.1145/1963192.1963245},
abstract = {In this work we propose that the high volumes of data on real-time networks like Twitter can be harnessed as a useful source of recommendation knowledge. We describe Buzzer, a news recommendation system that is capable of adapting to the conversations that are taking place on Twitter. Buzzer uses a content-based approach to ranking RSS news stories by mining trending terms from both the public Twitter timeline and from the timeline of tweets generated by a user's own social graph (friends and followers). We also describe the result of a live-user trial which demonstrates how these ranking strategies can add value to conventional RSS ranking techniques, which are largely recency-based.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {103–104},
numpages = {2},
keywords = {Twitter, content-based recommendation, social recommendation, news recommendation, real-time recommendation},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963246,
author = {Popescu, Ana-Maria and Pennacchiotti, Marco and Paranjpe, Deepa},
title = {Extracting Events and Event Descriptions from Twitter},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963246},
doi = {10.1145/1963192.1963246},
abstract = {This paper describes methods for automatically detecting events involving known entities from Twitter and understanding both the events as well as the audience reaction to them. We show that NLP techniques can be used to extract events, their main actors and the audience reactions with encouraging results.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {105–106},
numpages = {2},
keywords = {social media, Twitter, information extraction, opinion mining},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963247,
author = {Popescu, Ana-Maria and Jain, Alpa},
title = {Understanding the Functions of Business Accounts on Twitter},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963247},
doi = {10.1145/1963192.1963247},
abstract = {This paper performs an initial exploration of business Twitter accounts in order to start understanding how businesses interact with their users and viceversa. We provide an analysis of business tweet types and topics and show that specific business tweet classes such as deals and events can be reliably identified for customer use.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {107–108},
numpages = {2},
keywords = {Twitter, business intelligence, information extraction},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963248,
author = {Qian, Tieyun and Li, Qing and Srivastava, Jaideep},
title = {A Framework for Evaluating Network Measures for Functional Importance},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963248},
doi = {10.1145/1963192.1963248},
abstract = {Many metrics such as degree, closeness, and PageRank have been introduced to determine the relative importance of a node within a network. The desired function of a network, however, is domain-specific. For example, the robustness can be crucial for a communication network, while efficiency is more preferred for fast spreading of advertisements in viral marketing. The information provided by some widely used measures are often conflicting under such varying demands. In this paper, we present a novel framework for evaluating network metrics regarding typical functional requirements. We also propose an analysis of five well established measures to compare their performance of ranking nodes on functional importance in a real-life network.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {109–110},
numpages = {2},
keywords = {network metrics, functional importance},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963249,
author = {Rangrej, Aniket and Kulkarni, Sayali and Tendulkar, Ashish V.},
title = {Comparative Study of Clustering Techniques for Short Text Documents},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963249},
doi = {10.1145/1963192.1963249},
abstract = {We compare various document clustering techniques including K-means, SVD-based method and a graph-based approach and their performance on short text data collected from Twitter. We define a measure for evaluating the cluster error with these techniques. Observations show that graph-based approach using affinity propagation performs best in clustering short text data with minimal cluster error.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {111–112},
numpages = {2},
keywords = {short text, affinity propagation, SVD, clustering, K-means},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963250,
author = {Romero, Daniel M. and Galuba, Wojciech and Asur, Sitaram and Huberman, Bernardo A.},
title = {Influence and Passivity in Social Media},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963250},
doi = {10.1145/1963192.1963250},
abstract = {The ever-increasing amount of information flowing through Social Media forces the members of these networks to compete for attention and influence by relying on other people to spread their message. A large study of information propagation within Twitter reveals that the majority of users act as passive information consumers and do not forward the content to the network. Therefore, in order for individuals to become influential they must not only obtain attention and thus be popular, but also overcome user passivity. We propose an algorithm that determines the influence and passivity of users based on their information forwarding activity. An evaluation performed with a 2.5 million user dataset shows that our influence measure is a good predictor of URL clicks, outperforming several other measures that do not explicitly take user passivity into account. We demonstrate that high popularity does not necessarily imply high influence and vice-versa.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {113–114},
numpages = {2},
keywords = {influence, passivity},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963251,
author = {Satpal, Sandeepkumar and Bhadra, Sahely and Sellamanickam, Sundararajan and Rastogi, Rajeev and Sen, Prithviraj},
title = {Web Information Extraction Using Markov Logic Networks},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963251},
doi = {10.1145/1963192.1963251},
abstract = {In this paper, we consider the problem of extracting structured data from web pages taking into account both the content of individual attributes as well as the structure of pages and sites. We use Markov Logic Networks (MLNs) to capture both content and structural features in a single unified framework, and this enables us to perform more accurate inference. We show that inference in our information extraction scenario reduces to solving an instance of the maximum weight subgraph problem. We develop specialized procedures for solving the maximum subgraph variants that are far more efficient than previously proposed inference methods for MLNs that solve variants of MAX-SAT. Experiments with real-life datasets demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {115–116},
numpages = {2},
keywords = {Markov logic networks, information extraction},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963252,
author = {Sepehri Rad, Hoda and Barbosa, Denilson},
title = {Towards Identifying Arguments in Wikipedia Pages},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963252},
doi = {10.1145/1963192.1963252},
abstract = {Wikipedia is one of the most widely used repositories of human knowledge today, contributed mostly by a few hundred thousand regular editors. In this open environment, inevitably, differences of opinion arise among editors of the same article. Especially for polemical topics such as religion and politics, difference of opinions among editors may lead to intense "edit wars" in which editors compete to have their opinions and points of view accepted. While such disputes can compromise the reliability of the article (or at least portions of it), they are recorded in the edit history of the articles. We posit that exposing such disputes to the reader, and pointing to the portions of the text where they manifest most prominently can be beneficial in helping concerned readers in understanding such topics. In this paper, we discuss our initial efforts towards the problem of automatic evaluation of extracting controversial points in Wikipedia pages.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {117–118},
numpages = {2},
keywords = {controversy, Wikipedia, evaluation, argument},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963253,
author = {Shalem, Mirit and Kanza, Yaron},
title = {How to Choose Combinations in a Join of Search Results},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963253},
doi = {10.1145/1963192.1963253},
abstract = {We present novel measures for estimating the effectiveness of duplication-removal operations over a join of ranked lists. We introduce a duplication-removal approach, namely optimality rank, that outperforms existing approaches, according to the new measures.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {119–120},
numpages = {2},
keywords = {search, diversity, join, data integration, top-k},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963254,
author = {Shen, Wei and Wang, Jianyong and Luo, Ping and Wang, Min and Yao, Conglei},
title = {REACTOR: A Framework for Semantic Relation Extraction and Tagging over Enterprise Data},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963254},
doi = {10.1145/1963192.1963254},
abstract = {Relation extraction from Web data has attracted a lot of attention in recent years. However, little work has been done when it comes to relation extraction from enterprise data regardless of the urgent needs to such work in real applications (e.g., E-discovery). In this paper, we propose a novel unsupervised hybrid framework, called REACTOR (abbreviated for a fRamework for sEmantic relAtion extraCtion and Tagging Over enteRprise data). We evaluate REACTOR over a real-world enterprise data set and empirical results show the effectiveness of REACTOR.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {121–122},
numpages = {2},
keywords = {enterprise data, relation tagging, relation extraction},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963255,
author = {Shi, Xingtian and Yang, Zhenglu and Toyoda, Masashi and Kitsuregawa, Masaru},
title = {Harnessing the Wisdom of Crowds: Video Event Detection Based on Synchronous Comments},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963255},
doi = {10.1145/1963192.1963255},
abstract = {With the recent explosive growth of the number of videos on the Web, it becomes more important to facilitate users' demand for locating their preferred event clips in the lengthy and voluminous programs. Although there has been a great deal of study on generic event detection in recent years, the performance of existing approaches is still far from satisfactory. In this paper, we propose an integrated framework for general event detection. The key idea is that we utilize the synchronous comments to segment the video into clips with semantic text analysis, while taking into account the relationship between the users who write the comments. By borrowing the power of "the wisdom of crowds", we experimentally demonstrate that our approach can effectively detect video events.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {123–124},
numpages = {2},
keywords = {video search, tag analysis, information extraction},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963256,
author = {Sil, Dyut Kumar and Sengamedu, Srinivasan H. and Bhattacharyya, Chiranjib},
title = {ReadAlong: Reading Articles and Comments Together},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963256},
doi = {10.1145/1963192.1963256},
abstract = {We propose a new paradigm for displaying comments: showing comments alongside parts of the article they correspond to. We evaluate the effectiveness of various approaches for this task and show that a combination of bag of words and topic models performs the best.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {125–126},
numpages = {2},
keywords = {topic models, comments, comment alignment},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963257,
author = {Sinha, Pinaki and Mehrotra, Sharad and Jain, Ramesh},
title = {Effective Summarization of Large Collections of Personal Photos},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963257},
doi = {10.1145/1963192.1963257},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {127–128},
numpages = {2},
keywords = {diversity, summarization, optimization, coverage},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963258,
author = {Srinivasan, Sriram and Bhattachaya, Sourangshu},
title = {Learning to Tokenize Web Domains},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963258},
doi = {10.1145/1963192.1963258},
abstract = {Domain Match is an Internet monetization product offered by web companies like Yahoo! The product offers display of ads and search results, when a user requests a webpage from a domain which is non-existent or does not have any content. This product earns significant amount of advertising revenue for major internet companies like Yahoo! Hence it is an important product receiving millions of queries per day. Domain Match (DM) works by tokenizing the input domains and sub-folders into keywords and then displaying ads and search results queried on the keywords. In this poster, we describe a machine learning based solution, which automatically learns to tokenize new domains, given a training dataset containing a set of domains and their tokenizations. We use positional frequency and parts of speech as features for scoring tokens. Tokens are scored combined using various scoring models. We compare two ways of training the models: a simple gain function based training and a large margin training. Experimental results are encouraging.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {129–130},
numpages = {2},
keywords = {domain tokenization, large margin learning, internet monetization},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963259,
author = {Sripada, Bhargav and Polepalli, Krishna Reddy and Rage, Uday Kiran},
title = {Coverage Patterns for Efficient Banner Advertisement Placement},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963259},
doi = {10.1145/1963192.1963259},
abstract = {In an online banner advertising scenario, an advertiser expects that the banner advertisement should be displayed to certain percentage of web site visitors. In this context, to generate more revenue for a given web site, the publisher has to meet the demands of several advertisers by providing appropriate sets of web pages. To help the publishers and advertisers, in this paper, we propose a model of coverage patterns and a methodology to extract potential coverage patterns by analyzing click stream data. Given web pages of a site, a coverage pattern is a set of web pages visited by a certain percentage of visitors. The proposed approach has the potential to enable the publisher in meeting the demands of several advertisers. The efficiency and advantages of the proposed approach is shown by conducting experiments on real world data sets.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {131–132},
numpages = {2},
keywords = {online advertising, clickstream mining, internet monetization, computational advertising},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963260,
author = {Tang, Jintao and Wang, Ting and Wang, Ji and Lu, Qin and Li, Wenjie},
title = {Using Complex Network Features for Fast Clustering in the Web},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963260},
doi = {10.1145/1963192.1963260},
abstract = {Applying graph clustering algorithms in real world networks needs to overcome two main challenges: the lack of prior knowledge and the scalability issue. This paper proposes a novel method based on the topological features of complex networks to optimize the clustering algorithms in real-world networks. More specifically, the features are used for parameter estimation and performance optimization. The proposed method is evaluated on real-world networks extracted from the web. Experimental results show improvement both in terms of Adjusted Rand index values as well as runtime efficiency.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {133–134},
numpages = {2},
keywords = {complex networks, graph clustering, parameter estimation},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963261,
author = {Vadrevu, Srinivas and Velipasaoglu, Emre},
title = {Identifying Primary Content from Web Pages and Its Application to Web Search Ranking},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963261},
doi = {10.1145/1963192.1963261},
abstract = {Web pages are usually highly structured documents. In some documents, content with different functionality is laid out in blocks, some merely supporting the main discourse. In other documents, there may be several blocks of unrelated main content. Indexing a web page as if it were a linear document can cause problems because of the diverse nature of its content. If the retrieval function treats all blocks of the web page equally without attention to structure, it may lead to irrelevant query matches. In this paper, we describe how content quality of different blocks of a web page can be utilized to improve a retrieval function. Our method is based on segmenting a web page into semantically coherent blocks and learning a predictor of segment content quality. We also describe how to use segment content quality estimates as weights in the BM25F formulation. Experimental results show our method improves relevance of retrieved results by as much as 4.5% compared to BM25F that treats the body of a web page as a single section, and by a larger margin of over 9% for difficult queries.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {135–136},
numpages = {2},
keywords = {segmentation, search, page structure, content quality models},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963262,
author = {Venkatasubramanian, Suresh and Veilumuthu, Ashok and Krishnamurthy, Avanthi and C.E, Veni Madhavan and Nath, Kaushik and Arvindam, Sunil},
title = {A Non-Syntactic Approach for Text Sentiment Classification with Stopwords},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963262},
doi = {10.1145/1963192.1963262},
abstract = {The present approach uses stopwords and the gaps that occur between successive stopwords -formed by contentwords- as features for sentiment classification.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {137–138},
numpages = {2},
keywords = {topic models, sentiment mining, stop-words, latent dirichlet allocation, text and language applications},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963263,
author = {Volkovich, Yana and Kaltenbrunner, Andreas},
title = {Evaluation of Valuable User Generated Content on Social News Web Sites},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963263},
doi = {10.1145/1963192.1963263},
abstract = {Social news websites have gained significant popularity in the last few years. The participants of such websites are not only allowed to share news links but also to annotate, to evaluate and to comment them. To quantify interestingness and attractiveness of the user generated content in respect to the original link source we introduce the User Generated Content add-on (UGC+) index. Based on the definition of UGC+ we also propose a concept for comparing groups of links filtered by different properties, e.g. authorship or topic-categories. We apply the proposed measure on the Spanish Digg-clone Men\'{e}ame.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {139–140},
numpages = {2},
keywords = {user generated content, social media, news aggregator},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963264,
author = {Wang, Dakan and Wang, Gang and Lu, Pinyan and Wang, Yajun and Chen, Zheng and Hu, Botao},
title = {Is Pay-per-Click Efficient? An Empirical Analysis of Click Values},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963264},
doi = {10.1145/1963192.1963264},
abstract = {Current sponsored search auction adopts per-click bidding. It implicitly assumes that an advertiser treats all clicks to be equally valuable. This is not always true in real world situations. Clicks which lead to conversions are definitely more valuable than those fraudulent clicks. In this work, we use post-ad-click behavior to measure a click's value and empirically show that for an advertiser, values of different clicks are highly variant. Thus for many clicks, the advertiser's single bid does not reflect his true valuations. This indicates that the sponsored search system under PPC mechanism is not efficient, or does not always give a slot to the advertiser who needs it most.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {141–142},
numpages = {2},
keywords = {internet monetization},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963265,
author = {Wang, Yafang and Yang, Bin and Zoupanos, Spyros and Spaniol, Marc and Weikum, Gerhard},
title = {Scalable Spatio-Temporal Knowledge Harvesting},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963265},
doi = {10.1145/1963192.1963265},
abstract = {Knowledge harvesting enables the automated construction of large knowledge bases. In this work, we made a first attempt to harvest spatio-temporal knowledge from news archives to construct trajectories of individual entities for spatio-temporal entity tracking. Our approach consists of an entity extraction and disambiguation module and a fact generation module which produce pertinent trajectory records from textual sources. The evaluation on the 20 years' New York Times news article corpus showed that our methods are effective and scalable.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {143–144},
numpages = {2},
keywords = {knowledge harvesting, mapreduce, entity disambiguation, spatio-temporal facts, news archive},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963266,
author = {Weninger, Tim and Fumarola, Fabio and Lin, Cindy Xide and Barber, Rick and Han, Jiawei and Malerba, Donato},
title = {Growing Parallel Paths for Entity-Page Discovery},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963266},
doi = {10.1145/1963192.1963266},
abstract = {In this paper, we use the structural and relational information on the Web to find entity-pages. Specifically, given a Web site and an entity-page (e.g., department and faculty member homepage) we seek to find all of the entity-pages of the same type (e.g., all faculty members in the department). To do this, we propose a web structure mining method which grows parallel paths through the web graph and DOM trees. We show that by utilizing these parallel paths we can efficiently discover all entity-pages of the same type. Finally, we demonstrate the accuracy of our method with a case study on various domains.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {145–146},
numpages = {2},
keywords = {semi-structured data, entity pages, web structure mining, parallel paths},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963267,
author = {White, Ryen W. and Singla, Adish},
title = {Finding Our Way on the Web: Exploring the Role of Waypoints in Search Interaction},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963267},
doi = {10.1145/1963192.1963267},
abstract = {Information needs are rarely satisfied directly on search engine result pages. Searchers usually need to click through to search results (landing pages) and follow search trails beyond those pages to fulfill information needs. We use the term waypoints to describe pages visited by searchers between the trail origin (the landing page) and the trail destination. The role that waypoints play in search interaction is poorly understood yet can be vital in determining search success. In this poster we analyze log data to determine the arrangement and function of waypoints, and study how these are affected by variations in information goals. Our findings have implications for understanding search behavior and for the design of interactive search support based on waypoints.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {147–148},
numpages = {2},
keywords = {search trails, waypoints},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963268,
author = {Wu, Huayu and Takeda, Hideaki and Hamasaki, Masahiro and Ling, Tok Wang and Xu, Liang},
title = {An Adaptive Ontology-Based Approach to Identify Correlation between Publications},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963268},
doi = {10.1145/1963192.1963268},
abstract = {In this paper, we propose an adaptive ontology-based approach for related paper identification, to meet most researchers' practical needs. By searching ontology, we can return a diverse set of papers that are explicitly and implicitly related to an input paper. Moreover, our approach does not rely on known ontology. Instead, we build and update ontology for a collection with any domain of interest. Being independent from known ontology, our approach is much more adaptive for different domains.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {149–150},
numpages = {2},
keywords = {correlation of publications, ontology-based, adaptive approach},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963269,
author = {Wu, Shaomei and Liu, Shenwei and Cosley, Dan and Macy, Michael},
title = {Mining Collective Local Knowledge from Google MyMaps},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963269},
doi = {10.1145/1963192.1963269},
abstract = {The emerging popularity of location-aware devices and location-based services has generated a growing archive of digital traces of people's activities and opinions in physical space. In this study, we leverage geo-referenced user-generated content from Google MyMaps to discover collective local knowledge and understand the differing perceptions of urban space. Working with the large collection of publicly available, annotation-rich MyMaps data, we propose a highly parallelizable approach in order to merge identical places, discover landmarks, and recommend places. Additionally, we conduct interviews with New York City residents/visitors to validate the quantitative findings.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {151–152},
numpages = {2},
keywords = {place recommendation, geo-tagged data, user-generated content},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963270,
author = {Xu, Jun and Wu, Wei and Li, Hang and Xu, Gu},
title = {A Kernel Approach to Addressing Term Mismatch},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963270},
doi = {10.1145/1963192.1963270},
abstract = {This paper addresses the problem of dealing with term mismatch in web search using 'blending'. In blending, the input query as well as queries similar to it are used to retrieve documents, the ranking results of documents with respect to the queries are combined to generate a new ranking list. We propose a principled approach to blending, using a kernel method and click-through data. Our approach consists of three elements: a way of calculating query similarity using click-through data, a mixture model for combination of rankings using relevance, query similarity, and document similarity scores, and an algorithm for learning the weights of blending model based on the kernel method. Large scale experiments on web search and enterprise search data sets show that our approach can effectively solve term mismatch problem and significantly outperform the baseline methods of query expansion and heuristic blending.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {153–154},
numpages = {2},
keywords = {kernel methods, click-through data, term mismatch, blending},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963271,
author = {Xu, Xueke and Meng, Tao and Cheng, Xueqi and Liu, Yue},
title = {A Probabilistic Model for Opinionated Blog Feed Retrieval},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963271},
doi = {10.1145/1963192.1963271},
abstract = {In this poster, we study the problem of Opinionated Blog Feed Retrieval which can be considered as a particular type of the faceted blog distillation introduced by TREC 2009. It is a task of finding blogs not only having a principle and recurring interest in a given topic but also having a clear inclination towards expressing opinions on it. We propose a novel probabilistic model for this task which combines its two factors, topical relevance and opinionatedness, in a unified probabilistic framework. Experiments conducted in the context of the TREC 2009 &amp; 2010 Blog Track show the effectiveness of the proposed model.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {155–156},
numpages = {2},
keywords = {topical relevance, opinionated blog feed retrieval, opinionatedness},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963272,
author = {Yan, Rui and Kong, Liang and Li, Yu and Zhang, Yan and Li, Xiaoming},
title = {A Finegrained Digestion of News Webpages through Event Snippet Extraction},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963272},
doi = {10.1145/1963192.1963272},
abstract = {We describe a framework to digest news webpages in finer granularity: to extract event snippets from contexts. "Events" are atomic text snippets and a news article is constituted by more than one event snippet. Event Snippet Extraction (ESE) aims to mine these snippets out. The problem is important because its solutions may be applied to many information mining and retrieval tasks. The challenge is to exploit rich features to detect snippet boundaries, including various semantic, syntactic and visual features. We run experiments to present the effectiveness of our approaches.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {157–158},
numpages = {2},
keywords = {event snippet extraction, news digestion, web mining},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963273,
author = {Yang, Mengdong and Wu, Gang},
title = {Caching Intermediate Result of SPARQL Queries},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963273},
doi = {10.1145/1963192.1963273},
abstract = {The complexity and growing scale of RDF data has made data management back end the performance bottleneck of Semantic Web applications. Caching is one of the ways that could solve this problem. However, few existing research projects focus on caching in RDF data processing. We present an adaptive caching scheme that caches intermediate result of basic graph pattern SPARQL queries. Benchmark test results are provided to illustrate the effectiveness of our caching scheme.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {159–160},
numpages = {2},
keywords = {intermediate result, SPARQL, cache},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963274,
author = {Yao, Conglei and Jia, Xu and Shou, Sicong and Feng, Shicong and Zhou, Feng and Liu, Hongyan},
title = {Autopedia: Automatic Domain-Independent Wikipedia Article Generation},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963274},
doi = {10.1145/1963192.1963274},
abstract = {This paper proposes a general framework, named Autopedia, to generate high-quality wikipedia articles for given concepts in any domains, by automatically selecting the best wikipedia template consisting the sub-topics to organize the article for the input concept. Experimental results on 4,526 concepts validate the effectiveness of Autopedia, and the wikipedia template selection approach which takes into account both the template quality and the semantic relatedness between the input concept and its sibling concepts, performs the best.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {161–162},
numpages = {2},
keywords = {template selection, domain independent, Wikipedia, article generation},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963275,
author = {Ye, Mao and Xiao, Rong and Lee, Wang-Chien and Xie, Xing},
title = {Location Relevance Classification for Travelogue Digests},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963275},
doi = {10.1145/1963192.1963275},
abstract = {In this paper, we aim to develop a travelogue service to discover and convey various travelogue digests, in form of theme locations and geographical scope to their readers. In this service, theme locations in a travelogue are the core information to discover. Due to the inherent ambiguity of location relevance, we explore the textual (e.g., surrounding words) and geographical (e.g., geographical relationship among locations) features of locations to perform location relevance classification for theme location discovery. Finally, we conduct comprehensive experiments on collected travelogues to evaluate the performance of our location relevance classification technique and demonstrate the effectiveness of the travelogue service.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {163–164},
numpages = {2},
keywords = {classification, travelogue services},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963276,
author = {Yi, Jeonghe and Maghoul, Farzin},
title = {Mobile Search Pattern Evolution: The Trend and the Impact of Voice Queries},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963276},
doi = {10.1145/1963192.1963276},
abstract = {In this paper we study the characteristics of search queries submitted from mobile devices using Yahoo! Search for Mobile during a 2 months period in early of 2010, and compare the results with a similar study conducted in late 2007. The major findings include 1) mobile search queries have become much more diverse, and 2) user interest and information needs have been substantially changed at least in some areas of search topics, including adult and local intent queries. In addition we investigate the impact of voice query search interface offered by Yahoo!'s mobile search service. We examine how unstructured spoken queries differ from conventional search queries.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {165–166},
numpages = {2},
keywords = {voice queries, query categorization, mobile queries, query log analysis, mobile search query analysis, mobile search},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963277,
author = {Yin, Dawei and Hong, Liangjie and Davison, Brian D.},
title = {Exploiting Session-like Behaviors in Tag Prediction},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963277},
doi = {10.1145/1963192.1963277},
abstract = {In social bookmarking systems, existing methods in tag prediction have shown that the performance of prediction can be significantly improved by modeling users' preferences. However, these preferences are usually treated as constant over time, neglecting the temporal factor within users' behaviors. In this paper, we study the problem of session-like behavior in social tagging systems and demonstrate that the predictive performance can be improved by considering sessions. Experiments, conducted on three public datasets, show that our session-based method can outperform baselines and two state-of-the-art algorithms significantly.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {167–168},
numpages = {2},
keywords = {personalized tag prediction, social tagging, tag recommendation},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963278,
author = {Yoon, Seok-Ho and Kim, Sang-Wook and Kim, Ji-Soo and Hwang, Won-Seok},
title = {On Computing Text-Based Similarity in Scientific Literature},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963278},
doi = {10.1145/1963192.1963278},
abstract = {This paper addresses computing of similarity among papers using text-based measures. First, we analyze the accuracy of the similarities computed using different parts of a paper, and propose a method of Keyword-Extension, which is very useful when text information is incomplete.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {169–170},
numpages = {2},
keywords = {text-based similarity measure, scientific literature},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963279,
author = {Yu, Jianxing and Zha, Zheng-Jun and Wang, Meng and Chua, Tat-Seng},
title = {Hierarchical Organization of Unstructured Consumer Reviews},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963279},
doi = {10.1145/1963192.1963279},
abstract = {In this paper, we propose to organize the aspects of a specific product into a hierarchy by simultaneously taking advantages of domain structure knowledge as well as consumer reviews. Based on the derived hierarchy, we generate a hierarchical organization of the consumer reviews based on various aspects of the product, and aggregate consumer opinions on the aspects. With such hierarchical organization, people can easily grasp the overview of consumer reviews and opinions on various aspects, as well as seek consumer reviews and opinions on any specific aspect by navigating through the hierarchy. We conduct evaluation on two product review data sets: Liu et al.'s data set containing 314 reviews for five products [2], and our review corpus which is collected from forum Web sites containing 60,786 reviews for five popular products. The experimental results demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {171–172},
numpages = {2},
keywords = {product aspect hierarchy, consumer review organization},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963280,
author = {Zhang, Ying and Tatipamula, Mallik},
title = {The Freshman Handbook: A Hint for the Server Placement of Social Networks},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963280},
doi = {10.1145/1963192.1963280},
abstract = {There has been a recent unprecedented increase in the use of Online Social Networks (OSNs) to expand our social life, exchange information and share common interests. Many popular OSNs today attract hundreds of millions of users who share tremendous amount of data on it such as Facebook, Twitter, and Buzz. Given the huge business opportunities OSNs may bring, more and more new social applications has emerged on the Internet. For these newcomers in the social network business, one of the first key decisions to make is to where to deploy the computational resources to best accommodate future client requests. In this work, we aim at providing useful suggests to the new born social network providers (freshman) on the intelligent server placement, by exploring available public information from existing social network communities. In this work, we first propose three scalable server placement strategies for OSNs. Our solution can scalably select server locations among all the possible locations, at the same time reducing the cost for inter-user data sharing.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {173–174},
numpages = {2},
keywords = {social network},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963281,
author = {Zhou, Ning and Shen, Yi and Peng, Jinye and Feng, Xiaoyi and Fan, Jianping},
title = {Leveraging Auxiliary Text Terms for Automatic Image Annotation},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963281},
doi = {10.1145/1963192.1963281},
abstract = {This paper proposes a novel algorithm to annotate web images by automatically aligning the images with their most relevant auxiliary text terms. First, the DOM-based web page segmentation is performed to extract images and their most relevant auxiliary text blocks. Second, automatic image clustering is used to partition the web images into a set of groups according to their visual similarity contexts, which significantly reduces the uncertainty on the relatedness between the images and their auxiliary terms. The semantics of the visually-similar images in the same cluster are then described by the same ranked list of terms which frequently co-occur in their text blocks. Finally, a relevance re-ranking process is performed over a term correlation network to further refine the ranked term list. Our experiments on a large-scale database of web pages have provided very positive results.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {175–176},
numpages = {2},
keywords = {relevance re-ranking, image-text alignment, automatic image annotation},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963283,
author = {Bach, Benjamin and Pietriga, Emmanuel and Liccardi, Ilaria and Legostaev, Gennady},
title = {OntoTrix: A Hybrid Visualization for Populated Ontologies},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963283},
doi = {10.1145/1963192.1963283},
abstract = {Most Semantic Web data visualization tools structure the representation according to the concept definitions and interrelations that constitute the ontology's vocabulary. Instances are often treated as somewhat peripheral information, when considered at all. These instances, that populate ontologies, represent an essential part of any knowledge base, and are often orders of magnitude more numerous than the concept definitions that give them machine-processable meaning. We present a visualization technique designed to enable users to visualize large instance sets and the relations that connect them. This hybrid visualization uses both node-link and adjacency matrix representations of graphs to visualize different parts of the data depending on their semantic and local structural properties, exploiting ontological knowledge to drive the graph layout. The representation is embedded in an environment that features advanced interaction techniques for easy navigation, including support for smooth continuous zooming and coordinated views.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {177–180},
numpages = {4},
keywords = {semantic web, graphs, exploratory visualization, matrices},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963284,
author = {Balakrishnan, Raju and Kambhampati, Subbarao},
title = {Factal: Integrating Deep Web Based on Trust and Relevance},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963284},
doi = {10.1145/1963192.1963284},
abstract = {We demonstrate "Factal"--a system for integrating deep web sources. Factal is based on the recently introduced source selection method "SourceRank"; which is a measure of trust and relevance based on the agreement between the sources. SourceRank selects popular and trustworthy sources from autonomous and open collections like the deep web. This trust and popularity awareness distinguishes Factal from the existing systems like Google Product Search. Factal selects and searches active online databases on multiple domains. The demonstration scenarios include improved trustworthiness, relevance of results, and comparison shopping. We believe that by incorporating effective source selection based on the SourceRank, Factal demonstrates a significant step towards a deep-web-scale integration system.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {181–184},
numpages = {4},
keywords = {deep web, web integration, source selection, sourcerank},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963285,
author = {Blanco, Lorenzo and Bronzi, Mirko and Crescenzi, Valter and Merialdo, Paolo and Papotti, Paolo},
title = {Automatically Building Probabilistic Databases from the Web},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963285},
doi = {10.1145/1963192.1963285},
abstract = {A relevant number of web sites publish structured data about recognizable concepts (such as stock quotes, movies, restau- rants, etc.). There is a great chance to create applications that rely on a huge amount of data taken from the Web. We present an automatic and domain independent system that performs all the steps required to benefit from these data: it discovers data intensive web sites containing information about an entity of interest, extracts and integrate the published data, and finally performs a probabilistic analysis to characterize the impreciseness of the data and the accuracy of the sources. The results of the processing can be used to populate a probabilistic database.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {185–188},
numpages = {4},
keywords = {data integration, web data extraction, probabilistic data},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963286,
author = {Bozzon, Alessandro and Brambilla, Marco and Ceri, Stefano and Fraternali, Piero and Vadacca, Salvatore},
title = {Exploratory Search in Multi-Domain Information Spaces with Liquid Query},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963286},
doi = {10.1145/1963192.1963286},
abstract = {Search Computing (SeCo) aims at building search applications that bridge the gap between general-purpose and vertical search engines. SeCo queries extract ranked information about several interconnected domains, such as "hotels", "restaurants" or "concerts", by interacting with Web data sources which are wrapped as search services; an example of query is: "Find a good Jazz concert close to the user's current location, together with close-by good restaurants and hotels". The SeCo system supports the deployment of search applications, by providing a generic software architecture and the tools for service and query registration, for query formulation and execution, and for result browsing. In this demo paper, we focus on the Liquid Query (LQ) interface which supports the iteration over query formulation, result visualization and query refinement, with commands for perusing the result set, changing the visualization of data based on their type (e.g., geographical or temporal) and interacting with the remote search services. It also supports an exploratory search approach, where the user starts by accessing one data source (e.g., an event listing for finding interesting concerts), then is assisted in progressively joining other correlated sources in an interactive exploration of the search space. The exploration paths can be chosen on the fly and the navigation history can be browsed back and forth for cross-checking the retrieved options.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {189–192},
numpages = {4},
keywords = {web data integration, graphical user interface, exploratory search, multi-domain search},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963287,
author = {Castellanos, Malu and Ghosh, Riddhiman and Lu, Yue and Zhang, Lei and Ruiz, Perla and Dekhil, Mohamed and Dayal, Umeshwar and Hsu, Meichun},
title = {LivePulse: Tapping Social Media for Sentiments in Real-Time},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963287},
doi = {10.1145/1963192.1963287},
abstract = {The rise of Twitter, blogs, review sites and social sites has motivated people to express their opinions publicly and more frequently than ever before. This has fueled the emerging field known as sentiment analysis whose goal is to translate the vagaries of human emotion into hard data. LivePulse is a tool that taps into the growing business interest in what is being said online with the particular characteristic of doing so in real-time. LivePulse integrates novel algorithms for sentiment analysis and a configurable dashboard with different kinds of dynamic charts that change as new data is ingested. It also provides support to drill down and visually explore the sentiment scores to understand how they were computed and what are the emotions expressed about a given aspect or topic. Our tool has been researched and prototyped at HP Labs in close interaction with internal and external customers whose valuable feedback has been crucial for improving the tool. This paper presents an overview of LivePulse's architecture and functionality, and illustrates how it would be demoed.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {193–196},
numpages = {4},
keywords = {social media, sentiment analysis, real-time},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963288,
author = {Dan, Ovidiu and Feng, Junlan and Davison, Brian},
title = {Filtering Microblogging Messages for Social Tv},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963288},
doi = {10.1145/1963192.1963288},
abstract = {Social TV was named one of the ten most important emerging technologies in 2010 by the MIT Technology Review. Manufacturers of set-top boxes and televisions have recently started to integrate access to social networks into their products. Some of these systems allow users to read microblogging messages related to the TV program they are currently watching. However, such systems suffer from low precision and recall when they use the title of the show as keywords when retrieving messages, without any additional filtering.We propose a bootstrapping approach to collecting microblogging messages related to a given TV program. We start with a small set of annotated data, in which, for a given show and a candidate message, we annotate the pair to be relevant or irrelevant. From this annotated data set, we train an initial classifier. The features are designed to capture the association between the TV program and the message. Using our initial classifier and a large dataset of unlabeled messages we derive broader features for a second classifier to further improve precision.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {197–200},
numpages = {4},
keywords = {twitter, microblogging, filtering, classification, social tv},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963289,
author = {Datta, Anwitaman and Tan Teck Yong, Jackson and Ventresque, Anthony},
title = {T-RecS: Team Recommendation System through Expertise and Cohesiveness},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963289},
doi = {10.1145/1963192.1963289},
abstract = {Searching for people by exploration of social networks structure is an interesting problem which has recently gathered a lot of attention. Expert recommendation is an important but also extensively researched problem. In contrast, the generalized problem of team recommendation has not been studied a lot. The purpose of this demo is to show a multidisciplinary team search and recommendation prototype. While the current demo uses specific (NTU academic) data-set, the framework is generic, and can be extended for other domains subject to availability of suitable information.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {201–204},
numpages = {4},
keywords = {team recommendation, social network analysis, expert search},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963290,
author = {Eda, Takeharu and Uchiyama, Toshio and Bessho, Katsuji and Katafuchi, Norifumi and Chen, Alice and Kataoka, Ryoji},
title = {Accelerating Instant Question Search with Database Techniques},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963290},
doi = {10.1145/1963192.1963290},
abstract = {Distributed question answering services, like Yahoo Answer and Aardvark, are known to be useful for end users and have also opened up numerous topics ranging in many research fields. In this paper, we propose a user-support tool for composing questions in such services. Our system incrementally recommends similar questions while users are typing their question in a sentence, which gives the users opportunities to know that there are similar questions that have already been solved. A question database is semantically analyzed and searched in the semantic space by boosting the performance of similarity searches with database techniques such as server/client caching and LSH (Locality Sensitive Hashing). The more text the user enters, the more similar the recommendations will become to the ultimately desired question. This unconscious editing-as-a-sequence-of-searches approach helps users to form their question incrementally through interactive supplementary information. Not only askers nor repliers, but also service providers have advantages such as that the knowledge of the service will be autonomously refined by avoiding for novice users to repeat questions which have been already solved.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {205–208},
numpages = {4},
keywords = {lsi, question authoring, implementation, lsh},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963291,
author = {Fu, Haizhou and Gao, Sidan and Anyanwu, Kemafor},
title = {CoSi: Context-Sensitive Keyword Query Interpretation on RDF Databases},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963291},
doi = {10.1145/1963192.1963291},
abstract = {The demo will present CoSi, a system that enables context-sensitive interpretation of keyword queries on RDF databases. The techniques for representing, managing and exploiting query history are central to achieving this objective. The demonstration will show the effectiveness of our approach for capturing a user's querying context from their query history. Further, it will show how context is utilized to influence the interpretation of a new query. The demonstration is based on DBPedia, the RDF representation of Wikipedia.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {209–212},
numpages = {4},
keywords = {query history, keyword query interpretation},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963292,
author = {Grineva, Maria and Grinev, Maxim and Lizorkin, Dmitry and Boldakov, Alexander and Turdakov, Denis and Sysoev, Andrey and Kiyko, Alexander},
title = {Blognoon: Exploring a Topic in the Blogosphere},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963292},
doi = {10.1145/1963192.1963292},
abstract = {We demonstrate Blognoon, a semantic blog search engine with the focus on topic exploration and navigation. Blognoon provides concept search instead of traditional keywords search and improves ranking by identifying main topics of posts. It enhances navigation over the Blogosphere with faceted interfaces and recommendations.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {213–216},
numpages = {4},
keywords = {blogs, blogosphere, wikipedia, semantic search, concept search},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963293,
author = {Groppe, Jinghua and Groppe, Sven and Schleifer, Andreas},
title = {Visual Query System for Analyzing Social Semantic Web},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963293},
doi = {10.1145/1963192.1963293},
abstract = {The social web is becoming increasingly popular and important, because it creates the collective intelligence, which can produce more value than the sum of individuals. The social web uses the Semantic Web technology RDF to describe the social data in a machine-readable way. RDF query languages play certainly an important role in the social data analysis for extracting the collective intelligence. However, constructing such queries is not trivial since the social data is often quite large and assembled from a large number of different sources. In order to solve these challenges, we develop a Visual Query System (VQS) for helping the analysts of social data and other semantic data to formulate such queries easily and exactly. In this VQS, we suggest a condensed data view, a browser-like query creation system for absolute beginners and a Visual Query Language (VQL) for beginners and experienced users. Using the browser-like query creation or the VQL, the analysts of social data and other semantic data can construct queries with no or little syntax knowledge; using the condensed view, they can determine easily what queries should to be used. Furthermore, our system also supports precise suggestions to extend and refine existing queries.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {217–220},
numpages = {4},
keywords = {semantic web, rdf, visual query languages, sparql},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963294,
author = {Guabtni, Adnene and Clarke, Stuart and Benatallah, Boualem},
title = {Embedding MindMap as a Service for User-Driven Composition of Web Applications},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963294},
doi = {10.1145/1963192.1963294},
abstract = {The World Wide Web is evolving towards a very large distributed platform allowing ubiquitous access to a wide range of Web applications with minimal delay and no installation required. Such Web applications range from having users undertake simple tasks, such as filling a form, to more complex tasks including collaborative work, project management, and more generally, creating, consulting, annotating, and sharing Web content. However, users are lacking a simple but yet powerful mechanism to compose Web applications, similarly to what desktop environments allowed for decades using the file explorer paradigm and the desktop metaphor. Attempts have been made to adapt the desktop metaphor to the Web environment giving birth to Webtops (Web desktops). It essentially consisted of embedding a desktop environment in a Web browser and provide access to various Web applications within the same User Interface. However, those attempts did not take into consideration to the radical differences between Web and desktop environments and applications. In this work, we introduce a new approach for Web application composition based on the mindmap metaphor. It allows browsing artifacts (Web resources) and enabling user-driven composition of their associated Web applications. Essentially, a mindmap is a graph of widgets representing artifacts created or used by Web applications and allow to list and launch all possible Web applications associated to each artifact. A tool has been developed to experiment the new metaphor and is provided as a service to be embedded in Web applications via a Web browser's plug-in. We demonstrate in this paper three case studies regarding the DBLP Web site, Wikipedia and Google Picasa Web applications.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {221–224},
numpages = {4},
keywords = {mindmap, web application, user-driven composition},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963295,
author = {Hassanzadeh, Oktie and Duan, Songyun and Fokoue, Achille and Kementsietsidis, Anastasios and Srinivas, Kavitha and Ward, Michael J.},
title = {Helix: Online Enterprise Data Analytics},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963295},
doi = {10.1145/1963192.1963295},
abstract = {The size, heterogeneity and dynamicity of data within an enterprise makes indexing, integration and analysis of the data increasingly difficult tasks. On the other hand, there has been a massive increase in the amount of high-quality open data available on the Web that could provide invaluable insights to data analysts and business intelligence specialists within the enterprise. The goal of Helix project is to provide users within the enterprise with a platform that allows them to perform online analysis of almost any type and amount of internal data using the power of external knowledge bases available on the Web. Such a platform requires a novel, data-format agnostic indexing mechanism, and light-weight data linking techniques that could link semantically related records across internal and external data sources of various characteristics. We present the initial architecture of our system and discuss several research challenges involved in building such a system.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {225–228},
numpages = {4},
keywords = {semantic link discovery, data integration, enterprise data management, linked data},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963296,
author = {Hoffart, Johannes and Suchanek, Fabian M. and Berberich, Klaus and Lewis-Kelham, Edwin and de Melo, Gerard and Weikum, Gerhard},
title = {YAGO2: Exploring and Querying World Knowledge in Time, Space, Context, and Many Languages},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963296},
doi = {10.1145/1963192.1963296},
abstract = {We present YAGO2, an extension of the YAGO knowledge base with focus on temporal and spatial knowledge. It is automatically built from Wikipedia, GeoNames, and WordNet, and contains nearly 10 million entities and events, as well as 80 million facts representing general world knowledge. An enhanced data representation introduces time and location as first-class citizens. The wealth of spatio-temporal information in YAGO can be explored either graphically or through a special time- and space-aware query language.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {229–232},
numpages = {4},
keywords = {geo-spatial, ontology, temporal, knowledge base, textual, multilingual},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963297,
author = {Li, Beibei and Ghose, Anindya and Ipeirotis, Panagiotis G.},
title = {A Demo Search Engine for Products},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963297},
doi = {10.1145/1963192.1963297},
abstract = {Most product search engines today build on models of relevance devised for information retrieval. However, the decision mechanism that underlies the process of buying a product is different than the process of locating relevant documents or objects. We propose a theory model for product search based on expected utility theory from economics. Specifically, we propose a ranking technique in which we rank highest the products that generate the highest surplus, after the purchase. We instantiate our research by building a demo search engine for hotels that takes into account consumer heterogeneous preferences, and also accounts for the varying hotel price. Moreover, we achieve this without explicitly asking the preferences or purchasing histories of individual consumers but by using aggregate demand data. This new ranking system is able to recommend consumers products with "best value for money" in a privacy-preserving manner. The demo is accessible at http://nyuhotels.appspot.com/},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {233–236},
numpages = {4},
keywords = {user modeling, product search, web search, recommender systems},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963298,
author = {Nebot Romero, Victoria and Ye, Min and Albrecht, Mario and Eom, Jae-Hong and Weikum, Gehard},
title = {DIDO: A Disease-Determinants Ontology from Web Sources},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963298},
doi = {10.1145/1963192.1963298},
abstract = {This paper introduces DIDO, a system providing convenient access to knowledge about factors involved in human diseases, automatically extracted from textual Web sources. The knowledge base is bootstrapped by integrating entities from hand-crafted sources like MeSH and OMIM. As these are short on relationships between dierent types of biomedical entities, DIDO employs flexible and robust pattern learning and constraint-based reasoning methods to automatically extract new relational facts from textual sources. These facts can then be iteratively added to the knowledge base. The result is a semantic graph of typed entities and relations between diseases, their symptoms, and their factors, with emphasis on environmental factors but covering also molecular determinants. We demonstrate the value of DIDO for knowledge discovery about causal factors and properties of complex diseases, including factor-disease chains.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {237–240},
numpages = {4},
keywords = {disease factors, ontology, biomedical knowledge base, relation extraction},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963299,
author = {Pal, Dipali and Rao, Praveen R.},
title = {A Tool for Fast Indexing and Querying of Graphs},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963299},
doi = {10.1145/1963192.1963299},
abstract = {We present a tool called GiS for indexing and querying a large database of labeled, undirected graphs. Such graphs can model chemical compounds, represent contact maps constructed from 3D structure of proteins, and so forth. GiS supports exact subgraph matching and approximate graph matching queries. It adopts a suite of new techniques and algorithms for (a) fast construction of disk-based indexes with small index sizes, and (b) efficient query processing with high precision of matching. During the demo, the user can index real graph datasets using a recommendation facility in GiS, pose exact subgraph matching and approximate graph matching queries, and view matching graphs using the Jmol browser.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {241–244},
numpages = {4},
keywords = {graphs, querying, indexing},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963300,
author = {Parikh, Nish and Sundaresan, Neel},
title = {A User-Tunable Approach to Marketplace Search},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963300},
doi = {10.1145/1963192.1963300},
abstract = {The notion of relevance is key to the performance of search engines as they interpret the user queries and respond with matching results. Online search engines have used other features beyond pure IR features to return relevant matching documents. However, over-emphasis on relevance could lead to redundancy in search results. In document search, diversity is simply the variety of documents that span the result set. In an online marketplace the diversity in the result set is represented by items for sale by different sellers at different prices with different sales options. For such a marketplace, in order to minimize query abandonment and the risk of dissatisfaction to the average user, several factors like diversity, trust and value need to be taken into account. Previous work in this field [4] has shown an impossibility result that there exists no such function that can optimize for all these factors. Since these factors and the measures associated with the factors could be subjective we take an approach of giving the control back to the user.In this paper we describe an interface which enables users to have more control over the optimization function used to present the results. We demonstrate this for search on eBay - one of the largest online marketplaces with a vibrant user community and dynamic inventory. We use an algorithm based on bounded greedy selection [5] to construct the result set based on parameters specified by the user.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {245–248},
numpages = {4},
keywords = {relevance, value, search, diversity, trust, ecommerce},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963301,
author = {Ratkiewicz, Jacob and Conover, Michael and Meiss, Mark and Gon\c{c}alves, Bruno and Patil, Snehal and Flammini, Alessandro and Menczer, Filippo},
title = {Truthy: Mapping the Spread of Astroturf in Microblog Streams},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963301},
doi = {10.1145/1963192.1963301},
abstract = {Online social media are complementing and in some cases replacing person-to-person social interaction and redefining the diffusion of information. In particular, microblogs have become crucial grounds on which public relations, marketing, and political battles are fought. We demonstrate a web service that tracks political memes in Twitter and helps detect astroturfing, smear campaigns, and other misinformation in the context of U.S. political elections. We also present some cases of abusive behaviors uncovered by our service. Our web service is based on an extensible framework that will enable the real-time analysis of meme diffusion in social media by mining, visualizing, mapping, classifying, and modeling massive streams of public microblogging events.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {249–252},
numpages = {4},
keywords = {classification, twitter, social media, information diffusion, microblogs, truthy, memes, politics},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963302,
author = {Renger, Bernard and Feng, Junlan and Dan, Ovidiu and Chang, Harry and Barbosa, Luciano},
title = {VoiSTV: Voice-Enabled Social TV},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963302},
doi = {10.1145/1963192.1963302},
abstract = {Until recently, the TV viewing experience has not been a very social activity compared to activities on the World Wide Web. In this work, we will present a Voice-enabled Social TV system (VoiSTV) which allows users to interact, follow and monitor the online social media messages related to a TV show while watching it. Users can create, send, and reply to messages using spoken language. VoiSTV also provides metadata information about TV shows such as trends, hot topics, popularity as well as aggregated sentiment of show-related messages, all of which are valuable for TV program search and recommendation.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {253–256},
numpages = {4},
keywords = {twitter, social data mining, iptv, speech interface, social tv},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963303,
author = {Samet, Hanan and Teitler, Benjamin E. and Adelfio, Marco D. and Lieberman, Michael D.},
title = {Adapting a Map Query Interface for a Gesturing Touch Screen Interface},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963303},
doi = {10.1145/1963192.1963303},
abstract = {NewsStand is an example application of a general framework that we are developing to enable searching for information using a map query interface, where the information results from monitoring the output of over 8,000 RSS news sources and is available for retrieval within minutes of publication. The user interface of NewsStand was recently adapted so that NewsStand can execute on mobile and tablet devices with a gesturing touch screen interface such as the iPhone, iPod Touch, and iPad. This action led to a discovery of some shortcomings of current mapping APIs as well as devising some interesting new widgets. These issues are discussed, and the realization can be seen by a demo at http://newsstand.umiacs.umd.edu on any of the above Apple devices as well as other devices that support gestures such as an Android phone.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {257–260},
numpages = {4},
keywords = {newsstand, map query interface, touch screen gesturing interface},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963304,
author = {Sellers, Andrew Jon and Furche, Tim and Gottlob, Georg and Grasso, Giovanni and Schallhart, Christian},
title = {OXPath: Little Language, Little Memory, Great Value},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963304},
doi = {10.1145/1963192.1963304},
abstract = {Data about everything is readily available on the web-but often only accessible through elaborate user interactions. For automated decision support, extracting that data is essential, but infeasible with existing heavy-weight data extraction systems. In this demonstration, we present OXPath, a novel approach to web extraction, with a system that supports informed job selection and integrates information from several different web sites. By carefully extending XPath, OXPath exploits its familiarity and provides a light-weight interface, which is easy to use and embed. We highlight how OXPath guarantees optimal page buffering, storing only a constant number of pages for non-recursive queries.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {261–264},
numpages = {4},
keywords = {web extraction, xpath, web automation, ajax},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963305,
author = {Sengstock, Christian and Gertz, Michael},
title = {CONQUER: A System for Efficient Context-Aware Query Suggestions},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963305},
doi = {10.1145/1963192.1963305},
abstract = {Many of today's search engines provide autocompletion while the user is typing a query string. This type of dynamic query suggestion can help users to formulate queries that better represent their search intent during Web search interactions. In this paper, we demonstrate our query suggestion system called CONQUER, which allows to efficiently suggest queries for a given partial query and a number of available query context observations. The context-awareness allows for suggesting queries tailored to a given context, e.g., the user location or the time of day. CONQUER uses a suggestion model that is based on the combined probabilities of sequential query patterns and context observations. For this, the weight of a context in a query suggestion can be adjusted online, for example, based on the learned user behavior or user profiles. We demonstrate the functionality of CONQUER based on 6 million queries from an AOL query log using the time of day and the country domain of the clicked URLs in the search result as context observations.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {265–268},
numpages = {4},
keywords = {query context, dynamic query suggestion},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963306,
author = {Tuan, Tran Anh and Elbassuoni, Shady and Preda, Nicoleta and Weikum, Gerhard},
title = {CATE: Context-Aware Timeline for Entity Illustration},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963306},
doi = {10.1145/1963192.1963306},
abstract = {Wikipedia has become one of the most authoritative information sources on the Web. Each article in Wikipedia provides a portrait of a certain entity. However, such a portrait is far from complete. An informative portrait of an entity should also reveal the context the entity belongs to. For example, for a person, major historical, political and cultural events that coincide with her life are important and should be included in that person's portrait. Similarly, the person's interactions with other people are also important. All this information should be summarized and presented in an appealing and interactive visual interface that enables users to quickly scan the entity's portrait.We demonstrate CATE which is a system that utilizes Wikipedia to create a portrait of a given entity of interest. We provide a visualization tool that summarizes the important events related to the entity. The novelty of our approach lies in seeing the portrait of an entity in a broader context, synchronous with its time.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {269–272},
numpages = {4},
keywords = {visualization tools, knowledge ranking, timeline},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963307,
author = {Tylenda, Tomasz and Sozio, Mauro and Weikum, Gerhard},
title = {Einstein: Physicist or Vegetarian? Summarizing Semantic Type Graphs for Knowledge Discovery},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963307},
doi = {10.1145/1963192.1963307},
abstract = {The Web and, in particular, knowledge-sharing communities such as Wikipedia contain a huge amount of information encompassing disparate and diverse fields. Knowledge bases such as DBpedia or Yago represent the data in a concise and more structured way bearing the potential of bringing database tools to Web Search. The wealth of data, however, poses the challenge of how to retrieve important and valuable information, which is often intertwined with trivial and less important details. This calls for an efficient and automatic summarization method.In this demonstration proposal, we consider the novel problem of summarizing the information related to a given entity, like a person or an organization. To this end, we utilize the rich type graph that knowledge bases provide for each entity, and define the problem of selecting the best cost-restricted subset of types as summary with good coverage of salient properties.We propose a demonstration of our system which allows the user to specify the entity to summarize, an upper bound on the cost of the resulting summary, as well as to browse the knowledge base in a more simple and intuitive manner.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {273–276},
numpages = {4},
keywords = {summarization, semantic search, knowledge bases},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963309,
author = {Leskovec, Jure},
title = {Social Media Analytics: Tracking, Modeling and Predicting the Flow of Information through Networks},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963309},
doi = {10.1145/1963192.1963309},
abstract = {Online social media represent a fundamental shift of how information is being produced, transferred and consumed. User generated content in the form of blog posts, comments, and tweets establishes a connection between the producers and the consumers of information. Tracking the pulse of the social media outlets, enables companies to gain feedback and insight in how to improve and market products better. For consumers, the abundance of information and opinions from diverse sources helps them tap into the wisdom of crowds, to aid in making more informed decisions.The present tutorial investigates techniques for social media modeling, analytics and optimization. First we present methods for collecting large scale social media data and then discuss techniques for coping with and correcting for the effects arising from missing and incomplete data. We proceed by discussing methods for extracting and tracking information as it spreads among the users. Then we examine methods for extracting temporal patterns by which information popularity grows and fades over time. We show how to quantify and maximize the influence of media outlets on the popularity and attention given to particular piece of content, and how to build predictive models of information diffusion and adoption. As the information often spreads through implicit social and information networks we present methods for inferring networks of influence and diffusion. Last, we discuss methods for tracking the flow of sentiment through networks and emergence of polarization.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {277–278},
numpages = {2},
keywords = {information diffusion, social networks, information cascades, influence maximization, social media analytics},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963310,
author = {Baeza-Yates, Ricardo},
title = {Distributed Web Retrieval},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963310},
doi = {10.1145/1963192.1963310},
abstract = {In the ocean of Web data, Web search engines are the primary way to access content. As the data is on the order of petabytes, current search engines are very large centralized systems based on replicated clusters. Web data, however, is always evolving. The number of Web sites continues to grow rapidly (over 270 millions at the beginning of 2011) and there are currently more than 20 billion indexed pages. On the other hand, Internet users are above one billion and hundreds of million of queries are issued each day. In the near future, centralized systems are likely to become less effective against such a data-query load, thus suggesting the need of fully distributed search engines. Such engines need to maintain high quality answers, fast response time, high query throughput, high availability and scalability; in spite of network latency and scattered data. In this tutorial we present the architecture of current search engines and we explore the main challenges behind the design of all the processes of a distributed Web retrieval system crawling, indexing, and query processing.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {279–280},
numpages = {2},
keywords = {indexing, web search, distributed systems, crawling, query processing},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963311,
author = {Ahmed, Amr and Smola, Alexander},
title = {WWW 2011 Invited Tutorial Overview: Latent Variable Models on the Internet},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963311},
doi = {10.1145/1963192.1963311},
abstract = {Graphical models are an effective tool for analyzing structured and relational data. In particular, they allow us to arrive at insights that are implicit, i.e. latent in the data. Dealing with such data on the internet poses a range of challenges. Firstly, the sheer size renders many well-known inference algorithms infeasible. Secondly, the problems arising on the internet do not always fit well into the known categories for latent variable inference such as Latent Dirichlet Allocation or clustering.In this tutorial we address a number of aspects. Firstly, we present a variety of applications ranging from general purpose document analysis, ideology detection, clustering of sequential data, and dynamic user profiling to recommender systems and data integration. Secondly we give an overview over a number of popular models such as mixture models, topic models, nonparametric variants of temporal dependence, and an integrated analysis and clustering approach, all of which can be used to solve a range of data analysis problems at hand. Thirdly, we present a range of sampling based algorithms for large scale distributed inference using multicore systems and clusters of workstations.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {281–282},
numpages = {2},
keywords = {graphical models, topic models, sampling, clustering, latent variables},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963312,
author = {Guy, Ido and Carmel, David},
title = {Social Recommender Systems},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963312},
doi = {10.1145/1963192.1963312},
abstract = {The goal of this tutorial is to expose participants to the current research on social recommender systems (i.e., recommender systems for the social web). Participants will become familiar with state-of-the-art recommendation methods, their classifications according to various criteria, common evaluation methodologies, and potential applications that can utilize social recommender systems. Additionally, open issues and challenges in the field will be discussed.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {283–284},
numpages = {2},
keywords = {social recommendation, recommender systems, social media},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963313,
author = {Gao, Bin and Wang, Taifeng and Liu, Tie-Yan},
title = {Ranking on Large-Scale Graphs with Rich Metadata},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963313},
doi = {10.1145/1963192.1963313},
abstract = {For many Web applications, one needs to deal with the ranking problem on large-scale graphs with rich metadata. However, it is non-trivial to perform efficient and effective ranking on them. On one aspect, we need to design scalable algorithms. On another aspect, we also need to develop powerful computational infrastructure to support these algorithms. This tutorial aims at giving a timely introduction to the promising advances in the aforementioned aspects in recent years, and providing the audiences with a comprehensive view on the related literature.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {285–286},
numpages = {2},
keywords = {Markov process, graph ranking, large-scale graph, map-reduce},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963314,
author = {Ipeirotis, Panagiotis G. and Paritosh, Praveen K.},
title = {Managing Crowdsourced Human Computation: A Tutorial},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963314},
doi = {10.1145/1963192.1963314},
abstract = {The tutorial covers an emerging topic of wide interest: Crowdsourcing. Specifically, we cover areas of crowdsourcing related to managing structured and unstructured data in a web-related content. Many researchers and practitioners today see the great opportunity that becomes available through easily-available crowdsourcing platforms. However, most newcomers face the same questions: How can we manage the (noisy) crowds to generate high quality output? How to estimate the quality of the contributors? How can we best structure the tasks? How can we get results in small amounts of time and minimizing the necessary resources? How to setup the incentives? How should such crowdsourcing markets be setup? Their presented material will cover topics from a variety of fields, including computer science, statistics, economics, and psychology. Furthermore, the material will include real-life examples and case studies from years of experience in running and managing crowdsourcing applications in business settings.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {287–288},
numpages = {2},
keywords = {quality assurance, market design, reputation, mechanical turk, human computation, incentives, crowdsourcing, workflow control},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963315,
author = {Nagarajan, Meena and Sheth, Amit and Velmurugan, Selvam},
title = {Citizen Sensor Data Mining, Social Media Analytics and Development Centric Web Applications},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963315},
doi = {10.1145/1963192.1963315},
abstract = {With the rapid rise in the popularity of social media (500M+ Facebook users, 100M+ twitter users), and near ubiquitous mobile access (4+ billion actively-used mobile phones), the sharing of observations and opinions has become common-place (nearly 100M tweets a day, 1.8 trillion SMSs in US last year). This has given us an unprecedented access to the pulse of a populace and the ability to perform analytics on social data to support a variety of socially intelligent applications -- be it towards targeted online content delivery, crisis management, organizing revolutions or promoting social development in underdeveloped and developing countries.This tutorial will address challenges and techniques for building applications that support a broad variety of users and types of social media. This tutorial will focus on social intelligence applications for social development, and cover the following research efforts in sufficient depth: 1) understanding and analysis of informal text, esp. microblogs (e.g., issues of cultural entity extraction and role of semantic/background knowledge enhanced techniques), and 2) building social media analytics platforms. Technical insights will be coupled with identification of computational techniques and real-world examples.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {289–290},
numpages = {2},
keywords = {social development application, social media analysis, citizen sensing, semantic social web, semantic social mashup, social signals, user generated content, mobile development application, people-content-network view of social media},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963316,
author = {Yadati, Narahari and Narayanam, Ramasuri},
title = {Game Theoretic Models for Social Network Analysis},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963316},
doi = {10.1145/1963192.1963316},
abstract = {The existing methods and techniques for social network analysis are inadequate to capture both the behavior (such as rationality and intelligence) of individuals and the strategic interactions that occur among these individuals. Game theory is a natural tool to overcome this inadequacy since it provides rigorous mathematical models of strategic interaction among autonomous, intelligent, and rational agents. Motivated by the above observation, this tutorial provides the conceptual underpinnings of the use of game theoretic models in social network analysis. In the first part of the tutorial, we provide rigorous foundations of relevant concepts in game theory and social network analysis. In the second part of the tutorial, we present a comprehensive study of four contemporary and pertinent problems in social networks: social network formation, determining in influential individuals for viral marketing, query incentive networks, and community detection.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {291–292},
numpages = {2},
keywords = {social networks, query incentive networks, network formation, game theory, community detection, viral marketing},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963317,
author = {Feng, Junlan and Johnston, Michael and Bangalore, Srinivas},
title = {Speech and Multimodal Interaction in Mobile Search},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963317},
doi = {10.1145/1963192.1963317},
abstract = {This tutorial highlights the characteristics of mobile search comparing with its desktop counterpart, reviews the state of art technologies of speech-based mobile search, and presents opportunities for exploiting multimodal interaction to optimize the efficiency of mobile search. It is suitable for students, researchers and practitioners working in the areas of spoken language processing, multimodal and search with an emphasis on a synergistic integration of these technologies for applications on mobile devices. We will provide detailed bibliography and sufficient literature for everyone interested to jumpstart work on this topic},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {293–294},
numpages = {2},
keywords = {speech recognition, multimodal interface, mobile voice search},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963318,
author = {Harth, Andreas and Hogan, Aidan and Kotoulas, Spyros and Urbani, Jacopo},
title = {Scalable Integration and Processing of Linked Data},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963318},
doi = {10.1145/1963192.1963318},
abstract = {The goal of this tutorial is to introduce, motivate and detail techniques for integrating heterogeneous structured data from across the Web. Inspired by the growth in Linked Data publishing, our tutorial aims at educating Web researchers and practitioners about this new publishing paradigm. The tutorial will show how Linked Data enables uniform access, parsing and interpretation of data, and how this novel wealth of structured data can potentially be exploited for creating new applications or enhancing existing ones.As such, the tutorial will focus on Linked Data publishing and related Semantic Web technologies, introducing scalable techniques for crawling, indexing and automatically integrating structured heterogeneous Web data through reasoning.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {295–296},
numpages = {2},
keywords = {linked data},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963319,
author = {Pa\c{s}ca, Marius},
title = {Web-Based Open-Domain Information Extraction},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963319},
doi = {10.1145/1963192.1963319},
abstract = {This tutorial provides an overview of extraction methods developed in the area of Web-based open-domain information extraction, whose purpose is the acquisition of open-domain classes, instances and relations from Web text. The extraction methods operate over unstructured or semi-structured text. They take advantage of weak supervision provided in the form of seed examples or small amounts of annotated data, or draw upon knowledge already encoded within resources created strictly by experts or collaboratively by users. The tutorial teaches the audience about existing resources that include instances and relations; details of methods for extracting such data from structured and semi-structured text available on the Web; and strengths and limitations of resources extracted from text as part of recent literature, with applications in knowledge discovery and information retrieval.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {297–298},
numpages = {2},
keywords = {information extraction, knowledge acquisition, web corpora},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963320,
author = {Fortuna, Carolina and Grobelnik, Marko},
title = {The Web of Things},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963320},
doi = {10.1145/1963192.1963320},
abstract = {The Web, similar to other successful man made systems is continuously evolving. With the miniaturization and increased performance of computing devices which are also being embedded in common physical objects, it is natural that the Web evolved to also include these - therefore the Web of Things. This tutorial provides an overview of the system vertical structure by identifying the relevant components, illustrating their functionality and showing existing tools and systems. The aim is to show how small devices can be connected to the Web at various levels of abstraction and transform them into "first-class" Web residents.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {299–300},
numpages = {2},
keywords = {text-mining, web-mining, sensor network, stream-mining, machine-learning, web of things, sensor, semantic web},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963322,
author = {Nambiar, Ullas B. and SUbramaniam, L. Venkata},
title = {Eighth Workshop on Information Integration on the Web (IIWeb 2011)},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963322},
doi = {10.1145/1963192.1963322},
abstract = {The goal of the 8th Workshop on Information Integration on the Web (IIWeb) is to bring together academic researchers and industry practitioners in Information Integration with a special focus on integrating cyber physical systems for building a sustainable ecosystem for life on our planet. Towards this goal the workshop program consists of an engaging set of talks and papers.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {301–302},
numpages = {2},
keywords = {cyber-physical systems, information integration},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963323,
author = {Bizer, Christian and Heath, Tom and Berners-Lee, Tim and Hausenblas, Michael},
title = {4th Linked Data on the Web Workshop (LDOW2011)},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963323},
doi = {10.1145/1963192.1963323},
abstract = {The Web has developed into a global information space consisting not just of linked documents, but also of Linked Data. In 2010, we have seen significant growth in the size of the Web of Data, as well as in the number of communities contributing to its creation. In addition to publishing and interlinking datasets, there is intensive work on developing Linked Data browsers, Linked Data crawlers, Web of Data search engines and other applications that consume Linked Data from the Web.The goal of the 4th Linked Data on the Web workshop (LDOW2011) is to provide a forum for exposing high quality research on Linked Data as well as to showcase innovative Linked Data applications. In addition, by bringing together researchers in this field, we expect the event to further shape the Linked Data research agenda.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {303–304},
numpages = {2},
keywords = {web of data, RDF, URI, linked data, HTTP, semantic web, dataspaces},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963324,
author = {Berendt, Bettina and Hollink, Laura and Hollink, Vera and Luczak-R\"{o}sch, Markus and M\"{o}ller, Knud and Vallet, David},
title = {USEWOD2011: 1st International Workshop on Usage Analysis and the Web of Data},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963324},
doi = {10.1145/1963192.1963324},
abstract = {The USEWOD2011 workshop investigates combinations of usage data with semantics and the web of data. The analysis of usage data may be enhanced using semantic information. Now that more and more explicit knowledge is represented on the Web, the question arises how these semantics can be used to aid large scale web usage analysis and mining.Conversely, usage data analysis can enhance semantic resources as well as Semantic Web applications. Traces of users can be used to evaluate, adapt or personalize Semantic Web applications. Also, new ways of accessing information enabled by the Web of Data imply the need to develop or adapt algorithms, methods, and techniques to analyze and interpret the usage of Web data instead of Web pages.The USEWOD2011 program includes a challenge to the workshop participants: three months before the workshop two datasets consisting of server log files of Linked Open Data sources were released. Participants are invited to come up with interesting analyses, applications, alignments, etc. for these datasets.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {305–306},
numpages = {2},
keywords = {usage data, data mining, web of data},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963325,
author = {Baeza-Yates, Ricardo and Masan\`{e}s, Julien and Spaniol, Marc},
title = {The 1st Temporal Web Analytics Workshop (TWAW)},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963325},
doi = {10.1145/1963192.1963325},
abstract = {The objective of the 1st Temporal Web Analytics Workshop (TWAW) is to provide a venue for researchers of all domains (IE/IR, Web mining etc.) where the temporal dimension opens up an entirely new range of challenges and possibilities. The workshop's ambition is to help shaping a community of interest on the research challenges and possibilities resulting from the introduction of the time dimension in Web analysis. The maturity of the Web, the emergence of large scale repositories of Web material, makes this very timely and a growing sets of research and services (recorded future1, truthy2 launched just in the last months) are emerging that have this focus in common. Having a dedicated workshop will help, we believe, to take a rich and cross-domain approach to this new research challenge with a strong focus on the temporal dimension.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {307–308},
numpages = {2},
keywords = {distributed data analytics, temporal web analytics, web scale data analytics},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963326,
author = {Jaimes, Alejandro and Lalmas, Mounia and Volkovich, Yana},
title = {First International Workshop on Social Media Engagement (SoME 2011)},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963326},
doi = {10.1145/1963192.1963326},
abstract = {The goal of this workshop is to encourage discussion and sharing of ideas and research results on social media engagement. We aim to promote interdisciplinary research and exchange of ideas in this area, not only between industry and academia, but also between different fields (e.g., computer science, mathematics, physics, psychology, sociology, cultural anthropology, etc.). In particular, we would like to discuss approaches to address some of the serious research challenges we face in devising engagement metrics, in developing methodologies, and in understanding how different technical approaches can be used to enhance our understanding of user behavior in social media.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {309–310},
numpages = {2},
keywords = {metrics, user engagement, social media},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963327,
author = {Pautasso, Cesare and Wilde, Erik and Alarcon, Rosa},
title = {Second International Workshop on RESTful Design (WS-REST 2011)},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963327},
doi = {10.1145/1963192.1963327},
abstract = {Over the past few years, the discussion between the two major architectural styles for designing and implementing Web services, the RPC-oriented approach and the resource-oriented approach, has been mainly held outside of traditional research communities. Mailing lists, forums and developer communities have seen long and fascinating debates around the assumptions, strengths, and weaknesses of these two approaches. The Second International Workshop on RESTful Design (WS-REST 2011) has the goal of getting more researchers involved in the debate by providing a forum where discussions around the resource-oriented style of Web services design take place. Representational State Transfer (REST) is an architectural style and as such can be applied in different ways, can be extended by additional constraints, or can be specialized with more specific interaction patterns. WS-REST is the premier forum for discussing research ideas, novel applications and results centered around REST at the World Wide Web conference, which provides a great setting to host this second edition of the workshop dedicated to research on the architectural style underlying the Web.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {311–312},
numpages = {2},
keywords = {REST, HTTP, web services, web architecture},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963328,
author = {Castillo, Carlos and Gyongyi, Zoltan and Jatowt, Adam and Tanaka, Katsumi},
title = {Joint WICOW/AIRWeb Workshop on Web Quality (WebQuality 2011)},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963328},
doi = {10.1145/1963192.1963328},
abstract = {In this paper we overview the Joint WICOW/AIRWeb Workshop on Web Quality (WebQuality 2011) that was held in conjunction with the 20th International World Wide Web Conference in Hyderabad, India.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {313–314},
numpages = {2},
keywords = {web information credibility, adversarial information retrieval, web information quality, spam deetction},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963329,
author = {Tran, Thanh and Mika, Peter and Wang, Haofen and Grobelnik, Marko},
title = {SemSearch'11: The 4th Semantic Search Workshop},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963329},
doi = {10.1145/1963192.1963329},
abstract = {The use of semantics and semantic technologies for search and retrieval has attracted interests both from academia and industry in recent years. What is now commonly known as Semantic Search is in fact a broad field encompassing ideas and concepts from different areas, including Information Retrieval, Semantic Web and database. This is the fourth edition of the Semantic Search workshop which aims to bring together researchers and practitioners from various communities, to provide a forum for dissemination, discussion, and for the exchange and transfer of knowledge related to the use of semantics for search and retrieval. This year's workshop will continue to push and promote efforts towards an evaluation benchmark for Semantic Search systems.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {315–316},
numpages = {2},
keywords = {data retrieval, semantic search, document retrieval, information retrieval},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963330,
author = {Denecke, Kerstin and Dolog, Peter},
title = {Second International Workshop on Web Science and Information Exchange in the Medical Web (MedEx 2011)},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963330},
doi = {10.1145/1963192.1963330},
abstract = {The amount of Social Media Data dealing with medical and health issues increased significantly in the last couple of years. Medical Social Media Data now provides a new source of information within information gaining contexts. Facts, experiences, opinions or information on behavior can be found in the Medicine or Health 2.0 and could support a broad range of applications. This workshop is devoted to the technologies for dealing with social- and multi media for medical information gathering and exchange. This specific data and the processes of information gathering poses many challenges given the increasing content on the Web and the trade off of filtering noise at the cost of losing information which is potentially relevant.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {317–318},
numpages = {2},
keywords = {web science, social web},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963331,
author = {Simperl, Elena and Madalli, Devika P. and Vrande\v{c}i\'{c}, Denny and Alfonseca, Enrique},
title = {DiversiWeb 2011: First International Workshop on Knowledge Diversity on the Web},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963331},
doi = {10.1145/1963192.1963331},
abstract = {The workshop provides an interdisciplinary forum for researchers and practitioners to present and discuss their ideas related to the challenges posed by diversity on the Web. We address a wide array of interdisciplinary questions, which need to be tackled in order to preserve the fragile balance between a world that is continually converging and growing together, the rich diversity of the global society, and the dangers of fragmentation and splintering.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {319–320},
numpages = {2},
keywords = {diversity},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963332,
author = {Antin, Judd and Churchill, Elizabeth F. and Chen, Bee-Chung},
title = {Workshop on Online Reputation: Context, Privacy, and Reputation Management},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963332},
doi = {10.1145/1963192.1963332},
abstract = {In this workshop we bring together researchers and practitioners from diverse disciplines to discuss the future of online reputation systems. Our goal is to combine social and technical perspectives to address three challenges: (1) the social challenges around reputation, privacy, and online identity, (2) the technical challenges around designing adaptable reputation systems which cater to users' privacy concerns, and (3) the user experience challenges around transparency and the design or reputation management tools.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {321–322},
numpages = {2},
keywords = {reputation systems, privacy, reputation management, reputation},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963333,
author = {Siorpaes, Katharina and Simperl, Elena and Ghosh, Arpita and Fink, Michael},
title = {PlayIT 2011: First International Workshop on Games for Knowledge Acquisition},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963333},
doi = {10.1145/1963192.1963333},
abstract = {Many problems in knowledge acquisition, such as image labeling, still rely on extensive human input and intervention. In order to attract people to invest the necessary time into such tasks, rewarding incentives and motivation mechanisms have been employed. While recruting "human cycles" for such tasks is difficult, online games manage to attract plenty of attention (due to the fact that they provide inherent incentives, such as fun and competition). This workshop focuses on games that embed various knowledge acquisition tasks into the context of online games, with the end goal of attracting the sufficient manual labor.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {323–324},
numpages = {2},
keywords = {games},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963335,
author = {Paritosh, Praveen and Ipeirotis, Panos and Cooper, Matt and Suri, Siddharth},
title = {The Computer is the New Sewing Machine: Benefits and Perils of Crowdsourcing},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963335},
doi = {10.1145/1963192.1963335},
abstract = {There is increased participation by the developing world in the global manufacturing marketplace: the sewing machine in Bangladesh can be a means to support an entire family. Crowdsourcing for cognitive tasks consists of asking humans for questions that are otherwise impossible to answer by algorithms, e.g., is this image pornographic, are these two addresses the same, what is the translation for this text in French? In the last five years, there has been an exponential growth in the size of the global cognitive marketplace: Amazon.com's Mechanical Turk has an estimated 500,000 active workers in over 100 countries, and there are dozens of other companies in this space. This turns the computer into a modern-day sewing machine, where cognitive work of various levels of difficulty will pay anywhere from 5 to 50 dollars a day. Unlike outsourcing, which usually requires college education, competence at these tasks might be a month or even less of training. At its best, this could be a powerful bootstrap for a billion people. At its worst, this can lead to unprecedented exploitation. In this panel, we discuss the technical, social and economic questions and implications that a global cognitive marketplace raises.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {325–326},
numpages = {2},
keywords = {crowdsourcing, WWW 2011 panel, mechanical turk},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963336,
author = {El Abaddi, Amr and Backstrom, Lars and Chakrabarti, Soumen and Jaimes, Alejandros and Leskovec, Jure and Tomkins, Andrew},
title = {Social Media: Source of Information or Bunch of Noise},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963336},
doi = {10.1145/1963192.1963336},
abstract = {Social media has witnessed an explosive growth in the past few years. Wikipedia has over 3.5 million pages with descriptions of entities. Flickr members have uploaded over 5 billion photos,You Tube has 35 hours of videos uploaded to the site each minute, and Twitter users generate 65 million tweets a day. While some forms of social media like Wikipedia clearly have valuable information embedded in them, the jury is still out on other forms like tweets, comments, and social network (e.g., Facebook) updates. Some of the key questions that the panel will debate include: Is there useful information in social media like tweets? How to extract structured records from unstructured user-generated content like reviews? How to sift through the vast amounts of social media and filter out the spam/offensive content? How to rank social media like blogs and comments based on relevance or importance?How can social media be leveraged to achieve tasks like entity disambiguation, question answering, improved search, etc.? What are the novel Web applications where social media can be leveraged?},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {327–328},
numpages = {2},
keywords = {WWW 2011 panel, Twitter and Wikipedia, social media},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963337,
author = {Rastogi, Rajeev and Cutrell, Ed and Gupta, Manish and Jhunjhunwala, Ashok and Narayan, Ramkumar and Sanghal, Rajeev},
title = {Connecting the next Billion Web Users},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963337},
doi = {10.1145/1963192.1963337},
abstract = {With 2 billion users, the World Wide Web has indeed come a long way. However, of the 4.8 billion people living in Asia and Africa, only 1 in 5 has access to the Web. For instance, in India, the 100 million Web users constitute less than 10% of the total population of 1.2 billion. So it is universally accepted that the next billion users will come from emerging markets like Brazil, China, India, Indonesia and Russia. Emerging markets have a number of unique characteristics: Large dense populations with low incomes, Lack of infrastructure in terms of broadband, electricity, etc., Poor PC penetration due to limited affordability, High illiteracy rates and inability to read/write, Plethora of local languages and dialects, General paucity of local content, especially in local languages, Explosive growth in the number of mobile phones. The panel will debate the various technical challenges in overcoming the digital divide, and potential approaches to bring the Web to the underserved populations of the developing world.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {329–330},
numpages = {2},
keywords = {world wide web, www 2011 panel, emerging markets},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963339,
author = {Bai, Xi},
title = {Addressing the RDFa Publishing Bottleneck},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963339},
doi = {10.1145/1963192.1963339},
abstract = {In the more dynamic environments emerging from ad hoc and peer-to-peer networks, our research has explored the extent to which Web-based knowledge sharing as well as community formation require automation to understand human-readable content in a more distributed manner. RDFa is a syntactic format which can leverage this issue by allowing machine-readable data to be easily integrated into XHTML Web pages. Although there is a growing number of tools and techniques for generating and distilling RDFa, comparatively little work has been carried out on publishing existing RDF data sets as an XHTML+RDFa serialization. This paper proposes a generic approach to integrating RDF data into Web pages using the concept of automatically discovered "topic nodes". RDFa² is a proof-of-concept implementation of this approach and provides an on-line service assisting users in generating and personalizing pages with RDFa. We provide experimental results that support the viability of our approach to generating Web documents such as FOAF-based online profiles as well as RDF vocabularies with little user intervention.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {331–336},
numpages = {6},
keywords = {rdfa, semantic enhancement, linked data, federated markup},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963340,
author = {Bonetta, Daniele and Pautasso, Cesare},
title = {Towards Liquid Service Oriented Architectures},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963340},
doi = {10.1145/1963192.1963340},
abstract = {The advent of Cloud computing platforms, and the growing pervasiveness of Multicore processor architectures have revealed the inadequateness of traditional programming models based on sequential computations, opening up many challenges for research on parallel programming models for building distributed, service-oriented systems. More in detail, the dynamic nature of Cloud computing and its virtualized infrastructure pose new challenges in term of application design, deployment and dynamic reconfiguration. An application developed to be delivered as a service in the Cloud has to deal with poorly understood issues such as elasticity, infinite scalability and portability across heterogeneous virtualized environments. In this position paper we define the problem of providing a novel parallel programming model for building application services that can be transparently deployed on multicore and cloud execution environments. To this end, we introduce and motivate a research plan for the definition of a novel programming framework for Web service-based applications. Our vision called "Liquid Architecture" is based on a programming model inspired by core ideas tied to the REST architectural style coupled with a self-configuring runtime that allows transparent deployment of Web services on a broad range of heterogeneous platforms, from multicores to clouds.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {337–342},
numpages = {6},
keywords = {performance, liquid architectures, web services, rest, programming models},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963341,
author = {Das, Dipankar},
title = {Analysis and Tracking of Emotions in English and Bengali Texts: A Computational Approach},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963341},
doi = {10.1145/1963192.1963341},
abstract = {The present discussion highlights the aspects of an ongoing doctoral thesis grounded on the analysis and tracking of emotions from English and Bengali texts. Development of lexical resources and corpora meets the preliminary urgencies. The research spectrum aims to identify the evaluative emotional expressions at word, phrase, sentence, and document level granularities along with their associated holders and topics. Tracking of emotions based on topic or event was carried out by employing sense based affect scoring techniques. The labeled emotion corpora are being prepared from unlabeled examples to cope with the scarcity of emotional resources, especially for the resource constraint language like Bengali. Different unsupervised, supervised and semi-supervised strategies, adopted for coloring each outline of the research spectrum produce satisfactory outcomes},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {343–348},
numpages = {6},
keywords = {crf, expression, topic, holder, tracking, bengwal, syntactic argument structure, emotions, blogs, svm},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963342,
author = {Dave, Kushal S.},
title = {Computational Advertising: Leveraging User Interaction &amp; Contextual Factors for Improved Ad Retrieval &amp; Ranking},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963342},
doi = {10.1145/1963192.1963342},
abstract = {Computational advertising, popularly known as Online advertising or Web advertising, refers to finding the most relevant ads matching a particular context on the web. It is a scientific sub-discipline at the intersection of information retrieval, statistical modeling, machine learning, optimization, large scale search and text analysis. The core problem attacked in computational advertising (CA) is of the match making between the ads and the context. Based on the context, CA can be broadly compartmentalized into following three areas: Sponsored search, Contextual advertising and Social advertising. Sponsored search refers to the placement of ads on search results page. Contextual advertising deals with matching advertisements to the third party web pages. We refer the placements of ads on a social networking page, leveraging user's social contacts as social advertising.My research work aims at leveraging various user interactions, ad and advertiser related information and contextual information for these three areas of advertising. The research work focuses on the identification of various factors that contribute in retrieving and ranking the most relevant set of ads that match best with the context. Specifically, information associated with the user, publisher and advertiser is leveraged for this purpose.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {349–354},
numpages = {6},
keywords = {direct marketing, advertising, social networks, sponsored search, viral marketing, contextual advertising},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963343,
author = {Dipple, Aiden Charles},
title = {Standing on the Shoulders of Ants: Stigmergy in the Web},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963343},
doi = {10.1145/1963192.1963343},
abstract = {Stigmergy is a biological term used when discussing insect or swarm behaviour, and describes a model supporting environmental communication separately from artefacts or agents. This phenomenon is demonstrated in the behavior of ants and their food gathering process when following pheromone trails, or similarly termites and their termite mound building process. What is interesting with this mechanism is that highly organized societies are achieved without an apparent management structure.Stigmergic behavior is implicit in the Web where the volume of users provides a self-organizing and self-contextualization of content in sites which facilitate collaboration. However, the majority of content is generated by a minority of the Web participants. A significant contribution from this research would be to create a model of Web stigmergy, identifying virtual pheromones and their importance in the collaborative process.This paper explores how exploiting stigmergy has the potential of providing a valuable mechanism for identifying and analyzing online user behavior recording actionable knowledge otherwise lost in the existing web interaction dynamics. Ultimately this might assist our building better collaborative Web sites.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {355–360},
numpages = {6},
keywords = {web collaboration, virtual pheromones, stigmergy},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963344,
author = {K., Parthasarathy and Kumar, Sreenivasa P. and Damien, Dominic},
title = {Ranked Answer Graph Construction for Keyword Queries on RDF Graphs without Distance Neighbourhood Restriction},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963344},
doi = {10.1145/1963192.1963344},
abstract = {RDF and RDFS have recently become very popular as frameworks for representing data and meta-data in form of a domain description, respectively. RDF data can also be thought of as graph data. In this paper, we focus on keyword-based querying of RDF data. In the existing approaches for answering such keyword queries, keywords are mapped to nodes in the graph and their neighborhoods are explored to extract subgraph(s) of the data graph that contain(s) information relevant to the query. In order to restrict the computational effort, a fixed distance bound is used to define the neighborhoods of nodes. In this paper we present an elegant algorithm for keyword query processing on RDF data that does not assume such a fixed bound. The approach adopts a pruned exploration mechanism where closely related nodes are identified, subgraphs are pruned and joined using suitable hook nodes. The system dynamically manages the distance depending on the closeness between the keywords. The working of the algorithm is illustrated using a fragment of AIFB institute data represented as an RDF graph.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {361–366},
numpages = {6},
keywords = {rdfs, answer graph, keyword search, rdf},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963345,
author = {Kumar, Ritesh},
title = {A Politeness Recognition Tool for Hindi: With Special Emphasis on Online Texts},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963345},
doi = {10.1145/1963192.1963345},
abstract = {This paper gives an overview of a politeness recognition tool (PoRT) for Hindi that is currently under preparation. It describes the the kind of problems that need to be tackled with before developing the tool, the approach and the methodology that will be adopted for the development and testing of the tool, the current progress and the future plan to achieve this goal.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {367–372},
numpages = {6},
keywords = {cmc, port, co3h, hybrid system, linguistic politeness},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963346,
author = {Mahanti, Aniket},
title = {Measurement and Analysis of Cyberlocker Services},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963346},
doi = {10.1145/1963192.1963346},
abstract = {Cyberlocker Services (CLS) such as RapidShare and Megaupload have recently become popular. The decline of Peer-to-Peer (P2P) file sharing has prompted various services including CLS to replace it. We propose a comprehensive multi-level characterization of the CLS ecosystem. We answer three research questions: (a) what is a suitable measurement infrastructure for gathering CLS workloads; (b) what are the characteristics of the CLS ecosystem; and (c) what are the implications of CLS on Web 2.0 (and the Internet). To the best of our knowledge, this work is the first to characterize the CLS ecosystem. The work will highlight the content, usage, performance, infrastructure, quality of service, and evolution characteristics of CLS.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {373–378},
numpages = {6},
keywords = {file hosting services, rapidshare, web 2.0, measurement, cyberlockers, performance},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963347,
author = {Mangalampalli, Ashish and Pudi, Vikram},
title = {Fuzzy Associative Rule-Based Approach for Pattern Mining and Identification and Pattern-Based Classification},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963347},
doi = {10.1145/1963192.1963347},
abstract = {Associative Classification leverages Association Rule Mining (ARM) to train Rule-based classifiers. The classifiers are built on high quality Association Rules mined from the given dataset. Associative Classifiers are very accurate because Association Rules encapsulate all the dominant and statistically significant relationships between items in the dataset. They are also very robust as noise in the form of insignificant and low-frequency itemsets are eliminated during the mining and training stages. Moreover, the rules are easy-to-comprehend, thus making the classifier transparent.Conventional Associative Classification and Association Rule Mining (ARM) algorithms are inherently designed to work only with binary attributes, and expect any quantitative attributes to be converted to binary ones using ranges, like "Age = [25, 60]". In order to mitigate this constraint, Fuzzy logic is used to convert quantitative attributes to fuzzy binary attributes, like "Age = Middle-aged", so as to eliminate any loss of information arising due to sharp partitioning, especially at partition boundaries, and then generate Fuzzy Association Rules using an appropriate Fuzzy ARM algorithm. These Fuzzy Association Rules can then be used to train a Fuzzy Associative Classifier. In this paper, we also show how Fuzzy Associative Classifiers so built can be used in a wide variety of domains and datasets, like transactional datasets and image datasets.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {379–384},
numpages = {6},
keywords = {associative classification, association rule mining, fuzzy pre-processing, fuzzy logic},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963348,
author = {Mehta, Hemant Kumar and Kanungo, Priyesh and Chandwani, Manohar},
title = {Performance Enhancement of Scheduling Algorithms in Clusters and Grids Using Improved Dynamic Load Balancing Techniques},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963348},
doi = {10.1145/1963192.1963348},
abstract = {This paper describes the research work done for during PhD study. Cluster computing, grid computing and cloud computing are distributed computing environments (DCEs) widely accepted for the next generation Web based commercial and scientific applications. These applications work around the globally distributed data of petabyte scale that can only be processed by the aggregating the capability of globally distributed resources. The resource management and process scheduling in large scale distributed computing environment are a challenging task. In this research work we have devised new scheduling algorithms and resource management strategies specially designed for the cluster and grid cloud and peer-to-peer computing. The research work finally presented the distributed computing solutions to one scientific and one commercial application viz. e-Learning and data mining.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {385–390},
numpages = {6},
keywords = {cloud, grid, resource management, trust management, cluster, grid service},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963349,
author = {Mola-Velasco, Santiago M.},
title = {Wikipedia Vandalism Detection},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963349},
doi = {10.1145/1963192.1963349},
abstract = {Wikipedia is an online encyclopedia that anyone can access and edit. It has become one of the most important sources of knowledge online and many third party projects rely on it for a wide-range of purposes. The open model of Wikipedia allows pranksters, lobbyists and spammers to attack the integrity of the encyclopedia and this endangers it as a public resource. This is known in the community as vandalism.A plethora of methods have been developed within the Wikipedia and the scientific community to tackle this problem. We have participated in this effort and developed one of the leading approaches. Our research aims to create a fully-working antivandalism system and get it working in the real world.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {391–396},
numpages = {6},
keywords = {natural language processing, Wikipedia vandalism detection, machine learning, reputation},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963350,
author = {Nath, Swaprava},
title = {Dynamic Learning-Based Mechanism Design for Dependent Valued Exchange Economies},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963350},
doi = {10.1145/1963192.1963350},
abstract = {Learning private information from multiple strategic agents poses challenge in many Internet applications. Sponsored search auctions, crowdsourcing, Amazon's mechanical turk, various online review forums are examples where we are interested in learning true values of the advertisers or true opinion of the reviewers. The common thread in these decision problems is that the optimal outcome depends on the private information of all the agents, while the decision of the outcome can be chosen only through reported information which may be manipulated by the strategic agents. The other important trait of these applications is their dynamic nature. The advertisers in an online auction or the users of mechanical turk arrive and depart, and when present, interact with the system repeatedly, giving the opportunity to learn their types. Dynamic mechanisms, which learn from the past interactions and make present decisions depending on the expected future evolution of the game, has been shown to improve performance over repeated versions of static mechanisms. In this paper, we will survey the past and current state-of-the-art dynamic mechanisms and analyze a new setting where the agents consist of buyers and sellers, known as exchange economies, and agents having value interdependency, which are relevant in applications illustrated through examples. We show that known results of dynamic mechanisms with independent value settings cannot guarantee certain desirable properties in this new significantly different setting. In the future work, we propose to analyze similar settings with dynamic types and population.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {397–402},
numpages = {6},
keywords = {individual rationality, incentive compatibility, nash equilibrium},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963351,
author = {Orimaye, Sylvester Olubolu},
title = {Sentence-Level Contextual Opinion Retrieval},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963351},
doi = {10.1145/1963192.1963351},
abstract = {Existing opinion retrieval techniques do not provide context-dependent relevant results. Most of the approaches used by state-of-the-art techniques are based on frequency of query terms, such that all documents containing query terms are retrieved, regardless of contextual relevance to the intent of the human seeking the opinion. However, in a particular opinionated document, words could occur in different contexts, yet meet the frequency attached to a certain opinion threshold, thus explicitly creating a bias in overall opinion retrieved. In this paper we propose a sentence-level contextual model for opinion retrieval using grammatical tree derivations and approval voting mechanism. Model evaluation performed between our contextual model, BM25, and language model shows that the model can be effective for contextual opinion retrieval such as faceted opinion retrieval.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {403–408},
numpages = {6},
keywords = {sentence level, grammatical tree derivation, contextual opinion, approval voting},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963352,
author = {Sellers, Andrew Jon},
title = {The OXPath to Success in the Deep Web},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963352},
doi = {10.1145/1963192.1963352},
abstract = {The world wide web provides access to a wealth of data. Collecting and maintaining such large amounts of data necessitates automated processing for extraction, since appropriate automation can perform extraction tasks that would be otherwise infeasible. Modern web interfaces, however, are generally designed primarily for human users, delivering sophisticated interactions through the use of client-side scripting and asynchronous server communication. To this end, we introduce OXPath, a careful extension of XPath that facilitates data extraction from the deep web. OXPath exploits XPath's familiarity and theoretical foundations. OXPath, then, achieves favourable evaluation complexity and optimal page buffering, storing only a constant number of pages for non-recursive queries. Further, OXPath provides a lightweight interface, which is easy to use and embed. This paper outlines the motivation, theoretical framework, current implementation, and preliminary results obtained so far. We conclude with proposed future work on OXPath, including an investigation of how to deploy OXPath efficiently in a highly elastic computing framework (cloud).},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {409–414},
numpages = {6},
keywords = {AJAX, web automation, web extraction, XPath},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963353,
author = {Shi, Wenxuan and Xie, Maoqiang and Huang, Yalou},
title = {Cooperative Anti-Spam System Based on Multilayer Agents},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963353},
doi = {10.1145/1963192.1963353},
abstract = {Spam is unsolicited bulk email which is extremely annoying to the recipients and the ISPs. However, most of the traditional spam filtering methods commonly neglect the bulk character of spam. This paper proposes a model of cooperative anti-spam system based on multilayer agents. We compared our model to the state-of-the-art and found that our model achieved better performance and robustness on several known corpora.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {415–420},
numpages = {6},
keywords = {multilayer agent, shingle, fingerprint, cooperative anti-spam system, spam filtering},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963354,
author = {Sinha, Pinaki},
title = {Summarization of Archived and Shared Personal Photo Collections},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963354},
doi = {10.1145/1963192.1963354},
abstract = {The volume of personal photos hosted on photo archives and social sharing platforms has been increasing exponentially. It is difficult to get an overview of a large collection of personal photos without browsing though the entire database manually. In this research, we propose a framework to generate representative subset summaries from photo collections hosted on web archives or social networks. We define salient properties of an effective photo summary and model summarization as an optimization of these properties, given the size constraints. We also introduce metrics for evaluating photo summaries based on their information content and the ability to satisfy user's information needs. Our experiments show that our summarization framework performs better than baseline algorithms.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {421–426},
numpages = {6},
keywords = {personal photos, optimization, social networks, summarization},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963355,
author = {Verborgh, Ruben and Van de Walle, Rik},
title = {Application of Semantic Web Technologies for Multimedia Interpretation},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963355},
doi = {10.1145/1963192.1963355},
abstract = {Despite numerous outstanding results, highly complex and specialized multimedia algorithms have not been able to fulfill the promise of fully automated multimedia interpretation. An essential problem is that they are insufficiently aware of the context they operate in. Algorithms that do take a form of context in consideration, often function in a domain-specific environment. The generic framework proposed in this paper stimulates algorithm collaboration on an interpretation task by continuously actualizing the context of the multimedia item under interpretation. Semantic Web knowledge, combined with reasoning methods, forms the corner stone of the integration of these various interacting agents. We believe that this framework will enable an advanced interpretation of multimedia data that goes beyond the capabilities of individual algorithms. A basic platform implementation already indicates the potential of the concept, clearing the path for even more complex interpretation scenarios.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {427–432},
numpages = {6},
keywords = {semantic web, reasoning, service composition, multimedia annotation, feature extraction},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963357,
author = {Sharma Grover, Aditi and Barnard, Etienne},
title = {The Lwazi Community Communication Service: Design and Piloting of a Voice-Based Information Service},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963357},
doi = {10.1145/1963192.1963357},
abstract = {We present the design, development and pilot process of the Lwazi Community Communication Service (LCCS), a multilingual automated telephone-based information service. The service acts as a communication and dissemination tool that enables managers at local community centres to broadcast information (e.g. health, employment, social grants) to community workers and the communities they serve. The LCCS allows the recipients to obtain up-to-date, relevant information in a timely and efficient manner, overcoming the obstacles of transportation, time and costs incurred in trying to physically obtain information from the community centres. We discuss our experiences and fieldwork in piloting the LCCS at six locations nationally in the eleven official South African languages. We analyze the usage pattern from the pilot call logs and thereafter discuss the implications of these findings for future projects that design similar automated services for serving rural communities in developing world regions.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {433–442},
numpages = {10},
keywords = {literacy, mobile, spoken dialogue systems, voice user interfaces. ictd, developing regions, rural, speech technologies},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963358,
author = {Chen, Jay and Hutchful, David and Thies, William and Subramanian, Lakshminarayanan},
title = {Analyzing and Accelerating Web Access in a School in Peri-Urban India},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963358},
doi = {10.1145/1963192.1963358},
abstract = {While computers and Internet access have growing penetration amongst schools in the developing world, intermittent connectivity and limited bandwidth often prevent them from being fully utilized by students and teachers. In this paper, we make two contributions to help address this problem. First, we characterize six weeks of HTTP traffic from a primary school outside of Bangalore, India, illuminating opportunities and constraints for improving performance in such settings. Second, we deploy an aggressive caching and prefetching engine and show that it accelerates a user's overall browsing experience (apart from video content) by 2.8x. Our accelerator leverages innovative techniques that have been proposed, but not evaluated in detail, including the effectiveness of serving stale pages, cached page highlighting, and client-side prefetching. Unlike proxy-based techniques, our system is bundled as an open-source Firefox plugin and runs directly on client machines. This allows easy installation and configuration by end users, which is especially important in developing regions where a lack of permissions or technical expertise often prevents modification of internal network settings.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {443–452},
numpages = {10},
keywords = {browser extension, connectivity, web acceleration},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963359,
author = {Chen, Jay and Power, Russell and Subramanian, Lakshminarayanan and Ledlie, Jonathan},
title = {Design and Implementation of Contextual Information Portals},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963359},
doi = {10.1145/1963192.1963359},
abstract = {This paper presents a system for enabling offline web use to satisfy the information needs of disconnected communities. We describe the design, implementation, evaluation, and pilot deployment of an automated mechanism to construct Contextual Information Portals (CIPs). CIPs are large searchable information repositories of web pages tailored to the information needs of a target population. We combine an efficient classifier with a focused crawler to gather the web pages for the portal for any given topic. Given a set of topics of interest, our system constructs a CIP containing the most relevant pages from the web across these topics. Using several secondary school course syllabi, we demonstrate the effectiveness of our system for constructing CIPs for use as an education resource. We evaluate our system across several metrics: classification accuracy, crawl scalability, crawl accuracy and harvest rate. We describe the utility and usability of our system based on a preliminary deployment study at an after-school program in India, and also outline our ongoing larger-scale pilot deployment at five schools in Kenya.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {453–462},
numpages = {10},
keywords = {document classification, offline, focused crawling, web portal},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963360,
author = {Chakraborty, Sunandan and Subramanian, Lakshminarayanan},
title = {Location Specific Summarization of Climatic and Agricultural Trends},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963360},
doi = {10.1145/1963192.1963360},
abstract = {Climate change can directly impact agriculture. Failure in different aspects of agriculture due to climate change and other influencing factors, are extremely rampant in several agrarian economies, most of which go unnoticed. In this paper, we describe the design of a system that mines disparate information sources on the Web to automatically summarize important climatic and agricultural trends for any specific location and construct a location-specific climatic and agricultural information portal. We have evaluated the system across 605 different districts in India. The results revealed a pan-India scenario of different problem affected areas. The key findings from this work include, around 64.58% of the districts of India suffer from soil related issues and 76.02% have water related problems. We have also manually validated the authenticity of our information sources and validated our summarized results for specific locations with findings in reputed journals and authoritative sources.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {463–472},
numpages = {10},
keywords = {agriculture, emerging region, web service, climate},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963361,
author = {Isaacman, Sibren and Martonosi, Margaret},
title = {Low-Infrastructure Methods to Improve Internet Access for Mobile Users in Emerging Regions},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963361},
doi = {10.1145/1963192.1963361},
abstract = {As information technology supports more aspects of modern life, digital access has become an important tool for developing regions to lift themselves from poverty. Though broadband internet connectivity will not be universally available in the short-term, widely-employed mobile devices coupled with novel delay-tolerant networking do allow limited forms of connectivity. This paper explores the design space for internet access systems operating with constrained connectivity. Our starting point is C-LINK, a collaborative caching system that enhances the performance of interactive web access over DTN and cellular connectivity. We discuss our experiences and results from deploying C-LINK in Nicaragua, before moving on to a broader design study of other issues that further influence operation. We consider the impact of (i) storing web content collaboratively cached across all user nodes, (ii) hybrid transport layers exploiting the best attributes of limited cellular and DTN-style connectivity. We also explore the behavior of future systems under a range of usage and mobility scenarios. Even under adverse conditions, our techniques can improve average service latency for page requests by a factor of 2X. Our results point to the considerable power of leveraging user mobility and collaboration in providing very-low-infrastructure internet access to developing regions.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {473–482},
numpages = {10},
keywords = {simulation, caching, mobility, delay tolerant networking},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963362,
author = {Agrawal, Rakesh and Gollapudi, Sreenivas and Kannan, Anitha and Kenthapadi, Krishnaram},
title = {Identifying Enrichment Candidates in Textbooks},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963362},
doi = {10.1145/1963192.1963362},
abstract = {Many textbooks written in emerging countries lack clear and adequate coverage of important concepts. We propose a technological solution for algorithmically identifying those sections of a book that are not well written and could benefit from better exposition. We provide a decision model based on the syntactic complexity of writing and the dispersion of key concepts. The model parameters are learned using a tune set which is algorithmically generated using a versioned authoritative web resource as a proxy. We evaluate the proposed methodology over a corpus of Indian textbooks which demonstrates its effectiveness in identifying enrichment candidates.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {483–492},
numpages = {10},
keywords = {textbooks, readability, concepts, education, dispersion},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963363,
author = {Johnson, David L. and Pejovic, Veljko and Belding, Elizabeth M. and van Stam, Gertjan},
title = {Traffic Characterization and Internet Usage in Rural Africa},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963363},
doi = {10.1145/1963192.1963363},
abstract = {While Internet connectivity has reached a significant part of the world's population, those living in rural areas of the developing world are still largely disconnected. Recent efforts have provided Internet connectivity to a growing number of remote locations, yet Internet traffic demands cause many of these networks to fail to deliver basic quality of service needed for simple applications. For an in-depth investigation of the problem, we gather and analyze network traces from a rural wireless network in Macha, Zambia. We supplement our analysis with on-site interviews from Macha, Zambia and Dwesa, South Africa, another rural community that hosts a local wireless network. The results reveal that Internet traffic in rural Africa differs significantly from the developed world. We observe dominance of web-based traffic, as opposed to peer-to-peer traffic common in urban areas. Application-wise, online social networks are the most popular, while the majority of bandwidth is consumed by large operating system updates. Our analysis also uncovers numerous network anomalies, such as significant malware traffic. Finally, we find a strong feedback loop between network performance and user behavior. Based on our findings, we conclude with a discussion of new directions in network design that take into account both technical and social factors.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {493–502},
numpages = {10},
keywords = {internet usage, interviews, rural networks},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/1963192.1963364,
author = {Ajmera, Jitendra and Joshi, Anupam and Mukherjea, Sougata and Rajput, Nitendra and Sahay, Shrey and Shrivastava, Mayank and Srivastava, Kundan},
title = {Two-Stream Indexing for Spoken Web Search},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963364},
doi = {10.1145/1963192.1963364},
abstract = {This paper presents two-stream processing of audio to index the audio content for Spoken Web search. The first stream indexes the meta-data associated with a particular audio document. The meta-data is usually very sparse, but accurate. This therefore results in a high-precision, low-recall index. The second stream uses a novel language-independent speech recognition to generate text to be indexed. Owing to the multiple languages and the noise in user generated content on the Spoken Web, the speech recognition accuracy of such systems is not high, thus they result in a low-precision, high-recall index. The paper attempts to use these two complementary streams to generate a combined index to increase the precision-recall performance in audio content search.The problem of audio content search is motivated by the real world implication of the Web in developing regions, where due to literacy and affordability issues, people use Spoken Web which consists of interconnected VoiceSites, which have content in audio. The experiments are based on more than 20,000 audio documents spanning over seven live VoiceSites and four different languages. The results suggest significant improvement over a meta-data-only or a speech-recognitiononly system, thus justifying the two-stream processing approach. Audio content search is a growing problem area and this paper wishes to be a first step to solving this at a large scale, across languages, in a Web context.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {503–512},
numpages = {10},
keywords = {spoken web, developing regions, literacy, world wide telecom web, mobile phone, audio search},
location = {Hyderabad, India},
series = {WWW '11}
}

@dataset{10.1145/review-1963192.1963364_R46934,
author = {Friedland, Gerald},
title = {Review ID:R46934 for DOI: 10.1145/1963192.1963364},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1963192.1963364_R46934}
}

@inproceedings{10.1145/1963192.1963365,
author = {Pal, Joyojeet and Pradhan, Manas and Shah, Mihir and Babu, Rakesh},
title = {Assistive Technology for Vision-Impairments: Anagenda for the ICTD Community},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963365},
doi = {10.1145/1963192.1963365},
abstract = {In recent years, ICTD (Information Communications Technology and Development) has grown in significance as an area of engineering research that has focused on low-cost appropriate technologies for the needs of a developing world largely underserved by the dominant modes of technology design. Assistive Technologies (AT) used by people with disabilities facilitate greater equity in the social and economic public sphere. However, by and large such technologies are designed in the industrialized world, for people living in those countries. This is especially true in the case of AT for people with vision impairments -- market-prevalent technologies are both very expensive and are built to support the language and infrastructure typical in the industrialized world. While the community of researchers in the Web Accessibility space have made significant strides, the operational concerns of networks in the developing world, as well as challenges in support for new languages and contexts raises a new set of challenges for technologists in this space. We discuss the state of various technologies in the context of the developing world and propose directions in scientific and community-contributed efforts to increase the relevance and access to AT and accessibility in the developing world.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {513–522},
numpages = {10},
keywords = {accessibility, assistive technology, visually impaired},
location = {Hyderabad, India},
series = {WWW '11}
}

