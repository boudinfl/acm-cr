@inproceedings{10.1145/3244992,
author = {King, Irwin},
title = {Session Details: Information Retrieval},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244992},
doi = {10.1145/3244992},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775154,
author = {Henzinger, Monika and Chang, Bay-Wei and Milch, Brian and Brin, Sergey},
title = {Query-Free News Search},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775154},
doi = {10.1145/775152.775154},
abstract = {Many daily activities present information in the form of a stream of text, and often people can benefit from additional information on the topic discussed. TV broadcast news can be treated as one such stream of text; in this paper we discuss finding news articles on the web that are relevant to news currently being broadcast.We evaluated a variety of algorithms for this problem, looking at the impact of inverse document frequency, stemming, compounds, history, and query length on the relevance and coverage of news articles returned in real time during a broadcast. We also evaluated several postprocessing techniques for improving the precision, including reranking using additional terms, reranking by document similarity, and filtering on document similarity. For the best algorithm, 84%-91% of the articles found were relevant, with at least 64% of the articles being on the exact topic of the broadcast. In addition, a relevant article was found for at least 70% of the topics.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {1–10},
numpages = {10},
keywords = {web information retrieval, query-free search},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775155,
author = {Yu, Shipeng and Cai, Deng and Wen, Ji-Rong and Ma, Wei-Ying},
title = {Improving Pseudo-Relevance Feedback in Web Information Retrieval Using Web Page Segmentation},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775155},
doi = {10.1145/775152.775155},
abstract = {In contrast to traditional document retrieval, a web page as a whole is not a good information unit to search because it often contains multiple topics and a lot of irrelevant information from navigation, decoration, and interaction part of the page. In this paper, we propose a VIsion-based Page Segmentation (VIPS) algorithm to detect the semantic content structure in a web page. Compared with simple DOM based segmentation method, our page segmentation scheme utilizes useful visual cues to obtain a better partition of a page at the semantic level. By using our VIPS algorithm to assist the selection of query expansion terms in pseudo-relevance feedback in web information retrieval, we achieve 27% performance improvement on Web Track dataset.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {11–18},
numpages = {8},
keywords = {web information retrieval, query expansion, relevance feedback, page segmentation},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775156,
author = {Lempel, Ronny and Moran, Shlomo},
title = {Predictive Caching and Prefetching of Query Results in Search Engines},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775156},
doi = {10.1145/775152.775156},
abstract = {We study the caching of query result pages in Web search engines. Popular search engines receive millions of queries per day, and efficient policies for caching query results may enable them to lower their response time and reduce their hardware requirements. We present PDC (probability driven cache), a novel scheme tailored for caching search results, that is based on a probabilistic model of search engine users. We then use a trace of over seven million queries submitted to the search engine AltaVista to evaluate PDC, as well as traditional LRU and SLRU based caching schemes. The trace driven simulations show that PDC outperforms the other policies. We also examine the prefetching of search results, and demonstrate that prefetching can increase cache hit ratios by 50% for large caches, and can double the hit ratios of small caches. When integrating prefetching into PDC, we attain hit ratios of over 0.53.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {19–28},
numpages = {10},
keywords = {caching, query processing and optimization},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3244993,
author = {Sheth, Amit},
title = {Session Details: Foundations of the Semantic Web},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244993},
doi = {10.1145/3244993},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775158,
author = {Farrugia, James},
title = {Model-Theoretic Semantics for the Web},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775158},
doi = {10.1145/775152.775158},
abstract = {Model-theoretic semantics is a formal account of the interpretations of legitimate expressions of a language. It is increasingly being used to provide Web markup languages with well-defined semantics. But a discussion of its roles and limitations for the Semantic Web has not yet received a coherent and detailed treatment. This paper takes the first steps towards such a treatment. The major result is an introductory explication of key ideas that are usually only implicit in existing accounts of semantics for the Web. References to more detailed accounts of these ideas are also provided. The benefit of this explication is increased awareness among Web users of some important issues inherent in using model-theoretic semantics for Web markup languages.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {29–38},
numpages = {10},
keywords = {model-theoretic semantics, web markup languages, semantics},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775159,
author = {Horrocks, Ian and Patel-Schneider, Peter F.},
title = {Three Theses of Representation in the Semantic Web},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775159},
doi = {10.1145/775152.775159},
abstract = {The Semantic Web is vitally dependent on a formal meaning for the constructs of its languages. For Semantic Web languages to work well together their formal meanings must employ a common view (or thesis) of representation, otherwise it will not be possible to reconcile documents written in different languages. The thesis of representation underlying RDF and RDFS is particularly troublesome in this regard, as it has several unusual aspects, both semantic and syntactic. A more-standard thesis of representation would result in the ability to reuse existing results and tools in the Semantic Web.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {39–47},
numpages = {9},
keywords = {representation, model-theoretic semantics, semantic web},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775160,
author = {Grosof, Benjamin N. and Horrocks, Ian and Volz, Raphael and Decker, Stefan},
title = {Description Logic Programs: Combining Logic Programs with Description Logic},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775160},
doi = {10.1145/775152.775160},
abstract = {We show how to interoperate, semantically and inferentially, between the leading Semantic Web approaches to rules (RuleML Logic Programs) and ontologies (OWL/DAML+OIL Description Logic) via analyzing their expressive intersection. To do so, we define a new intermediate knowledge representation (KR) contained within this intersection: Description Logic Programs (DLP), and the closely related Description Horn Logic (DHL) which is an expressive fragment of first-order logic (FOL). DLP provides a significant degree of expressiveness, substantially greater than the RDF-Schema fragment of Description Logic. We show how to perform DLP-fusion: the bidirectional translation of premises and inferences (including typical kinds of queries) from the DLP fragment of DL to LP, and vice versa from the DLP fragment of LP to DL. In particular, this translation enables one to "build rules on top of ontologies": it enables the rule KR to have access to DL ontological definitions for vocabulary primitives (e.g., predicates and individual constants) used by the rules. Conversely, the DLP-fusion technique likewise enables one to "build ontologies on top of rules": it enables ontological definitions to be supplemented by rules, or imported into DL from rules. It also enables available efficient LP inferencing algorithms/implementations to be exploited for reasoning over large-scale DL ontologies.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {48–57},
numpages = {10},
keywords = {rules, description logic, RDF, model-theoretic semantics, knowledge representation, ontologies, semantic web, inferencing, logic programs, information integration, translation, interoperability, XML},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3244994,
author = {Hjelm, Johan},
title = {Session Details: Mobility &amp; Wireless Access},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244994},
doi = {10.1145/3244994},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775162,
author = {Chuang, Siu-Nam and Chan, Alvin T.S. and Cao, Jiannong and Cheung, Ronnie},
title = {Dynamic Service Reconfiguration for Wireless Web Access},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775162},
doi = {10.1145/775152.775162},
abstract = {This paper describes a dynamic service reconfiguration model where the proxy is composed of a chain of service objects called mobilets (pronounced as mo-be-lets), which can be deployed onto the network actively. This model offers flexibility because the chain of mobilets can be dynamically reconfigured to adapt to the vigorous changes in the characteristics of the wireless environment, without interrupting the service provision for other mobile nodes. Furthermore, mobilets can also be migrated to a new proxy server when the mobile node moves to a different network domain. We have realized the dynamic service reconfiguration model by crafting its design into a programmable infrastructure that forms the baseline architecture of the WebPADS (short for Web Proxy for Actively Deployable Services) system.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {58–67},
numpages = {10},
keywords = {active services, dynamic service reconfiguration, wireless environment adaptation, wireless web access},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775163,
author = {Yang, S. Jae and Nieh, Jason and Krishnappa, Shilpa and Mohla, Aparna and Sajjadpour, Mahdi},
title = {Web Browsing Performance of Wireless Thin-Client Computing},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775163},
doi = {10.1145/775152.775163},
abstract = {Web applications are becoming increasingly popular for mobile wireless systems. However, wireless networks can have high packet loss rates, which can degrade web browsing performance on wireless systems. An alternative approach is wireless thin-client computing, in which the web browser runs on a remote thin server with a more reliable wired connection to the Internet. A mobile client then maintains a connection to the thin server to receive display updates over the lossy wireless network. To assess the viability of this thin-client approach, we compare the web browsing performance of thin clients against fat clients that run the web browser locally in lossy wireless networks. Our results show that thin clients can operate quite effectively over lossy networks. Compared to fat clients running web browsers locally, our results show surprisingly that thin clients can be faster and more resilient on web applications over lossy wireless LANs despite having to send more data over the network. We characterize and analyze different design choices in various thin-client systems and explain why these approaches can yield superior web browsing performance in lossy wireless networks.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {68–79},
numpages = {12},
keywords = {thin-client computing, wireless and mobility, web performance},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775164,
author = {Barton, John and Kindberg, Tim and Dai, Hui and Priyantha, Nissanka B. and Al-bin-ali, Fahd},
title = {Sensor-Enhanced Mobile Web Clients: An XForms Approach},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775164},
doi = {10.1145/775152.775164},
abstract = {This paper describes methods for service selection and service access for mobile, sensor-enhanced web clients such as wireless cameras or wireless PDAs with sensor devices attached. The clients announce their data-creating capabilities in "Produce" headers sent to servers; servers respond with forms that match these capabilities. Clients fill in these forms with sensor data as well as text or file data. The resultant system enables clients to access dynamically discovered services spontaneously, as their users engage in everyday nomadic activities.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {80–89},
numpages = {10},
keywords = {mobile computing, MIME types, sensors, forms, browsers, ubiquitous computing},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3244995,
author = {Henzinger, Monika},
title = {Session Details: Information Retrieval 2},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244995},
doi = {10.1145/3244995},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775166,
author = {Gravano, Luis and Ipeirotis, Panagiotis G. and Koudas, Nick and Srivastava, Divesh},
title = {Text Joins in an RDBMS for Web Data Integration},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775166},
doi = {10.1145/775152.775166},
abstract = {The integration of data produced and collected across autonomous, heterogeneous web services is an increasingly important and challenging problem. Due to the lack of global identifiers, the same entity (e.g., a product) might have different textual representations across databases. Textual data is also often noisy because of transcription errors, incomplete information, and lack of standard formats. A fundamental task during data integration is matching of strings that refer to the same entity. In this paper, we adopt the widely used and established cosine similarity metric from the information retrieval field in order to identify potential string matches across web sources. We then use this similarity metric to characterize this key aspect of data integration as a join between relations on textual attributes, where the similarity of matches exceeds a specified threshold. Computing an exact answer to the text join can be expensive. For query processing efficiency, we propose a sampling-based join approximation strategy for execution in a standard, unmodified relational database management system (RDBMS), since more and more web sites are powered by RDBMSs with a web-based front end. We implement the join inside an RDBMS, using SQL queries, for scalability and robustness reasons. Finally, we present a detailed performance evaluation of an implementation of our algorithm within a commercial RDBMS, using real-life data sets. Our experimental results demonstrate the efficiency and accuracy of our techniques.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {90–101},
numpages = {12},
keywords = {text indexing, approximate text matching, data cleaning},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775167,
author = {Lim, Lipyeow and Wang, Min and Padmanabhan, Sriram and Vitter, Jeffrey Scott and Agarwal, Ramesh},
title = {Dynamic Maintenance of Web Indexes Using Landmarks},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775167},
doi = {10.1145/775152.775167},
abstract = {Recent work on incremental crawling has enabled the indexed document collection of a search engine to be more synchronized with the changing World Wide Web. However, this synchronized collection is not immediately searchable, because the keyword index is rebuilt from scratch less frequently than the collection can be refreshed. An inverted index is usually used to index documents crawled from the web. Complete index rebuild at high frequency is expensive. Previous work on incremental inverted index updates have been restricted to adding and removing documents. Updating the inverted index for previously indexed documents that have changed has not been addressed.In this paper, we propose an efficient method to update the inverted index for previously indexed documents whose contents have changed. Our method uses the idea of landmarks together with the diff algorithm to significantly reduce the number of postings in the inverted index that need to be updated. Our experiments verify that our landmark-diff method results in significant savings in the number of update operations on the inverted index.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {102–111},
numpages = {10},
keywords = {update processing, inverted files},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775168,
author = {Myllymaki, Jussi and Kaufman, James},
title = {High-Performance Spatial Indexing for Location-Based Services},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775168},
doi = {10.1145/775152.775168},
abstract = {Much attention has been accorded to Location-Based Services and location tracking, a necessary component in active, trigger-based LBS applications. Tracking the location of a large population of moving objects requires very high update and query performance of the underlying spatial index. In this paper we investigate the performance and scalability of three main-memory based spatial indexing methods under dynamic update and query loads: an R-tree, a ZB-tree, and an array/hashtable method. By leveraging the LOCUS performance evaluation testbed and the City Simulator dynamic spatial data generator, we are able to demonstrate the scalability of these methods and determine the maximum population size supported by each method, a useful parameter for capacity planning by wireless carriers.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {112–117},
numpages = {6},
keywords = {mobile computing, location-based services, spatial indexing},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3244996,
author = {Dahlin, Mike},
title = {Session Details: Provisioning},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244996},
doi = {10.1145/3244996},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775170,
author = {Mao, Z. Morley and Johnson, David and Spatscheck, Oliver and van der Merwe, Jacobus E. and Wang, Jia},
title = {Efficient and Robust Streaming Provisioning in VPNs},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775170},
doi = {10.1145/775152.775170},
abstract = {Today, most large companies maintain virtual private networks (VPNs) to connect their remote locations into a single secure network. VPNs can be quite large covering more than 1000 locations and in most cases use standard Internet protocols and services. Such VPNs are implemented using a diverse set of technologies such as Frame Relay, MPLS, or IPSEC to achieve the goal of privacy and performance isolation from the public Internet.Using VPNs to distribute live content has recently received tremendous interest. For example, a VPN could be used to broadcast a CEO-employee town hall meeting. To distribute this type of content economically without overloading the network, the deployment of streaming caches or splitters is most likely required.In this paper, we address the problem of optimally placing such streaming splitters or caches to broadcast to a given set of VPN endpoints under the constraints typically found within a VPN. In particular, we introduce an efficient algorithm with complexity O(V), V being the number of routers in the VPN. This guarantees the optimal cache placement if interception is used for redirection. We prove that the general problem is NP-hard and introduce multiple heuristics for efficient and robust cache placement suitable under different constraints. At the expense of increased implementation complexity, each heuristic solution provides additional saving in the number of caches required. We evaluate proposed solutions using extensive simulations. In particular, we show our flow-based solution is very close to the optimal.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {118–127},
numpages = {10},
keywords = {streaming server placement, VPNs},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775171,
author = {Verma, Akshat and Ghosal, Sugata},
title = {On Admission Control for Profit Maximization of Networked Service Providers},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775171},
doi = {10.1145/775152.775171},
abstract = {Variability and diverseness among incoming requests to a service hosted on a finite capacity resource necessitates sophisticated request admission control techniques for providing guaranteed quality of service (QoS). We propose in this paper a service time based online admission control methodology for maximizing profits of a service provider. The proposed methodology chooses a subset of incoming requests such that the revenue of the provider is maximized. Admission control decision in our proposed system is based upon an estimate of the service time of the request, QoS bounds, prediction of arrivals and service times of requests to come in the short-term future, and rewards associated with servicing a request within its QoS bounds. Effectiveness of the proposed admission control methodology is demonstrated using experiments with a content-based messaging middleware service.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {128–137},
numpages = {10},
keywords = {service level agreement (SLA), web service, service time estimation, quality of service (QoS), shortest remaining job first (SRJF), profit maximization, short-term prediction, admission control},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775172,
author = {Krishnamurthy, Balachander and Zhang, Yin and Wills, Craig E. and Vishwanath, Kashi},
title = {Design, Implementation, and Evaluation of a Client Characterization Driven Web Server},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775172},
doi = {10.1145/775152.775172},
abstract = {In earlier work we proposed a way for a Web server to detect connectivity information about clients accessing it in order to take tailored actions for a client request. This paper describes the design, implementation, and evaluation of such a working system. A Web site has a strong incentive to reduce the 'time-to-glass' to retain users who may otherwise lose interest and leave the site. We have performed a measurement study from multiple client sites around the world with various levels of connectivity to the Internet communicating with modified Apache Web servers under our control. The results show that clients can be classified in a correct and stable manner and that user-perceived latency can be reduced via tailored actions. Our measurements show that classification and determination of server actions are done without significant overhead on the Web server. We explore a variety of modified actions ranging from selecting a lower quality version of the resource to altering the manner of content delivery. By studying numerous performance related factors in a single unified framework and examining both individual actions as well as combination of actions, our modified Web server implementation shows the efficacy of various server actions.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {138–147},
numpages = {10},
keywords = {web performance, content delivery, apache server, server adaptation, httperf, client classification},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3244997,
author = {TBA},
title = {Session Details: Data Integrity},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244997},
doi = {10.1145/3244997},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775174,
author = {Huang, Yao-Wen and Huang, Shih-Kun and Lin, Tsung-Po and Tsai, Chung-Hung},
title = {Web Application Security Assessment by Fault Injection and Behavior Monitoring},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775174},
doi = {10.1145/775152.775174},
abstract = {As a large and complex application platform, the World Wide Web is capable of delivering a broad range of sophisticated applications. However, many Web applications go through rapid development phases with extremely short turnaround time, making it difficult to eliminate vulnerabilities. Here we analyze the design of Web application security assessment mechanisms in order to identify poor coding practices that render Web applications vulnerable to attacks such as SQL injection and cross-site scripting. We describe the use of a number of software-testing techniques (including dynamic analysis, black-box testing, fault injection, and behavior monitoring), and suggest mechanisms for applying these techniques to Web applications. Real-world situations are used to test a tool we named the Web Application Vulnerability and Error Scanner (WAVES, an open-source project available at http://waves.sourceforge.net) and to compare it with other tools. Our results show that WAVES is a feasible platform for assessing Web application security.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {148–159},
numpages = {12},
keywords = {complete crawling, fault injection, web application testing, security assessment, black-box testing},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775175,
author = {Mont, Marco Casassa and Harrison, Keith and Sadler, Martin},
title = {The HP Time Vault Service: Exploiting IBE for Timed Release of Confidential Information},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775175},
doi = {10.1145/775152.775175},
abstract = {Digital information is increasingly more and more important to enable interactions and transactions on the Internet. On the other hand, leakages of sensitive information can have harmful effects for people, enterprises and governments.This paper focuses on the problems of dealing with timed release of confidential information and simplifying its access once public: it is a common issue in the industry, government and day-to-day life.We introduce the "HP Time Vault Service", based on the emerging Identifier-based Encryption (IBE) cryptography schema. IBE (public) encryption keys specify the disclosure time. These keys are used to encrypt confidential information. An independent time server generates and publishes IBE decryption keys correspondent to the current time, at predefined intervals.We discuss the advantages of this approach against current approaches based on traditional cryptography. A web-service based prototype is described, as a proof of concept.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {160–169},
numpages = {10},
keywords = {web service, timed-release, security, privacy, identifier-based encryption, disclosure policies},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775176,
author = {Bull, Laurence and Stanski, Peter and Squire, David McG.},
title = {Content Extraction Signatures Using XML Digital Signatures and Custom Transforms On-Demand},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775176},
doi = {10.1145/775152.775176},
abstract = {Content Extraction Signatures (CES) enable selective disclosure of verifiable content, provide privacy for blinded content, and enable the signer to specify the content the document owner is allowed to extract or blind. Combined, these properties give what we call CES functionality. In this paper we describe our work in developing custom transform algorithms to expand the functionality of an XML Signature to include CES functionality in XML Signature Core Validation.We also describe a custom revocation mechanism and our implementation for non-XML content where the custom transforms are dynamically loaded demonstrating that custom signing and verification is not constrained to a 'closed system'. Through the use of dynamic loading we show that a verifier can still verify an XML Signature-compliant signature even though a custom signature was produced.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {170–177},
numpages = {8},
keywords = {XML signature custom transforms, XML signatures, content extraction signatures, dynamic signature verification, .Net framework XML signature API},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3244998,
author = {Horrocks, Ian},
title = {Session Details: Establishing the Semantic Web 1},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244998},
doi = {10.1145/3244998},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775178,
author = {Dill, Stephen and Eiron, Nadav and Gibson, David and Gruhl, Daniel and Guha, R. and Jhingran, Anant and Kanungo, Tapas and Rajagopalan, Sridhar and Tomkins, Andrew and Tomlin, John A. and Zien, Jason Y.},
title = {SemTag and Seeker: Bootstrapping the Semantic Web via Automated Semantic Annotation},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775178},
doi = {10.1145/775152.775178},
abstract = {This paper describes Seeker, a platform for large-scale text analytics, and SemTag, an application written on the platform to perform automated semantic tagging of large corpora. We apply SemTag to a collection of approximately 264 million web pages, and generate approximately 434 million automatically disambiguated semantic tags, published to the web as a label bureau providing metadata regarding the 434 million annotations. To our knowledge, this is the largest scale semantic tagging effort to date.We describe the Seeker platform, discuss the architecture of the SemTag application, describe a new disambiguation algorithm specialized to support ontological disambiguation of large-scale data, evaluate the algorithm, and present our final results with information about acquiring and making use of the semantic tags. We argue that automated large scale semantic tagging of ambiguous content can bootstrap and accelerate the creation of the semantic web.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {178–186},
numpages = {9},
keywords = {data mining, text analytics, information retrieval, large text datasets, automated semantic tagging},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775179,
author = {Wang, Jiying and Lochovsky, Fred H.},
title = {Data Extraction and Label Assignment for Web Databases},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775179},
doi = {10.1145/775152.775179},
abstract = {Many tools have been developed to help users query, extract and integrate data from web pages generated dynamically from databases, i.e., from the Hidden Web. A key prerequisite for such tools is to obtain the schema of the attributes of the retrieved data. In this paper, we describe a system called, DeLa, which reconstructs (part of) a "hidden" back-end web database. It does this by sending queries through HTML forms, automatically generating regular expression wrappers to extract data objects from the result pages and restoring the retrieved data into an annotated (labelled) table. The whole process needs no human involvement and proves to be fast (less than one minute for wrapper induction for each site) and accurate (over 90% correctness for data extraction and around 80% correctness for label assignment).},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {187–196},
numpages = {10},
keywords = {automatic wrapper induction, web information extraction, hidden web, data annotation, information integration, HTML forms},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775180,
author = {Aberer, Karl and Cudr\'{e}-Mauroux, Philippe and Hauswirth, Manfred},
title = {The Chatty Web: Emergent Semantics through Gossiping},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775180},
doi = {10.1145/775152.775180},
abstract = {This paper describes a novel approach for obtaining semantic interoperability among data sources in a bottom-up, semi-automatic manner without relying on pre-existing, global semantic models. We assume that large amounts of data exist that have been organized and annotated according to local schemas. Seeing semantics as a form of agreement, our approach enables the participating data sources to incrementally develop global agreement in an evolutionary and completely decentralized process that solely relies on pair-wise, local interactions: Participants provide translations between schemas they are interested in and can learn about other translations by routing queries (gossiping). To support the participants in assessing the semantic quality of the achieved agreements we develop a formal framework that takes into account both syntactic and semantic criteria. The assessment process is incremental and the quality ratings are adjusted along with the operation of the system. Ultimately, this process results in global agreement, i.e., the semantics that all participants understand. We discuss strategies to efficiently find translations and provide results from a case study to justify our claims. Our approach applies to any system which provides a communication infrastructure (existing websites or databases, decentralized systems, P2P systems) and offers the opportunity to study semantic interoperability as a global phenomenon in a network of information sharing parties.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {197–206},
numpages = {10},
keywords = {semantic agreements, semantic integration, self-organization},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3244999,
author = {Najork, Marc},
title = {Session Details: Adapting Content to Mobile Devices},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244999},
doi = {10.1145/3244999},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775182,
author = {Gupta, Suhit and Kaiser, Gail and Neistadt, David and Grimm, Peter},
title = {DOM-Based Content Extraction of HTML Documents},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775182},
doi = {10.1145/775152.775182},
abstract = {Web pages often contain clutter (such as pop-up ads, unnecessary images and extraneous links) around the body of an article that distracts a user from actual content. Extraction of "useful and relevant" content from web pages has many applications, including cell phone and PDA browsing, speech rendering for the visually impaired, and text summarization. Most approaches to removing clutter or making content more readable involve changing font size or removing HTML and data components such as images, which takes away from a webpage's inherent look and feel. Unlike "Content Reformatting", which aims to reproduce the entire webpage in a more convenient form, our solution directly addresses "Content Extraction". We have developed a framework that employs easily extensible set of techniques that incorporate advantages of previous work on content extraction. Our key insight is to work with the DOM trees, rather than with raw HTML markup. We have implemented our approach in a publicly available Web proxy to extract content from HTML web pages.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {207–214},
numpages = {8},
keywords = {speech rendering, reformatting, content extraction, accessibility, HTML documents, DOM trees},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775183,
author = {Yang, Christopher C. and Wang, Fu Lee},
title = {Fractal Summarization for Mobile Devices to Access Large Documents on the Web},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775183},
doi = {10.1145/775152.775183},
abstract = {Wireless access with mobile (or handheld) devices is a promising addition to the WWW and traditional electronic business. Mobile devices provide convenience and portable access to the huge information space on the Internet without requiring users to be stationary with network connection. However, the limited screen size, narrow network bandwidth, small memory capacity and low computing power are the shortcomings of handheld devices. Loading and visualizing large documents on handheld devices become impossible. The limited resolution restricts the amount of information to be displayed. The download time is intolerably long. In this paper, we introduce the fractal summarization model for document summarization on handheld devices. Fractal summarization is developed based on the fractal theory. It generates a brief skeleton of summary at the first stage, and the details of the summary on different levels of the document are generated on demands of users. Such interactive summarization reduces the computation load in comparing with the generation of the entire summary in one batch by the traditional automatic summarization, which is ideal for wireless access. Three-tier architecture with the middle-tier conducting the major computation is also discussed. Visualization of summary on handheld devices is also investigated.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {215–224},
numpages = {10},
keywords = {fractal summarization, document summarization, handheld devices, mobile commerce},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775184,
author = {Chen, Yu and Ma, Wei-Ying and Zhang, Hong-Jiang},
title = {Detecting Web Page Structure for Adaptive Viewing on Small Form Factor Devices},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775184},
doi = {10.1145/775152.775184},
abstract = {Mobile devices have already been widely used to access the Web. However, because most available web pages are designed for desktop PC in mind, it is inconvenient to browse these large web pages on a mobile device with a small screen. In this paper, we propose a new browsing convention to facilitate navigation and reading on a small-form-factor device. A web page is organized into a two level hierarchy with a thumbnail representation at the top level for providing a global view and index to a set of sub-pages at the bottom level for detail information. A page adaptation technique is also developed to analyze the structure of an existing web page and split it into small and logically related units that fit into the screen of a mobile device. For a web page not suitable for splitting, auto-positioning or scrolling-by-block is used to assist the browsing as an alterative. Our experimental results show that our proposed browsing convention and developed page adaptation scheme greatly improve the user's browsing experiences on a device with a small display.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {225–233},
numpages = {9},
keywords = {mobile browser, content adaptation, adaptive hypermedia},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245000,
author = {TBA},
title = {Session Details: Writing the Web},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245000},
doi = {10.1145/3245000},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775186,
author = {Miles-Board, Timothy and Carr, Leslie and Kampa, Simon and Hall, Wendy},
title = {Supporting Management Reporting: A Writable Web Case Study},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775186},
doi = {10.1145/775152.775186},
abstract = {The World-Wide Web was originally developed as a shared, writable, hypertext medium, a facility that is still widely needed.We have recently developed a Web-based management reporting system for a legal firm in an attempt to improve the efficiency and management of their overall business process. This paper shares our experiences in relating the firm's specific writing and issue tracking tasks to existing Web, open hypermedia, and Semantic Web research, and describes why we chose to develop a new solution --- a set of open hypermedia components collectively called the Management Reporting System --- rather than employ an existing system.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {234–243},
numpages = {10},
keywords = {hypertext writing, open hypermedia, structural computing, management reporting},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775187,
author = {Uren, Victoria and Shum, Simon Buckingham and Li, Gangmin and Domingue, John and Motta, Enrico},
title = {Scholarly Publishing and Argument in Hyperspace},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775187},
doi = {10.1145/775152.775187},
abstract = {The World Wide Web is opening up access to documents and data for scholars. However it has not yet impacted on one of the primary activities in research: assessing new findings in the light of current knowledge and debating it with colleagues. The ClaiMaker system uses a directed graph model with similarities to hypertext, in which new ideas are published as nodes, which other contributors can build on or challenge in a variety of ways by linking to them. Nodes and links have semantic structure to facilitate the provision of specialist services for interrogating and visualizing the emerging network. By way of example, this paper is grounded in a ClaiMaker model to illustrate how new claims can be described in this structured way.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {244–250},
numpages = {7},
keywords = {scientific publishing, modeling debate, collaborative web, scholarly interpretation},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775188,
author = {Liu, Bing and Chin, Chee Wee and Ng, Hwee Tou},
title = {Mining Topic-Specific Concepts and Definitions on the Web},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775188},
doi = {10.1145/775152.775188},
abstract = {Traditionally, when one wants to learn about a particular topic, one reads a book or a survey paper. With the rapid expansion of the Web, learning in-depth knowledge about a topic from the Web is becoming increasingly important and popular. This is also due to the Web's convenience and its richness of information. In many cases, learning from the Web may even be essential because in our fast changing world, emerging topics appear constantly and rapidly. There is often not enough time for someone to write a book on such topics. To learn such emerging topics, one can resort to research papers. However, research papers are often hard to understand by non-researchers, and few research papers cover every aspect of the topic. In contrast, many Web pages often contain intuitive descriptions of the topic. To find such Web pages, one typically uses a search engine. However, current search techniques are not designed for in-depth learning. Top ranking pages from a search engine may not contain any description of the topic. Even if they do, the description is usually incomplete since it is unlikely that the owner of the page has good knowledge of every aspect of the topic. In this paper, we attempt a novel and challenging task, mining topic-specific knowledge on the Web. Our goal is to help people learn in-depth knowledge of a topic systematically on the Web. The proposed techniques first identify those sub-topics or salient concepts of the topic, and then find and organize those informative pages, containing definitions and descriptions of the topic and sub-topics, just like those in a book. Experimental results using 28 topics show that the proposed techniques are highly effective.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {251–260},
numpages = {10},
keywords = {domain concept mining, knowledge compilation, definition mining, information integration, web content mining},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245001,
author = {Chakrabarti, Soumen},
title = {Session Details: Link-Based Ranking 1},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245001},
doi = {10.1145/3245001},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775190,
author = {Kamvar, Sepandar D. and Haveliwala, Taher H. and Manning, Christopher D. and Golub, Gene H.},
title = {Extrapolation Methods for Accelerating PageRank Computations},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775190},
doi = {10.1145/775152.775190},
abstract = {We present a novel algorithm for the fast computation of PageRank, a hyperlink-based estimate of the ''importance'' of Web pages. The original PageRank algorithm uses the Power Method to compute successive iterates that converge to the principal eigenvector of the Markov matrix representing the Web link graph. The algorithm presented here, called Quadratic Extrapolation, accelerates the convergence of the Power Method by periodically subtracting off estimates of the nonprincipal eigenvectors from the current iterate of the Power Method. In Quadratic Extrapolation, we take advantage of the fact that the first eigenvalue of a Markov matrix is known to be 1 to compute the nonprincipal eigenvectors using successive iterates of the Power Method. Empirically, we show that using Quadratic Extrapolation speeds up PageRank computation by 25-300% on a Web graph of 80 million nodes, with minimal overhead. Our contribution is useful to the PageRank community and the numerical linear algebra community in general, as it is a fast method for determining the dominant eigenvector of a matrix that is too large for standard fast methods to be practical.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {261–270},
numpages = {10},
keywords = {link analysis, PageRank, eigenvector computation},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775191,
author = {Jeh, Glen and Widom, Jennifer},
title = {Scaling Personalized Web Search},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775191},
doi = {10.1145/775152.775191},
abstract = {Recent web search techniques augment traditional text matching with a global notion of "importance" based on the linkage structure of the web, such as in Google's PageRank algorithm. For more refined searches, this global notion of importance can be specialized to create personalized views of importance--for example, importance scores can be biased according to a user-specified set of initially-interesting pages. Computing and storing all possible personalized views in advance is impractical, as is computing personalized views at query time, since the computation of each view requires an iterative computation over the web graph. We present new graph-theoretical results, and a new technique based on these results, that encode personalized views as partial vectors. Partial vectors are shared across multiple personalized views, and their computation and storage costs scale well with the number of views. Our approach enables incremental computation, so that the construction of personalized views from partial vectors is practical at query time. We present efficient dynamic programming algorithms for computing partial vectors, an algorithm for constructing personalized views from partial vectors, and experimental results demonstrating the effectiveness and scalability of our techniques.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {271–279},
numpages = {9},
keywords = {web search, PageRank},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775192,
author = {Abiteboul, Serge and Preda, Mihai and Cobena, Gregory},
title = {Adaptive On-Line Page Importance Computation},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775192},
doi = {10.1145/775152.775192},
abstract = {The computation of page importance in a huge dynamic graph has recently attracted a lot of attention because of the web. Page importance, or page rank is defined as the fixpoint of a matrix equation. Previous algorithms compute it off-line and require the use of a lot of extra CPU as well as disk resources (e.g. to store, maintain and read the link matrix). We introduce a new algorithm OPIC that works on-line, and uses much less resources. In particular, it does not require storing the link matrix. It is on-line in that it continuously refines its estimate of page importance while the web/graph is visited. Thus it can be used to focus crawling to the most interesting pages. We prove the correctness of OPIC. We present Adaptive OPIC that also works on-line but adapts dynamically to changes of the web. A variant of this algorithm is now used by Xyleme.We report on experiments with synthetic data. In particular, we study the convergence and adaptiveness of the algorithms for various scheduling strategies for the pages to visit. We also report on experiments based on crawls of significant portions of the web.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {280–290},
numpages = {11},
keywords = {page importance, hyperlink, web graph},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245002,
author = {Hoschka, Philipp},
title = {Session Details: Applications and Architecture},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245002},
doi = {10.1145/3245002},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775194,
author = {Lukose, Rajan M. and Adar, Eytan and Tyler, Joshua R. and Sengupta, Caesar},
title = {SHOCK: Communicating with Computational Messages and Automatic Private Profiles},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775194},
doi = {10.1145/775152.775194},
abstract = {A computationally enhanced message contains some embedded programmatic components that are interpreted and executed automatically upon receipt. Unlike ordinary text email or instant messages, they make possible a number of useful applications. In this paper, we describe a general and flexible messaging system called SHOCK that extends the functionality of prior computational email systems by allowing XML-encoded SHOCK messages to interact with an automatically created profile of a user. These profiles consist of information about the most common tasks users perform, such as their Web browsing behavior, their conventional email usage, etc. Since users are sensitive about such data, the system is designed with privacy as a central design goal, and employs a distributed peer-to-peer architecture to achieve it. The system is largely implemented with commodity Web technologies and provides both a Web interface as well as one that is tightly integrated with users ordinary email clients. With SHOCK, users can send highly targeted messages without violating others privacy, and engage in structured conversation appropriate to the context without disrupting their existing work practices. We describe our implementation in detail, the most useful novel applications of the system, and our experiences with the system in a pilot field test.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {291–300},
numpages = {10},
keywords = {privacy and preferences, networking and distributed web applications, collaborative systems},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775195,
author = {Guo, Yang and Suh, Kyoungwon and Kurose, Jim and Towsley, Don},
title = {P2Cast: Peer-to-Peer Patching Scheme for VoD Service},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775195},
doi = {10.1145/775152.775195},
abstract = {Providing video on demand (VoD) service over the Internet in a scalable way is a challenging problem. In this paper, we propose P2Cast - an architecture that uses a peer-to-peer approach to cooperatively stream video using patching techniques, while only relying on unicast connections among peers. We address the following two key technical issues in P2Cast: (1) constructing an application overlay appropriate for streaming; and (2) providing continuous stream playback (without glitches) in the face of disruption from an early departing client. Our simulation experiments show that P2Cast can serve many more clients than traditional client-server unicast service, and that it generally out-performs multicast-based patching if clients can cache more than of a stream's initial portion. We handle disruptions by delaying the start of playback and applying the shifted forwarding technique. A threshold on the length of time during which arriving clients are served in a single session in P2Cast serves as a knob to adjust the balance between the scalability and the clients' viewing quality in P2Cast.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {301–309},
numpages = {9},
keywords = {peer-to-peer networks, patching, video on-demand service, performance evaluation},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775196,
author = {Krishnamurthy, Balachander and Liston, Richard and Rabinovich, Michael},
title = {DEW: DNS-Enhanced Web for Faster Content Delivery},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775196},
doi = {10.1145/775152.775196},
abstract = {With a key component of latency on the Web being connection set up between clients and Web servers, several ways to avoid connections have been explored. While the work in recent years on Content Distribution Networks (CDNs) have moved some content 'closer' to users at the cost of increasing DNS traffic, they have not fully exploited the available unused potential of existing protocols. We explore ways by which a variety of Web responses can be piggybacked on DNS messages. While we evaluated our idea in the Web context, the approach is generic and not restricted to Web responses. We propose an architecture for HTTP piggybacking in DNS messages and carry out a detailed performance analysis based on a trace-driven simulation study. Our architecture requires minimal extensions to existing protocols, utilizing only the allowed optional fields for these extensions. It is fully compatible and can coexist with the current Web.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {310–320},
numpages = {11},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245003,
author = {TBA},
title = {Session Details: E-Commerce},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245003},
doi = {10.1145/3245003},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775198,
author = {Di Noia, Tommaso and Di Sciascio, Eugenio and Donini, Francesco M. and Mongiello, Marina},
title = {A System for Principled Matchmaking in an Electronic Marketplace},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775198},
doi = {10.1145/775152.775198},
abstract = {More and more resources are becoming available on the Web, and there is a growing need for infrastructures that, based on advertised descriptions, are able to semantically match demands with supplies.We formalize general properties a matchmaker should have, then we present a matchmaking facilitator, compliant with desired properties.The system embeds a NeoClassic reasoner, whose structural subsumption algorithm has been modified to allow match categorization into potential and partial, and ranking of matches within categories. Experiments carried out show the good correspondence between users and system rankings.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {321–330},
numpages = {10},
keywords = {knowledge representation, matchmaking, description logics, e-commerce},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775199,
author = {Li, Lei and Horrocks, Ian},
title = {A Software Framework for Matchmaking Based on Semantic Web Technology},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775199},
doi = {10.1145/775152.775199},
abstract = {An important objective of the Semantic Web is to make Electronic Commerce interactions more flexible and automated. To achieve this, standardization of ontologies, message content and message protocols will be necessary.In this paper we investigate how Semantic and Web Services technologies can be used to support service advertisement and discovery in e-commerce. In particular, we describe the design and implementation of a service matchmaking prototype which uses a DAML-S based ontology and a Description Logic reasoner to compare ontology based service descriptions. We also present the results of initial experiments testing the performance of this prototype implementation in a realistic agent based e-commerce scenario.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {331–339},
numpages = {9},
keywords = {semantic web, ontologies, web services},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775200,
author = {Grosof, Benjamin N. and Poon, Terrence C.},
title = {SweetDeal: Representing Agent Contracts with Exceptions Using XML Rules, Ontologies, and Process Descriptions},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775200},
doi = {10.1145/775152.775200},
abstract = {SweetDeal is a rule-based approach to representation of business contracts that enables software agents to create, evaluate, negotiate, and execute contracts with substantial automation and modularity. It builds upon the situated courteous logic programs knowledge representation in RuleML, the emerging standard for Semantic Web XML rules. Here, we newly extend the SweetDeal approach by also incorporating process knowledge descriptions whose ontologies are represented in DAML+OIL (emerging standard for Semantic Web ontologies) thereby enabling more complex contracts with behavioral provisions, especially for handling exception conditions (e.g., late delivery or non-payment) that might arise during the execution of the contract. This provides a foundation for representing and automating deals about services -- in particular, about Web Services, so as to help search, select, and compose them. Our system is also the first to combine emerging Semantic Web standards for knowledge representation of rules (RuleML) with ontologies (DAML+OIL) for a practical e-business application domain, and further to do so with process knowledge. This also newly fleshes out the evolving concept of Semantic Web Services. A prototype (soon public) is running.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {340–349},
numpages = {10},
keywords = {semantic web services, description logic, semantic web, business process automation, electronic commerce, electronic contracts, web services, process knowledge, knowledge representation, knowledge-based, rules, logic programs, OWL, ontologies, RDF, process descriptions, XML, intelligent software agents, DAML+OIL, declarative},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245004,
author = {Anick, Peter},
title = {Session Details: Link-Based Ranking 2},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245004},
doi = {10.1145/3245004},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775202,
author = {Tomlin, John A.},
title = {A New Paradigm for Ranking Pages on the World Wide Web},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775202},
doi = {10.1145/775152.775202},
abstract = {This paper describes a new paradigm for modeling traffic levels on the world wide web (WWW) using a method of entropy maximization. This traffic is subject to the conservation conditions of a circulation flow in the entire WWW, an aggregation of the WWW, or a subgraph of the WWW (such as an intranet or extranet). We specifically apply the primal and dual solutions of this model to the (static) ranking of web sites. The first of these uses an imputed measure of total traffic through a web page, the second provides an analogy of local "temperature", allowing us to quantify the "HOTness" of a page.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {350–355},
numpages = {6},
keywords = {static ranking, search engines, optimization, entropy},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775203,
author = {Tsoi, Ah Chung and Morini, Gianni and Scarselli, Franco and Hagenbuchner, Markus and Maggini, Marco},
title = {Adaptive Ranking of Web Pages},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775203},
doi = {10.1145/775152.775203},
abstract = {In this paper, we consider the possibility of altering the PageRank of web pages, from an administrator's point of view, through the modification of the PageRank equation. It is shown that this problem can be solved using the traditional quadratic programming techniques. In addition, it is shown that the number of parameters can be reduced by clustering web pages together through simple clustering techniques. This problem can be formulated and solved using quadratic programming techniques. It is demonstrated experimentally on a relatively large web data set, viz., the WT10G, that it is possible to modify the PageRanks of the web pages through the proposed method using a set of linear constraints. It is also shown that the PageRank of other pages may be affected; and that the quality of the result depends on the clustering technique used. It is shown that our results compared well with those obtained by a HITS based method.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {356–365},
numpages = {10},
keywords = {search engine, quadratic programming applications, adaptive PageRank determinations, learning PageRank, PageRank},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775204,
author = {Fagin, Ronald and Kumar, Ravi and McCurley, Kevin S. and Novak, Jasmine and Sivakumar, D. and Tomlin, John A. and Williamson, David P.},
title = {Searching the Workplace Web},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775204},
doi = {10.1145/775152.775204},
abstract = {The social impact from the World Wide Web cannot be underestimated, but technologies used to build the Web are also revolutionizing the sharing of business and government information within intranets. In many ways the lessons learned from the Internet carry over directly to intranets, but others do not apply. In particular, the social forces that guide the development of intranets are quite different, and the determination of a "good answer" for intranet search is quite different than on the Internet. In this paper we study the problem of intranet search. Our approach focuses on the use of rank aggregation, and allows us to examine the effects of different heuristics on ranking of search results.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {366–375},
numpages = {10},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245005,
author = {Ma, Wei-Ying},
title = {Session Details: Multimedia},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245005},
doi = {10.1145/3245005},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775206,
author = {Yang, Cheng},
title = {Peer-to-Peer Architecture for Content-Based Music Retrieval on Acoustic Data},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775206},
doi = {10.1145/775152.775206},
abstract = {In traditional peer-to-peer search networks, operations focus on properly labeled files such as music or video, and the actual search is often limited to text tags. The explosive growth of available multimedia documents in recent years calls for more flexible search capabilities, namely search by content. Most content-based search algorithms are computationally intensive, making them inappropriate for a peer-to-peer environment. In this paper, we discuss a content-based music retrieval algorithm that can be decomposed and parallelized efficiently. We present a peer-to-peer architecture for such a system that makes use of spare resources among subscribers, with protocols that dynamically redistribute load in order to maximize throughput and minimize inconvenience to subscribers. Our framework can be extended beyond the music retrieval domain and adapted to other scenarios where resource pooling is desired, as long as the underlying algorithm satisfies certain conditions.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {376–383},
numpages = {8},
keywords = {load balancing, distributed, acoustic data, content-based music retrieval, resource pooling, peer-to-peer},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775207,
author = {van Ossenbruggen, Jacco and Hardma, Lynda and Geurts, Joost and Rutledge, Lloyd},
title = {Towards a Multimedia Formatting Vocabulary},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775207},
doi = {10.1145/775152.775207},
abstract = {Time-based, media-centric Web presentations can be described declaratively in the XML world through the development of languages such as SMIL. It is difficult, however, to fully integrate them in a complete document transformation processing chain. In order to achieve the desired processing of data-driven, time-based, media-centric presentations, the text-flow based formatting vocabularies used by style languages such as XSL, CSS and DSSSL need to be extended. The paper presents a selection of use cases which are used to derive a list of requirements for a multimedia style and transformation formatting vocabulary. The boundaries of applicability of existing text-based formatting models for media-centric transformations are analyzed. The paper then discusses the advantages and disadvantages of a fully-fledged time-based multimedia formatting model. Finally, the discussion is illustrated by describing the key properties of the example multimedia formatting vocabulary currently implemented in the back-end of our Cuypers multimedia transformation engine.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {384–393},
numpages = {10},
keywords = {document transformation, Cuypers, hyper-media, multimedia, formatting objects},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775208,
author = {Schojer, Peter and B\"{o}sz\"{o}rmenyi, Laszlo and Hellwagner, Hermann and Penz, Bernhard and Podlipnig, Stefan},
title = {Architecture of a Quality Based Intelligent Proxy (QBIX) for MPEG-4 Videos},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775208},
doi = {10.1145/775152.775208},
abstract = {Due to the increasing availability and use of digital video data on the Web, video caching will be an important performance factor in the future WWW. We propose an architecture of a video proxy cache that integrates modern multimedia and communication standards. Especially we describe features of the MPEG-4 and MPEG-7 multimedia standards that can be helpful for a video proxy cache. QBIX supports real-time adaptation in the compressed and in the decompressed domain. It uses adaptation to improve the cache replacement strategies in the proxy, but also to realize media gateway functionality driven by the clients' terminal capabilities.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {394–402},
numpages = {9},
keywords = {LRU, replacement, video proxy, media adaptation, video caching, RTP, MPEG-4, RTSP, MPEG-7, media gateway},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245006,
author = {Gaedke, Martin},
title = {Session Details: Web Engineering},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245006},
doi = {10.1145/3245006},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775210,
author = {Bultan, Tevfik and Fu, Xiang and Hull, Richard and Su, Jianwen},
title = {Conversation Specification: A New Approach to Design and Analysis of e-Service Composition},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775210},
doi = {10.1145/775152.775210},
abstract = {This paper introduces a framework for modeling and specifying the global behavior of e-service compositions. Under this framework, peers (individual e-services) communicate through asynchronous messages and each peer maintains a queue for incoming messages. A global "watcher" keeps track of messages as they occur. We propose and study a central notion of a "conversation", which is a sequence of (classes of) messages observed by the watcher. We consider the case where the peers are represented by Mealy machines (finite state machines with input and output). The sets of conversations exhibit unexpected behaviors. For example, there exists a composite e-service based on Mealy peers whose set of conversations is not context free (and not regular). (The set of conversations is always context sensitive.) One cause for this is the queuing of messages; we introduce an operator "prepone" that simulates queue delays from a global perspective and show that the set of conversations of each Mealy e-service is closed under prepone. We illustrate that the global prepone fails to completely capture the queue delay effects and refine prepone to a "local" version on conversations seen by individual peers. On the other hand, Mealy implementations of a composite e-service will always generate conversations whose "projections" are consistent with individual e-services. We use projection-join to reflect such situations. However, there are still Mealy peers whose set of conversations is not the local prepone and projection-join closure of any regular language. Therefore, we propose conversation specifications as a formalism to define the conversations allowed by an e-service composition. We give two technical results concerning the interplay between the local behaviors of Mealy peers and the global behaviors of their compositions. One result shows that for each regular language, its local prepone and projection-join closure corresponds to the set of conversations by some Mealy peers effectively constructed from . The second result gives a condition on the shape of a composition which guarantees that the set of conversations that can be realized is the local prepone and projection-join closure of a regular language.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {403–410},
numpages = {8},
keywords = {e-service composition, conversation specification, communicating finite sate automata},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775211,
author = {Zeng, Liangzhao and Benatallah, Boualem and Dumas, Marlon and Kalagnanam, Jayant and Sheng, Quan Z.},
title = {Quality Driven Web Services Composition},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775211},
doi = {10.1145/775152.775211},
abstract = {The process-driven composition of Web services is emerging as a promising approach to integrate business applications within and across organizational boundaries. In this approach, individual Web services are federated into composite Web services whose business logic is expressed as a process model. The tasks of this process model are essentially invocations to functionalities offered by the underlying component services. Usually, several component services are able to execute a given task, although with different levels of pricing and quality. In this paper, we advocate that the selection of component services should be carried out during the execution of a composite service, rather than at design-time. In addition, this selection should consider multiple criteria (e.g., price, duration, reliability), and it should take into account global constraints and preferences set by the user (e.g., budget constraints). Accordingly, the paper proposes a global planning approach to optimally select component services during the execution of a composite service. Service selection is formulated as an optimization problem which can be solved using efficient linear programming methods. Experimental results show that this global planning approach outperforms approaches in which the component services are selected individually for each task in a composite service.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {411–421},
numpages = {11},
keywords = {QoS, service composition, web services},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775212,
author = {Yesilada, Yeliz and Stevens, Robert and Goble, Carole},
title = {A Foundation for Tool Based Mobility Support for Visually Impaired Web Users},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775212},
doi = {10.1145/775152.775212},
abstract = {Users make journeys through the Web. Web travel encompasses the tasks of orientation and navigation, the environment and the purpose of the journey. The ease of travel, its mobility, varies from page to page and site to site. For visually impaired users, in particular, mobility is reduced; the objects that support travel are inaccessible or missing altogether. Web development tools need to include support to increase mobility. We present a framework for finding and classifying travel objects within Web pages. The evaluation carried out has shown that this framework supports a systematic and consistent method for assessing travel upon the Web. We propose that such a framework can provide the foundation for a semi-automated tool for the support of travel upon the Web.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {422–430},
numpages = {9},
keywords = {travel objects, mobility support tool, visual impairment, mobility, travel},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245007,
author = {McBride, Brian},
title = {Session Details: Establishing the Semantic Web 11},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245007},
doi = {10.1145/3245007},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775214,
author = {Handschuh, Siegfried and Staab, Steffen and Volz, Raphael},
title = {On Deep Annotation},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775214},
doi = {10.1145/775152.775214},
abstract = {The success of the Semantic Web crucially depends on the easy creation, integration and use of semantic data. For this purpose, we consider an integration scenario that defies core assumptions of current metadata construction methods. We describe a framework of metadata creation when web pages are generated from a database and the database owner is cooperatively participating in the Semantic Web. This leads us to the definition of ontology mapping rules by manual semantic annotation and the usage of the mapping rules and of web services for semantic queries. In order to create metadata, the framework combines the presentation layer with the data description layer -- in contrast to "conventional" annotation, which remains at the presentation layer. Therefore, we refer to the framework as deep annotation 1.We consider deep annotation as particularly valid because, (i), web pages generated from databases outnumber static web pages, (ii), annotation of web pages may be a very intuitive way to create semantic data from a database and, (iii), data from databases should not be materialized as RDF files, it should remain where it can be handled most efficiently -- in its databases.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {431–438},
numpages = {8},
keywords = {semantic web, metadata, mapping and merging, annotation, wrapping, information integration},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775215,
author = {Maedche, A. and Motik, B. and Stojanovic, L. and Studer, R. and Volz, R.},
title = {An Infrastructure for Searching, Reusing and Evolving Distributed Ontologies},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775215},
doi = {10.1145/775152.775215},
abstract = {The vision of the Semantic Web can only be realized through proliferation of well-known ontologies describing different domains. To enable interoperability in the Semantic Web, it will be necessary to break these ontologies down into smaller, well-focused units that may be reused. Currently, three problems arise in that scenario. Firstly, it is difficult to locate ontologies to be reused, thus leading to many ontologies modeling the same thing. Secondly, current tools do not provide means for reusing existing ontologies while building new ontologies. Finally, ontologies are rarely static, but are being adapted to changing requirements. Hence, an infrastructure for management of ontology changes, taking into account dependencies between ontologies is needed. In this paper we present such an infrastructure addressing the aforementioned problems.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {439–448},
numpages = {10},
keywords = {ontology evolution, ontology registry, ontology reuse},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245008,
author = {Wills, Craig},
title = {Session Details: Consistency and Replication},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245008},
doi = {10.1145/3245008},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775217,
author = {Gao, Lei and Dahlin, Mike and Nayate, Amol and Zheng, Jiandan and Iyengar, Arun},
title = {Application Specific Data Replication for Edge Services},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775217},
doi = {10.1145/775152.775217},
abstract = {The emerging edge services architecture promises to improve the availability and performance of web services by replicating servers at geographically distributed sites. A key challenge in such systems is data replication and consistency so that edge server code can manipulate shared data without incurring the availability and performance penalties that would be incurred by accessing a traditional centralized database. This paper explores using a distributed object architecture to build an edge service system for an e-commerce application, an online bookstore represented by the TPC-W benchmark. We take advantage of application specific semantics to design distributed objects to manage a specific subset of shared information using simple and effective consistency models. Our experimental results show that by slightly relaxing consistency within individual distributed objects, we can build an edge service system that is highly available and efficient. For example, in one experiment we find that our object-based edge server system provides a factor of five improvement in response time over a traditional centralized cluster architecture and a factor of nine improvement over an edge service system that distributes code but retains a centralized database.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {449–460},
numpages = {12},
keywords = {edge services, performance, data replication, distributed objects, wide area networks (WAN), availability},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775218,
author = {Yuan, Chun and Chen, Yu and Zhang, Zheng},
title = {Evaluation of Edge Caching/Offloading for Dynamic Content Delivery},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775218},
doi = {10.1145/775152.775218},
abstract = {As dynamic content becomes increasingly dominant, it becomes an important research topic as how the edge resources such as client-side proxies, which are otherwise underutilized for such content, can be put into use. However, it is unclear what will be the best strategy and the design/deployment tradeoffs lie therein. In this paper, using one representative e-commerce benchmark, we report our experience of an extensive investigation of different offloading and caching options. Our results point out that, while great benefits can be reached in general, advanced offloading strategies can be overly complex and even counter-productive. In contrast, simple augmentation at proxies to enable fragment caching and page composition achieves most of the benefit without compromising important considerations such as security.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {461–471},
numpages = {11},
keywords = {edge caching, offloading, dynamic content},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775219,
author = {Amini, Lisa and Shaikh, Anees and Schulzrinne, Henning},
title = {Modeling Redirection in Geographically Diverse Server Sets},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775219},
doi = {10.1145/775152.775219},
abstract = {Internet server selection mechanisms attempt to optimize, subject to a variety of constraints, the distribution of client requests to a geographically and topologically diverse pool of servers. Research on server selection has thus far focused primarily on techniques for choosing a server from a group administered by single entity, like a content distribution network provider. In a federated, multi-provider computing system, however, selection must occur over distributed server sets deployed by the participating providers, without the benefit of the full information available in the single-provider case. Intelligent server set selection algorithms will require a model of the expected performance clients would receive from a candidate server set.In this paper, we study whether the complex policies and dynamics of intelligent server selection can be effectively modeled in order to predict client performance for server sets. We introduce a novel server set distance metric, and use it in a measurement study of several million server selection transactions to develop simple models of existing server selection schemes. We then evaluate these models in terms of their ability to accurately predict performance for a second, larger set of distributed clients. We show that our models are able to predict performance within 20ms for over 90% of the observed samples. Our analysis demonstrates that although existing deployments use a variety of complex and dynamic server selection criteria, most of which are proprietary, these schemes can be modeled with surprising accuracy.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {472–481},
numpages = {10},
keywords = {server selection, content distribution network (CDN), web traffic redirection, performance},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245009,
author = {TBA},
title = {Session Details: Open Hypermedia and the Web},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245009},
doi = {10.1145/3245009},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775221,
author = {Karousos, Nikos and Pandis, Ippokratis and Reich, Siegfried and Tzagarakis, Manolis},
title = {Offering Open Hypermedia Services to the WWW: A Step-by-Step Approach for Developers},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775221},
doi = {10.1145/775152.775221},
abstract = {Hypermedia systems and more specifically open hypermedia systems (OHS) provide a rich set of implementations of different hypertext flavors such as navigational hypertext, spatial hypertext or taxonomic hypertext. Additionally, these systems offer component-based modular architectures and address interoperability between hypertext domains. Despite multiple efforts of integrating Web clients, a widespread adoption of OHS technology by Web developers has not taken place. In this paper it is argued that Web Services - which offer a component model for Web applications - can be integrated in OHSs. An architectural integration is proposed, a step-by-step process is outlined and an example of integration is provided. This very approach is aimed to benefit both worlds: the Web community with new rich hypermedia functionality that extends the current navigational hypermedia, and the OHS community by opening its tools and platforms to the many developer groups of the Web community.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {482–489},
numpages = {8},
keywords = {hypermedia services, open hypermedia systems, babylon system, web services},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775222,
author = {Christensen, Bent G. and Hansen, Frank Allan and Bouvin, Niels Olof},
title = {Xspect: Bridging Open Hypermedia and XLink},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775222},
doi = {10.1145/775152.775222},
abstract = {This paper evaluates the XLink format in comparison with other linking formats. The comparison is based on Xspect, an implementation of XLink. Xspect handles transformation between an open hypermedia format (OHIF) and XLink, and the paper discusses this isomorphic transformation and generalises it to include another open hypermedia format, FOHM. The Xspect system, based on XSLT and Javascript, provides users with an interface to browse and merge linkbases. Xspect supports navigational hypermedia in the form of links inserted on the fly into Web pages, as well as guided tours presented as SVG. Xspect has two implementations: one server-side and one running on the client. Both implementation provide the user with an interface for the creation of annotations. The main result of the paper is a critique of XLink. XLink is shown to be a format well suited for navigational hypermedia, but lacking in more advanced constructs. More problematic are the issues regarding large-scale use, such as evaluating validity and credibility of linkbases, and ensuring general support for a format as flexible as XLink.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {490–499},
numpages = {10},
keywords = {XPointer, FOHM, annotations, SVG, Xspect, OHIF, open hypermedia, XLink},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775223,
author = {Mignet, Laurent and Barbosa, Denilson and Veltri, Pierangelo},
title = {The XML Web: A First Study},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775223},
doi = {10.1145/775152.775223},
abstract = {Although originally designed for large-scale electronic publishing, XML plays an increasingly important role in the exchange of data on the Web. In fact, it is expected that XML will become the lingua franca of the Web, eventually replacing HTML. Not surprisingly, there has been a great deal of interest on XML both in industry and in academia. Nevertheless, to date no comprehensive study on the XML Web (i.e., the subset of the Web made of XML documents only) nor on its contents has been made. This paper is the first attempt at describing the XML Web and the documents contained in it. Our results are drawn from a sample of a repository of the publicly available XML documents on the Web, consisting of about 200,000 documents. Our results show that, despite its short history, XML already permeates the Web, both in terms of generic domains and geographically. Also, our results about the contents of the XML Web provide valuable input for the design of algorithms, tools and systems that use XML in one form or another.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {500–510},
numpages = {11},
keywords = {XML documents, XML web, statistical analysis, structural properties},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245010,
author = {Etzioni, Oren},
title = {Session Details: Data Mining},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245010},
doi = {10.1145/3245010},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775225,
author = {Mandhani, Bhushan and Joshi, Sachindra and Kummamuru, Krishna},
title = {A Matrix Density Based Algorithm to Hierarchically Co-Cluster Documents and Words},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775225},
doi = {10.1145/775152.775225},
abstract = {This paper proposes an algorithm to hierarchically cluster documents. Each cluster is actually a cluster of documents and an associated cluster of words, thus a document-word co-cluster. Note that, the vector model for documents creates the document-word matrix, of which every co-cluster is a submatrix. One would intuitively expect a submatrix made up of high values to be a good document cluster, with the corresponding word cluster containing its most distinctive features. Our algorithm looks to exploit this. We have defined matrix density, and our algorithm basically uses matrix density considerations in its working.The algorithm is a partitional-agglomerative algorithm. The partitioning step involves the identification of dense submatrices so that the respective row sets partition the row set of the complete matrix. The hierarchical agglomerative step involves merging the most "similar" submatrices until we are down to the required number of clusters (if we want a flat clustering) or until we have just the single complete matrix left (if we are interested in a hierarchical arrangement of documents). It also generates apt labels for each cluster or hierarchy node. The similarity measure between clusters that we use here for the merging cleverly uses the fact that the clusters here are co-clusters, and is a key point of difference from existing agglomerative algorithms. We will refer to the proposed algorithm as RPSA (Rowset Partitioning and Submatrix Agglomeration). We have compared it as a clustering algorithm with Spherical K-Means and Spectral Graph Partitioning. We have also evaluated some hierarchies generated by the algorithm.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {511–518},
numpages = {8},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775226,
author = {Dave, Kushal and Lawrence, Steve and Pennock, David M.},
title = {Mining the Peanut Gallery: Opinion Extraction and Semantic Classification of Product Reviews},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775226},
doi = {10.1145/775152.775226},
abstract = {The web contains a wealth of product reviews, but sifting through them is a daunting task. Ideally, an opinion mining tool would process a set of search results for a given item, generating a list of product attributes (quality, features, etc.) and aggregating opinions about each of them (poor, mixed, good). We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews. Our classifier draws on information retrieval techniques for feature extraction and scoring, and the results for various metrics and heuristics vary depending on the testing situation. The best methods work as well as or better than traditional machine learning. When operating on individual sentences collected from web searches, performance is limited due to noise and ambiguity. But in the context of a complete web-based tool and aided by a simple method for grouping sentences into attributes, the results are qualitatively quite useful.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {519–528},
numpages = {10},
keywords = {opinion mining, document classification},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775227,
author = {Agrawal, Rakesh and Rajagopalan, Sridhar and Srikant, Ramakrishnan and Xu, Yirong},
title = {Mining Newsgroups Using Networks Arising from Social Behavior},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775227},
doi = {10.1145/775152.775227},
abstract = {Recent advances in information retrieval over hyperlinked corpora have convincingly demonstrated that links carry less noisy information than text. We investigate the feasibility of applying link-based methods in new applications domains. The specific application we consider is to partition authors into opposite camps within a given topic in the context of newsgroups. A typical newsgroup posting consists of one or more quoted lines from another posting followed by the opinion of the author. This social behavior gives rise to a network in which the vertices are individuals and the links represent "responded-to" relationships. An interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree. This behavior is in sharp contrast to the WWW link graph, where linkage is an indicator of agreement or common interest. By analyzing the graph structure of the responses, we are able to effectively classify people into opposite camps. In contrast, methods based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two sides tends to be largely identical, and many newsgroup postings consist of relatively few words of text.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {529–535},
numpages = {7},
keywords = {social network, data mining, web mining, newsgroup, text mining, link analysis},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245011,
author = {Studer, Rudi},
title = {Session Details: Scaling up the Semantic Web},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245011},
doi = {10.1145/3245011},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775229,
author = {Nejdl, Wolfgang and Wolpers, Martin and Siberski, Wolf and Schmitz, Christoph and Schlosser, Mario and Brunkhorst, Ingo and L\"{o}ser, Alexander},
title = {Super-Peer-Based Routing and Clustering Strategies for RDF-Based Peer-to-Peer Networks},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775229},
doi = {10.1145/775152.775229},
abstract = {RDF-based P2P networks have a number of advantages compared with simpler P2P networks such as Napster, Gnutella or with approaches based on distributed indices such as CAN and CHORD. RDF-based P2P networks allow complex and extendable descriptions of resources instead of fixed and limited ones, and they provide complex query facilities against these metadata instead of simple keyword-based searches.In previous papers, we have described the Edutella infrastructure and different kinds of Edutella peers implementing such an RDF-based P2P network. In this paper we will discuss these RDF-based P2P networks as a specific example of a new type of P2P networks, schema-based P2P networks, and describe the use of super-peer based topologies for these networks. Super-peer based networks can provide better scalability than broadcast based networks, and do provide perfect support for inhomogeneous schema-based networks, which support different metadata schemas and ontologies (crucial for the Semantic Web). Furthermore, as we will show in this paper, they are able to support sophisticated routing and clustering strategies based on the metadata schemas, attributes and ontologies used. Especially helpful in this context is the RDF functionality to uniquely identify schemas, attributes and ontologies. The resulting routing indices can be built using dynamic frequency counting algorithms and support local mediation and transformation rules, and we will sketch some first ideas for implementing these advanced functionalities as well.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {536–543},
numpages = {8},
keywords = {peer-to-peer, distributed RDF repositories, semantic web, schema-based routing},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775230,
author = {Christophides, Vassilis and Plexousakis, Dimitris and Scholl, Michel and Tourtounis, Sotirios},
title = {On Labeling Schemes for the Semantic Web},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775230},
doi = {10.1145/775152.775230},
abstract = {This paper focuses on the optimization of the navigation through voluminous subsumption hierarchies of topics employed by Portal Catalogs like Netscape Open Directory (ODP). We advocate for the use of labeling schemes for modeling these hierarchies in order to efficiently answer queries such as subsumption check, descendants, ancestors or nearest common ancestor, which usually require costly transitive closure computations. We first give a qualitative comparison of three main families of schemes, namely bit vector, prefix and interval based schemes. We then show that two labeling schemes are good candidates for an efficient implementation of label querying using standard relational DBMS, namely, the Dewey Prefix scheme [6] and an Interval scheme by Agrawal, Borgida and Jagadish [1]. We compare their storage and query evaluation performance for the 16 ODP hierarchies using the PostgreSQL engine.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {544–555},
numpages = {12},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775231,
author = {Halevy, Alon Y. and Ives, Zachary G. and Mork, Peter and Tatarinov, Igor},
title = {Piazza: Data Management Infrastructure for Semantic Web Applications},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775231},
doi = {10.1145/775152.775231},
abstract = {The Semantic Web envisions a World Wide Web in which data is described with rich semantics and applications can pose complex queries. To this point, researchers have defined new languages for specifying meanings for concepts and developed techniques for reasoning about them, using RDF as the data model. To flourish, the Semantic Web needs to be able to accommodate the huge amounts of existing data and the applications operating on them. To achieve this, we are faced with two problems. First, most of the world's data is available not in RDF but in XML; XML and the applications consuming it rely not only on the domain structure of the data, but also on its document structure. Hence, to provide interoperability between such sources, we must map between both their domain structures and their document structures. Second, data management practitioners often prefer to exchange data through local point-to-point data translations, rather than mapping to common mediated schemas or ontologies.This paper describes the Piazza system, which addresses these challenges. Piazza offers a language for mediating between data sources on the Semantic Web, which maps both the domain structure and document structure. Piazza also enables interoperation of XML data with RDF data that is accompanied by rich OWL ontologies. Mappings in Piazza are provided at a local scale between small sets of nodes, and our query answering algorithm is able to chain sets mappings together to obtain relevant data from across the Piazza network. We also describe an implemented scenario in Piazza and the lessons we learned from it.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {556–567},
numpages = {12},
keywords = {semantic web, peer data management systems, XML},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245012,
author = {Haveliwala, Taher},
title = {Session Details: Dynamic Services and Analysis},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245012},
doi = {10.1145/3245012},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775233,
author = {Kumar, Ravi and Novak, Jasmine and Raghavan, Prabhakar and Tomkins, Andrew},
title = {On the Bursty Evolution of Blogspace},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775233},
doi = {10.1145/775152.775233},
abstract = {We propose two new tools to address the evolution of hyperlinked corpora. First, we define time graphs to extend the traditional notion of an evolving directed graph, capturing link creation as a point phenomenon in time. Second, we develop definitions and algorithms for time-dense community tracking, to crystallize the notion of community evolution. We develop these tools in the context of Blogspace , the space of weblogs (or blogs). Our study involves approximately 750K links among 25K blogs. We create a time graph on these blogs by an automatic analysis of their internal time stamps. We then study the evolution of connected component structure and microscopic community structure in this time graph. We show that Blogspace underwent a transition behavior around the end of 2001, and has been rapidly expanding over the past year, not just in metrics of scale, but also in metrics of community structure and connectedness. This expansion shows no sign of abating, although measures of connectedness must plateau within two years. By randomizing link destinations in Blogspace, but retaining sources and timestamps, we introduce a concept of randomized Blogspace . Herein, we observe similar evolution of a giant component, but no corresponding increase in community structure. Having demonstrated the formation of micro-communities over time, we then turn to the ongoing activity within active communities. We extend recent work of Kleinberg [11] to discover dense periods of "bursty" intra-community link creation.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {568–576},
numpages = {9},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775234,
author = {Bawa, Mayank and Bayardo, Roberto J. and Rajagopalan, Sridhar and Shekita, Eugene J.},
title = {Make It Fresh, Make It Quick: Searching a Network of Personal Webservers},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775234},
doi = {10.1145/775152.775234},
abstract = {Personal webservers have proven to be a popular means of sharing files and peer collaboration. Unfortunately, the transient availability and rapidly evolving content on such hosts render centralized, crawl-based search indices stale and incomplete. To address this problem, we propose YouSearch, a distributed search application for personal webservers operating within a shared context (e.g., a corporate intranet). With YouSearch, search results are always fast, fresh and complete -- properties we show arise from an architecture that exploits both the extensive distributed resources available at the peer webservers in addition to a centralized repository of summarized network state. YouSearch extends the concept of a shared context within web communities by enabling peers to aggregate into groups and users to search over specific groups. In this paper, we describe the challenges, design, implementation and experiences with a successful intranet deployment of YouSearch.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {577–586},
numpages = {10},
keywords = {intranet search, web search, decentralized systems, P2P, information communities, peer-to-peer networks},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775235,
author = {Li, Wen-Syan and Po, Oliver and Hsiung, Wang-Pin and Candan, K. Sel\c{c}uk and Agrawal, Divyakant},
title = {Engineering and Hosting Adaptive Freshness-Sensitive Web Applications on Data Centers},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775235},
doi = {10.1145/775152.775235},
abstract = {Wide-area database replication technologies and the availability of content delivery networks allow Web applications to be hosted and served from powerful data centers. This form of application support requires a complete Web application suite to be distributed along with the database replicas. A major advantage of this approach is that dynamic content is served from locations closer to users, leading into reduced network latency and fast response times. However, this is achieved at the expense of overheads due to (a) invalidation of cached dynamic content in the edge caches and (b) synchronization of database replicas in the data center. These have adverse effects on the freshness of delivered content. In this paper, we propose a freshness-driven adaptive dynamic content caching, which monitors the system status and adjusts caching policies to provide content freshness guarantees. The proposed technique has been intensively evaluated to validate its effectiveness. The experimental results show that the freshness-driven adaptive dynamic content caching technique consistently provides good content freshness. Furthermore, even a Web site that enables dynamic content caching can further benefit from our solution, which improves content freshness up to 7 times, especially under heavy user request traffic and long network latency conditions. Our approach also provides better scalability and significantly reduced response times up to 70% in the experiments.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {587–598},
numpages = {12},
keywords = {net-work latency, response time, database-driven web applications, web acceleration, dynamic content, freshness},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245013,
author = {Krishnamurthy, Balachander},
title = {Session Details: CDNs and Caching},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245013},
doi = {10.1145/3245013},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775237,
author = {Mikhailov, Mikhail and Wills, Craig E.},
title = {Evaluating a New Approach to Strong Web Cache Consistency with Snapshots of Collected Content},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775237},
doi = {10.1145/775152.775237},
abstract = {The problem of Web cache consistency continues to be an important one. Current Web caches use heuristic-based policies for determining the freshness of cached objects, often forcing content providers to unnecessarily mark their content as uncacheable simply to retain control over it. Server-driven invalidation has been proposed as a mechanism for providing strong cache consistency for Web objects, but it requires servers to maintain per-client state even for infrequently changing objects. We propose an alternative approach to strong cache consistency, called MONARCH, which does not require servers to maintain per-client state. In this work we focus on a new approach for evaluation of MONARCH in comparison with current practice and other cache consistency policies. This approach uses snapshots of content collected from real Web sites as input to a simulator. Results of the evaluation show MONARCH generates little more request traffic than an optimal cache coherency policy.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {599–608},
numpages = {10},
keywords = {collected content, web caching, object composition, object relationships, server invalidation, change characteristics, cache consistency},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775238,
author = {Kulkarni, Purushottam and Shenoy, Prashant and Gong, Weibo},
title = {Scalable Techniques for Memory-Efficient CDN Simulations},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775238},
doi = {10.1145/775152.775238},
abstract = {Since CDN simulations are known to be highly memory-intensive, in this paper, we argue the need for reducing the memory requirements of such simulations. We propose a novel memory-efficient data structure that stores cache state for a small subset of popular objects accurately and uses approximations for storing the state for the remaining objects. Since popular objects receive a large fraction of the requests while less frequently accessed objects consume much of the memory space, this approach yields large memory savings and reduces errors. We use bloom filters to store approximate state and show that careful choice of parameters can substantially reduce the probability of errors due to approximations. We implement our techniques into a user library for constructing proxy caches in CDN simulators. Our experimental results show up to an order of magnitude reduction in memory requirements of CDN simulations, while incurring a 5-10% error.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {609–618},
numpages = {10},
keywords = {simulation, content distribution networks, approximate data structures, web proxy cache},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775239,
author = {Rhea, Sean C. and Liang, Kevin and Brewer, Eric},
title = {Value-Based Web Caching},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775239},
doi = {10.1145/775152.775239},
abstract = {Despite traditional web caching techniques, redundant data is often transferred over HTTP links. These redundant transfers result from both resource modification and aliasing. Resource modification causes the data represented by a single URI to change; often, in transferring the new data, some old data is retransmitted. Aliasing, in contrast, occurs when the same data is named by multiple URIs, often in the context of dynamic or advertising content. Traditional web caching techniques index data by its name and thus often fail to recognize and take advantage of aliasing.Despite traditional web caching techniques, redundant data is often transferred over HTTP links. These redundant transfers result from both resource modification and aliasing. Resource modification causes the data represented by a single URI to change; often, in transferring the new data, some old data is retransmitted. Aliasing, in contrast, occurs when the same data is named by multiple URIs, often in the context of dynamic or advertising content. Traditional web caching techniques index data by its name and thus often fail to recognize and take advantage of aliasing.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {619–628},
numpages = {10},
keywords = {privacy, dynamic content, proxy, caching, scalability, hypertext transfer protocol, WWW, redundant transfers, HTTP, duplicate suppression, world wide web, aliasing, resource modification},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245014,
author = {Huang, Yao-Wen},
title = {Session Details: Protocols},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245014},
doi = {10.1145/3245014},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775241,
author = {Agrawal, Rakesh and Kiernan, Jerry and Srikant, Ramakrishnan and Xu, Yirong},
title = {An XPath-Based Preference Language for P3P},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775241},
doi = {10.1145/775152.775241},
abstract = {The Platform for Privacy Preferences (P3P) is the most significant effort currently underway to enable web users to gain control over their private information. The designers of P3P simultaneously designed a preference language called APPEL to allow users to express their privacy preferences, thus enabling automatic matching of privacy preferences against P3P policies. Unfortunately subtle interactions between P3P and APPEL result in serious problems when using APPEL: Users can only directly specify what is unacceptable in a policy, not what is acceptable; simple preferences are hard to express; and writing APPEL preferences is error prone. We show that these problems follow from a fundamental design choice made by APPEL, and cannot be solved without completely redesigning the language. Therefore we explore alternatives to APPEL that can overcome these problems. In particular, we show that XPath serves quite nicely as a preference language and solves all the above problems. We identify the minimal subset of XPath that is needed, thus allowing matching programs to potentially use a smaller memory footprint. We also give an APPEL to XPath translator that shows that XPath is as expressive as APPEL.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {629–639},
numpages = {11},
keywords = {hippocratic databases, preference, P3P, XPath, APPEL, XPref, privacy-aware data management},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775242,
author = {Kamvar, Sepandar D. and Schlosser, Mario T. and Garcia-Molina, Hector},
title = {The Eigentrust Algorithm for Reputation Management in P2P Networks},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775242},
doi = {10.1145/775152.775242},
abstract = {Peer-to-peer file-sharing networks are currently receiving much attention as a means of sharing and distributing information. However, as recent experience shows, the anonymous, open nature of these networks offers an almost ideal environment for the spread of self-replicating inauthentic files.We describe an algorithm to decrease the number of downloads of inauthentic files in a peer-to-peer file-sharing network that assigns each peer a unique global trust value, based on the peer's history of uploads. We present a distributed and secure method to compute global trust values, based on Power iteration. By having peers use these global trust values to choose the peers from whom they download, the network effectively identifies malicious peers and isolates them from the network.In simulations, this reputation system, called EigenTrust, has been shown to significantly decrease the number of inauthentic files on the network, even under a variety of conditions where malicious peers cooperate in an attempt to deliberately subvert the system.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {640–651},
numpages = {12},
keywords = {distributed eigenvector computation, peer-to-peer, reputation},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775243,
author = {Zeng, Chun and Xing, Chun-Xiao and Zhou, Li-Zhu},
title = {Similarity Measure and Instance Selection for Collaborative Filtering},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775243},
doi = {10.1145/775152.775243},
abstract = {Collaborative filtering has been very successful in both research and applications such as information filtering and E-commerce. The k-Nearest Neighbor (KNN) method is a popular way for its realization. Its key technique is to find k nearest neighbors for a given user to predict his interests. However, this method suffers from two fundamental problems: sparsity and scalability. In this paper, we present our solutions for these two problems. We adopt two techniques: a matrix conversion method for similarity measure and an instance selection method. And then we present an improved collaborative filtering algorithm based on these two methods. In contrast with existing collaborative algorithms, our method shows its satisfactory accuracy and performance.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {652–658},
numpages = {7},
keywords = {collaborative filtering, similarity measure, instance selection},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245015,
author = {Baeza-Yates, Ricardo},
title = {Session Details: Web Crawling and Measurement},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245015},
doi = {10.1145/3245015},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775245,
author = {Pandey, Sandeep and Ramamritham, Krithi and Chakrabarti, Soumen},
title = {Monitoring the Dynamic Web to Respond to Continuous Queries},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775245},
doi = {10.1145/775152.775245},
abstract = {Continuous queries are queries for which responses given to users must be continuously updated, as the sources of interest get updated. Such queries occur, for instance, during on-line decision making, e.g., traffic flow control, weather monitoring, etc. The problem of keeping the responses current reduces to the problem of deciding how often to visit a source to determine if and how it has been modified, in order to update earlier responses accordingly. On the surface, this seems to be similar to the crawling problem since crawlers attempt to keep indexes up-to-date as pages change and users pose search queries. We show that this is not the case, both due to the inherent differences between the nature of the two problems as well as the performance metric. We propose, develop and evaluate a novel multi-phase (Continuous Adaptive Monitoring) (CAM) solution to the problem of maintaining the currency of query results. Some of the important phases are: The tracking phase, in which changes, to an initially identified set of relevant pages, are tracked. From the observed change characteristics of these pages, a probabilistic model of their change behavior is formulated and weights are assigned to pages to denote their importance for the current queries. During the next phase, the resource allocation phase, based on these statistics, resources, needed to continuously monitor these pages for changes, are allocated. Given these resource allocations, the scheduling phase produces an optimal achievable schedule for the monitoring tasks. An experimental evaluation of our approach compared to prior approaches for crawling dynamic web pages shows the effectiveness of CAM for monitoring dynamic changes. For example, by monitoring just 5% of the page changes, CAM is able to return 90% of the changed information to the users. The experiments also produce some interesting observations pertaining to the differences between the two problems of crawling--to build an index--and the problem of change tracking--to respond to continuous queries.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {659–668},
numpages = {10},
keywords = {continuous queries, allocation policies},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775246,
author = {Fetterly, Dennis and Manasse, Mark and Najork, Marc and Wiener, Janet},
title = {A Large-Scale Study of the Evolution of Web Pages},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775246},
doi = {10.1145/775152.775246},
abstract = {How fast does the web change? Does most of the content remain unchanged once it has been authored, or are the documents continuously updated? Do pages change a little or a lot? Is the extent of change correlated to any other property of the page? All of these questions are of interest to those who mine the web, including all the popular search engines, but few studies have been performed to date to answer them.One notable exception is a study by Cho and Garcia-Molina, who crawled a set of 720,000 pages on a daily basis over four months, and counted pages as having changed if their MD5 checksum changed. They found that 40% of all web pages in their set changed within a week, and 23% of those pages that fell into the .com domain changed daily.This paper expands on Cho and Garcia-Molina's study, both in terms of coverage and in terms of sensitivity to change. We crawled a set of 150,836,209 HTML pages once every week, over a span of 11 weeks. For each page, we recorded a checksum of the page, and a feature vector of the words on the page, plus various other data such as the page length, the HTTP status code, etc. Moreover, we pseudo-randomly selected 0.1% of all of our URLs, and saved the full text of each download of the corresponding pages.After completion of the crawl, we analyzed the degree of change of each page, and investigated which factors are correlated with change intensity. We found that the average degree of change varies widely across top-level domains, and that larger pages change more often and more severely than smaller ones.This paper describes the crawl and the data transformations we performed on the logs, and presents some statistical observations on the degree of change of different classes of pages.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {669–678},
numpages = {10},
keywords = {web pages, degree of change, web characterization, web evolution, rate of change},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775247,
author = {Broder, Andrei Z. and Najork, Marc and Wiener, Janet L.},
title = {Efficient URL Caching for World Wide Web Crawling},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775247},
doi = {10.1145/775152.775247},
abstract = {Crawling the web is deceptively simple: the basic algorithm is (a) Fetch a page (b) Parse it to extract all linked URLs (c) For all the URLs not seen before, repeat (a)-(c). However, the size of the web (estimated at over 4 billion pages) and its rate of change (estimated at 7% per week) move this plan from a trivial programming exercise to a serious algorithmic and system design challenge. Indeed, these two factors alone imply that for a reasonably fresh and complete crawl of the web, step (a) must be executed about a thousand times per second, and thus the membership test (c) must be done well over ten thousand times per second against a set too large to store in main memory. This requires a distributed architecture, which further complicates the membership test.A crucial way to speed up the test is to cache, that is, to store in main memory a (dynamic) subset of the "seen" URLs. The main goal of this paper is to carefully investigate several URL caching techniques for web crawling. We consider both practical algorithms: random replacement, static cache, LRU, and CLOCK, and theoretical limits: clairvoyant caching and infinite cache. We performed about 1,800 simulations using these algorithms with various cache sizes, using actual log data extracted from a massive 33 day web crawl that issued over one billion HTTP requests.Our main conclusion is that caching is very effective - in our setup, a cache of roughly 50,000 entries can achieve a hit rate of almost 80%. Interestingly, this cache size falls at a critical point: a substantially smaller cache is much less effective while a substantially larger cache brings little additional benefit. We conjecture that such critical points are inherent to our problem and venture an explanation for this phenomenon.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {679–689},
numpages = {11},
keywords = {web crawlers, distributed crawlers, crawling, caching, URL caching, web graph models},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245016,
author = {Patel-Schneider, Peter},
title = {Session Details: Using the Semantic Web},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245016},
doi = {10.1145/3245016},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775249,
author = {Anyanwu, Kemafor and Sheth, Amit},
title = {Ρ-Queries: Enabling Querying for Semantic Associations on the Semantic Web},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775249},
doi = {10.1145/775152.775249},
abstract = {This paper presents the notion of Semantic Associations as complex relationships between resource entities. These relationships capture both a connectivity of entities as well as similarity of entities based on a specific notion of similarity called r-isomorphism. It formalizes these notions for the RDF data model, by introducing a notion of a Property Sequence as a type. In the context of a graph model such as that for RDF, Semantic Associations amount to specific certain graph signatures. Specifically, they refer to sequences (i.e. directed paths) here called Property Sequences, between entities, networks of Property Sequences (i.e. undirected paths), or subgraphs of r-isomorphic Property Sequences.The ability to query about the existence of such relationships is fundamental to tasks in analytical domains such as national security and business intelligence, where tasks often focus on finding complex yet meaningful and obscured relationships between entities. However, support for such queries is lacking in contemporary query systems, including those for RDF.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {690–699},
numpages = {10},
keywords = {graph traversals, complex data relationships, semantic web querying, semantic associations, RDF},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775250,
author = {Guha, R. and McCool, Rob and Miller, Eric},
title = {Semantic Search},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775250},
doi = {10.1145/775152.775250},
abstract = {Activities such as Web Services and the Semantic Web are working to create a web of distributed machine understandable data. In this paper we present an application called 'Semantic Search' which is built on these supporting technologies and is designed to improve traditional web searching. We provide an overview of TAP, the application framework upon which the Semantic Search is built. We describe two implemented Semantic Search systems which, based on the denotation of the search query, augment traditional search results with relevant data aggregated from distributed sources. We also discuss some general issues related to searching and the Semantic Web and outline how an understanding of the semantics of the search terms can be used to provide better results.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {700–709},
numpages = {10},
keywords = {search, semantic web},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775251,
author = {Gibbins, Nicholas and Harris, Stephen and Shadbolt, Nigel},
title = {Agent-Based Semantic Web Services},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775251},
doi = {10.1145/775152.775251},
abstract = {The Web Services world consists of loosely-coupled distributed systems which adapt to ad-hoc changes by the use of service descriptions that enable opportunistic service discovery. At present, these service descriptions are semantically impoverished, being concerned with describing the functional signature of the services rather than characterising their meaning. In the Semantic Web community, the DAML Services effort attempts to rectify this by providing a more expressive way of describing Web services using ontologies. However, this approach does not separate the domain-neutral communicative intent of a message (considered in terms of speech acts) from its domain-specific content, unlike similar developments from the multi-agent systems community.In this paper, we describe our experiences of designing and building an ontologically motivated Web Services system for situational awareness and information triage in a simulated humanitarian aid scenario. In particular, we discuss the merits of using techniques from the multi-agent systems community for separating the intentional force of messages from their content, and the implementation of these techniques within the DAML Services model.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {710–717},
numpages = {8},
keywords = {web services, DAML-S, semantic web, agent communication languages, ontologies},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/3245017,
author = {Soffer, Aya},
title = {Session Details: Improving the Browsing Experience},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245017},
doi = {10.1145/3245017},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
numpages = {1},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775253,
author = {Coles, Alistair and Deliot, Eric and Melamed, Tom and Lansard, Kevin},
title = {A Framework for Coordinated Multi-Modal Browsing with Multiple Clients},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775253},
doi = {10.1145/775152.775253},
abstract = {As users acquire or gain access to an increasingly diverse range of web access clients, web applications are adapting their user interfaces to support multiple modalities on multiple client types. User experiences can be enhanced by clients with differing capabilities combining to provide a distributed user interface to applications. Indeed, users will be frustrated if their interaction with applications is limited to one client at a time.This paper discusses the requirements for coordinating web interaction across an aggregation of clients. We present a framework for multi-device browsing that provides both coordinated navigation between web resources and coordinated interaction between variants, or representations, of those resources once instantiated in the clients. The framework protects the application from some of the complexities of client aggregation.We show how a small number of enhancements to the XForms and XML Events vocabularies can facilitate coordination between clients and provide an appropriate level of control to applications. We also describe a novel proxy which consolidates HTTP requests from aggregations of clients and reduces the burden that multi-client browsing places on the application.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {718–726},
numpages = {9},
keywords = {web proxy, multi-modal browsing},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775254,
author = {Nadamoto, Akiyo and Tanaka, Katsumi},
title = {A Comparative Web Browser (CWB) for Browsing and Comparing Web Pages},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775254},
doi = {10.1145/775152.775254},
abstract = {In this paper, we propose a new type of Web browser, called the Comparative Web Browser(CWB), which concurrently presents multiple Web pages in a way that enables the content of the Web pages to be automatically synchronized. The ability to view multiple Web pages at one time is useful when we wish to make a comparison on the Web, such as when we compare similar products or news articles from different newspapers. The CWB is characterized by (1) automatic content-based retrieval of passages from another Web page based on a passage of the Web page the user is reading, and (2) automatic transformation of a user's behavior (scrolling, clicking, or moving backward or forward) on a Web page into a series of behaviors on the other Web pages. The CWB tries to concurrently present "similar" passages from different Web pages, and for this purpose our CWB automatically navigates Web pages that contain passages similar to those of the initial Web page. Furthermore, we propose an enhancement to the CWB, which enables it to use linkage information to find related documents based on link structure.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {727–735},
numpages = {9},
keywords = {web browser, passage retrieval, content synchronization, comparison},
location = {Budapest, Hungary},
series = {WWW '03}
}

@inproceedings{10.1145/775152.775255,
author = {Obendorf, Hartmut and Weinreich, Harald},
title = {Comparing Link Marker Visualization Techniques: Changes in Reading Behavior},
year = {2003},
isbn = {1581136803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775152.775255},
doi = {10.1145/775152.775255},
abstract = {Links are one of the most important means for navigation in the World Wide Web. However, the visualization of and the interaction with Web links have been scarcely explored, although Links have severe implications on the appearance and usability of Web pages and the World Wide Web as such.This paper presents two studies giving first insights of the effects of link visualization techniques on reading habits and performance. The first user study compares different highlighting techniques for link markers and evaluates their effect on reading performance and user acceptance. The second study examines links-on-demand, links that appear when pressing a dedicated key, and discusses their possible effects on reading and browsing habits.The findings of the conducted studies imply that the standard appearance of link markers has seriously underestimated effects on the usability of Web pages. They can significantly reduce the readability of the text, and alternatives should be carefully considered for the design of future Web browsers.},
booktitle = {Proceedings of the 12th International Conference on World Wide Web},
pages = {736–745},
numpages = {10},
keywords = {link marking techniques, hypermedia, links-on-demand, usability survey, input interaction technologies},
location = {Budapest, Hungary},
series = {WWW '03}
}

