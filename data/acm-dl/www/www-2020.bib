@inproceedings{10.1145/3366423.3380089,
author = {Zhang, Ningyu and Deng, Shumin and Sun, Zhanlin and Chen, Jiaoyan and Zhang, Wei and Chen, Huajun},
title = {Relation Adversarial Network for Low Resource Knowledge Graph Completion},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380089},
doi = {10.1145/3366423.3380089},
abstract = {Knowledge Graph Completion (KGC) has been proposed to improve Knowledge Graphs by filling in missing connections via link prediction or relation extraction. One of the main difficulties for KGC is a low resource problem. Previous approaches assume sufficient training triples to learn versatile vectors for entities and relations, or a satisfactory number of labeled sentences to train a competent relation extraction model. However, low resource relations are very common in KGs, and those newly added relations often do not have many known samples for training. In this work, we aim at predicting new facts under a challenging setting where only limited training instances are available. We propose a general framework called Weighted Relation Adversarial Network, which utilizes an adversarial procedure to help adapt knowledge/features learned from high resource relations to different but related low resource relations. Specifically, the framework takes advantage of a relation discriminator to distinguish between samples from different relations, and help learn relation-invariant features more transferable from source relations to target relations. Experimental results show that the proposed approach outperforms previous methods regarding low resource settings for both link prediction and relation extraction.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1–12},
numpages = {12},
keywords = {Link Prediction, Relation Extraction, Knowledge Graphs, Low Resource Knowledge Graph Completion, Adversarial Transfer Learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380090,
author = {Zheng, Wenbo and Gou, Chao and Yan, Lan and Mo, Shaocong},
title = {Learning to Classify: A Flow-Based Relation Network for Encrypted Traffic Classification},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380090},
doi = {10.1145/3366423.3380090},
abstract = {As the size and source of network traffic increase, so does the challenge of monitoring and analyzing network traffic. The challenging problems of classifying encrypted traffic are the imbalanced property of network data, the generalization on an unseen dataset, and overly dependent on data size. In this paper, we propose an application of a meta-learning approach to address these problems in encrypted traffic classification, named Flow-Based Relation Network (RBRN). The RBRN is an end-to-end classification model that learns representative features from the raw flows and then classifies them in a unified framework. Moreover, we design “hallucinator” to produce additional training samples for the imbalanced classification, and then focus on meta-learning to classify unseen categories from few labeled samples. We validate the effectiveness of the RBRN on the real-world network traffic dataset, and the experimental results demonstrate that the RBRN can achieve an excellent classification performance and outperform the state-of-the-art methods on encrypted traffic classification. What is more interesting, our model trained on the real-world dataset can generalize very well to unseen datasets, outperforming multiple state-of-art methods.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {13–22},
numpages = {10},
keywords = {meta-learning, relational networks, traffic classification, data augmentation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380091,
author = {Chen, Xi and Li, Hang and Zhou, Chenyi and Liu, Xue and Wu, Di and Dudek, Gregory},
title = {FiDo: Ubiquitous Fine-Grained WiFi-Based Localization for Unlabelled Users via Domain Adaptation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380091},
doi = {10.1145/3366423.3380091},
abstract = {To fully support the emerging location-aware applications, location information with meter-level resolution (or even higher) is required anytime and anywhere. Unfortunately, most of the current location sources (e.g., GPS and check-in data) either are unavailable indoor or provide only house-level resolutions. To fill the gap, this paper utilizes the ubiquitous WiFi signals to establish a (sub)meter-level localization system, which employs WiFi propagation characteristics as location fingerprints. However, an unsolved issue of these WiFi fingerprints lies in their inconsistency across different users. In other words, WiFi fingerprints collected from one user may not be used to localize another user. To address this issue, we propose a WiFi-based Domain-adaptive system FiDo, which is able to localize many different users with labelled data from only one or two example users. FiDo contains two modules: 1) a data augmenter that introduces data diversity using a Variational Autoencoder (VAE); and 2) a domain-adaptive classifier that adjusts itself to newly collected unlabelled data using a joint classification-reconstruction structure. Compared to the state of the art, FiDo increases average F1 score by 11.8% and improves the worst-case accuracy by 20.2%.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {23–33},
numpages = {11},
keywords = {domain adaptation, data augmentation, WiFi-based localization},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380092,
author = {Chapuis, Bertil and Omolola, Olamide and Cherubini, Mauro and Humbert, Mathias and Huguenin, K\'{e}vin},
title = {An Empirical Study of the Use of Integrity Verification Mechanisms for Web Subresources},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380092},
doi = {10.1145/3366423.3380092},
abstract = {Web developers can (and do) include subresources such as scripts, stylesheets and images in their webpages. Such subresources might be stored on content delivery networks (CDNs). This practice creates security and privacy risks, should a subresource be corrupted. The subresource integrity (SRI) recommendation, released in mid-2016 by the W3C, enables developers to include digests in their webpages in order for web browsers to verify the integrity of subresources before loading them. In this paper, we conduct the first large-scale longitudinal study of the use of SRI on the Web by analyzing massive crawls (≈ 3B URLs) of the Web over the last 3.5 years. Our results show that the adoption of SRI is modest (≈), but grows at an increasing rate and is highly influenced by the practices of popular library developers (e.g., Bootstrap) and CDN operators (e.g., jsDelivr). We complement our analysis about SRI with a survey of web developers (N=): It shows that a substantial proportion of developers know SRI and understand its basic functioning, but most of them ignore important aspects of the recommendation. The results of the survey also show that the integration of SRI by developers is mostly manual – hence not scalable and error prone. This calls for a better integration of SRI in build tools.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {34–45},
numpages = {12},
keywords = {common crawl, subresource integrity, web security},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380093,
author = {Xu, Wanyue and Sheng, Yibin and Zhang, Zuobai and Kan, Haibin and Zhang, Zhongzhi},
title = {Power-Law Graphs Have Minimal Scaling of Kemeny Constant for Random Walks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380093},
doi = {10.1145/3366423.3380093},
abstract = {The mean hitting time from a node i to a node j selected randomly according to the stationary distribution of random walks is called the Kemeny constant, which has found various applications. It was proved that over all graphs with N vertices, complete graphs have the exact minimum Kemeny constant, growing linearly with N. Here we study numerically or analytically the Kemeny constant on many sparse real-world and model networks with scale-free small-world topology, and show that their Kemeny constant also behaves linearly with N. Thus, sparse networks with scale-free and small-world topology are favorable architectures with optimal scaling of Kemeny constant. We then present a theoretically guaranteed estimation algorithm, which approximates the Kemeny constant for a graph in nearly linear time with respect to the number of edges. Extensive numerical experiments on model and real networks show that our approximation algorithm is both efficient and accurate.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {46–56},
numpages = {11},
keywords = {Normalized Laplacian matrix, Random walk, Hitting time, Linear system solver, Kemeny constant, Spectral graph theory, Random projection},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380094,
author = {Xu, Furong and Zhang, Wei and Cheng, Yuan and Chu, Wei},
title = {Metric Learning with Equidistant and Equidistributed Triplet-Based Loss for Product Image Search},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380094},
doi = {10.1145/3366423.3380094},
abstract = {Product image search in E-commerce systems is a challenging task, because of a huge number of product classes, low intra-class similarity and high inter-class similarity. Deep metric learning, based on paired distances independent of the number of classes, aims to minimize intra-class variances and inter-class similarity in feature embedding space. Most existing approaches strictly restrict the distance between samples with fixed values to distinguish different classes of samples. However, the distance of paired samples has various magnitudes during different training stages. Therefore, it is difficult to directly restrict absolute distances with fixed values. In this paper, we propose a novel Equidistant and Equidistributed Triplet-based (EET) loss function to adjust the distance between samples with relative distance constraints. By optimizing the loss function, the algorithm progressively maximizes intra-class similarity and inter-class variances. Specifically, 1) the equidistant loss pulls the matched samples closer by adaptively constraining two samples of the same class to be equally distant from another one of a different class in each triplet, 2) the equidistributed loss pushes the mismatched samples farther away by guiding different classes to be uniformly distributed while keeping intra-class structure compact in embedding space. Extensive experimental results on product search benchmarks verify the improved performance of our method. We also achieve improvements on other retrieval datasets, which show superior generalization capacity of our method in image search.  },
booktitle = {Proceedings of The Web Conference 2020},
pages = {57–65},
numpages = {9},
keywords = {Equidistant Loss and Equidistributed Loss, Product Image Search, Triplet-based Loss, Metric Learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380095,
author = {Li, Tong and Zhang, Mingyang and Cao, Hancheng and Li, Yong and Tarkoma, Sasu and Hui, Pan},
title = {”What Apps Did You Use?”: Understanding the Long-Term Evolution of Mobile App Usage},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380095},
doi = {10.1145/3366423.3380095},
abstract = {The prevalence of smartphones has promoted the popularity of mobile apps in recent years. Although significant effort has been made to understand mobile app usage, existing studies are based primarily on short-term datasets with limited time span, e.g., a few months. Therefore, many basic facts about the long-term evolution of mobile app usage are unknown. In this paper, we study how mobile app usage evolves over a long-term period. We first introduce an app usage collection platform named carat, from which we have gathered app usage records of 1,465 users from 2012 to 2017. We then conduct the first study on the long-term evolution processes on a macro-level, i.e., app-category, and micro-level, i.e., individual app. We discover that, on both levels, there is a growth stage enabled by the introduction of new technologies. Then there is a plateau stage caused by high correlations between app categories and a pareto effect in individual app usage, respectively. Additionally, the evolution of individual app usage undergoes an elimination stage due to fierce intra-category competition. Nevertheless, the diverseness of app-category and individual app usage exhibit opposing trends: app-category usage assimilates while individual app usage diversifies. Our study provides useful implications for app developers, market intermediaries, and service providers.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {66–76},
numpages = {11},
keywords = {App usage, App categories, Long-term evolution, Google play},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380096,
author = {Lin, Yusan and Moosaei, Maryam and Yang, Hao},
title = {OutfitNet: Fashion Outfit Recommendation with Attention-Based Multiple Instance Learning},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380096},
doi = {10.1145/3366423.3380096},
abstract = {Recommending fashion outfits to users presents several challenges. First of all, an outfit consists of multiple fashion items, and each user emphasizes different parts of an outfit when considering whether they like it or not. Secondly, a user’s liking for a fashion outfit considers not only the aesthetics of each item but also the compatibility among them. Lastly, fashion outfit data is often sparse in terms of the relationship between users and fashion outfits. Not to mention, we can only obtain what the users like, but not what they dislike. To address the above challenges, in this paper, we formulate the fashion outfit recommendation problem as a multiple-instance-learning (MIL) problem. We propose OutfitNet, a fashion outfit recommendation framework that includes two stages. The first stage is a Fashion Item Relevancy network (FIR), which learns the compatibility between fashion items and further generates relevancy embedding of fashion items. In the second stage, an Outfit Preference network (OP) learns the users’ tastes for fashion outfits using visual information. OutfitNet takes in multiple fashion items in a fashion outfit as input, learns the compatibility among fashion items, the users’ tastes toward each item, as well as the users’ attention on different items in the outfit with the attention mechanism. Quantitatively, our experiments show that OutfitNet outperforms state-of-the-art models in two tasks: fill-in-the-blank (FITB) and personalized outfit recommendation. Qualitatively, we demonstrate that the learned personalized item scores and attention scores capture well the users’ fashion tastes, and the learned fashion item embeddings capture well the compatibility relationships among fashion items. We also leverage the learned fashion item embedding and propose a simple fashion outfit generation framework, which is shown to produce high-quality fashion outfit combinations.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {77–87},
numpages = {11},
keywords = {fashion item relevancy, fashion outfit generation, fashion outfit recommendation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380097,
author = {He, Suining and Shin, Kang G.},
title = {Towards Fine-Grained Flow Forecasting: A Graph Attention Approach for Bike Sharing Systems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380097},
doi = {10.1145/3366423.3380097},
abstract = {As a healthy, efficient and green alternative to motorized urban travel, bike sharing has been increasingly popular, leading to wide deployment and use of bikes instead of cars. Accurate bike-flow prediction at the individual station level is essential for bike sharing service. Due to the spatial and temporal complexities of traffic networks and the lack of data-driven design for bike stations, existing methods cannot predict the fine-grained bike flows to/from each station. To remedy this problem, we propose a novel data-driven spatio-temporal Graph attention convolutional neural network for Bikestation-level flow prediction (GBikes). We develop data-driven and spatio-temporal designs, and model bike stations (nodes) and inter-station bike rides (edges) as a graph. In particular, we design a novel graph attention convolutional neural network (GACNN) with attention mechanisms capturing and differentiating station-to-station correlations. Multi-level temporal closeness, spatial distances and other external factors (e.g., weather and points of interest) are jointly considered for comprehensive learning and accurate prediction of bike flows at each station. Extensive experiments upon a total of over 11 million trips collected from three large-scale bike-sharing systems in New York City, Chicago, and Los Angeles have corroborated GBikes’s significant improvement of accuracy, robustness and effectiveness over prior work.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {88–98},
numpages = {11},
keywords = {flow forecasting, Bike sharing, smart city, station-level},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380098,
author = {Wang, Xiang and Xu, Yaokun and He, Xiangnan and Cao, Yixin and Wang, Meng and Chua, Tat-Seng},
title = {Reinforced Negative Sampling over Knowledge Graph for Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380098},
doi = {10.1145/3366423.3380098},
abstract = {Properly handling missing data is a fundamental challenge in recommendation. Most present works perform negative sampling from unobserved data to supply the training of recommender models with negative signals. Nevertheless, existing negative sampling strategies, either static or adaptive ones, are insufficient to yield high-quality negative samples — both informative to model training and reflective of user real needs. In this work, we hypothesize that item knowledge graph (KG), which provides rich relations among items and KG entities, could be useful to infer informative and factual negative samples. Towards this end, we develop a new negative sampling model, Knowledge Graph Policy Network (KGPolicy), which works as a reinforcement learning agent to explore high-quality negatives. Specifically, by conducting our designed exploration operations, it navigates from the target positive interaction, adaptively receives knowledge-aware negative signals, and ultimately yields a potential negative item to train the recommender. We tested on a matrix factorization (MF) model equipped with KGPolicy, and it achieves significant improvements over both state-of-the-art sampling methods like DNS&nbsp;[39] and IRGAN&nbsp;[30], and KG-enhanced recommender models like KGAT&nbsp;[32]. Further analyses from different angles provide insights of knowledge-aware sampling. We release the codes and datasets at https://github.com/xiangwang1223/kgpolicy. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {99–109},
numpages = {11},
keywords = {Knowledge Graph, Recommendation, Negative Sampling},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380099,
author = {Zhou, Yao and Nelakurthi, Arun Reddy and Maciejewski, Ross and Fan, Wei and He, Jingrui},
title = {Crowd Teaching with Imperfect Labels},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380099},
doi = {10.1145/3366423.3380099},
abstract = {The need for annotated labels to train machine learning models led to a surge in crowdsourcing - collecting labels from non-experts. Instead of annotating from scratch, given an imperfect labeled set, how can we leverage the label information obtained from amateur crowd workers to improve the data quality? Furthermore, is there a way to teach the amateur crowd workers using this imperfect labeled set in order to improve their labeling performance? In this paper, we aim to answer both questions via a novel interactive teaching framework, which uses visual explanations to simultaneously teach and gauge the confidence level of the crowd workers. Motivated by the huge demand for fine-grained label information in real-world applications, we start from the realistic and yet challenging assumption that neither the teacher nor the crowd workers are perfect. Then, we propose an adaptive scheme that could improve both of them through a sequence of interactions: the teacher teaches the workers using labeled data, and in return, the workers provide labels and the associated confidence level based on their own expertise. In particular, the teacher performs teaching using an empirical risk minimizer learned from an imperfect labeled set; the workers are assumed to have a forgetting behavior during learning and their learning rate depends on the interpretation difficulty of the teaching item. Furthermore, depending on the level of confidence when the workers perform labeling, we also show that the empirical risk minimizer used by the teacher is a reliable and realistic substitute of the unknown target concept by utilizing the unbiased surrogate loss. Finally, the performance of the proposed framework is demonstrated through experiments on multiple real-world image and text data sets.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {110–121},
numpages = {12},
keywords = {Explanation., Personalized crowdsourcing, Interactive teaching},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380100,
author = {Li, Xueqi and Jiang, Wenjun and Chen, Weiguang and Wu, Jie and Wang, Guojun and Li, Kenli},
title = {Directional and Explainable Serendipity Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380100},
doi = {10.1145/3366423.3380100},
abstract = {Serendipity recommendation has attracted more and more attention in recent years; it is committed to providing recommendations which could not only cater to users’ demands but also broaden their horizons. However, existing approaches usually measure user-item relevance with a scalar instead of a vector, ignoring user preference direction, which increases the risk of unrelated recommendations. In addition, reasonable explanations increase users’ trust and acceptance, but there is no work to provide explanations for serendipitous recommendations. To address these limitations, we propose a Directional and Explainable Serendipity Recommendation method named DESR. Specifically, we extract users’ long-term preferences with an unsupervised method based on GMM (Gaussian Mixture Model) and capture their short-term demands with the capsule network at first. Then, we propose the serendipity vector to combine long-term preferences with short-term demands and generate directionally serendipitous recommendations with it. Finally, a back-routing scheme is exploited to offer explanations. Extensive experiments on real-world datasets show that DESR could effectively improve the serendipity and explainability, and give impetus to the diversity, compared with existing serendipity-based methods.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {122–132},
numpages = {11},
keywords = {Explainability, Recommendation, User Preference Direction, Serendipity},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380101,
author = {He, Suining and Shin, Kang G.},
title = {Dynamic Flow Distribution Prediction for Urban Dockless E-Scooter Sharing Reconfiguration},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380101},
doi = {10.1145/3366423.3380101},
abstract = {Thanks to recent progresses in mobile payment, IoT, electric motors, batteries and location-based services, Dockless E-scooter Sharing (DES) has become a popular means of last-mile commute for a growing number of (smart) cities. As e-scooters are getting deployed dynamically and flexibly across city regions that expand and/or shrink, with subsequent social, commercial and environmental evaluation, accurate prediction of the distribution of e-scooters given reconfigured regions becomes essential for the city planners and service providers. To meet this need, we propose GCScoot, a novel dynamic flow distribution prediction for reconfiguring urban DES systems. Based on the real-world datasets with reconfiguration, we analyze the mobility features of the e-scooter distribution and flow dynamics for the data-driven designs. To adapt to dynamic reconfiguration of DES deployment, we propose a novel spatio-temporal graph capsule neural network within GCScoot to predict the future dockless e-scooter flows given the reconfigured regions. GCScoot preprocesses the historical spatial e-scooter distributions into flow graph structures, where discretized city regions are considered as nodes and their mutual flows as edges. Given data-driven designs regarding distance, ride flows and region connectivity, the dynamic region-to-region correlations embedded within the temporal flow graphs are captured through the graph capsule neural network which accurately predicts the DES flows. We have conducted extensive empirical studies upon three different e-scooter datasets (&gt;2.8 million rides in total) in populous US cities including Austin TX, Louisville KY and Minneapolis MN. The evaluation results have corroborated the accuracy and effectiveness of GCScoot in predicting dynamic distribution of dockless e-scooters’ mobility.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {133–143},
numpages = {11},
keywords = {distribution prediction, Dockless e-scooter, reconfiguration},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380102,
author = {Yang, Liang and Wu, Fan and Gu, Junhua and Wang, Chuan and Cao, Xiaochun and Jin, Di and Guo, Yuanfang},
title = {Graph Attention Topic Modeling Network},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380102},
doi = {10.1145/3366423.3380102},
abstract = {Existing topic modeling approaches possess several issues, including the overfitting issue of Probablistic Latent Semantic Indexing (pLSI), the failure of capturing the rich topical correlations among topics in Latent Dirichlet Allocation (LDA), and high inference complexity. In this paper, we provide a new method to overcome the overfitting issue of pLSI by using the amortized inference with word embedding as input, instead of the Dirichlet prior in LDA. For generative topic model, the large number of free latent variables is the root of overfitting. To reduce the number of parameters, the amortized inference replaces the inference of latent variable with a function which possesses the shared (amortized) learnable parameters. The number of the shared parameters is fixed and independent of the scale of the corpus. To overcome the limited application of amortized inference to independent and identically distributed (i.i.d) data, a novel graph neural network, Graph Attention TOpic Network (GATON), is proposed to model the topic structure of non-i.i.d documents according to the following two observations. First, pLSI can be interpreted as stochastic block model (SBM) on a specific bi-partite graph. Second, graph attention network (GAT) can be explained as the semi-amortized inference of SBM, which relaxes the i.i.d data assumption of vanilla amortized inference. GATON provides a novel scheme, i.e. graph convolution operation based scheme, to integrate word similarity and word co-occurrence structure. Specifically, the bag-of-words document representation is modeled as a bi-partite graph topology. Meanwhile, word embedding, which captures the word similarity, is modeled as attribute of the word node and the term frequency vector is adopted as the attribute of the document node. Based on the weighted (attention) graph convolution operation, the word co-occurrence structure and word similarity patterns are seamlessly integrated for topic identification. Extensive experiments demonstrate that the effectiveness of GATON on topic identification not only benefits the document classification, but also significantly refines the input word embedding. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {144–154},
numpages = {11},
keywords = {Graph Neural Network, Topic Modeling, Bipartite Network, Stochastic Block Model, Graph Attention Network},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380103,
author = {Lee, Xi Tong and Khan, Arijit and Sen Gupta, Sourav and Ong, Yu Hann and Liu, Xuan},
title = {Measurements, Analyses, and Insights on the Entire Ethereum Blockchain Network},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380103},
doi = {10.1145/3366423.3380103},
abstract = {Blockchains are increasingly becoming popular due to the prevalence of cryptocurrencies and decentralized applications. Ethereum is a distributed public blockchain network that focuses on running code (smart contracts) for decentralized applications. More simply, it is a platform for sharing information in a global state that cannot be manipulated or changed. Ethereum blockchain introduces a novel ecosystem of human users and autonomous agents (smart contracts). In this network, we are interested in all possible interactions: user-to-user, user-to-contract, contract-to-user, and contract-to-contract. This requires us to construct interaction networks from the entire Ethereum blockchain data, where vertices are accounts (users, contracts) and arcs denote interactions. Our analyses on the networks reveal new insights by combining information from the four networks. We perform an in-depth study of these networks based on several graph properties consisting of both local and global properties, discuss their similarities and differences with social networks and the Web, draw interesting conclusions, and highlight important, future research directions.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {155–166},
numpages = {12},
keywords = {Ethereum, Tokens, Network Analysis, Smart Contracts, Blockchain},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380104,
author = {Zeber, David and Bird, Sarah and Oliveira, Camila and Rudametkin, Walter and Segall, Ilana and Wolls\'{e}n, Fredrik and Lopatka, Martin},
title = {The Representativeness of Automated Web Crawls as a Surrogate for Human Browsing},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380104},
doi = {10.1145/3366423.3380104},
abstract = {Large-scale Web crawls have emerged as the state of the art for studying characteristics of the Web. In particular, they are a core tool for online tracking research. Web crawling is an attractive approach to data collection, as crawls can be run at relatively low infrastructure cost and don’t require handling sensitive user data such as browsing histories. However, the biases introduced by using crawls as a proxy for human browsing data have not been well studied. Crawls may fail to capture the diversity of user environments, and the snapshot view of the Web presented by one-time crawls does not reflect its constantly evolving nature, which hinders reproducibility of crawl-based studies. In this paper, we quantify the repeatability and representativeness of Web crawls in terms of common tracking and fingerprinting metrics, considering both variation across crawls and divergence from human browser usage. We quantify baseline variation of simultaneous crawls, then isolate the effects of time, cloud IP address vs. residential, and operating system. This provides a foundation to assess the agreement between crawls visiting a standard list of high-traffic websites and actual browsing behaviour measured from an opt-in sample of over 50,000 users of the Firefox Web browser. Our analysis reveals differences between the treatment of stateless crawling infrastructure and generally stateful human browsing, showing, for example, that crawlers tend to experience higher rates of third-party activity than human browser users on loading pages from the same domains.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {167–178},
numpages = {12},
keywords = {Tracking, Online Privacy, Web Crawling, World Wide Web, Browser Fingerprinting},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380105,
author = {An, Kijin and Tilevich, Eli},
title = {Client Insourcing: Bringing Ops In-House for Seamless Re-Engineering of Full-Stack JavaScript Applications},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380105},
doi = {10.1145/3366423.3380105},
abstract = {Modern web applications are distributed across a browser-based client and a cloud-based server. Distribution provides access to remote resources, accessed over the web and shared by clients. Much of the complexity of inspecting and evolving web applications lies in their distributed nature. Also, the majority of mature program analysis and transformation tools works only with centralized software. Inspired by business process re-engineering, in which remote operations can be insourced back in house to restructure and outsource anew, we bring an analogous approach to the re-engineering of web applications. Our target domain are full-stack JavaScript applications that implement both the client and server code in this language. Our approach is enabled by Client Insourcing, a novel automatic refactoring that creates a semantically equivalent centralized version of a distributed application. This centralized version is then inspected, modified, and redistributed to meet new requirements. After describing the design and implementation of Client Insourcing, we demonstrate its utility and value in addressing changes in security, reliability, and performance requirements. By reducing the complexity of the non-trivial program inspection and evolution tasks performed to meet these requirements, our approach can become a helpful aid in the re-engineering of web applications in this domain. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {179–189},
numpages = {11},
keywords = {Middleware, Web Applications, Program Analysis &amp; Transformation, Mobile Apps, Re-Engineering, JavaScript, Software Engineering},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380106,
author = {Meurisch, Christian and Bayrak, Bekir and M\"{u}hlh\"{a}user, Max},
title = {Privacy-Preserving AI Services Through Data&nbsp;Decentralization},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380106},
doi = {10.1145/3366423.3380106},
abstract = {User services increasingly base their actions on AI models, e.g.,&nbsp;to offer personalized and proactive support. However, the underlying AI algorithms require a continuous stream of personal data—leading to privacy issues, as users typically have to share this data out of their territory. Current privacy-preserving concepts are either not applicable to such AI-based services or to the disadvantage of any party. This paper presents PrivAI, a new decentralized and privacy-by-design platform for overcoming the need for sharing user data to benefit from personalized AI services. In short, PrivAI complements existing approaches to personal data stores, but strictly enforces the confinement of raw user data. PrivAI further addresses the resulting challenges by (1)&nbsp;dividing AI algorithms into cloud-based general model training, subsequent local personalization, and community-based sharing of model updates for new users; by (2)&nbsp;loading confidential AI models into a trusted execution environment, and thus, protecting provider’s intellectual property (IP). Our experiments show the feasibility and effectiveness of PrivAI with comparable performance as currently-practiced approaches.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {190–200},
numpages = {11},
keywords = {IP/data protection, AI services, PDS, ad-hoc personalization},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380107,
author = {Zhang, Hongming and Liu, Xin and Pan, Haojie and Song, Yangqiu and Leung, Cane Wing-Ki},
title = {ASER: A Large-Scale Eventuality Knowledge Graph},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380107},
doi = {10.1145/3366423.3380107},
abstract = {Understanding human’s language requires complex world knowledge. However, existing large-scale knowledge graphs mainly focus on knowledge about entities while ignoring knowledge about activities, states, or events, which are used to describe how entities or things act in the real world. To fill this gap, we develop ASER (activities, states, events, and their relations), a large-scale eventuality knowledge graph extracted from more than 11-billion-token unstructured textual data. ASER contains 15 relation types belonging to five categories, 194-million unique eventualities, and 64-million unique edges among them. Both intrinsic and extrinsic evaluations demonstrate the quality and effectiveness of ASER. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {201–211},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380108,
author = {Lu, Chris Xiaoxuan and Li, Yang and Xiangli, Yuanbo and Li, Zhengxiong},
title = {Nowhere to Hide: Cross-Modal Identity Leakage between Biometrics and Devices},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380108},
doi = {10.1145/3366423.3380108},
abstract = {Along with the benefits of Internet of Things (IoT) come potential privacy risks, since billions of the connected devices are granted permission to track information about their users and communicate it to other parties over the Internet. Of particular interest to the adversary is the user identity which constantly plays an important role in launching attacks. While the exposure of a certain type of physical biometrics or device identity is extensively studied, the compound effect of leakage from both sides remains unknown in multi-modal sensing environments. In this work, we explore the feasibility of the compound identity leakage across cyber-physical spaces and unveil that co-located smart device IDs (e.g., smartphone MAC addresses) and physical biometrics (e.g., facial/vocal samples) are side channels to each other. It is demonstrated that our method is robust to various observation noise in the wild and an attacker can comprehensively profile victims in multi-dimension with nearly zero analysis effort. Two real-world experiments on different biometrics and device IDs show that the presented approach can compromise more than 70% of device IDs and harvests multiple biometric clusters with purity at the same time. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {212–223},
numpages = {12},
keywords = {Identity Leakage, Cross-modality Association, Internet of Things},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380109,
author = {Silva, M\'{a}rcio and Santos de Oliveira, Lucas and Andreou, Athanasios and Vaz de Melo, Pedro Olmo and Goga, Oana and Benevenuto, Fabricio},
title = {Facebook Ads Monitor: An Independent Auditing System for Political Ads on Facebook},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380109},
doi = {10.1145/3366423.3380109},
abstract = {The 2016 United States presidential election was marked by the abuse of targeted advertising on Facebook. Concerned with the risk of the same kind of abuse to happen in the 2018 Brazilian elections, we designed and deployed an independent auditing system to monitor political ads on Facebook in Brazil. To do that we first adapted a browser plugin to gather ads from the timeline of volunteers using Facebook. We managed to convince more than 2000 volunteers to help our project and install our tool. Then, we use a Convolution Neural Network (CNN) to detect political Facebook ads using word embeddings. To evaluate our approach, we manually label a data collection of 10k ads as political or non-political and then we provide an in-depth evaluation of proposed approach for identifying political ads by comparing it with classic supervised machine learning methods. Finally, we deployed a real system that shows the ads identified as related to politics. We noticed that not all political ads we detected were present in the Facebook Ad Library for political ads. Our results emphasize the importance of enforcement mechanisms for declaring political ads and the need for independent auditing platforms.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {224–234},
numpages = {11},
keywords = {Facebook, transparency mechanisms, political ads, Misinformation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380110,
author = {Shi, Yuxuan and Cheng, Gong and Kharlamov, Evgeny},
title = {Keyword Search over Knowledge Graphs via Static and Dynamic Hub Labelings},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380110},
doi = {10.1145/3366423.3380110},
abstract = {Keyword search is a prominent approach to querying Web data. For graph-structured data, a widely accepted semantics for keywords is based on group Steiner trees. For this NP-hard problem, existing algorithms with provable quality guarantees have prohibitive run time on large graphs. In this paper, we propose practical approximation algorithms with a guaranteed quality of computed answers and very low run time. Our algorithms rely on Hub Labeling (HL), a structure that labels each vertex in a graph with a list of vertices reachable from it, which we use to compute distances and shortest paths. We devise two HLs: a conventional static HL that uses a new heuristic to improve pruned landmark labeling, and a novel dynamic HL that inverts and aggregates query-relevant static labels to more efficiently process vertex sets. Our approach allows to compute a reasonably good approximation of answers to keyword queries in milliseconds on million-scale knowledge graphs.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {235–245},
numpages = {11},
keywords = {group Steiner tree, knowledge graph, hub labeling, keyword search},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380111,
author = {Ma, Meng and Xu, Jingmin and Wang, Yuan and Chen, Pengfei and Zhang, Zonghua and Wang, Ping},
title = {AutoMAP: Diagnose Your Microservice-Based Web Applications Automatically},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380111},
doi = {10.1145/3366423.3380111},
abstract = {The high complexity and dynamics of the microservice architecture make its application diagnosis extremely challenging. Static troubleshooting approaches may fail to obtain reliable model applies for frequently changing situations. Even if we know the calling dependency of services, we lack a more dynamic diagnosis mechanism due to the existence of indirect fault propagation. Besides, algorithm based on single metric usually fail to identify the root cause of anomaly, as single type of metric is not enough to characterize the anomalies occur in diverse services. In view of this, we design a novel tool, named AutoMAP, which enables dynamic generation of service correlations and automated diagnosis leveraging multiple types of metrics. In AutoMAP, we propose the concept of anomaly behavior graph to describe the correlations between services associated with different types of metrics. Two binary operations, as well as a similarity function on behavior graph are defined to help AutoMAP choose appropriate diagnosis metric in any particular scenario. Following the behavior graph, we design a heuristic investigation algorithm by using forward, self, and backward random walk, with an objective to identify the root cause services. To demonstrate the strengths of AutoMAP, we develop a prototype and evaluate it in both simulated environment and real-work enterprise cloud system. Experimental results clearly indicate that AutoMAP achieves over 90% precision, which significantly outperforms other selected baseline methods. AutoMAP can be quickly deployed in a variety of microservice-based systems without any system knowledge. It also supports introduction of various expert knowledge to improve accuracy.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {246–258},
numpages = {13},
keywords = {web application, root cause, cloud computing, anomaly diagnosis, Microservice architecture},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380112,
author = {Peng, Zhen and Huang, Wenbing and Luo, Minnan and Zheng, Qinghua and Rong, Yu and Xu, Tingyang and Huang, Junzhou},
title = {Graph Representation Learning via Graphical Mutual Information Maximization},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380112},
doi = {10.1145/3366423.3380112},
abstract = {The richness in the content of various information networks such as social networks and communication networks provides the unprecedented potential for learning high-quality expressive representations without external supervision. This paper investigates how to preserve and extract the abundant information from graph-structured data into embedding space in an unsupervised manner. To this end, we propose a novel concept, Graphical Mutual Information (GMI), to measure the correlation between input graphs and high-level hidden representations. GMI generalizes the idea of conventional mutual information computations from vector space to the graph domain where measuring mutual information from two aspects of node features and topological structure is indispensable. GMI exhibits several benefits: First, it is invariant to the isomorphic transformation of input graphs—an inevitable constraint in many existing graph representation learning algorithms; Besides, it can be efficiently estimated and maximized by current mutual information estimation methods such as MINE; Finally, our theoretical analysis confirms its correctness and rationality. With the aid of GMI, we develop an unsupervised learning model trained by maximizing GMI between the input and output of a graph neural encoder. Considerable experiments on transductive as well as inductive node classification and link prediction demonstrate that our method outperforms state-of-the-art unsupervised counterparts, and even sometimes exceeds the performance of supervised ones.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {259–270},
numpages = {12},
keywords = {InfoMax., Mutual information, Graph representation learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380113,
author = {Ahmad, Syed Suleman and Dar, Muhammad Daniyal and Zaffar, Muhammad Fareed and Vallina-Rodriguez, Narseo and Nithyanand, Rishab},
title = {Apophanies or Epiphanies? How Crawlers Impact Our Understanding of the Web},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380113},
doi = {10.1145/3366423.3380113},
abstract = {Data generated by web crawlers has formed the basis for much of our current understanding of the Internet. However, not all crawlers are created equal and crawlers generally find themselves trading off between computational overhead, developer effort, data accuracy, and completeness. Therefore, the choice of crawler has a critical impact on the data generated and knowledge inferred from it. In this paper, we conduct a systematic study of the trade-offs presented by different crawlers and the impact that these can have on various types of measurement studies. We make the following contributions: First, we conduct a survey of all research published since 2015 in the premier security and Internet measurement venues to identify and verify the repeatability of crawling methodologies deployed for different problem domains and publication venues. Next, we conduct a qualitative evaluation of a subset of all crawling tools identified in our survey. This evaluation allows us to draw conclusions about the suitability of each tool for specific types of data gathering. Finally, we present a methodology and a measurement framework to empirically highlight the differences between crawlers and how the choice of crawler can impact our understanding of the web. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {271–280},
numpages = {10},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380114,
author = {Yu, Jianxing and Quan, Xiaojun and Su, Qinliang and Yin, Jian},
title = {Generating Multi-Hop Reasoning Questions to Improve Machine Reading Comprehension},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380114},
doi = {10.1145/3366423.3380114},
abstract = {This paper focuses on the topic of multi-hop question generation, which aims to generate questions needed reasoning over multiple sentences and relations to derive answers. In particular, we first build an entity graph to integrate various entities scattered over text based on their contextual relations. We then heuristically extract the sub-graph by the evidential relations and type, so as to obtain the reasoning chain and textual related contents for each question. Guided by the chain, we propose a holistic generator-evaluator network to form the questions, where such guidance helps to ensure the rationality of generated questions which need multi-hop deduction to correspond to the answers. The generator is a sequence-to-sequence model, designed with several techniques to make the questions syntactically and semantically valid. The evaluator optimizes the generator network by employing a hybrid mechanism combined of supervised and reinforced learning. Experimental results on HotpotQA data set demonstrate the effectiveness of our approach, where the generated samples can be used as pseudo training data to alleviate the data shortage problem for neural network and assist to learn the state-of-the-arts for multi-hop machine comprehension.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {281–291},
numpages = {11},
keywords = {multi-hop question generation, machine reading comprehension, reasoning chain},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380115,
author = {Yang, Mengyue and Li, Qingyang and Qin, Zhiwei and Ye, Jieping},
title = {Hierarchical Adaptive Contextual Bandits for Resource Constraint Based Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380115},
doi = {10.1145/3366423.3380115},
abstract = {Contextual multi-armed bandit (MAB) achieves cutting-edge performance on a variety of problems. When it comes to real-world scenarios such as recommendation system and online advertising, however, it is essential to consider the resource consumption of exploration. In practice, there is typically non-zero cost associated with executing a recommendation (arm) in the environment, and hence, the policy should be learned with a fixed exploration cost constraint. It is challenging to learn a global optimal policy directly, since it is a NP-hard problem and significantly complicates the exploration and exploitation trade-off of bandit algorithms. Existing approaches focus on solving the problems by adopting the greedy policy which estimates the expected rewards and costs and uses a greedy selection based on each arm’s expected reward/cost ratio using historical observation until the exploration resource is exhausted. However, existing methods are hard to extend to infinite time horizon, since the learning process will be terminated when there is no more resource. In this paper, we propose a hierarchical adaptive contextual bandit method (HATCH) to conduct the policy learning of contextual bandits with a budget constraint. HATCH adopts an adaptive method to allocate the exploration resource based on the remaining resource/time and the estimation of reward distribution among different user contexts. In addition, we utilize full of contextual feature information to find the best personalized recommendation. Finally, in order to prove the theoretical guarantee, we present a regret bound analysis and prove that HATCH achieves a regret bound as low as . The experimental results demonstrate the effectiveness and efficiency of the proposed method on both synthetic data sets and the real-world applications.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {292–302},
numpages = {11},
keywords = {reinforcement learning, exploration resource allocation, recommendation system, contextual bandits, budget constrain},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380116,
author = {Yuan, Fajie and He, Xiangnan and Jiang, Haochuan and Guo, Guibing and Xiong, Jian and Xu, Zhezhao and Xiong, Yilin},
title = {Future Data Helps Training: Modeling Future Contexts for Session-Based Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380116},
doi = {10.1145/3366423.3380116},
abstract = {Session-based recommender systems have attracted much attention recently. To capture the sequential dependencies, existing methods resort either to data augmentation techniques or left-to-right style autoregressive training. Since these methods are aimed to model the sequential nature of user behaviors, they ignore the future data of a target interaction when constructing the prediction model for it. However, we argue that the future interactions after a target interaction, which are also available during training, provide valuable signal on user preference and can be used to enhance the recommendation quality. Properly integrating future data into model training, however, is non-trivial to achieve, since it disobeys machine learning principles and can easily cause data leakage. To this end, we propose a new encoder-decoder framework named Gap-filling based Recommender (GRec), which trains the encoder and decoder by a gap-filling mechanism. Specifically, the encoder takes a partially-complete session sequence (where some items are masked by purpose) as input, and the decoder predicts these masked items conditioned on the encoded representation. We instantiate the general GRec framework using convolutional neural network with sparse kernels, giving consideration to both accuracy and efficiency. We conduct experiments on two real-world datasets covering short-, medium-, and long-range user sessions, showing that GRec significantly outperforms the state-of-the-art sequential recommendation methods. More empirical studies verify the high utility of modeling future contexts under our GRec framework. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {303–313},
numpages = {11},
keywords = {Seq2Seq Learning, Encoder and Decoder, Gap-filling, Data Leakage, Sequential Recommendation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380117,
author = {Kumar, Adithya and Narayanan, Iyswarya and Zhu, Timothy and Sivasubramaniam, Anand},
title = {The Fast and The Frugal: Tail Latency Aware Provisioning for Coping with Load Variations},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380117},
doi = {10.1145/3366423.3380117},
abstract = {Small and medium sized enterprises use the cloud for running online, user-facing, tail latency sensitive applications with well-defined fixed monthly budgets. For these applications, adequate system capacity must be provisioned to extract maximal performance despite the challenges of uncertainties in load and request-sizes. In this paper, we address the problem of capacity provisioning under fixed budget constraints with the goal of minimizing tail latency. To tackle this problem, we propose building systems using a heterogeneous mix of low latency expensive resources and cheap resources that provide high throughput per dollar. As load changes through the day, we use more faster resources to reduce tail latency during low load periods and more cheaper resources to handle the high load periods. To achieve these tail latency benefits, we introduce novel heterogeneity-aware scheduling and autoscaling algorithms that are designed for minimizing tail latency. Using software prototypes and by running experiments on the public cloud, we show that our approach can outperform existing capacity provisioning systems by reducing the tail latency by as much as 45% under fixed-budget settings.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {314–326},
numpages = {13},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380118,
author = {Rama, Daniele and Mejova, Yelena and Tizzoni, Michele and Kalimeri, Kyriaki and Weber, Ingmar},
title = {Facebook Ads as a Demographic Tool to Measure the Urban-Rural Divide},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380118},
doi = {10.1145/3366423.3380118},
abstract = {In the global move toward urbanization, making sure the people remaining in rural areas are not left behind in terms of development and policy considerations is a priority for governments worldwide. However, it is increasingly challenging to track important statistics concerning this sparse, geographically dispersed population, resulting in a lack of reliable, up-to-date data. In this study, we examine the usefulness of the Facebook Advertising platform, which offers a digital “census” of over two billions of its users, in measuring potential rural-urban inequalities. We focus on Italy, a country where about 30% of the population lives in rural areas. First, we show that the population statistics that Facebook produces suffer from instability across time and incomplete coverage of sparsely populated municipalities. To overcome such limitation, we propose an alternative methodology for estimating Facebook Ads audiences that nearly triples the coverage of the rural municipalities from 19% to 55% and makes feasible fine-grained sub-population analysis. Using official national census data, we evaluate our approach and confirm known significant urban-rural divides in terms of educational attainment and income. Extending the analysis to Facebook-specific user “interests” and behaviors, we provide further insights on the divide, for instance, finding that rural areas show a higher interest in gambling. Notably, we find that the most predictive features of income in rural areas differ from those for urban centres, suggesting researchers need to consider a broader range of attributes when examining rural wellbeing. The findings of this study illustrate the necessity of improving existing tools and methodologies to include under-represented populations in digital demographic studies – the failure to do so could result in misleading observations, conclusions, and most importantly, policies.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {327–338},
numpages = {12},
keywords = {social networks, digital demography, online advertising, urban-rural divide},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380119,
author = {Chen, Zi and Yuan, Long and Lin, Xuemin and Qin, Lu and Yang, Jianye},
title = {Efficient Maximal Balanced Clique Enumeration in Signed Networks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380119},
doi = {10.1145/3366423.3380119},
abstract = {Clique is one of the most fundamental models for cohesive subgraph mining in network analysis. Existing clique model mainly focuses on unsigned networks. In real world, however, many applications are modeled as signed networks with positive and negative edges. As the signed networks hold their own properties different from the unsigned networks, the existing clique model is inapplicable for the signed networks. Motivated by this, we propose the balanced clique model that considers the most fundamental and dominant theory, structural balance theory, for signed networks, and study the maximal balanced clique enumeration problem which computes all the maximal balanced cliques in a given signed network. We show that the maximal balanced clique enumeration problem is NP-Hard. A straightforward solution for the maximal balanced clique enumeration problem is to treat the signed network as two unsigned networks and leverage the off-the-shelf techniques for unsigned networks. However, such a solution is inefficient for large signed networks. To address this problem, in this paper, we first propose a new maximal balanced clique enumeration algorithm by exploiting the unique properties of signed networks. Based on the new proposed algorithm, we devise two optimization strategies to further improve the efficiency of the enumeration. We conduct extensive experiments on large real and synthetic datasets. The experimental results demonstrate the efficiency, effectiveness and scalability of our proposed algorithms.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {339–349},
numpages = {11},
keywords = {Signed Network, Graph Algorithm, Maximal Balanced Clique},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380120,
author = {Wang, Ping and Shi, Tian and Reddy, Chandan K.},
title = {Text-to-SQL Generation for Question Answering on Electronic Medical Records},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380120},
doi = {10.1145/3366423.3380120},
abstract = {Electronic medical records (EMR) contain comprehensive patient information and are typically stored in a relational database with multiple tables. Effective and efficient patient information retrieval from EMR data is a challenging task for medical experts. Question-to-SQL generation methods tackle this problem by first predicting the SQL query for a given question about a database, and then, executing the query on the database. However, most of the existing approaches have not been adapted to the healthcare domain due to a lack of healthcare Question-to-SQL dataset for learning models specific to this domain. In addition, wide use of the abbreviation of terminologies and possible typos in questions introduce additional challenges for accurately generating the corresponding SQL queries. In this paper, we tackle these challenges by developing a deep learning based TRanslate-Edit Model for Question-to-SQL&nbsp;(TREQS) generation, which adapts the widely used sequence-to-sequence model to directly generate the SQL query for a given question, and further performs the required edits using an attentive-copying mechanism and task-specific look-up tables. Based on the widely used publicly available electronic medical database, we create a new large-scale Question-SQL pair dataset, named MIMICSQL, in order to perform the Question-to-SQL generation task in healthcare domain. An extensive set of experiments are conducted to evaluate the performance of our proposed model on MIMICSQL. Both quantitative and qualitative experimental results indicate the flexibility and efficiency of our proposed method in predicting condition values and its robustness to random questions with abbreviations and typos.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {350–361},
numpages = {12},
keywords = {Sequence-to-sequence model, attention mechanism, SQL query., pointer-generator network, electronic medical records},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380121,
author = {Xiao, Han and Ordozgoiti, Bruno and Gionis, Aristides},
title = {Searching for Polarization in Signed Graphs: A Local Spectral Approach},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380121},
doi = {10.1145/3366423.3380121},
abstract = {Signed graphs have been used to model interactions in social networks, which can be either positive (friendly) or negative (antagonistic). The model has been used to study polarization and other related phenomena in social networks, which can be harmful to the process of democratic deliberation in our society. An interesting and challenging task in this application domain is to detect polarized communities in signed graphs. A number of different methods have been proposed for this task. However, existing approaches aim at finding globally optimal solutions. Instead, in this paper we are interested in finding polarized communities that are related to a small set of seed nodes provided as input. Seed nodes may consist of two sets, which constitute the two sides of a polarized structure. In this paper we formulate the problem of finding local polarized communities in signed graphs as a locally-biased eigen-problem. By viewing the eigenvector associated with the smallest eigenvalue of the Laplacian matrix as the solution of a constrained optimization problem, we are able to incorporate the local information as an additional constraint. In addition, we show that the locally-biased vector can be used to find communities with approximation guarantee with respect to a local analogue of the Cheeger constant on signed graphs. By exploiting the sparsity in the input graph, an indicator-vector for the polarized communities can be found in time linear to the graph size. Our experiments on real-world networks validate the proposed algorithm and demonstrate its usefulness in finding local structures in this semi-supervised manner. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {362–372},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380122,
author = {Carmel, David and Haramaty, Elad and Lazerson, Arnon and Lewin-Eytan, Liane},
title = {Multi-Objective Ranking Optimization for Product Search Using Stochastic Label Aggregation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380122},
doi = {10.1145/3366423.3380122},
abstract = {Learning a ranking model in product search involves satisfying many requirements such as maximizing the relevance of retrieved products with respect to the user query, as well as maximizing the purchase likelihood of these products. Multi-Objective Ranking Optimization (MORO) is the task of learning a ranking model from training examples while optimizing multiple objectives simultaneously. Label aggregation is a popular solution approach for multi-objective optimization, which reduces the problem into a single objective optimization problem, by aggregating the multiple labels of the training examples, related to the different objectives, to a single label. In this work we explore several label aggregation methods for MORO in product search. We propose a novel stochastic label aggregation method which randomly selects a label per training example according to a given distribution over the labels. We provide a theoretical proof showing that stochastic label aggregation is superior to alternative aggregation approaches, in the sense that any optimal solution of the MORO problem can be generated by a proper parameter setting of the stochastic aggregation process. We experiment on three different datasets: two from the voice product search domain, and one publicly available dataset from the Web product search domain. We demonstrate empirically over these three datasets that MORO with stochastic label aggregation provides a family of ranking models that fully dominates the set of MORO models built using deterministic label aggregation. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {373–383},
numpages = {11},
keywords = {stochastic label aggregation, product search, multi-objective ranking optimization},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380123,
author = {Cao, Ermei and Wang, Difeng and Huang, Jiacheng and Hu, Wei},
title = {Open Knowledge Enrichment for Long-Tail Entities},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380123},
doi = {10.1145/3366423.3380123},
abstract = {Knowledge bases (KBs) have gradually become a valuable asset for many AI applications. While many current KBs are quite large, they are widely acknowledged as incomplete, especially lacking facts of long-tail entities, e.g., less famous persons. Existing approaches enrich KBs mainly on completing missing links or filling missing values. However, they only tackle a part of the enrichment problem and lack specific considerations regarding long-tail entities. In this paper, we propose a full-fledged approach to knowledge enrichment, which predicts missing properties and infers true facts of long-tail entities from the open Web. Prior knowledge from popular entities is leveraged to improve every enrichment step. Our experiments on the synthetic and real-world datasets and comparison with related work demonstrate the feasibility and superiority of the approach.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {384–394},
numpages = {11},
keywords = {graph neural networks, fact verification, knowledge base augmentation, Knowledge enrichment, long-tail entities, property prediction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380124,
author = {Sanchez-Rola, Iskander and Balzarotti, Davide and Kruegel, Christopher and Vigna, Giovanni and Santos, Igor},
title = {Dirty Clicks: A Study of the Usability and Security Implications of Click-Related Behaviors on the Web},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380124},
doi = {10.1145/3366423.3380124},
abstract = {Web pages have evolved into very complex dynamic applications, which are often very opaque and difficult for non-experts to understand. At the same time, security researchers push for more transparent web applications, which can help users in taking important security-related decisions about which information to disclose, which link to visit, and which online service to trust. In this paper, we look at one of the simplest but also most representative aspect that captures the struggle between these opposite demands: a mouse click. In particular, we present the first comprehensive study of the possible security and privacy implications that clicks can have from a user perspective, analyzing the disconnect that exists between what is shown to users and what actually happens after. We started by identifying and classifying possible problems. We then implemented a crawler that performed nearly 2.5M clicks looking for signs of misbehavior. We analyzed all the interactions created as a result of those clicks, and discovered that the vast majority of domains are putting users at risk by either obscuring the real target of links or by not providing sufficient information for users to make an informed decision. We conclude the paper by proposing a set of countermeasures. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {395–406},
numpages = {12},
keywords = {usability, web security, browser click},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380125,
author = {Han, Shuguang and Bendersky, Michael and Gajda, Przemek and Novikov, Sergey and Najork, Marc and Brodowsky, Bernhard and Popescul, Alexandrin},
title = {Adversarial Bandits Policy for Crawling Commercial Web Content},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380125},
doi = {10.1145/3366423.3380125},
abstract = {The rapid growth of commercial web content has driven the development of shopping search services to help users find product offers. Due to the dynamic nature of commercial content, an effective recrawl policy is a key component in a shopping search service; it ensures that users have access to the up-to-date product details. Most of the existing strategies either relied on simple heuristics, or overlooked the resource budgets. To address this, Azar et al. [5] recently proposed an optimization strategy LambdaCrawl aiming to maximize content freshness within a given resource budget. In this paper, we demonstrate that the effectiveness of LambdaCrawl is governed in large part by how well future content change rate can be estimated. By adopting the state-of-the-art deep learning models for change rate prediction, we obtain a substantial increase of content freshness over the common LambdaCrawl implementation with change rate estimated from the past history. Moreover, we demonstrate that while LambdaCrawl is a significant advancement upon existing recrawl strategies, it can be further improved upon by a unified multi-strategy recrawl policy. To this end, we adopt the K-armed adversarial bandits algorithm that can provably optimize the overall freshness by combining multiple strategies. Empirical results over a large-scale production dataset confirm its superiority to LambdaCrawl, especially under tight resource budgets.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {407–417},
numpages = {11},
keywords = {Adversarial Bandit, Commercial Web Crawling, Predictive Crawling},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380126,
author = {Zamani, Hamed and Dumais, Susan and Craswell, Nick and Bennett, Paul and Lueck, Gord},
title = {Generating Clarifying Questions for Information Retrieval},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380126},
doi = {10.1145/3366423.3380126},
abstract = {Search queries are often short, and the underlying user intent may be ambiguous. This makes it challenging for search engines to predict possible intents, only one of which may pertain to the current user. To address this issue, search engines often diversify the result list and present documents relevant to multiple intents of the query. An alternative approach is to ask the user a question to clarify her information need. Asking clarifying questions is particularly important for scenarios with “limited bandwidth” interfaces, such as speech-only and small-screen devices. In addition, our user studies and large-scale online experiments show that asking clarifying questions is also useful in web search. Although some recent studies have pointed out the importance of asking clarifying questions, generating them for open-domain search tasks remains unstudied and is the focus of this paper. Lack of training data even within major search engines for this task makes it challenging. To mitigate this issue, we first identify a taxonomy of clarification for open-domain search queries by analyzing large-scale query reformulation data sampled from Bing search logs. This taxonomy leads us to a set of question templates and a simple yet effective slot filling algorithm. We further use this model as a source of weak supervision to automatically generate clarifying questions for training. Furthermore, we propose supervised and reinforcement learning models for generating clarifying questions learned from weak supervision data. We also investigate methods for generating candidate answers for each clarifying question, so users can select from a set of pre-defined answers. Human evaluation of the clarifying questions and candidate answers for hundreds of search queries demonstrates the effectiveness of the proposed solutions.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {418–428},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380127,
author = {Li, Jing and Shang, Shuo and Shao, Ling},
title = {MetaNER: Named Entity Recognition with Meta-Learning},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380127},
doi = {10.1145/3366423.3380127},
abstract = {Recent neural architectures in named entity recognition (NER) have yielded state-of-the-art performance on single domain data such as newswires. However, they still suffer from (i) requiring massive amounts of training data to avoid overfitting; (ii) huge performance degradation when there is a domain shift in the data distribution between training and testing. In this paper, we investigate the problem of domain adaptation for NER under homogeneous and heterogeneous settings. We propose MetaNER, a novel meta-learning approach for domain adaptation in NER. Specifically, MetaNER incorporates meta-learning and adversarial training strategies to encourage robust, general and transferable representations for sequence labeling. The key advantage of MetaNER is that it is capable of adapting to new unseen domains with a small amount of annotated data from those domains. We extensively evaluate MetaNER on multiple datasets under homogeneous and heterogeneous settings. The experimental results show that MetaNER achieves state-of-the-art performance against eight baselines. Impressively, MetaNER surpasses the in-domain performance using only 16.17% and 34.76% of target domain data on average for homogeneous and heterogeneous settings, respectively. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {429–440},
numpages = {12},
keywords = {meta-learning, domain adaptation, Named entity recognition},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380128,
author = {Yang, Linyi and Ng, Tin Lok James and Smyth, Barry and Dong, Riuhai},
title = {HTML: Hierarchical Transformer-Based Multi-Task Learning for Volatility Prediction},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380128},
doi = {10.1145/3366423.3380128},
abstract = {The volatility forecasting task refers to predicting the amount of variability in the price of a financial asset over a certain period. It is an important mechanism for evaluating the risk associated with an asset and, as such, is of significant theoretical and practical importance in financial analysis. While classical approaches have framed this task as a time-series prediction one – using historical pricing as a guide to future risk forecasting – recent advances in natural language processing have seen researchers turn to complementary sources of data, such as analyst reports, social media, and even the audio data from earnings calls. This paper proposes a novel hierarchical, transformer, multi-task architecture designed to harness the text and audio data from quarterly earnings conference calls to predict future price volatility in the short and long term. This includes a comprehensive comparison to a variety of baselines, which demonstrates very significant improvements in prediction accuracy, in the range 17% - 49% compared to the current state-of-the-art. In addition, we describe the results of an ablation study to evaluate the relative contributions of each component of our approach and the relative contributions of text and audio data with respect to prediction accuracy. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {441–451},
numpages = {11},
keywords = {Multi-task learning, Hierarchical transformer, Volatility forecasting},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380129,
author = {Gujral, Ekta and Pasricha, Ravdeep and Papalexakis, Evangelos},
title = {Beyond Rank-1: Discovering Rich Community Structure in Multi-Aspect Graphs},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380129},
doi = {10.1145/3366423.3380129},
abstract = {How are communities in real multi-aspect or multi-view graphs structured? How we can effectively and concisely summarize and explore those communities in a high-dimensional, multi-aspect graph without losing important information? State-of-the-art studies focused on patterns in single graphs, identifying structures in a single snapshot of a large network or in time evolving graphs and stitch them over time. However, to the best of our knowledge, there is no method that discovers and summarizes community structure from a multi-aspect graph, by jointly leveraging information from all aspects. State-of-the-art in multi-aspect/tensor community extraction is limited to discovering clique structure in the extracted communities, or even worse, imposing clique structure where it does not exist. In this paper we bridge that gap by empowering tensor-based methods to extract rich community structure from multi-aspect graphs. In particular, we introduce cLL1, a novel constrained Block Term Tensor Decomposition, that is generally capable of extracting higher than rank-1 but still interpretable structure from a multi-aspect dataset. Subsequently, we propose RichCom, a community structure extraction and summarization algorithm that leverages cLL1to identify rich community structure (e.g., cliques, stars, chains, etc) while leveraging higher-order correlations between the different aspects of the graph. Our contributions are four-fold: (a) Novel algorithm: we develop cLL1, an efficient framework to extract rich and interpretable structure from general multi-aspect data; (b) Graph summarization and exploration: we provide RichCom, a summarization and encoding scheme to discover and explore structures of communities identified by cLL1; (c) Multi-aspect graph generator: we provide a simple and effective synthetic multi-aspect graph generator, and (d) Real-world utility: we present empirical results on small and large real datasets that demonstrate performance on par or superior to existing state-of-the-art.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {452–462},
numpages = {11},
keywords = {Graph Summarizing, Multi-aspect data, Tensors, Block Term Decomposition, Community Structures},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380130,
author = {Ma, Jiaqi and Zhao, Zhe and Yi, Xinyang and Yang, Ji and Chen, Minmin and Tang, Jiaxi and Hong, Lichan and Chi, Ed H.},
title = {Off-Policy Learning in Two-Stage Recommender Systems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380130},
doi = {10.1145/3366423.3380130},
abstract = {Many real-world recommender systems need to be highly scalable: matching millions of items with billions of users, with milliseconds latency. The scalability requirement has led to widely used two-stage recommender systems, consisting of efficient candidate generation model(s) in the first stage and a more powerful ranking model in the second stage. Logged user feedback, e.g., user clicks or dwell time, are often used to build both candidate generation and ranking models for recommender systems. While it’s easy to collect large amount of such data, they are inherently biased because the feedback can only be observed on items recommended by the previous systems. Recently, off-policy correction on such biases have attracted increasing interest in the field of recommender system research. However, most existing work either assumed that the recommender system is a single-stage system or only studied how to apply off-policy correction to the candidate generation stage of the system without explicitly considering the interactions between the two stages. In this work, we propose a two-stage off-policy policy gradient method, and showcase that ignoring the interaction between the two stages leads to a sub-optimal policy in two-stage recommender systems. The proposed method explicitly takes into account the ranking model when training the candidate generation model, which helps improve the performance of the whole system. We conduct experiments on real-world datasets with large item space and demonstrate the effectiveness of our proposed method.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {463–473},
numpages = {11},
keywords = {Two-stage Systems, Recommender Systems, Neural Networks, Off-policy Learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380131,
author = {Zhang, Kaitao and Xiong, Chenyan and Liu, Zhenghao and Liu, Zhiyuan},
title = {Selective Weak Supervision for Neural Information Retrieval},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380131},
doi = {10.1145/3366423.3380131},
abstract = {This paper democratizes neural information retrieval to scenarios where large scale relevance training signals are not available. We revisit the classic IR intuition that anchor-document relations approximate query-document relevance and propose a reinforcement weak supervision selection method, ReInfoSelect, which learns to select anchor-document pairs that best weakly supervise the neural ranker (action), using the ranking performance on a handful of relevance labels as the reward. Iteratively, for a batch of anchor-document pairs, ReInfoSelect back propagates the gradients through the neural ranker, gathers its NDCG reward, and optimizes the data selection network using policy gradients, until the neural ranker’s performance peaks on target relevance metrics (convergence). In our experiments on three TREC benchmarks, neural rankers trained by ReInfoSelect, with only publicly available anchor data, significantly outperform feature-based learning to rank methods and match the effectiveness of neural rankers trained with private commercial search logs. Our analyses show that ReInfoSelect effectively selects weak supervision signals based on the stage of the neural ranker training, and intuitively picks anchor-document pairs similar to query-document pairs.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {474–485},
numpages = {12},
keywords = {Weak Supervision, Pre-Training Data Selection, Neural IR},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380132,
author = {Shen, Jiaming and Shen, Zhihong and Xiong, Chenyan and Wang, Chi and Wang, Kuansan and Han, Jiawei},
title = {TaxoExpan: Self-Supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380132},
doi = {10.1145/3366423.3380132},
abstract = {Taxonomies consist of machine-interpretable semantics and provide valuable knowledge for many web applications. For example, online retailers (e.g., Amazon and eBay) use taxonomies for product recommendation, and web search engines (e.g., Google and Bing) leverage taxonomies to enhance query understanding. Enormous efforts have been made on constructing taxonomies either manually or semi-automatically. However, with the fast-growing volume of web content, existing taxonomies will become outdated and fail to capture emerging knowledge. Therefore, in many applications, dynamic expansions of an existing taxonomy are in great demand. In this paper, we study how to expand an existing taxonomy by adding a set of new concepts. We propose a novel self-supervised framework, named TaxoExpan, which automatically generates a set of ⟨query concept, anchor concept⟩ pairs from the existing taxonomy as training data. Using such self-supervision data, TaxoExpan learns a model to predict whether a query concept is the direct hyponym of an anchor concept. We develop two innovative techniques in TaxoExpan: (1) a position-enhanced graph neural network that encodes the local structure of an anchor concept in the existing taxonomy, and (2) a noise-robust training objective that enables the learned model to be insensitive to the label noise in the self-supervision data. Extensive experiments on three large-scale datasets from different domains demonstrate both the effectiveness and the efficiency of TaxoExpan for taxonomy expansion. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {486–497},
numpages = {12},
keywords = {Taxonomy Expansion, Self-supervised Learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380133,
author = {Yuan, Ye and Luo, Xin and Shang, Mingsheng and Wu, Di},
title = {A Generalized and Fast-Converging Non-Negative Latent Factor Model for Predicting User Preferences in Recommender Systems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380133},
doi = {10.1145/3366423.3380133},
abstract = {Recommender systems (RSs) commonly describe its user-item preferences with a high-dimensional and sparse (HiDS) matrix filled with non-negative data. A non-negative latent factor (NLF) model relying on a single latent factor-dependent, non-negative and multiplicative update (SLF-NMU) algorithm is frequently adopted to process such an HiDS matrix. However, an NLF model mostly adopts Euclidean distance for its objective function, which is naturally a special case of α-β-divergence. Moreover, it frequently suffers slow convergence. For addressing these issues, this study proposes a generalized and fast-converging non-negative latent factor (GFNLF) model. Its main idea is two-fold: a) adopting α-β-divergence for its objective function, thereby enhancing its representation ability for HiDS data; b) deducing its momentum-incorporated non-negative multiplicative update (MNMU) algorithm, thereby achieving its fast convergence. Empirical studies on two HiDS matrices emerging from real RSs demonstrate that with carefully-tuned hyperparameters, a GFNLF model outperforms state-of-the-art models in both computational efficiency and prediction accuracy for missing data of an HiDS matrix.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {498–507},
numpages = {10},
keywords = {Recommender system, High-dimensional and sparse, Non-negative latent factor, α-β-divergence, User preference, Momentum},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380134,
author = {Zhao, Kai and Bai, Ting and Wu, Bin and Wang, Bai and Zhang, Youjie and Yang, Yuanyu and Nie, Jian-Yun},
title = {Deep Adversarial Completion for Sparse Heterogeneous Information Network Embedding},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380134},
doi = {10.1145/3366423.3380134},
abstract = {Heterogeneous information network (HIN) contains multiple types of entities and relations. Most of existing HIN embedding methods learn the semantic information based on the heterogeneous structures between different entities, which are implicitly assumed to be complete. However, in real world, it is common that some relations are partially observed due to privacy or other reasons, resulting in a sparse network, in which the structure may be incomplete, and the ”unseen” links may also be positive due to the missing relations in data collection. To address this problem, we propose a novel and principled approach: a Multi-View Adversarial Completion Model (MV-ACM). Each relation space is characterized in a single viewpoint, enabling us to use the topological structural information in each view. Based on the multi-view architecture, an adversarial learning process is utilized to learn the reciprocity (i.e., complementary information) between different relations: In the generator, MV-ACM generates the complementary views by computing the similarity of the semantic representation of the same node in different views; while in the discriminator, MV-ACM discriminates whether the view is complementary by the topological structural similarity. Then we update the node’s semantic representation by aggregating neighborhoods information from the syncretic views. We conduct systematical experiments1 on six real-world networks from varied domains: AMiner, PPI, YouTube, Twitter, Amazon and Alibaba. Empirical results show that MV-ACM significantly outperforms the state-of-the-art approaches for both link prediction and node classification tasks. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {508–518},
numpages = {11},
keywords = {Relational Completion, Heterogeneous Information Network, Adversarial Learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380135,
author = {Khawar, Farhan and Poon, Leonard and Zhang, Nevin L.},
title = {Learning the Structure of Auto-Encoding Recommenders},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380135},
doi = {10.1145/3366423.3380135},
abstract = {Autoencoder recommenders have recently shown state-of-the-art performance in the recommendation task due to their ability to model non-linear item relationships effectively. However, existing autoencoder recommenders use fully-connected neural network layers and do not employ structure learning. This can lead to inefficient training, especially when the data is sparse as commonly found in collaborative filtering. The aforementioned results in lower generalization ability and reduced performance. In this paper, we introduce structure learning for autoencoder recommenders by taking advantage of the inherent item groups present in the collaborative filtering domain. Due to the nature of items in general, we know that certain items are more related to each other than to other items. Based on this, we propose a method that first learns groups of related items and then uses this information to determine the connectivity structure of an auto-encoding neural network. This results in a network that is sparsely connected. This sparse structure can be viewed as a prior that guides the network training. Empirically we demonstrate that the proposed structure learning enables the autoencoder to converge to a local optimum with a much smaller spectral norm and generalization error bound than the fully-connected network. The resultant sparse network considerably outperforms the state-of-the-art methods like Mult-vae/Mult-dae on multiple benchmarked datasets even when the same number of parameters and flops are used. It also has a better cold-start performance. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {519–529},
numpages = {11},
keywords = {Shallow Networks., Sparse Autoencoder, Wide Autoencoder, Structure Learning, Collaborative Filtering},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380136,
author = {Gao, Junyi and Xiao, Cao and Wang, Yasha and Tang, Wen and Glass, Lucas M. and Sun, Jimeng},
title = {StageNet: Stage-Aware Neural Networks for Health Risk Prediction},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380136},
doi = {10.1145/3366423.3380136},
abstract = {Deep learning has demonstrated success in health risk prediction especially for patients with chronic and progressing conditions. Most existing works focus on learning disease patterns from longitudinal patient data, but pay little attention to the disease progression stage itself. To fill the gap, we propose a Stage-aware neural Network (StageNet) model to extract disease stage information from patient data and integrate it into risk prediction. StageNet is enabled by (1) a stage-aware long short-term memory (LSTM) module that extracts health stage variations unsupervisedly; (2) a stage-adaptive convolutional module that incorporates stage-related progression patterns into risk prediction. We evaluate StageNet on two real-world datasets and show that StageNet outperforms state-of-the-art models in risk prediction task and patient subtyping task. Compared to the best baseline model, StageNet achieves up to 12% higher AUPRC for risk prediction task on two real-world patient datasets. StageNet also achieves over 58% higher Calinski-Harabasz score (a cluster quality metric) for a patient subtyping task.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {530–540},
numpages = {11},
keywords = {healthcare informatics, risk prediction, electronic health record},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380137,
author = {Biswal, Siddharth and Xiao, Cao and Glass, Lucas M. and Westover, Brandon and Sun, Jimeng},
title = {CLARA: Clinical Report Auto-Completion},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380137},
doi = {10.1145/3366423.3380137},
abstract = {Generating clinical reports from raw recordings such as X-rays and electroencephalogram (EEG) is an essential and routine task for doctors. However, it is often time-consuming to write accurate and detailed reports. Most existing methods try to generate the whole reports from the raw input with limited success because 1) generated reports often contain errors that need manual review and correction, 2) it does not save time when doctors want to write additional information into the report, and 3) the generated reports are not customized based on individual doctors’ preference. We propose CLinicAl Report Auto-completion (, an interactive method that generates reports in a sentence by sentence fashion based on doctors’ anchor words and partially completed sentences. earches for most relevant sentences from existing reports as the template for the current report. The retrieved sentences are sequentially modified by combining with the input feature representations to create the final report. In our experimental evaluation chieved 0.393 CIDEr and 0.248 BLEU-4 on X-ray reports and 0.482 CIDEr and 0.491 BLEU-4 for EEG reports for sentence-level generation, which is up to 35% improvement over the best baseline. Also via our qualitative evaluation, s shown to produce reports which have a significantly higher level of approval by doctors in a user study (3.74 out of 5 for s 2.52 out of 5 for the baseline).},
booktitle = {Proceedings of The Web Conference 2020},
pages = {541–550},
numpages = {10},
keywords = {X-ray report Generation, Auto-completion, EEG text generation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380138,
author = {Liu, Huafeng and Jing, Liping and Wen, Jingxuan and Wu, Zhicheng and Sun, Xiaoyi and Wang, Jiaqi and Xiao, Lin and Yu, Jian},
title = {Deep Global and Local Generative Model for Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380138},
doi = {10.1145/3366423.3380138},
abstract = {Deep generative model, especially variational auto-encoder (VAE), has been successfully employed by more and more recommendation systems. The reason is that it combines the flexibility of probabilistic generative model with the powerful non-linear feature representation ability of deep neural networks. The existing VAE-based recommendation models are usually proposed under global assumption by incorporating simple priors, e.g., a single Gaussian, to regularize the latent variables. This strategy, however, is ineffective when the user is simultaneously interested in different kinds of items, i.e., the user’s preference may be highly diverse. In this paper, thus, we propose a Deep Global and Local Generative Model for recommendation to consider both local and global structure among users (DGLGM) under the Wasserstein auto-encoder framework. Besides keeping the global structure like the existing model, DGLGM adopts a non-parametric Mixture Gaussian distribution with several components to capture the diversity of the users’ preferences. Each component is corresponding to one local structure and its optimal size can be determined via the automatic relevance determination technique. These two parts can be seamlessly integrated and enhance each other. The proposed DGLGM can be efficiently inferred by minimizing its penalized upper bound with the aid of local variational optimization technique. Meanwhile, we theoretically analyze its generalization error bounds to guarantee its performance in sparse feedback data with diversity. By comparing with the state-of-the-art methods, the experimental results demonstrate that DGLGM consistently benefits the recommendation system in top-N recommendation task.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {551–561},
numpages = {11},
keywords = {Deep Generative Model, Personalized Recommendation, Bayesian Graphical Model},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380139,
author = {Hounsel, Austin and Borgolte, Kevin and Schmitt, Paul and Holland, Jordan and Feamster, Nick},
title = {Comparing the Effects of DNS, DoT, and DoH on Web Performance},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380139},
doi = {10.1145/3366423.3380139},
abstract = {Nearly every service on the Internet relies on the Domain Name System (DNS), which translates a human-readable name to an IP address before two endpoints can communicate. Today, DNS traffic is unencrypted, leaving users vulnerable to eavesdropping and tampering. Past work has demonstrated that DNS queries can reveal a user’s browsing history and even what smart devices they are using at home. In response to these privacy concerns, two new protocols have been proposed: DNS-over-HTTPS (DoH) and DNS-over-TLS (DoT). Instead of sending DNS queries and responses in the clear, DoH and DoT establish encrypted connections between users and resolvers. By doing so, these protocols provide privacy and security guarantees that traditional DNS (Do53) lacks. In this paper, we measure the effect of Do53, DoT, and DoH on query response times and page load times from five global vantage points. We find that although DoH and DoT response times are generally higher than Do53, both protocols can perform better than Do53 in terms of page load times. However, as throughput decreases and substantial packet loss and latency are introduced, web pages load fastest with Do53. Additionally, web pages load successfully more often with Do53 and DoT than DoH. Based on these results, we provide several recommendations to improve DNS performance, such as opportunistic partial responses and wire format caching.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {562–572},
numpages = {11},
keywords = {networks, security, network performance, privacy},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380140,
author = {Boob, Digvijay and Gao, Yu and Peng, Richard and Sawlani, Saurabh and Tsourakakis, Charalampos and Wang, Di and Wang, Junxing},
title = {Flowless: Extracting Densest Subgraphs Without Flow Computations},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380140},
doi = {10.1145/3366423.3380140},
abstract = {The problem of finding dense components of a graph is a major primitive in graph mining and data analysis. The densest subgraph problem (DSP) that asks to find a subgraph with maximum average degree forms a basic primitive in dense subgraph discovery with applications ranging from community detection to unsupervised discovery of biological network modules [16]. The DSP is exactly solvable in polynomial time using maximum flows [14, 17, 22]. Due to the high computational cost of maximum flows, Charikar’s greedy approximation algorithm is usually preferred in practice due to its linear time and linear space complexity [3, 8]. It constitutes a key algorithmic idea in scalable solutions for large-scale dynamic graphs [5, 7]. However, its output density can be a factor 2 off the optimal solution. In this paper we design Greedy++, an iterative peeling algorithm that improves upon the performance of Charikar’s greedy algorithm significantly. Our iterative greedy algorithm is able to output near-optimal and optimal solutions fast by adding a few more passes to Charikar’s greedy algorithm. Furthermore Greedy++ is more robust against the structural heterogeneities (e.g., skewed degree distributions) in real-world datasets. An additional property of our algorithm is that it is able to assess quickly, without computing maximum flows, whether Charikar’s approximation quality on a given graph instance is closer to the worst case theoretical guarantee of or to optimality. We also demonstrate that our method has significant efficiency advantage over the maximum flow based exact optimal algorithm. For example, our algorithm achieves ∼ 145 \texttimes{} speedup on average across a variety of real-world graphs while finding subgraphs of density that are at least 90% as dense as the densest subgraph. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {573–583},
numpages = {11},
keywords = {dense subgraph discovery, graph mining, algorithm design, applications},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380141,
author = {Fang, Zhihan and Wang, Guang and Wang, Shuai and Zuo, Chaoji and Zhang, Fan and Zhang, Desheng},
title = {CellRep: Usage Representativeness Modeling and Correction Based on Multiple City-Scale Cellular Networks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380141},
doi = {10.1145/3366423.3380141},
abstract = {Understanding representativeness in cellular web logs at city scale is essential for web applications. Most of the existing work on cellular web analyses or applications is built upon data from a single network in a city, which may not be representative of the overall usage patterns since multiple cellular networks coexist in most cities in the world. In this paper, we conduct the first comprehensive investigation of multiple cellular networks in a city with a 100% user penetration rate. We study web usage pattern (e.g., internet access services) correlation and difference between diverse cellular networks in terms of spatial and temporal dimensions to quantify the representativeness of web usage from a single network in usage patterns of all users in the same city. Moreover, relying on three external datasets, we study the correlation between the representativeness and contextual factors (e.g., Point-of-Interest, population, and mobility) to explain the potential causalities for the representativeness difference. We found that contextual diversity is a key reason for representativeness difference, and representativeness has a significant impact on the performance of real-world applications. Based on the analysis results, we further design a correction model to address the bias of single cellphone networks and improve representativeness by 45.8%.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {584–595},
numpages = {12},
keywords = {Correction, Representativeness, Cellular Networks},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380142,
author = {Paes Leme, Renato and Sivan, Balasubramanian and Teng, Yifeng},
title = {Why Do Competitive Markets Converge to First-Price Auctions?},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380142},
doi = {10.1145/3366423.3380142},
abstract = {We consider a setting in which bidders participate in multiple auctions run by different sellers, and optimize their bids for the aggregate auction. We analyze this setting by formulating a game between sellers, where a seller’s strategy is to pick an auction to run. Our analysis aims to shed light on the recent change in the Display Ads market landscape: here, ad exchanges (sellers) were mostly running second-price auctions earlier and over time they switched to variants of the first-price auction, culminating in Google’s Ad Exchange moving to a first-price auction in 2019. Our model and results offer an explanation for why the first-price auction occurs as a natural equilibrium in such competitive markets. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {596–605},
numpages = {10},
keywords = {First-price auction, competitive market, Nash equilibrium},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380143,
author = {Van Koevering, Katherine and Benson, Austin R. and Kleinberg, Jon},
title = {Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380143},
doi = {10.1145/3366423.3380143},
abstract = {There is inherent information captured in the order in which we write words in a list. The orderings of binomials — lists of two words separated by ‘and’ or ‘or’ — has been studied for more than a century. These binomials are common across many areas of speech, in both formal and informal text. In the last century, numerous explanations have been given to describe what order people use for these binomials, from differences in semantics to differences in phonology. These rules describe primarily ‘frozen’ binomials that exist in exactly one ordering and have lacked large-scale trials to determine efficacy. Text in online social media such as Reddit provides a unique opportunity to study these lists in the context of informal text at a very large scale. In this work, we expand the view of binomials to include a large-scale analysis of both frozen and non-frozen binomials in a quantitative way. Using this data, we then demonstrate that most previously proposed rules are ineffective at predicting binomial ordering. By tracking the order of these binomials across time and communities we are able to establish additional, unexplored dimensions central to these predictions and demonstrate the global structure of the binomials across communities. Expanding beyond the question of individual binomials, we also explore the global structure of binomials in various communities, establishing a new model for these lists and analyzing this structure for non-frozen and frozen binomials. Additionally, novel analysis of trinomials — lists of length three — suggests that none of the binomials analysis applies in these cases. Finally, we demonstrate how large data sets gleaned from the web can be used in conjunction with older theories to expand and improve on old questions. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {606–616},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380144,
author = {Miao, Zhengjie and Li, Yuliang and Wang, Xiaolan and Tan, Wang-Chiew},
title = {Snippext: Semi-Supervised Opinion Mining with Augmented&nbsp;Data},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380144},
doi = {10.1145/3366423.3380144},
abstract = {Online services are interested in solutions to opinion mining, which is the problem of extracting aspects, opinions, and sentiments from text. One method to mine opinions is to leverage the recent success of pre-trained language models which can be fine-tuned to obtain high-quality extractions from reviews. However, fine-tuning language models still requires a non-trivial amount of training data. In this paper, we study the problem of how to significantly reduce the amount of labeled training data required in fine-tuning language models for opinion mining. We describe , an opinion mining system developed over a language model that is fine-tuned through semi-supervised learning with augmented data. A novelty of is its clever use of a two-prong approach to achieve state-of-the-art (SOTA) performance with little labeled training data through: (1) data augmentation to automatically generate more labeled training data from existing ones, and (2) a semi-supervised learning technique to leverage the massive amount of unlabeled data in addition to the (limited amount of) labeled data. We show with extensive experiments that performs comparably and can even exceed previous SOTA results on several opinion mining tasks with only half the training data required. Furthermore, it achieves new SOTA results when all training data are leveraged. By comparison to a baseline pipeline, we found that extracts significantly more fine-grained opinions which enable new opportunities of downstream applications.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {617–628},
numpages = {12},
keywords = {MixUp, Sentiment Analysis, Data augmentation, Fine-tuning Language Models, Semi-supervised Learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380145,
author = {Shao, Huajie and Sun, Dachun and Wu, Jiahao and Zhang, Zecheng and Zhang, Aston and Yao, Shuochao and Liu, Shengzhong and Wang, Tianshi and Zhang, Chao and Abdelzaher, Tarek},
title = {Paper2repo: GitHub Repository Recommendation for Academic Papers},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380145},
doi = {10.1145/3366423.3380145},
abstract = {GitHub has become a popular social application platform, where a large number of users post their open source projects. In particular, an increasing number of researchers release repositories of source code related to their research papers in order to attract more people to follow their work. Motivated by this trend, we describe a novel item-item cross-platform recommender system, paper2repo, that recommends relevant repositories on GitHub that match a given paper in an academic search system such as Microsoft Academic. The key challenge is to identify the similarity between an input paper and its related repositories across the two platforms, without the benefit of human labeling. Towards that end, paper2repo integrates text encoding and constrained graph convolutional networks (GCN) to automatically learn and map the embeddings of papers and repositories into the same space, where proximity offers the basis for recommendation. To make our method more practical in real life systems, labels used for model training are computed automatically from features of user actions on GitHub. In machine learning, such automatic labeling is often called distant supervision. To the authors’ knowledge, this is the first distant-supervised cross-platform (paper to repository) matching system. We evaluate the performance of paper2repo on real-world data sets collected from GitHub and Microsoft Academic. Results demonstrate that it outperforms other state of the art recommendation methods.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {629–639},
numpages = {11},
keywords = {Recommender system, text encoding, constrained graph convolutional networks, cross-platform recommendation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380146,
author = {Fang, Zheng and Cao, Yanan and Li, Ren and Zhang, Zhenyu and Liu, Yanbing and Wang, Shi},
title = {High Quality Candidate Generation and Sequential Graph Attention Network for Entity Linking},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380146},
doi = {10.1145/3366423.3380146},
abstract = {Entity Linking (EL) is a task for mapping mentions in text to corresponding entities in knowledge base (KB). This task usually includes candidate generation (CG) and entity disambiguation (ED) stages. Recent EL systems based on neural network models have achieved good performance, but they still face two challenges: (i) Previous studies evaluate their models without considering the differences between candidate entities. In fact, the quality (gold recall in particular) of candidate sets has an effect on the EL results. So, how to promote the quality of candidates needs more attention. (ii) In order to utilize the topical coherence among the referred entities, many graph and sequence models are proposed for collective ED. However, graph-based models treat all candidate entities equally which may introduce much noise information. On the contrary, sequence models can only observe previous referred entities, ignoring the relevance between the current mention and its subsequent entities. To address the first problem, we propose a multi-strategy based CG method to generate high recall candidate sets. For the second problem, we design a Sequential Graph Attention Network (SeqGAT) which combines the advantages of graph and sequence methods. In our model, mentions are dealt with in a sequence manner. Given the current mention, SeqGAT dynamically encodes both its previous referred entities and subsequent ones, and assign different importance to these entities. In this way, it not only makes full use of the topical consistency, but also reduce noise interference. We conduct experiments on different types of datasets and compare our method with previous EL system on the open evaluation platform. The comparison results show that our model achieves significant improvements over the state-of-the-art methods. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {640–650},
numpages = {11},
keywords = {graph attention network, BERT, Entity linking, candidate generation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380147,
author = {Li, Shuangyin and Zhang, Yu and Pan, Rong and Mo, Kaixiang},
title = {Adaptive Probabilistic Word Embedding},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380147},
doi = {10.1145/3366423.3380147},
abstract = {Word embeddings have been widely used and proven to be effective in many natural language processing and text modeling tasks. It is obvious that one ambiguous word could have very different semantics in various contexts, which is called polysemy. Most existing works aim at generating only one single embedding for each word while a few works build a limited number of embeddings to present different meanings for each word. However, it is hard to determine the exact number of senses for each word as the word meaning is dependent on contexts. To address this problem, we propose a novel Adaptive Probabilistic Word Embedding (APWE) model, where the word polysemy is defined over a latent interpretable semantic space. Specifically, at first each word is represented by an embedding in the latent semantic space and then based on the proposed APWE model, the word embedding can be adaptively adjusted and updated based on different contexts to obtain the tailored word embedding. Empirical comparisons with state-of-the-art models demonstrate the superiority of the proposed APWE model. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {651–661},
numpages = {11},
keywords = {Word Polysemy, Probabilistic Word Embedding, Word Embedding, Adaptive Word Representations},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380148,
author = {Zhang, Xiaoying and Xie, Hong and Li, Hang and C.S. Lui, John},
title = {Conversational Contextual Bandit: Algorithm and Application},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380148},
doi = {10.1145/3366423.3380148},
abstract = {Contextual bandit algorithms provide principled online learning solutions to balance the exploitation-exploration trade-off in various applications such as recommender systems. However, the learning speed of the traditional contextual bandit algorithms is often slow due to the need for extensive exploration. This poses a critical issue in applications like recommender systems, since users may need to provide feedbacks on a lot of uninterested items. To accelerate the learning speed, we generalize contextual bandit to conversational contextual bandit. Conversational contextual bandit leverages not only behavioral feedbacks on arms (e.g., articles in news recommendation), but also occasional conversational feedbacks on key-terms from the user. Here, a key-term can relate to a subset of arms, for example, a category of articles in news recommendation. We then design the Conversational UCB algorithm (ConUCB) to address two challenges in conversational contextual bandit: (1) which key-terms to select to conduct conversation, (2) how to leverage conversational feedbacks to accelerate the speed of bandit learning. We theoretically prove that ConUCB can achieve a smaller regret upper bound than the traditional contextual bandit algorithm LinUCB, which implies a faster learning speed. Experiments on synthetic data, as well as real datasets from Yelp and Toutiao, demonstrate the efficacy of the ConUCB algorithm. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {662–672},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380149,
author = {Sun, Yiwei and Wang, Suhang and Tang, Xianfeng and Hsieh, Tsung-Yu and Honavar, Vasant},
title = {Adversarial Attacks on Graph Neural Networks via Node Injections: A Hierarchical Reinforcement Learning Approach},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380149},
doi = {10.1145/3366423.3380149},
abstract = {Graph Neural Networks (GNN) offer the powerful approach to node classification in complex networks across many domains including social media, E-commerce, and FinTech. However, recent studies show that GNNs are vulnerable to attacks aimed at adversely impacting their node classification performance. Existing studies of adversarial attacks on GNN focus primarily on manipulating the connectivity between existing nodes, a task that requires greater effort on the part of the attacker in real-world applications. In contrast, it is much more expedient on the part of the attacker to inject adversarial nodes, e.g., fake profiles with forged links, into existing graphs so as to reduce the performance of the GNN in classifying existing nodes. Hence, we consider a novel form of node injection poisoning attacks on graph data. We model the key steps of a node injection attack, e.g., establishing links between the injected adversarial nodes and other nodes, choosing the label of an injected node, etc. by a Markov Decision Process. We propose a novel reinforcement learning method for Node Injection Poisoning Attacks (NIPA), to sequentially modify the labels and links of the injected nodes, without changing the connectivity between existing nodes. Specifically, we introduce a hierarchical Q-learning network to manipulate the labels of the adversarial nodes and their links with other nodes in the graph, and design an appropriate reward function to guide the reinforcement learning agent to reduce the node classification performance of GNN. The results of the experiments show that NIPA is consistently more effective than the baseline node injection attack methods for poisoning graph data on three benchmark datasets. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {673–683},
numpages = {11},
keywords = {Adversarial Attack, Reinforcement learning;, Graph Poisoning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380150,
author = {Doan, Khoa D. and Reddy, Chandan K.},
title = {Efficient Implicit Unsupervised Text Hashing Using Adversarial Autoencoder},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380150},
doi = {10.1145/3366423.3380150},
abstract = {Searching for documents with semantically similar content is a fundamental problem in the information retrieval domain with various challenges, primarily, in terms of efficiency and effectiveness. Despite the promise of modeling structured dependencies in documents, several existing text hashing methods lack an efficient mechanism to incorporate such vital information. Additionally, the desired characteristics of an ideal hash function, such as robustness to noise, low quantization error and bit balance/uncorrelation, are not effectively learned with existing methods. This is because of the requirement to either tune additional hyper-parameters or optimize these heuristically and explicitly constructed cost functions. In this paper, we propose a Denoising Adversarial Binary Autoencoder (DABA) model which presents a novel representation learning framework that captures structured representation of text documents in the learned hash function. Also, adversarial training provides an alternative direction to implicitly learn a hash function that captures all the desired characteristics of an ideal hash function. Essentially, DABA adopts a novel single-optimization adversarial training procedure that minimizes the Wasserstein distance in its primal domain to regularize the encoder’s output of either a recurrent neural network or a convolutional autoencoder. We empirically demonstrate the effectiveness of our proposed method in capturing the intrinsic semantic manifold of the related documents. The proposed method outperforms the current state-of-the-art shallow and deep unsupervised hashing methods for the document retrieval task on several prominent document collections.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {684–694},
numpages = {11},
keywords = {autoencoder, deep learning., Hashing, adversarial training},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380151,
author = {Lian, Defu and Wang, Haoyu and Liu, Zheng and Lian, Jianxun and Chen, Enhong and Xie, Xing},
title = {LightRec: A Memory and Search-Efficient Recommender System},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380151},
doi = {10.1145/3366423.3380151},
abstract = {Deep recommender systems have achieved remarkable improvements in recent years. Despite its superior ranking precision, the running efficiency and memory consumption turn out to be severe bottlenecks in reality. To overcome both limitations, we propose LightRec, a lightweight recommender system which enjoys fast online inference and economic memory consumption. The backbone of LightRec is a total of B codebooks, each of which is composed of W latent vectors, known as codewords. On top of such a structure, LightRec will have an item represented as additive composition of B codewords, which are optimally selected from each of the codebooks. To effectively learn the codebooks from data, we devise an end-to-end learning workflow, where challenges on the inherent differentiability and diversity are conquered by the proposed techniques. In addition, to further improve the representation quality, several distillation strategies are employed, which better preserves user-item relevance scores and relative ranking orders. LightRec is extensively evaluated with four real-world datasets, which gives rise to two empirical findings: 1) compared with those the state-of-the-art lightweight baselines, LightRec achieves over 11% relative improvements in terms of recall performance; 2) compared to conventional recommendation algorithms, LightRec merely incurs negligible accuracy degradation while leads to more than 27x speedup in top-k recommendation.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {695–705},
numpages = {11},
keywords = {Composite Encoding, Quantization, Recommender System, Online Recommendation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380152,
author = {Amburg, Ilya and Veldt, Nate and Benson, Austin},
title = {Clustering in Graphs and Hypergraphs with Categorical Edge Labels},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380152},
doi = {10.1145/3366423.3380152},
abstract = {Modern graph or network datasets often contain rich structure that goes beyond simple pairwise connections between nodes. This calls for complex representations that can capture, for instance, edges of different types as well as so-called “higher-order interactions” that involve more than two nodes at a time. However, we have fewer rigorous methods that can provide insight from such representations. Here, we develop a computational framework for the problem of clustering hypergraphs with categorical edge labels — or different interaction types — where clusters corresponds to groups of nodes that frequently participate in the same type of interaction. Our methodology is based on a combinatorial objective function that is related to correlation clustering on graphs but enables the design of much more efficient algorithms that also seamlessly generalize to hypergraphs. When there are only two label types, our objective can be optimized in polynomial time, using an algorithm based on minimum cuts. Minimizing our objective becomes NP-hard with more than two label types, but we develop fast approximation algorithms based on linear programming relaxations that have theoretical cluster quality guarantees. We demonstrate the efficacy of our algorithms and the scope of the model through problems in edge-label community detection, clustering with temporal data, and exploratory data analysis.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {706–717},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380153,
author = {Wang, Zhuoyi and Wang, Yigong and Lin, Yu and Delord, Evan and Latifur, Khan},
title = {Few-Sample and Adversarial Representation Learning for Continual Stream Mining},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380153},
doi = {10.1145/3366423.3380153},
abstract = {Deep Neural Networks (DNNs) have primarily been demonstrated to be useful for closed-world classification problems where the number of categories is fixed. However, DNNs notoriously fail when tasked with label prediction in a non-stationary data stream scenario, which has the continuous emergence of the unknown or novel class (categories not in the training set). For example, new topics continually emerge in social media or e-commerce. To solve this challenge, a DNN should not only be able to detect the novel class effectively but also incrementally learn new concepts from limited samples over time. Literature that addresses both problems simultaneously is limited. In this paper, we focus on improving the generalization of the model on the novel classes, and making the model continually learn from only a few samples from the novel categories. Different from existing approaches that rely on abundant labeled instances to re-train/update the model, we propose a new approach based on Few Sample and Adversarial Representation Learning (FSAR). The key novelty is that we introduce the adversarial confusion term into both the representation learning and few-sample learning process, which reduces the over-confidence of the model on the seen classes, further enhance the generalization of the model to detect and learn new categories with only a few samples. We train the FSAR operated in two stages: first, FSAR learns an intra-class compacted and inter-class separated feature embedding to detect the novel classes; next, we collect a few labeled samples belong to the new categories, utilize episode-training to exploit the intrinsic features for few-sample learning. We evaluated FSAR on different datasets, using extensive experimental results from various simulated stream benchmarks to show that FSAR effectively outperforms current state-of-the-art approaches. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {718–728},
numpages = {11},
keywords = {Adversarial learning, Deep Neural Network, Stream Mining, Novel Class Detection, Few Sample Learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380154,
author = {Pan, Feiyang and Ao, Xiang and Tang, Pingzhong and Lu, Min and Liu, Dapeng and Xiao, Lei and He, Qing},
title = {Field-Aware Calibration: A Simple and Empirically Strong Method for Reliable Probabilistic Predictions},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380154},
doi = {10.1145/3366423.3380154},
abstract = {It is often observed that the probabilistic predictions given by a machine learning model can disagree with averaged actual outcomes on specific subsets of data, which is also known as the issue of miscalibration. It is responsible for the unreliability of practical machine learning systems. For example, in online advertising, an ad can receive a click-through rate prediction of 0.1 over some population of users where its actual click rate is 0.15. In such cases, the probabilistic predictions have to be fixed before the system can be deployed. In this paper, we first introduce a new evaluation metric named field-level calibration error that measures the bias in predictions over the sensitive input field that the decision-maker concerns. We show that existing post-hoc calibration methods have limited improvements in the new field-level metric and other non-calibration metrics such as the AUC score. To this end, we propose Neural Calibration, a simple yet powerful post-hoc calibration method that learns to calibrate by making full use of the field-aware information over the validation set. We present extensive experiments on five large-scale datasets. The results showed that Neural Calibration significantly improves against uncalibrated predictions in common metrics such as the negative log-likelihood, Brier score and AUC, as well as the proposed field-level calibration error.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {729–739},
numpages = {11},
keywords = {Probabilistic prediction, Neural Calibration, Field-level Calibration Error, Field-aware Calibration},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380155,
author = {He, Gaole and Li, Junyi and Zhao, Wayne Xin and Liu, Peiju and Wen, Ji-Rong},
title = {Mining Implicit Entity Preference from User-Item Interaction Data for Knowledge Graph Completion via Adversarial Learning},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380155},
doi = {10.1145/3366423.3380155},
abstract = {The task of Knowledge Graph Completion&nbsp;(KGC) aims to automatically infer the missing fact information in Knowledge Graph (KG). In this paper, we take a new perspective that aims to leverage rich user-item interaction data (user interaction data for short) for improving the KGC task. Our work is inspired by the observation that many KG entities correspond to online items in application systems. However, the two kinds of data sources have very different intrinsic characteristics, and it is likely to hurt the original performance using simple fusion strategy. To address this challenge, we propose a novel adversarial learning approach by leveraging user interaction data for the KGC task. Our generator is isolated from user interaction data, and serves to improve the performance of the discriminator. The discriminator takes the learned useful information from user interaction data as input, and gradually enhances the evaluation capacity in order to identify the fake samples generated by the generator. To discover implicit entity preference of users, we design an elaborate collaborative learning algorithms based on graph neural networks, which will be jointly optimized with the discriminator. Such an approach is effective to alleviate the issues about data heterogeneity and semantic complexity for the KGC task. Extensive experiments on three real-world datasets have demonstrated the effectiveness of our approach on the KGC task.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {740–751},
numpages = {12},
keywords = {User Preference, Adversarial Learning, Knowledge Graph},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380156,
author = {Zhao, Xiangyu and Wang, Longbiao and He, Ruifang and Yang, Ting and Chang, Jinxin and Wang, Ruifang},
title = {Multiple Knowledge Syncretic Transformer for Natural Dialogue Generation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380156},
doi = {10.1145/3366423.3380156},
abstract = {Knowledge is essential for intelligent conversation systems to generate informative responses. This knowledge comprises a wide range of diverse modalities such as knowledge graphs (KGs), grounding documents and conversation topics. However, limited abilities in understanding language and utilizing different types of knowledge still challenge existing approaches. Some researchers try to enhance models’ language comprehension ability by employing the pre-trained language models, but they neglect the importance of external knowledge in specific tasks. In this paper, we propose a novel universal transformer-based architecture for dialogue system, the Multiple Knowledge Syncretic Transformer (MKST), which fuses multi-knowledge in open-domain conversation. Firstly, the model is pre-trained on a large-scale corpus to learn commonsense knowledge. Then during fine-tuning, we divide the type of knowledge into two specific categories that are handled in different ways by our model. While the encoder is responsible for encoding dialogue contexts with multifarious knowledge together, the decoder with a knowledge-aware mechanism attentively reads the fusion of multi-knowledge to promote better generation. This is the first attempt that fuses multi-knowledge in one conversation model. The experimental results have been demonstrated that our model achieves significant improvement on knowledge-driven dialogue generation tasks than state-of-the-art baselines. Meanwhile, our new benchmark could facilitate the further study in this research area. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {752–762},
numpages = {11},
keywords = {Multiple Knowledge, Dialogue Generation, Syncretic Transformer, Pre-trained Model},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380157,
author = {Chaqfeh, Moumena and Zaki, Yasir and Hu, Jacinta and Subramanian, Lakshmi},
title = {JSCleaner: De-Cluttering Mobile Webpages Through JavaScript Cleanup},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380157},
doi = {10.1145/3366423.3380157},
abstract = {A significant fraction of the World Wide Web suffers from the excessive usage of JavaScript (JS). Based on an analysis of popular webpages, we observed that a considerable number of JS elements utilized by these pages are not essential for their visual and functional features. In this paper, we propose JSCleaner, a JavaScript de-cluttering engine that aims at simplifying webpages without compromising their content or functionality. JSCleaner relies on a rule-based classification algorithm that classifies JS into three main categories: non-critical, replaceable, and critical. JSCleaner removes non-critical JS from a webpage, translates replaceable JS elements with their HTML outcomes, and preserves critical JS. Our quantitative evaluation of 500 popular webpages shows that JSCleaner achieves around 30% reduction in page load times coupled with a 50% reduction in the number of requests and the page size. In addition, our qualitative user study of 103 evaluators shows that JSCleaner preserves 95% of the page content similarity, while maintaining nearly 88% of the page functionality (the remaining 12% did not have a major impact on the user browsing experience).},
booktitle = {Proceedings of The Web Conference 2020},
pages = {763–773},
numpages = {11},
keywords = {User Experience, JavaScript, Web Simplification, Classification},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380158,
author = {Huang, Yen-Hao and Liu, Ting-Wei and Lee, Ssu-Rui and Calderon Alvarado, Fernando Henrique and Chen, Yi-Shin},
title = {Conquering Cross-Source Failure for News Credibility: Learning Generalizable Representations beyond Content Embedding},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380158},
doi = {10.1145/3366423.3380158},
abstract = {False information on the Internet has caused severe damage to society. Researchers have proposed methods to determine the credibility of news and have obtained good results. As different media sources (publishers) have different content generators (writers) and may focus on different topics or aspects, the word/topic distribution for each media source is divergent from others. We expose a challenge in the generalizability of existing content-based methods to perform consistently when applied to news from media sources non-existing in the training set, namely the cross-source failure. A cross-source setting can cause a decrease beyond in accuracy for current methods; content-sensitive features are considered one of the major causes of cross-source failure for a content-based approach. To overcome this challenge, we propose a syntactic network for news credibility (SYNC), which focuses on function words and syntactic structure to learn generalizable representations for news credibility and further reinforce the cross-source robustness for different media. Experiments with cross-validation on 194 real-world media sources showed that the proposed method could learn the generalizable features and outperformed the state-of-the-art methods on unseen media sources. Extensive analysis on the embedding feature representation represents a strength of the proposed method compared to current content embedding feature approaches. We envision that the proposed method is more robust for real-life application with SYNC on account of its good generalizability.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {774–784},
numpages = {11},
keywords = {cross-source failure, fake news, generalizability, neural network},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380159,
author = {Zhong, Qiwei and Liu, Yang and Ao, Xiang and Hu, Binbin and Feng, Jinghua and Tang, Jiayu and He, Qing},
title = {Financial Defaulter Detection on Online Credit Payment via Multi-View Attributed Heterogeneous Information Network},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380159},
doi = {10.1145/3366423.3380159},
abstract = {Default user detection plays one of the backbones in credit risk forecasting and management. It aims at, given a set of corresponding features, e.g., patterns extracted from trading behaviors, predicting the polarity indicating whether a user will fail to make required payments in the future. Recent efforts attempted to incorporate attributed heterogeneous information network&nbsp;(AHIN) for extracting complex interactive features of users and achieved remarkable success on discovering specific default users such as fraud, cash-out users, etc. In this paper, we consider default users, a more general concept in credit risk, and propose a multi-view attributed heterogeneous information network based approach coined MAHINDER to remedy the special challenges. First, multiple views of user behaviors are adopted to learn personal profile due to the endogenous aspect of financial default. Second, local behavioral patterns are specifically modeled since financial default is adversarial and accumulated. With the real datasets contained 1.38 million users on Alibaba platform, we investigate the effectiveness of MAHINDER, and the experimental results exhibit the proposed approach is able to improve AUC over 2.8% and Recall@Precision=0.1 over 13.1% compared with the state-of-the-art methods. Meanwhile, MAHINDER has as good interpretability as tree-based methods like GBDT, which buoys the deployment in online platforms.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {785–795},
numpages = {11},
keywords = {Meta-path encoder, Multi-view attributed heterogeneous information network, Financial defaulter detection},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380160,
author = {Qi, Yiyan and Wang, Pinghui and Zhang, Yuanming and Zhao, Junzhou and Tian, Guangjian and Guan, Xiaohong},
title = {Fast Generating A Large Number of Gumbel-Max Variables},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380160},
doi = {10.1145/3366423.3380160},
abstract = {The well-known Gumbel-Max Trick for sampling elements from a categorical distribution (or more generally a nonnegative vector) and its variants have been widely used in areas such as machine learning and information retrieval. To sample a random element i (or a Gumbel-Max variable i) in proportion to its positive weight vi, the Gumbel-Max Trick first computes a Gumbel random variable gi for each positive weight element i, and then samples the element i with the largest value of gi + ln vi. Recently, applications including similarity estimation and graph embedding require to generate k independent Gumbel-Max variables from high dimensional vectors. However, it is computationally expensive for a large k (e.g., hundreds or even thousands) when using the traditional Gumbel-Max Trick. To solve this problem, we propose a novel algorithm, FastGM, that reduces the time complexity from O(kn+) to O(kln k + n+), where n+ is the number of positive elements in the vector of interest. Instead of computing k independent Gumbel random variables directly, we find that there exists a technique to generate these variables in descending order. Using this technique, our method FastGM computes variables gi + ln vi for all positive elements i in descending order. As a result, FastGM significantly reduces the computation time because we can stop the procedure of Gumbel random variables computing for many elements especially for those with small weights. Experiments on a variety of real-world datasets show that FastGM is orders of magnitude faster than state-of-the-art methods without sacrificing accuracy and incurring additional expenses.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {796–807},
numpages = {12},
keywords = {Graph embedding, Gumbel-Max Trick, Sketching},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380161,
author = {Mishra, Vikas and Laperdrix, Pierre and Vastel, Antoine and Rudametkin, Walter and Rouvoy, Romain and Lopatka, Martin},
title = {Don’t Count Me Out: On the Relevance of IP&nbsp;Address In&nbsp;The&nbsp;Tracking&nbsp;Ecosystem},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380161},
doi = {10.1145/3366423.3380161},
abstract = {Targeted online advertising has become an inextricable part of the way Web content and applications are monetized. At the beginning, online advertising consisted of simple ad-banners broadly shown to website visitors. Over time, it evolved into a complex ecosystem that tracks and collects a wealth of data to learn user habits and show targeted and personalized ads. To protect users against tracking, several countermeasures have been proposed, ranging from browser extensions that leverage filter lists, to features natively integrated into popular browsers like Firefox and Brave to combat more modern techniques like browser fingerprinting. Nevertheless, few browsers offer protections against IP address-based tracking techniques. Notably, the most popular browsers, Chrome, Firefox, Safari and Edge do not offer any. In this paper, we study the stability of the public IP addresses a user device uses to communicate with our server. Over time, a same device communicates with our server using a set of distinct IP addresses, but we find that devices reuse some of their previous IP addresses for long periods of time. We call this IP address retention and, the duration for which an IP address is retained by a device, is named the IP address retention period. We present an analysis of 34,488 unique public IP addresses collected from 2,230 users over a period of 111 days and we show that IP addresses remain a prime vector for online tracking. 87&nbsp;% of participants retain at least one IP address for more than a month and 45&nbsp;% of ISPs in our dataset allow keeping the same IP address for more than 30 days. Furthermore, we also detect the presence of cycles of IP addresses in a user’s history and highlight their potential to be abused to infer traits of the user behaviour, as well as mobility traces. Our findings paint a bleak picture of the current state of online tracking at a time where IP addresses are overlooked compared to other techniques like cookies or fingerprinting.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {808–815},
numpages = {8},
keywords = {IP address tracking, online privacy},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380162,
author = {van Wegberg, Rolf and Miedema, Fieke and Akyazi, Ugur and Noroozian, Arman and Klievink, Bram and van Eeten, Michel},
title = {Go See a Specialist? Predicting Cybercrime Sales on Online Anonymous Markets from Vendor and Product Characteristics},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380162},
doi = {10.1145/3366423.3380162},
abstract = {Many cybercriminal entrepreneurs lack the skills and techniques to provision certain parts of their business model, leading them to outsource these parts to specialized criminal vendors. Online anonymous markets, from Silk Road to AlphaBay, have been used to search for these products and contract with their criminal vendors. While one listing of a product generates high sales numbers, another identical listing fails to sell. In this paper, we investigate which factors determine the performance of cybercrime products. To answer this question, we analyze scraped data on the business-to-business cybercrime segments of AlphaBay (2015-2017), consisting of 7,543 listings from 1,339 vendors, sold at least 126,934 times. We construct new variables to capture product differentiators and price. We capture the influence of vendor characteristics by identifying five distinct vendor profiles based on latent profile analysis of six properties. We leverage these product and vendor characteristics to empirically predict the performance of cybercrime products, whilst controlling for the lifespan and type of solution. Consistent with earlier insights into carding forums, we identify prevalent product differentiators to be influencing the relative success of a product. While all these product differentiators do correlate significantly with product performance, their explanatory power is lower than that of vendor profiles. When outsourcing, the vendor seems to be of more importance to the buyers than product differentiators.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {816–826},
numpages = {11},
keywords = {Criminal performance, Cybercrime, Online anonymous markets},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380163,
author = {Li, Xiang and Wang, Chao and Tan, Jiwei and Zeng, Xiaoyi and Ou, Dan and Ou, Dan and Zheng, Bo},
title = {Adversarial Multimodal Representation Learning for Click-Through Rate Prediction},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380163},
doi = {10.1145/3366423.3380163},
abstract = {For better user experience and business effectiveness, Click-Through Rate (CTR) prediction has been one of the most important tasks in E-commerce. Although extensive CTR prediction models have been proposed, learning good representation of items from multimodal features is still less investigated, considering an item in E-commerce usually contains multiple heterogeneous modalities. Previous works either concatenate the multiple modality features, that is equivalent to giving a fixed importance weight to each modality; or learn dynamic weights of different modalities for different items through technique like attention mechanism. However, a problem is that there usually exists common redundant information across multiple modalities. The dynamic weights of different modalities computed by using the redundant information may not correctly reflect the different importance of each modality. To address this, we explore the complementarity and redundancy of modalities by considering modality-specific and modality-invariant features differently. We propose a novel Multimodal Adversarial Representation Network (MARN) for the CTR prediction task. A multimodal attention network first calculates the weights of multiple modalities for each item according to its modality-specific features. Then a multimodal adversarial network learns modality-invariant representations where a double-discriminators strategy is introduced. Finally, we achieve the multimodal item representations by combining both modality-specific and modality-invariant representations. We conduct extensive experiments on both public and industrial datasets, and the proposed method consistently achieves remarkable improvements to the state-of-the-art methods. Moreover, the approach has been deployed in an operational E-commerce system and online A/B testing further demonstrates the effectiveness.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {827–836},
numpages = {10},
keywords = {attention, recurrent neural network, e-commerce search, representation learning, multimodal learning, adversarial learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380164,
author = {Sun, Peijie and Wu, Le and Zhang, Kun and Fu, Yanjie and Hong, Richang and Wang, Meng},
title = {Dual Learning for Explainable Recommendation: Towards Unifying User Preference Prediction and Review Generation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380164},
doi = {10.1145/3366423.3380164},
abstract = {In many recommender systems, users express item opinions through two kinds of behaviors: giving preferences and writing detailed reviews. As both kinds of behaviors reflect users’ assessment of items, review enhanced recommender systems leverage these two kinds of user behaviors to boost recommendation performance. On the one hand, researchers proposed to better model the user and item embeddings with additional review information for enhancing preference prediction accuracy. On the other hand, some recent works focused on automatically generating item reviews for recommendation explanations with related user and item embeddings. We argue that, while the task of preference prediction with the accuracy goal is well recognized in the community, the task of generating reviews for explainable recommendation is also important to gain user trust and increase conversion rate. Some preliminary attempts have considered jointly modeling these two tasks, with the user and item embeddings are shared. These studies empirically showed that these two tasks are correlated, and jointly modeling them would benefit the performance of both tasks. In this paper, we make a further study of unifying these two tasks for explainable recommendation. Instead of simply correlating these two tasks with shared user and item embeddings, we argue that these two tasks are presented in dual forms. In other words, the input of the primal preference prediction task is exactly the output of the dual review generation task , with and denote the preference value space and review space. Therefore, we could explicitly model the probabilistic correlation between these two dual tasks with . We design a unified dual framework of how to inject the probabilistic duality of the two tasks in the training stage. Furthermore, as the detailed preference and review information are not available for each user-item pair in the test stage, we propose a transfer learning based model for preference prediction and review generation. Finally, extensive experimental results on two real-world datasets clearly show the effectiveness of our proposed model for both user preference prediction and review generation.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {837–847},
numpages = {11},
keywords = {review generation, recommender system, dual learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380165,
author = {Elyashar, Aviad and Uziel, Sagi and Paradise, Abigail and Puzis, Rami},
title = {The Chameleon Attack: Manipulating Content Display in Online Social Media},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380165},
doi = {10.1145/3366423.3380165},
abstract = {Online social networks (OSNs) are ubiquitous attracting millions of users all over the world. Being a popular communication media OSNs are exploited in a variety of cyber-attacks. In this article, we discuss the chameleon attack technique, a new type of OSN-based trickery where malicious posts and profiles change the way they are displayed to OSN users to conceal themselves before the attack or avoid detection. Using this technique, adversaries can, for example, avoid censorship by concealing true content when it is about to be inspected; acquire social capital to promote new content while piggybacking a trending one; cause embarrassment and serious reputation damage by tricking a victim to like, retweet, or comment a message that he wouldn’t normally do without any indication for the trickery within the OSN. An experiment performed with closed Facebook groups of sports fans shows that (1) chameleon pages can pass by the moderation filters by changing the way their posts are displayed and (2) moderators do not distinguish between regular and chameleon pages. We list the OSN weaknesses that facilitate the chameleon attack and propose a set of mitigation guidelines.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {848–859},
numpages = {12},
keywords = {Online Social Networks, Link Previews, Chameleon Attack},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380166,
author = {Tian, Sheng and Xiong, Tao},
title = {A Generic Solver Combining Unsupervised Learning and Representation Learning for Breaking Text-Based Captchas},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380166},
doi = {10.1145/3366423.3380166},
abstract = {Although there are many alternative captcha schemes available, text-based captchas are still one of the most popular security mechanism to maintain Internet security and prevent malicious attacks, due to the user preferences and ease of design. Over the past decade, different methods of breaking captchas have been proposed, which helps captcha keep evolving and become more robust. However, these previous works generally require heavy expert involvement and gradually become ineffective with the introduction of new security features. This paper proposes a generic solver combining unsupervised learning and representation learning to automatically remove the noisy background of captchas and solve text-based captchas. We introduce a new training scheme for constructing mini-batches, which contain a large number of unlabeled hard examples, to improve the efficiency of representation learning. Unlike existing deep learning algorithms, our method requires significantly fewer labeled samples and surpasses the recognition performance of a fully-supervised model with the same network architecture. Moreover, extensive experiments show that the proposed method outperforms state-of-the-art by delivering a higher accuracy on various captcha schemes. We provide further discussions of potential applications of the proposed unified framework. We hope that our work can inspire the community to enhance the security of text-based captchas.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {860–871},
numpages = {12},
keywords = {Internet security, representation Learning, text-based captchas, unsupervised learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380167,
author = {Szpektor, Idan and Cohen, Deborah and Elidan, Gal and Fink, Michael and Hassidim, Avinatan and Keller, Orgad and Kulkarni, Sayali and Ofek, Eran and Pudinsky, Sagie and Revach, Asaf and Salant, Shimi and Matias, Yossi},
title = {Dynamic Composition for Conversational Domain Exploration},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380167},
doi = {10.1145/3366423.3380167},
abstract = {We study conversational domain exploration (CODEX), where the user’s goal is to enrich her knowledge of a given domain by conversing with an informative bot. Such conversations should be well grounded in high-quality domain knowledge as well as engaging and open-ended. A CODEX bot should be proactive and introduce relevant information even if not directly asked for by the user. The bot should also appropriately pivot the conversation to undiscovered regions of the domain. To address these dialogue characteristics, we introduce a novel approach termed dynamic composition that decouples candidate content generation from the flexible composition of bot responses. This allows the bot to control the source, correctness and quality of the offered content, while achieving flexibility via a dialogue manager that selects the most appropriate contents in a compositional manner. We implemented a CODEX bot based on dynamic composition and integrated it into the Google Assistant . As an example domain, the bot conversed about the NBA basketball league in a seamless experience, such that users were not aware whether they were conversing with the vanilla system or the one augmented with our CODEX bot. Results are positive and offer insights into what makes for a good conversation. To the best of our knowledge, this is the first real user experiment of open-ended dialogues as part of a commercial assistant system. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {872–883},
numpages = {12},
keywords = {Conversational system, open-ended dialogue, personal assistant},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380168,
author = {Tahir, Ammar and Munir, Muhammad Tahir and Malik, Shaiq Munir and Qazi, Zafar Ayyub and Qazi, Ihsan Ayyub},
title = {Deconstructing Google’s Web Light Service},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380168},
doi = {10.1145/3366423.3380168},
abstract = {Web Light is a transcoding service introduced by Google to show lighter and faster webpages to users searching on slow mobile clients. The service detects slow clients (e.g., users on 2G) and tries to convert webpages on the fly into a version optimized for these clients. Web Light claims to significantly reduce page load times, save user data, and substantially increase traffic to such webpages. However, there are several concerns around this service, including, its effectiveness in, preserving relevant content on a page, showing third-party advertisements, improving user performance as well as privacy concerns for users and publishers. In this paper, we perform the first independent, empirical analysis of Google’s Web Light service to shed light on these concerns. Through a combination of experiments with thousands of real Web Light pages as well as controlled experiments with synthetic Web Light pages, we (i) deconstruct how Web Light modifies webpages, (ii) investigate how ads are shown on Web Light and which ad networks are supported, (iii) measure and compare Web Light’s page load performance, (iv) discuss privacy concerns for users and publishers and (v) investigate the potential use of Web Light as a censorship circumvention tool.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {884–893},
numpages = {10},
keywords = {Web Light, circumvention, page load, transcoding, privacy, censorship},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380169,
author = {Narayanan, Arvind and Ramadan, Eman and Carpenter, Jason and Liu, Qingxu and Liu, Yu and Qian, Feng and Zhang, Zhi-Li},
title = {A First Look at Commercial 5G Performance on Smartphones},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380169},
doi = {10.1145/3366423.3380169},
abstract = {We conduct to our knowledge a first measurement study of commercial 5G performance on smartphones by closely examining 5G networks of three carriers (two mmWave carriers, one mid-band carrier) in three U.S. cities. We conduct extensive field tests on 5G performance in diverse urban environments. We systematically analyze the handoff mechanisms in 5G and their impact on network performance. We explore the feasibility of using location and possibly other environmental information to predict the network performance. We also study the app performance (web browsing and HTTP download) over 5G. Our study consumes more than 15 TB of cellular data. Conducted when 5G just made its debut, it provides a “baseline” for studying how 5G performance evolves, and identifies key research directions on improving 5G users’ experience in a cross-layer manner. We have released the data collected from our study (referred to as 5Gophers) at https://fivegophers.umn.edu/www20. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {894–905},
numpages = {12},
keywords = {Cellular Network Measurement., Millimeter Wave, 5G, 5Gophers, Cellular Performance},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380170,
author = {Wang, Qinyong and Yin, Hongzhi and Chen, Tong and Huang, Zi and Wang, Hao and Zhao, Yanchang and Viet Hung, Nguyen Quoc},
title = {Next Point-of-Interest Recommendation on Resource-Constrained Mobile Devices},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380170},
doi = {10.1145/3366423.3380170},
abstract = {In the modern tourism industry, next point-of-interest (POI) recommendation is an important mobile service as it effectively aids hesitating travelers to decide the next POI to visit. Currently, most next POI recommender systems are built upon a cloud-based paradigm, where the recommendation models are trained and deployed on the powerful cloud servers. When a recommendation request is made by a user via mobile devices, the current contextual information will be uploaded to the cloud servers to help the well-trained models generate personalized recommendation results. However, in reality, this paradigm heavily relies on high-quality network connectivity, and is subject to high energy footprint in the operation and increasing privacy concerns among the public. To bypass these defects, we propose a novel Light Location Recommender System (LLRec) to perform next POI recommendation locally on resource-constrained mobile devices. To make LLRec fully compatible with the limited computing resources and memory space, we leverage FastGRNN, a lightweight but effective gated Recurrent Neural Network (RNN) as its main building block, and significantly compress the model size by adopting the tensor-train composition in the embedding layer. As a compact model, LLRec maintains its robustness via an innovative teacher-student training framework, where a powerful teacher model is trained on the cloud to learn essential knowledge from available contextual data, and the simplified student model LLRec is trained under the guidance of the teacher model. The final LLRec is downloaded and deployed on users’ mobile devices to generate accurate recommendations solely utilizing users’ local data. As a result, LLRec significantly reduces the dependency on cloud servers, thus allowing for next POI recommendation in a stable, cost-effective and secure way. Extensive experiments on two large-scale recommendation datasets further demonstrate the superiority of our proposed solution. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {906–916},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380171,
author = {Li, Jia and Zhang, Honglei and Han, Zhichao and Rong, Yu and Cheng, Hong and Huang, Junzhou},
title = {Adversarial Attack on Community Detection by Hiding Individuals},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380171},
doi = {10.1145/3366423.3380171},
abstract = {It has been demonstrated that adversarial graphs, i.e., graphs with imperceptible perturbations added, can cause deep graph models to fail on node/graph classification tasks. In this paper, we extend adversarial graphs to the problem of community detection which is much more difficult. We focus on black-box attack and aim to hide targeted individuals from the detection of deep graph community detection models, which has many applications in real-world scenarios, for example, protecting personal privacy in social networks and understanding camouflage patterns in transaction networks. We propose an iterative learning framework that takes turns to update two modules: one working as the constrained graph generator and the other as the surrogate community detection model. We also find that the adversarial graphs generated by our method can be transferred to other learning based community detection models. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {917–927},
numpages = {11},
keywords = {graph generation, community detection, adversarial attack},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380172,
author = {Zhu, Yongchun and Xi, Dongbo and Song, Bowen and Zhuang, Fuzhen and Chen, Shuai and Gu, Xi and He, Qing},
title = {Modeling Users’ Behavior Sequences with Hierarchical Explainable Network for Cross-Domain Fraud Detection},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380172},
doi = {10.1145/3366423.3380172},
abstract = {With the explosive growth of the e-commerce industry, detecting online transaction fraud in real-world applications has become increasingly important to the development of e-commerce platforms. The sequential behavior history of users provides useful information in differentiating fraudulent payments from regular ones. Recently, some approaches have been proposed to solve this sequence-based fraud detection problem. However, these methods usually suffer from two problems: the prediction results are difficult to explain and the exploitation of the internal information of behaviors is insufficient. To tackle the above two problems, we propose a Hierarchical Explainable Network (HEN) to model users’ behavior sequences, which could not only improve the performance of fraud detection but also make the inference process interpretable. Meanwhile, as e-commerce business expands to new domains, e.g., new countries or new markets, one major problem for modeling user behavior in fraud detection systems is the limitation of data collection, e.g., very few data/labels available. Thus, in this paper, we further propose a transfer framework to tackle the cross-domain fraud detection problem, which aims to transfer knowledge from existing domains (source domains) with enough and mature data to improve the performance in the new domain (target domain). Our proposed method is a general transfer framework that could not only be applied upon HEN but also various existing models in the Embedding &amp; MLP paradigm. By utilizing data from a world-leading cross-border e-commerce platform, we conduct extensive experiments in detecting card-stolen transaction frauds in different countries to demonstrate the superior performance of HEN. Besides, based on 90 transfer task experiments, we also demonstrate that our transfer framework could not only contribute to the cross-domain fraud detection task with HEN, but also be universal and expandable for various existing models. Moreover, HEN and the transfer framework form three-level attention which greatly increases the explainability of the detection results.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {928–938},
numpages = {11},
keywords = {Fraud Detection, Transfer, Explainable, Hierarchical},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380173,
author = {Datta, Pubali and Kumar, Prabuddha and Morris, Tristan and Grace, Michael and Rahmati, Amir and Bates, Adam},
title = {Valve: Securing Function Workflows on Serverless Computing Platforms},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380173},
doi = {10.1145/3366423.3380173},
abstract = {Serverless Computing has quickly emerged as a dominant cloud computing paradigm, allowing developers to rapidly prototype event-driven applications using a composition of small functions that each perform a single logical task. However, many such application workflows are based in part on publicly-available functions developed by third-parties, creating the potential for functions to behave in unexpected, or even malicious, ways. At present, developers are not in total control of where and how their data is flowing, creating significant security and privacy risks in growth markets that have embraced serverless (e.g., IoT). As a practical means of addressing this problem, we present Valve, a serverless platform that enables developers to exert complete fine-grained control of information flows in their applications. Valve enables workflow developers to reason about function behaviors, and specify restrictions, through auditing of network-layer information flows. By proxying network requests and propagating taint labels across network flows, Valve is able to restrict function behavior without code modification. We demonstrate that Valve is able defend against known serverless attack behaviors including container reuse-based persistence and data exfiltration over cloud platform APIs with less than 2.8% runtime overhead, 6.25% deployment overhead and 2.35% teardown overhead.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {939–950},
numpages = {12},
keywords = {Serverless Computing, Security, Information Flow},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380174,
author = {Yu, Wenhao and Peng, Wei and Shu, Yu and Zeng, Qingkai and Jiang, Meng},
title = {Experimental Evidence Extraction System in Data Science with Hybrid Table Features and Ensemble Learning},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380174},
doi = {10.1145/3366423.3380174},
abstract = {Data Science has been one of the most popular fields in higher education and research activities. It takes tons of time to read the experimental section of thousands of papers and figure out the performance of the data science techniques. In this work, we build an experimental evidence extraction system to automate the integration of tables (in the paper PDFs) into a database of experimental results. First, it crops the tables and recognizes the templates. Second, it classifies the column names and row names into “method”, “dataset”, or “evaluation metric”, and then unified all the table cells into (method, dataset, metric, score)-quadruples. We propose hybrid features including structural and semantic table features as well as an ensemble learning approach for column/row name classification and table unification. SQL statements can be used to answer questions such as whether a method is the state-of-the-art or whether the reported numbers are conflicting.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {951–961},
numpages = {11},
keywords = {Data Science Education, PDF Tables, Information Extraction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380175,
author = {Yu, Wenhao and Yu, Mengxia and Zhao, Tong and Jiang, Meng},
title = {Identifying Referential Intention with Heterogeneous Contexts},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380175},
doi = {10.1145/3366423.3380175},
abstract = {Citing, quoting, and forwarding &amp; commenting behaviors are widely seen in academia, news media, and social media. Existing behavior modeling approaches focused on mining content and describing preferences of authors, speakers, and users. However, behavioral intention plays an important role in generating content on the platforms. In this work, we propose to identify the referential intention which motivates the action of using the referred (e.g., cited, quoted, and retweeted) source and content to support their claims. We adopt a theory in sociology to develop a schema of four types of intentions. The challenge lies in the heterogeneity of observed contextual information surrounding the referential behavior, such as referred content (e.g., a cited paper), local context (e.g., the sentence citing the paper), neighboring context (e.g., the former and latter sentences), and network context (e.g., the academic network of authors, affiliations, and keywords). We propose a new neural framework with Interactive Hierarchical Attention (IHA) to identify the intention of referential behavior by properly aggregating the heterogeneous contexts. Experiments demonstrate that the proposed method can effectively identify the type of intention of citing behaviors (on academic data) and retweeting behaviors (on Twitter). And learning the heterogeneous contexts collectively can improve the performance. This work opens a door for understanding content generation from a fundamental perspective of behavior sciences. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {962–972},
numpages = {11},
keywords = {Interactive Hierarchical Attention, Referential Intention, Heterogeneous Contexts},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380176,
author = {Zhang, Jiangwei and Tay, Y.C.},
title = {PG2S+: Stack Distance Construction Using Popularity, Gap and Machine Learning},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380176},
doi = {10.1145/3366423.3380176},
abstract = {Stack distance characterizes temporal locality of workloads and plays a vital role in cache analysis since the 1970s. However, exact stack distance calculation is too costly, and impractical for online use. Hence, much work was done to optimize the exact computation, or approximate it through sampling or modeling. This paper introduces a new approximation technique PG2S that is based on reference popularity and gap distance. This approximation is exact under the Independent Reference Model (IRM). The technique is further extended, using machine learning, to PG2S+ for non-IRM reference patterns. Extensive experiments show that PG2S+ is much more accurate and robust than other state-of-the-art algorithms for determining stack distance. PG2S+ is the first technique to exploit the strong correlation among reference popularity, gap distance and stack distance. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {973–983},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380177,
author = {Azzam, Amr and Fern\'{a}ndez, Javier D. and Acosta, Maribel and Beno, Martin and Polleres, Axel},
title = {SMART-KG: Hybrid Shipping for SPARQL Querying on the Web},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380177},
doi = {10.1145/3366423.3380177},
abstract = {While Linked Data (LD) provides standards for publishing (RDF) and (SPARQL) querying Knowledge Graphs (KGs) on the Web, serving, accessing and processing such open, decentralized KGs is often practically impossible, as query timeouts on publicly available SPARQL endpoints show. Alternative solutions such as Triple Pattern Fragments (TPF) attempt to tackle the problem of availability by pushing query processing workload to the client side, but suffer from unnecessary transfer of irrelevant data on complex queries with large intermediate results. In this paper we present smart-KG, a novel approach to share the load between servers and clients, while significantly reducing data transfer volume, by combining TPF with shipping compressed KG partitions. Our evaluations show that smart-KG outperforms state-of-the-art client-side solutions and increases server-side availability towards more cost-effective and balanced hosting of open and decentralized KGs. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {984–994},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380178,
author = {Wu, Xian and Cetintas, Suleyman and Kong, Deguang and Lu, Miao and Yang, Jian and Chawla, Nitesh},
title = {Learning from Cross-Modal Behavior Dynamics with Graph-Regularized Neural Contextual Bandit},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380178},
doi = {10.1145/3366423.3380178},
abstract = {Contextual multi-armed bandit algorithms have received significant attention in modeling users’ preferences for online personalized recommender systems in a timely manner. While significant progress has been made along this direction, a few major challenges have not been well addressed yet: (i) a vast majority of the literature is based on linear models that cannot capture complex non-linear inter-dependencies of user-item interactions; (ii) existing literature mainly ignores the latent relations among users and non-recommended items: hence may not properly reflect users’ preferences in the real-world; (iii) current solutions are mainly based on historical data and are prone to cold-start problems for new users who have no interaction history. To address the above challenges, we develop a Graph Regularized Cross-modal (GRC) learning model, a general framework to exploit transferable knowledge learned from user-item interactions as well as the external features of users and items in online personalized recommendations. In particular, the GRC framework leverage a non-linearity of neural network to model complex inherent structure of user-item interactions. We further augment GRC with the cooperation of the metric learning technique and a graph-constrained embedding module, to map the units from different dimensions (temporal, social and semantic) into the same latent space. An extensive set of experiments are conducted on two benchmark datasets as well as a large scale proprietary dataset from a major search engine demonstrates the power of the proposed GRC model in effectively capturing users’ dynamic preferences under different settings by outperforming all baselines by a large margin.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {995–1005},
numpages = {11},
keywords = {Contextual bandits, neural networks, online recommendation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380179,
author = {Shezan, Faysal Hossain and Hu, Hang and Wang, Jiamin and Wang, Gang and Tian, Yuan},
title = {Read Between the Lines: An Empirical Measurement of Sensitive Applications of Voice Personal Assistant Systems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380179},
doi = {10.1145/3366423.3380179},
abstract = {Voice Personal Assistant (VPA) systems such as Amazon Alexa and Google Home have been used by tens of millions of households. Recent work demonstrated proof-of-concept attacks against their voice interface to invoke unintended applications or operations. However, there is still a lack of empirical understanding of what type of third-party applications that VPA systems support, and what consequences these attacks may cause. In this paper, we perform an empirical analysis of the third-party applications of Amazon Alexa and Google Home to systematically assess the attack surfaces. A key methodology is to characterize a given application by classifying the sensitive voice commands it accepts. We develop a natural language processing tool that classifies a given voice command from two dimensions: (1) whether the voice command is designed to insert action or retrieve information; (2) whether the command is sensitive or nonsensitive. The tool combines a deep neural network and a keyword-based model, and uses Active Learning to reduce the manual labeling effort. The sensitivity classification is based on a user study (N=404) where we measure the perceived sensitivity of voice commands. A ground-truth evaluation shows that our tool achieves over 95% of accuracy for both types of classifications. We apply this tool to analyze 77,957 Amazon Alexa applications and 4,813 Google Home applications (198,199 voice commands from Amazon Alexa, 13,644 voice commands from Google Home) over two years (2018-2019). In total, we identify 19,263 sensitive “action injection” commands and 5,352 sensitive “information retrieval” commands. These commands are from 4,596 applications (5.55% out of all applications), most of which belong to the “smart home” category. While the percentage of sensitive applications is small, we show the percentage is increasing over time from 2018 to 2019. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1006–1017},
numpages = {12},
keywords = {Sensitive-keyword, Voice-applications, Skill, Malicious-command, Active-learning., Alexa, Sensitive-commands, Google-Home},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380180,
author = {Rosenfeld, Nir and Szanto, Aron and Parkes, David C.},
title = {A Kernel of Truth: Determining Rumor Veracity on Twitter by Diffusion Pattern Alone},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380180},
doi = {10.1145/3366423.3380180},
abstract = {Recent work in the domain of misinformation detection has leveraged rich signals in the text and user identities associated with content on social media. But text can be strategically manipulated and accounts reopened under different aliases, suggesting that these approaches are inherently brittle. In this work, we investigate an alternative modality that is naturally robust: the pattern in which information propagates. Can the veracity of an unverified rumor spreading online be discerned solely on the basis of its pattern of diffusion through the social network? Using graph kernels to extract complex topological information from Twitter cascade structures, we train accurate predictive models that are blind to language, user identities, and time, demonstrating for the first time that such “sanitized” diffusion patterns are highly informative of veracity. Our results indicate that, with proper aggregation, the collective sharing pattern of the crowd may reveal powerful signals of rumor truth or falsehood, even in the early stages of propagation. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1018–1028},
numpages = {11},
keywords = {Information propagation, Social networks, Social media, Rumors, Information diffusion, Graph kernels, Misinformation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380181,
author = {Zhang, Xingyao and Xiao, Cao and Glass, Lucas M. and Sun, Jimeng},
title = {DeepEnroll: Patient-Trial Matching with Deep Embedding and Entailment Prediction},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380181},
doi = {10.1145/3366423.3380181},
abstract = {Clinical trials are essential for drug development but often suffer from expensive, inaccurate and insufficient patient recruitment. The core problem of patient-trial matching is to find qualified patients for a trial, where patient information is stored in electronic health records (EHR) while trial eligibility criteria (EC) are described in text documents available on the web. How to represent longitudinal patient EHR? How to extract complex logical rules from EC? Most existing works rely on manual rule-based extraction, which is time consuming and inflexible for complex inference. To address these challenges, we proposed a cross-modal inference learning model to jointly encode enrollment criteria (text) and patients records (tabular data) into a shared latent space for matching inference. pplies a pre-trained Bidirectional Encoder Representations from Transformers(BERT) model to encode clinical trial information into sentence embedding. And uses a hierarchical embedding model to represent patient longitudinal EHR. In addition, s augmented by a numerical information embedding and entailment module to reason over numerical information in both EC and EHR. These encoders are trained jointly to optimize patient-trial matching score. We evaluated n the trial-patient matching task with demonstrated on real world datasets. utperformed the best baseline by up to 12.4% in average F1.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1029–1037},
numpages = {9},
keywords = {Trial Recruitment, Machine Learning, Entailment Prediction, Attention Mechanism},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380182,
author = {Javari, Amin and He, Zhankui and Huang, Zijie and Jeetu, Raj and Chen-Chuan Chang, Kevin},
title = {Weakly Supervised Attention for Hashtag Recommendation Using Graph Data},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380182},
doi = {10.1145/3366423.3380182},
abstract = {Personalized hashtag recommendation for users could substantially promote user engagement in microblogging websites; users can discover microblogs aligned with their interests. However, user profiling on microblogging websites is challenging because most users tend not to generate content. Our core idea is to build a graph-based profile of users and incorporate it into hashtag recommendation. Indeed, user’s followee/follower links implicitly indicate their interests. Considering that microblogging networks are scale-free networks, to maintain the efficiency and effectiveness of the model, rather than analyzing the entire network, we model users based on their links towards hub nodes. That is, hashtags and hub nodes are projected into a shared latent space. To predict the relevance of a user to a hashtag, a projection of the user is built by aggregating the embeddings of her hub neighbors guided by an attention model and then compared with the hashtag. Classically, attention models can be trained in an end to end manner. However, due to the high complexity of our problem, we propose a novel weak supervision model for the attention component, which significantly improves the effectiveness of the model. We performed extensive experiments on two datasets collected from Twitter and Weibo, and the results confirm that our method substantially outperforms the baselines. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1038–1048},
numpages = {11},
keywords = {Hashtag recommendation, Scale-free graph, Attention mechanism},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380183,
author = {Chan, Gromit Yeuk-Yin and Du, Fan and Rossi, Ryan A. and Rao, Anup B. and Koh, Eunyee and Silva, Cl\'{a}udio T. and Freire, Juliana},
title = {Real-Time Clustering for Large Sparse Online Visitor Data},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380183},
doi = {10.1145/3366423.3380183},
abstract = {Online visitor behaviors are often modeled as a large sparse matrix, where rows represent visitors and columns represent behavior. To discover customer segments with different hierarchies, marketers often need to cluster the data in different splits. Such analyses require the clustering algorithm to provide real-time responses on user parameter changes, which the current techniques cannot support. In this paper, we propose a real-time clustering algorithm, sparse density peaks, for large-scale sparse data. It pre-processes the input points to compute annotations and a hierarchy for cluster assignment. While the assignment is only a single scan of the points, a naive pre-processing requires measuring all pairwise distances, which incur a quadratic computation overhead and is infeasible for any moderately sized data. Thus, we propose a new approach based on MinHash and LSH that provides fast and accurate estimations. We also describe an efficient implementation on Spark that addresses data skew and memory usage. Our experiments show that our approach (1) provides a better approximation compared to a straightforward MinHash and LSH implementation in terms of accuracy on real datasets, (2) achieves a 20 \texttimes{} speedup in the end-to-end clustering pipeline, and (3) can maintain computations with a small memory. Finally, we present an interface to explore customer segments from millions of online visitor records in real-time. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1049–1059},
numpages = {11},
keywords = {Spark, Density peaks, Sketching, Sparse binary data, Clustering},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380184,
author = {Guerraoui, Rachid and Kermarrec, Anne-Marie and Ruas, Olivier and Ta\"{\i}ani, Fran\c{c}ois},
title = {Smaller, Faster &amp; Lighter KNN Graph Constructions},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380184},
doi = {10.1145/3366423.3380184},
abstract = {We propose GoldFinger, a new compact and fast-to-compute binary representation of datasets to approximate Jaccard’s index. We illustrate the effectiveness of GoldFinger on the emblematic big data problem of K-Nearest-Neighbor (KNN) graph construction and show that GoldFinger can drastically accelerate a large range of existing KNN algorithms with little to no overhead. As a side effect, we also show that the compact representation of the data protects users’ privacy for free by providing k-anonymity and l-diversity. Our extensive evaluation of the resulting approach on several realistic datasets shows that our approach delivers speedups of up to 78.9% compared to the use of raw data while only incurring a negligible to moderate loss in terms of KNN quality. To convey the practical value of such a scheme, we apply it to item recommendation and show that the loss in recommendation quality is negligible.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1060–1070},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380185,
author = {Scells, Harrisen and Zuccon, Guido and Koopman, Bevan and Clark, Justin},
title = {Automatic Boolean Query Formulation for Systematic Review Literature Search},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380185},
doi = {10.1145/3366423.3380185},
abstract = {Formulating Boolean queries for systematic review literature search is a challenging task. Commonly, queries are formulated by information specialists using the protocol specified in the review and interactions with the research team. Information specialists have in-depth experience on how to formulate queries in this domain, but may not have in-depth knowledge about the reviews’ topics. Query formulation requires a significant amount of time and effort, and is performed interactively; specialists repeatedly formulate queries, attempt to validate their results, and reformulate specific Boolean clauses. In this paper, we investigate the possibility of automatically formulating a Boolean query from the systematic review protocol. We propose a novel five-step approach to automatic query formulation, specific to Boolean queries in this domain, which approximates the process by which information specialists formulate queries. In this process, we use syntax parsing to derive the logical structure of high-level concepts in a query, automatically extract and map concepts to entities in order to perform entity expansion, and finally apply post-processing operations (such as stemming and search filters). Automatic query formulation for systematic review literature search has several benefits: (i) it can provide reviewers with an indication of the types of studies that will be retrieved, without the involvement of an information specialist, (ii) it can provide information specialists with an initial query to begin the formulation process, (iii) it can provide researchers that perform rapid reviews with a method to quickly perform searches. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1071–1081},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380186,
author = {Wang, Xiaoyang and Ma, Yao and Wang, Yiqi and Jin, Wei and Wang, Xin and Tang, Jiliang and Jia, Caiyan and Yu, Jian},
title = {Traffic Flow Prediction via Spatial Temporal Graph Neural Network},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380186},
doi = {10.1145/3366423.3380186},
abstract = {Traffic flow analysis, prediction and management are keystones for building smart cities in the new era. With the help of deep neural networks and big traffic data, we can better understand the latent patterns hidden in the complex transportation networks. The dynamic of the traffic flow on one road not only depends on the sequential patterns in the temporal dimension but also relies on other roads in the spatial dimension. Although there are existing works on predicting the future traffic flow, the majority of them have certain limitations on modeling spatial and temporal dependencies. In this paper, we propose a novel spatial temporal graph neural network for traffic flow prediction, which can comprehensively capture spatial and temporal patterns. In particular, the framework offers a learnable positional attention mechanism to effectively aggregate information from adjacent roads. Meanwhile, it provides a sequential component to model the traffic flow dynamics which can exploit both local and global temporal dependencies. Experimental results on various real traffic datasets demonstrate the effectiveness of the proposed framework.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1082–1092},
numpages = {11},
keywords = {Transformer, Dynamic, Graph Neural Networks, Traffic Prediction, Recurrent Neural Network, Spatial Temporal Model},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380187,
author = {Lian, Defu and Liu, Qi and Chen, Enhong},
title = {Personalized Ranking with Importance Sampling},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380187},
doi = {10.1145/3366423.3380187},
abstract = {As the task of predicting a personalized ranking on a set of items, item recommendation has become an important way to address information overload. Optimizing ranking loss aligns better with the ultimate goal of item recommendation, so many ranking-based methods were proposed for item recommendation, such as collaborative filtering with Bayesian Personalized Ranking (BPR) loss, and Weighted Approximate-Rank Pairwise (WARP) loss. However, the ranking-based methods can not consistently beat regression-based models with the gravity regularizer. The key challenge in ranking-based optimization is difficult to fully use the limited number of negative samples, particularly when they are not so informative. To this end, we propose a new ranking loss based on importance sampling so that more informative negative samples can be better used. We then design a series of negative samplers from simple to complex, whose informativeness of negative samples is from less to more. With these samplers, the loss function is easy to use and can be optimized by popular solvers. The proposed algorithms are evaluated with five real-world datasets of varying size and difficulty. The results show that they consistently outperform the state-of-the-art item recommendation algorithms, and the relative improvements with respect to NDCG@50 are more than 19.2% on average. Moreover, the loss function is verified to make better use of negative samples and to require fewer negative samples when they are more informative.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1093–1103},
numpages = {11},
keywords = {Implicit Feedback, Personalized Ranking, Negative Sampling},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380188,
author = {Liu, Yu and Yao, Quanming and Li, Yong},
title = {Generalizing Tensor Decomposition for N-Ary Relational Knowledge Bases},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380188},
doi = {10.1145/3366423.3380188},
abstract = {With the rapid development of knowledge bases (KBs), link prediction task, which completes KBs with missing facts, has been broadly studied in especially binary relational KBs (a.k.a knowledge graph) with powerful tensor decomposition related methods. However, the ubiquitous n-ary relational KBs with higher-arity relational facts are paid less attention, in which existing translation based and neural network based approaches have weak expressiveness and high complexity in modeling various relations. Tensor decomposition has not been considered for n-ary relational KBs, while directly extending tensor decomposition related methods of binary relational KBs to the n-ary case does not yield satisfactory results due to exponential model complexity and their strong assumptions on binary relations. To generalize tensor decomposition for n-ary relational KBs, in this work, we propose GETD, a generalized model based on Tucker decomposition and Tensor Ring decomposition. The existing negative sampling technique is also generalized to the n-ary case for GETD. In addition, we theoretically prove that GETD is fully expressive to completely represent any KBs. Extensive evaluations on two representative n-ary relational KB datasets demonstrate the superior performance of GETD, significantly improving the state-of-the-art methods by over 15%. Moreover, GETD further obtains the state-of-the-art results on the benchmark binary relational KB datasets.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1104–1114},
numpages = {11},
keywords = {N-ary Relation, Link Prediction, Knowledge Base, Tucker Decomposition, Tensor Ring Decomposition},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380189,
author = {Belth, Caleb and Zheng, Xinyi and Vreeken, Jilles and Koutra, Danai},
title = {What is Normal, What is Strange, and What is Missing in a Knowledge Graph: Unified Characterization via Inductive Summarization},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380189},
doi = {10.1145/3366423.3380189},
abstract = {Knowledge graphs (KGs) store highly heterogeneous information about the world in the structure of a graph, and are useful for tasks such as question answering and reasoning. However, they often contain errors and are missing information. Vibrant research in KG refinement has worked to resolve these issues, tailoring techniques to either detect specific types of errors or complete a KG. In this work, we introduce a unified solution to KG characterization by formulating the problem as unsupervised KG summarization with a set of inductive, soft rules, which describe what is normal in a KG, and thus can be used to identify what is abnormal, whether it be strange or missing. Unlike first-order logic rules, our rules are labeled, rooted graphs, i.e., patterns that describe the expected neighborhood around a (seen or unseen) node, based on its type, and information in the KG. Stepping away from the traditional support/confidence-based rule mining techniques, we propose KGist, Knowledge Graph Inductive SummarizaTion, which learns a summary of inductive rules that best compress the KG according to the Minimum Description Length principle—a formulation that we are the first to use in the context of KG rule mining. We apply our rules to three large KGs (NELL, DBpedia, and Yago), and tasks such as compression, various types of error detection, and identification of incomplete information. We show that KGist outperforms task-specific, supervised and unsupervised baselines in error detection and incompleteness identification, (identifying the location of up to 93% of missing entities—over 10% more than baselines), while also being efficient for large knowledge graphs.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1115–1126},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380190,
author = {Guo, Xueliang and Shi, Chongyang and Liu, Chuanming},
title = {Intention Modeling from Ordered and Unordered Facets for Sequential Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380190},
doi = {10.1145/3366423.3380190},
abstract = {Recently, sequential recommendation has attracted substantial attention from researchers due to its status as an essential service for e-commerce. Accurately understanding user intention is an important factor to improve the performance of recommendation system. However, user intention is highly time-dependent and flexible, so it is very challenging to learn the latent dynamic intention of users for sequential recommendation. To this end, in this paper, we propose a novel intention modeling from ordered and unordered facets (IMfOU) for sequential recommendation. Specifically, the global and local item embedding (GLIE) we proposed can comprehensively capture the sequential context information in the sequences and highlight the important features that users care about. We further design ordered preference drift learning (OPDL) and unordered purchase motivation learning (UPML) to obtain user’s the process of preference drift and purchase motivation respectively. With combining the users’ dynamic preference and current motivation, it considers not only sequential dependencies between items but also flexible dependencies and models the user purchase intention more accurately from ordered and unordered facets respectively. Evaluation results on three real-world datasets demonstrate that our proposed approach achieves better performance than the state-of-the-art sequential recommendation methods achieving improvement of AUC by an average of 2.26%.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1127–1137},
numpages = {11},
keywords = {sequential recommendation, purchased motivation, user intention, preference drift},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380191,
author = {Gao, Shen and Chen, Xiuying and Liu, Chang and Liu, Li and Zhao, Dongyan and Yan, Rui},
title = {Learning to Respond with Stickers: A Framework of Unifying Multi-Modality in Multi-Turn Dialog},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380191},
doi = {10.1145/3366423.3380191},
abstract = {Stickers with vivid and engaging expressions are becoming increasingly popular in online messaging apps, and some works are dedicated to automatically select sticker response by matching text labels of stickers with previous utterances. However, due to their large quantities, it is impractical to require text labels for the all stickers. Hence, in this paper, we propose to recommend an appropriate sticker to user based on multi-turn dialog context history without any external labels. Two main challenges are confronted in this task. One is to learn semantic meaning of stickers without corresponding text labels. Another challenge is to jointly model the candidate sticker with the multi-turn dialog context. To tackle these challenges, we propose a sticker response selector (SRS) model. Specifically, SRS first employs a convolutional based sticker image encoder and a self-attention based multi-turn dialog encoder to obtain the representation of stickers and utterances. Next, deep interaction network is proposed to conduct deep matching between the sticker with each utterance in the dialog history. SRS then learns the short-term and long-term dependency between all interaction results by a fusion network to output the the final matching score. To evaluate our proposed method, we collect a large-scale real-world dialog dataset with stickers from one of the most popular online chatting platform. Extensive experiments conducted on this dataset show that our model achieves the state-of-the-art performance for all commonly-used metrics. Experiments also verify the effectiveness of each component of SRS. To facilitate further research in sticker selection field, we release this dataset of 340K multi-turn dialog and sticker pairs1.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1138–1148},
numpages = {11},
keywords = {online chatting, multi-turn dialog, sticker selection},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380192,
author = {Wu, Junshuang and Zhang, Richong and Mao, Yongyi and Guo, Hongyu and Soflaei, Masoumeh and Huai, Jinpeng},
title = {Dynamic Graph Convolutional Networks for Entity Linking},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380192},
doi = {10.1145/3366423.3380192},
abstract = {Entity linking, which maps named entity mentions in a document into the proper entities in a given knowledge graph, has been shown to be able to significantly benefit from modeling the entity relatedness through Graph Convolutional Networks (GCN). Nevertheless, existing GCN entity linking models fail to take into account the fact that the structured graph for a set of entities not only depends on the contextual information of the given document but also adaptively changes on different aggregation layers of the GCN, resulting in insufficiency in terms of capturing the structural information among entities. In this paper, we propose a dynamic GCN architecture to effectively cope with this challenge. The graph structure in our model is dynamically computed and modified during training. Through aggregating knowledge from dynamically linked nodes, our GCN model can collectively identify the entity mappings between the document and the knowledge graph, and efficiently capture the topical coherence among various entity mentions in the entire document. Empirical studies on benchmark entity linking data sets confirm the superior performance of our proposed strategy and the benefits of the dynamic graph structure.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1149–1159},
numpages = {11},
keywords = {Entity linking, graph attention networks, dynamic graph convolutional networks, graph convolutional networks},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380193,
author = {Rosset, Corbin and Xiong, Chenyan and Song, Xia and Campos, Daniel and Craswell, Nick and Tiwary, Saurabh and Bennett, Paul},
title = {Leading Conversational Search by Suggesting Useful Questions},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380193},
doi = {10.1145/3366423.3380193},
abstract = {This paper studies a new scenario in conversational search, conversational question suggestion, which leads search engine users to more engaging experiences by suggesting interesting, informative, and useful follow-up questions. We first establish a novel evaluation metric, usefulness, which goes beyond relevance and measures whether the suggestions provide valuable information for the next step of a user’s journey, and construct a public benchmark for useful question suggestion. Then we develop two suggestion systems, a BERT based ranker and a GPT-2 based generator, both trained with novel weak supervision signals that convey past users’ search behaviors in search sessions. The weak supervision signals help ground the suggestions to users’ information-seeking trajectories: we identify more coherent and informative sessions using encodings, and then weakly supervise our models to imitate how users transition to the next state of search. Our offline experiments demonstrate the crucial role our “next-turn” inductive training plays in improving usefulness over a strong online system. Our online A/B test in Bing shows that our more useful question suggestions receive 8% more user clicks than the previous system.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1160–1170},
numpages = {11},
keywords = {Question Suggestion, Conversational Search, Usefulness},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380194,
author = {Warrior, Marc Anthony and Xiao, Yunming and Varvello, Matteo and Kuzmanovic, Aleksandar},
title = {De-Kodi: Understanding the Kodi Ecosystem},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380194},
doi = {10.1145/3366423.3380194},
abstract = {Free and open source media centers are currently experiencing a boom in popularity for the convenience and flexibility they offer users seeking to remotely consume digital content. This newfound fame is matched by increasing notoriety—for their potential to serve as hubs for illegal content—and a presumably ever-increasing network footprint. It is fair to say that a complex ecosystem has developed around Kodi, composed of millions of users, thousands of “add-ons”—Kodi extensions from 3rd-party developers—and content providers. Motivated by these observations, this paper conducts the first analysis of the Kodi ecosystem. Our approach is to build “crawling” software around Kodi which can automatically install an addon, explore its menu, and locate (video) content. This is challenging for many reasons. First, Kodi largely relies on visual information and user input which intrinsically complicates automation. Second, no central aggregators for Kodi addons exist. Third, the potential sheer size of this ecosystem requires a highly scalable crawling solution. We address these challenges with de-Kodi, a full fledged crawling system capable of discovering and crawling large cross-sections of Kodi’s decentralized ecosystem. With de-Kodi, we discovered and tested over 9,000 distinct Kodi addons. Our results demonstrate de-Kodi, which we make available to the general public, to be an essential asset in studying one of the largest multimedia platforms in the world. Our work further serves as the first ever transparent and repeatable analysis of the Kodi ecosystem at large.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1171–1181},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380195,
author = {Pei, Weiping and Mayer, Arthur and Tu, Kaylynn and Yue, Chuan},
title = {Attention Please: Your Attention Check Questions in Survey Studies Can Be Automatically Answered},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380195},
doi = {10.1145/3366423.3380195},
abstract = {Attention check questions have become commonly used in online surveys published on popular crowdsourcing platforms as a key mechanism to filter out inattentive respondents and improve data quality. However, little research considers the vulnerabilities of this important quality control mechanism that can allow attackers including irresponsible and malicious respondents to automatically answer attention check questions for efficiently achieving their goals. In this paper, we perform the first study to investigate such vulnerabilities, and demonstrate that attackers can leverage deep learning techniques to pass attention check questions automatically. We propose AC-EasyPass, an attack framework with a concrete model, that combines convolutional neural network and weighted feature reconstruction to easily pass attention check questions. We construct the first attention check question dataset that consists of both original and augmented questions, and demonstrate the effectiveness of AC-EasyPass. We explore two simple defense methods, adding adversarial sentences and adding typos, for survey designers to mitigate the risks posed by AC-EasyPass; however, these methods are fragile due to their limitations from both technical and usability perspectives, underlining the challenging nature of defense. We hope our work will raise sufficient attention of the research community towards developing more robust attention check mechanisms. More broadly, our work intends to prompt the research community to seriously consider the emerging risks posed by the malicious use of machine learning techniques to the quality, validity, and trustworthiness of crowdsourcing and social computing. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1182–1193},
numpages = {12},
keywords = {Crowdsourcing, Deep Learning, Online Survey, Automatic Answer Generation, Attention Check},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380196,
author = {Patro, Gourab K and Biswas, Arpita and Ganguly, Niloy and Gummadi, Krishna P. and Chakraborty, Abhijnan},
title = {FairRec: Two-Sided Fairness for Personalized Recommendations in Two-Sided Platforms},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380196},
doi = {10.1145/3366423.3380196},
abstract = {We investigate the problem of fair recommendation in the context of two-sided online platforms, comprising customers on one side and producers on the other. Traditionally, recommendation services in these platforms have focused on maximizing customer satisfaction by tailoring the results according to the personalized preferences of individual customers. However, our investigation reveals that such customer-centric design may lead to unfair distribution of exposure among the producers, which may adversely impact their well-being. On the other hand, a producer-centric design might become unfair to the customers. Thus, we consider fairness issues that span both customers and producers. Our approach involves a novel mapping of the fair recommendation problem to a constrained version of the problem of fairly allocating indivisible goods. Our proposed FairRec algorithm guarantees at least Maximin Share (MMS) of exposure for most of the producers and Envy-Free up to One Good (EF1) fairness for every customer. Extensive evaluations over multiple real-world datasets show the effectiveness of FairRec in ensuring two-sided fairness while incurring a marginal loss in the overall recommendation quality.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1194–1204},
numpages = {11},
keywords = {Maximin Share, Envy-Freeness, Fair Recommendation, Fair Allocation, Two-Sided Markets},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380197,
author = {Zhao, Chen and Xiong, Chenyan and Qian, Xin and Boyd-Graber, Jordan},
title = {Complex Factoid Question Answering with a Free-Text Knowledge Graph},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380197},
doi = {10.1145/3366423.3380197},
abstract = {We introduce delft, a factoid question answering system which combines the nuance and depth of knowledge graph question answering approaches with the broader coverage of free-text. delft builds a free-text knowledge graph from Wikipedia, with entities as nodes and sentences in which entities co-occur as edges. For each question, delft finds the subgraph linking question entity nodes to candidates using text sentences as edges, creating a dense and high coverage semantic graph. A novel graph neural network reasons over the free-text graph—combining evidence on the nodes via information along edge sentences—to select a final answer. Experiments on three question answering datasets show delft can answer entity-rich questions better than machine reading based models, bert-based answer ranking and memory networks. delft’s advantage comes from both the high coverage of its free-text knowledge graph—more than double that of dbpedia relations—and the novel graph neural network which reasons on the rich but noisy free-text evidence. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1205–1216},
numpages = {12},
keywords = {Graph Neural Network, Factoid Question Answering, Free-Text Knowledge Graph},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380198,
author = {Biddle, Rhys and Joshi, Aditya and Liu, Shaowu and Paris, Cecile and Xu, Guandong},
title = {Leveraging Sentiment Distributions to Distinguish Figurative From Literal Health Reports on Twitter},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380198},
doi = {10.1145/3366423.3380198},
abstract = {Harnessing data from social media to monitor health events is a promising avenue for public health surveillance. A key step is the detection of reports of a disease (referred to as ‘health mention classification’) amongst tweets that mention disease words. Prior work shows that figurative usage of disease words may prove to be challenging for health mention classification. Since the experience of a disease is associated with a negative sentiment, we present a method that utilises sentiment information to improve health mention classification. Specifically, our classifier for health mention classification combines pre-trained contextual word representations with sentiment distributions of words in the tweet. For our experiments, we extend a benchmark dataset of tweets for health mention classification, adding over 14k manually annotated tweets across diseases. We also additionally annotate each tweet with a label that indicates if the disease words are used in a figurative sense. Our classifier outperforms current SOTA approaches in detecting both health-related and figurative tweets that mention disease words. We also show that tweets containing disease words are mentioned figuratively more often than in a health-related context, proving to be challenging for classifiers targeting health-related tweets.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1217–1227},
numpages = {11},
keywords = {Twitter, Figurative Language, Health Mention Classification, Public Health Surveillance},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380199,
author = {Toxtli, Carlos and Richmond-Fuller, Angela and Savage, Saiph},
title = {Reputation Agent: Prompting Fair Reviews in Gig Markets},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380199},
doi = {10.1145/3366423.3380199},
abstract = {Our study presents a new tool, Reputation Agent, to promote fairer reviews from requesters (employers or customers) on gig markets. Unfair reviews, created when requesters consider factors outside of a worker’s control, are known to plague gig workers and can result in lost job opportunities and even termination from the marketplace. Our tool leverages machine learning to implement an intelligent interface that: (1) uses deep learning to automatically detect when an individual has included unfair factors into her review (factors outside the worker’s control per the policies of the market); and (2) prompts the individual to reconsider her review if she has incorporated unfair factors. To study the effectiveness of Reputation Agent, we conducted a controlled experiment over different gig markets. Our experiment illustrates that across markets, Reputation Agent, in contrast with traditional approaches, motivates requesters to review gig workers’ performance more fairly. We discuss how tools that bring more transparency to employers about the policies of a gig market can help build empathy thus resulting in reasoned discussions around potential injustices towards workers generated by these interfaces. Our vision is that with tools that promote truth and transparency we can bring fairer treatment to gig workers. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1228–1240},
numpages = {13},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380200,
author = {Savage, Saiph and Chiang, Chun Wei and Saito, Susumu and Toxtli, Carlos and Bigham, Jeffrey},
title = {Becoming the Super Turker:Increasing Wages via a Strategy from High Earning Workers},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380200},
doi = {10.1145/3366423.3380200},
abstract = {Crowd markets have traditionally limited workers by not providing transparency information concerning which tasks pay fairly or which requesters are unreliable. Researchers believe that a key reason why crowd workers earn low wages is due to this lack of transparency. As a result, tools have been developed to provide more transparency within crowd markets to help workers. However, while most workers use these tools, they still earn less than minimum wage. We argue that the missing element is guidance on how to use transparency information. In this paper, we explore how novice workers can improve their earnings by following the transparency criteria of Super Turkers, i.e., crowd workers who earn higher salaries on Amazon Mechanical Turk (MTurk). We believe that Super Turkers have developed effective processes for using transparency information. Therefore, by having novices follow a Super Turker criteria (one that is simple and popular among Super Turkers), we can help novices increase their wages. For this purpose, we: (i) conducted a survey and data analysis to computationally identify a simple yet common criteria that Super Turkers use for handling transparency tools; (ii) deployed a two-week field experiment with novices who followed this Super Turker criteria to find better work on MTurk. Novices in our study viewed over 25,000 tasks by 1,394 requesters. We found that novices who utilized this Super Turkers’ criteria earned better wages than other novices. Our results highlight that tool development to support crowd workers should be paired with educational opportunities that teach workers how to effectively use the tools and their related metrics (e.g., transparency values). We finish with design recommendations for empowering crowd workers to earn higher salaries.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1241–1252},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380201,
author = {Goyal, Nikhil and Jain, Harsh Vardhan and Ranu, Sayan},
title = {GraphGen: A Scalable Approach to Domain-Agnostic Labeled Graph Generation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380201},
doi = {10.1145/3366423.3380201},
abstract = {Graph generative models have been extensively studied in the data mining literature. While traditional techniques are based on generating structures that adhere to a pre-decided distribution, recent techniques have shifted towards learning this distribution directly from the data. While learning-based approaches have imparted significant improvement in quality, some limitations remain to be addressed. First, learning graph distributions introduces additional computational overhead, which limits their scalability to large graph databases. Second, many techniques only learn the structure and do not address the need to also learn node and edge labels, which encode important semantic information and influence the structure itself. Third, existing techniques often incorporate domain-specific rules and lack generalizability. Fourth, the experimentation of existing techniques is not comprehensive enough due to either using weak evaluation metrics or focusing primarily on synthetic or small datasets. In this work, we develop a domain-agnostic technique called GraphGen to overcome all of these limitations. GraphGen converts graphs to sequences using minimum DFS codes. Minimum DFS codes are canonical labels and capture the graph structure precisely along with the label information. The complex joint distributions between structure and semantic labels are learned through a novel LSTM architecture. Extensive experiments on million-sized, real graph datasets show GraphGen to be 4 times faster on average than state-of-the-art techniques while being significantly better in quality across a comprehensive set of 11 different metrics. Our code is released at: https://github.com/idea-iitd/graphgen.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1253–1263},
numpages = {11},
keywords = {neural networks, graph generative model, canonical labels},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380202,
author = {Yu, Fuqiang and Cui, Lizhen and Guo, Wei and Lu, Xudong and Li, Qingzhong and Lu, Hua},
title = {A Category-Aware Deep Model for Successive POI Recommendation on Sparse Check-in Data},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380202},
doi = {10.1145/3366423.3380202},
abstract = {As considerable amounts of POI check-in data have been accumulated, successive point-of-interest (POI) recommendation is increasingly popular. Existing successive POI recommendation methods only predict where user will go next, ignoring when this behavior will occur. In this work, we focus on predicting POIs that will be visited by users in the next 24 hours. As check-in data is very sparse, it is challenging to accurately capture user preferences in temporal patterns. To this end, we propose a category-aware deep model CatDM that incorporates POI category and geographical influence to reduce search space to overcome data sparsity. We design two deep encoders based on LSTM to model the time series data. The first encoder captures user preferences in POI categories, whereas the second exploits user preferences in POIs. Considering clock influence in the second encoder, we divide each user’s check-in history into several different time windows and develop a personalized attention mechanism for each window to facilitate CatDM to exploit temporal patterns. Moreover, to sort the candidate set, we consider four specific dependencies: user-POI, user-category, POI-time and POI-user current preferences. Extensive experiments are conducted on two large real datasets. The experimental results demonstrate that our CatDM outperforms the state-of-the-art models for successive POI recommendation on sparse check-in data.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1264–1274},
numpages = {11},
keywords = {POI recommendation, deep model, category-aware, sparse data},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380203,
author = {Urban, Tobias and Degeling, Martin and Holz, Thorsten and Pohlmann, Norbert},
title = {Beyond the Front Page:Measuring Third Party Dynamics in the Field},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380203},
doi = {10.1145/3366423.3380203},
abstract = {In the modern Web, service providers often rely heavily on third parties to run their services. For example, they make use of ad networks to finance their services, externally hosted libraries to develop features quickly, and analytics providers to gain insights into visitor behavior. For security and privacy, website owners need to be aware of the content they provide their users. However, in reality, they often do not know which third parties are embedded, for example, when these third parties request additional content as it is common in real-time ad auctions. In this paper, we present a large-scale measurement study to analyze the magnitude of these new challenges. To better reflect the connectedness of third parties, we measured their relations in a model we call third party trees, which reflects an approximation of the loading dependencies of all third parties embedded into a given website. Using this concept, we show that including a single third party can lead to subsequent requests from up to eight additional services. Furthermore, our findings indicate that the third parties embedded on a page load are not always deterministic, as 50&nbsp;% of the branches in the third party trees change between repeated visits. In addition, we found that 93&nbsp;% of the analyzed websites embedded third parties that are located in regions that might not be in line with the current legal framework. Our study also replicates previous work that mostly focused on landing pages of websites. We show that this method is only able to measure a lower bound as subsites show a significant increase of privacy-invasive techniques. For example, our results show an increase of used cookies by about 36&nbsp;% when crawling websites more deeply.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1275–1286},
numpages = {12},
keywords = {third parties, privacy, cookies, web measurement},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380204,
author = {Breuer, Adam and Eilat, Roee and Weinsberg, Udi},
title = {Friend or Faux: Graph-Based Early Detection of Fake Accounts on Social Networks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380204},
doi = {10.1145/3366423.3380204},
abstract = {In this paper, we study the problem of early detection of fake user accounts on social networks based solely on their network connectivity with other users. Removing such accounts is a core task for maintaining the integrity of social networks, and early detection helps to reduce the harm that such accounts inflict. However, new fake accounts are notoriously difficult to detect via graph-based algorithms, as their small number of connections are unlikely to reflect a significant structural difference from those of new real accounts. We present the SybilEdge algorithm, which determines whether a new user is a fake account (‘sybil’) by aggregating over (I) her choices of friend request targets and (II) these targets’ respective responses. SybilEdge performs this aggregation giving more weight to a user’s choices of targets to the extent that these targets are preferred by other fakes versus real users, and also to the extent that these targets respond differently to fakes versus real users. We show that SybilEdge rapidly detects new fake users at scale on the Facebook network and outperforms state-of-the-art algorithms. We also show that SybilEdge is robust to label noise in the training data, to different prevalences of fake accounts in the network, and to several different ways fakes can select targets for their friend requests. To our knowledge, this is the first time a graph-based algorithm has been shown to achieve high performance (AUC &gt; 0.9) on new users who have only sent a small number of friend requests. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1287–1297},
numpages = {11},
keywords = {security, sybil detection., crowdsourcing and human computation, and trust, privacy, Social network analysis and graph algorithms},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380205,
author = {Zhang, Shuo and Meij, Edgar and Balog, Krisztian and Reinanda, Ridho},
title = {Novel Entity Discovery from Web Tables},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380205},
doi = {10.1145/3366423.3380205},
abstract = {When working with any sort of knowledge base (KB) one has to make sure it is as complete and also as up-to-date as possible. Both tasks are non-trivial as they require recall-oriented efforts to determine which entities and relationships are missing from the KB. As such they require a significant amount of labor. Tables on the Web on the other hand are abundant and have the distinct potential to assist with these tasks. In particular, we can leverage the content in such tables to discover new entities, properties, and relationships. Because web tables typically only contain raw textual content we first need to determine which cells refer to which known entities—a task we dub table-to-KB matching. This first task aims to infer table semantics by linking table cells and heading columns to elements of a KB. We propose a feature-based method and on two public test collections we demonstrate substantial improvements over the state-of-the-art in terms of precision whilst also improving recall. Then second task builds upon these linked entities and properties to not only identify novel ones in the same table but also to bootstrap their type and additional relationships. We refer to this process as novel entity discovery and, to the best of our knowledge, it is the first endeavor on mining the unlinked cells in web tables. Our method identifies not only out-of-KB (“novel”) information but also novel aliases for in-KB (“known”) entities. When evaluated using three purpose-built test collections, we find that our proposed approaches obtain a marked improvement in terms of precision over our baselines whilst keeping recall stable.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1298–1308},
numpages = {11},
keywords = {Novel entity discovery, tabular data extraction, KBP, entity linking},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380206,
author = {Chen, Wei-Fan and Syed, Shahbaz and Stein, Benno and Hagen, Matthias and Potthast, Martin},
title = {Abstractive Snippet Generation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380206},
doi = {10.1145/3366423.3380206},
abstract = {An abstractive snippet is an originally created piece of text to summarize a web page on a search engine results page. Compared to the conventional extractive snippets, which are generated by extracting phrases and sentences verbatim from a web page, abstractive snippets circumvent copyright issues; even more interesting is the fact that they open the door for personalization. Abstractive snippets have been evaluated as equally powerful in terms of user acceptance and expressiveness—but the key question remains: Can abstractive snippets be automatically generated with sufficient quality? This paper introduces a new approach to abstractive snippet generation: We identify the first two large-scale sources for distant supervision, namely anchor contexts and web directories. By mining the entire ClueWeb09 and ClueWeb12 for anchor contexts and by utilizing the DMOZ Open Directory Project, we compile the Webis Abstractive Snippet Corpus&nbsp;2020, comprising more than 3.5&nbsp;million triples of the form ⟨query, snippet, document⟩ as training examples, where the snippet is either an anchor context or a web directory description in lieu of a genuine query-biased abstractive snippet of the web document. We propose a bidirectional abstractive snippet generation model and assess the quality of both our corpus and the generated abstractive snippets with standard measures, crowdsourcing, and in comparison to the state of the art. The evaluation shows that our novel data sources along with the proposed model allow for producing usable query-biased abstractive snippets while minimizing text reuse. Code, data, and slides: https://webis.de/publications.html#?q=WWW+2020 },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1309–1319},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380207,
author = {Eriksson, Benjamin and Sabelfeld, Andrei},
title = {AutoNav: Evaluation and Automatization of Web Navigation Policies},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380207},
doi = {10.1145/3366423.3380207},
abstract = {Undesired navigation in browsers powers a significant class of attacks on web applications. In a move to mitigate risks associated with undesired navigation, the security community has proposed a standard that gives control to web pages to restrict navigation. The standard draft introduces a new navigate-to directive of the Content Security Policy (CSP). The directive is currently being implemented by mainstream browsers. This paper is a first evaluation of navigate-to, focusing on security, performance, and automatization of navigation policies. We present new vulnerabilities introduced by the directive into the web ecosystem, opening up for attacks such as probing to detect if users are logged in to other websites or have active shopping carts, bypassing third-party cookie blocking, exfiltrating secrets, as well as leaking browsing history. Unfortunately, the directive triggers vulnerabilities even in websites that do not use the directive in their policies. We identify both specification- and implementation-level vulnerabilities and propose countermeasures to mitigate both. To aid developers in configuring navigation policies, we develop and implement AutoNav1, an automated black-box mechanism to infer navigation policies. AutoNav leverages the benefits of origin-wide policies in order to improve security without degrading performance. We evaluate the viability of navigate-to and AutoNav by an empirical study on Alexa’s top 10,000 websites. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1320–1331},
numpages = {12},
keywords = {web navigations, web application security, csp},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380208,
author = {Arnold, Sebastian and van Aken, Betty and Grundmann, Paul and Gers, Felix A. and L\"{o}ser, Alexander},
title = {Learning Contextualized Document Representations for Healthcare Answer Retrieval},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380208},
doi = {10.1145/3366423.3380208},
abstract = {We present Contextual Discourse Vectors (CDV), a distributed document representation for efficient answer retrieval from long healthcare documents. Our approach is based on structured query tuples of entities and aspects from free text and medical taxonomies. Our model leverages a dual encoder architecture with hierarchical LSTM layers and multi-task training to encode the position of clinical entities and aspects alongside the document discourse. We use our continuous representations to resolve queries with short latency using approximate nearest neighbor search on sentence level. We apply the CDV model for retrieving coherent answer passages from nine English public health resources from the Web, addressing both patients and medical professionals. Because there is no end-to-end training data available for all application scenarios, we train our model with self-supervised data from Wikipedia. We show that our generalized model significantly outperforms several state-of-the-art baselines for healthcare passage ranking and is able to adapt to heterogeneous domains without additional fine-tuning.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1332–1343},
numpages = {12},
keywords = {passage ranking, discourse vectors, document representation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380209,
author = {Aydin, Roland and Klein, Lars and Miribel, Arnaud and West, Robert},
title = {Broccoli: Sprinkling Lightweight Vocabulary Learning into Everyday Information Diets},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380209},
doi = {10.1145/3366423.3380209},
abstract = {The learning of a new language remains to this date a cognitive task that requires considerable diligence and willpower, recent advances and tools notwithstanding. In this paper, we propose Broccoli, a new paradigm aimed at reducing the required effort by seamlessly embedding vocabulary learning into users’ everyday information diets. This is achieved by inconspicuously switching chosen words encountered by the user for their translation in the target language. Thus, by seeing words in context, the user can assimilate new vocabulary without much conscious effort. We validate our approach in a careful user study, finding that the efficacy of the lightweight Broccoli approach is competitive with traditional, memorization-based vocabulary learning. The low cognitive overhead is manifested in a pronounced decrease in learners’ usage of mnemonic learning strategies, as compared to traditional learning. Finally, we establish that language patterns in typical information diets are compatible with spaced-repetition strategies, thus enabling an efficient use of the Broccoli paradigm. Overall, our work establishes the feasibility of a novel and powerful “install-and-forget” approach for embedded language acquisition.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1344–1354},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380210,
author = {He, Tianfu and Bao, Jie and Li, Ruiyuan and Ruan, Sijie and Li, Yanhua and Song, Li and He, Hui and Zheng, Yu},
title = {What is the Human Mobility in a New City: Transfer Mobility Knowledge Across Cities},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380210},
doi = {10.1145/3366423.3380210},
abstract = {With the advances of web-of-things, human mobility, e.g., GPS trajectories of vehicles, sharing bikes, and mobile devices, reflects people’s travel patterns and preferences, which are especially crucial for urban applications such as urban planning and business location selection. However, collecting a large set of human mobility data is not easy because of the privacy and commercial concerns, as well as the high cost to deploy sensors and a long time to collect the data, especially in newly developed cities. Realizing this, in this paper, based on the intuition that the human mobility is driven by the mobility intentions reflected by the origin and destination (or OD) features, as well as the preference to select the path between them, we investigate the problem to generate mobility data for a new target city, by transferring knowledge from mobility data and multi-source data of the source cities. Our framework contains three main stages: 1)&nbsp;mobility intention transfer, which learns a latent unified mobility intention distribution across the source cities, and transfers the model of the distribution to the target city; 2)&nbsp;OD generation, which generates the OD pairs in the target city based on the transferred mobility intention model, and 3)&nbsp;path generation, which generates the paths for each OD pair, based on a utility model learned from the real trajectory data in the source cities. Also, a demo of our trajectory generator is publicly available online for two city regions. Extensive experiment results over four regions in China validate the effectiveness of the proposed solution. Besides, an on-field case study is presented in a newly developed region, i.e., Xiongan, China. With the generated trajectories in the new city, many trajectory mining techniques can be applied. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1355–1365},
numpages = {11},
keywords = {Web of Things, Urban Computing, Trajectory Data Mining},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380211,
author = {Youngmann, Brit and Yom-Tov, Elad and Gilad-Bachrach, Ran and Karmon, Danny},
title = {The Automated Copywriter: Algorithmic Rephrasing of Health-Related Advertisements to Improve Their Performance},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380211},
doi = {10.1145/3366423.3380211},
abstract = {Search advertising is one of the most commonly-used methods of advertising. Past work has shown that search advertising can be employed to improve health by eliciting positive behavioral change. However, writing effective advertisements requires expertise and (possible expensive) experimentation, both of which may not be available to public health authorities wishing to elicit such behavioral changes, especially when dealing with a public health crises such as epidemic outbreaks. Here we develop an algorithm which builds on past advertising data to train a sequence-to-sequence Deep Neural Network which “translates” advertisements into optimized ads that are more likely to be clicked. The network is trained using more than 114 thousands ads shown on Microsoft Advertising. We apply this translator to two health related domains: Medical Symptoms (MS) and Preventative Healthcare (PH) and measure the improvements in click-through rates (CTR). Our experiments show that the generated ads are predicted to have higher CTR in 81% of MS ads and 76% of PH ads. To understand the differences between the generated ads and the original ones we develop estimators for the affective attributes of the ads. We show that the generated ads contain more calls-to-action and that they reflect higher valence (36% increase) and higher arousal (87%) on a sample of 1000 ads. Finally, we run an advertising campaign where 10 random ads and their rephrased versions from each of the domains are run in parallel. We show an average improvement in CTR of 68% for the generated ads compared to the original ads. Our results demonstrate the ability to automatically optimize advertisement for the health domain. We believe that our work offers health authorities an improved ability to help nudge people towards healthier behaviors while saving the time and cost needed to optimize advertising campaigns. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1366–1377},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380212,
author = {Ordozgoiti, Bruno and Matakos, Antonis and Gionis, Aristides},
title = {Finding Large Balanced Subgraphs in Signed Networks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380212},
doi = {10.1145/3366423.3380212},
abstract = {Signed networks are graphs whose edges are labelled with either a positive or a negative sign, and can be used to capture nuances in interactions that are missed by their unsigned counterparts. The concept of balance in signed graph theory determines whether a network can be partitioned into two perfectly opposing subsets, and is therefore useful for modelling phenomena such as the existence of polarized communities in social networks. While determining whether a graph is balanced is easy, finding a large balanced subgraph is hard. The few heuristics available in the literature for this purpose are either ineffective or non-scalable. In this paper we propose an efficient algorithm for finding large balanced subgraphs in signed networks. The algorithm relies on signed spectral theory and a novel bound for perturbations of the graph Laplacian. In a wide variety of experiments on real-world data we show that our algorithm can find balanced subgraphs much larger than those detected by existing methods, and in addition, it is faster. We test its scalability on graphs of up to 34 million edges.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1378–1388},
numpages = {11},
keywords = {community detection, signed graphs, graph mining, dense subgraph},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380213,
author = {Zhang, Han and Zheng, Wenhao and Chen, Charley and Gao, Kevin and Hu, Yao and Huang, Ling and Xu, Wei},
title = {Modeling Heterogeneous Statistical Patterns in High-Dimensional Data by Adversarial Distributions: An Unsupervised Generative Framework},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380213},
doi = {10.1145/3366423.3380213},
abstract = {Since the label collecting is prohibitive and time-consuming, unsupervised methods are preferred in applications such as fraud detection. Meanwhile, such applications usually require modeling the intrinsic clusters in high-dimensional data, which usually displays heterogeneous statistical patterns as the patterns of different clusters may appear in different dimensions. Existing methods propose to model the data clusters on selected dimensions, yet globally omitting any dimension may damage the pattern of certain clusters. To address the above issues, we propose a novel unsupervised generative framework called FIRD, which utilizes adversarial distributions to fit and disentangle the heterogeneous statistical patterns. When applying to discrete spaces, FIRD effectively distinguishes the synchronized fraudsters from normal users. Besides, FIRD also provides superior performance on anomaly detection datasets compared with SOTA anomaly detection methods (over 5% average AUC improvement). The significant experiment results on various datasets verify that the proposed method can better model the heterogeneous statistical patterns in high-dimensional data and benefit downstream applications. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1389–1399},
numpages = {11},
keywords = {prior knowledge, high-dimensional data, unsupervised learning, heterogeneous statistical patterns, adversarial distributions},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380214,
author = {Bo, Deyu and Wang, Xiao and Shi, Chuan and Zhu, Meiqi and Lu, Emiao and Cui, Peng},
title = {Structural Deep Clustering Network},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380214},
doi = {10.1145/3366423.3380214},
abstract = {Clustering is a fundamental task in data analysis. Recently, deep clustering, which derives inspiration primarily from deep learning approaches, achieves state-of-the-art performance and has attracted considerable attention. Current deep clustering methods usually boost the clustering results by means of the powerful representation ability of deep learning, e.g., autoencoder, suggesting that learning an effective representation for clustering is a crucial requirement. The strength of deep clustering methods is to extract the useful representations from the data itself, rather than the structure of data, which receives scarce attention in representation learning. Motivated by the great success of Graph Convolutional Network (GCN) in encoding the graph structure, we propose a Structural Deep Clustering Network (SDCN) to integrate the structural information into deep clustering. Specifically, we design a delivery operator to transfer the representations learned by autoencoder to the corresponding GCN layer, and a dual self-supervised mechanism to unify these two different deep neural architectures and guide the update of the whole model. In this way, the multiple structures of data, from low-order to high-order, are naturally combined with the multiple representations learned by autoencoder. Furthermore, we theoretically analyze the delivery operator, i.e., with the delivery operator, GCN improves the autoencoder-specific representation as a high-order graph regularization constraint and autoencoder helps alleviate the over-smoothing problem in GCN. Through comprehensive experiments, we demonstrate that our propose model can consistently perform better over the state-of-the-art techniques.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1400–1410},
numpages = {11},
keywords = {self-supervised learning, graph convolutional network, deep clustering, neural network},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380215,
author = {Chen, Weili and Zhang, Tuo and Chen, Zhiguang and Zheng, Zibin and Lu, Yutong},
title = {Traveling the Token World: A Graph Analysis of Ethereum ERC20 Token Ecosystem},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380215},
doi = {10.1145/3366423.3380215},
abstract = {The birth of Bitcoin ushered in the era of cryptocurrency, which has now become a financial market attracted extensive attention worldwide. The phenomenon of startups launching Initial Coin Offerings (ICOs) to raise capital led to thousands of tokens being distributed on blockchains. Many studies have analyzed this phenomenon from an economic perspective. However, little is know about the characteristics of participants in the ecosystem. To fill this gap and considering over 80% of ICOs launched based on ERC20 token on Ethereum, in this paper, we conduct a systematic investigation on the whole Ethereum ERC20 token ecosystem to characterize the token creator, holder, and transfer activity. By downloading the whole blockchain and parsing the transaction records and event logs, we construct three graphs, namely token creator graph, token holder graph, and token transfer graph. We obtain many observations and findings by analyzing these graphs. Besides, we propose an algorithm to discover potential relationships between tokens and other accounts. The reported case shows that our algorithm can effectively reveal entities and the complex relationship between various accounts in the token ecosystem.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1411–1421},
numpages = {11},
keywords = {ERC20 token, Blockchain, Ethereum, Graph analysis},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380216,
author = {Wang, Zhihao and Li, Qiang and Song, Jinke and Wang, Haining and Sun, Limin},
title = {Towards IP-Based Geolocation via Fine-Grained and Stable Webcam Landmarks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380216},
doi = {10.1145/3366423.3380216},
abstract = {IP-based geolocation is essential for various location-aware Internet applications, such as online advertisement, content delivery, and online fraud prevention. Achieving accurate geolocation enormously relies on the number of high-quality (i.e., the fine-grained and stable over time) landmarks. However, the previous efforts of garnering landmarks have been impeded by the limited visible landmarks on the Internet and manual time cost. In this paper, we leverage the availability of numerous online webcams that are used to monitor physical surroundings as a rich source of promising high-quality landmarks for serving IP-based geolocation. In particular, we present a new framework called GeoCAM, which is designed to automatically generate qualified landmarks from online webcams, providing IP-based geolocation services with high accuracy and wide coverage. GeoCAM periodically monitors websites that are hosting live webcams and uses the natural language processing technique to extract the IP addresses and latitude/longitude of webcams for generating landmarks at large-scale. We develop a prototype of GeoCAM and conduct real-world experiments for validating its efficacy. Our results show that GeoCam can detect 282,902 live webcams hosted in webpages with 94.2% precision and 90.4% recall, and then generate 16,863 stable and fine-grained landmarks, which are two orders of magnitude more than the landmarks used in prior works. Thus, by correlating a large scale of landmarks, GeoCAM is able to provide a geolocation service with high accuracy and wide coverage. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1422–1432},
numpages = {11},
keywords = {Webcam, Data Mining, Information Extraction, IP Geolocation, Landmarks, Internet of Things},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380217,
author = {Papadopoulos, Panagiotis and Snyder, Peter and Athanasakis, Dimitrios and Livshits, Benjamin},
title = {Keeping out the Masses: Understanding the Popularity and Implications of Internet Paywalls},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380217},
doi = {10.1145/3366423.3380217},
abstract = {Funding the production of quality online content is a pressing problem for content producers. The most common funding method, online advertising, is rife with well-known performance and privacy harms, and an intractable subject-agent conflict: many users do not want to see advertisements, depriving the site of needed funding. Because of these negative aspects of advertisement-based funding, paywalls are an increasingly popular alternative for websites. This shift to a “pay-for-access” web is one that has potentially huge implications for the web and society. Instead of a system where information (nominally) flows freely, paywalls create a web where high quality information is available to fewer and fewer people, leaving the rest of the web users with less information, that might be also less accurate and of lower quality. Despite the potential significance of a move from an “advertising-but-open” web to a “paywalled” web, we find this issue understudied. This work addresses this gap in our understanding by measuring how widely paywalls have been adopted, what kinds of sites use paywalls, and the distribution of policies enforced by paywalls. A partial list of our findings include that (i) paywall use has increased, and at an increasing rate (2 \texttimes{} more paywalls every 6 months), (ii) paywall adoption differs by country (e.g., 18.75% in US, 12.69% in Australia), (iii) paywall deployment significantly changes how users interact with the site (e.g., higher bounce rates, less incoming links), (iv) the median cost of an annual paywall access is 108 USD per site, and (v) paywalls are in general trivial to circumvent. Finally, we present the design of a novel, automated system for detecting whether a site uses a paywall, through the combination of runtime browser instrumentation and repeated programmatic interactions with the site. We intend this classifier to augment future, longitudinal measurements of paywall use and behavior.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1433–1444},
numpages = {12},
keywords = {Paywalls, User privacy, Web Monetization, User Subscription},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380218,
author = {Greiner-Petter, Andr\'{e} and Schubotz, Moritz and M\"{u}ller, Fabian and Breitinger, Corinna and Cohl, Howard and Aizawa, Akiko and Gipp, Bela},
title = {Discovering Mathematical Objects of Interest—A Study of Mathematical Notations},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380218},
doi = {10.1145/3366423.3380218},
abstract = {Mathematical notation, i.e., the writing system used to communicate concepts in mathematics, encodes valuable information for a variety of information search and retrieval systems. Yet, mathematical notations remain mostly unutilized by today’s systems. In this paper, we present the first in-depth study on the distributions of mathematical notation in two large scientific corpora:&nbsp;the open access arXiv (2.5B mathematical objects) and the mathematical reviewing service for pure and applied mathematics zbMATH (61M mathematical objects). Our study lays a foundation for future research projects on mathematical information retrieval for large scientific corpora. Further, we demonstrate the relevance of our results to a variety of use-cases. For example, to assist semantic extraction systems, to improve scientific search engines, and to facilitate specialized math recommendation systems. The contributions of our presented research are as follows: (1) we present the first distributional analysis of mathematical formulae on arXiv and zbMATH; (2) we retrieve relevant mathematical objects for given textual search queries (e.g., linking with ‘Jacobi polynomial’); (3) we extend zbMATH’s search engine by providing relevant mathematical formulae; and (4) we exemplify the applicability of the results by presenting auto-completion for math inputs as the first contribution to math recommendation systems. To expedite future research projects, we have made available our source code and data.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1445–1456},
numpages = {12},
keywords = {Mathematical Information Retrieval, Mathematical Search Engine, Term Frequency-Inverse Document Frequency, Mathematical Objects of Interest, Distributions of Mathematical Objects},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380219,
author = {Wu, Man and Pan, Shirui and Zhou, Chuan and Chang, Xiaojun and Zhu, Xingquan},
title = {Unsupervised Domain Adaptive Graph Convolutional Networks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380219},
doi = {10.1145/3366423.3380219},
abstract = {Graph convolutional networks (GCNs) have achieved impressive success in many graph related analytics tasks. However, most GCNs only work in a single domain (graph) incapable of transferring knowledge from/to other domains (graphs), due to the challenges in both graph representation learning and domain adaptation over graph structures. In this paper, we present a novel approach, unsupervised domain adaptive graph convolutional networks (UDA-GCN), for domain adaptation learning for graphs. To enable effective graph representation learning, we first develop a dual graph convolutional network component, which jointly exploits local and global consistency for feature aggregation. An attention mechanism is further used to produce a unified representation for each node in different graphs. To facilitate knowledge transfer between graphs, we propose a domain adaptive learning module to optimize three different loss functions, namely source classifier loss, domain classifier loss, and target classifier loss as a whole, thus our model can differentiate class labels in the source domain, samples from different domains, the class labels from the target domain, respectively. Experimental results on real-world datasets in the node classification task validate the performance of our method, compared to state-of-the-art graph neural network algorithms.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1457–1467},
numpages = {11},
keywords = {Domain Adaptation, node classification, graph convolutional networks},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380220,
author = {Garc\'{\i}a-Soriano, David and Kutzkov, Konstantin and Bonchi, Francesco and Tsourakakis, Charalampos},
title = {Query-Efficient Correlation Clustering},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380220},
doi = {10.1145/3366423.3380220},
abstract = {Correlation clustering is arguably the most natural formulation of clustering. Given n objects and a pairwise similarity measure, the goal is to cluster the objects so that, to the best possible extent, similar objects are put in the same cluster and dissimilar objects are put in different clusters. A main drawback of correlation clustering is that it requires as input the Θ(n2) pairwise similarities. This is often infeasible to compute or even just to store. In this paper we study query-efficient algorithms for correlation clustering. Specifically, we devise a correlation clustering algorithm that, given a budget of Q queries, attains a solution whose expected number of disagreements is at most , where is the optimal cost for the instance. Its running time is O(Q), and can be easily made non-adaptive (meaning it can specify all its queries at the outset and make them in parallel) with the same guarantees. Up to constant factors, our algorithm yields a provably optimal trade-off between the number of queries Q and the worst-case error attained, even for adaptive algorithms. Finally, we perform an experimental study of our proposed method on both synthetic and real data, showing the scalability and the accuracy of our algorithm. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1468–1478},
numpages = {11},
keywords = {query complexity, correlation clustering, algorithm design, active learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380221,
author = {Agarwal, Pushkal and Joglekar, Sagar and Papadopoulos, Panagiotis and Sastry, Nishanth and Kourtellis, Nicolas},
title = {Stop Tracking Me Bro! Differential Tracking of User Demographics on Hyper-Partisan Websites},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380221},
doi = {10.1145/3366423.3380221},
abstract = {Websites with hyper-partisan, left or right-leaning focus offer content that is typically biased towards the expectations of their target audience. Such content often polarizes users, who are repeatedly primed to specific (extreme) content, usually reflecting hard party lines on political and socio-economic topics. Though this polarization has been extensively studied with respect to content, it is still unknown how it associates with the online tracking experienced by browsing users, especially when they exhibit certain demographic characteristics. For example, it is unclear how such websites enable the ad-ecosystem to track users based on their gender or age. In this paper, we take a first step to shed light and measure such potential differences in tracking imposed on users when visiting specific party-line’s websites. For this, we design and deploy a methodology to systematically probe such websites and measure differences in user tracking. This methodology allows us to create user personas with specific attributes like gender and age and automate their browsing behavior in a consistent and repeatable manner. Thus, we systematically study how personas are being tracked by these websites and their third parties, especially if they exhibit particular demographic properties. Overall, we test 9 personas on 556 hyper-partisan websites and find that right-leaning websites tend to track users more intensely than left-leaning, depending on user demographics, using both cookies and cookie synchronization methods and leading to more costly delivered ads. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1479–1490},
numpages = {12},
keywords = {advertisements, web cookies, hyper-partisan news, web tracking.},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380222,
author = {Woo, Simon S.},
title = {How Do We Create a Fantabulous Password?},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380222},
doi = {10.1145/3366423.3380222},
abstract = {Although pronounceability can improve password memorability, most existing password generation approaches have not properly integrated the pronounceability of passwords in their designs. In this work, we demonstrate several shortfalls of current pronounceable password generation approaches, and then propose, ProSemPass, a new method of generating passwords that are pronounceable and semantically meaningful. In our approach, users supply initial input words and our system improves the pronounceability and meaning of the user-provided words by automatically creating a portmanteau. To measure the strength of our approach, we use attacker models, where attackers have complete knowledge of our password generation algorithms. We measure strength in guess numbers and compare those with other existing password generation approaches. Using a large-scale IRB-approved user study with 1,563 Amazon MTurkers over 9 different conditions, our approach achieves a 30% higher recall than those from current pronounceable password approaches, and is stronger than the offline guessing attack limit.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1491–1501},
numpages = {11},
keywords = {Pronounceable Password, Password, Semantic Password},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380223,
author = {Zeng, Jichuan and Li, Jing and He, Yulan and Gao, Cuiyun and Lyu, Michael and King, Irwin},
title = {What Changed Your Mind: The Roles of Dynamic Topics and Discourse in Argumentation Process},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380223},
doi = {10.1145/3366423.3380223},
abstract = {In our world with full of uncertainty, debates and argumentation contribute to the progress of science and society. Despite of the increasing attention to characterize human arguments, most progress made so far focus on the debate outcome, largely ignoring the dynamic patterns in argumentation processes. This paper presents a study that automatically analyzes the key factors in argument persuasiveness, beyond simply predicting who will persuade whom. Specifically, we propose a novel neural model that is able to dynamically track the changes of latent topics and discourse in argumentative conversations, allowing the investigation of their roles in influencing the outcomes of persuasion. Extensive experiments have been conducted on argumentative conversations on both social media and supreme court. The results show that our model outperforms state-of-the-art models in identifying persuasive arguments via explicitly exploring dynamic factors of topic and discourse. We further analyze the effects of topics and discourse on persuasiveness, and find that they are both useful — topics provide concrete evidence while superior discourse styles may bias participants, especially in social media arguments. In addition, we draw some findings from our empirical results, which will help people better engage in future persuasive conversations.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1502–1513},
numpages = {12},
keywords = {discourse modeling, social media, topic modeling, argumentation mining, dynamic data processing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380224,
author = {Choi, Minje and Aiello, Luca Maria and Varga, Kriszti\'{a}n Zsolt and Quercia, Daniele},
title = {Ten Social Dimensions of Conversations and Relationships},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380224},
doi = {10.1145/3366423.3380224},
abstract = {Decades of social science research identified ten fundamental dimensions that provide the conceptual building blocks to describe the nature of human relationships. Yet, it is not clear to what extent these concepts are expressed in everyday language and what role they have in shaping observable dynamics of social interactions. After annotating conversational text through crowdsourcing, we trained NLP tools to detect the presence of these types of interaction from conversations, and applied them to 160M messages written by geo-referenced Reddit users, 290k emails from the Enron corpus and 300k lines of dialogue from movie scripts. We show that social dimensions can be predicted purely from conversations with an AUC up to 0.98, and that the combination of the predicted dimensions suggests both the types of relationships people entertain (conflict vs. support) and the types of real-world communities (wealthy vs. deprived) they shape.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1514–1525},
numpages = {12},
keywords = {twitter, social relationships, NLP, conversations, tinghy, reddit, enron},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380225,
author = {Quattrone, Giovanni and Nocera, Antonino and Capra, Licia and Quercia, Daniele},
title = {Social Interactions or Business Transactions?What Customer Reviews Disclose about Airbnb Marketplace},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380225},
doi = {10.1145/3366423.3380225},
abstract = {Airbnb is one of the most successful examples of sharing economy marketplaces. With rapid and global market penetration, understanding its attractiveness and evolving growth opportunities is key to plan business decision making. There is an ongoing debate, for example, about whether Airbnb is a hospitality service that fosters social exchanges between hosts and guests, as the sharing economy manifesto originally stated, or whether it is (or is evolving into being) a purely business transaction platform, the way hotels have traditionally operated. To answer these questions, we propose a novel market analysis approach that exploits customers’ reviews. Key to the approach is a method that combines thematic analysis and machine learning to inductively develop a custom dictionary for guests’ reviews. Based on this dictionary, we then use quantitative linguistic analysis on a corpus of 3.2 million reviews collected in 6 different cities, and illustrate how to answer a variety of market research questions, at fine levels of temporal, thematic, user and spatial granularity, such as (i) how the business vs social dichotomy is evolving over the years, (ii) what exact words within such top-level categories are evolving, (iii) whether such trends vary across different user segments and (iv) in different neighbourhoods.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1526–1536},
numpages = {11},
keywords = {Airbnb, market analysis, Sharing economy, linguistic analysis},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380226,
author = {Chen, Jiaoyan and Chen, Xi and Horrocks, Ian and B. Myklebust, Erik and Jimenez-Ruiz, Ernesto},
title = {Correcting Knowledge Base Assertions},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380226},
doi = {10.1145/3366423.3380226},
abstract = {The usefulness and usability of knowledge bases (KBs) is often limited by quality issues. One common issue is the presence of erroneous assertions, often caused by lexical or semantic confusion. We study the problem of correcting such assertions, and present a general correction framework which combines lexical matching, semantic embedding, soft constraint mining and semantic consistency checking. The framework is evaluated using DBpedia and an enterprise medical KB.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1537–1547},
numpages = {11},
keywords = {Semantic Embedding, Assertion Correction, Constraint Mining, Knowledge Base Quality, Consistency Checking},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380227,
author = {Mathew, Binny and Sikdar, Sandipan and Lemmerich, Florian and Strohmaier, Markus},
title = {The POLAR Framework: Polar Opposites Enable Interpretability of Pre-Trained Word Embeddings},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380227},
doi = {10.1145/3366423.3380227},
abstract = {We introduce ‘POLAR’ — a framework that adds interpretability to pre-trained word embeddings via the adoption of semantic differentials. Semantic differentials are a psychometric construct for measuring the semantics of a word by analysing its position on a scale between two polar opposites (e.g., cold – hot, soft – hard). The core idea of our approach is to transform existing, pre-trained word embeddings via semantic differentials to a new “polar” space with interpretable dimensions defined by such polar opposites. Our framework also allows for selecting the most discriminative dimensions from a set of polar dimensions provided by an oracle, i.e., an external source. We demonstrate the effectiveness of our framework by deploying it to various downstream tasks, in which our interpretable word embeddings achieve a performance that is comparable to the original word embeddings. We also show that the interpretable dimensions selected by our framework align with human judgement. Together, these results demonstrate that interpretability can be added to word embeddings without compromising performance. Our work is relevant for researchers and engineers interested in interpreting pre-trained word embeddings.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1548–1558},
numpages = {11},
keywords = {interpretable, neural networks, semantic differential, word embeddings},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380228,
author = {Hossain, Safwan and Mladenovic, Andjela and Shah, Nisarg},
title = {Designing Fairly Fair Classifiers Via Economic Fairness Notions},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380228},
doi = {10.1145/3366423.3380228},
abstract = {The past decade has witnessed a rapid growth of research on fairness in machine learning. In contrast, fairness has been formally studied for almost a century in microeconomics in the context of resource allocation, during which many general-purpose notions of fairness have been proposed. This paper explore the applicability of two such notions — envy-freeness and equitability — in machine learning. We propose novel relaxations of these fairness notions which apply to groups rather than individuals, and are compelling in a broad range of settings. Our approach provides a unifying framework by incorporating several recently proposed fairness definitions as special cases. We provide generalization bounds for our approach, and theoretically and experimentally evaluate the tradeoff between loss minimization and our fairness guarantees.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1559–1569},
numpages = {11},
keywords = {generalization, fairness, group equitability, Group envy-freeness},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380229,
author = {Andresel, Medina and Corman, Julien and Ortiz, Magdalena and Reutter, Juan L. and Savkovic, Ognjen and Simkus, Mantas},
title = {Stable Model Semantics for Recursive SHACL},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380229},
doi = {10.1145/3366423.3380229},
abstract = {SHACL (SHape Constraint Language) is a W3C recommendation for validating graph-based data against a set of constraints (called shapes). Importantly, SHACL allows to define recursive shapes, i.e. a shape may refer to itself, directly of indirectly. The recommendation left open the semantics of recursive shapes, but proposals have emerged recently to extend the official semantics to support recursion. These proposals are based on the principle of possibility (or non-contradiction): a graph is considered valid against a schema if one can assign shapes to nodes in such a way that all constraints are satisfied. This semantics is not constructive, as it does not provide guidelines about how to obtain such an assignment, and it may lead to unfounded assignments, where the only reason to assign a shape to a node is that it allows validating the graph. In contrast, we propose in this paper a stricter, more constructive semantics for SHACL, based on stable models, which are well-known in Answer Set Programming (ASP). This semantics additionally requires a shape assignment to be properly justified by the input constraints. We further exploit the connection to logic programming, and show that SHACL constraints can be naturally represented as logic programs, and that the validation problem for a graph and a SHACL schema can be encoded as an ASP reasoning task. The proposed semantics also enjoys computationally tractable validation in the presence of constraints with stratified negation (as opposed to the previous semantics). We also extend our semantics to 3-valued stable models, which yields a more relaxed notion of validation, tolerant to certain faults in the schema or data. By exploiting a connection between 3-valued stable model semantics and the well-founded semantics for logic programs, we can use our translation into ASP to show another tractability result. Finally, we provide a preliminary evaluation of the approach, which leverages an ASP solver to perform graph validation.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1570–1580},
numpages = {11},
keywords = {graph-structured data, SHACL, answer set programming},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380230,
author = {Jiang, Zhuoren and Gao, Zheng and Lan, Jinjiong and Yang, Hongxia and Lu, Yao and Liu, Xiaozhong},
title = {Task-Oriented Genetic Activation for Large-Scale Complex Heterogeneous Graph Embedding},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380230},
doi = {10.1145/3366423.3380230},
abstract = {The recent success of deep graph embedding innovates the graphical information characterization methodologies. However, in real-world applications, such a method still struggles with the challenges of heterogeneity, scalability, and multiplex. To address these challenges, in this study, we propose a novel solution, Genetic hEterogeneous gRaph eMbedding (GERM), which enables flexible and efficient task-driven vertex embedding in a complex heterogeneous graph. Unlike prior efforts for this track of studies, we employ a task-oriented genetic activation strategy to efficiently generate the “Edge Type Activated Vector” (ETAV) over the edge types in the graph. The generated ETAV can not only reduce the incompatible noise and navigate the heterogeneous graph random walk at the graph-schema level, but also activate an optimized subgraph for efficient representation learning. By revealing the correlation between the graph structure and task information, the model interpretability can be enhanced as well. Meanwhile, an activated heterogeneous skip-gram framework is proposed to encapsulate both topological and task-specific information of a given heterogeneous graph. Through extensive experiments on both scholarly and e-commerce datasets, we demonstrate the efficacy and scalability of the proposed methods via various search/recommendation tasks. GERM can significantly reduces the running time and remove expert-intervention without sacrificing the performance (or even modestly improve) by comparing with baselines.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1581–1591},
numpages = {11},
keywords = {Heterogeneous Graph Embedding, Edge Type Activation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380231,
author = {Jiang, Shan and Baumgartner, Simon and Ittycheriah, Abe and Yu, Cong},
title = {Factoring Fact-Checks: Structured Information Extraction from Fact-Checking Articles},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380231},
doi = {10.1145/3366423.3380231},
abstract = {Fact-checking, which investigates claims made in public to arrive at a verdict supported by evidence and logical reasoning, has long been a significant form of journalism to combat misinformation in the news ecosystem. Most of the fact-checks share common structured information (called factors) such as claim, claimant, and verdict. In recent years, the emergence of ClaimReview as the standard schema for annotating those factors within fact-checking articles has led to wide adoption of fact-checking features by online platforms (e.g., Google, Bing). However, annotating fact-checks is a tedious process for fact-checkers and distracts them from their core job of investigating claims. As a result, less than half of the fact-checkers worldwide have adopted ClaimReview as of mid-2019. In this paper, we propose the task of factoring fact-checks for automatically extracting structured information from fact-checking articles. Exploring a public dataset of fact-checks, we empirically show that factoring fact-checks is a challenging task, especially for fact-checkers that are under-represented in the existing dataset. We then formulate the task as a sequence tagging problem and fine-tune the pre-trained BERT models with a modification made from our observations to approach the problem. Through extensive experiments, we demonstrate the performance of our models for well-known fact-checkers and promising initial results for under-represented fact-checkers.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1592–1603},
numpages = {12},
keywords = {fact-checking, sequence tagging, misinformation, BERT, computational journalism, ClaimReview, information extraction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380232,
author = {Liu, Yuanxing and Ren, Zhaochun and Zhang, Wei-Nan and Che, Wanxiang and Liu, Ting and Yin, Dawei},
title = {Keywords Generation Improves E-Commerce Session-Based Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380232},
doi = {10.1145/3366423.3380232},
abstract = {By exploring fine-grained user behaviors, session-based recommendation predicts a user’s next action from short-term behavior sessions. Most of previous work learns about a user’s implicit behavior by merely taking the last click action as the supervision signal. However, in e-commerce scenarios, large-scale products with elusive click behaviors make such task challenging because of the low inclusiveness problem, i.e., many relevant products that satisfy the user’s shopping intention are neglected by recommenders. Since similar products with different IDs may share the same intention, we argue that the textual information (e.g., keywords of product titles) from sessions can be used as additional supervision signals to tackle above problem through learning more shared intention within similar products. Therefore, to improve the performance of e-commerce session-based recommendation, we explicitly infer the user’s intention by generating keywords entirely from the click sequence in the current session.  In this paper, we propose the e-commerce session-based recommendation model with keywords generation (abbreviated as ESRM-KG) to integrate keywords generation into e-commerce session-based recommendation. Specifically, the ESRM-KG model firstly encodes an input action sequence into a high dimensional representation; then it presents a bi-linear decoding scheme to predict the next action in the current session; synchronously, the ESRM-KG model addresses incepts the high dimensional representation of its encoder to generate explainable keywords for the whole session. We carried out extensive experiments in the context of click prediction on a large-scale real-world e-commerce dataset. Our experimental results show that the ESRM-KG model outperforms state-of-the-art baselines with the help of keywords generation. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1604–1614},
numpages = {11},
keywords = {transformer network, keywords generation, Session-based recommendation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380233,
author = {Park, Dongmin and Song, Hwanjun and Kim, Minseok and Lee, Jae-Gil},
title = {TRAP: Two-Level Regularized Autoencoder-Based Embedding for Power-Law Distributed Data},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380233},
doi = {10.1145/3366423.3380233},
abstract = {Recently, autoencoder&nbsp;(AE)-based embedding approaches have achieved state-of-the-art performance in many tasks, especially in top-k recommendation with user embedding or node classification with node embedding. However, we find that many real-world data follow the power-law distribution with respect to the data object sparsity. When learning AE-based embeddings of these data, dense inputs move away from sparse inputs in an embedding space even when they are highly correlated. This phenomenon, which we call polarization, obviously distorts the embedding. In this paper, we propose TRAP that leverages two-level regularizers to effectively alleviate the polarization problem. The macroscopic regularizer generally prevents dense input objects from being distant from other sparse input objects, and the microscopic regularizer individually attracts each object to correlated neighbor objects rather than uncorrelated ones. Importantly, TRAP is a meta-algorithm that can be easily coupled with existing AE-based embedding methods with a simple modification. In extensive experiments on two representative embedding tasks using six-real world datasets, TRAP boosted the performance of the state-of-the-art algorithms by up to 31.53% and 94.99% respectively.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1615–1624},
numpages = {10},
keywords = {Graph Embedding, Power-law Distribution, Autoencoder, Recommender System},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380234,
author = {Nagendra, Vasudevan and Bhattacharya, Arani and Yegneswaran, Vinod and Rahmati, Amir and Das, Samir},
title = {An Intent-Based Automation Framework for Securing Dynamic Consumer IoT Infrastructures},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380234},
doi = {10.1145/3366423.3380234},
abstract = {Consumer IoT networks are characterized by heterogeneous devices with diverse functionality and programming interfaces. This lack of homogeneity makes the integration and secure management of IoT infrastructures a daunting task for users and administrators. In this paper, we introduce VISCR, a Vendor-Independent policy Specification and Conflict Resolution engine that enables intent-based conflict-free policy specification and enforcement in IoT environments. VISCR converts the topology of the IoT infrastructure into a tree-based abstraction and translates existing policies from heterogeneous vendor-specific programming languages, such as Groovy-based SmartThings, OpenHAB, IFTTT-based templates, and MUD-based profiles, into a vendor-independent graph-based specification. These are then used to automatically detect rogue policies, policy conflicts, and automation bugs. We evaluated VISCR using a dataset of 907 IoT apps, programmed using heterogeneous automation specifications, in a simulated smart-building IoT infrastructure. In our experiments, among 907 IoT apps, VISCR exposed 342 of IoT apps as exhibiting one or more violations, while also running 14.2x faster than the state-of-the-art tool (Soteria). VISCR detected 100% of violations reported by Soteria while also detecting new types of violations in 266 additional apps. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1625–1636},
numpages = {12},
keywords = {Consumer IoT security, Intent-based policy and automation framework, Conflict detection and resolution.},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380235,
author = {Yang, Mingzhou and Li, Yanhua and Zhou, Xun and Lu, Hui and Tian, Zhihong and Luo, Jun},
title = {Inferring Passengers’ Interactive Choices on Public Transits via MA-AL: Multi-Agent Apprenticeship Learning},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380235},
doi = {10.1145/3366423.3380235},
abstract = {Public transports, such as subway lines and buses, offer affordable ride-sharing services and reduce the road network traffic. Extracting passengers’ preferences from their public transit choices is important to city planners but technically non-trivial. When traveling by taking public transits, passengers make sequences of transit choices, and their rewards are usually influenced by other passengers’ choices. This process can be modeled as a Markov Game (MG). In this paper, we make the first effort to model travelers’ preferences of making transit choices using MGs. Based on the discovery that passengers usually do not change their policies, we propose novel algorithms to extract reward functions from the observed deterministic equilibrium joint policy of all agents in a general-sum MG to infer travelers’ preferences. First, we assume we have the access to the entire joint policy. We characterize the set of all reward functions for which the given joint policy is a Nash equilibrium policy. In order to remove the degeneracy of the solution, we then attempt to pick reward functions so as to maximize the sum of the deviation between the the observed policy and the sub-optimal policy of each agent. This results in a skillfully solvable linear programming algorithm for the multi-agent inverse reinforcement learning (MA-IRL) problem. Then, we deal with the case where we have access to the equilibrium joint policy through a set of actual trajectories. We propose an iterative algorithm inspired by single-agent apprenticeship learning algorithms and the cyclic coordinate descent approach. We evaluate the proposed algorithms on both a simple Grid Game and a unique real-world dataset (from Shenzhen, China). Results show that when we have access to the full policy, our algorithm can efficiently recover most of the reward structure, especially the interaction of agents. In the case where we only have access to a set of sampled expert trajectories, our algorithm can provide an explanation of the expert trajectories. Measured with respect to the experts’ unknown reward function, the performance of the policy output by our algorithm is close to that of the expert policy.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1637–1647},
numpages = {11},
keywords = {Markov Game, Crowd-generated Data Mining, Multi-Agent Apprenticeship Learning, Urban computing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380236,
author = {Wang, Chao and Zhu, Hengshu and Zhu, Chen and Zhang, Xi and Chen, Enhong and Xiong, Hui},
title = {Personalized Employee Training Course Recommendation with Career Development Awareness},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380236},
doi = {10.1145/3366423.3380236},
abstract = {As a major component of strategic talent management, learning and development (L&amp;D) aims at improving the individual and organization performances through planning tailored training for employees to increase and improve their skills and knowledge. While many companies have developed the learning management systems (LMSs) for facilitating the online training of employees, a long-standing important issue is how to achieve personalized training recommendations with the consideration of their needs for future career development. To this end, in this paper, we propose an explainable personalized online course recommender system for enhancing employee training and development. A unique perspective of our system is to jointly model both the employees’ current competencies and their career development preferences in an explainable way. Specifically, the recommender system is based on a novel end-to-end hierarchical framework, namely Demand-aware Collaborative Bayesian Variational Network (DCBVN). In DCBVN, we first extract the latent interpretable representations of the employees’ competencies from their skill profiles with autoencoding variational inference based topic modeling. Then, we develop an effective demand recognition mechanism for learning the personal demands of career development for employees. In particular, all the above processes are integrated into a unified Bayesian inference view for obtaining both accurate and explainable recommendations. Finally, extensive experimental results on real-world data clearly demonstrate the effectiveness and the interpretability of DCBVN, as well as its robustness on sparse and cold-start scenarios.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1648–1659},
numpages = {12},
keywords = {Employee training course recommendation, Intelligent education, Recommender system},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380237,
author = {Yao, Quanming and Chen, Xiangning and Kwok, James T. and Li, Yong and Hsieh, Cho-Jui},
title = {Efficient Neural Interaction Function Search for Collaborative Filtering},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380237},
doi = {10.1145/3366423.3380237},
abstract = {In collaborative filtering (CF), interaction function (IFC) play the important role of capturing interactions among items and users. The most popular IFC is the inner product, which has been successfully used in low-rank matrix factorization. However, interactions in real-world applications can be highly complex. Thus, other operations (such as plus and concatenation), which may potentially offer better performance, have been proposed. Nevertheless, it is still hard for existing IFCs to have consistently good performance across different application scenarios. Motivated by the recent success of automated machine learning (AutoML), we propose in this paper the search for simple neural interaction functions (SIF) in CF. By examining and generalizing existing CF approaches, an expressive SIF search space is designed and represented as a structured multi-layer perceptron. We propose an one-shot search algorithm that simultaneously updates both the architecture and learning parameters. Experimental results demonstrate that the proposed method can be much more efficient than popular AutoML approaches, can obtain much better prediction performance than state-of-the-art CF approaches, and can discover distinct IFCs for different data sets and tasks.1},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1660–1670},
numpages = {11},
keywords = {Collaborative Filtering, neural architecture search, recommeder system, automated machine learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380238,
author = {Hatt, Tobias and Feuerriegel, Stefan},
title = {Early Detection of User Exits from Clickstream Data: A Markov Modulated Marked Point Process Model},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380238},
doi = {10.1145/3366423.3380238},
abstract = {Most users leave e-commerce websites with no purchase. Hence, it is important for website owners to detect users at risk of exiting and intervene early (e.&nbsp;g., adapting website content or offering price promotions). Prior approaches make widespread use of clickstream data; however, state-of-the-art algorithms only model the sequence of web pages visited and not the time spent on them. In this paper, we develop a novel Markov modulated marked point process (M3PP) model for detecting users at risk of exiting with no purchase from clickstream data. It accommodates clickstream data in a holistic manner: our proposed M3PP models both the sequence of pages visited and the temporal dynamics between them, i.&nbsp;e., the time spent on pages. This is achieved by a continuous-time marked point process. Different from previous Markovian clickstream models, our M3PP is the first model in which the continuous nature of time is considered. The marked point process is modulated by a continuous-time Markov process in order to account for different latent shopping phases. As a secondary contribution, we suggest a risk assessment framework. Rather than predicting future page visits, we compute a user’s risk of exiting with no purchase. For this purpose, we build upon sequential hypothesis testing in order to suggest a risk score for user exits. Our computational experiments draw upon real-world clickstream data provided by a large online retailer. Based on this, we find that state-of-the-art algorithms are consistently outperformed by our M3PP model in terms of both AUROC (+ 6.24 percentage points) and so-called time of early warning (+ 12.93&nbsp;%). Accordingly, our M3PP model allows for timely detections of user exits and thus provides sufficient time for e-commerce website owners to trigger dynamic online interventions.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1671–1681},
numpages = {11},
keywords = {Marked point process, Clickstream data, Continuous-time Markov process, Risk scoring, Online user behavior},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380239,
author = {Sj\"{o}sten, Alexander and Snyder, Peter and Pastor, Antonio and Papadopoulos, Panagiotis and Livshits, Benjamin},
title = {Filter List Generation for Underserved Regions},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380239},
doi = {10.1145/3366423.3380239},
abstract = {Filter lists play a large and growing role in protecting and assisting web users. The vast majority of popular filter lists are crowd-sourced, where a large number of people manually label resources related to undesirable web resources (e.g. ads, trackers, paywall libraries), so that they can be blocked by browsers and extensions. Because only a small percentage of web users participate in the generation of filter lists, a crowd-sourcing strategy works well for blocking either uncommon resources that appear on “popular” websites, or resources that appear on a large number of “unpopular” websites. A crowd-sourcing strategy will perform poorly for parts of the web with small “crowds”, such as regions of the web serving languages with (relatively) few speakers. This work addresses this problem through the combination of two novel techniques: (i) deep browser instrumentation that allows for the accurate generation of request chains, in a way that is robust in situations that confuse existing measurement techniques, and (ii) an ad classifier that uniquely combines perceptual and page-context features to remain accurate across multiple languages. We apply our unique two-step filter list generation pipeline to three regions of the web that currently have poorly maintained filter lists: Sri Lanka, Hungary, and Albania. We generate new filter lists that complement existing filter lists. Our complementary lists block an additional&nbsp;3,349 of ad and ad-related resources (1,771 unique) when applied to&nbsp;6,475 pages targeting these three regions. We hope that this work can be part of an increased effort at ensuring that the security, privacy, and performance benefits of web resource blocking can be shared with all users, and not only those in dominant linguistic or economic regions.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1682–1692},
numpages = {11},
keywords = {ad blocking, filter lists, crowdsource},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380240,
author = {Syed, Rohail and Collins-Thompson, Kevyn and Bennett, Paul N. and Teng, Mengqiu and Williams, Shane and Tay, Dr. Wendy W. and Iqbal, Shamsi},
title = {Improving Learning Outcomes with Gaze Tracking and Automatic Question Generation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380240},
doi = {10.1145/3366423.3380240},
abstract = {As AI technology advances, it offers promising opportunities to improve educational outcomes when integrated with an overall learning experience. We investigate forward-looking interactive reading experiences that leverage both automatic question generation and analysis of attention signals, such as gaze tracking, to improve short- and long-term learning outcomes. We aim to expand the known pedagogical benefits of adjunct questions to more general reading scenarios, by investigating the benefits of adjunct questions generated after participants attend to passages in an article, based on their gaze behavior. We also compare the effectiveness of manually-written questions with those produced by Automatic Question Generation (AQG). We further investigate gaze and reading patterns indicative of low vs. high learning in both short- and long-term scenarios (one-week followup). We show AQG-generated adjunct questions have promise as a way to scale to a wide variety of reading material where the cost of manually curating questions may be prohibitive.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1693–1703},
numpages = {11},
keywords = {Lab study, User modeling, Education/Learning, Gaze tracking, Personalization},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380241,
author = {Duggal, Rahul and Freitas, Scott and Xiao, Cao and Chau, Duen Horng and Sun, Jimeng},
title = {REST: Robust and Efficient Neural Networks for Sleep Monitoring in the Wild},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380241},
doi = {10.1145/3366423.3380241},
abstract = {In recent years, significant attention has been devoted towards integrating deep learning technologies in the healthcare domain. However, to safely and practically deploy deep learning models for home health monitoring, two significant challenges must be addressed: the models should be (1) robust against noise; and (2) compact and energy-efficient. We propose Rest , a new method that simultaneously tackles both issues via 1) adversarial training and controlling the Lipschitz constant of the neural network through spectral regularization while 2) enabling neural network compression through sparsity regularization. We demonstrate that Rest produces highly-robust and efficient models that substantially outperform the original full-sized models in the presence of noise. For the sleep staging task over single-channel electroencephalogram (EEG), the Rest model achieves a macro-F1 score of 0.67 vs. 0.39 achieved by a state-of-the-art model in the presence of Gaussian noise while obtaining 19 \texttimes{} parameter reduction and 15 \texttimes{} MFLOPS reduction on two large, real-world EEG datasets. By deploying these models to an Android application on a smartphone, we quantitatively observe that Rest allows models to achieve up to 17 \texttimes{} energy reduction and 9 \texttimes{} faster inference. We open source the code repository with this paper: https://github.com/duggalrahul/REST.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1704–1714},
numpages = {11},
keywords = {sleep staging, deep learning, compression, adversarial},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380242,
author = {Liu, Tianming and Wang, Haoyu and Li, Li and Luo, Xiapu and Dong, Feng and Guo, Yao and Wang, Liu and Bissyand\'{e}, Tegawend\'{e} and Klein, Jacques},
title = {MadDroid: Characterizing and Detecting Devious Ad Contents for Android Apps},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380242},
doi = {10.1145/3366423.3380242},
abstract = {Advertisement drives the economy of the mobile app ecosystem. As a key component in the mobile ad business model, mobile ad content has been overlooked by the research community, which poses a number of threats, e.g., propagating malware and undesirable contents. To understand the practice of these devious ad behaviors, we perform a large-scale study on the app contents harvested through automated app testing. In this work, we first provide a comprehensive categorization of devious ad contents, including five kinds of behaviors belonging to two categories: ad loading content and ad clicking content. Then, we propose MadDroid, a framework for automated detection of devious ad contents. MadDroid leverages an automated app testing framework with a sophisticated ad view exploration strategy for effectively collecting ad-related network traffic and subsequently extracting ad contents. We then integrate dedicated approaches into the framework to identify devious ad contents. We have applied MadDroid to 40,000 Android apps and found that roughly 6% of apps deliver devious ad contents, e.g., distributing malicious apps that cannot be downloaded via traditional app markets. Experiment results indicate that devious ad contents are prevalent, suggesting that our community should invest more effort into the detection and mitigation of devious ads towards building a trustworthy mobile advertising ecosystem.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1715–1726},
numpages = {12},
keywords = {malware, Android app, mobile advertising, ad fraud},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380243,
author = {Hu, Yangyu and Wang, Haoyu and He, Ren and Li, Li and Tyson, Gareth and Castro, Ignacio and Guo, Yao and Wu, Lei and Xu, Guoai},
title = {Mobile App Squatting},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380243},
doi = {10.1145/3366423.3380243},
abstract = {Domain squatting, the adversarial tactic where attackers register domain names that mimic popular ones, has been observed for decades. However, there has been growing anecdotal evidence that this style of attack has spread to other domains. In this paper, we explore the presence of squatting attacks in the mobile app ecosystem. In “App Squatting”, attackers release apps with identifiers (e.g., app name or package name) that are confusingly similar to those of popular apps or well-known Internet brands. This paper presents the first in-depth measurement study of app squatting showing its prevalence and implications. We first identify 11 common deformation approaches of app squatters and propose “AppCrazy”, a tool for automatically generating variations of app identifiers. We have applied AppCrazy to the top-500 most popular apps in Google Play, generating 224,322 deformation keywords which we then use to test for app squatters on popular markets. Through this, we confirm the scale of the problem, identifying 10,553 squatting apps (an average of over 20 squatting apps for each legitimate one). Our investigation reveals that more than 51% of the squatting apps are malicious, with some being extremely popular (up to 10 million downloads). Meanwhile, we also find that mobile app markets have not been successful in identifying and eliminating squatting apps. Our findings demonstrate the urgency to identify and prevent app squatting abuses. To this end, we have publicly released all the identified squatting apps, as well as our tool AppCrazy. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1727–1738},
numpages = {12},
keywords = {Android, fake app, app squatting, typosquatting, malware},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380244,
author = {Coleman, Benjamin and Shrivastava, Anshumali},
title = {Sub-Linear RACE Sketches for Approximate Kernel Density Estimation on Streaming Data},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380244},
doi = {10.1145/3366423.3380244},
abstract = {Kernel density estimation is a simple and effective method that lies at the heart of many important machine learning applications. Unfortunately, kernel methods scale poorly for large, high dimensional datasets. Approximate kernel density estimation has a prohibitively high memory and computation cost, especially in the streaming setting. Recent sampling algorithms for high dimensional densities can reduce the computation cost but cannot operate online, while streaming algorithms cannot handle high dimensional datasets due to the curse of dimensionality. We propose RACE, an efficient sketching algorithm for kernel density estimation on high-dimensional streaming data. RACE compresses a set of N high dimensional vectors into tiny arrays of integer counters. These arrays are sufficient to estimate the kernel density for a large class of kernels. Our one-pass sketch is simple to implement and comes with strong theoretical guarantees. We evaluate our method on real-world high-dimensional datasets and show that our sketch achieves 10x better compression compared to existing methods.1},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1739–1749},
numpages = {11},
keywords = {compression, locality sensitive hashing, kernel density estimation, sketching, streaming data},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380245,
author = {Kolbe, Niklas and Vandenbussche, Pierre-Yves and Kubler, Sylvain and Le Traon, Yves},
title = {LOVBench: Ontology Ranking Benchmark},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380245},
doi = {10.1145/3366423.3380245},
abstract = {Ontology search and ranking are key building blocks to establish and reuse shared conceptualizations of domain knowledge on the Web. However, the effectiveness of proposed ontology ranking models is difficult to compare since these are often evaluated on diverse datasets that are limited by their static nature and scale. In this paper, we first introduce the LOVBench dataset as a benchmark for ontology term ranking. With inferred relevance judgments for more than 7000 queries, LOVBench is large enough to perform a comparison study using learning to rank (LTR) with complex ontology ranking models. Instead of relying on relevance judgments from a few experts, we consider implicit feedback from many actual users collected from the Linked Open Vocabularies (LOV) platform. Our approach further enables continuous updates of the benchmark, capturing the evolution of ontologies’ relevance in an ever-changing data community. Second, we compare the performance of several feature configurations from the literature using LOVBench in LTR settings and discuss the results in the context of the observed real-world user behavior. Our experimental results show that feature configurations which are (i) well-suited to the user behavior, (ii) cover all features types, and (iii) consider decomposition of features can significantly improve the ranking performance.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1750–1760},
numpages = {11},
keywords = {ground truth mining, semantic interoperability, ontology reuse, ontology search, learning to rank},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380246,
author = {Urbani, Jacopo and Jacobs, Ceriel},
title = {Adaptive Low-Level Storage of Very Large Knowledge Graphs},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380246},
doi = {10.1145/3366423.3380246},
abstract = {The increasing availability and usage of Knowledge Graphs (KGs) on the Web calls for scalable and general-purpose solutions to store this type of data structures. We propose Trident, a novel storage architecture for very large KGs on centralized systems. Trident uses several interlinked data structures to provide fast access to nodes and edges, with the physical storage changing depending on the topology of the graph to reduce the memory footprint. In contrast to single architectures designed for single tasks, our approach offers an interface with few low-level and general-purpose primitives that can be used to implement tasks like SPARQL query answering, reasoning, or graph analytics. Our experiments show that Trident can handle graphs with 1011 edges using inexpensive hardware, delivering competitive performance on multiple workloads. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1761–1772},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380247,
author = {Gu, Xiaotao and Mao, Yuning and Han, Jiawei and Liu, Jialu and Wu, You and Yu, Cong and Finnie, Daniel and Yu, Hongkun and Zhai, Jiaqi and Zukoski, Nicholas},
title = {Generating Representative Headlines for News Stories},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380247},
doi = {10.1145/3366423.3380247},
abstract = {Millions of news articles are published online every day, which can be overwhelming for readers to follow. Grouping articles that are reporting the same event into news stories is a common way of assisting readers in their news consumption. However, it remains a challenging research problem to efficiently and effectively generate a representative headline for each story. Automatic summarization of a document set has been studied for decades, while few studies have focused on generating representative headlines for a set of articles. Unlike summaries, which aim to capture most information with least redundancy, headlines aim to capture information jointly shared by the story articles in short length and exclude information specific to each individual article. In this work, we study the problem of generating representative headlines for news stories. We develop a distant supervision approach to train large-scale generation models without any human annotation. The proposed approach centers on two technical components. First, we propose a multi-level pre-training framework that incorporates massive unlabeled corpus with different quality-vs.-quantity balance at different levels. We show that models trained within the multi-level pre-training framework outperform those only trained with human-curated corpus. Second, we propose a novel self-voting-based article attention layer to extract salient information shared by multiple articles. We show that models that incorporate this attention layer are robust to potential noises in news stories and outperform existing baselines on both clean and noisy datasets. We further enhance our model by incorporating human labels, and show that our distant supervision approach significantly reduces the demand on labeled data. Finally, to serve the research community, we publish the first manually curated benchmark dataset on headline generation for news stories, NewSHead, which contains 367K stories (each with 3-5 articles), 6.5 times larger than the current largest multi-document summarization dataset. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1773–1784},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380248,
author = {Wang, Lu and Yu, Wenchao and He, Xiaofeng and Cheng, Wei and Ren, Martin Renqiang and Wang, Wei and Zong, Bo and Chen, Haifeng and Zha, Hongyuan},
title = {Adversarial Cooperative Imitation Learning for Dynamic Treatment Regimes✱},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380248},
doi = {10.1145/3366423.3380248},
abstract = {Recent developments in discovering dynamic treatment regimes (DTRs) have heightened the importance of deep reinforcement learning (DRL) which are used to recover the doctor’s treatment policies. However, existing DRL-based methods expose the following limitations: 1) supervised methods based on behavior cloning suffer from compounding errors; 2) the self-defined reward signals in reinforcement learning models are either too sparse or need clinical guidance; 3) only positive trajectories (e.g. survived patients) are considered in current imitation learning models, with negative trajectories (e.g. deceased patients) been largely ignored, which are examples of what not to do and could help the learned policy avoid repeating mistakes. To address these limitations, in this paper, we propose the adversarial cooperative imitation learning model, ACIL, to deduce the optimal dynamic treatment regimes that mimics the positive trajectories while differs from the negative trajectories. Specifically, two discriminators are used to help achieve this goal: an adversarial discriminator is designed to minimize the discrepancies between the trajectories generated from the policy and the positive trajectories, and a cooperative discriminator is used to distinguish the negative trajectories from the positive and generated trajectories. The reward signals from the discriminators are utilized to refine the policy for dynamic treatment regimes. Experiments on the publicly real-world medical data demonstrate that ACIL improves the likelihood of patient survival and provides better dynamic treatment regimes with the exploitation of information from both positive and negative trajectories.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1785–1795},
numpages = {11},
keywords = {reinforcement learning, dynamic treatment regimes, generative adversarial networks, imitation learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380249,
author = {Deng, Yuan and Lahaie, S\'{e}bastien and Mirrokni, Vahab and Zuo, Song},
title = {A Data-Driven Metric of Incentive Compatibility},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380249},
doi = {10.1145/3366423.3380249},
abstract = {An incentive-compatible auction incentivizes buyers to truthfully reveal their private valuations. However, many ad auction mechanisms deployed in practice are not incentive-compatible, such as first-price auctions (for display advertising) and the generalized second-price auction (for search advertising). We introduce a new metric to quantify incentive compatibility in both static and dynamic environments. Our metric is data-driven and can be computed directly through black-box auction simulations without relying on reference mechanisms or complex optimizations. We provide interpretable characterizations of our metric and prove that it is monotone in auction parameters for several mechanisms used in practice, such as soft floors and dynamic reserve prices. We empirically evaluate our metric on ad auction data from a major ad exchange and a major search engine to demonstrate its broad applicability in practice. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1796–1806},
numpages = {11},
keywords = {incentive compatibility metric, ad auction, truthfulness},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380250,
author = {Braylan, Alexander and Lease, Matthew},
title = {Modeling and Aggregation of Complex Annotations via Annotation Distances},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380250},
doi = {10.1145/3366423.3380250},
abstract = {Modeling annotators and their labels is valuable for ensuring collected data quality. Though many models have been proposed for binary or categorical labels, prior methods do not generalize to complex annotations (e.g., open-ended text, multivariate, or structured responses) without devising new models for each specific task. To obviate the need for task-specific modeling, we propose to model distances between labels, rather than the labels themselves. Our models are largely agnostic to the distance function; we leave it to the requesters to specify an appropriate distance function for their given annotation task. We propose three models of annotation quality, including a Bayesian hierarchical extension of multidimensional scaling which can be trained in an unsupervised or semi-supervised manner. Results show the generality and effectiveness of our models across diverse complex annotation tasks: sequence labeling, translation, syntactic parsing, and ranking.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1807–1818},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380251,
author = {Paul, Udit and Ermakov, Alexander and Nekrasov, Michael and Adarsh, Vivek and Belding, Elizabeth},
title = {#Outage: Detecting Power and Communication Outages from Social Networks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380251},
doi = {10.1145/3366423.3380251},
abstract = {Natural disasters are increasing worldwide at an alarming rate. To aid relief operations during and post disaster, humanitarian organizations rely on various types of situational information such as missing, trapped or injured people and damaged infrastructure in an area. Crucial and timely identification of infrastructure and utility damage is critical to properly plan and execute search and rescue operations. However, in the wake of natural disasters, real-time identification of this information becomes challenging. In this research, we investigate the use of tweets posted on the Twitter social media platform to detect power and communication outages during natural disasters. We first curate a data set of 18,097 tweets based on domain-specific keywords obtained using Latent Dirichlet Allocation. We annotate the gathered data set to separate the tweets into different types of outage-related events: power outage, communication outage and both power-communication outage. We analyze the tweets to identify information such as popular words, length of words and hashtags as well as sentiments that are associated with tweets in these outage-related categories. Furthermore, we apply machine learning algorithms to classify these tweets into their respective categories. Our results show that simple classifiers such as the boosting algorithm are able to classify outage related tweets from unrelated tweets with close to 100% f1-score. Additionally, we observe that the transfer learning model, BERT, is able to classify different categories of outage-related tweets with close to 90% accuracy in less than 90 seconds of training and testing time, demonstrating that tweets can be mined in real-time to assist first responders during natural disasters. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1819–1829},
numpages = {11},
keywords = {Social Networks, Classification., Event Detection, Natural Language Processing, Crisis Informatics, Information Extraction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380252,
author = {Harting, Tom and Mesbah, Sepideh and Lofi, Christoph},
title = {LOREM: Language-Consistent Open Relation Extraction from Unstructured Text},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380252},
doi = {10.1145/3366423.3380252},
abstract = {We introduce a Language-consistent multi-lingual Open Relation Extraction Model (LOREM) for finding relation tuples of any type between entities in unstructured texts. LOREM does not rely on language-specific knowledge or external NLP tools such as translators or PoS-taggers, and exploits information and structures that are consistent over different languages. This allows our model to be easily extended with only limited training efforts to new languages, but also provides a boost to performance for a given single language. An extensive evaluation performed on 5 languages shows that LOREM outperforms state-of-the-art mono-lingual and cross-lingual open relation extractors. Moreover, experiments on languages with no or only little training data indicate that LOREM generalizes to other languages than the languages that it is trained on.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1830–1838},
numpages = {9},
keywords = {multi-lingual relation extraction, text mining, open domain relation extraction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380253,
author = {Wang, Wen and Zhao, Han and Zhuang, Honglei and Shah, Nirav and Padman, Rema},
title = {DyCRS: Dynamic Interpretable Postoperative Complication Risk Scoring},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380253},
doi = {10.1145/3366423.3380253},
abstract = {Early identification of patients at risk for postoperative complications can facilitate timely workups and treatments and improve health outcomes. Currently, a widely-used surgical risk calculator online web system developed by the American College of Surgeons (ACS) uses patients’ static features, e.g. gender, age, to assess the risk of postoperative complications. However, the most crucial signals that reflect the actual postoperative physical conditions of patients are usually real-time dynamic signals, including the vital signs of patients (e.g., heart rate, blood pressure) collected from postoperative monitoring. In this paper, we develop a dynamic postoperative complication risk scoring framework (DyCRS) to detect the “at-risk” patients in a real-time way based on postoperative sequential vital signs and static features. DyCRS is based on adaptations of the Hidden Markov Model (HMM) that captures hidden states as well as observable states to generate a real-time, probabilistic, complication risk score. Evaluating our model using electronic health record (EHR) on elective Colectomy surgery from a major health system, we show that DyCRS significantly outperforms the state-of-the-art ACS calculator and real-time predictors with 50.16% area under precision-recall curve (AUCPRC) gain on average in terms of detection effectiveness. In terms of earliness, our DyCRS can predict 15hrs55mins earlier on average than clinician’s diagnosis with the recall of 60% and precision of 55%. Furthermore, Our DyCRS can extract interpretable patients’ stages, which are consistent with previous medical postoperative complication studies. We believe that our contributions demonstrate significant promise for developing a more accurate, robust and interpretable postoperative complication risk scoring system, which can benefit more than 50 million annual surgeries in the US by substantially lowering adverse events and healthcare costs.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1839–1850},
numpages = {12},
keywords = {Postoperative complications, Hidden Markov Model, Interpretability, Real-time risk score},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380254,
author = {Arous, Ines and Yang, Jie and Khayati, Mourad and Cudr\'{e}-Mauroux, Philippe},
title = {OpenCrowd: A Human-AI Collaborative Approach for Finding Social Influencers via Open-Ended Answers Aggregation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380254},
doi = {10.1145/3366423.3380254},
abstract = {Finding social influencers is a fundamental task in many online applications ranging from brand marketing to opinion mining. Existing methods heavily rely on the availability of expert labels, whose collection is usually a laborious process even for domain experts. Using open-ended questions, crowdsourcing provides a cost-effective way to find a large number of social influencers in a short time. Individual crowd workers, however, only possess fragmented knowledge that is often of low quality. To tackle those issues, we present OpenCrowd, a unified Bayesian framework that seamlessly incorporates machine learning and crowdsourcing for effectively finding social influencers. To infer a set of influencers, OpenCrowd bootstraps the learning process using a small number of expert labels and then jointly learns a feature-based answer quality model and the reliability of the workers. Model parameters and worker reliability are updated iteratively, allowing their learning processes to benefit from each other until an agreement on the quality of the answers is reached. We derive a principled optimization algorithm based on variational inference with efficient updating rules for learning OpenCrowd parameters. Experimental results on finding social influencers in different domains show that our approach substantially improves the state of the art by 11.5% AUC. Moreover, we empirically show that our approach is particularly useful in finding micro-influencers, who are very directly engaged with smaller audiences.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1851–1862},
numpages = {12},
keywords = {Human-AI Collaboration, Variational Inference, Influencer finding},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380255,
author = {Ovaisi, Zohreh and Ahsan, Ragib and Zhang, Yifan and Vasilaky, Kathryn and Zheleva, Elena},
title = {Correcting for Selection Bias in Learning-to-Rank Systems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380255},
doi = {10.1145/3366423.3380255},
abstract = {Click data collected by modern recommendation systems are an important source of observational data that can be utilized to train learning-to-rank (LTR) systems. However, these data suffer from a number of biases that can result in poor performance for LTR systems. Recent methods for bias correction in such systems mostly focus on position bias, the fact that higher ranked results (e.g., top search engine results) are more likely to be clicked even if they are not the most relevant results given a user’s query. Less attention has been paid to correcting for selection bias, which occurs because clicked documents are reflective of what documents have been shown to the user in the first place. Here, we propose new counterfactual approaches which adapt Heckman’s two-stage method and accounts for selection and position bias in LTR systems. Our empirical evaluation shows that our proposed methods are much more robust to noise and have better accuracy compared to existing unbiased LTR algorithms, especially when there is moderate to no position bias. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1863–1873},
numpages = {11},
keywords = {recommender systems, learning-to-rank, selection bias, position bias},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380256,
author = {Weerasinghe, Janith and Flanigan, Bailey and Stein, Aviel and McCoy, Damon and Greenstadt, Rachel},
title = {The Pod People: Understanding Manipulation of Social Media Popularity via Reciprocity Abuse},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380256},
doi = {10.1145/3366423.3380256},
abstract = {Online Social Network (OSN) Users’ demand to increase their account popularity has driven the creation of an underground ecosystem that provides services or techniques to help users manipulate content curation algorithms. One method of subversion that has recently emerged occurs when users form groups, called pods, to facilitate reciprocity abuse, where each member reciprocally interacts with content posted by other members of the group. We collect 1.8 million Instagram posts that were posted in pods hosted on Telegram. We first summarize the properties of these pods and how they are used, uncovering that they are easily discoverable by Google search and have a low barrier to entry. We then create two machine learning models for detecting Instagram posts that have gained interaction through two different kinds of pods, achieving 0.91 and 0.94 AUC, respectively. Finally, we find that pods are effective tools for increasing users’ Instagram popularity, we estimate that pod utilization leads to a significantly increased level of likely organic comment interaction on users’ subsequent posts.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1874–1884},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380257,
author = {Rosso, Paolo and Yang, Dingqi and Cudr\'{e}-Mauroux, Philippe},
title = {Beyond Triplets: Hyper-Relational Knowledge Graph Embedding for Link Prediction},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380257},
doi = {10.1145/3366423.3380257},
abstract = {Knowledge Graph (KG) embeddings are a powerful tool for predicting missing links in KGs. Existing techniques typically represent a KG as a set of triplets, where each triplet (h, r, t) links two entities h and t through a relation r, and learn entity/relation embeddings from such triplets while preserving such a structure. However, this triplet representation oversimplifies the complex nature of the data stored in the KG, in particular for hyper-relational facts, where each fact contains not only a base triplet (h, r, t), but also the associated key-value pairs (k, v). Even though a few recent techniques tried to learn from such data by transforming a hyper-relational fact into an n-ary representation (i.e., a set of key-value pairs only without triplets), they result in suboptimal models as they are unaware of the triplet structure, which serves as the fundamental data structure in modern KGs and preserves the essential information for link prediction. To address this issue, we propose HINGE, a hyper-relational KG embedding model, which directly learns from hyper-relational facts in a KG. HINGE captures not only the primary structural information of the KG encoded in the triplets, but also the correlation between each triplet and its associated key-value pairs. Our extensive evaluation shows the superiority of HINGE on various link prediction tasks over KGs. In particular, HINGE consistently outperforms not only the KG embedding methods learning from triplets only (by 0.81-41.45% depending on the link prediction tasks and settings), but also the methods learning from hyper-relational facts using the n-ary representation (by 13.2-84.1%).},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1885–1896},
numpages = {12},
keywords = {Hyper-relation, Link prediction, Knowledge graph embedding},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380258,
author = {Dai, Zhuyun and Callan, Jamie},
title = {Context-Aware Document Term Weighting for Ad-Hoc Search},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380258},
doi = {10.1145/3366423.3380258},
abstract = {Bag-of-words document representations play a fundamental role in modern search engines, but their power is limited by the shallow frequency-based term weighting scheme. This paper proposes HDCT, a context-aware document term weighting framework for document indexing and retrieval. It first estimates the semantic importance of a term in the context of each passage. These fine-grained term weights are then aggregated into a document-level bag-of-words representation, which can be stored into a standard inverted index for efficient retrieval. This paper also proposes two approaches that enable training HDCT without relevance labels. Experiments show that an index using HDCT weights significantly improved the retrieval accuracy compared to typical term-frequency and state-of-the-art embedding-based indexes. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1897–1907},
numpages = {11},
keywords = {Document Representation, Term Weighting, Neural IR},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380259,
author = {Shang, Jingbo and Zhang, Xinyang and Liu, Liyuan and Li, Sha and Han, Jiawei},
title = {NetTaxo: Automated Topic Taxonomy Construction from Text-Rich Network},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380259},
doi = {10.1145/3366423.3380259},
abstract = {The automated construction of topic taxonomies can benefit numerous applications, including web search, recommendation, and knowledge discovery. One of the major advantages of automatic taxonomy construction is the ability to capture corpus-specific information and adapt to different scenarios. To better reflect the characteristics of a corpus, we take the meta-data of documents into consideration and view the corpus as a text-rich network. In this paper, we propose NetTaxo, a novel automatic topic taxonomy construction framework, which goes beyond the existing paradigm and allows text data to collaborate with network structure. Specifically, we learn term embeddings from both text and network as contexts. Network motifs are adopted to capture appropriate network contexts. We conduct an instance-level selection for motifs, which further refines term embedding according to the granularity and semantics of each taxonomy node. Clustering is then applied to obtain sub-topics under a taxonomy node. Extensive experiments on two real-world datasets demonstrate the superiority of our method over the state-of-the-art, and further verify the effectiveness and importance of instance-level motif selection. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1908–1919},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380260,
author = {Li, Ang and Wang, Alice and Nazari, Zahra and Chandar, Praveen and Carterette, Benjamin},
title = {Do Podcasts and Music Compete with One Another? Understanding Users’ Audio Streaming Habits},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380260},
doi = {10.1145/3366423.3380260},
abstract = {Over the past decade, podcasts have been one of the fastest growing online streaming media. Many online audio streaming platforms such as Pandora, Spotify, etc. that traditionally focused on music content have started to incorporate services related to podcasts. Although incorporating new media types such as podcasts has created tremendous opportunities for these streaming platforms to expand their content offering, it also introduces new challenges. Since the functional use of podcasts and music may largely overlap for many people, the two types of content may compete with one another for the finite amount of time that users may allocate for audio streaming. As a result, incorporating podcast listening may influence and change the way users have originally consumed music. Adopting quasi-experimental techniques, the current study assesses the causal influence of adding a new class of content on user listening behavior by using large scale observational data collected from a widely used audio streaming platform. Our results demonstrate that podcast and music consumption compete slightly but do not replace one another – users open another time window to listen to podcasts. In addition, users who have added podcasts to their music listening demonstrate significantly different consumption habits for podcasts vs. music in terms of the streaming time, duration and frequency. Taking all the differences as input features to a machine learning model, we demonstrate that a podcast listening session is predictable at the start of a new listening session. Our study provides a novel contribution for online audio streaming and consumption services to understand their potential consumers and to best support their current users with an improved recommendation system.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1920–1931},
numpages = {12},
keywords = {music, listening habits, podcast},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380261,
author = {Chen, Yu and Kannan, Sampath and Khanna, Sanjeev},
title = {Near-Perfect Recovery in the One-Dimensional Latent Space Model},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380261},
doi = {10.1145/3366423.3380261},
abstract = {Suppose a graph G is stochastically created by uniformly sampling vertices along a line segment and connecting each pair of vertices with a probability that is a known decreasing function of their distance. We ask if it is possible to reconstruct the actual positions of the vertices in G by only observing the generated unlabeled graph. We study this question for two natural edge probability functions — one where the probability of an edge decays exponentially with the distance and another where this probability decays only linearly. We initiate our study with the weaker goal of recovering only the order in which vertices appear on the line segment. For a segment of length n and a precision parameter δ, we show that for both exponential and linear decay edge probability functions, there is an efficient algorithm that correctly recovers (up to reflection symmetry) the order of all vertices that are at least δ apart, using only samples (vertices). Building on this result, we then show that vertices (samples) are sufficient to additionally recover the location of each vertex on the line to within a precision of δ. We complement this result with an lower bound on samples needed for reconstructing positions (even by a computationally unbounded algorithm), showing that the task of recovering positions is information-theoretically harder than recovering the order. We give experimental results showing that our algorithm recovers the positions of almost all points with high accuracy.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1932–1942},
numpages = {11},
keywords = {sample complexity, latent space model, reconstruction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380262,
author = {Bannihatti Kumar, Vinayshekhar and Iyengar, Roger and Nisal, Namita and Feng, Yuanyuan and Habib, Hana and Story, Peter and Cherivirala, Sushain and Hagan, Margaret and Cranor, Lorrie and Wilson, Shomir and Schaub, Florian and Sadeh, Norman},
title = {Finding a Choice in a Haystack: Automatic Extraction of Opt-Out Statements from Privacy Policy Text},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380262},
doi = {10.1145/3366423.3380262},
abstract = {Website privacy policies sometimes provide users the option to opt-out of certain collections and uses of their personal data. Unfortunately, many privacy policies bury these instructions deep in their text, and few web users have the time or skill necessary to discover them. We describe a method for the automated detection of opt-out choices in privacy policy text and their presentation to users through a web browser extension. We describe the creation of two corpora of opt-out choices, which enable the training of classifiers to identify opt-outs in privacy policies. Our overall approach for extracting and classifying opt-out choices combines heuristics to identify commonly found opt-out hyperlinks with supervised machine learning to automatically identify less conspicuous instances. Our approach achieves a precision of 0.93 and a recall of 0.9. We introduce Opt-Out Easy, a web browser extension designed to present available opt-out choices to users as they browse the web. We evaluate the usability of our browser extension with a user study. We also present results of a large-scale analysis of opt-outs found in the text of thousands of the most popular websites.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1943–1954},
numpages = {12},
keywords = {Privacy, machine learning, opt-out, privacy policy, text analysis.},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380263,
author = {Kumar, Ramnath and Yadav, Shweta and Daniulaityte, Raminta and Lamy, Francois and Thirunarayan, Krishnaprasad and Lokala, Usha and Sheth, Amit},
title = {EDarkFind: Unsupervised Multi-View Learning for Sybil Account Detection},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380263},
doi = {10.1145/3366423.3380263},
abstract = {Darknet crypto markets are online marketplaces using crypto currencies (e.g., Bitcoin, Monero) and advanced encryption techniques to offer anonymity to vendors and consumers trading for illegal goods or services. The exact volume of substances advertised and sold through these crypto markets is difficult to assess, at least partially, because vendors tend to maintain multiple accounts (or Sybil accounts) within and across different crypto markets. Linking these different accounts will allow us to accurately evaluate the volume of substances advertised across the different crypto markets by each vendor. In this paper, we present a multi-view unsupervised framework (eDarkFind) that helps modeling vendor characteristics and facilitates Sybil account detection. We employ a multi-view learning paradigm to generalize and improve the performance by exploiting the diverse views from multiple rich sources such as BERT, stylometric, and location representation. Our model is further tailored to take advantage of domain-specific knowledge such as the Drug Abuse Ontology to take into consideration the substance information. We performed extensive experiments and demonstrated that the multiple views obtained from diverse sources can be effective in linking Sybil accounts. Our proposed eDarkFind model achieves an accuracy of 98% on three real-world datasets which shows the generality of the approach. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1955–1965},
numpages = {11},
keywords = {Correlation Analysis, Darknet Market, Drug Trafficker Identification, Multi-view Learning, Stylometry, Sybil Detection},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380264,
author = {Jain, Shweta and Seshadhri, C.},
title = {Provably and Efficiently Approximating Near-Cliques Using the Tur\'{a}n Shadow: PEANUTS},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380264},
doi = {10.1145/3366423.3380264},
abstract = {Clique and near-clique counts are important graph properties with applications in graph generation, graph modeling, graph analytics, community detection among others. They are the archetypal examples of dense subgraphs. While there are several different definitions of near-cliques, most of them share the attribute that they are cliques that are missing a small number of edges. Clique counting is itself considered a challenging problem. Counting near-cliques is significantly harder more so since the search space for near-cliques is orders of magnitude larger than that of cliques. We give a formulation of a near-clique as a clique that is missing a constant number of edges. We exploit the fact that a near-clique contains a smaller clique, and use techniques for clique sampling to count near-cliques. This method allows us to count near-cliques with 1 or 2 missing edges, in graphs with tens of millions of edges. To the best of our knowledge, there was no known efficient method for this problem, and we obtain a 10x − 100x speedup over existing algorithms for counting near-cliques. Our main technique is a space efficient adaptation of the Tur\'{a}n Shadow sampling approach, recently introduced by Jain and Seshadhri (WWW 2017). This approach constructs a large recursion tree (called the Tur\'{a}n Shadow) that represents cliques in a graph. We design a novel algorithm that builds an estimator for near-cliques, using a online, compact construction of the Tur\'{a}n Shadow. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1966–1976},
numpages = {11},
keywords = {near-cliques, graphs, defective-cliques, Tur\'{a}n Shadow, sampling, Cliques},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380265,
author = {Dell'Aglio, Daniele and Bernstein, Abraham},
title = {Differentially Private Stream Processing for the Semantic Web},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380265},
doi = {10.1145/3366423.3380265},
abstract = {Data often contains sensitive information, which poses a major obstacle to publishing it. Some suggest to obfuscate the data or only releasing some data statistics. These approaches have, however, been shown to provide insufficient safeguards against de-anonymisation. Recently, differential privacy (DP), an approach that injects noise into the query answers to provide statistical privacy guarantees, has emerged as a solution to release sensitive data. This study investigates how to continuously release privacy-preserving histograms (or distributions) from online streams of sensitive data by combining DP and semantic web technologies. We focus on distributions, as they are the basis for many analytic applications. Specifically, we propose SihlQL, a query language that processes RDF streams in a privacy-preserving fashion. SihlQL builds on top of SPARQL and the w-event DP framework. We show how some peculiarities of w-event privacy constrain the expressiveness of SihlQL queries. Addressing these constraints, we propose an extension of w-event privacy that provides answers to a larger class of queries while preserving their privacy. To evaluate SihlQL, we implemented a prototype engine that compiles queries to Apache Flink topologies and studied its privacy properties using real-world data from an IPTV provider and an online e-commerce web site.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1977–1987},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380266,
author = {Tan, Qiaoyu and Liu, Ninghao and Zhao, Xing and Yang, Hongxia and Zhou, Jingren and Hu, Xia},
title = {Learning to Hash with Graph Neural Networks for Recommender Systems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380266},
doi = {10.1145/3366423.3380266},
abstract = {Recommender systems in industry generally include two stages: recall and ranking. Recall refers to efficiently identify hundreds of candidate items that user may interest in from a large volume of item corpus, while the latter aims to output a precise ranking list using complex ranking models. Recently, graph representation learning has attracted much attention in supporting high quality candidate search at scale. Despite its effectiveness in learning embedding vectors for objects in the user-item interaction network, the computational costs to infer users’ preferences in continuous embedding space are tremendous. In this work, we investigate the problem of hashing with graph neural networks (GNNs) for high quality retrieval, and propose a simple yet effective discrete representation learning framework to jointly learn continuous and discrete codes. Specifically, a deep hashing with GNNs (HashGNN) is presented, which consists of two components, a GNN encoder for learning node representations, and a hash layer for encoding representations to hash codes. The whole architecture is trained end-to-end by jointly optimizing two losses, i.e., reconstruction loss from reconstructing observed links, and ranking loss from preserving the relative ordering of hash codes. A novel discrete optimization strategy based on straight through estimator (STE) with guidance is proposed. The principal idea is to avoid gradient magnification in back-propagation of STE with continuous embedding guidance, in which we begin from learning an easier network that mimic the continuous embedding and let it evolve during the training until it finally goes back to STE. Comprehensive experiments over three publicly available and one real-world Alibaba company datasets demonstrate that our model not only can achieve comparable performance compared with its continuous counterpart but also runs multiple times faster during inference. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1988–1998},
numpages = {11},
keywords = {Network embedding, Hierarchical retrieval, Unsupervised hashing, Discrete representation learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380267,
author = {Lo, Chun and de Longueau, Emilie and Saha, Ankan and Chatterjee, Shaunak},
title = {Edge Formation in Social Networks to Nurture Content Creators},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380267},
doi = {10.1145/3366423.3380267},
abstract = {Social networks act as major content marketplaces where creators and consumers come together to share and consume various kinds of content. Content ranking applications (e.g., newsfeed, moments, notifications) and edge recommendation products (e.g., connect to members, follow celebrities or groups or hashtags) on such platforms aim at improving the consumer experience. In this work, we focus on the creator experience and specifically on improving edge recommendations to better serve creators in such ecosystems. The audience and reach of creators – individuals, celebrities, publishers and companies – are critically shaped by these edge recommendation products. Hence, incorporating creator utility in such recommendations can have a material impact on their success, and in turn, on the marketplace. In this paper, we (i) propose a general framework to incorporate creator utility in edge recommendations, (ii) devise a specific method to estimate edge-level creator utilities for currently unformed edges, (iii) outline the challenges of measurement and propose a practical experiment design, and finally (iv) discuss the implementation of our proposal at scale on LinkedIn, a professional network with 645M+ members, and report our findings. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {1999–2008},
numpages = {10},
keywords = {Network Measurement, Content Marketplace, Statistical Modeling, Social Network},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380268,
author = {Vedula, Nikhita and Lipka, Nedim and Maneriker, Pranav and Parthasarathy, Srinivasan},
title = {Open Intent Extraction from Natural Language Interactions},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380268},
doi = {10.1145/3366423.3380268},
abstract = {Accurately discovering user intents from their written or spoken language plays a critical role in natural language understanding and automated dialog response. Most existing research models this as a classification task with a single intent label per utterance, grouping user utterances into a single intent type from a set of categories known beforehand. Going beyond this formulation, we define and investigate a new problem of open intent discovery. It involves discovering one or more generic intent types from text utterances, that may not have been encountered during training. We propose a novel domain-agnostic approach, OPINE, which formulates the problem as a sequence tagging task under an open-world setting. It employs a CRF on top of a bidirectional LSTM to extract intents in a consistent format, subject to constraints among intent tag labels. We apply a multi-head self-attention mechanism to effectively learn dependencies between distant words. We further use adversarial training to improve performance and robustly adapt our model across varying domains. Finally, we curate and plan to release an open intent annotated dataset of 25K real-life utterances spanning diverse domains. Extensive experiments show that our approach outperforms state-of-the-art baselines by 5-15% F1 score points. We also demonstrate the efficacy of OPINE in recognizing multiple, diverse domain intents with limited (can also be zero) training examples per unique domain. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2009–2020},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380269,
author = {Hung, Hui-Ju and Lee, Wang-Chien and Yang, De-Nian and Shen, Chih-Ya and Lei, Zhen and Chow, Sy-Miin},
title = {Efficient Algorithms towards Network Intervention},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380269},
doi = {10.1145/3366423.3380269},
abstract = {Research suggests that social relationships have substantial impacts on individuals’ health outcomes. Network intervention, through careful planning, can assist a network of users to build healthy relationships. However, most previous work is not designed to assist such planning by carefully examining and improving multiple network characteristics. In this paper, we propose and evaluate algorithms that facilitate network intervention planning through simultaneous optimization of network degree, closeness, betweenness, and local clustering coefficient, under scenarios involving Network Intervention with Limited Degradation - for Single target (NILD-S) and Network Intervention with Limited Degradation - for Multiple targets (NILD-M). We prove that NILD-S and NILD-M are NP-hard and cannot be approximated within any ratio in polynomial time unless P=NP. We propose the Candidate Re-selection with Preserved Dependency (CRPD) algorithm for NILD-S, and the Objective-aware Intervention edge Selection and Adjustment (OISA) algorithm for NILD-M. Various pruning strategies are designed to boost the efficiency of the proposed algorithms. Extensive experiments on various real social networks collected from public schools and Web and an empirical study are conducted to show that CRPD and OISA outperform the baselines in both efficiency and effectiveness.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2021–2031},
numpages = {11},
keywords = {Network intervention, social networks, optimization algorithms},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380270,
author = {Liu, Bang and Wei, Haojie and Niu, Di and Chen, Haolan and He, Yancheng},
title = {Asking Questions the Human Way: Scalable Question-Answer Generation from Text Corpus},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380270},
doi = {10.1145/3366423.3380270},
abstract = {The ability to ask questions is important in both human and machine intelligence. Learning to ask questions helps knowledge acquisition, improves question-answering and machine reading comprehension tasks, and helps a chatbot to keep the conversation flowing with a human. Existing question generation models are ineffective at generating a large amount of high-quality question-answer pairs from unstructured text, since given an answer and an input passage, question generation is inherently a one-to-many mapping. In this paper, we propose Answer-Clue-Style-aware Question Generation (ACS-QG), which aims at automatically generating high-quality and diverse question-answer pairs from unlabeled text corpus at scale by imitating the way a human asks questions. Our system consists of: i) an information extractor, which samples from the text multiple types of assistive information to guide question generation; ii) neural question generators, which generate diverse and controllable questions, leveraging the extracted assistive information; and iii) a neural quality controller, which removes low-quality generated data based on text entailment. We compare our question generation models with existing approaches and resort to voluntary human evaluation to assess the quality of the generated question-answer pairs. The evaluation results suggest that our system dramatically outperforms state-of-the-art neural question generation models in terms of the generation quality, while being scalable in the meantime. With models trained on a relatively smaller amount of data, we can generate 2.8 million quality-assured question-answer pairs from a million sentences found in Wikipedia.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2032–2043},
numpages = {12},
keywords = {Question Generation, Machine Reading Comprehension, Sequence-to-Sequence},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380271,
author = {Manzoor, Emaad and Li, Rui and Shrouty, Dhananjay and Leskovec, Jure},
title = {Expanding Taxonomies with Implicit Edge Semantics},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380271},
doi = {10.1145/3366423.3380271},
abstract = {Curated taxonomies enhance the performance of machine-learning systems via high-quality structured knowledge. However, manually curating a large and rapidly-evolving taxonomy is infeasible. In this work, we propose Arborist, an approach to automatically expand textual taxonomies by predicting the parents of new taxonomy nodes. Unlike previous work, Arborist handles the more challenging scenario of taxonomies with heterogeneous edge semantics that are unobserved. Arborist learns latent representations of the edge semantics along with embeddings of the taxonomy nodes to measure taxonomic relatedness between node pairs. Arborist is then trained by optimizing a large-margin ranking loss with a dynamic margin function. We propose a principled formulation of the margin function, which theoretically guarantees that Arborist minimizes an upper-bound on the shortest-path distance between the predicted parents and actual parents in the taxonomy. Via extensive evaluation on a curated taxonomy at Pinterest and several public datasets, we demonstrate that Arborist outperforms the state-of-the-art, achieving up to 59% in mean reciprocal rank and 83% in recall at 15. We also explore the ability of Arborist to infer nodes’ taxonomic-roles, without explicit supervision on this task. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2044–2054},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380272,
author = {Maldeniya, Danaja and Budak, Ceren and Robert Jr., Lionel P. and Romero, Daniel M.},
title = {Herding a Deluge of Good Samaritans: How GitHub Projects Respond to Increased Attention},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380272},
doi = {10.1145/3366423.3380272},
abstract = {Collaborative crowdsourcing is a well-established model of work, especially in the case of open source software development. The structure and operation of these virtual and loosely-knit teams differ from traditional organizations. As such, little is known about how their behavior may change in response to an increase in external attention. To understand these dynamics, we analyze millions of actions of thousands of contributors in over 1100 open source software projects that topped the GitHub Trending Projects page and thus experienced a large increase in attention, in comparison to a control group of projects identified through propensity score matching. In carrying out our research, we use the lens of organizational change, which considers the challenges teams face during rapid growth and how they adapt their work routines, organizational structure, and management style. We show that trending results in an explosive growth in the effective team size. However, most newcomers make only shallow and transient contributions. In response, the original team transitions towards administrative roles, responding to requests and reviewing work done by newcomers. Projects evolve towards a more distributed coordination model with newcomers becoming more central, albeit in limited ways. Additionally, teams become more modular with subgroups specializing in different aspects of the project. We discuss broader implications for collaborative crowdsourcing teams that face attention shocks. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2055–2065},
numpages = {11},
keywords = {PSM, attention shocks, crowdsourcing, GitHub, coordination},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380273,
author = {Chang, Jonathan P. and Cheng, Justin and Danescu-Niculescu-Mizil, Cristian},
title = {Don’t Let Me Be Misunderstood:Comparing Intentions and Perceptions in Online Discussions},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380273},
doi = {10.1145/3366423.3380273},
abstract = {Discourse involves two perspectives: a person’s intention in making an utterance and others’ perception of that utterance. The misalignment between these perspectives can lead to undesirable outcomes, such as misunderstandings, low productivity and even overt strife. In this work, we present a computational framework for exploring and comparing both perspectives in online public discussions. We combine logged data about public comments on Facebook with a survey of over 16,000 people about their intentions in writing these comments or about their perceptions of comments that others had written. Unlike previous studies of online discussions that have largely relied on third-party labels to quantify properties such as sentiment and subjectivity, our approach also directly captures what the speakers actually intended when writing their comments. In particular, our analysis focuses on judgments of whether a comment is stating a fact or an opinion, since these concepts were shown to be often confused. We show that intentions and perceptions diverge in consequential ways. People are more likely to perceive opinions than to intend them, and linguistic cues that signal how an utterance is intended can differ from those that signal how it will be perceived. Further, this misalignment between intentions and perceptions can be linked to the future health of a conversation: when a comment whose author intended to share a fact is misperceived as sharing an opinion, the subsequent conversation is more likely to derail into uncivil behavior than when the comment is perceived as intended. Altogether, these findings may inform the design of discussion platforms that better promote positive interactions. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2066–2077},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380274,
author = {Xiao, Yuxin and Krishnan, Adit and Sundaram, Hari},
title = {Discovering Strategic Behaviors for Collaborative Content-Production in Social Networks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380274},
doi = {10.1145/3366423.3380274},
abstract = {Some social networks provide explicit mechanisms to allocate social rewards such as reputation based on users’ actions, while the mechanism is more opaque in other networks. Nonetheless, there are always individuals who obtain greater rewards and reputation than their peers. An intuitive yet important question to ask is whether these successful users employ strategic behaviors to become influential. It might appear that the influencers ”have gamed the system.” However, it remains difficult to conclude the rationality of their actions due to factors like the combinatorial strategy space, inability to determine payoffs, and resource limitations faced by individuals. The challenging nature of this question has drawn attention from both the theory and data mining communities. Therefore, in this paper, we are motivated to investigate if resource-limited individuals discover strategic behaviors associated with high payoffs when producing collaborative/interactive content in social networks. We propose a novel framework of Dynamic Dual Attention Networks (DDAN) which models individuals’ content production strategies through a generative process, under the influence of social interactions involved in the process. Extensive experimental results illustrate the model’s effectiveness in user behavior modeling. We make three strong empirical findings: (1) Different strategies give rise to different social payoffs; (2) The best performing individuals exhibit stability in their preference over the discovered strategies, which indicates the emergence of strategic behavior; and (3) The stability of a user’s preference is correlated with high payoffs.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2078–2088},
numpages = {11},
keywords = {Social Network Analysis, Strategic Behavior Modeling},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380275,
author = {Stoica, Ana-Andreea and Han, Jessy Xinyi and Chaintreau, Augustin},
title = {Seeding Network Influence in Biased Networks and the Benefits of Diversity},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380275},
doi = {10.1145/3366423.3380275},
abstract = {The problem of social influence maximization is widely applicable in designing viral campaigns, news dissemination, or medical aid. State-of-the-art algorithms often select “early adopters” that are most central in a network unfortunately mirroring or exacerbating historical biases and leaving under-represented communities out of the loop. Through a theoretical model of biased networks, we characterize the intricate relationship between diversity and efficiency, which sometimes may be at odds but may also reinforce each other. Most importantly, we find a mathematically proven analytical condition under which more equitable choices of early adopters lead simultaneously to fairer outcomes and larger outreach. Analysis of data on the DBLP network confirms that our condition is often met in real networks. We design and test a set of algorithms leveraging the network structure to optimize the diffusion of a message while avoiding to create disparate impact among participants based on their demographics, such as gender or race.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2089–2098},
numpages = {10},
keywords = {social networks, influence, fairness, graph algorithms},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380276,
author = {Ramseyer, Geoffrey and Goel, Ashish and Mazi\`{e}res, David},
title = {Liquidity in Credit Networks with Constrained Agents},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380276},
doi = {10.1145/3366423.3380276},
abstract = {In order to scale transaction rates for deployment across the global web, many cryptocurrencies have deployed so-called ”Layer-2” networks of private payment channels. An idealized payment network behaves like a Credit Network, a model for transactions across a network of bilateral trust relationships. Credit Networks capture many aspects of traditional currencies as well as new virtual currencies and payment mechanisms. In the traditional credit network model, if an agent defaults, every other node that trusted it is vulnerable to loss. In a cryptocurrency context, trust is manufactured by capital deposits, and thus there arises a natural tradeoff between network liquidity (i.e. the fraction of transactions that succeed) and the cost of capital deposits. In this paper, we introduce constraints that bound the total amount of loss that the rest of the network can suffer if an agent (or a set of agents) were to default - equivalently, how the network changes if agents can support limited solvency guarantees. We show that these constraints preserve the analytical structure of a credit network. Furthermore, we show that aggregate borrowing constraints greatly simplify the network structure and in the payment network context achieve the optimal tradeoff between liquidity and amount of escrowed capital. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2099–2108},
numpages = {10},
keywords = {Credit Networks, Trust, Electronic Fund Transfer},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380277,
author = {Farinholt, Brown and Rezaeirad, Mohammad and McCoy, Damon and Levchenko, Kirill},
title = {Dark Matter: Uncovering the DarkComet RAT Ecosystem},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380277},
doi = {10.1145/3366423.3380277},
abstract = {Remote Access Trojans (RATs) are a persistent class of malware that give an attacker direct, interactive access to a victim’s personal computer, allowing the attacker to steal private data, spy on the victim in real-time using the camera and microphone, and verbally harass the victim through the speaker. To date, the users and victims of this pernicious form of malware have been challenging to observe in the wild due to the unobtrusive nature of infections. In this work, we report the results of a longitudinal study of the DarkComet RAT ecosystem. Using a known method for collecting victim log databases from DarkComet controllers, we present novel techniques for tracking RAT controllers across hostname changes and improve on established techniques for filtering spurious victim records caused by scanners and sandboxed malware executions. We downloaded 6,620 DarkComet databases from 1,029 unique controllers spanning over 5 years of operation. Our analysis shows that there have been at least 57,805 victims of DarkComet over this period, with 69 new victims infected every day; many of whose keystrokes have been captured, actions recorded, and webcams monitored during this time. Our methodologies for more precisely identifying campaigns and victims could potentially be useful for improving the efficiency and efficacy of victim cleanup efforts and prioritization of law enforcement investigations. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2109–2120},
numpages = {12},
keywords = {malware, remote access trojan, security},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380278,
author = {Meng, Yu and Huang, Jiaxin and Wang, Guangyuan and Wang, Zihan and Zhang, Chao and Zhang, Yu and Han, Jiawei},
title = {Discriminative Topic Mining via Category-Name Guided Text Embedding},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380278},
doi = {10.1145/3366423.3380278},
abstract = {Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2121–2132},
numpages = {12},
keywords = {Text Classification, Topic Mining, Discriminative Analysis, Text Embedding},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380279,
author = {Barbosa, Nat\~{a} M. and Sun, Emily and Antin, Judd and Parigi, Paolo},
title = {Designing for Trust: A Behavioral Framework for Sharing Economy Platforms},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380279},
doi = {10.1145/3366423.3380279},
abstract = {Trust is a fundamental prerequisite in the growth and sustainability of sharing economy platforms. Many of such platforms rely on actions that require trust to take place, such as entering a stranger’s car or sleeping at a stranger’s place. For this reason, understanding, measuring, and tracking trust can be of great benefit to such platforms, enabling them to identify trust behaviors, both online and offline, and identify groups which may benefit from trust-building interventions. In this work, we present the design and evaluation of a behavioral framework to measure a user’s propensity to trust others on Airbnb. We conducted an online experiment with 4,499 Airbnb users in the form of an investment game in order to capture users’ propensity to trust other users on Airbnb. Then, we used the experimental data to generate both explanatory and predictive models of trust propensity. Our contribution is a framework that can be used to measure trust propensity in sharing economy platforms via online and offline signals. We discuss which affordances need to be in place so that sharing economy platforms can get signals of trust, in addition to how such a framework can be used to inform design around trust in the short and long term. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2133–2143},
numpages = {11},
keywords = {sharing economy, trust, modeling, Airbnb},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380280,
author = {Wang, Pengyang and Gui, Jiaping and Chen, Zhengzhang and Rhee, Junghwan and Chen, Haifeng and Fu, Yanjie},
title = {A Generic Edge-Empowered Graph Convolutional Network via Node-Edge Mutual Enhancement},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380280},
doi = {10.1145/3366423.3380280},
abstract = {Graph Convolutional Networks (GCNs) have shown to be a powerful tool for analyzing graph-structured data. Most of previous GCN methods focus on learning a good node representation by aggregating the representations of neighboring nodes, whereas largely ignoring the edge information. Although few recent methods have been proposed to integrate edge attributes into GCNs to initialize edge embeddings, these methods do not work when edge attributes are (partially) unavailable. Can we develop a generic edge-empowered framework to exploit node-edge enhancement, regardless of the availability of edge attributes? In this paper, we propose a novel framework EE-GCN&nbsp;that achieves node-edge enhancement. In particular, the framework EE-GCN&nbsp;includes three key components: (i) Initialization: this step is to initialize the embeddings of both nodes and edges. Unlike node embedding initialization, we propose a line graph-based method to initialize the embedding of edges regardless of edge attributes. (ii) Feature space alignment: we propose a translation-based mapping method to align edge embedding with node embedding space, and the objective function is penalized by a translation loss when both spaces are not aligned. (iii) Node-edge mutually enhanced updating: node embedding is updated by aggregating embedding of neighboring nodes and associated edges, while edge embedding is updated by the embedding of associated nodes and itself. Through the above improvements, our framework provides a generic strategy for all of the spatial-based GCNs to allow edges to participate in embedding computation and exploit node-edge mutual enhancement. Finally, we present extensive experimental results to validate the improved performances of our method in terms of node classification, link prediction, and graph classification.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2144–2154},
numpages = {11},
keywords = {Node-Edge Enhancement, Graph Representation Learning, Edge Embedding, Node Embedding, Graph Convolutional Networks},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380281,
author = {Anderson, Ashton and Maystre, Lucas and Anderson, Ian and Mehrotra, Rishabh and Lalmas, Mounia},
title = {Algorithmic Effects on the Diversity of Consumption on Spotify},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380281},
doi = {10.1145/3366423.3380281},
abstract = {On many online platforms, users can engage with millions of pieces of content, which they discover either organically or through algorithmically-generated recommendations. While the short-term benefits of recommender systems are well-known, their long-term impacts are less well understood. In this work, we study the user experience on Spotify, a popular music streaming service, through the lens of diversity—the coherence of the set of songs a user listens to. We use a high-fidelity embedding of millions of songs based on listening behavior on Spotify to quantify how musically diverse every user is, and find that high consumption diversity is strongly associated with important long-term user metrics, such as conversion and retention. However, we also find that algorithmically-driven listening through recommendations is associated with reduced consumption diversity. Furthermore, we observe that when users become more diverse in their listening over time, they do so by shifting away from algorithmic consumption and increasing their organic consumption. Finally, we deploy a randomized experiment and show that algorithmic recommendations are more effective for users with lower diversity. Our work illuminates a central tension in online platforms: how do we recommend content that users are likely to enjoy in the short term while simultaneously ensuring they can remain diverse in their consumption in the long term?},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2155–2165},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380282,
author = {Zhou, Wenxuan and Lin, Hongtao and Lin, Bill Yuchen and Wang, Ziqi and Du, Junyi and Neves, Leonardo and Ren, Xiang},
title = {NERO: A Neural Rule Grounding Framework for Label-Efficient Relation Extraction},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380282},
doi = {10.1145/3366423.3380282},
abstract = {Deep neural models for relation extraction tend to be less reliable when perfectly labeled data is limited, despite their success in label-sufficient scenarios. Instead of seeking more instance-level labels from human annotators, here we propose to annotate frequent surface patterns to form labeling rules. These rules can be automatically mined from large text corpora and generalized via a soft rule matching mechanism. Prior works use labeling rules in an exact matching fashion, which inherently limits the coverage of sentence matching and results in the low-recall issue. In this paper, we present a neural approach to ground rules for RE, named Nero, which jointly learns a relation extraction module and a soft matching module. One can employ any neural relation extraction models as the instantiation for the RE module. The soft matching module learns to match rules with semantically similar sentences such that raw corpora can be automatically labeled and leveraged by the RE module (in a much better coverage) as augmented supervision, in addition to the exactly matched sentences. Extensive experiments and analysis on two public and widely-used datasets demonstrate the effectiveness of the proposed Nero framework, comparing with both rule-based and semi-supervised methods. Through user studies, we find that the time efficiency for a human to annotate rules and sentences are similar (0.30 vs. 0.35 min per label). In particular, Nero’s performance using 270 rules is comparable to the models trained using 3,000 labeled sentences, yielding a 9.5x speedup. Moreover, Nero can predict for unseen relations at test time and provide interpretable predictions. We release our code1 to the community for future research.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2166–2176},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380283,
author = {Jiang, Jyun-Yu and Chen, Patrick H. and Hsieh, Cho-Jui and Wang, Wei},
title = {Clustering and Constructing User Coresets to Accelerate Large-Scale Top-K Recommender Systems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380283},
doi = {10.1145/3366423.3380283},
abstract = {Top-K recommender systems aim to generate few but satisfactory personalized recommendations for various practical applications, such as item recommendation for e-commerce and link prediction for social networks. However, the numbers of users and items can be enormous, thereby leading to myriad potential recommendations as well as the bottleneck in evaluating and ranking all possibilities. Existing Maximum Inner Product Search (MIPS) based methods treat the item ranking problem for each user independently and the relationship between users has not been explored. In this paper, we propose a novel model for clustering and navigating for top-K recommenders (CANTOR) to expedite the computation of top-K recommendations based on latent factor models. A clustering-based framework is first presented to leverage user relationships to partition users into affinity groups, each of which contains users with similar preferences. CANTOR then derives a coreset of representative vectors for each affinity group by constructing a set cover with a theoretically guaranteed difference to user latent vectors. Using these representative vectors in the coreset, approximate nearest neighbor search is then applied to obtain a small set of candidate items for each affinity group to be used when computing recommendations for each user in the affinity group. This approach can significantly reduce the computation without compromising the quality of the recommendations. Extensive experiments are conducted on six publicly available large-scale real-world datasets for item recommendation and personalized link prediction. The experimental results demonstrate that CANTOR significantly speeds up matrix factorization models with high precision. For instance, CANTOR can achieve 355.1x speedup for inferring recommendations in a million-user network with 99.5% precision@1 to the original system while the state-of-the-art method can only obtain 93.7x speedup with 99.0% precision@1. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2177–2187},
numpages = {11},
keywords = {Approximate nearest neighbor search *Equal contribution., Large-scale top-K recommender systems, Latent factor models},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380284,
author = {Huang, Jiaxin and Xie, Yiqing and Meng, Yu and Shen, Jiaming and Zhang, Yunyi and Han, Jiawei},
title = {Guiding Corpus-Based Set Expansion by Auxiliary Sets Generation and Co-Expansion},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380284},
doi = {10.1145/3366423.3380284},
abstract = {Given a small set of seed entities (e.g., “USA”, “Russia”), corpus-based set expansion is to induce an extensive set of entities which share the same semantic class (Country in this example) from a given corpus. Set expansion benefits a wide range of downstream applications in knowledge discovery, such as web search, taxonomy construction, and query suggestion. Existing corpus-based set expansion algorithms typically bootstrap the given seeds by incorporating lexical patterns and distributional similarity. However, due to no negative sets provided explicitly, these methods suffer from semantic drift caused by expanding the seed set freely without guidance. We propose a new framework, Set-CoExpan, that automatically generates auxiliary sets as negative sets that are closely related to the target set of user’s interest, and then performs multiple sets co-expansion that extracts discriminative features by comparing target set with auxiliary sets, to form multiple cohesive sets that are distinctive from one another, thus resolving the semantic drift issue. In this paper we demonstrate that by generating auxiliary sets, we can guide the expansion process of target set to avoid touching those ambiguous areas around the border with auxiliary sets, and we show that Set-CoExpan outperforms strong baseline methods significantly.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2188–2198},
numpages = {11},
keywords = {Set Expansion, Web Mining, Bootstrap Methods, Semantic Computing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380285,
author = {Wu, Jibang and Cai, Renqin and Wang, Hongning},
title = {D\'{e}J\`{a} vu: A Contextualized Temporal Attention Mechanism for Sequential Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380285},
doi = {10.1145/3366423.3380285},
abstract = {Predicting users’ preferences based on their sequential behaviors in history is challenging and crucial for modern recommender systems. Most existing sequential recommendation algorithms focus on transitional structure among the sequential actions, but largely ignore the temporal and context information, when modeling the influence of a historical event to current prediction. In this paper, we argue that the influence from the past events on a user’s current action should vary over the course of time and under different context. Thus, we propose a Contextualized Temporal Attention Mechanism that learns to weigh historical actions’ influence on not only what action it is, but also when and how the action took place. More specifically, to dynamically calibrate the relative input dependence from the self-attention mechanism, we deploy multiple parameterized kernel functions to learn various temporal dynamics, and then use the context information to determine which of these reweighing kernels to follow for each input. In empirical evaluations on two large public recommendation datasets, our model consistently outperformed an extensive set of state-of-the-art sequential recommendation methods.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2199–2209},
numpages = {11},
keywords = {Attention Mechanism, Context, Neural Recommender System, Temporal Dynamics, Sequential Recommendation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380286,
author = {Wang, Chang and Wang, Bang},
title = {An End-to-End Topic-Enhanced Self-Attention Network for Social Emotion Classification},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380286},
doi = {10.1145/3366423.3380286},
abstract = {Social emotion classification is to predict the distribution of different emotions evoked by an article among its readers. Prior studies have shown that document semantic and topical features can help improve classification performance. However, how to effectively extract and jointly exploit such features have not been well researched. In this paper, we propose an end-to-end topic-enhanced self-attention network (TESAN) that jointly encodes document semantics and extracts document topics. In particular, TESAN first constructs a neural topic model to learn topical information and generates a topic embedding for a document. We then propose a topic-enhanced self-attention mechanism to encode semantic and topical information into a document vector. Finally, a fusion gate is used to compose the document representation for emotion classification by integrating the document vector and the topic embedding. The entire TESAN is trained in an end-to-end manner. Experimental results on three public datasets reveal that TESAN outperforms the state-of-the-art schemes in terms of higher classification accuracy and higher average Pearson correlation coefficient. Furthermore, the TESAN is computation efficient and can generate more coherent topics.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2210–2219},
numpages = {10},
keywords = {neural topic model, social emotion classification, self-attention},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380287,
author = {Xu, Zhe and Men, Chang and Li, Peng and Jin, Bicheng and Li, Ge and Yang, Yue and Liu, Chunyang and Wang, Ben and Qie, Xiaohu},
title = {When Recommender Systems Meet Fleet Management: Practical Study in Online Driver Repositioning System},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380287},
doi = {10.1145/3366423.3380287},
abstract = {E-hailing platforms have become an important component of public transportation in recent years. The supply (online drivers) and demand (passenger requests) are intrinsically imbalanced because of the pattern of human behavior, especially in time and locations such as peak hours and train stations. Hence, how to balance supply and demand is one of the key problems to satisfy passengers and drivers and increase social welfare. As an intuitive and effective approach to address this problem, driver repositioning has been employed by some real-world e-hailing platforms. In this paper, we describe a novel framework of driver repositioning system, which meets various requirements in practical situations, including robust driver experience satisfaction and multi-driver collaboration. We introduce an effective and user-friendly driver interaction design called “driver repositioning task”. A novel modularized algorithm is developed to generate the repositioning tasks in real time. To our knowledge, this is the first industry-level application of driver repositioning. We evaluate the proposed method in real-world experiments, achieving a 2% improvement of driver income. Our framework has been fully deployed in the online system of DiDi Chuxing and serves millions of drivers on a daily basis. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2220–2229},
numpages = {10},
keywords = {recommender system, driver repositioning, fleet management},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380288,
author = {Zhou, Dawei and Zheng, Lecheng and Zhu, Yada and Li, Jianbo and He, Jingrui},
title = {Domain Adaptive Multi-Modality Neural Attention Network for Financial Forecasting},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380288},
doi = {10.1145/3366423.3380288},
abstract = {Financial time series analysis plays a central role in optimizing investment decision and hedging market risks. This is a challenging task as the problems are always accompanied by dual-level (i.e, data-level and task-level) heterogeneity. For instance, in stock price forecasting, a successful portfolio with bounded risks usually consists of a large number of stocks from diverse domains (e.g, utility, information technology, healthcare, etc.), and forecasting stocks in each domain can be treated as one task; within a portfolio, each stock is characterized by temporal data collected from multiple modalities (e.g, finance, weather, and news), which corresponds to the data-level heterogeneity. Furthermore, the finance industry follows highly regulated processes, which require prediction models to be interpretable, and the output results to meet compliance. Therefore, a natural research question is how to build a model that can achieve satisfactory performance on such multi-modality multi-task learning problems, while being able to provide comprehensive explanations for the end users.To answer this question, in this paper, we propose a generic time series forecasting framework named Dandelion, which leverages the consistency of multiple modalities and explores the relatedness of multiple tasks using a deep neural network. In addition, to ensure the interpretability of the framework, we integrate a novel trinity attention mechanism, which allows the end users to investigate the variable importance over three dimensions (i.e, tasks, modality and time). Extensive empirical results demonstrate that Dandelion achieves superior performance for financial market prediction across 396 stocks from 4 different domains over the past 15 years. In particular, two interesting case studies show the efficacy of Dandelion in terms of its profitability performance, and the interpretability of output results to end users. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2230–2240},
numpages = {11},
keywords = {Time Series Forecasting, Interpretable Machine Learning, Heterogeneous Learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380289,
author = {Zhu, Qi and Wei, Hao and Sisman, Bunyamin and Zheng, Da and Faloutsos, Christos and Dong, Xin Luna and Han, Jiawei},
title = {Collective Multi-Type Entity Alignment Between Knowledge Graphs},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380289},
doi = {10.1145/3366423.3380289},
abstract = {Knowledge graph (e.g. Freebase, YAGO) is a multi-relational graph representing rich factual information among entities of various types. Entity alignment is the key step towards knowledge graph integration from multiple sources. It aims to identify entities across different knowledge graphs that refer to the same real world entity. However, current entity alignment systems overlook the sparsity of different knowledge graphs and can not align multi-type entities by one single model. In this paper, we present a Collective Graph neural network for Multi-type entity Alignment, called CG-MuAlign. Different from previous work, CG-MuAlign jointly aligns multiple types of entities, collectively leverages the neighborhood information and generalizes to unlabeled entity types. Specifically, we propose novel collective aggregation function tailored for this task, that (1) relieves the incompleteness of knowledge graphs via both cross-graph and self attentions, (2) scales up efficiently with mini-batch training paradigm and effective neighborhood sampling strategy. We conduct experiments on real world knowledge graphs with millions of entities and observe the superior performance beyond existing methods. In addition, the running time of our approach is much less than the current state-of-the-art deep learning methods. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2241–2252},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380290,
author = {Spangher, Alexander and Ranade, Gireeja and Nushi, Besmira and Fourney, Adam and Horvitz, Eric},
title = {Characterizing Search-Engine Traffic to Internet Research Agency Web Properties},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380290},
doi = {10.1145/3366423.3380290},
abstract = {The Russia-based Internet Research Agency (IRA) carried out a broad information campaign in the U.S. before and after the 2016 presidential election. The organization created an expansive set of internet properties: web domains, Facebook pages, and Twitter bots, which received traffic via purchased Facebook ads, tweets, and search engines indexing their domains. In this paper, we focus on IRA activities that received exposure through search engines, by joining data from Facebook and Twitter with logs from the Internet Explorer 11 and Edge browsers and the Bing.com search engine. We find that a substantial volume of Russian content was apolitical and emotionally-neutral in nature. Our observations demonstrate that such content gave IRA web-properties considerable exposure through search-engines and brought readers to websites hosting inflammatory content and engagement hooks. Our findings show that, like social media, web search also directed traffic to IRA generated web content, and the resultant traffic patterns are distinct from those of social media.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2253–2263},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380291,
author = {Hu, Wenjie and Yang, Yang and Wang, Jianbo and Huang, Xuanwen and Cheng, Ziqiang},
title = {Understanding Electricity-Theft Behavior via Multi-Source Data},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380291},
doi = {10.1145/3366423.3380291},
abstract = {Electricity theft, the behavior that involves users conducting illegal operations on electrical meters to avoid individual electricity bills, is a common phenomenon in the developing countries. Considering its harmfulness to both power grids and the public, several mechanized methods have been developed to automatically recognize electricity-theft behaviors. However, these methods, which mainly assess users’ electricity usage records, can be insufficient due to the diversity of theft tactics and the irregularity of user behaviors. In this paper, we propose to recognize electricity-theft behavior via multi-source data. In addition to users’ electricity usage records, we analyze user behaviors by means of regional factors (non-technical loss) and climatic factors (temperature) in the corresponding transformer area. By conducting analytical experiments, we unearth several interesting patterns: for instance, electricity thieves are likely to consume much more electrical power than normal users, especially under extremely high or low temperatures. Motivated by these empirical observations, we further design a novel hierarchical framework for identifying electricity thieves. Experimental results based on a real-world dataset demonstrate that our proposed model can achieve the best performance in electricity-theft detection (e.g., at least +3.0% in terms of F0.5) compared with several baselines. Last but not least, our work has been applied by the State Grid of China and used to successfully catch electricity thieves in Hangzhou with a precision of 15% (an improvement from 0% attained by several other models the company employed) during monthly on-site investigation.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2264–2274},
numpages = {11},
keywords = {User modeling, electricity-theft detection, hierarchical recurrent neural network, power grids},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380292,
author = {Borgolte, Kevin and Feamster, Nick},
title = {Understanding the Performance Costs and Benefits of Privacy-Focused Browser Extensions},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380292},
doi = {10.1145/3366423.3380292},
abstract = {Advertisements and behavioral tracking have become an invasive nuisance on the Internet in recent years. Indeed, privacy advocates and expert users consider the invasion significant enough to warrant the use of ad blockers and anti-tracking browser extensions. At the same time, one of the largest advertisement companies in the world, Google, is developing the most popular browser, Google Chrome. This conflict of interest, that is developing a browser (a user agent) and being financially motivated to track users’ online behavior, possibly violating their privacy expectations, while claiming to be a ”user agent,” did not remain unnoticed. As a matter of fact, Google recently sparked an outrage when proposing changes to Chrome how extensions can inspect and modify requests to ”improve extension performance and privacy,” which would render existing privacy-focused extensions inoperable. In this paper, we analyze how eight popular privacy-focused browser extensions for Google Chrome and Mozilla Firefox, the two desktop browsers with the highest market share, affect browser performance. We measure browser performance through several metrics focused on user experience, such as page-load times, number of fetched resources, as well as response sizes. To address potential regional differences in advertisements or tracking, such as influenced by the European General Data Protection Regulation (GDPR), we perform our study from two vantage points, the United States of America and Germany. Moreover, we also analyze how these extensions affect system performance, in particular CPU time, which serves as a proxy indicator for battery runtime of mobile devices. Contrary to Google’s claims that extensions which inspect and block requests negatively affect browser performance, we find that a browser with privacy-focused request-modifying extensions performs similar or better on our metrics compared to a browser without extensions. In fact, even a combination of such extensions performs no worse than a browser without any extensions. Our results highlight that privacy-focused extensions not only improve users’ privacy, but can also increase users’ browsing experience.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2275–2286},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380293,
author = {Ren, Shaogang and Li, Dingcheng and Zhou, Zhixin and Li, Ping},
title = {Estimate the Implicit Likelihoods of GANs with Application to Anomaly Detection},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380293},
doi = {10.1145/3366423.3380293},
abstract = {The thriving of deep models and generative models provides approaches to model high dimensional distributions. Generative adversarial networks&nbsp;(GANs) can approximate data distributions and generate data samples from the learned data manifolds as well. In this paper, we propose an approach to estimate the implicit likelihoods of GAN models. A stable inverse function of the generator can be learned with the help of a variance network of the generator. The local variance of the sample distribution can be approximated by the normalized distance in the latent space. Simulation studies and likelihood testing on real-world data sets validate the proposed algorithm, which outperforms several baseline methods in these tasks. The proposed method has been further applied to anomaly detection. Experiments show that the method can achieve state-of-the-art anomaly detection performance on real-world data sets.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2287–2297},
numpages = {11},
keywords = {generative adversarial networks, anomaly detection, density estimation, unsupervised learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380294,
author = {Yao, Jing and Dou, Zhicheng and Xu, Jun and Wen, Ji-Rong},
title = {RLPer: A Reinforcement Learning Model for Personalized Search},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380294},
doi = {10.1145/3366423.3380294},
abstract = {Personalized search improves generic ranking models by taking user interests into consideration and returning more accurate search results to individual users. In recent years, machine learning and deep learning techniques have been successfully applied in personalized search. Most existing personalization models simply regard the search history as a static set of user behaviours and learn fixed ranking strategies based on the recorded data. Though improvements have been observed, it is obvious that these methods ignore the dynamic nature of the search process: search is a sequence of interactions between the search engine and the user. During the search process, the user interests may dynamically change. It would be more helpful if a personalized search model could track the whole interaction process and update its ranking strategy continuously. In this paper, we propose a reinforcement learning based personalization model, referred to as RLPer, to track the sequential interactions between the users and search engine with a hierarchical Markov Decision Process (MDP). In RLPer, the search engine interacts with the user to update the underlying ranking model continuously with real-time feedback. And we design a feedback-aware personalized ranking component to catch the user’s feedback which has impacts on the user interest profile for the next query. Experimental results on the publicly available AOL search log verify that our proposed model can significantly outperform state-of-the-art personalized search models.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2298–2308},
numpages = {11},
keywords = {Personalized Search, Reinforcement Learning, MDP},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380295,
author = {Ye, Wei and Xie, Rui and Zhang, Jinglei and Hu, Tianxiang and Wang, Xiaoyin and Zhang, Shikun},
title = {Leveraging Code Generation to Improve Code Retrieval and Summarization via Dual Learning},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380295},
doi = {10.1145/3366423.3380295},
abstract = {Code summarization generates brief natural language description given a source code snippet, while code retrieval fetches relevant source code given a natural language query. Since both tasks aim to model the association between natural language and programming language, recent studies have combined these two tasks to improve their performance. However, researchers have yet been able to effectively leverage the intrinsic connection between the two tasks as they train these tasks in a separate or pipeline manner, which means their performance can not be well balanced. In this paper, we propose a novel end-to-end model for the two tasks by introducing an additional code generation task. More specifically, we explicitly exploit the probabilistic correlation between code summarization and code generation with dual learning, and utilize the two encoders for code summarization and code generation to train the code retrieval task via multi-task learning. We have carried out extensive experiments on an existing dataset of SQL and Python, and results show that our model can significantly improve the results of the code retrieval task over the-state-of-art models, as well as achieve competitive performance in terms of BLEU score for the code summarization task.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2309–2319},
numpages = {11},
keywords = {code generation, code retrieval, dual learning, code summarization},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380296,
author = {Wu, Xian and Huang, Chao and Zhang, Chuxu and Chawla, Nitesh V.},
title = {Hierarchically Structured Transformer Networks for Fine-Grained Spatial Event Forecasting},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380296},
doi = {10.1145/3366423.3380296},
abstract = {Spatial event forecasting is challenging and crucial for urban sensing scenarios, which is beneficial for a wide spectrum of spatial-temporal mining applications, ranging from traffic management, public safety, to environment policy making. In spite of significant progress has been made to solve spatial-temporal prediction problem, most existing deep learning based methods based on a coarse-grained spatial setting and the success of such methods largely relies on data sufficiency. In many real-world applications, predicting events with a fine-grained spatial resolution do play a critical role to provide high discernibility of spatial-temporal data distributions. However, in such cases, applying existing methods will result in weak performance since they may not well capture the quality spatial-temporal representations when training triple instances are highly imbalanced across locations and time. To tackle this challenge, we develop a hierarchically structured Spatial-Temporal ransformer network (STtrans) which leverages a main embedding space to capture the inter-dependencies across time and space for alleviating the data imbalance issue. In our STtrans framework, the first-stage transformer module discriminates different types of region and time-wise relations. To make the latent spatial-temporal representations be reflective of the relational structure between categories, we further develop a cross-category fusion transformer network to endow STtrans with the capability to preserve the semantic signals in a fully dynamic manner. Finally, an adversarial training strategy is introduced to yield a robust spatial-temporal learning under data imbalance. Extensive experiments on real-world imbalanced spatial-temporal datasets from NYC and Chicago demonstrate the superiority of our method over various state-of-the-art baselines.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2320–2330},
numpages = {11},
keywords = {Deep neural networks, Spatial-temporal data mining},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380297,
author = {Fu, Xinyu and Zhang, Jiani and Meng, Ziqiao and King, Irwin},
title = {MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380297},
doi = {10.1145/3366423.3380297},
abstract = {A large number of real-world graphs or networks are inherently heterogeneous, involving a diversity of node types and relation types. Heterogeneous graph embedding is to embed rich structural and semantic information of a heterogeneous graph into low-dimensional node representations. Existing models usually define multiple metapaths in a heterogeneous graph to capture the composite relations and guide neighbor selection. However, these models either omit node content features, discard intermediate nodes along the metapath, or only consider one metapath. To address these three limitations, we propose a new model named Metapath Aggregated Graph Neural Network (MAGNN) to boost the final performance. Specifically, MAGNN employs three major components, i.e., the node content transformation to encapsulate input node attributes, the intra-metapath aggregation to incorporate intermediate semantic nodes, and the inter-metapath aggregation to combine messages from multiple metapaths. Extensive experiments on three real-world heterogeneous graph datasets for node classification, node clustering, and link prediction show that MAGNN achieves more accurate prediction results than state-of-the-art baselines.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2331–2341},
numpages = {11},
keywords = {Graph neural network, Heterogeneous graph, Graph embedding},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380298,
author = {Naumzik, Christof and Zoechbauer, Patrick and Feuerriegel, Stefan},
title = {Mining Points-of-Interest for Explaining Urban Phenomena: A&nbsp;Scalable Variational Inference Approach},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380298},
doi = {10.1145/3366423.3380298},
abstract = {Points-of-interest (POIs; i.e., restaurants, bars, landmarks, and other entities) are common in web-mined data: they greatly explain the spatial distributions of urban phenomena. The conventional modeling approach relies upon feature engineering, yet it ignores the spatial structure among POIs. In order to overcome this shortcoming, the present paper proposes a novel spatial model for explaining spatial distributions based on web-mined POIs. Our key contributions are: (1)&nbsp;We present a rigorous yet highly interpretable formalization in order to model the influence of POIs on a given outcome variable. Specifically, we accommodate the spatial distributions of both the outcome and POIs. In our case, this modeled by the sum of latent Gaussian processes. (2)&nbsp;In contrast to previous literature, our model infers the influence of POIs without feature engineering, instead we model the influence of POIs via distance-weighted kernel functions with fully learnable parameterizations. (3)&nbsp;We propose a scalable learning algorithm based on sparse variational approximation. For this purpose, we derive a tailored evidence lower bound&nbsp;(ELBO) and, for appropriate likelihoods, we even show that an analytical expression can be obtained. This allows fast and accurate computation of the ELBO. Finally, the value of our approach for web mining is demonstrated in two real-world case studies. Our findings provide substantial improvements over state-of-the-art baselines with regard to both predictive and, in particular, explanatory performance. Altogether, this yields a novel spatial model for leveraging web-mined POIs. Within the context of location-based social networks, it promises an extensive range of new insights and use cases.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2342–2353},
numpages = {12},
keywords = {Point-of-Interest, Gaussian Process, Spatial Analytics, Variational Inference, Web-Mined Location Data},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380299,
author = {Zhang, Le and Xu, Tong and Zhu, Hengshu and Qin, Chuan and Meng, Qingxin and Xiong, Hui and Chen, Enhong},
title = {Large-Scale Talent Flow Embedding for Company Competitive Analysis},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380299},
doi = {10.1145/3366423.3380299},
abstract = {Recent years have witnessed the growing interests in investigating the competition among companies. Existing studies for company competitive analysis generally rely on subjective survey data and inferential analysis. Instead, in this paper, we aim to develop a new paradigm for studying the competition among companies through the analysis of talent flows. The rationale behind this is that the competition among companies usually leads to talent movement. Along this line, we first build a Talent Flow Network based on the large-scale job transition records of talents, and formulate the concept of “competitiveness” for companies with consideration of their bi-directional talent flows in the network. Then, we propose a Talent Flow Embedding (TFE) model to learn the bi-directional talent attractions of each company, which can be leveraged for measuring the pairwise competitive relationships between companies. Specifically, we employ the random-walk based model in original and transpose networks respectively to learn representations of companies by preserving their competitiveness. Furthermore, we design a multi-task strategy to refine the learning results from a fine-grained perspective, which can jointly embed multiple talent flow networks by assuming the features of company keep stable but take different roles in networks of different job positions. Finally, extensive experiments on a large-scale real-world dataset clearly validate the effectiveness of our TFE model in terms of company competitive analysis and reveal some interesting rules of competition based on the derived insights on talent flows. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2354–2364},
numpages = {11},
keywords = {Network Embedding, Talent Flow, Competitive Analysis},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380300,
author = {Piccardi, Tiziano and Redi, Miriam and Colavizza, Giovanni and West, Robert},
title = {Quantifying Engagement with Citations on Wikipedia},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380300},
doi = {10.1145/3366423.3380300},
abstract = {Wikipedia is one of the most visited sites on the Web and a common source of information for many users. As an encyclopedia, Wikipedia was not conceived as a source of original information, but as a gateway to secondary sources: according to Wikipedia’s guidelines, facts must be backed up by reliable sources that reflect the full spectrum of views on the topic. Although citations lie at the heart of Wikipedia, little is known about how users interact with them. To close this gap, we built client-side instrumentation for logging all interactions with links leading from English Wikipedia articles to cited references during one month, and conducted the first analysis of readers’ interactions with citations. We find that overall engagement with citations is low: about one in 300 page views results in a reference click (0.29% overall; 0.56% on desktop; 0.13% on mobile). Matched observational studies of the factors associated with reference clicking reveal that clicks occur more frequently on shorter pages and on pages of lower quality, suggesting that references are consulted more commonly when Wikipedia itself does not contain the information sought by the user. Moreover, we observe that recent content, open access sources, and references about life events (births, deaths, marriages, etc.) are particularly popular. Taken together, our findings deepen our understanding of Wikipedia’s role in a global information economy where reliability is ever less certain, and source attribution ever more vital.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2365–2376},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380301,
author = {Zhao, Xinyan and Xiao, Feng and Zhong, Haoming and Yao, Jun and Chen, Huanhuan},
title = {Condition Aware and Revise Transformer for Question Answering},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380301},
doi = {10.1145/3366423.3380301},
abstract = {The study of question answering has received increasing attention in recent years. This work focuses on providing an answer that compatible with both user intent and conditioning information corresponding to the question, such as delivery status and stock information in e-commerce. However, these conditions may be wrong or incomplete in real-world applications. Although existing question answering systems have considered the external information, such as categorical attributes and triples in knowledge base, they all assume that the external information is correct and complete. To alleviate the effect of defective condition values, this paper proposes condition aware and revise Transformer (CAR-Transformer). CAR-Transformer (1) revises each condition value based on the whole conversation and original conditions values, and (2) it encodes the revised conditions and utilizes the conditions embedding to select an answer. Experimental results on a real-world customer service dataset demonstrate that the CAR-Transformer can still select an appropriate reply when conditions corresponding to the question exist wrong or missing values, and substantially outperforms baseline models on automatic and human evaluations. The proposed CAR-Transformer can be extended to other NLP tasks which need to consider conditioning information.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2377–2387},
numpages = {11},
keywords = {language modeling, Question answering, natural language processing.},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380302,
author = {Guo, Zhen and Zhang, Zhe and Singh, Munindar},
title = {In Opinion Holders’ Shoes: Modeling Cumulative Influence for View Change in Online Argumentation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380302},
doi = {10.1145/3366423.3380302},
abstract = {Understanding how people change their views during multiparty argumentative discussions is important in applications that involve human communication, e.g., in social media and education. Existing research focuses on lexical features of individual comments, dynamics of discussions, or the personalities of participants but deemphasizes the cumulative influence of the interplay of comments by different participants on a participant’s mindset. We address the task of predicting the points where a user’s view changes given an entire discussion, thereby tackling the confusion due to multiple plausible alternatives when considering the entirety of a discussion. We make the following contributions. (1) Through a human study, we show that modeling a user’s perception of comments is crucial in predicting persuasiveness. (2) We present a sequential model for cumulative influence that captures the interplay between comments as both local and nonlocal dependencies, and demonstrate its capability of selecting the most effective information for changing views. (3) We identify contextual and interactive features and propose sequence structures to incorporate these features. Our empirical evaluation using a Reddit Change My View dataset shows that contextual and interactive features are valuable in predicting view changes, and a sequential model notably outperforms the nonsequential baseline models. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2388–2399},
numpages = {12},
keywords = {Persuasion, Social media, Online discussion modeling},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380303,
author = {Chen, Chong and Zhang, Min and Ma, Weizhi and Liu, Yiqun and Ma, Shaoping},
title = {Efficient Non-Sampling Factorization Machines for Optimal Context-Aware Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380303},
doi = {10.1145/3366423.3380303},
abstract = {To provide more accurate recommendation, it is a trending topic to go beyond modeling user-item interactions and take context features into account. Factorization Machines (FM) with negative sampling is a popular solution for context-aware recommendation. However, it is not robust as sampling may lost important information and usually leads to non-optimal performances in practical. Several recent efforts have enhanced FM with deep learning architectures for modelling high-order feature interactions. While they either focus on rating prediction task only, or typically adopt the negative sampling strategy for optimizing the ranking performance. Due to the dramatic fluctuation of sampling, it is reasonable to argue that these sampling-based FM methods are still suboptimal for context-aware recommendation. In this paper, we propose to learn FM without sampling for ranking tasks that helps context-aware recommendation particularly. Despite effectiveness, such a non-sampling strategy presents strong challenge in learning efficiency of the model. Accordingly, we further design a new ideal framework named Efficient Non-Sampling Factorization Machines (ENSFM). ENSFM not only seamlessly connects the relationship between FM and Matrix Factorization (MF), but also resolves the challenging efficiency issue via novel memorization strategies. Through extensive experiments on three real-world public datasets, we show that 1) the proposed ENSFM consistently and significantly outperforms the state-of-the-art methods on context-aware Top-K recommendation, and 2) ENSFM achieves significant advantages in training efficiency, which makes it more applicable to real-world large-scale systems. Moreover, the empirical results indicate that a proper learning method is even more important than advanced neural network structures for Top-K recommendation task. Our implementation has been released 1 to facilitate further developments on efficient non-sampling methods. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2400–2410},
numpages = {11},
keywords = {Context-aware, Top-K Recommendation, Implicit Feedback, Factorization Machines, Efficient Learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380304,
author = {Karisani, Payam and Ho, Joyce C. and Agichtein, Eugene},
title = {Domain-Guided Task Decomposition with Self-Training for Detecting Personal Events in Social Media},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380304},
doi = {10.1145/3366423.3380304},
abstract = {Mining social media content for tasks such as detecting personal experiences or events, suffer from lexical sparsity, insufficient training data, and inventive lexicons. To reduce the burden of creating extensive labeled data and improve classification performance, we propose to perform these tasks in two steps: 1. Decomposing the task into domain-specific sub-tasks by identifying key concepts, thus utilizing human domain understanding; and 2. Combining the results of learners for each key concept using co-training to reduce the requirements for labeled training data. We empirically show the effectiveness and generality of our approach, Co-Decomp, using three representative social media mining tasks, namely Personal Health Mention detection, Crisis Report detection, and Adverse Drug Reaction monitoring. The experiments show that our model is able to outperform the state-of-the-art text classification models–including those using the recently introduced BERT model–when small amounts of training data are available.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2411–2420},
numpages = {10},
keywords = {event detection, semi-supervised learning, social media analysis, classification},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380305,
author = {Wu, Zhijing and Mao, Jiaxin and Liu, Yiqun and Zhan, Jingtao and Zheng, Yukun and Zhang, Min and Ma, Shaoping},
title = {Leveraging Passage-Level Cumulative Gain for Document Ranking},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380305},
doi = {10.1145/3366423.3380305},
abstract = {Document ranking is one of the most studied but challenging problems in information retrieval (IR) research. A number of existing document ranking models capture relevance signals at the whole document level. Recently, more and more research has begun to address this problem from fine-grained document modeling. Several works leveraged fine-grained passage-level relevance signals in ranking models. However, most of these works focus on context-independent passage-level relevance signals and ignore the context information, which may lead to inaccurate estimation of passage-level relevance. In this paper, we investigate how information gain accumulates with passages when users sequentially read a document. We propose the context-aware Passage-level Cumulative Gain (PCG), which aggregates relevance scores of passages and avoids the need to formally split a document into independent passages. Next, we incorporate the patterns of PCG into a BERT-based sequential model called Passage-level Cumulative Gain Model (PCGM) to predict the PCG sequence. Finally, we apply PCGM to the document ranking task. Experimental results on two public ad hoc retrieval benchmark datasets show that PCGM outperforms most existing ranking models and also indicates the effectiveness of PCG signals. We believe that this work contributes to improving ranking performance and providing more explainability for document ranking.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2421–2431},
numpages = {11},
keywords = {document ranking, Passage-level cumulative gain, neural network},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380306,
author = {Liu, Anthony and Guerra, Santiago and Fung, Isaac and Matute, Gabriel and Kamar, Ece and Lasecki, Walter},
title = {Towards Hybrid Human-AI Workflows for Unknown Unknown Detection},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380306},
doi = {10.1145/3366423.3380306},
abstract = {Predictive models are susceptible to errors called unknown unknowns, in which the model assigns incorrect labels to instances with high confidence. These commonly arise when training data does not represent variations of a class encountered at model deployment. Prior work showed that crowd workers can identify instances of unknown unknowns, but asking the crowd to identify a sufficient number of individual instances can be costly to acquire&nbsp;[2]. Instead, this paper presents an approach that leverages people’s ability to find patterns to retrain classifiers more effectively with fewer examples. We ask crowd workers to suggest and verify patterns in unknown unknowns. We then use these patterns to train an expansion classifier to identify additional examples from existing data that the primary classifier has encountered (and potentially misclassified) in the past. Our experiments show that our approach outperforms existing unknown unknown detection methods at improving classifier performance. This work is the first to leverage crowds to identify error patterns in large datasets to improve ML training.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2432–2442},
numpages = {11},
keywords = {blind spots, human computation, pattern finding, hybrid intelligence, crowdsourcing, human-in-the-loop, machine learning, unknown unknowns, classification},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380307,
author = {Wei, Kai and Lin, Yu-Ru and Yan, Muheng},
title = {Examining Protest as An Intervention to Reduce Online Prejudice: A Case Study of Prejudice Against Immigrants},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380307},
doi = {10.1145/3366423.3380307},
abstract = {There has been a growing concern about online users using social media to incite prejudice and hatred against other individuals or groups. While there has been research in developing automated techniques to identify online prejudice acts and hate speech, how to effectively counter online prejudice remains a societal challenge. Social protests, on the other hand, have been frequently used as an intervention for countering prejudice. However, research to date has not examined the relationship between protests and online prejudice. Using large-scale panel data collected from Twitter, we examine the changes in users’ tweeting behaviors relating to prejudice against immigrants following recent protests in the U.S. on immigration related topics. This is the first empirical study examining the effect of protests on reducing online prejudice. Our results show that there were both negative and positive changes in the measured prejudice after a protest, suggesting protest might have a mixed effect on reducing prejudice. We further identify users who are likely to change (or resist change) after a protest. This work contributes to the understanding of online prejudice and its intervention effect. The findings of this research have implications for designing targeted intervention. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2443–2454},
numpages = {12},
keywords = {social movement, online hate and prejudice, immigration, civic protest, computational social science},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380764,
author = {Chakrabarti, Soumen},
title = {Interpretable Complex Question Answering},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380764},
doi = {10.1145/3366423.3380764},
abstract = {We will review cross-community co-evolution of question answering (QA) with the advent of large-scale knowledge graphs (KGs), continuous representations of text and graphs, and deep sequence analysis. Early QA systems were information retrieval (IR) systems enhanced to extract named entity spans from high-scoring passages. Starting with WordNet, a series of structured curations of language and world knowledge, called KGs, enabled further improvements. Corpus is unstructured and messy to exploit for QA. If a question can be answered using the KG alone, it is attractive to ‘interpret’ the free-form question into a structured query, which is then executed on the structured KG. This process is called KGQA. Answers can be high-quality and explainable if the KG has an answer, but manual curation results in low coverage. KGs were soon found useful to harness corpus information. Named entity mention spans could be tagged with fine-grained types (e.g., scientist), or even specific entities (e.g., Einstein). The QA system can learn to decompose a query into functional parts, e.g., “which scientist” and “played the violin”. With increasing success of such systems, ambition grew to address multi-hop or multi-clause queries, e.g., “the father of the director of La La Land teaches at which university?” or “who directed an award-winning movie and is the son of a Princeton University professor?” Questions limited to simple path traversals in KGs have been encoded to a vector representation, which a decoder then uses to guide the KG traversal. Recently the corpus counterpart of such strategies has also been proposed. However, for general multi-clause queries that do not necessarily translate to paths, and seek to bind multiple variables to satisfy multiple clauses, or involve logic, comparison, aggregation and other arithmetic, neural programmer-interpreter systems have seen some success. Our key focus will be on identifying situations where manual introduction of structural bias is essential for accuracy, as against cases where sufficient data can get around distant or no supervision.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2455–2457},
numpages = {3},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3379992,
author = {Zhang, Hengtong and Li, Yaliang and Ding, Bolin and Gao, Jing},
title = {Practical Data Poisoning Attack against Next-Item Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3379992},
doi = {10.1145/3366423.3379992},
abstract = {Online recommendation systems make use of a variety of information sources to provide users the items that users are potentially interested in. However, due to the openness of the online platform, recommendation systems are vulnerable to data poisoning attacks. Existing attack approaches are either based on simple heuristic rules or designed against specific recommendations approaches. The former often suffers unsatisfactory performance, while the latter requires strong knowledge of the target system. In this paper, we focus on a general next-item recommendation setting and propose a practical poisoning attack approach named LOKI against blackbox recommendation systems. The proposed LOKI utilizes the reinforcement learning algorithm to train the attack agent, which can be used to generate user behavior samples for data poisoning. In real-world recommendation systems, the cost of retraining recommendation models is high, and the interaction frequency between users and a recommendation system is restricted. Given these real-world restrictions, we propose to let the agent interact with a recommender simulator instead of the target recommendation system and leverage the transferability of the generated adversarial samples to poison the target system. We also propose to use the influence function to efficiently estimate the influence of injected samples on the recommendation results, without re-training the models within the simulator. Extensive experiments on two datasets against four representative recommendation models show that the proposed LOKI achieves better attacking performance than existing methods.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2458–2464},
numpages = {7},
keywords = {Adversarial Learning, Recommendation System, Data Poisoning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3379993,
author = {Yang, Peng and Li, Ping},
title = {Efficient Online Multi-Task Learning via Adaptive Kernel Selection},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3379993},
doi = {10.1145/3366423.3379993},
abstract = {Conventional multi-task model restricts the task structure to be linearly related, which may not be suitable when data is linearly nonseparable. To remedy this issue, we propose a kernel algorithm for online multi-task classification, as the large approximation space provided by reproducing kernel Hilbert spaces often contains an accurate function. Specifically, it maintains a local-global Gaussian distribution over each task model that guides the direction and scale of parameter updates. Nonetheless, optimizing over this space is computationally expensive. Moreover, most multi-task learning methods require accessing to the entire training instances, which is luxury unavailable in the large-scale streaming learning scenario. To overcome this issue, we propose a randomized kernel sampling technique across multiple tasks. Instead of requiring all inputs’ labels, the proposed algorithm determines whether to query a label or not via considering the confidence from the related tasks over label prediction. Theoretically, the algorithm trained on actively sampled labels can achieve a comparable result with one learned on all labels. Empirically, the proposed algorithm is able to achieve promising learning efficacy, while reducing the computational complexity and labeling cost simultaneously. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2465–2471},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3379994,
author = {Li, Ruirui and Wu, Xian and Wu, Xian and Wang, Wei},
title = {Few-Shot Learning for New User Recommendation in Location-Based Social Networks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3379994},
doi = {10.1145/3366423.3379994},
abstract = {The proliferation of GPS-enabled devices establishes the prosperity of location-based social networks, which results in a tremendous amount of user check-ins. These check-ins bring in preeminent opportunities to understand users’ preferences and facilitate matching between users and businesses. However, the user check-ins are extremely sparse due to the huge user and business bases, which makes matching a daunting task. In this work, we investigate the recommendation problem in the context of identifying potential new customers for businesses in LBSNs. In particular, we focus on investigating the geographical influence, composed of geographical convenience and geographical dependency. In addition, we leverage metric-learning-based few-shot learning to fully utilize the user check-ins and facilitate the matching between users and businesses. To evaluate our proposed method, we conduct a series of experiments to extensively compare with 13 baselines using two real-world datasets. The results demonstrate that the proposed method outperforms all these baselines by a significant margin. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2472–2478},
numpages = {7},
keywords = {few-shot learning, self-attention, Customer recommendation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3379995,
author = {Shraga, Roee and Roitman, Haggai and Feigenblat, Guy and Canim, Mustafa},
title = {Ad Hoc Table Retrieval Using Intrinsic and Extrinsic Similarities},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3379995},
doi = {10.1145/3366423.3379995},
abstract = {Given a keyword query, the ad hoc table retrieval task aims at retrieving a ranked list of the top-k most relevant tables in a given table corpus. Previous works have primarily focused on designing table-centric lexical and semantic features, which could be utilized for learning-to-rank (LTR) tables. In this work, we make a novel use of intrinsic (passage-based) and extrinsic (manifold-based) table similarities for enhanced retrieval. Using the WikiTables benchmark, we study the merits of utilizing such similarities for this task. To this end, we combine both similarity types via a simple, yet an effective, cascade re-ranking approach. Overall, our proposed approach results in a significantly better table retrieval quality, which even transcends that of strong semantically-rich baselines.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2479–2485},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3379996,
author = {Ling, Yanxiang and Cai, Fei and Chen, Honghui and de Rijke, Maarten},
title = {Leveraging Context for Neural Question Generation in Open-Domain Dialogue Systems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3379996},
doi = {10.1145/3366423.3379996},
abstract = {Question generation in open-domain dialogue systems is a challenging but less-explored task. It aims to enhance the interactivity and persistence of human-machine interactions. Previous work mainly focuses on question generation in the setting of single-turn dialogues, or investigates it as a data augmentation method for machine comprehension. We propose a Context-augmented Neural Question Generation (CNQG) model that leverages the conversational context to generate questions for promoting interactivity and persistence of multi-turn dialogues. More specifically, we formulate the task of question generation as a two-stage process. First, we employ an encoder-decoder framework to predict a question pattern, which denotes a set of representative interrogatives, and identify the potential topics from the conversational context by employing point-wise mutual information. Then, we generate the question by decoding the concatenation of the current dialogue utterance, the pattern, and the topics with an attention mechanism. To the best of our knowledge, ours is the first work on question generation in multi-turn open-domain dialogue systems. Our experimental results on two publicly available multi-turn conversation datasets show that CNQG outperforms the state-of-the-art baselines in terms of BLEU-1, BLEU-2, Distinct-1 and Distinct-2. In addition, we find that CNQG allows one to efficiently distill useful features from long contexts, and maintain robust effectiveness even for short contexts. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2486–2492},
numpages = {7},
keywords = {open-domain., context, Question generation, dialogue systems},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3379997,
author = {Eswaran, Dhivya and Kumar, Srijan and Faloutsos, Christos},
title = {Higher-Order Label Homogeneity and Spreading in Graphs},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3379997},
doi = {10.1145/3366423.3379997},
abstract = {Do higher-order network structures aid graph semi-supervised learning? Given a graph and a few labeled vertices, labeling the remaining vertices is a high-impact problem with applications in several tasks, such as recommender systems, fraud detection and protein identification. However, traditional methods rely on edges for spreading labels, which is limited as all edges are not equal. Vertices with stronger connections participate in higher-order structures in graphs, which calls for methods that can leverage these structures in the semi-supervised learning tasks. To this end, we propose Higher-Order Label Spreading (HOLS) to spread labels using higher-order structures. HOLS has strong theoretical guarantees and reduces to standard label spreading in the base case. Via extensive experiments, we show that higher-order label spreading using triangles in addition to edges is up to 4.7% better than label spreading using edges alone. Compared to prior traditional and state-of-the-art methods, the proposed method leads to statistically significant accuracy gains in all-but-one cases, while remaining fast and scalable to large graphs. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2493–2499},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3379998,
author = {Peng, Shuang and Cui, Hengbin and Xie, Niantao and Li, Sujian and Zhang, Jiaxing and Li, Xiaolong},
title = {Enhanced-RCNN: An Efficient Method for Learning Sentence Similarity},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3379998},
doi = {10.1145/3366423.3379998},
abstract = {Learning sentence similarity is a fundamental research topic and has been explored using various deep learning methods recently. In this paper, we further propose an enhanced recurrent convolutional neural network (Enhanced-RCNN) model for learning sentence similarity. Compared to the state-of-the-art BERT model, the architecture of our proposed model is far less complex. Experimental results show that our similarity learning method outperforms the baselines and achieves the competitive performance on two real-world paraphrase identification datasets.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2500–2506},
numpages = {7},
keywords = {Recurrent Convolutional Neural Network, Sentence Similarity, Deep Learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3379999,
author = {Luo, Mi and Chen, Fei and Cheng, Pengxiang and Dong, Zhenhua and He, Xiuqiang and Feng, Jiashi and Li, Zhenguo},
title = {MetaSelector: Meta-Learning for Recommendation with User-Level Adaptive Model Selection},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3379999},
doi = {10.1145/3366423.3379999},
abstract = {Recommender systems often face heterogeneous datasets containing highly personalized historical data of users, where no single model could give the best recommendation for every user. We observe this ubiquitous phenomenon on both public and private datasets and address the model selection problem in pursuit of optimizing the quality of recommendation for each user. We propose a meta-learning framework to facilitate user-level adaptive model selection in recommender systems. In this framework, a collection of recommenders is trained with data from all users, on top of which a model selector is trained via meta-learning to select the best single model for each user with the user-specific historical data. We conduct extensive experiments on two public datasets and a real-world production dataset, demonstrating that our proposed framework achieves improvements over single model baselines and sample-level model selector in terms of AUC and LogLoss. In particular, the improvements may lead to huge profit gain when deployed in online recommender systems. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2507–2513},
numpages = {7},
keywords = {meta-learning, recommender systems, model selection},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380000,
author = {Wang, Zilong and Wan, Zhaohong and Wan, Xiaojun},
title = {TransModality: An End2End Fusion Method with Transformer for Multimodal Sentiment Analysis},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380000},
doi = {10.1145/3366423.3380000},
abstract = {Multimodal sentiment analysis is an important research area that predicts speaker’s sentiment tendency through features extracted from textual, visual and acoustic modalities. The central challenge is the fusion method of the multimodal information. A variety of fusion methods have been proposed, but few of them adopt end-to-end translation models to mine the subtle correlation between modalities. Enlightened by recent success of Transformer in the area of machine translation, we propose a new fusion method, TransModality, to address the task of multimodal sentiment analysis. We assume that translation between modalities contributes to a better joint representation of speaker’s utterance. With Transformer, the learned features embody the information both from the source modality and the target modality. We validate our model on multiple multimodal datasets: CMU-MOSI, MELD, IEMOCAP. The experiments show that our proposed method achieves the state-of-the-art performance.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2514–2520},
numpages = {7},
keywords = {sentiment analysis, neural network, multimodal},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380001,
author = {Zhou, Yichao and Mishra, Shaunak and Verma, Manisha and Bhamidipati, Narayan and Wang, Wei},
title = {Recommending Themes for Ad Creative Design via Visual-Linguistic Representations},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380001},
doi = {10.1145/3366423.3380001},
abstract = {There is a perennial need in the online advertising industry to refresh ad creatives, i.e., images and text used for enticing online users towards a brand. Such refreshes are required to reduce the likelihood of ad fatigue among online users, and to incorporate insights from other successful campaigns in related product categories. Given a brand, to come up with themes for a new ad is a painstaking and time consuming process for creative strategists. Strategists typically draw inspiration from the images and text used for past ad campaigns, as well as world knowledge on the brands. To automatically infer ad themes via such multimodal sources of information in past ad campaigns, we propose a theme (keyphrase) recommender system for ad creative strategists. The theme recommender is based on aggregating results from a visual question answering (VQA) task, which ingests the following: (i) ad images, (ii) text associated with the ads as well as Wikipedia pages on the brands in the ads, and (iii) questions around the ad. We leverage transformer based cross-modality encoders to train visual-linguistic representations for our VQA task. We study two formulations for the VQA task along the lines of classification and ranking; via experiments on a public dataset, we show that cross-modal representations lead to significantly better classification accuracy and ranking precision-recall metrics. Cross-modal representations show better performance compared to separate image and text representations. In addition, the use of multimodal information shows a significant lift over using only textual or visual information.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2521–2527},
numpages = {7},
keywords = {Online advertising, visual-linguistic representation, transformers},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380002,
author = {Tanjim, Md Mehrab and Su, Congzhe and Benjamin, Ethan and Hu, Diane and Hong, Liangjie and McAuley, Julian},
title = {Attentive Sequential Models of Latent Intent for Next Item Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380002},
doi = {10.1145/3366423.3380002},
abstract = {Users exhibit different intents across e-commerce services (e.g.&nbsp;discovering items, purchasing gifts, etc.) which drives them to interact with a wide variety of items in multiple ways (e.g.&nbsp;click, add-to-cart, add-to-favorites, purchase). To give better recommendations, it is important to capture user intent, in addition to considering their historic interactions. However these intents are by definition latent, as we observe only a user’s interactions, and not their underlying intent. To discover such latent intents, and use them effectively for recommendation, in this paper we propose an Attentive Sequential model of Latent Intent (ASLI in short). Our model first learns item similarities from users’ interaction histories via a self-attention layer, then uses a Temporal Convolutional Network layer to obtain a latent representation of the user’s intent from her actions on a particular category. We use this representation to guide an attentive model to predict the next item. Results from our experiments show that our model can capture the dynamics of user behavior and preferences, leading to state-of-the-art performance across datasets from two major e-commerce platforms, namely Etsy and Alibaba.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2528–2534},
numpages = {7},
keywords = {Latent Intent, User Modeling, Next Item Recommendation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380003,
author = {Luo, Kai and Sanner, Scott and Wu, Ga and Li, Hanze and Yang, Hojin},
title = {Latent Linear Critiquing for Conversational Recommender Systems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380003},
doi = {10.1145/3366423.3380003},
abstract = {Critiquing is a method for conversational recommendation that iteratively adapts recommendations in response to user preference feedback. In this setting, a user is iteratively provided with an item recommendation and attribute description for that item; a user may either accept the recommendation, or critique the attributes in the item description to generate a new recommendation. Historical critiquing methods were largely based on explicit constraint- and utility-based methods for modifying recommendations w.r.t. critiqued item attributes. In this paper, we revisit the critiquing approach in the era of recommendation methods based on latent embeddings with subjective item descriptions (i.e., keyphrases from user reviews). Two critical research problems arise: (1) how to co-embed keyphrase critiques with user preference embeddings to update recommendations, and (2) how to modulate the strength of multi-step critiquing feedback, where critiques are not necessarily independent, nor of equal importance. To address (1), we build on an existing state-of-the-art linear embedding recommendation algorithm to align review-based keyphrase attributes with user preference embeddings. To address (2), we exploit the linear structure of the embeddings and recommendation prediction to formulate a linear program (LP) based optimization problem to determine optimal weights for incorporating critique feedback. We evaluate the proposed framework on two recommendation datasets containing user reviews with simulated users. Empirical results compared to a standard approach of averaging critique feedback show that our approach reduces the number of interactions required to find a satisfactory item and increases the overall success rate.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2535–2541},
numpages = {7},
keywords = {Critiquing, Conversational Recommendation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380004,
author = {Xie, Jiayi and Zhu, Yaochen and Zhang, Zhibin and Peng, Jian and Yi, Jing and Hu, Yaosi and Liu, Hongyi and Chen, Zhenzhong},
title = {A Multimodal Variational Encoder-Decoder Framework for Micro-Video Popularity Prediction},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380004},
doi = {10.1145/3366423.3380004},
abstract = {Predicting the popularity of a micro-video is a challenging task, due to a number of factors impacting the distribution such as the diversity of the video content and user interests, complex online interactions, etc. In this paper, we propose a multimodal variational encoder-decoder (MMVED) framework that considers the uncertain factors as the randomness for the mapping from the multimodal features to the popularity. Specifically, the MMVED first encodes features from multiple modalities in the observation space into latent representations and learns their probability distributions based on variational inference, where only relevant features in the input modalities can be extracted into the latent representations. Then, the modality-specific hidden representations are fused through Bayesian reasoning such that the complementary information from all modalities is well utilized. Finally, a temporal decoder implemented as a recurrent neural network is designed to predict the popularity sequence of a certain micro-video. Experiments conducted on a real-world dataset demonstrate the effectiveness of our proposed model in the micro-video popularity prediction task.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2542–2548},
numpages = {7},
keywords = {Variational inference, Micro-video popularity prediction, Deep neural networks, Multimodal learning, Deep information bottleneck},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380005,
author = {Lissandrini, Matteo and Mottin, Davide and Palpanas, Themis and Velegrakis, Yannis},
title = {Graph-Query Suggestions for Knowledge Graph Exploration},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380005},
doi = {10.1145/3366423.3380005},
abstract = {We consider the task of exploratory search through graph queries on knowledge graphs. We propose to assist the user by expanding the query with intuitive suggestions to provide a more informative (full) query that can retrieve more detailed and relevant answers. To achieve this result, we propose a model that can bridge graph search paradigms with well-established techniques for information-retrieval. Our approach does not require any additional knowledge from the user and builds on principled language modelling approaches. We empirically show the effectiveness and efficiency of our approach on a large knowledge graph and how our suggestions are able to help build more complete and informative queries. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2549–2555},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380006,
author = {Muraoka, Masayasu and Nasukawa, Tetsuya and Raymond, Rudy and Bhattacharjee, Bishwaranjan},
title = {Visual Concept Naming: Discovering Well-Recognized Textual Expressions of Visual Concepts},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380006},
doi = {10.1145/3366423.3380006},
abstract = {We propose a task called Visual Concept Naming to associate visual concepts with the corresponding textual expressions, i.e., names of visual concepts found in real-world multimodal data. To tackle the task, we create a dataset consisting of 3.4 million tweets in total in three languages. We also propose a method for extracting candidate names of visual concepts and validating them by exploiting Web-based knowledge obtained through image search. To demonstrate the capability of our method, we conduct an experiment with the dataset we create and evaluate names obtained by our method through crowdsourcing, where we establish an evaluation method to verify the names. The experimental results indicate that the proposed method can identify a wide variety of names of visual concepts. The names we obtained also show interesting insights regarding languages and countries where the languages are used.1},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2556–2562},
numpages = {7},
keywords = {image search, crowdsourcing, social media analysis, text mining, multimodal grounding, vision and language},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380007,
author = {Chong, Xiaoya and Li, Qing and Leung, Howard and Men, Qianhui and Chao, Xianjin},
title = {Hierarchical Visual-Aware Minimax Ranking Based on Co-Purchase Data for Personalized Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380007},
doi = {10.1145/3366423.3380007},
abstract = {Personalized recommendation aims at ranking a set of items according to the learnt preferences of the user. Existing methods optimize the ranking function by considering an item that the user has not bought yet as a negative item and assuming that the user prefers the positive item that he has bought to the negative item. The strategy is to exclude irrelevant items from the dataset to narrow down the set of potential positive items to improve ranking accuracy. It conflicts with the goal of recommendation from the seller’s point of view, which aims to enlarge that set for each user. In this paper, we diminish this limitation by proposing a novel learning method called Hierarchical Visual-aware Minimax Ranking (H-VMMR), in which a new concept of predictive sampling is proposed to sample items in a close relationship with the positive items (e.g., substitutes, compliments). We set up the problem by maximizing the preference discrepancy between positive and negative items, as well as minimizing the gap between positive and predictive items based on visual features. We also build a hierarchical learning model based on co-purchase data to solve the data sparsity problem. Our method is able to enlarge the set of potential positive items as well as true negative items during ranking. The experimental results show that our H-VMMR outperforms the state-of-the-art learning methods. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2563–2569},
numpages = {7},
keywords = {Visual Features, Recommender Systems, Personalized Ranking},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380008,
author = {Wang, Weichao and Feng, Shi and Gao, Wei and Wang, Daling and Zhang, Yifei},
title = {A Cue Adaptive Decoder for Controllable Neural Response Generation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380008},
doi = {10.1145/3366423.3380008},
abstract = {In open-domain dialogue systems, dialogue cues such as emotion, persona, and emoji can be incorporated into conversation models for strengthening the semantic relevance of generated responses. Existing neural response generation models either incorporate dialogue cue into decoder’s initial state or embed the cue indiscriminately into the state of every generated word, which may cause the gradients of the embedded cue to vanish or disturb the semantic relevance of generated words during back propagation. In this paper, we propose a Cue Adaptive Decoder (CueAD) that aims to dynamically determine the involvement of a cue at each generation step in the decoding. For this purpose, we extend the Gated Recurrent Unit (GRU) network with an adaptive cue representation for facilitating cue incorporation, in which an adaptive gating unit is utilized to decide when to incorporate cue information so that the cue can provide useful clues for enhancing the semantic relevance of the generated words. Experimental results show that CueAD outperforms state-of-the-art baselines with large margins.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2570–2576},
numpages = {7},
keywords = {dialogue generation, vanishing gradient problem, cue adaptive decoder, disturbing gradient problem},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380009,
author = {Roitman, Haggai and Feigenblat, Guy and Cohen, Doron and Boni, Odellia and Konopnicki, David},
title = {Unsupervised Dual-Cascade Learning with Pseudo-Feedback Distillation for Query-Focused Extractive Summarization},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380009},
doi = {10.1145/3366423.3380009},
abstract = {We propose Dual-CES – a novel unsupervised, query-focused, multi-document extractive summarizer. Dual-CES builds on top of the Cross Entropy Summarizer (CES) and is designed to better handle the tradeoff between saliency and focus in summarization. To this end, Dual-CES employs a two-step dual-cascade optimization approach with saliency-based pseudo-feedback distillation. Overall, Dual-CES significantly outperforms all other state-of-the-art unsupervised alternatives. Dual-CES is even shown to be able to outperform strong supervised summarizers.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2577–2584},
numpages = {8},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380010,
author = {Liu, Guiliang and Li, Xu and Wang, Jiakang and Sun, Mingming and Li, Ping},
title = {Extracting Knowledge from Web Text with Monte Carlo Tree Search},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380010},
doi = {10.1145/3366423.3380010},
abstract = {To extract knowledge from general web text, it requires to build a domain-independent extractor that scales to the entire web corpus. This task is known as Open Information Extraction (OIE). This paper proposes to apply Monte-Carlo Tree Search (MCTS) to accomplish OIE. To achieve this goal, we define a Markov Decision Process for OIE and build a simulator to learn the reward signals, which provides a complete reinforcement learning framework for MCTS. Using this framework, MCTS explores candidate words (and symbols) under the guidance of a pre-trained Sequence-to-Sequence (Seq2Seq) predictor and generates abundant exploration samples during training. We apply the exploration samples to update the reward simulator and the predictor, based on which we implement another MCTS to search the optimal predictions during inference. Empirical evaluation demonstrates that the MCTS inference substantially improves the accuracy of prediction (more than 10%) and achieves a leading performance over other state-of-the-art comparison models.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2585–2591},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380011,
author = {Yang, Liu and Qiu, Minghui and Qu, Chen and Chen, Cen and Guo, Jiafeng and Zhang, Yongfeng and Croft, W. Bruce and Chen, Haiqing},
title = {IART: Intent-Aware Response Ranking with Transformers in Information-Seeking Conversation Systems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380011},
doi = {10.1145/3366423.3380011},
abstract = {Personal assistant systems, such as Apple Siri, Google Assistant, Amazon Alexa, and Microsoft Cortana, are becoming ever more widely used. Understanding user intent such as clarification questions, potential answers and user feedback in information-seeking conversations is critical for retrieving good responses. In this paper, we analyze user intent patterns in information-seeking conversations and propose an intent-aware neural response ranking model “IART”, which refers to “Intent-Aware Ranking with Transformers”. IART is built on top of the integration of user intent modeling and language representation learning with the Transformer architecture, which relies entirely on a self-attention mechanism instead of recurrent nets [35]. It incorporates intent-aware utterance attention to derive an importance weighting scheme of utterances in conversation context with the aim of better conversation history understanding. We conduct extensive experiments with three information-seeking conversation data sets including both standard benchmarks and commercial data. Our proposed model outperforms all baseline methods with respect to a variety of metrics. We also perform case studies and analysis of learned user intent and its impact on response ranking in information-seeking conversations to provide interpretation of results.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2592–2598},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380012,
author = {Rezaei, Hamed and Vamanan, Balajee},
title = {ResQueue: A Smarter Datacenter Flow Scheduler},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380012},
doi = {10.1145/3366423.3380012},
abstract = {Datacenters host a mix of applications: foreground applications perform distributed lookups in order to service user queries and background applications perform batch processing tasks such as data reorganization, backup, and replication. While background flows produce the most load, foreground applications produce the most number of flows. Because packets from both types of applications compete at switches for network bandwidth, the performance of applications is sensitive to scheduling mechanisms. Existing schedulers use flow size to distinguish critical flows from non-critical flows. However, recent studies on datacenter workloads reveal that most flows are small (e.g., most flows consist of only a handful number of packets). In light of recent findings, we make the key observation that because most flows are small, flow size is not sufficient to distinguish critical flows from non-critical flows and therefore existing flow schedulers do not achieve the desired prioritization. In this paper, we introduce ResQueue, which uses a combination of flow size and packet history to calculate the priority of each flow. Our evaluation shows that ResQueue improves tail flow completion times of short flows by up to 60% over the state-of-the-art flow scheduling mechanisms.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2599–2605},
numpages = {7},
keywords = {Congestion Control, Flow Scheduling, Datacenter Networks},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380013,
author = {Mao, Huiqiang and Li, Yanzhi and Li, Chenliang and Chen, Di and Wang, Xiaoqing and Deng, Yuming},
title = {PARS: Peers-Aware Recommender System},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380013},
doi = {10.1145/3366423.3380013},
abstract = {The presence or absence of one item in a recommendation list will affect the demand for other items because customers are often willing to switch to other items if their most preferred items are not available. The cross-item influence, called “peers effect”, has been largely ignored in the literature. In this paper, we develop a peers-aware recommender system, named PARS. We apply a ranking-based choice model to capture the cross-item influence and solve the resultant MaxMin problem with a decomposition algorithm. The MaxMin model solves for the recommendation decision in the meanwhile of estimating users’ preferences towards the items, which yields high-quality recommendations robust to input data variation. Experimental results illustrate that PARS outperforms a few frequently used methods in practice. An online evaluation with a flash sales scenario at Taobao also shows that PARS delivers significant improvements in terms of both conversion rates and user value. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2606–2612},
numpages = {7},
keywords = {Recommender system, Demand substitution, Ranking-based model, E-commerce},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380014,
author = {Tran, Trung-Kien and Gad-Elrab, Mohamed H. and Stepanova, Daria and Kharlamov, Evgeny and Str\"{o}tgen, Jannik},
title = {Fast Computation of Explanations for Inconsistency in Large-Scale Knowledge Graphs},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380014},
doi = {10.1145/3366423.3380014},
abstract = {Knowledge graphs (KGs) are essential resources for many applications including Web search and question answering. As KGs are often automatically constructed, they may contain incorrect facts. Detecting them is a crucial, yet extremely expensive task. Prominent solutions detect and explain inconsistency in KGs with respect to accompanying ontologies that describe the KG domain of interest. Compared to machine learning methods they are more reliable and human-interpretable but scale poorly on large KGs. In this paper, we present a novel approach to dramatically speed up the process of detecting and explaining inconsistency in large KGs by exploiting KG abstractions that capture prominent data patterns. Though much smaller, KG abstractions preserve inconsistency and their explanations. Our experiments with large KGs (e.g., DBpedia and Yago) demonstrate the feasibility of our approach and show that it significantly outperforms the popular baseline.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2613–2619},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380015,
author = {Zhang, Wenxuan and Lam, Wai and Deng, Yang and Ma, Jing},
title = {Review-Guided Helpful Answer Identification in E-Commerce},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380015},
doi = {10.1145/3366423.3380015},
abstract = {Product-specific community question answering platforms can greatly help address the concerns of potential customers. However, the user-provided answers on such platforms often vary a lot in their qualities. Helpfulness votes from the community can indicate the overall quality of the answer, but they are often missing. Accurately predicting the helpfulness of an answer to a given question and thus identifying helpful answers is becoming a demanding need. Since the helpfulness of an answer depends on multiple perspectives instead of only topical relevance investigated in typical QA tasks, common answer selection algorithms are insufficient for tackling this task. In this paper, we propose the Review-guided Answer Helpfulness Prediction (RAHP) model that not only considers the interactions between QA pairs but also investigates the opinion coherence between the answer and crowds’ opinions reflected in the reviews, which is another important factor to identify helpful answers. Moreover, we tackle the task of determining opinion coherence as a language inference problem and explore the utilization of pre-training strategy to transfer the textual inference knowledge obtained from a specifically designed trained network. Extensive experiments conducted on real-world data across seven product categories show that our proposed model achieves superior performance on the prediction task.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2620–2626},
numpages = {7},
keywords = {question answering, answer helpfulness prediction, E-commerce},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380016,
author = {Yoon, Se-eun and Song, Hyungseok and Shin, Kijung and Yi, Yung},
title = {How Much and When Do We Need Higher-Order Information in Hypergraphs? A Case Study on Hyperedge Prediction},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380016},
doi = {10.1145/3366423.3380016},
abstract = {Hypergraphs provide a natural way of representing group relations, whose complexity motivates an extensive array of prior work to adopt some form of abstraction and simplification of higher-order interactions. However, the following question has yet to be addressed: How much abstraction of group interactions is sufficient in solving a hypergraph task, and how different such results become across datasets? This question, if properly answered, provides a useful engineering guideline on how to trade off between complexity and accuracy of solving a downstream task. To this end, we propose a method of incrementally representing group interactions using a notion of n-projected graph whose accumulation contains information on up to n-way interactions, and quantify the accuracy of solving a task as n grows for various datasets. As a downstream task, we consider hyperedge prediction, an extension of link prediction, which is a canonical task for evaluating graph models. Through experiments on 15 real-world datasets, we draw the following messages: (a) Diminishing returns: small n is enough to achieve accuracy comparable with near-perfect approximations, (b) Troubleshooter: as the task becomes more challenging, larger n brings more benefit, and (c) Irreducibility: datasets whose pairwise interactions do not tell much about higher-order interactions lose much accuracy when reduced to pairwise abstractions.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2627–2633},
numpages = {7},
keywords = {hypergraphs, hyperedge prediction, graph mining, link prediction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380017,
author = {Zhang, Dongxiang and Nie, Yuyang and Wu, Sai and Shen, Yanyan and Tan, Kian-Lee},
title = {Multi-Context Attention for Entity Matching},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380017},
doi = {10.1145/3366423.3380017},
abstract = {Entity matching (EM) is a classic research problem that identifies data instances referring to the same real-world entity. Recent technical trend in this area is to take advantage of deep learning (DL) to automatically extract discriminative features. DeepER and DeepMatcher have emerged as two pioneering DL models for EM. However, these two state-of-the-art solutions simply incorporate vanilla RNNs and straightforward attention mechanisms. In this paper, we fully exploit the semantic context of embedding vectors for the pair of entity text descriptions. In particular, we propose an integrated multi-context attention framework that takes into account self-attention, pair-attention and global-attention from three types of context. The idea is further extended to incorporate attribute attention in order to support structured datasets. We conduct extensive experiments with 7 benchmark datasets that are publicly accessible. The experimental results clearly establish our superiority over DeepER and DeepMatcher in all the datasets.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2634–2640},
numpages = {7},
keywords = {classification network, entity matching, multi-context attention},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380018,
author = {Liu, Zitao and Xu, Guowei and Liu, Tianqiao and Fu, Weiping and Qi, Yubi and Ding, Wenbiao and Song, Yujia and Guo, Chaoyou and Kong, Cong and Yang, Songfan and Huang, Gale Yan},
title = {Dolphin: A Spoken Language Proficiency Assessment System for Elementary Education},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380018},
doi = {10.1145/3366423.3380018},
abstract = {Spoken language proficiency is critically important for children’s growth and personal development. Due to the limited and imbalanced educational resources in China, elementary students barely have chances to improve their oral language skills in classes. Verbal fluency tasks (VFTs) were invented to let the students practice their spoken language proficiency after school. VFTs are simple but concrete math related questions that ask students to not only report answers but speak out the entire thinking process. In spite of the great success of VFTs, they bring a heavy grading burden to elementary teachers. To alleviate this problem, we develop Dolphin, a spoken language proficiency assessment system for Chinese elementary education. Dolphin is able to automatically evaluate both phonological fluency and semantic relevance of students’ VFT answers. We conduct a wide range of offline and online experiments to demonstrate the effectiveness of Dolphin. In our offline experiments, we show that Dolphin improves both phonological fluency and semantic relevance evaluation performance when compared to state-of-the-art baselines on real-world educational data sets. In our online A/B experiments, we test Dolphin with 183 teachers from 2 major cities (Hangzhou and Xi’an) in China for 10 weeks and the results show that VFT assignments grading coverage is improved by 22%. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2641–2647},
numpages = {7},
keywords = {Verbal fluency, Spoken language proficiency, Assessment, Disfluency detection, K-12 education},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380019,
author = {Tian, Hechan and Zhang, Meng and Luo, Xiangyang and Liu, Fenlin and Qiao, Yaqiong},
title = {Twitter User Location Inference Based on Representation Learning and Label Propagation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380019},
doi = {10.1145/3366423.3380019},
abstract = {Social network user location inference technology has been widely used in various geospatial applications like public health monitoring and local advertising recommendation. Due to insufficient consideration of relationships between users and location indicative words, most of existing inference methods estimate label propagation probabilities solely based on statistical features, resulting in large location inference error. In this paper, a Twitter user location inference method based on representation learning and label propagation is proposed. Firstly, the heterogeneous connection relation graph is constructed based on relationships between Twitter users and relationships between users and location indicative words, and relationships unrelated to geographic attributes are filtered. Then, vector representations of users are learnt from the connection relation graph. Finally, label propagation probabilities between adjacent users are calculated based on vector representations, and the locations of unknown users are predicted through iterative label propagation. Experiments on two representative Twitter datasets - GeoText and TwUs, show that the proposed method can accurately calculate label propagation probabilities based on vector representations and improve the accuracy of location inference. Compared with existing typical Twitter user location inference methods - GCN and MLP-TXT+NET, the median error distance of the proposed method is reduced by 18% and 16%, respectively.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2648–2654},
numpages = {7},
keywords = {Representation Learning, Location Inference, Social Network Analysis, Label Propagation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380020,
author = {Pipergias Analytis, Pantelis and Barkoczi, Daniel and Lorenz-Spreen, Philipp and Herzog, Stefan},
title = {The Structure of Social Influence in Recommender Networks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380020},
doi = {10.1145/3366423.3380020},
abstract = {People’s ability to influence others’ opinion on matters of taste varies greatly—both offline and in recommender systems. What are the mechanisms underlying these striking differences? Using the weighted k-nearest neighbors algorithm (k-nn) to represent an array of social learning strategies, we show—leveraging methods from network science—how the k-nn algorithm gives rise to networks of social influence in six real-world domains of taste. We show three novel results that apply both to offline advice taking and online recommender settings. First, influential individuals have mainstream tastes and high dispersion in their taste similarity with others. Second, the fewer people an individual or algorithm consults (i.e., the lower k is) or the larger the weight placed on the opinions of more similar others, the smaller the group of people with substantial influence. Third, the influence networks emerging from deploying the k-nn algorithm are hierarchically organized. Our results shed new light on classic empirical findings in communication and network science and can help improve the understanding of social influence offline and online.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2655–2661},
numpages = {7},
keywords = {collaborative filtering, influencers, social influence, social networks},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380021,
author = {Yin, Yifang and Varadarajan, Jagannadan and Wang, Guanfeng and Wang, Xueou and Sahrawat, Dhruva and Zimmermann, Roger and Ng, See-Kiong},
title = {A Multi-Task Learning Framework for Road Attribute Updating via Joint Analysis of Map Data and GPS Traces},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380021},
doi = {10.1145/3366423.3380021},
abstract = {The quality of a digital map is of utmost importance for geo-aware services. However, maintaining an accurate and up-to-date map is a highly challenging task that usually involves a substantial amount of manual work. To reduce the manual efforts, methods have been proposed to automatically derive road attributes by mining GPS traces. However, previous methods always modeled each road attribute separately based on intuitive hand-crafted features extracted from GPS traces. This observation motivates us to propose a machine learning based method to learn joint features not only from GPS traces but also from map data. To model the relations among the target road attributes, we extract low-level shared feature embeddings via multi-task learning, while still being able to generate task-specific fused representations by applying attention-based feature fusion. To model the relations between the target road attributes and other contextual information that is available from a digital map, we propose to leverage map tiles at road centers as visual features that capture the information of the surrounding geographic objects around the roads. We perform extensive experiments on the OpenStreetMap where state-of-the-art classification accuracy has been obtained compared to existing road attribute detection approaches.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2662–2668},
numpages = {7},
keywords = {digital maps, Road attributes, GPS trajectories, multi-task learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380022,
author = {Miller, Sam and El-Bahrawy, Abeer and Dittus, Martin and Graham, Mark and Wright, Joss},
title = {Predicting Drug Demand with Wikipedia Views: Evidence from Darknet Markets.},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380022},
doi = {10.1145/3366423.3380022},
abstract = {Rapid changes in illicit drug demand, such as the Fentanyl epidemic, are a major public health issue. Policymakers currently rely on annual surveys to monitor public consumption, which are arguably too infrequent to detect rapid shifts in drug use. We present a novel method to predict drug use based on high-frequency sales data from darknet markets. We show that models based on historic trades alone cannot accurately predict drug demand. However, augmenting these models with data on Wikipedia page views for each drug greatly improves predictive accuracy, particularly for less popular drugs, suggesting such models may be particularly useful for detecting newly emerging substances. These results hold out-of-sample at high time frequency, across a range of drugs and countries. Therefore Wikipedia data may enable us to build a high-frequency measure of drug demand, which could help policymakers respond more quickly to future drug crises.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2669–2675},
numpages = {7},
keywords = {policy support, nowcasting, deep web, web search, web traffic},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380023,
author = {Deng, Xiaotie and Lin, Tao and Xiao, Tao},
title = {Private Data Manipulation in Optimal Sponsored Search Auction},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380023},
doi = {10.1145/3366423.3380023},
abstract = {In this paper, We revisit the sponsored search auction as a repeated auction. We view it as a learning and exploiting task of the seller against the private data distribution of the buyers. We model such a game between the seller and buyers by a Private Data Manipulation (PDM) game: the auction seller first announces an auction for which allocation and payment rules are based on the value distributions submitted by buyers. The seller’s expected revenue depends on the design of the protocol and the game played among the buyers in their choice on the submitted (fake) value distributions. Under the PDM game, we re-evaluate the theory, methodology, and techniques in the sponsored search auctions that have been the most intensively studied in Internet economics.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2676–2682},
numpages = {7},
keywords = {Internet economics, private data, sponsored search auction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380024,
author = {Jin, Lichen and Zhang, Yizhou and Song, Guojie and Jin, Yilun},
title = {Active Domain Transfer on Network Embedding},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380024},
doi = {10.1145/3366423.3380024},
abstract = {Recent works show that end-to-end, (semi-) supervised network embedding models can generate satisfactory vectors to represent network topology, and are even applicable to unseen graphs by inductive learning. However, domain mismatch between training and testing network for inductive learning, as well as lack of labeled data often compromises the outcome of such methods. To make matters worse, while transfer learning and active learning techniques, being able to solve such problems correspondingly, have been well studied on regular i.i.d data, relatively few attention has been paid on networks. Consequently, we propose in this paper a method for active transfer learning on networks named active-transfer network embedding, abbreviated ATNE. In ATNE we jointly consider the influence of each node on the network from the perspectives of transfer and active learning, and hence design novel and effective influence scores combining both aspects in the training process to facilitate node selection. We demonstrate that ATNE is efficient and decoupled from the actual model used. Further extensive experiments show that ATNE outperforms state-of-the-art active node selection methods and shows versatility in different situations. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2683–2689},
numpages = {7},
keywords = {Transfer Learning, Selection, Active Learning, Network Embedding},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380025,
author = {Zhao, Shuai and Kalra, Achir and Borcea, Cristian and Chen, Yi},
title = {To Be Tough or Soft: Measuring the Impact of Counter-Ad-Blocking Strategies on User Engagement},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380025},
doi = {10.1145/3366423.3380025},
abstract = {The fast growing ad-blocker usage results in large revenue decrease for ad-supported online websites. Facing this problem, many online publishers choose either to cooperate with ad-blocker software companies to show acceptable ads or to build a wall that requires users to whitelist the site for content access. However, there is lack of studies on the impact of these two counter-ad-blocking strategies on user behaviors. To address this issue, we conduct a randomized field experiment on the website of Forbes Media, a major US media publisher. The ad-blocker users are divided into a treatment group, which receives the wall strategy, and a control group, which receives the acceptable ads strategy. We utilize the difference-in-differences method to estimate the causal effects. Our study shows that the wall strategy has an overall negative impact on user engagements. However, it has no statistically significant effect on high-engaged users as they would view the pages no matter what strategy is used. It has a big impact on low-engaged users, who have no loyalty to the site. Our study also shows that revisiting behavior decreases over time, but the ratio of session whitelisting increases over time as the remaining users have relatively high loyalty and high engagement. The paper concludes with discussions of managerial insights for publishers when determining counter-ad-blocking strategies.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2690–2696},
numpages = {7},
keywords = {Randomized Field Experiment, User Studies, Difference-In-Differences, Ad Blocking, Online Advertising},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380026,
author = {Tsitsulin, Anton and Munkhoeva, Marina and Perozzi, Bryan},
title = {Just SLaQ When You Approximate: Accurate Spectral Distances for Web-Scale Graphs},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380026},
doi = {10.1145/3366423.3380026},
abstract = {Graph comparison is a fundamental operation in data mining and information retrieval. Due to the combinatorial nature of graphs, it is hard to balance the expressiveness of the similarity measure and its scalability. Spectral analysis provides quintessential tools for studying the multi-scale structure of graphs and is a well-suited foundation for reasoning about differences between graphs. However, computing full spectrum of large graphs is computationally prohibitive; thus, spectral graph comparison methods often rely on rough approximation techniques with weak error guarantees. In this work, we propose SLaQ , an efficient and effective approximation technique for computing spectral distances between graphs with billions of nodes and edges. We derive the corresponding error bounds and demonstrate that accurate computation is possible in time linear in the number of graph edges. In a thorough experimental evaluation, we show that SLaQ outperforms existing methods, oftentimes by several orders of magnitude in approximation accuracy, and maintains comparable performance, allowing to compare million-scale graphs in a matter of minutes on a single machine. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2697–2703},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380027,
author = {Hu, Ziniu and Dong, Yuxiao and Wang, Kuansan and Sun, Yizhou},
title = {Heterogeneous Graph Transformer},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380027},
doi = {10.1145/3366423.3380027},
abstract = {Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making it infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm—HGSampling—for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9–21 on various downstream tasks. The dataset and source code of HGT are publicly available at https://github.com/acbull/pyHGT. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2704–2710},
numpages = {7},
keywords = {Graph Embedding, Heterogeneous Information Networks, Representation Learning, Graph Neural Networks, Graph Attention},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380028,
author = {Logins, Alvis and Li, Yuchen and Karras, Panagiotis},
title = {On the Robustness of Cascade Diffusion under Node Attacks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380028},
doi = {10.1145/3366423.3380028},
abstract = {How can we assess a network’s ability to maintain its functionality under attacks? Network robustness has been studied extensively in the case of deterministic networks. However, applications such as online information diffusion and the behavior of networked public raise a question of robustness in probabilistic networks. We propose three novel robustness measures for networks hosting a diffusion under the Independent Cascade (IC) model, susceptible to node attacks. The outcome of such a process depends on the selection of its initiators, or seeds, by the seeder, as well as on two factors outside the seeder’s discretion: the attack strategy and the probabilistic diffusion outcome. We consider three levels of seeder awareness regarding these two uncontrolled factors, and evaluate the network’s viability aggregated over all possible extents of node attacks. We introduce novel algorithms from building blocks found in previous works to evaluate the proposed measures. A thorough experimental study with synthetic and real, scale-free and homogeneous networks establishes that these algorithms are effective and efficient, while the proposed measures highlight differences among networks in terms of robustness and the surprise they furnish when attacked. Last, we devise a new measure of diffusion entropy that can inform the design of probabilistically robust networks.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2711–2717},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380029,
author = {Jia, Jinyuan and Wang, Binghui and Cao, Xiaoyu and Gong, Neil Zhenqiang},
title = {Certified Robustness of Community Detection against Adversarial Structural Perturbation via Randomized Smoothing},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380029},
doi = {10.1145/3366423.3380029},
abstract = {Community detection plays a key role in understanding graph structure. However, several recent studies showed that community detection is vulnerable to adversarial structural perturbation. In particular, via adding or removing a small number of carefully selected edges in a graph, an attacker can manipulate the detected communities. However, to the best of our knowledge, there are no studies on certifying robustness of community detection against such adversarial structural perturbation. In this work, we aim to bridge this gap. Specifically, we develop the first certified robustness guarantee of community detection against adversarial structural perturbation. Given an arbitrary community detection method, we build a new smoothed community detection method via randomly perturbing the graph structure. We theoretically show that the smoothed community detection method provably groups a given arbitrary set of nodes into the same community (or different communities) when the number of edges added/removed by an attacker is bounded. Moreover, we show that our certified robustness is tight. We also empirically evaluate our method on multiple real-world graphs with ground truth communities. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2718–2724},
numpages = {7},
keywords = {Certified Robustness, Community Detection},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380030,
author = {Li, Ruilin and Qin, Zhen and Wang, Xuanhui and Chen, Suming J. and Metzler, Donald},
title = {Stabilizing Neural Search Ranking Models},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380030},
doi = {10.1145/3366423.3380030},
abstract = {Neural search ranking models have been not only actively studied in the information retrieval community, but also widely adopted in real-world industrial applications. However, due to the non-convexity and stochastic training of neural model formulations, the obtained models are unstable in the sense that model predictions can vary a lot for two models trained with the same configuration. In practice, new features are continuously introduced and new model architectures are explored to improve model effectiveness. In these cases, the instability of neural models leads to unnecessary document ranking changes for a large portion of queries. Such changes not only lead to inconsistent user experience, but also add noise to online experimentation and can slow down model improvement cycles. How to stabilize neural search ranking models during model update is an important but largely unexplored problem. Motivated by trigger analysis, we suggest balancing the trade-off between performance improvement and the number of affected queries. Concretely, we formulate it as an optimization problem with the objective as maximizing the average effect over the affected queries. We propose two heuristics and one theory-guided stabilization method to solve the optimization problem. Our proposed methods are evaluated on two of the world’s largest personal search services: Gmail search and Google Drive search. Empirical results show that our proposed methods are very effective in optimizing the proposed objective and are applicable to different model update scenarios.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2725–2732},
numpages = {8},
keywords = {neural network, trigger analysis, learning to rank},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380031,
author = {Gusev, Andrey and Xu, Jiajing},
title = {Evolution of a Web-Scale Near Duplicate Image Detection System},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380031},
doi = {10.1145/3366423.3380031},
abstract = {Detecting near duplicate images is fundamental to the content ecosystem of photo sharing web applications. However, such a task is challenging when involving a web-scale image corpus containing billions of images. In this paper, we present an efficient system for detecting near duplicate images across 8 billion images. Our system consists of three stages: candidate generation, candidate selection, and clustering. We also demonstrate that this system can be used to greatly improve the quality of recommendations and search results across a number of real-world applications. In addition, we include the evolution of the system over the course of six years, bringing out experiences and lessons on how new systems are designed to accommodate organic content growth as well as the latest technology. Finally, we are releasing a human-labeled dataset of ~53,000 pairs of images introduced in this paper. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2733–2739},
numpages = {7},
keywords = {locality sensitive hashing, near-duplicate detection, transfer learning, recommendation systems, clustering},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380032,
author = {Yasui, Shota and Morishita, Gota and Komei, Fujita and Shibata, Masashi},
title = {A Feedback Shift Correction in Predicting Conversion Rates under Delayed Feedback},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380032},
doi = {10.1145/3366423.3380032},
abstract = {In display advertising, predicting the conversion rate, that is, the probability that a user takes a predefined action on an advertiser’s website, such as purchasing goods is fundamental in estimating the value of displaying the advertisement. However, there is a relatively long time delay between a click and its resultant conversion. Because of the delayed feedback, some positive instances at the training period are labeled as negative because some conversions have not yet occurred when training data are gathered. As a result, the conditional label distributions differ between the training data and the production environment. This situation is referred to as a feedback shift. We address this problem by using an importance weight approach typically used for covariate shift correction. We prove its consistency for the feedback shift. Results in both offline and online experiments show that our proposed method outperforms the existing method. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2740–2746},
numpages = {7},
keywords = {Conversion Prediction, Importance Weight, Delayed Feedback},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380033,
author = {Chang, Lijun and Qiao, Miao},
title = {Deconstruct Densest Subgraphs},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380033},
doi = {10.1145/3366423.3380033},
abstract = {In this paper, we aim to understand the distribution of the densest subgraphs of a given graph under the density notion of average-degree. We show that the structures, the relationships and the distributions of all the densest subgraphs of a graph G can be encoded in O(L) space in an index called the ds-Index. Here L denotes the maximum output size of a densest subgraph of G. More importantly, ds-Indexcan report all the minimal densest subgraphs of G collectively in O(L) time and can enumerate all the densest subgraphs of G with an O(L) delay. Besides, the construction of ds-Indexcosts no more than finding a single densest subgraph using the state-of-the-art approach. Our empirical study shows that for web-scale graphs with one billion edges, the ds-Indexcan be constructed in several minutes on an ordinary commercial machine.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2747–2753},
numpages = {7},
keywords = {Graph Analytics, Densest Subgraph, Query Processing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380034,
author = {Zheng, Yaowei and Zhang, Richong and Wang, Suyuchen and Mensah, Samuel and Mao, Yongyi},
title = {Anchored Model Transfer and Soft Instance Transfer for Cross-Task Cross-Domain Learning: A Study Through Aspect-Level Sentiment Classification},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380034},
doi = {10.1145/3366423.3380034},
abstract = {Supervised learning relies heavily on readily available labelled data to infer an effective classification function. However, proposed methods under the supervised learning paradigm are faced with the scarcity of labelled data within domains, and are not generalized enough to adapt to other tasks. Transfer learning has proved to be a worthy choice to address these issues, by allowing knowledge to be shared across domains and tasks. In this paper, we propose two transfer learning methods Anchored Model Transfer (AMT) and Soft Instance Transfer (SIT), which are both based on multi-task learning, and account for model transfer and instance transfer, and can be combined into a common framework. We demonstrate the effectiveness of AMT and SIT for aspect-level sentiment classification showing the competitive performance against baseline models on benchmark datasets. Interestingly, we show that the integration of both methods AMT+SIT achieves state-of-the-art performance on the same task.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2754–2760},
numpages = {7},
keywords = {transfer learning, sentiment analysis},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380035,
author = {Stergiou, Stergios},
title = {Scaling PageRank to 100 Billion Pages},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380035},
doi = {10.1145/3366423.3380035},
abstract = {Distributed graph processing frameworks formulate tasks as sequences of supersteps within which communication is performed asynchronously by sending messages over the graph edges. PageRank’s communication pattern is identical across all its supersteps since each vertex sends messages to all its edges. We exploit this pattern to develop a new communication paradigm that allows us to exchange messages that include only edge payloads, dramatically reducing bandwidth requirements. Experiments on a web graph of 38 billion vertices and 3.1 trillion edges yield execution times of 34.4 seconds per iteration, suggesting more than an order of magnitude improvement over the state-of-the-art.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2761–2767},
numpages = {7},
keywords = {implicit targets, graph processing, pagerank},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380036,
author = {Liu, Jian and Zhao, Pengpeng and Zhuang, Fuzhen and Liu, Yanchi and Sheng, Victor S. and Xu, Jiajie and Zhou, Xiaofang and Xiong, Hui},
title = {Exploiting Aesthetic Preference in Deep Cross Networks for Cross-Domain Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380036},
doi = {10.1145/3366423.3380036},
abstract = {Visual aesthetics of products plays an important role in the decision process when purchasing appearance-first products, e.g., clothes. Indeed, user’s aesthetic preference, which serves as a personality trait and a basic requirement, is domain independent and could be used as a bridge between domains for knowledge transfer. However, existing work has rarely considered the aesthetic information in product images for cross-domain recommendation. To this end, in this paper, we propose a new deep Aesthetic Cross-Domain Networks (ACDN), in which parameters characterizing personal aesthetic preferences are shared across networks to transfer knowledge between domains. Specifically, we first leverage an aesthetic network to extract aesthetic features. Then, we integrate these features into a cross-domain network to transfer users’ domain independent aesthetic preferences. Moreover, network cross-connections are introduced to enable dual knowledge transfer across domains. Finally, the experimental results on real-world datasets show that our proposed model ACDN outperforms benchmark methods in terms of recommendation accuracy.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2768–2774},
numpages = {7},
keywords = {Cross-domain Recommendation;Knowledge Transfer;Aesthetic Feature},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380037,
author = {Zhang, Wenhao and Bao, Wentian and Liu, Xiao-Yang and Yang, Keping and Lin, Quan and Wen, Hong and Ramezani, Ramin},
title = {Large-Scale Causal Approaches to Debiasing Post-Click Conversion Rate Estimation with Multi-Task Learning},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380037},
doi = {10.1145/3366423.3380037},
abstract = {Post-click conversion rate (CVR) estimation is a critical task in e-commerce recommender systems. This task is deemed quite challenging under industrial setting with two major issues: 1) selection bias caused by user self-selection, and 2) data sparsity due to the rare click events. A successful conversion typically has the following sequential events: ”exposure → click → conversion”. Conventional CVR estimators are trained in the click space, but inference is done in the entire exposure space. They fail to account for the causes of the missing data and treat them as missing at random. Hence, their estimations are highly likely to deviate from the real values by large. In addition, the data sparsity issue can also handicap many industrial CVR estimators which usually have large parameter spaces. In this paper, we propose two principled, efficient and highly effective CVR estimators for industrial CVR estimation, namely, Multi-IPW and Multi-DR. The proposed models approach the CVR estimation from a causal perspective and account for the causes of missing not at random. In addition, our methods are based on the multi-task learning framework and mitigate the data sparsity issue. Extensive experiments on industrial-level datasets show that our methods outperform the state-of-the-art CVR models.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2775–2781},
numpages = {7},
keywords = {multi-task learning, conversion rate estimation, selection bias, recommender systems, causal inference},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380038,
author = {Javari, Amin and Derr, Tyler and Esmailian, Pouya and Tang, Jiliang and Chang, Kevin Chen-Chuan},
title = {ROSE: Role-Based Signed Network Embedding},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380038},
doi = {10.1145/3366423.3380038},
abstract = {In real-world networks, nodes might have more than one type of relationship. Signed networks are an important class of such networks consisting of two types of relations: positive and negative. Recently, embedding signed networks has attracted increasing attention and is more challenging than classic networks since nodes are connected by paths with multi-types of links. Existing works capture the complex relationships by relying on social theories. However, this approach has major drawbacks, including the incompleteness/inaccurateness of such theories. Thus, we propose network transformation based embedding to address these shortcomings. The core idea is that rather than directly finding the similarities of two nodes from the complex paths connecting them, we can obtain their similarities through simple paths connecting their different roles. We employ this idea to build our proposed embedding technique that can be described in three steps: (1) the input directed signed network is transformed into an unsigned bipartite network with each node mapped to a set of nodes we denote as role-nodes. Each role-node captures a certain role that a node in the original network plays; (2) the network of role-nodes is embedded; and (3) the original network is encoded by aggregating the embedding vectors of role-nodes. Our experiments show the novel proposed technique substantially outperforms existing models. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2782–2788},
numpages = {7},
keywords = {Network Transformation, Embedding, Signed networks},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380039,
author = {Bornemann, Leon and Bleifu\ss{}, Tobias and Kalashnikov, Dmitri V. and Naumann, Felix and Srivastava, Divesh},
title = {Natural Key Discovery in Wikipedia Tables},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380039},
doi = {10.1145/3366423.3380039},
abstract = {Wikipedia is the largest encyclopedia to date. Scattered among its articles, there is an enormous number of tables that contain structured, relational information. In contrast to database tables, these webtables lack metadata, making it difficult to automatically interpret the knowledge they harbor. The natural key is a particularly important piece of metadata, which acts as a primary key and consists of attributes inherent to an entity. Determining natural keys is crucial for many tasks, such as information integration, table augmentation, or tracking changes to entities over time. To address this challenge, we formally define the notion of natural keys and propose a supervised learning approach to automatically detect natural keys in Wikipedia tables using carefully engineered features. Our solution includes novel features that extract information from time (a table’s version history) and space (other similar tables). On a curated dataset of 1,000 Wikipedia table histories, our model achieves 80% F-measure, which is at least 20% more than all related approaches. We use our model to discover natural keys in the entire corpus of Wikipedia tables and provide the dataset to the community to facilitate future research.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2789–2795},
numpages = {7},
keywords = {information integration, natural key, Webtables, key discovery},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380040,
author = {Atouati, Samed and Lu, Xiao and Sozio, Mauro},
title = {Negative Purchase Intent Identification in Twitter},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380040},
doi = {10.1145/3366423.3380040},
abstract = {Social network users often express their discontent with a product or a service from a company on social media. Such a reaction is more pronounced in the aftermath of a corporate scandal such as a corruption scandal or food poisoning in a chain restaurant. In our work, we focus on identifying negative purchase intent in a tweet, i.e. the intent of a user of not purchasing any product or consuming any service from a company. We develop a binary classifier for such a task, which consists of a generalization of logistic regression leveraging the locality of purchase intent in posts from Twitter. We conduct an extensive experimental evaluation against state-of-the-art approaches on a large collection of tweets, showing the effectiveness of our approach in terms of F1 score. We also provide some preliminary results on which kinds of corporate scandals might affect the purchase intent of customers the most.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2796–2802},
numpages = {7},
keywords = {social media, hashtag segmentation, purchase intent, company scandal, neural networks, classification},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380041,
author = {Kristof, Victor and Grossglauser, Matthias and Thiran, Patrick},
title = {War of Words: The Competitive Dynamics of Legislative Processes},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380041},
doi = {10.1145/3366423.3380041},
abstract = {A body of law is an example of a dynamic corpus of text documents that are jointly maintained by a group of editors who compete and collaborate in complex constellations. Our goal is to develop predictive models for this process, thereby shedding light on the competitive dynamics of parliamentarians who make laws. For this purpose, we curated a dataset of 450000 legislative edits introduced by European parliamentarians over the last ten years. An edit modifies the status quo of a law, and could be in competition with another edit if it modifies the same part of that law. We propose a model for predicting the success of such edits, in the face of both the inertia of the status quo and the competition between overlapping edits. The parameters of this model can be interpreted in terms of the influence of parliamentarians and of the controversy of laws. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2803–2809},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380042,
author = {Kweon, Wonbin and Kang, Seongku and Hwang, Junyoung and Yu, Hwanjo},
title = {Deep Rating Elicitation for New Users in Collaborative Filtering},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380042},
doi = {10.1145/3366423.3380042},
abstract = {Recent recommender systems started to use rating elicitation, which asks new users to rate a small seed itemset for inferring their preferences, to improve the quality of initial recommendations. The key challenge of the rating elicitation is to choose the seed items which can best infer the new users’ preference. This paper proposes a novel end-to-end Deep learning framework for Rating Elicitation (DRE), that chooses all the seed items at a time with consideration of the non-linear interactions. To this end, it first defines categorical distributions to sample seed items from the entire itemset, then it trains both the categorical distributions and a neural reconstruction network to infer users’ preferences on the remaining items from CF information of the sampled seed items. Through the end-to-end training, the categorical distributions are learned to select the most representative seed items while reflecting the complex non-linear interactions. Experimental results show that DRE outperforms the state-of-the-art approaches in the recommendation quality by accurately inferring the new users’ preferences and its seed itemset better represents the latent space than the seed itemset obtained by the other methods. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2810–2816},
numpages = {7},
keywords = {Cold Start, Initial Recommendation, Recommender System},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380043,
author = {Joshi, Unmesh and Urbani, Jacopo},
title = {Searching for Embeddings in a Haystack: Link Prediction on Knowledge Graphs with Subgraph Pruning},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380043},
doi = {10.1145/3366423.3380043},
abstract = {Embedding-based models of Knowledge Graphs (KGs) can be used to predict the existence of missing links by ranking the entities according to some likelihood scores. An exhaustive computation of all likelihood scores is very expensive if the KG is large. To counter this problem, we propose a technique to reduce the search space by identifying smaller subsets of promising entities. Our technique first creates embeddings of subgraphs using the embeddings from the model. Then, it ranks the subgraphs with some proposed ranking functions and considers only the entities in the top k subgraphs. Our experiments show that our technique is able to reduce the search space significantly while maintaining a good recall.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2817–2823},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380044,
author = {Chierichetti, Flavio and Kumar, Ravi and Tomkins, Andrew},
title = {Asymptotic Behavior of Sequence Models},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380044},
doi = {10.1145/3366423.3380044},
abstract = {In this paper we study the limiting dynamics of a sequential process that generalizes P\'{o}lya’s urn. This process has been studied also in the context of language generation, discrete choice, repeat consumption, and models for the web graph. The process we study generates future items by copying from past items. It is parameterized by a sequence of weights describing how much to prefer copying from recent versus more distant locations. We show that, if the weight sequence follows a power law with exponent α ∈ [0, 1), then the sequences generated by the model tend toward a limiting behavior in which the eventual frequency of each token in the alphabet attains a limit. Moreover, in the case α &gt; 2, we show that the sequence converges to a token being chosen infinitely often, and each other token being chosen only constantly many times.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2824–2830},
numpages = {7},
keywords = {power laws, urn processes, copying models},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380045,
author = {Green Larsen, Kasper and Mitzenmacher, Michael and Tsourakakis, Charalampos},
title = {Clustering with a Faulty Oracle},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380045},
doi = {10.1145/3366423.3380045},
abstract = {Clustering, i.e., finding groups in the data, is a problem that permeates multiple fields of science and engineering. Recently, the problem of clustering with a noisy oracle has drawn attention due to various applications including crowdsourced entity resolution [33], and predicting signs of interactions in large-scale online social networks [20, 21]. Here, we consider the following fundamental model for two clusters as proposed by Mitzenmacher and Tsourakakis [28], and Mazumdar and Saha [25]; there exist n items, belonging to two unknown groups. We are allowed to query any pair of nodes whether they belong to the same cluster or not, but the answer to the query is corrupted with some probability . Let 1 &gt; δ = 1 − 2q &gt; 0 be the bias. In this work, we provide a polynomial time algorithm that recovers all signs correctly with high probability in the presence of noise with queries. This is the best known result for this problem for all but tiny δ, improving on the current state-of-the-art due to Mazumdar and Saha [25].},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2831–2834},
numpages = {4},
keywords = {active learning, randomized algorithms, clustering},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380046,
author = {Qin, Zhen and Li, Zhongliang and Bendersky, Michael and Metzler, Donald},
title = {Matching Cross Network for Learning to Rank in Personal Search},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380046},
doi = {10.1145/3366423.3380046},
abstract = {Recent neural ranking algorithms focus on learning semantic matching between query and document terms. However, practical learning to rank systems typically rely on a wide range of side information beyond query and document textual features, like location, user context, etc. It is common practice to concatenate all of these features and rely on deep models to learn a complex representation. We study how to effectively and efficiently combine textual information from queries and documents with other useful but less prominent side information for learning to rank. We conduct synthetic experiments to show that: 1) neural networks are inefficient at learning the interaction between two prominent features (e.g., query and document embedding features) in the presence of other less prominent features; 2) direct application of a state-of-art method for higher-order feature generation is also inefficient. Based on the above observations, we propose a simple but effective matching cross network (MCN) method for learning to rank with side information. MCN conducts an element-wise multiplication matching of query and document embeddings and leverages a technique called latent cross to effectively learn the interaction between matching output and all side information. The approach is easy to implement, and adds minimal parameters and latency overhead to standard neural ranking architectures. We conduct extensive experiments using two of the world’s largest personal search engines, Gmail and Google Drive search, and show that each proposed component adds meaningful gains against a strong production baseline with minimal latency overhead, thereby demonstrating the practical effectiveness and efficiency of the proposed approach. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2835–2841},
numpages = {7},
keywords = {neural networks, embeddings, learning to rank},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380047,
author = {Zhou, Jianghong and Agichtein, Eugene},
title = {RLIRank: Learning to Rank with Reinforcement Learning for Dynamic Search},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380047},
doi = {10.1145/3366423.3380047},
abstract = {To support complex search tasks, where the initial information requirements are complex or may change during the search, a search engine must adapt the information delivery as the user’s information requirements evolve. To support this dynamic ranking paradigm effectively, search result ranking must incorporate both the user feedback received, and the information displayed so far. To address this problem, we introduce a novel reinforcement learning-based approach, RLIRank. We first build an adapted reinforcement learning framework to integrate the key components of the dynamic search. Then, we implement a new Learning to Rank (LTR) model for each iteration of the dynamic search, using a recurrent Long Short Term Memory neural network (LSTM), which estimates the gain for each next result, learning from each previously ranked document. To incorporate the user’s feedback, we develop a word-embedding variation of the classic Rocchio Algorithm, to help guide the ranking towards the high-value documents. Those innovations enable RLIRank to outperform the previously reported methods from the TREC Dynamic Domain Tracks 2017 and exceed all the methods in 2016 TREC Dynamic Domain after multiple search iterations, advancing the state of the art for dynamic search.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2842–2848},
numpages = {7},
keywords = {deep reinforcement Learning to Rank, Learning to Rank for dynamic search, dynamic search ranking},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3380048,
author = {Zehlike, Meike and Castillo, Carlos},
title = {Reducing Disparate Exposure in Ranking: A Learning To Rank Approach},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3380048},
doi = {10.1145/3366424.3380048},
abstract = {Ranked search results have become the main mechanism by which we find content, products, places, and people online. Thus their ordering contributes not only to the satisfaction of the searcher, but also to career and business opportunities, educational placement, and even social success of those being ranked. Researchers have become increasingly concerned with systematic biases in data-driven ranking models, and various post-processing methods have been proposed to mitigate discrimination and inequality of opportunity. This approach, however, has the disadvantage that it still allows an unfair ranking model to be trained. In this paper we explore a new in-processing approach: DELTR, a learning-to-rank framework that addresses potential issues of discrimination and unequal opportunity in rankings at training time. We measure these problems in terms of discrepancies in the average group exposure and design a ranker that optimizes search results in terms of relevance and in terms of reducing such discrepancies. We perform an extensive experimental study showing that being “colorblind” can be among the best or the worst choices from the perspective of relevance and exposure, depending on how much and which kind of bias is present in the training set. We show that our in-processing method performs better in terms of relevance and exposure than a pre-processing and a post-processing method across all tested scenarios.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2849–2855},
numpages = {7},
keywords = {Ranking, Disparate Impact, Algorithmic Fairness},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380049,
author = {Jenkins, Porter and Zhao, Jennifer and Vinicombe, Heath and Subramanian, Anant and Prasad, Arun and Dobi, Atillia and Li, Eileen and Guo, Yunsong},
title = {Natural Language Annotations for Search Engine Optimization},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380049},
doi = {10.1145/3366423.3380049},
abstract = {Understanding content at scale is a difficult but important problem for many platforms. Many previous studies focus on content understanding to optimize engagement with existing users. However, little work studies how to leverage better content understanding to attract new users. In this work, we build a framework for generating natural language content annotations and show how they can be used for search engine optimization. The proposed framework relies on an XGBoost model that labels “pins” with high probability phrases, and a logistic regression layer that learns to rank aggregated annotations for groups of content. The pipeline identifies keywords that are descriptive and contextually meaningful. We perform a large-scale production experiment deployed on the Pinterest platform and show that natural language annotations cause a 1-2% increase in traffic from leading search engines. This increase is statistically significant. Finally, we explore and interpret the characteristics of our annotations framework.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2856–2862},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380050,
author = {Ge, Suyu and Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Huang, Yongfeng},
title = {Graph Enhanced Representation Learning for News Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380050},
doi = {10.1145/3366423.3380050},
abstract = {With the explosion of online news, personalized news recommendation becomes increasingly important for online news platforms to help their users find interesting information. Existing news recommendation methods achieve personalization by building accurate news representations from news content and user representations from their direct interactions with news (e.g., click), while ignoring the high-order relatedness between users and news. Here we propose a news recommendation method which can enhance the representation learning of users and news by modeling their relatedness in a graph setting. In our method, users and news are both viewed as nodes in a bipartite graph constructed from historical user click behaviors. For news representations, a transformer architecture is first exploited to build news semantic representations. Then we combine it with the information from neighbor news in the graph via a graph attention network. For user representations, we not only represent users from their historically clicked news, but also attentively incorporate the representations of their neighbor users in the graph. Improved performances on a large-scale real-world dataset validate the effectiveness of our proposed method.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2863–2869},
numpages = {7},
keywords = {Transformer, News Recommendation, Graph Attention Network},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380051,
author = {Jiang, Jyun-Yu and Wu, Tao and Roumpos, Georgios and Cheng, Heng-Tze and Yi, Xinyang and Chi, Ed and Ganapathy, Harish and Jindal, Nitin and Cao, Pei and Wang, Wei},
title = {End-to-End Deep Attentive Personalized Item Retrieval for Online Content-Sharing Platforms},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380051},
doi = {10.1145/3366423.3380051},
abstract = {Modern online content-sharing platforms host billions of items like music, videos, and products uploaded by various providers for users to discover items of their interests. To satisfy the information needs, the task of effective item retrieval (or item search ranking) given user search queries has become one of the most fundamental problems to online content-sharing platforms. Moreover, the same query can represent different search intents for different users, so personalization is also essential for providing more satisfactory search results. Different from other similar research tasks, such as ad-hoc retrieval and product retrieval with copious words and reviews, items in content-sharing platforms usually lack sufficient descriptive information and related meta-data as features. In this paper, we propose the end-to-end deep attentive model (EDAM) to deal with personalized item retrieval for online content-sharing platforms using only discrete personal item history and queries. Each discrete item in the personal item history of a user and its content provider are first mapped to embedding vectors as continuous representations. A query-aware attention mechanism is then applied to identify the relevant contexts in the user history and construct the overall personal representation for a given query. Finally, an extreme multi-class softmax classifier aggregates the representations of both query and personal item history to provide personalized search results. We conduct extensive experiments on a large-scale real-world dataset with hundreds of million users from a large video media platform at Google. The experimental results demonstrate that our proposed approach significantly outperforms several competitive baseline methods. It is also worth mentioning that this work utilizes a massive dataset from a real-world commercial content-sharing platform for personalized item retrieval to provide more insightful analysis from the industrial aspects.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2870–2877},
numpages = {8},
keywords = {personalization, Item retrieval, online content-sharing platforms, real-world log analysis., attention mechanism},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380052,
author = {Kim, Seungbae and Jiang, Jyun-Yu and Nakada, Masaki and Han, Jinyoung and Wang, Wei},
title = {Multimodal Post Attentive Profiling for Influencer Marketing},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380052},
doi = {10.1145/3366423.3380052},
abstract = {Influencer marketing has become a key marketing method for brands in recent years. Hence, brands have been increasingly utilizing influencers’ social networks to reach niche markets, and researchers have been studying various aspects of influencer marketing. However, brands have often suffered from searching and hiring the right influencers with specific interests/topics for their marketing due to a lack of available influencer data and/or limited capacity of marketing agencies. This paper proposes a multimodal deep learning model that uses text and image information from social media posts (i) to classify influencers into specific interests/topics (e.g., fashion, beauty) and (ii) to classify their posts into certain categories. We use the attention mechanism to select the posts that are more relevant to the topics of influencers, thereby generating useful influencer representations. We conduct experiments on the dataset crawled from Instagram, which is the most popular social media for influencer marketing. The experimental results show that our proposed model significantly outperforms existing user profiling methods by achieving 98% and 96% accuracy in classifying influencers and their posts, respectively. We release our influencer dataset of 33,935 influencers labeled with specific topics based on 10,180,500 posts to facilitate future research.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2878–2884},
numpages = {7},
keywords = {Multi-task learning, Social media, Influencer marketing, Multimodal neural network, User profiling, Influencer profiling},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380053,
author = {Filice, Simone and Cohen, Nachshon and Carmel, David},
title = {Voice-Based Reformulation of Community Answers},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380053},
doi = {10.1145/3366423.3380053},
abstract = {Community Question Answering (CQA) websites, such as Stack Exchange1 or Quora2, allow users to freely ask questions and obtain answers from other users, i.e.,&nbsp;the community. Personal assistants, such as Amazon Alexa or Google Home, can also exploit CQA data to answer a broader range of questions and increase customers’ engagement. However, the voice-based interaction poses new challenges to the Question Answering scenario. Even assuming that we are able to retrieve a previously asked question that perfectly matches the user’s query, we cannot simply read its answer to the user. A major limitation is the answer length. Reading these answers to the user is cumbersome and boring. Furthermore, many answers contain non-voice-friendly parts, such as images, or URLs. In this paper, we define the Answer Reformulation task and propose a novel solution to automatically reformulate a community provided answer making it suitable for a voice interaction. Results on a manually annotated dataset3 extracted from Stack Exchange show that our models improve strong baselines.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2885–2891},
numpages = {7},
keywords = {text summarization, community question answering, deep learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380054,
author = {Cheng, Mingxi and Nazarian, Shahin and Bogdan, Paul},
title = {VRoC: Variational Autoencoder-Aided Multi-Task Rumor Classifier Based on Text},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380054},
doi = {10.1145/3366423.3380054},
abstract = {Social media became popular and percolated almost all aspects of our daily lives. While online posting proves very convenient for individual users, it also fosters fast-spreading of various rumors. The rapid and wide percolation of rumors can cause persistent adverse or detrimental impacts. Therefore, researchers invest great efforts on reducing the negative impacts of rumors. Towards this end, the rumor classification system aims to to detect, track, and verify rumors in social media. Such systems typically include four components: (i) a rumor detector, (ii) a rumor tracker, (iii) a stance classifier, and (iv) a veracity classifier. In order to improve the state-of-the-art in rumor detection, tracking, and verification, we propose VRoC, a tweet-level variational autoencoder-based rumor classification system. VRoC consists of a co-train engine that trains variational autoencoders (VAEs) and rumor classification components. The co-train engine helps the VAEs to tune their latent representations to be classifier-friendly. We also show that VRoC is able to classify unseen rumors with high levels of accuracy. For the PHEME dataset, VRoC consistently outperforms several state-of-the-art techniques, on both observed and unobserved rumors, by up to 26.9%, in terms of macro-F1 scores. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2892–2898},
numpages = {7},
keywords = {Veracity Classification, LSTM., Variational Autoencoder, Stance Classification, Text Mining, Rumor Tracking, Rumor Detection, False Rumors, Fake News, Misinformation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380055,
author = {Joshi, Nikesh and Spezzano, Francesca and Green, Mayson and Hill, Elijah},
title = {Detecting Undisclosed Paid Editing in Wikipedia},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380055},
doi = {10.1145/3366423.3380055},
abstract = {Wikipedia, the free and open-collaboration based online encyclopedia, has millions of pages that are maintained by thousands of volunteer editors. As per Wikipedia’s fundamental principles, pages on Wikipedia are written with a neutral point of view and maintained by volunteer editors for free with well-defined guidelines in order to avoid or disclose any conflict of interest. However, there have been several known incidents where editors intentionally violate such guidelines in order to get paid (or even extort money) for maintaining promotional spam articles without disclosing such. In this paper, we address for the first time the problem of identifying undisclosed paid articles in Wikipedia. We propose a machine learning-based framework using a set of features based on both the content of the articles as well as the patterns of edit history of users who create them. To test our approach, we collected and curated a new dataset from English Wikipedia with ground truth on undisclosed paid articles. Our experimental evaluation shows that we can identify undisclosed paid articles with an AUROC of 0.98 and an average precision of 0.91. Moreover, our approach outperforms ORES, a scoring system tool currently used by Wikipedia to automatically detect damaging content, in identifying undisclosed paid articles. Finally, we show that our user-based features can also detect undisclosed paid editors with an AUROC of 0.94 and an average precision of 0.92, outperforming existing approaches.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2899–2905},
numpages = {7},
keywords = {Detection of abusive content, Wikipedia, Malicious editors, Sockpuppet accounts.},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380056,
author = {Kushner, Taisa and Sharma, Amit},
title = {Bursts of Activity: Temporal Patterns of Help-Seeking and Support in Online Mental Health Forums},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380056},
doi = {10.1145/3366423.3380056},
abstract = {Recent years have seen a rise in social media platforms that provide peer-to-peer support to individuals suffering from mental distress. Studies on the impact of these platforms have focused on either short-term scales of single-post threads, or long-term changes over arbitrary period of time (months or years). While important, such periods of time do not necessarily follow users’ progressions through acute periods of distress. Using data from Talklife, a mental health platform, we find that user activity follows a distinct pattern of high activity periods with interleaving periods of no activity, and propose a method for identifying such bursts &amp; breaks in activity. We then show how studying activity during bursts can provide a personalized, medium-term analysis for a key question in online mental health communities: What characteristics of user activity lead some users to find support and help, while others fall short? Using two independent outcome metrics, moments of cognitive change and self-reported changes in mood during a burst of activity, we identify two actionable features that can improve outcomes for users: persistence within bursts, and giving complex emotional support to others. Our results demonstrate the value of considering bursts as a natural unit of analysis for psychosocial change in online mental health communities. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2906–2912},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380057,
author = {Colini-Baldeschi, Riccardo and Leonardi, Stefano and Schrijvers, Okke and Sodomka, Eric},
title = {Envy, Regret, and Social Welfare Loss},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380057},
doi = {10.1145/3366423.3380057},
abstract = {Incentive compatibility (IC) is a desirable property for any auction mechanism, including those used in online advertising. However, in real world applications practical constraints and complex environments often result in mechanisms that lack incentive compatibility. Recently, several papers investigated the problem of deploying black-box statistical tests to determine if an auction mechanism is incentive compatible by using the notion of IC-Regret that measures the regret of a truthful bidder. Unfortunately, most of those methods are computationally intensive, since they require the execution of many counterfactual experiments. In this work, we show that similar results can be obtained using the notion of IC-Envy. The advantage of IC-Envy is its efficiency: it can be computed using only the auction’s outcome. In particular, we focus on position auctions. For position auctions, we show that for a large class of pricing schemes (which includes e.g. VCG and GSP), IC-Envy ≥ IC-Regret (and IC-Envy = IC-Regret under mild supplementary conditions). Our theoretical results are completed showing that, in the position auction environment, IC-Envy can be used to bound the loss in social welfare due to the advertiser untruthful behavior. Finally, we show experimentally that IC-Envy can be used as a feature to predict IC-Regret in settings not covered by the theoretical results. In particular, using IC-Envy yields better results than training models using only price and value features. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2913–2919},
numpages = {7},
keywords = {Envy-freeness, Social Welfare Loss, Incentive-Compatibility Measurement, Position Auctions},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380058,
author = {Kumari, Nupur and R., Siddarth and Rupela, Akash and Gupta, Piyush and Krishnamurthy, Balaji},
title = {ShapeVis: High-Dimensional Data Visualization at Scale},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380058},
doi = {10.1145/3366423.3380058},
abstract = {We present ShapeVis, a scalable visualization technique for point cloud data inspired from topological data analysis. Our method captures the underlying geometric and topological structure of the data in a compressed graphical representation. Much success has been reported by the data visualization technique Mapper, that discreetly approximates the Reeb graph of a filter function on the data. However, when using standard dimensionality reduction algorithms as the filter function, Mapper suffers from considerable computational cost. This makes it difficult to scale to high-dimensional data. Our proposed technique relies on finding a subset of points called landmarks along the data manifold to construct a weighted witness-graph over it. This graph captures the structural characteristics of the point cloud, and its weights are determined using a Finite Markov Chain. We further compress this graph by applying induced maps from standard community detection algorithms. Using techniques borrowed from manifold tearing, we prune and reinstate edges in the induced graph based on their modularity to summarize the shape of data. We empirically demonstrate how our technique captures the structural characteristics of real and synthetic data sets. Further, we compare our approach with Mapper using various filter functions like t-SNE, UMAP, LargeVis and show that our algorithm scales to millions of data points while preserving the quality of data visualization.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2920–2926},
numpages = {7},
keywords = {visualization, manifold learning, topological data analysis},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380059,
author = {Nassar, Huda and Kennedy, Caitlin and Jain, Shweta and Benson, Austin R. and Gleich, David},
title = {Using Cliques with Higher-Order Spectral Embeddings Improves Graph Visualizations},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380059},
doi = {10.1145/3366423.3380059},
abstract = {In the simplest setting, graph visualization is the problem of producing a set of two-dimensional coordinates for each node that meaningfully shows connections and latent structure in a graph. Among other uses, having a meaningful layout is often useful to help interpret the results from network science tasks such as community detection and link prediction. There are several existing graph visualization techniques in the literature that are based on spectral methods, graph embeddings, or optimizing graph distances. Despite the large number of methods, it is still often challenging or extremely time consuming to produce meaningful layouts of graphs with hundreds of thousands of vertices. Existing methods often either fail to produce a visualization in a meaningful time window, or produce a layout colorfully called a “hairball”, which does not illustrate any internal structure in the graph. Here, we show that adding higher-order information based on cliques to a classic eigenvector based graph visualization technique enables it to produce meaningful plots of large graphs. We further evaluate these visualizations along a number of graph visualization metrics and we find that it outperforms existing techniques on a metric that uses random walks to measure the local structure. Finally, we show many examples of how our algorithm successfully produces layouts of large networks. Code to reproduce our results is available.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2927–2933},
numpages = {7},
keywords = {spectral methods, cliques sampling, graph visualization, graph layout, higher-order methods},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380060,
author = {Xie, Yuqing and Yang, Wei and Tan, Luchen and Xiong, Kun and Yuan, Nicholas Jing and Huai, Baoxing and Li, Ming and Lin, Jimmy},
title = {Distant Supervision for Multi-Stage Fine-Tuning in Retrieval-Based Question Answering},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380060},
doi = {10.1145/3366423.3380060},
abstract = {We tackle the problem of question answering directly on a large document collection, combining simple “bag of words” passage retrieval with a BERT-based reader for extracting answer spans. In the context of this architecture, we present a data augmentation technique using distant supervision to automatically annotate paragraphs as either positive or negative examples to supplement existing training data, which are then used together to fine-tune BERT. We explore a number of details that are critical to achieving high accuracy in this setup: the proper sequencing of different datasets during fine-tuning, the balance between “difficult” vs. “easy” examples, and different approaches to gathering negative examples. Experimental results show that, with the appropriate settings, we can achieve large gains in effectiveness on two English and two Chinese QA datasets. We are able to achieve results at or near the state of the art without any modeling advances, which once again affirms the clich\'{e} “there’s no data like more data”.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2934–2940},
numpages = {7},
keywords = {BERT, data augmentation, reranking},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380061,
author = {Artemenkov, Aleksandr and Panov, Maxim},
title = {NCVis: Noise Contrastive Approach for Scalable Visualization},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380061},
doi = {10.1145/3366423.3380061},
abstract = {Modern methods for data visualization via dimensionality reduction, such as t-SNE, usually have performance issues that prohibit their application to large amounts of high-dimensional data. In this work, we propose NCVis – a high-performance dimensionality reduction method built on a sound statistical basis of noise contrastive estimation. We show that NCVis outperforms state-of-the-art techniques in terms of speed while preserving the representation quality of other methods. In particular, the proposed approach successfully proceeds a large dataset of more than 1 million news headlines in several minutes and presents the underlying structure in a human-readable way. Moreover, it provides results consistent with classical methods like t-SNE on more straightforward datasets like images of hand-written digits. We believe that the broader usage of such software can significantly simplify the large-scale data analysis and lower the entry barrier to this area.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2941–2947},
numpages = {7},
keywords = {embedding algorithms, visualization, dimensionality reduction, noise contrastive estimation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380062,
author = {Woo, Simon and Jang, Hanbin and Ji, Woojung and Kim, Hyoungshick},
title = {I’ve Got Your Packages: Harvesting Customers’ Delivery Order Information Using Package Tracking Number Enumeration Attacks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380062},
doi = {10.1145/3366423.3380062},
abstract = {A package tracking number (PTN) is widely used to monitor and track a shipment. Through the lenses of security and privacy, however, a package tracking number can possibly reveal certain personal information, leading to security and privacy breaches. In this work, we examine the privacy issues associated with online package tracking systems used in the top three most popular package delivery service providers (FedEx, DHL, and UPS) in the world and found that those websites inadvertently leak users’ personal data with a PTN. Moreover, we discovered that PTNs are highly structured and predictable. Therefore, customers’ personal data can be massively collected via PTN enumeration attacks. We analyzed more than one million package tracking records obtained from Fedex, DHL, and UPS, and showed that within 5 attempts, an attacker can efficiently guess more than 90% of PTNs for FedEx and DHL, and close to 50% of PTNs for UPS. In addition, we present two practical attack scenarios: 1) to infer business transactions information and 2) to uniquely identify recipients. Also, we found that more than 109 recipients can be uniquely identified with less than 10 comparisons by linking the PTN information with the online people search service, Whitepages. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2948–2954},
numpages = {7},
keywords = {Enumeration Attacks, User Privacy, Package Tracking},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380063,
author = {Hu, Xiao and Wang, Haobo and Vegesana, Anirudh and Dube, Somesh and Yu, Kaiwen and Kao, Gore and Chen, Shuo-Han and Lu, Yung-Hsiang and Thiruvathukal, George K. and Yin, Ming},
title = {Crowdsourcing Detection of Sampling Biases in Image Datasets},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380063},
doi = {10.1145/3366423.3380063},
abstract = {Despite many exciting innovations in computer vision, recent studies reveal a number of risks in existing computer vision systems, suggesting results of such systems may be unfair and untrustworthy. Many of these risks can be partly attributed to the use of a training image dataset that exhibits sampling biases and thus does not accurately reflect the real visual world. Being able to detect potential sampling biases in the visual dataset prior to model development is thus essential for mitigating the fairness and trustworthy concerns in computer vision. In this paper, we propose a three-step crowdsourcing workflow to get humans into the loop for facilitating bias discovery in image datasets. Through two sets of evaluation studies, we find that the proposed workflow can effectively organize the crowd to detect sampling biases in both datasets that are artificially created with designed biases and real-world image datasets that are widely used in computer vision research and system development.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2955–2961},
numpages = {7},
keywords = {image dataset, workflow design, crowdsourcing, sampling bias},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380064,
author = {Rongali, Subendhu and Soldaini, Luca and Monti, Emilio and Hamza, Wael},
title = {Don’t Parse, Generate! A Sequence to Sequence Architecture for Task-Oriented Semantic Parsing},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380064},
doi = {10.1145/3366423.3380064},
abstract = {Virtual assistants such as Amazon Alexa, Apple Siri, and Google Assistant often rely on a semantic parsing component to understand which action(s) to execute for an utterance spoken by its users. Traditionally, rule-based or statistical slot-filling systems have been used to parse “simple” queries; that is, queries that contain a single action and can be decomposed into a set of non-overlapping entities. More recently, shift-reduce parsers have been proposed to process more complex utterances. These methods, while powerful, impose specific limitations on the type of queries that can be parsed; namely, they require a query to be representable as a parse tree. In this work, we propose a unified architecture based on Sequence to Sequence models and Pointer Generator Network to handle both simple and complex queries. Unlike other works, our approach does not impose any restriction on the semantic parse schema. Furthermore, experiments show that it achieves state of the art performance on three publicly available datasets (ATIS, SNIPS, Facebook TOP), relatively improving between 3.3% and 7.7% in exact match accuracy over previous systems. Finally, we show the effectiveness of our approach on two internal datasets.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2962–2968},
numpages = {7},
keywords = {Sequence to Sequence models, Voice Assistants, Natural Language Understanding, Semantic Parsing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380065,
author = {Zhao, Xing and Zhu, Ziwei and Alfifi, Majid and Caverlee, James},
title = {Addressing the Target Customer Distortion Problem in Recommender Systems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380065},
doi = {10.1145/3366423.3380065},
abstract = {Predicting the potential target customers for a product is essential. However, traditional recommender systems typically aim to optimize an engagement metric without considering the overall distribution of target customers, thereby leading to serious distortion problems. In this paper, we conduct a data-driven study to reveal several distortions that arise from conventional recommenders. Toward overcoming these issues, we propose a target customer re-ranking algorithm to adjust the population distribution and composition in the Top-k target customers of an item while maintaining recommendation quality. By applying this proposed algorithm onto a real-world dataset, we find the proposed method can effectively make the class distribution of items’ target customers close to the desired distribution, thereby mitigating distortion.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2969–2975},
numpages = {7},
keywords = {Distribution Distortion, Recommendation System, Calibration},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380066,
author = {Abebe, Rediet and Giorgi, Salvatore and Tedijanto, Anna and Buffone, Anneke and Schwartz, H. Andrew Andrew},
title = {Quantifying Community Characteristics of Maternal Mortality Using Social Media},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380066},
doi = {10.1145/3366423.3380066},
abstract = {While most mortality rates have decreased in the US, maternal mortality has increased and is among the highest of any OECD nation. Extensive public health research is ongoing to better understand the characteristics of communities with relatively high or low rates. In this work, we explore the role that social media language can play in providing insights into such community characteristics. Analyzing pregnancy-related tweets generated in US counties, we reveal a diverse set of latent topics including Morning Sickness, Celebrity Pregnancies, and Abortion Rights. We find that rates of mentioning these topics on Twitter predicts maternal mortality rates with higher accuracy than standard socioeconomic and risk variables such as income, race, and access to health-care, holding even after reducing the analysis to six topics chosen for their interpretability and connections to known risk factors. We then investigate psychological dimensions of community language, finding the use of less trustful, more stressed, and more negative affective language is significantly associated with higher mortality rates, while trust and negative affect also explain a significant portion of racial disparities in maternal mortality. We discuss the potential for these insights to inform actionable health interventions at the community-level.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2976–2983},
numpages = {8},
keywords = {topic modeling, community characteristics, health disparities, language, maternal mortality},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380067,
author = {Zhang, Yin and He, Yun and Wang, Jianling and Caverlee, James},
title = {Adaptive Hierarchical Translation-Based Sequential Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380067},
doi = {10.1145/3366423.3380067},
abstract = {We propose an adaptive hierarchical translation-based sequential recommendation called HierTrans that first extends traditional item-level relations to the category-level, to help capture dynamic sequence patterns that can generalize across users and time. Then unlike item-level based methods, we build a novel hierarchical temporal graph that contains item multi-relations at the category-level and user dynamic sequences at the item-level. Based on the graph, HierTrans adaptively aggregates the high-order multi-relations among items and dynamic user preferences to capture the dynamic joint influence for next-item recommendation. Specifically, the user translation vector in HierTrans can adaptively change based on both a user’s previous interacted items and the item relations inside the user’s sequences, as well as the user’s personal dynamic preference. Experiments on public datasets demonstrate the proposed model HierTrans consistently outperforms state-of-the-art sequential recommendation methods. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2984–2990},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380068,
author = {Raji, Shahab and de Melo, Gerard},
title = {What Sparks Joy: The AffectVec Emotion Database},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380068},
doi = {10.1145/3366423.3380068},
abstract = {Affective analysis of textual data is instrumental in understanding human communication in the modern era of social media. A number of resources have been proposed in attempts to characterize the emotions tied to words in a text. In this work, we show that we can obtain a database that goes beyond the common binary scores for emotion classification provided by past work. Instead, we harness the power of Big Data by using neural vector space models trained with large-scale supervision from co-occurrence patterns. We modify the vector space to better account for emotional associations, which then enables us to induce AffectVec, a new emotion database providing graded emotion intensity scores for English language words with regard to a fine-grained inventory of over 200 different emotion categories. Our experiments show that AffectVec outperforms existing emotion lexicons by substantial margins in intrinsic evaluations as well as for affective text classification.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2991–2997},
numpages = {7},
keywords = {emotion lexicon, affective computing, language resources},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380069,
author = {Rashtchian, Cyrus and Sharma, Aneesh and Woodruff, David},
title = {LSF-Join: Locality Sensitive Filtering for Distributed All-Pairs Set Similarity Under Skew},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380069},
doi = {10.1145/3366423.3380069},
abstract = {All-pairs set similarity is a widely used data mining task, even for large and high-dimensional datasets. Traditionally, similarity search has focused on discovering very similar pairs, for which a variety of efficient algorithms are known. However, recent work has highlighted the importance of discovering pairs of sets with relatively small intersection sizes. For example, in a recommender system, two users may be alike even though their interests only overlap on a small percentage of items. In such systems, it is also common that some dimensions are highly-skewed, because they are very popular. Together, these two properties render previous approaches infeasible for large input sizes. To address this problem, we present a new distributed algorithm, LSF-Join, for approximate all-pairs set similarity. The core of our algorithm is a randomized selection procedure based on Locality Sensitive Filtering. In particular, our method deviates from prior approximate algorithms, which are based on Locality Sensitive Hashing. Theoretically, we show that LSF-Join efficiently finds most close pairs, even for small similarity thresholds and for skewed input sets. We prove guarantees on the communication, work, and maximum load of LSF-Join, and we also experimentally demonstrate its accuracy on multiple graphs.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2998–3004},
numpages = {7},
keywords = {similarity search, social networks, distributed algorithms},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380070,
author = {Maros, Alexandre and Almeida, Jussara and Benevenuto, Fabr\'{\i}cio and Vasconcelos, Marisa},
title = {Analyzing the Use of Audio Messages in WhatsApp Groups},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380070},
doi = {10.1145/3366423.3380070},
abstract = {WhatsApp is a free messaging app with more than one billion active monthly users which has become one of the main communication platforms in many countries, including Saudi Arabia, Germany, and Brazil. In addition to allowing the direct exchange of messages among pairs of users, the app also enables group conversations, where multiple people can interact with one another. A number of recent studies have shown that WhatsApp groups play an important role as an information dissemination platform, especially during important social mobilization events. In this paper, we build upon those prior efforts by taking a first look into the use of audio messages in WhatsApp groups, a type of content that is becoming increasingly important in the platform. We present a methodology to analyze audio messages shared in WhatsApp groups, characterizing content properties (e.g, topics and language characteristics), their propagation dynamics and the impact of different types of audios (e.g., speech versus music) on such dynamics.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3005–3011},
numpages = {7},
keywords = {Content Propagation, Audio Messages, WhatsApp},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380071,
author = {Xu, Xuhai and Hassan Awadallah, Ahmed and T. Dumais, Susan and Omar, Farheen and Popp, Bogdan and Rounthwaite, Robert and Jahanbakhsh, Farnaz},
title = {Understanding User Behavior For Document Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380071},
doi = {10.1145/3366423.3380071},
abstract = {Personalized document recommendation systems aim to provide users with a quick shortcut to the documents they may want to access next, usually with an explanation about why the document is recommended. Previous work explored various methods for better recommendations and better explanations in different domains. However, there are few efforts that closely study how users react to the recommended items in a document recommendation scenario. We conducted a large-scale log study of users’ interaction behavior with the explainable recommendation on one of the largest cloud document platforms office.com. Our analysis reveals a number of factors, including display position, file type, authorship, recency of last access, and most importantly, the recommendation explanations, that are associated with whether users will recognize or open the recommended documents. Moreover, we specifically focus on explanations and conduct an online experiment to investigate the influence of different explanations on user behavior. Our analysis indicates that the recommendations help users access their documents significantly faster, but sometimes users miss a recommendation and resort to other more complicated methods to open the documents. Our results suggest opportunities to improve explanations and more generally the design of systems that provide and explain recommendations for documents. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {3012–3018},
numpages = {7},
keywords = {Large Scale Log Analysis, Explanation, Document Recommendation, User Behavior},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380072,
author = {Fang, Minghong and Gong, Neil Zhenqiang and Liu, Jia},
title = {Influence Function Based Data Poisoning Attacks to Top-N Recommender Systems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380072},
doi = {10.1145/3366423.3380072},
abstract = {Recommender system is an essential component of web services to engage users. Popular recommender systems model user preferences and item properties using a large amount of crowdsourced user-item interaction data, e.g., rating scores; then top-N items that match the best with a user’s preference are recommended to the user. In this work, we show that an attacker can launch a data poisoning attack to a recommender system to make recommendations as the attacker desires via injecting fake users with carefully crafted user-item interaction data. Specifically, an attacker can trick a recommender system to recommend a target item to as many normal users as possible. We focus on matrix factorization based recommender systems because they have been widely deployed in industry. Given the number of fake users the attacker can inject, we formulate the crafting of rating scores for the fake users as an optimization problem. However, this optimization problem is challenging to solve as it is a non-convex integer programming problem. To address the challenge, we develop several techniques to approximately solve the optimization problem. For instance, we leverage influence function to select a subset of normal users who are influential to the recommendations and solve our formulated optimization problem based on these influential users. Our results show that our attacks are effective and outperform existing methods. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {3019–3025},
numpages = {7},
keywords = {data poisoning attacks, Adversarial recommender systems, adversarial machine learning.},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380073,
author = {Qu, Liang and Zhu, Huaisheng and Duan, Qiqi and Shi, Yuhui},
title = {Continuous-Time Link Prediction via Temporal Dependent Graph Neural Network},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380073},
doi = {10.1145/3366423.3380073},
abstract = {Recently, graph neural networks (GNNs) have been shown to be an effective tool for learning the node representations of the networks and have achieved good performance on the semi-supervised node classification task. However, most existing GNNs methods fail to take networks’ temporal information into account, therefore, cannot be well applied to dynamic network applications such as the continuous-time link prediction task. To address this problem, we propose a Temporal Dependent Graph Neural Network (TDGNN), a simple yet effective dynamic network representation learning framework which incorporates the network temporal information into GNNs. TDGNN introduces a novel Temporal Aggregator (TDAgg) to aggregate the neighbor nodes’ features and edges’ temporal information to obtain the target node representations. Specifically, it assigns the neighbor nodes aggregation weights using an exponential distribution to bias different edges’ temporal information. The performance of the proposed method has been validated on six real-world dynamic network datasets for the continuous-time link prediction task. The experimental results show that the proposed method outperforms several state-of-the-art baselines.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3026–3032},
numpages = {7},
keywords = {dynamic networks, graph neural networks, link prediction, network representation learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380074,
author = {Almerekhi, Hind and Kwak, Haewoon and Salminen, Joni and Jansen, Bernard J.},
title = {Are These Comments Triggering? Predicting Triggers of Toxicity in Online Discussions},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380074},
doi = {10.1145/3366423.3380074},
abstract = {Understanding the causes or triggers of toxicity adds a new dimension to the prevention of toxic behavior in online discussions. In this research, we define toxicity triggers in online discussions as a non-toxic comment that lead to toxic replies. Then, we build a neural network-based prediction model for toxicity trigger. The prediction model incorporates text-based features and derived features from previous studies that pertain to shifts in sentiment, topic flow, and discussion context. Our findings show that triggers of toxicity contain identifiable features and that incorporating shift features with the discussion context can be detected with a ROC-AUC score of 0.87. We discuss implications for online communities and also possible further analysis of online toxicity and its root causes.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3033–3040},
numpages = {8},
keywords = {trigger detection, Reddit, neural networks, toxicity, online discussion},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380075,
author = {Scells, Harrisen and Zuccon, Guido and Sharaf, Mohamed A. and Koopman, Bevan},
title = {Sampling Query Variations for Learning to Rank to Improve Automatic Boolean Query Generation in Systematic Reviews},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380075},
doi = {10.1145/3366423.3380075},
abstract = {Searching medical literature for synthesis in a systematic review is a complex and labour intensive task. In this context, expert searchers construct lengthy Boolean queries. The universe of possible query variations can be massive: a single query can be composed of hundreds of field-restricted search terms/phrases or ontological concepts, each grouped by a logical operator nested to depths of sometimes five or more levels deep. With the many choices about how to construct a query, it is difficult to both formulate and recognise effective queries. To address this challenge, automatic methods have recently been explored for generating and selecting effective Boolean query variations for systematic reviews. The limiting factor of these methods is that it is computationally infeasible to process all query variations for training the methods. To overcome this, we propose novel query variation sampling methods for training Learning to Rank models to rank queries. Our results show that query sampling methods do directly impact the ability of a Learning to Rank model to effectively identify good query variations. Thus, selecting appropriate query sampling methods is a key problem for the automatic reformulation of effective Boolean queries for systematic review literature search. We find that the best sampling strategies are those which balance the diversity of queries with the quantity of queries.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3041–3048},
numpages = {8},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380076,
author = {Zhang, Zhen and Bu, Jiajun and Ester, Martin and Zhang, Jianfeng and Yao, Chengwei and Li, Zhao and Wang, Can},
title = {Learning Temporal Interaction Graph Embedding via Coupled Memory Networks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380076},
doi = {10.1145/3366423.3380076},
abstract = {Graph embedding has become the research focus in both academic and industrial communities due to its powerful capabilities. The majority of existing work overwhelmingly learn node embeddings in the context of static, plain or attributed, homogeneous graphs. However, many real-world applications frequently involve bipartite graphs with temporal and attributed interaction edges, named temporal interaction graphs. The temporal interactions usually imply different facets of interest and might even evolve over time, thus putting forward huge challenges in learning effective node representations. In this paper, we propose a novel framework named TigeCMN to learn node representations from a sequence of temporal interactions. Specifically, we devise two coupled memory networks to store and update node embeddings in external matrices explicitly and dynamically, which forms deep matrix representations and could enhance the expressiveness of the node embeddings. We conduct experiments on two real-world datasets and the experimental results empirically demonstrate that TigeCMN can outperform the state-of-the-arts with different gains.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3049–3055},
numpages = {7},
keywords = {Temporal Interaction Graphs, Graph Embedding},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380077,
author = {Wang, Wen and Zhang, Wei and Liu, Shukai and Liu, Qi and Zhang, Bo and Lin, Leyu and Zha, Hongyuan},
title = {Beyond Clicks: Modeling Multi-Relational Item Graph for Session-Based Target Behavior Prediction},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380077},
doi = {10.1145/3366423.3380077},
abstract = {Session-based target behavior prediction aims to predict the next item to be interacted with specific behavior types (e.g., clicking). Although existing methods for session-based behavior prediction leverage powerful representation learning approaches to encode items’ sequential relevance in a low-dimensional space, they suffer from several limitations. Firstly, they focus on only utilizing the same type of user behavior for prediction, but ignore the potential of taking other behavior data as auxiliary information. This is particularly crucial when the target behavior is sparse but important (e.g., buying or sharing an item). Secondly, item-to-item relations are modeled separately and locally in one behavior sequence, and they lack a principled way to globally encode these relations more effectively. To overcome these limitations, we propose a novel Multi-relational Graph Neural Network model for Session-based target behavior Prediction, namely MGNN-SPred for short. Specifically, we build a Multi-Relational Item Graph (MRIG) based on all behavior sequences from all sessions, involving target and auxiliary behavior types. Based on MRIG, MGNN-SPred learns global item-to-item relations and further obtains user preferences w.r.t. current target and auxiliary behavior sequences, respectively. In the end, MGNN-SPred leverages a gating mechanism to adaptively fuse user representations for predicting next item interacted with target behavior. The extensive experiments on two real-world datasets demonstrate the superiority of MGNN-SPred by comparing with state-of-the-art session-based prediction methods, validating the benefits of leveraging auxiliary behavior and learning item-to-item relations over MRIG.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3056–3062},
numpages = {7},
keywords = {user behavior modeling, graph neural networks, Sequential recommendation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380078,
author = {Farhang, Sadegh and Kirdan, Mehmet Bahadir and Laszka, Aron and Grossklags, Jens},
title = {An Empirical Study of Android Security Bulletins in Different Vendors},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380078},
doi = {10.1145/3366423.3380078},
abstract = {Mobile devices encroach on almost every part of our lives, including work and leisure, and contain a wealth of personal and sensitive information. It is, therefore, imperative that these devices uphold high security standards. A key aspect is the security of the underlying operating system. In particular, Android plays a critical role due to being the most dominant platform in the mobile ecosystem with more than one billion active devices and due to its openness, which allows vendors to adopt and customize it. Similar to other platforms, Android maintains security by providing monthly security patches and announcing them via the Android security bulletin. To absorb this information successfully across the Android ecosystem, impeccable coordination by many different vendors is required. In this paper, we perform a comprehensive study of 3,171 Android-related vulnerabilities and study to which degree they are reflected in the Android security bulletin, as well as in the security bulletins of three leading vendors: Samsung, LG, and Huawei. In our analysis, we focus on the metadata of these security bulletins (e.g., timing, affected layers, severity, and CWE data) to better understand the similarities and differences among vendors. We find that (i) the studied vendors in the Android ecosystem have adopted different structures for vulnerability reporting, (ii) vendors are less likely to react with delay for CVEs with Android Git repository references, (iii) vendors handle Qualcomm-related CVEs different from the rest of external layer CVEs.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3063–3069},
numpages = {7},
keywords = {Android Security Bulletins, Security, Technology Policy},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380079,
author = {Fan, Shaohua and Wang, Xiao and Shi, Chuan and Lu, Emiao and Lin, Ken and Wang, Bai},
title = {One2Multi Graph Autoencoder for Multi-View Graph Clustering},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380079},
doi = {10.1145/3366423.3380079},
abstract = {Multi-view graph clustering, which seeks a partition of the graph with multiple views that often provide more comprehensive yet complex information, has received considerable attention in recent years. Although some efforts have been made for multi-view graph clustering and achieve decent performances, most of them employ shallow model to deal with the complex relation within multi-view graph, which may seriously restrict the capacity for modeling multi-view graph information. In this paper, we make the first attempt to employ deep learning technique for attributed multi-view graph clustering, and propose a novel task-guided One2Multi graph autoencoder clustering framework. The One2Multi graph autoencoder is able to learn node embeddings by employing one informative graph view and content data to reconstruct multiple graph views. Hence, the shared feature representation of multiple graphs can be well captured. Furthermore, a self-training clustering objective is proposed to iteratively improve the clustering results. By integrating the self-training and autoencoder’s reconstruction into a unified framework, our model can jointly optimize the cluster label assignments and embeddings suitable for graph clustering. Experiments on real-world attributed multi-view graph datasets well validate the effectiveness of our model.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3070–3076},
numpages = {7},
keywords = {Graph Autoencoder, Attributed Multi-view Graph Clustering, Graph Convolutional Network},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380080,
author = {Wang, Xin and Li, Xu and Yu, Jinxing and Sun, Mingming and Li, Ping},
title = {Improved Touch-Screen Inputting Using Sequence-Level Prediction Generation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380080},
doi = {10.1145/3366423.3380080},
abstract = {Recent years have witnessed the continuing growth of people’s dependence on touchscreen devices. As a result, input speed with the onscreen keyboard has become crucial to communication efficiency and user experience. In this work, we formally discuss the general problem of input expectation prediction with a touch-screen input method editor (IME). Taken input efficiency as the optimization target, we proposed a neural end-to-end candidates generation solution to handle automatic correction, reordering, insertion, deletion as well as completion. Evaluation metrics are also discussed base on real use scenarios. For a more thorough comparison, we also provide a statistical strategy for mapping touch coordinate sequences to text input candidates. The proposed model and baselines are evaluated on a real-world dataset. The experiment (conducted on the PaddlePaddle deep learning platform1) shows that the proposed model outperforms the baselines.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3077–3083},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380081,
author = {Dey, Prasenjit and Goel, Kunal and Agrawal, Rahul},
title = {P-Simrank: Extending Simrank to Scale-Free Bipartite Networks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380081},
doi = {10.1145/3366423.3380081},
abstract = {The measure of similarity between nodes in a graph is a useful tool in many areas of computer science. SimRank, proposed by Jeh and Widom [7], is a classic measure of similarities of nodes in graph that has both theoretical and intuitive properties and has been extensively studied and used in many applications such as Query-Rewriting, link prediction, collaborative filtering and so on. Existing works based on Simrank primarily focus on preserving the microscopic structure, such as the second and third order proximity of the vertices, while the macroscopic scale-free property is largely ignored. Scale-free property is a critical property of any real-world web graphs where the vertex degrees follow a heavy-tailed distribution. In this paper, we introduce P-Simrank which extends the idea of Simrank to Scale-free bipartite networks. To study the efficacy of the proposed solution on a real world problem, we tested the same on the well known query-rewriting problem in sponsored search domain using bipartite click graph, similar to Simrank++ [1], which acts as our baseline. We show that Simrank++ produces sub-optimal similarity scores in case of bipartite graphs where degree distribution of vertices follow power-law. We also show how P-Simrank can be optimized for real-world large graphs. Finally, we experimentally evaluate P-Simrank algorithm against Simrank++, using actual click graphs obtained from Bing, and show that P-Simrank outperforms Simrank++ in variety of metrics.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3084–3090},
numpages = {7},
keywords = {Network Analysis, Power-law graphs, Simrank},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380082,
author = {Vieira, Carolina and Ribeiro, Filipe and Vaz de Melo, Pedro Olmo and Benevenuto, Fabricio and Zagheni, Emilio},
title = {Using Facebook Data to Measure Cultural Distance between Countries: The Case of Brazilian Cuisine},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380082},
doi = {10.1145/3366423.3380082},
abstract = {Measuring the affinity to a particular culture has been an active area of research. Countries and their residents can be characterized by many cultural aspects, such as clothing, music, art and food. As one of the central aspects, the cuisine of a country can reflect one of the dominant aspects of its culture. As such, the number of people interested in a typical national dish can be used to estimate the prevalence of that culture inside the host region. In this study, we measure the global spread of Brazilian culture across countries by exploring Facebook user’s preferences for typical Brazilian dishes through the Facebook Advertising Platform. To decide which dish will be considered typical from Brazil, we made use of spatial analysis to understand the distribution of interests around the world and to quantify how typical the dish is in Brazil and among Brazilian immigrants. This methodology can be generalized to other countries to infer cultural elements that emigrants usually take to and preserve in the countries they migrate to. Also, the interest in Brazilian typical dishes can be used to characterize countries in terms of Brazilian cultural exposition. While evaluating the cultural distance between Brazil and the countries with more Brazilian immigrants, we explore several measures of distance to compare these in the context of affinity to Brazilian cuisine. Our results revealed that these cultural distance measures can complement other metrics of distance applied to gravity-type models, for example, in order to explain flows of people between countries.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3091–3097},
numpages = {7},
keywords = {social media, social networks, sociology},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380083,
author = {Zhang, Liang and Wang, Xudong and Li, Hongsheng and Zhu, Guangming and Shen, Peiyi and Li, Ping and Lu, Xiaoyuan and Shah, Syed Afaq Ali and Bennamoun, Mohammed},
title = {Structure-Feature Based Graph Self-Adaptive Pooling},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380083},
doi = {10.1145/3366423.3380083},
abstract = {Various methods to deal with graph data have been proposed in recent years. However, most of these methods focus on graph feature aggregation rather than graph pooling. Besides, the existing top-k selection graph pooling methods have a few problems. First, to construct the pooled graph topology, current top-k selection methods evaluate the importance of the node from a single perspective only, which is simplistic and unobjective. Second, the feature information of unselected nodes is directly lost during the pooling process, which inevitably leads to a massive loss of graph feature information. To solve these problems mentioned above, we propose a novel graph self-adaptive pooling method with the following objectives: (1) to construct a reasonable pooled graph topology, structure and feature information of the graph are considered simultaneously, which provide additional veracity and objectivity in node selection; and (2) to make the pooled nodes contain sufficiently effective graph information, node feature information is aggregated before discarding the unimportant nodes; thus, the selected nodes contain information from neighbor nodes, which can enhance the use of features of the unselected nodes. Experimental results on four different datasets demonstrate that our method is effective in graph classification and outperforms state-of-the-art graph pooling methods.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3098–3104},
numpages = {7},
keywords = {graph pooling, graph neural network, graph classification},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380084,
author = {Zhang, Xingwen and Qi, Feng and Hua, Zhigang and Yang, Shuang},
title = {Solving Billion-Scale Knapsack Problems},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380084},
doi = {10.1145/3366423.3380084},
abstract = {Knapsack problems (KPs) are common in industry, but solving KPs is known to be NP-hard and has been tractable only at a relatively small scale. This paper examines KPs in a slightly generalized form and shows that they can be solved nearly optimally at scale via distributed algorithms. The proposed approach can be implemented fairly easily with off-the-shelf distributed computing frameworks (e.g. MPI, Hadoop, Spark). As an example, our implementation leads to one of the most efficient KP solvers known to date – capable to solve KPs at an unprecedented scale (e.g., KPs with 1 billion decision variables and 1 billion constraints can be solved within 1 hour). The system has been deployed to production and called on a daily basis, yielding significant business impacts at Ant Financial.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3105–3111},
numpages = {7},
keywords = {Dual Descent, Distributed Algorithms, Large-scale Optimization, Knapsack Problems, Synchronous Coordinate Descent, MapReduce},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380085,
author = {Bian, Weikang and Meng, Wei and Zhang, Mingxue},
title = {MineThrottle: Defending against Wasm In-Browser Cryptojacking},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380085},
doi = {10.1145/3366423.3380085},
abstract = {In-browser cryptojacking is an urgent threat to web users, where an attacker abuses the users’ computing resources without obtaining their consent. In-browser mining programs are usually developed in WebAssembly (Wasm) for its great performance. Several prior works have measured cryptojacking in the wild and proposed detection methods using static features and dynamic features. However, there exists no good defense mechanism within the user’s browser to stop the malicious drive-by mining behavior. In this work, we propose MineThrottle, a browser-based defense mechanism against Wasm cryptojacking. MineThrottle instruments Wasm code on the fly to detect mining behavior using block-level program profiling. It then throttles drive-by mining behavior based on a user-configurable policy. Our evaluation of MineThrottle with the Alexa top 1M websites demonstrates that it can accurately detect and mitigate in-browser cryptojacking with both a low false positive rate and a low false negative rate.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3112–3118},
numpages = {7},
keywords = {Cryptocurrency mining, Cryptojacking, WebAssembly},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380086,
author = {Naumzik, Christof and Feuerriegel, Stefan},
title = {One Picture Is Worth a Thousand Words? The Pricing Power of Images in e-Commerce},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380086},
doi = {10.1145/3366423.3380086},
abstract = {In e-commerce, product presentations, and particularly images, are known to provide important information for user decision-making, and yet the relationship between images and prices has not been studied. To close this research gap, we suggest a tailored web mining framework, since one must quantify the relative contribution of image content in describing prices ceteris paribus. That is, one must account for the fact that such images inherently depict heterogeneous products. In order to isolate the pricing power of image content, we suggest a three-stage framework involving deep learning and statistical inference. Our empirical evaluation draws upon a comprehensive dataset of more than 20,000 online real estate listings. We find that the image content describes a large portion of the variance in prices, even when controlling for location and common characteristics of apartments. A one standard deviation in the image variable is associated with a increase in price. By utilizing a carefully designed instrumental variables estimation, we further set out to obtain causal estimates. Our empirical findings contribute to theory by quantifying the hedonic value of images and thus establishing a causal link between visual appearance and product pricing. Even though a positive relationship seems intuitive, we provide for the first time an empirical confirmation. Based on our large-scale computational study, we further yield evidence of a picture superiority effect: simply put, a beneficial image corresponds to the same price change as 2856.03 additional words in the textual description. In sum, images capture valuable information for users that goes beyond narrative explanations. As a direct implication, we aid online platforms and their users in assessing and improving the multi-modal presentation of product offerings. Finally, we contribute to web mining by highlighting the importance of visual information.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3119–3125},
numpages = {7},
keywords = {E-commerce, Images, Data mining, Econometrics, Product presentation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380087,
author = {Pawelczyk, Martin and Broelemann, Klaus and Kasneci, Gjergji},
title = {Learning Model-Agnostic Counterfactual Explanations for Tabular Data},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380087},
doi = {10.1145/3366423.3380087},
abstract = {Counterfactual explanations can be obtained by identifying the smallest change made to an input vector to influence a prediction in a positive way from a user’s viewpoint; for example, from ’loan rejected’ to ’awarded’ or from ’high risk of cardiovascular disease’ to ’low risk’. Previous approaches would not ensure that the produced counterfactuals be proximate (i.e., not local outliers) and connected to regions with substantial data density (i.e., close to correctly classified observations), two requirements known as counterfactual faithfulness. Our contribution is twofold. First, drawing ideas from the manifold learning literature, we develop a framework, called C-CHVAE, that generates faithful counterfactuals. Second, we suggest to complement the catalog of counterfactual quality measures using a criterion to quantify the degree of difficulty for a certain counterfactual suggestion. Our real world experiments suggest that faithful counterfactuals come at the cost of higher degrees of difficulty.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3126–3132},
numpages = {7},
keywords = {Counterfactual explanations, Transparency, Interpretability},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380088,
author = {Xi, Dongbo and Zhuang, Fuzhen and Zhou, Ganbin and Cheng, Xiaohu and Lin, Fen and He, Qing},
title = {Domain Adaptation with Category Attention Network for Deep Sentiment Analysis},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380088},
doi = {10.1145/3366423.3380088},
abstract = {Domain adaptation tasks such as cross-domain sentiment classification aim to utilize existing labeled data in the source domain and unlabeled or few labeled data in the target domain to improve the performance in the target domain via reducing the shift between the data distributions. Existing cross-domain sentiment classification methods need to distinguish pivots, i.e., the domain-shared sentiment words, and non-pivots, i.e., the domain-specific sentiment words, for excellent adaptation performance. In this paper, we first design a Category Attention Network (CAN), and then propose a model named CAN-CNN to integrate CAN and a Convolutional Neural Network (CNN). On the one hand, the model regards pivots and non-pivots as unified category attribute words and can automatically capture them to improve the domain adaptation performance; on the other hand, the model makes an attempt at interpretability to learn the transferred category attribute words. Specifically, the optimization objective of our model has three different components: 1) the supervised classification loss; 2) the distributions loss of category feature weights; 3) the domain invariance loss. Finally, the proposed model is evaluated on three public sentiment analysis datasets and the results demonstrate that CAN-CNN can outperform other various baseline methods.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3133–3139},
numpages = {7},
keywords = {Interpretability, Sentiment Analysis, Domain Adaptation, Category Attention Network},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3382667,
author = {Gil, Yolanda},
title = {Embedding the Scientific Record on the Web: Towards Automating Scientific Discoveries},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3382667},
doi = {10.1145/3366423.3382667},
abstract = {Future AI systems will be key contributors to science, but this is unlikely to happen unless we reinvent our current publications and embed our scientific records in the Web as structured Web objects. This implies that our scientific papers of the future will be complemented with explicit, structured descriptions of the experiments, software, data, and workflows used to reach new findings. These scientific papers of the future will not only culminate the promise of open science and reproducible research, but also enable the creation of AI systems that can ingest and organize scientific methods and processes, re-run experiments and re-analyze results, and explore their own hypothesis in systematic and unbiased ways. In this talk, I will describe guidelines for writing scientific papers of the future that embed the scientific record on the Web, and our progress on AI systems capable of using them to systematically explore experiments. I will also outline a research agenda with seven key characteristics for creating AI scientists that will exploit the Web to independently make new discoveries [1]. AI scientists have the potential to transform science and the processes of scientific discovery [2, 3].},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3140},
numpages = {1},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3382668,
author = {Nigel Shadbolt, Sir},
title = {Architectures for Autonomy: Towards an Equitable Web of Data in the Age of AI},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3382668},
doi = {10.1145/3366423.3382668},
abstract = {Today, the Web connects over half the world's population, many of whom use it to stay connected to a multiplicity of vital digital public and private services, impacting every aspect of their lives. Access to the Web and underlying Internet is seen as essential for all—even a fundamental human right [7]. However, many contend that the power structure on large swaths of the Web has become inverted; they argue that instead of being run for and by users, it has been made to serve the platforms themselves, and the powerful actors that sponsor such platforms to run targeted advertising on their behalf. In such an ad-driven platform ecosystem, users, including their beliefs, data, and attention, have become traded commodities [13].There is concern that the emergence of powerful data analytics and AI techniques threaten to further entrench the power of these same platforms, by putting the control of powerful and valuable new capabilities in their hands rather than the users who produce the data [10]. The fear is that it is giving rise to data and AI monopolies [2,6]. Individuals have no long-term control or agency over their personal data or many of the decisions made using it.This may be one reason we are witnessing a so called Renaissance of Ethics - a plethora of initiatives and activities that call out the range of threats to individual autonomy, self-determination and privacy, the lack of transparency and accountability, a concern around bias and fairness, equity and access in our data driven ecosystem. This keynote will argue as the remaining half of the world's population comes online, we need digital infrastructures that will promote a plurality of methods of data sovereignty and governance instead of imposing a ’single policy fits-all’ platform governance model, which has strained and undermined the ability for governments to protect and support their citizens digital rights.This is an opportunity to re-imagine and re-architect elements of the Web, data, algorithms and institutions so as to ensure a more equitable distribution of these new digital potentialities. Based on our existing research we have been developing methods and tech-nologies pertaining to the following core principles: informational self-determination and autonomy, balanced and equitable access to AI and data, accountability and redress of AI/algorithmic decisions, and new models of ethical participation and contribution.The technology that underpins the modern web has seen exponential rates of change that have continuously improved the capabilities of the processors, memory and communications upon which it depends. This has enabled huge amounts of data to be linked and stored as well as providing for increasing use of AI. A variety of projects will be described where we sought to unlock the potential of this increasingly powerful infrastructure [1, 4, 5, 9]. The lessons learnt through various efforts to develop the Seman-tic Web [8] and the insights gained through the release of open data at scale will be reviewed [11]. We will review our attempts to understand how the blending of humans, algorithms and data at scale results in social machines whose emergent properties results in behaviour and problem solving which any of the individual elements would not have been able to achieve [12]. Understanding these emergent properties of the web was one of the motivating factors behind the establishment of Web Science [3]. We will briefly review the prospects for Web Science.The importance of data as infrastructure to enable wide spread innovation, accountability and trusted reproducible science will be stressed. Recent work will be described that seeks to promote an equitable and balanced Web environment in which privacy can be upheld and better mutualities realised. Developments in technical and institutional architectures that could underpin an Ethical Web of data will be outlined.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3141–3142},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3382669,
author = {Ma, Wei-Ying},
title = {Democratizing Content Creation and Dissemination through AI Technology},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3382669},
doi = {10.1145/3366423.3382669},
abstract = {With the rise of mobile video, user-generated content, and social networks, there is a massive opportunity for disruptive innovations in the media and content industry. It is now a fast-changing landscape with rapid advances in AI-powered content creation, dissemination and interaction technologies. I believe the current trends are leading us towards a world where everyone is equally empowered to produce high-quality content in video, music, augmented reality or more – and to share their information, knowledge, and stories with a large global audience. This new AI- powered content platform can further lead to innovations in advertising, e-commerce, online education, and productivity. I will share the current research efforts at ByteDance connected to this emerging new platform through products such as Douyin and TikTok, and discuss the challenges and the direction of our future research.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3143},
numpages = {1},
location = {Taipei, Taiwan},
series = {WWW '20}
}

