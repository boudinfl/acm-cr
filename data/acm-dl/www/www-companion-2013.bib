@inproceedings{10.1145/2487788.2487790,
author = {Le Hors, Arnaud J. and Speicher, Steve},
title = {The Linked Data Platform (LDP)},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487790},
doi = {10.1145/2487788.2487790},
abstract = {As a result of the Linked Data Basic Profile submission, made by several organizations including IBM, EMC, and Oracle, the W3C launched in June 2012 the Linked Data Platform (LDP) Working Group (WG).The LDP WG is chartered to produce a W3C Recommendation for HTTP-based (RESTful) application integration patterns using read/write Linked Data. This work will benefit both small-scale in-browser applications (WebApps) and large-scale Enterprise Application Integration (EAI) efforts. It will complement SPARQL and will be compatible with standards for publishing Linked Data, bringing the data integration features of RDF to RESTful, data-oriented software development.This presentation introduces developers to the Linked Data Platform, explains its origins in the Open Services Lifecycle Collaboration (OSLC) initiative, describes how it fits with other existing Semantic Web technologies and the problems developers will be able to address using LDP, based on use cases such as the integration challenge the industry faces in the Application Lifecycle Management (ALM) space.By attending this presentation developers will get an understanding of this upcoming W3C Recommendation which is posed to become a major stepping stone in enabling broader adoption of Linked Data in the industry, not only for publishing data but also for integrating applications.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1–2},
numpages = {2},
keywords = {w3c, ldp, application integration, rdf, linked data},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487791,
author = {Motti, Vivian Genaro and Raggett, Dave},
title = {Quill: A Collaborative Design Assistant for Cross Platform Web Application User Interfaces},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487791},
doi = {10.1145/2487788.2487791},
abstract = {Web application development teams face an increasing burden when they need to come up with a consistent user interface across different platforms with different characteristics, for example, desktop, smart phone and tablet devices. This is going to get even worse with the adoption of HTML5 on TVs and cars. This short paper describes a browser-based collaborative design assistant that does the drudge work of ensuring that the user interfaces are kept in sync across all of the target platforms and with changes to the domain data and task models. This is based upon an expert system that dynamically updates the user interface design to reflect the developer's decisions. This is implemented in terms of constraint propagation and search through the design space. An additional benefit is the ease of providing accessible user interfaces in conjunction with assistive technologies.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {3–6},
numpages = {4},
keywords = {model-based user interface design, collaborative assistant},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487792,
author = {Nixon, Lyndon},
title = {Linked Services Infrastructure: A Single Entry Point for Online Media Related to Any Linked Data Concept},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487792},
doi = {10.1145/2487788.2487792},
abstract = {In this submission, we describe the Linked Services Infrastructure (LSI). It uses Semantic Web Service technology to map individual concepts (identified by Linked Data URIs) to sets of online media content aggegrated from heterogeneous Web APIs. It exposes this mapping service in a RESTful API and returns RDF based responses for further processing if desired. The LSI can be used as a general purpose tool for user agents to retrieve different online media resources to illustrate a concept to a user.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {7–10},
numpages = {4},
keywords = {linked media, semantic web services, media retrieval, media selection, web apis, linked data, linked services},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487793,
author = {Haslhofer, Bernhard and Warner, Simeon and Lagoze, Carl and Klein, Martin and Sanderson, Robert and Nelson, Michael L. and Van de Sompel, Herbert},
title = {ResourceSync: Leveraging Sitemaps for Resource Synchronization},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487793},
doi = {10.1145/2487788.2487793},
abstract = {Many applications need up-to-date copies of collections of changing Web resources. Such synchronization is currently achieved using ad-hoc or proprietary solutions. We propose ResourceSync, a general Web resource synchronization protocol that leverages XML Sitemaps. It provides a set of capabilities that can be combined in a modular manner to meet local or community requirements. We report on work to implement this protocol for arXiv.org and also provide an experimental prototype for the English Wikipedia as well as a client API.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {11–14},
numpages = {4},
keywords = {resourcesync, web, resource synchronization},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487794,
author = {Canou, Benjamin and Chailloux, Emmanuel and Botbol, Vincent},
title = {Static Typing &amp; JavaScript Libraries: Towards a More Considerate Relationship},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487794},
doi = {10.1145/2487788.2487794},
abstract = {In this paper, after relating a short history of the mostly unhappy relationship between static typing and JavaScript (JS), we explain a new attempt at conciliating them which is more respectful of both worlds than other approaches. As an example, we present Onyo, an advanced binding of the Enyo JS library for the OCaml language. Onyo exploits the expressiveness of OCaml's type system to properly encode the structure of the library, preserving its design while statically checking that it is used correctly, and without introducing runtime overhead.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {15–18},
numpages = {4},
keywords = {ocaml, static typing, interoperability, javascript},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487795,
author = {Balat, Vincent},
title = {Client-Server Web Applications Widgets},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487795},
doi = {10.1145/2487788.2487795},
abstract = {The evolution of the Web from a content platform into an application platform has raised many new issues for developers. One of the most significant is that we are now developing distributed applications, in the specific context of the underlying Web technologies. In particular, one should be able to compute some parts of the page either on server or client sides, depending on the needs of developers, and preferably in the same language, with the same functions. This paper deals with the particular problem of user interface generation in this client-server setting. Many widget libraries for browsers are fully written in JavaScript and do not allow to generate the interface on server side, making more difficult the indexing of pages by search engines. We propose a solution that makes possible to generate widgets either on client side or on server side in a very flexible way. It is implemented in the Ocsigen framework.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {19–22},
numpages = {4},
keywords = {web applications, graphical user interfaces, mobile applications, javascript},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487796,
author = {Grasso, Giovanni and Furche, Tim and Schallhart, Christian},
title = {Effective Web Scraping with OXPath},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487796},
doi = {10.1145/2487788.2487796},
abstract = {Even in the third decade of the Web, scraping web sites remains a challenging task: Most scraping programs are still developed as ad-hoc solutions using a complex stack of languages and tools. Where comprehensive extraction solutions exist, they are expensive, heavyweight, and proprietary.OXPath is a minimalistic wrapping language that is nevertheless expressive and versatile enough for a wide range of scraping tasks. In this presentation, we want to introduce you to a new paradigm of scraping: declarative navigation--instead of complex scripting or heavyweight, limited visual tools, OXPath turns scraping into a simple two step process: pick the relevant nodes through an XPath expression and then specify which action to apply to those nodes. OXPath takes care of browser synchronisation, page and state management, making scraping as easy as node selection with XPath. To achieve this, OXPath does not require a complex or heavyweight infrastructure. OXPath is an open source project and has seen first adoption in a wide variety of scraping tasks.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {23–26},
numpages = {4},
keywords = {xpath, ajax, web automation, web extraction},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487797,
author = {Johansen, Richard Duchatsch and Britto, Talita Cristina Pagani and Cusin, Cesar Augusto},
title = {CSS Browser Selector plus: A JavaScript Library to Support Cross-Browser Responsive Design},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487797},
doi = {10.1145/2487788.2487797},
abstract = {Developing websites for multiples devices have been a rough task for the past ten years. Devices features - such as screen size, resolution, internet access, operating system, etc. - change frequently and new devices emerge every day. Since W3C introduced media queries in CSS3, it's possible to developed tailored interfaces for multiple devices using a single HTML document. The approach of Responsive Web Design has been used media queries as support for developing adaptive and flexible layouts, however, it's not supported in legacy browsers. In this paper, we present CSS Browser Selector Plus, a cross-browser alternative method using JavaScript to support CSS3 media queries for developing responsive web considering older browsers.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {27–30},
numpages = {4},
keywords = {javascript, cross-browser, responsive web design, web standards},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487798,
author = {Steiner, Thomas},
title = {A Meteoroid on Steroids: Ranking Media Items Stemming from Multiple Social Networks},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487798},
doi = {10.1145/2487788.2487798},
abstract = {We have developed an application called Social Media Illustrator that allows for finding media items on multiple social networks, clustering them by visual similarity, ranking them by different criteria, and finally arranging them in media galleries that were evaluated to be perceived as aesthetically pleasing. In this paper, we focus on the ranking aspect and show how, for a given set of media items, the most adequate ranking criterion combination can be found by interactively applying different criteria and seeing their effect on-the-fly. This leads us to an empirically optimized media item ranking formula, which takes social network interactions into account. While the ranking formula is not universally applicable, it can serve as a good starting point for an individually adapted formula, all within the context of Social Media Illustrator. A demo of the application is available publicly online at the URL http://social-media-illustrator.herokuapp.com/.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {31–34},
numpages = {4},
keywords = {social networks, event summarization, ranking},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487799,
author = {Lanthaler, Markus},
title = {Creating 3rd Generation Web APIs with Hydra},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487799},
doi = {10.1145/2487788.2487799},
abstract = {In this paper we describe a novel approach to build hypermedia-driven Web APIs based on Linked Data technologies such as JSON-LD. We also present the result of implementing a first prototype featuring both a RESTful Web API and a generic API client. To the best of our knowledge, no comparable integrated system to develop Linked Data-based APIs exists.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {35–38},
numpages = {4},
keywords = {rest, linked data, hydra, web, web services, json-ld, hypermedia},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487801,
author = {Tang, Lei and Harrington, Patrick},
title = {Scaling Matrix Factorization for Recommendation with Randomness},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487801},
doi = {10.1145/2487788.2487801},
abstract = {Recommendation is one of the core problems in eCommerce. In our application, different from conventional collaborative filtering, one user can engage in various types of activities in a sequence. Meanwhile, the number of users and items involved are quite huge, entailing scalable approaches. In this paper, we propose one simple approach to integrate multiple types of user actions for recommendation. A two-stage randomized matrix factorization is presented to handle large-scale collaborative filtering where alternating least squares or stochastic gradient descent is not viable. Empirical results show that the method is quite scalable, and is able to effectively capture correlations between different actions, thus making more relevant recommendations.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {39–40},
numpages = {2},
keywords = {ecommerce, matrix factorization, randomized matrix factorization, randomness, recommendation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487802,
author = {Li, Dong and Xu, Zhiming and Li, Sheng and Sun, Xin},
title = {Link Prediction in Social Networks Based on Hypergraph},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487802},
doi = {10.1145/2487788.2487802},
abstract = {In recent years, online social networks have undergone a significant growth and attracted much attention. In these online social networks, link prediction is a critical task that not only offers insights into the factors behind creation of individual social relationship but also plays an essential role in the whole network growth. In this paper, we propose a novel link prediction method based on hypergraph. In contrast with conventional methods that using ordinary graph, we model the social network as a hypergraph, which can fully capture all types of objects and either the pair wise or high-order relations among these objects in the network. Then the link prediction task is formulated as a ranking problem on this hypergraph. Experimental results on Sina-Weibo dataset have demonstrated the effectiveness of our methods.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {41–42},
numpages = {2},
keywords = {hypergraph, link prediction, ranking},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487803,
author = {Weber, Ingmar and Garimella, Venkata Rama Kiran and Borra, Erik},
title = {Inferring Audience Partisanship for YouTube Videos},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487803},
doi = {10.1145/2487788.2487803},
abstract = {Political campaigning and the corresponding advertisement money are increasingly moving online. Some analysts claim that the U.S.~elections were partly won through a smart use of (i) targeted advertising and (ii) social media. But what type of information do politicized users consume online? And, the other way around, for a given content, e.g. a YouTube video, is it possible to predict its political audience? To address this latter question, we present a large scale study of anonymous YouTube video consumption of politicized users, where political orientation is derived from visits to "beacon pages", namely, political partisan blogs. Though our techniques are relevant for targeted political advertising, we believe that our findings are also of a wider interest.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {43–44},
numpages = {2},
keywords = {youtube, partisan blogs, political polarization, audience prediction},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487804,
author = {Zheng, Ning and Jin, Xiaoming and Li, Lianghao},
title = {Cross-Region Collaborative Filtering for New Point-of-Interest Recommendation},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487804},
doi = {10.1145/2487788.2487804},
abstract = {With the rapid growth of location-based social networks (LBSNs), Point-of-Interest (POI) recommendation is in increasingly higher demand these years. In this paper, our aim is to recommend new POIs to a user in regions where he has rarely been before. Different from the classical memory-based recommendation algorithms using user rating data to compute similarity between users or items to make recommendation, we propose a cross-region collaborative filtering method based on hidden topics mined from user check-in records to recommend new POIs. Experimental results on a real-world LBSNs dataset show that our method consistently outperforms naive CF method.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {45–46},
numpages = {2},
keywords = {collaborative filtering, location based social network, cross-region},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487805,
author = {Mukherjee, Subhabrata and Basu, Gaurab and Joshi, Sachindra},
title = {Incorporating Author Preference in Sentiment Rating Prediction of Reviews},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487805},
doi = {10.1145/2487788.2487805},
abstract = {Traditional works in sentiment analysis do not incorporate author preferences during sentiment classification of reviews. In this work, we show that the inclusion of author preferences in sentiment rating prediction of reviews improves the correlation with ground ratings, over a generic author independent rating prediction model. The overall sentiment rating prediction for a review has been shown to improve by capturing facet level rating. We show that this can be further developed by considering author preferences in predicting the facet level ratings, and hence the overall review rating. To the best of our knowledge, this is the first work to incorporate author preferences in rating prediction.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {47–48},
numpages = {2},
keywords = {sentiment analysis, aspect rating, author preference},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487806,
author = {Kamath, Krishna Y. and Popescu, Ana-Maria and Caverlee, James},
title = {Board Coherence in Pinterest: Non-Visual Aspects of a Visual Site},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487806},
doi = {10.1145/2487788.2487806},
abstract = {Pinterest is a fast-growing interest network with significant user engagement and monetization potential. This paper explores quality signals for Pinterest boards, in particular the notion of board coherence. We find that coherence can be assessed with promising results and we explore its relation to quality signals based on social interaction.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {49–50},
numpages = {2},
keywords = {social media, quality analysis, pinterest},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487807,
author = {An, Jisun and Quercia, Daniele and Crowcroft, Jon},
title = {Fragmented Social Media: A Look into Selective Exposure to Political News},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487807},
doi = {10.1145/2487788.2487807},
abstract = {The hypothesis of selective exposure assumes that people crave like-minded information and eschew information that conflicts with their beliefs, and that has negative consequences on political life. Yet, despite decades of research, this hypothesis remains theoretically promising but empirically difficult to test. We look into news articles shared on Facebook and examine whether selective exposure exists or not in social media. We find a concrete evidence for a tendency that users predominantly share like-minded news articles and avoid conflicting ones, and partisans are more likely to do that. Building tools to counter partisanship on social media would require the ability to identify partisan users first. We will show that those users cannot be distinguished from the average user as the two subgroups do not show any demographic difference.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {51–52},
numpages = {2},
keywords = {online news consumption, facebook, selective exposure, news aggregator, computational political science, social media},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487808,
author = {Priest, Ben and Gold, Kevin},
title = {Utility Discounting Explains Informational Website Traffic Patterns before a Hurricane},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487808},
doi = {10.1145/2487788.2487808},
abstract = {We demonstrate that psychological models of utility discounting can explain the pattern of increased hits to weather websites in the days preceding a predicted weather disaster. We parsed the HTTP request lines issued by the web proxy for a mid-sized enterprise leading up to a hurricane, filtering for visits to weather-oriented websites. We fit four discounting models to the observed activity and found that our data matched hyperboloid models extending hyperbolic discounting.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {53–54},
numpages = {2},
keywords = {temporal discounting, humans, model comparison, delay discounting},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487809,
author = {Hadgu, Asmelash Teka and Garimella, Kiran and Weber, Ingmar},
title = {Political Hashtag Hijacking in the U.S.},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487809},
doi = {10.1145/2487788.2487809},
abstract = {We study the change in polarization of hashtags on Twitter over time and show that certain jumps in polarity are caused by "hijackers" engaged in a particular type of hashtag war.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {55–56},
numpages = {2},
keywords = {Twitter, political leaning classification, partisanship, political trends},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487810,
author = {Feng, Wei and Wang, Jianyong},
title = {Learning to Annotate Tweets with Crowd Wisdom},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487810},
doi = {10.1145/2487788.2487810},
abstract = {In Twitter, users can annotate tweets with hashtags to indicate the ongoing topics. Hashtags provide users a convenient way to categorize tweets. However, two problems remain unsolved during an annotation: (1) Users have no way to know whether some related hashtags have already been created. (2) Users have their own way to categorize tweets. Thus personalization is needed. To address the above problems, we develop a statistical model for Personalized Hashtag Recommendation. With millions of "tweet, hashtag" pairs being generated everyday, we are able to learn the complex mappings from tweets to hashtags with the wisdom of the crowd. Our model considers rich auxiliary information like URLs, locations, social relation, temporal characteristics of hashtag adoption, etc. We show our model successfully outperforms existing methods on real datasets crawled from Twitter.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {57–58},
numpages = {2},
keywords = {recommender systems, social media},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487811,
author = {Zhu, Yanan and Goharian, Nazli},
title = {To Follow or Not to Follow: A Feature Evaluation},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487811},
doi = {10.1145/2487788.2487811},
abstract = {The features available in Twitter provide meaningful information that can be harvested to provide a ranked list of followees to each user. We hypothesize that retweet and mention features can be further enriched by incorporating both temporal and additional/indirect links from within user's community. Our empirical results provide insights into the effectiveness of each feature, and evaluate our proposed similarity measures in ranking the followees. Utilizing temporal information and indirect links improves the effectiveness of retweet and mention features in terms of nDCG.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {59–60},
numpages = {2},
keywords = {mention, Twitter, temporal ranking, retweet, personalization, social media, user recommendation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487812,
author = {Jain, Vidit and Galbrun, Esther},
title = {Topical Organization of User Comments and Application to Content Recommendation},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487812},
doi = {10.1145/2487788.2487812},
abstract = {On a news website, an article may receive thousands of comments from its readers on a variety of topics. The usual display of these comments in a ranked list, e.g. by popularity, does not allow the user to follow discussions on a particular topic. Organizing them by semantic topics enables the user not only to selectively browse comments on a topic, but also to discover other significant topics of discussion in comments. This topical organization further allows to explicitly capture the immediate interests of the user even when she is not logged in. Here we use this information to recommend content that is relevant in the context of the comments being read by the user. We present an algorithm for building such a topical organization in a practical setting and study different recommendation schemes. In a pilot study, we observe these comments-to-article recommendations to be preferred over the standard article-to-article recommendations.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {61–62},
numpages = {2},
keywords = {recommendation, user generated content, algorithm},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487813,
author = {Salem, Yasser and Hong, Jun},
title = {History-Aware Critiquing-Based Conversational Recommendation},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487813},
doi = {10.1145/2487788.2487813},
abstract = {In this paper we present a new approach to critiquing-based conversational recommendation, which we call History-Aware Critiquing (HAC). It takes a case-based reasoning approach by reusing relevant recommendation sessions of past users to short-cut the recommendation session of the current user. It selects relevant recommendation sessions from a case base that contains the successful recommendation sessions of past users. A past recommendation session can be selected if it contains similar recommended items to the ones in the current session and its critiques sufficiently overlap with the critiques so far in the current session. HAC extends experience-based critiquing (EBC).Our experimental results show that, in terms of recommendation efficiency, while EBC performs better than standard critiquing (STD), it does not perform as well as more recent techniques such as incremental critiquing (IC), whereas HAC achieves better recommendation efficiency over both STD and IC.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {63–64},
numpages = {2},
keywords = {conversational recommendation, recommender systems},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487814,
author = {Inagaki, Yoshiyuki and Bian, Jiang and Chang, Yi},
title = {An Effective General Framework for Localized Content Optimization},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487814},
doi = {10.1145/2487788.2487814},
abstract = {Local search services have been gaining interests from Web users who seek the information near certain geographical locations. Particularly, those users usually want to find interesting information about what is happening nearby. In this poster, we introduce the localized content optimization problem to provide Web users with authoritative, attractive and fresh information that are really interesting to people around the certain location. To address this problem, we propose a general learning framework and develop a variety of features. Our evaluations based on the data set from a commercial localized Web service demonstrate that our framework is highly effective at providing contents that are more relevant to users' localized information need.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {65–66},
numpages = {2},
keywords = {localized content optimization, online local service},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487815,
author = {Yang, Zhi and Xue, Ji long and Zhao, Han Xiao and Wang, Xiao and Zhao, Ben Y. and Dai, Yafei},
title = {Unfolding Dynamics in a Social Network: Co-Evolution Oflink Formation and User Interaction},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487815},
doi = {10.1145/2487788.2487815},
abstract = {Measurement studies of online social networks show that all social links are not equal, and the strength of each link is best characterized by the frequency of interactions between the linked users.To date, few studies have been able to examine detailed interaction data over time, and none have studied the problem of modeling user interactions. This paper proposes a generative model of social interactions that captures the inherently heterogeneous strengths of social links, thus having broad implications on the design of social network algorithms such as friend recommendation, information diffusion and viral marketing.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {67–68},
numpages = {2},
keywords = {activity network, graph modeling, social network},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487816,
author = {Orellana-Rodriguez, Claudia and Diaz-Aviles, Ernesto and Nejdl, Wolfgang},
title = {Mining Emotions in Short Films: User Comments or Crowdsourcing?},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487816},
doi = {10.1145/2487788.2487816},
abstract = {Short films are regarded as an alternative form of artistic creation, and they express, in a few minutes, a whole gamma of different emotions oriented to impact the audience and communicate a story. In this paper, we exploit a multi-modal sentiment analysis approach to extract emotions in short films, based on the film criticism expressed through social comments from the video-sharing platform YouTube. We go beyond the traditional polarity detection (i.e., positive/negative), and extract, for each analyzed film, four opposing pairs of primary emotions: joy-sadness, anger-fear, trust-disgust, and anticipation-surprise. We found that YouTube comments are a valuable source of information for automatic emotion detection when compared to human analysis elicited via crowdsourcing.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {69–70},
numpages = {2},
keywords = {social media analytics, computational social science, sentiment analysis, emotion detection, youtube},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487817,
author = {Khapra, Mitesh M. and Joshi, Salil and Ramanathan, Ananthakrishnan and Visweswariah, Karthik},
title = {Offering Language Based Services on Social Media by Identifying User's Preferred Language(s) from Romanized Text},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487817},
doi = {10.1145/2487788.2487817},
abstract = {With the increase of multilingual content and multilingual users on the web, it is prudent to offer personalized services and ads to users based on their language profile (textit{i.e.}, the list of languages that a user is conversant with). Identifying the language profile of a user is often non-trivial because (i) users often do not specify all the languages known to them while signing up for an online service (ii) users of many languages (especially Indian languages) largely use Latin/Roman script to write content in their native language. This makes it non-trivial for a machine to distinguish the language of one comment from another. This situation presents an opportunity for offering following language based services for romanized content (i) hide romanized comments which belong to a language which is not known to the user (ii) translate romanized comments which belong to a language which is not known to the user (iii) transliterate romanized comments which belong to a language which is known to the user (iv) show language based ads by identifying languages known to a user based on the romanized comments that he wrote/read/liked. We first use a simple bootstrapping based semi-supervised algorithm for identify the language of a romanized comment. We then apply this algorithm to all the comments written/read/liked by a user to build a language profile of the user and propose that this profile can be used to offer the services mentioned above.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {71–72},
numpages = {2},
keywords = {language profile, romanized content, social media},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487819,
author = {Gkotsis, George and Stepanyan, Karen and Cristea, Alexandra I. and Joy, Mike S.},
title = {Zero-Cost Labelling with Web Feeds for Weblog Data Extraction},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487819},
doi = {10.1145/2487788.2487819},
abstract = {Data extraction from web pages often involves either human intervention for training a wrapper or a reduced level of granularity in the information acquired. Even though the study of social media has drawn the attention of researchers, weblogs remain a part of the web that cannot be harvested efficiently. In this paper, we propose a fully automated approach in generating a wrapper for weblogs, which exploits web feeds for cheap labelling of weblog properties. Instead of performing a pairwise comparison between posts, the model matches the values of the web feeds against their corresponding HTML elements retrieved from multiple weblog posts. It adopts a probabilistic approach for deriving a set of rules and automating the process of wrapper generation. Our evaluation shows that our approach is robust, accurate and efficient in handling different types of weblogs.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {73–74},
numpages = {2},
keywords = {data extraction, wrapper induction, weblogs},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487820,
author = {Rodriguez Perez, Jesus A. and Moshfeghi, Yashar and Jose, Joemon M.},
title = {On Using Inter-Document Relations in Microblog Retrieval},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487820},
doi = {10.1145/2487788.2487820},
abstract = {Microblog Ad-hoc retrieval has received much attention in recent years. As a result of the high vocabulary diversity of the publishing users, a mismatch is formed between the queries being formulated and the tweets representing the actual topics. In this work, we present a re-ranking approach relying on inter-document relations, which attempts to bridge this gap. Experiments with TREC's Microblog 2012 collection show that including such information in the retrieval process, statistically significantly improves retrieval effectiveness in terms of Precision and MAP, when the baseline performs well as a starting point.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {75–76},
numpages = {2},
keywords = {ad-hoc retrieval, diversification, microblog, re-ranking, retrieval model},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487821,
author = {Fetahu, Besnik and Pereira Nunes, Bernardo and Dietze, Stefan},
title = {Towards Focused Knowledge Extraction: Query-Based Extraction of Structured Summaries},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487821},
doi = {10.1145/2487788.2487821},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {77–78},
numpages = {2},
keywords = {entity recognition, query-based summaries, pos pattern analysis, text summarization, knowledge extraction},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487822,
author = {Amer-Yahia, Sihem and Bonchi, Francesco and Castillo, Carlos and Feuerstein, Esteban and M\'{e}ndez-D\'{\i}az, Isabel and Zabala, Paula},
title = {Complexity and Algorithms for Composite Retrieval},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487822},
doi = {10.1145/2487788.2487822},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {79–80},
numpages = {2},
keywords = {complementarity, composite retrieval, diversity, maximum edge subgraph},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487823,
author = {Murnane, Elizabeth L. and Haslhofer, Bernhard and Lagoze, Carl},
title = {RESLVE: Leveraging User Interest to Improve Entity Disambiguation on Short Text},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487823},
doi = {10.1145/2487788.2487823},
abstract = {We address the Named Entity Disambiguation (NED) problem for short, user-generated texts on the social Web. In such settings, the lack of linguistic features and sparse lexical context result in a high degree of ambiguity and sharp performance drops of nearly 50% in the accuracy of conventional NED systems. We handle these challenges by developing a general model of user-interest with respect to a personal knowledge context and instantiate it using Wikipedia. We conduct systematic evaluations using individuals' posts from Twitter, YouTube, and Flickr and demonstrate that our novel technique is able to achieve performance gains beyond state-of-the-art NED methods.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {81–82},
numpages = {2},
keywords = {personalized ir, entity resolution, user interest modeling, social web, semantic knowledge graph},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487824,
author = {Stepanyan, Karen and Gkotsis, George and Banos, Vangelis and Cristea, Alexandra I. and Joy, Mike},
title = {A Hybrid Approach for Spotting, Disambiguating and Annotating Places in User-Generated Text},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487824},
doi = {10.1145/2487788.2487824},
abstract = {We introduce a geolocation-aware semantic annotation model that extends the existing solutions for spotting and disambiguation of places within user-generated texts. The implemented prototype processes the text of weblog posts and annotates the places and toponyms. It outperforms existing solutions by taking into consideration the embedded geolocation data. The evaluation of the model is based on a set of randomly selected 3,165 geolocation embedded weblog posts, obtained from 1,775 web feeds. The results demonstrate a high degree of accuracy in annotation (87.7%) and a considerable gain (27.8%) in identifying additional entities, and therefore support the adoption of the model for supplementing the existing solutions.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {83–84},
numpages = {2},
keywords = {named entity recognition, semantic annotation, geolocation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487825,
author = {Kondreddi, Sarath Kumar and Triantafillou, Peter and Weikum, Gerhard},
title = {HIGGINS: Knowledge Acquisition Meets the Crowds},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487825},
doi = {10.1145/2487788.2487825},
abstract = {We present HIGGINS, a system for Knowledge Acquisition (KA), placing emphasis on its architecture. The distinguishing characteristic and novelty of HIGGINS lies in its blending of two engines: an automated Information Extraction (IE) engine, aided by semantic resources and statistics, and a game-based Human Computing (HC) engine. We focus on KA from web pages and text sources and, in particular, on deriving relationships between entities. As a running application we utilize movie narratives, from which we wish to derive relationships among movie characters.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {85–86},
numpages = {2},
keywords = {knowledge acquisition, information extraction, human computing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487826,
author = {Pereira, Bianca and Aggarwal, Nitish and Buitelaar, Paul},
title = {AELA: An Adaptive Entity Linking Approach},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487826},
doi = {10.1145/2487788.2487826},
abstract = {The number of available Linked Data datasets has been increasing over time. Despite this, their use to recognise entities in unstructured plain text (Entity Linking task) is still limited to a small number of datasets. In this paper we propose a framework adaptable to the structure of generic Linked Data datasets. This adaptability allows a broader use of Linked Data datasets for the Entity Linking task.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {87–88},
numpages = {2},
keywords = {linked data, named entity, entity linking},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487828,
author = {Peters, Matthew E. and Lecocq, Dan},
title = {Content Extraction Using Diverse Feature Sets},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487828},
doi = {10.1145/2487788.2487828},
abstract = {The goal of content extraction or boilerplate detection is to separate the main content from navigation chrome, advertising blocks, copyright notices and the like in web pages. In this paper we explore a machine learning approach to content extraction that combines diverse feature sets and methods. Our main contributions are: a) preliminary results that show combining feature sets generally improves performance; and b) a method for including semantic information via id and class attributes applicable to HTML5. We also show that performance decreases on a new benchmark data set that better represents modern chrome.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {89–90},
numpages = {2},
keywords = {content extraction, template detection, boilerplate removal},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487829,
author = {Binh Tran, Giang and Alrifai, Mohammad and Quoc Nguyen, Dat},
title = {Predicting Relevant News Events for Timeline Summaries},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487829},
doi = {10.1145/2487788.2487829},
abstract = {This paper presents a framework for automatically constructing timeline summaries from collections of web news articles. We also evaluate our solution against manually created timelines and in comparison with related work.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {91–92},
numpages = {2},
keywords = {summarization, timeline, news event},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487830,
author = {Sachan, Mrinmaya and Srivastava, Shashank},
title = {Collective Matrix Factorization for Co-Clustering},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487830},
doi = {10.1145/2487788.2487830},
abstract = {We outline some matrix factorization approaches for co- clustering polyadic data (like publication data) using non-negative factorization (NMF). NMF approximates the data as a product of non-negative low-rank matrices, and can induce desirable clustering properties in the matrix factors through a flexible range of constraints. We show that simultaneous factorization of one or more matrices provides potent approaches for co-clustering.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {93–94},
numpages = {2},
keywords = {social networks, co-clustering, matrix factorization},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487831,
author = {Xu, Liheng and Liu, Kang and Lai, Siwei and Chen, Yubo and Zhao, Jun},
title = {Walk and Learn: A Two-Stage Approach for Opinion Words and Opinion Targets Co-Extraction},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487831},
doi = {10.1145/2487788.2487831},
abstract = {This paper proposes a novel two-stage method for opinion words and opinion targets co-extraction. In the first stage, a Sentiment Graph Walking algorithm is proposed, which naturally incorporates syntactic patterns in a graph to extract opinion word/target candidates. In the second stage, we adopt a self-Learning strategy to refine the results from the first stage, especially for filtering out noises with high frequency and capturing long-tail terms. Preliminary experimental evaluation shows that considering pattern confidence in the graph is beneficial and our approach achieves promising improvement over three competitive baselines.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {95–96},
numpages = {2},
keywords = {opinion words, sentiment analysis, opinion targets},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487832,
author = {Venkataramani, Rahul and Gupta, Atul and Asadullah, Allahbaksh and Muddu, Basavaraju and Bhat, Vasudev},
title = {Discovery of Technical Expertise from Open Source Code Repositories},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487832},
doi = {10.1145/2487788.2487832},
abstract = {Online Question and Answer websites for developers have emerged as the main forums for interaction during the software development process. The veracity of an answer in such websites is typically verified by the number of 'upvotes' that the answer garners from peer programmers using the same forum. Although this mechanism has proved to be extremely successful in rating the usefulness of the answers, it does not lend itself very elegantly to model the expertise of a user in a particular domain. In this paper, we propose a model to rank the expertise of the developers in a target domain by mining their activity in different opensource projects. To demonstrate the validity of the model, we built a recommendation system for StackOverflow which uses the data mined from GitHub.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {97–98},
numpages = {2},
keywords = {technical expertise, recommendations, source code repository, github, knowledge discovery, stackoverflow},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487833,
author = {Prabhakaran, Vinodkumar and John, Ajita and Seligmann, Dor\'{e}e D.},
title = {Power Dynamics in Spoken Interactions: A Case Study on 2012 Republican Primary Debates},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487833},
doi = {10.1145/2487788.2487833},
abstract = {In this paper, we explore how the power differential between participants of an interaction affects the way they interact in the context of political debates. We analyze the 2012 Republican presidential primary debates where we model the power index of each candidate in terms of their poll standings. We find that the candidates' power indices affected the way they interacted with others in the debates as well as how others interacted with them.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {99–100},
numpages = {2},
keywords = {relations, power, political science, debates, social status},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487834,
author = {Soo, Jason},
title = {A Non-Learning Approach to Spelling Correction in Web Queries},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487834},
doi = {10.1145/2487788.2487834},
abstract = {We describe an adverse environment spelling correction algorithm, known as Segments. Segments is language and domain independent and does not require any training data. We evaluate Segments' correction rate of transcription errors in web query logs with the state-of-the-art learning approach. We show that in environments where learning approaches are not applicable, such as multilingual documents, Segments has an F1-score within 0.005 of the learning approach.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {101–102},
numpages = {2},
keywords = {learning vs non-learning, web spelling correction},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487835,
author = {Zhang, Yu and Zhu, Weixiang},
title = {Extracting Implicit Features in Online Customer Reviews for Opinion Mining},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487835},
doi = {10.1145/2487788.2487835},
abstract = {As the number of customer reviews grows very rapidly, it is essential to summarize useful opinions for buyers, sellers and producers. One key step of opinion mining is feature extraction. Most existing research focus on finding explicit features, only a few attempts have been made to extract implicit features. Nearly all existing research only concentrate on product features, few has paid attention to other features that relates to sellers, services and logistics. Therefore in this paper, we propose a novel co-occurrence association-based method, which aims to extract implicit features in customer reviews and provide more comprehensive and fine-grained mining results.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {103–104},
numpages = {2},
keywords = {co-occurrence, association, opinion mining, implicit feature},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487836,
author = {Liu, Shenghua and Zhu, Wenjun and Xu, Ning and Li, Fangtao and Cheng, Xue-qi and Liu, Yue and Wang, Yuanzhuo},
title = {Co-Training and Visualizing Sentiment Evolvement for Tweet Events},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487836},
doi = {10.1145/2487788.2487836},
abstract = {Sentiment classification on tweet events attracts more interest in recent years. The large tweet stream stops people reading the whole classified list to understand the insights. We employ the co-training framework in the proposed algorithm. Features are split into text view features and non-text view features. Two Random Forest (RF) classifiers are trained with the common labeled data on the two views of features separately. Then for each specific event, they collaboratively and periodically train together to boost the classification performance. At last, we propose a "river" graph to visualize the intensity and evolvement of sentiment on an event, which demonstrates the intensity by both color gradient and opinion labels, and the ups and downs of confronting opinions by the river flow. Comparing with the well-known sentiment classifiers, our algorithm achieves consistent increases in accuracy on the tweet events from TREC 2011 Microblogging and our database. The visualization helps people recognize turning and bursting patterns, and predict sentiment trend in an intuitive way.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {105–106},
numpages = {2},
keywords = {sentiment analysis, microblog events, visualization, co-training},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487837,
author = {Chen, Kai and Zhou, Yi and Zha, Hongyuan and He, Jianhua and Shen, Pei and Yang, Xiaokang},
title = {Cost-Effective Node Monitoring for Online Hot Eventdetection in Sina Weibo Microblogging},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487837},
doi = {10.1145/2487788.2487837},
abstract = {We propose a cost-effective hot event detection system over Sina Weibo platform, currently the dominant microblogging service provider in China. The problem of finding a proper subset of microbloggers under resource constraints is formulated as a mixed-integer problem for which heuristic algorithms are developed to compute approximate solution. Preliminary results show that by tracking about 500 out of 1.6 million candidate microbloggers and processing 15,000 microposts daily, 62% of the hot events can be detected five hours on average earlier than they are published by Weibo.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {107–108},
numpages = {2},
keywords = {greedy algorithm, microblog, event detection, subnet},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487838,
author = {Sachan, Mrinmaya and Hovy, Dirk and Hovy, Eduard},
title = {Solving Electrical Networks to Incorporate Supervision in Random Walks},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487838},
doi = {10.1145/2487788.2487838},
abstract = {Random walks is one of the most popular ideas in computer science. A critical assumption in random walks is that the probability of the walk being at a given vertex at a time instance converges to a limit independent of the start state. While this makes it computationally efficient to solve, it limits their use to incorporate label information. In this paper, we exploit the connection between Random Walks and Electrical Networks to incorporate label information in classification, ranking, and seed expansion.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {109–110},
numpages = {2},
keywords = {random walks, classification, ranking, electrical networks},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487839,
author = {Liu, Peilei and Tang, Jintao and Wang, Ting},
title = {Information Current in Twitter: Which Brings Hot Events to the World},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487839},
doi = {10.1145/2487788.2487839},
abstract = {In this paper we investigate information propagation in Twitter from the geographical view on the global scale. An information propagation phenomenon what we call "information current" has been discovered. According to this phenomenon, we propose a hypothesis that changes of information flows may be related to real-time events. Through analysis of retweets, we show that our hypothesis is supported by experiment results. Moreover, it is discovered that the retweet texts are more effective than common tweet texts for real-time event detection. This means that Twitter could be a good filter of texts for event detection.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {111–112},
numpages = {2},
keywords = {geography, information propagation, twitter, event detection},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487841,
author = {Amirbekian, Rouben and Chen, Ye and Lu, Alan and Yan, Tak W. and Yin, Liangzhong},
title = {Traffic Quality Based Pricing in Paid Search Using Two-Stage Regression},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487841},
doi = {10.1145/2487788.2487841},
abstract = {While the cost-per-click (CPC) pricing model is main stream in sponsored search, the quality of clicks with respect to conversion rates and hence their values to advertisers may vary considerably from publisher to publisher in a large syndication network. Traffic quality shall be used to establish price discounts for clicks from different publishers. These discounts are intended to maintain incentives for high-quality online traffic and to make it easier for advertisers to maintain long-term bid stability. Conversion signal is noisy as each advertiser defines conversion in their own way. It is also very sparse. Traditional way of overcoming signal sparseness is to allow for longer time in accumulating modeling data. However, due to fast-changing conversion trends, such longer time leads to deterioration of the precision in measuring quality. To allow models to adjust to fast-changing trends with sufficient speed, we had to limit time-window for conversion data collection and make it much shorter than the several weeks window commonly used. Such shorter time makes conversions in the training set extremely sparse. To overcome resulting obstacles, we used two-stage regression similar to hurdle regression. First we employed logistic regression to predict zero conversion outcomes. Next, conditioned on non-zero outcomes, we used random forest regression to predict the value of the quotient of two conversion rates. Two-stage model accounts for the zero inflation due to the sparseness of the conversion signal. The combined model maintains good precision and allows faster reaction to the temporal changes in traffic quality including changes due to certain actions by publishers that may lead to click-price inflation.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {113–114},
numpages = {2},
keywords = {machine learning, online advertising, conversion},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487842,
author = {Barajas, Joel and Akella, Ram and Holtan, Marius and Kwon, Jaimie and Flores, Aaron and Andrei, Victor},
title = {Dynamic Evaluation of Online Display Advertising with Randomized Experiments: An Aggregated Approach},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487842},
doi = {10.1145/2487788.2487842},
abstract = {We perform a randomized experiment to estimate the effects of a display advertising campaign on online user conversions. We present a time series approach using Dynamic Linear Models to decompose the daily aggregated conversions into seasonal and trend components. We attribute the difference between control and study trends to the campaign. We test the method using two real campaigns run for 28 and 21 days respectively from the Advertising.com ad network.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {115–116},
numpages = {2},
keywords = {marketing, a/b testing, dlm, causal attribution},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487843,
author = {Trofimov, Ilya},
title = {New Features for Query Dependent Sponsored Search Click Prediction},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487843},
doi = {10.1145/2487788.2487843},
abstract = {Click prediction for sponsored search is an important problem for commercial search engines. Good click prediction algorithm greatly affects on the revenue of the search engine, user experience and brings more clicks to landing pages of advertisers. This paper presents new query-dependent features for the click prediction algorithm based on treating query and advertisement as bags of words. New features can improve prediction accuracy both for ads having many and few views.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {117–118},
numpages = {2},
keywords = {click prediction, sponsored search, web advertising},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487844,
author = {Zhang, Wei Vivian and Chen, Ye and Gupta, Mitali and Sett, Swaraj and Yan, Tak W.},
title = {Modeling Click and Relevance Relationship for Sponsored Search},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487844},
doi = {10.1145/2487788.2487844},
abstract = {Click-through rate (CTR) prediction and relevance ranking are two fundamental problems in web advertising. In this study, we address the problem of modeling the relationship between CTR and relevance for sponsored search. We used normalized relevance scores comparable across all queries to represent relevance when modeling with CTR, instead of directly using human judgment labels or relevance scores valid only within same query. We classified clicks by identifying their relevance quality using dwell time and session information, and compared all clicks versus selective clicks effects when modeling relevance.Our results showed that the cleaned click signal outperforms raw click signal and others we explored, in terms of relevance score fitting. The cleaned clicks include clicks with dwell time greater than 5 seconds and last clicks in session. Besides traditional thoughts that there is no linear relation between click and relevance, we showed that the cleaned click based CTR can be fitted well with the normalized relevance scores using a quadratic regression model. This relevance-click model could help to train ranking models using processed click feedback to complement expensive human editorial relevance labels, or better leverage relevance signals in CTR prediction.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {119–120},
numpages = {2},
keywords = {click-through rate, sponsored search, ad click, ad relevance},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487845,
author = {Chervonenkis, Alexey and Sorokina, Anna and Topinsky, Valery A.},
title = {Optimization of Ads Allocation in Sponsored Search},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487845},
doi = {10.1145/2487788.2487845},
abstract = {We introduce the optimization problem of target-specific ads allocation. Technique for solving this problem for different target-constraints structures is presented. This technique allows us to find optimal ads allocation which maximize the target such as CTR, Revenue or other system performances subject to some linear constraints. We show that the optimal ads allocation depends on both the target and constraints variables.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {121–122},
numpages = {2},
keywords = {sponsored search, optimization, ads allocation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487846,
author = {Pechyony, Dmitry and Jones, Rosie and Li, Xiaojing},
title = {A Joint Optimization of Incrementality and Revenue to Satisfy Both Advertiser and Publisher},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487846},
doi = {10.1145/2487788.2487846},
abstract = {A long-standing goal in advertising is to reduce wasted costs due to advertising to people who are unlikely to buy, as well as to those who would make a purchase whether they saw an ad or not. The ideal audience for the advertiser are those incremental users who would buy if shown an ad, and would not buy, if not shown the ad. On the other hand, for publishers who are paid when the user clicks or buys, revenue may be maximized by showing ads to those users who are most likely to click or purchase. We show analytically and empirically that an optimization towards one metric might result in an inferior performance in the other one. We present a novel algorithm, called SLC, that performs a joint optimization towards both advertisers' and publishers' goals and provides superior results in both.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {123–124},
numpages = {2},
keywords = {digital advertising, multi-objective optimization},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487847,
author = {Vandic, Damir and Nibbering, Didier and Frasincar, Flavius},
title = {A Case-Based Analysis of the Effect of Offline Media on Online Conversion Actions},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487847},
doi = {10.1145/2487788.2487847},
abstract = {In this paper, we investigate how offline advertising, by means of TV and radio, influences online search engine advertisement. Our research is based on the search engine-driven conversion actions of a 2012 marketing campaign of the potato chips manufacturer Lays. In our analysis we use several models, including linear regression (linear model) and Support Vector Regression (non-linear model). Our results confirm that offline commercials have a positive effect on the number of conversion actions from online marketing campaigns. This effect is especially visible in the first 50 minutes after the advertisement broadcasting.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {125–126},
numpages = {2},
keywords = {marketing campaign, offline media, online conversion},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487849,
author = {Zhang, Wei and Cao, Yunbo and Lin, Chin-Yew and Su, Jian and Tan, Chew-Lim},
title = {An Error Driven Approach to Query Segmentation},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487849},
doi = {10.1145/2487788.2487849},
abstract = {Query segmentation is the task of splitting a query into a sequence of non-overlapping segments that completely cover all tokens in the query. The majority of query segmentation methods are unsupervised. In this paper, we propose an error-driven approach to query segmentation (EDQS) with the help of search logs, which enables unsupervised training with guidance from the system-specific errors. In EDQS, we first detect the system's errors by examining the consistency among the segmentations of similar queries. Then, a model is trained by the detected errors to select the correct segmentation of a new query from the top-n outputs of the system. Our evaluation results show that EDQS can significantly boost the performance of state-of-the-art query segmentation methods on a publicly available data set.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {127–128},
numpages = {2},
keywords = {error driven, query segmentation, search log mining},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487850,
author = {Zhukovskiy, Maxim and Khropov, Andrei and Gusev, Gleb and Serdyukov, Pavel},
title = {Introducing Search Behavior into Browsing Based Models of Page's Importance},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487850},
doi = {10.1145/2487788.2487850},
abstract = {BrowseRank algorithm and its modifications are based on analyzing users' browsing trails. Our paper proposes a new method for computing page importance using a more realistic and effective search-aware model of user browsing behavior than the one used in BrowseRank.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {129–130},
numpages = {2},
keywords = {page authority, web search, queries, browserank},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487851,
author = {Muntean, Cristina Ioana and Nardini, Franco Maria and Silvestri, Fabrizio and Sydow, Marcin},
title = {Learning to Shorten Query Sessions},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487851},
doi = {10.1145/2487788.2487851},
abstract = {We propose the use of learning to rank techniques to shorten query sessions by maximizing the probability that the query we predict is the "final" query of the current search session. We present a preliminary evaluation showing that this approach is a promising research direction.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {131–132},
numpages = {2},
keywords = {query prediction, learning to rank, gbrt},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487852,
author = {Umbrich, J\"{u}rgen and Gutierrez, Claudio and Hogan, Aidan and Karnstedt, Marcel and Xavier Parreira, Josiane},
title = {The ACE Theorem for Querying the Web of Data},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487852},
doi = {10.1145/2487788.2487852},
abstract = {Inspired by the CAP theorem, we identify three desirable properties when querying the Web of Data: Alignment (results up-to-date with sources), Coverage (results covering available remote sources), and Efficiency (bounded resources). In this short paper, we show that no system querying the Web can meet all three ACE properties, but instead must make practical trade-offs that we outline.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {133–134},
numpages = {2},
keywords = {ace properties, web of data, query processing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487853,
author = {Blanco, Roi and De Francisci Morales, Gianmarco and Silvestri, Fabrizio},
title = {Towards Leveraging Closed Captions for News Retrieval},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487853},
doi = {10.1145/2487788.2487853},
abstract = {IntoNow from Yahoo! is a second screen application that enhances the way of watching TV programs. The application uses audio from the TV set to recognize the program being watched, and provides several services for different use cases. For instance, while watching a football game on TV it can show statistics about the teams playing, or show the title of the song performed by a contestant in a talent show. The additional content provided by IntoNow is a mix of editorially curated and automatically selected one. From a research perspective, one of the most interesting and challenging use cases addressed by IntoNow is related to news programs (newscasts). When a user is watching a newscast, IntoNow detects it and starts showing online news articles from the Web. This work presents a preliminary study of this problem, i.e., to find an online news article that matches the piece of news discussed in the newscast currently airing on TV, and display it in real-time.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {135–136},
numpages = {2},
keywords = {continuous retrieval, news retrieval, intonow},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487854,
author = {Wu, Wensheng and Zhong, Tingting},
title = {Searching the Deep Web Using Proactive Phrase Queries},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487854},
doi = {10.1145/2487788.2487854},
abstract = {This paper proposes ipq, a novel search engine that proactively transforms query forms of Deep Web sources into phrase queries, constructs query evaluation plans, and caches results for popular queries offline. Then at query time, keyword queries are simply matched with phrase queries to retrieve results. ipq embodies a novel dual-ranking framework for query answering and novel solutions for discovering frequent attributes and queries. Preliminary experiments show the great potentials of ipq.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {137–138},
numpages = {2},
keywords = {proactive search engine, natural language queries, deep web},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487855,
author = {Yates, Andrew and Goharian, Nazli and Frieder, Ophir},
title = {Graded Relevance Ranking for Synonym Discovery},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487855},
doi = {10.1145/2487788.2487855},
abstract = {Interest in domain-specific search is steadfastly increasing, yielding a growing need for domain-specific synonym discovery. Existing synonym discovery methods perform poorly when faced with the realistic task of identifying a target term's synonyms from among many candidates. We approach domain-specific synonym discovery as a graded relevance ranking problem in which a target term's synonym candidates are ranked by their quality. In this scenario a human editor uses each ranked list of synonym candidates to build a domain-specific thesaurus. We evaluate our method for graded relevance ranking of synonym candidates and find that it outperforms existing methods.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {139–140},
numpages = {2},
keywords = {domain-specific search, thesaurus construction, domain-specific thesaurus construction, synonym discovery},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487856,
author = {Kuribayashi, Taku and Asano, Yasuhito and Yoshikawa, Masatoshi},
title = {Ranking Method Specialized for Content Descriptions of Classical Music},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487856},
doi = {10.1145/2487788.2487856},
abstract = {In this paper, we propose novel ranking methods of effectively finding content descriptions of classical music compositions. In addition to rather naive methods using technical term frequency and latent Dirichlet allocation(LDA), we proposed a novel classification of web pages about classical music and used the characteristics of the classification for our method of search by labeled LDA(L-LDA). The experimental results showed our method performed well at finding content descriptions of classical music compositions.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {141–142},
numpages = {2},
keywords = {labeled lda, classical music, vertical search},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487857,
author = {Ahlers, Dirk},
title = {Towards a Development Process for Geospatial Information Retrieval and Search},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487857},
doi = {10.1145/2487788.2487857},
abstract = {Geospatial search as a special type of vertical search has specific requirements and challenges. While the general principle of resource discovery, extraction, indexing, and search holds, geospatial search systems are tailored to the specific use case at hand with many individual adaptations. In this short overview, we aim to collect and organize the main organizing principles for the multitude of challenges and adaptations to be considered within the development process to work towards a more formal description.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {143–144},
numpages = {2},
keywords = {information and knowledge management, developing countries, search engines, geospatial web search},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487858,
author = {Mejova, Yelena and Bordino, Ilaria and Lalmas, Mounia and Gionis, Aristides},
title = {Searching for Interestingness in Wikipedia and Yahoo! Answers},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487858},
doi = {10.1145/2487788.2487858},
abstract = {In many cases, when browsing the Web, users are searching for specific information. Sometimes, though, users are also looking for something interesting, surprising, or entertaining. Serendipitous search puts interestingness on par with relevance. We investigate how interesting are the results one can obtain via serendipitous search, and what makes them so, by comparing entity networks extracted from two prominent social media sites, Wikipedia and Yahoo! Answers.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {145–146},
numpages = {2},
keywords = {serendipity, exploratory search},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487859,
author = {Lee, Seung Eun and Kim, Dongug},
title = {A Click Model for Time-Sensitive Queries},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487859},
doi = {10.1145/2487788.2487859},
abstract = {User behavior on search results pages provides a clue about the query intent and the relevance of documents. To incorporate this information into search rankings, a variety of click modeling techniques have been proposed so far and now they are widely used in commercial search engines. For time-sensitive queries, however, applying click models can degrade the search relevance because the best document in the past may not be the current best answer. To address this problem, it is required to detect a time point, a turning point, where the search intent for a given query changes and to reflect it in click models. In this work, we devised a method to detect the turning point of a query from its search volume history. The proposed click model is designed to take only user behavior observed after the turning points. We applied our model in a commercial search engine and evaluated its relevance.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {147–148},
numpages = {2},
keywords = {click modeling, time-sensitive queries},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487860,
author = {Mukherjee, Subhabrata and Verma, Ashish and Church, Kenneth W.},
title = {Intent Classification of Voice Queries on Mobile Devices},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487860},
doi = {10.1145/2487788.2487860},
abstract = {Mobile query classification faces the usual challenges of encountering short and noisy queries as in web search. However, the task of mobile query classification is made difficult by the presence of more inter-active and personalized queries like map, command and control, dialogue, joke etc. Voice queries are made more difficult than typed queries due to the errors introduced by the automatic speech recognizer. This is the first paper, to the best of our knowledge, to bring the complexities of voice search and intent classification together. In this paper, we propose some novel features for intent classification, like the url's of the search engine results for the given query. We also show the effectiveness of other features derived from the part-of-speech information of the query and search engine results, in proposing a multi-stage classifier for intent classification. We evaluate the classifier using tagged data, collected from a voice search android application, where we achieve an average of 22% f-score improvement per category, over the commonly used bag-of-words baseline.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {149–150},
numpages = {2},
keywords = {mobile search, intent classification},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487861,
author = {Kotov, Alexander and Wang, Yu and Agichtein, Eugene},
title = {Leveraging Geographical Metadata to Improve Search over Social Media},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487861},
doi = {10.1145/2487788.2487861},
abstract = {We propose the methods for document, query and relevance model expansion that leverage geographical metadata provided by social media. In particular, we propose a geographically-aware extension of the LDA topic model and utilize the resulting topics and language models in our expansion methods. The proposed approach has been experimentally evaluated over a large sample of Twitter, demonstrating significant improvements in search accuracy over traditional (geographically-unaware) retrieval models.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {151–152},
numpages = {2},
keywords = {probabilistic retrieval models, topic models, microblog retrieval, social media, language models},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487862,
author = {Saha Roy, Rishiraj and Suresh, Anusha and Ganguly, Niloy and Choudhury, Monojit},
title = {Place Value: Word Position Shifts Vital to Search Dynamics},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487862},
doi = {10.1145/2487788.2487862},
abstract = {With fast changing information needs in today's world, it is imperative that search engines precisely understand and exploit temporal changes in Web queries. In this work, we look at shifts in preferred positions of segments in queries over an interval of four years. We find that such shifts can predict key changes in usage patterns, and explain the observed increase in query lengths. Our findings indicate that recording positional statistics can be vital for understanding user intent in Web search queries.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {153–154},
numpages = {2},
keywords = {query understanding, position shifts, query log analysis},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487864,
author = {Morales, Alex and Sun, Huan and Yan, Xifeng},
title = {Synthetic Review Spamming and Defense},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487864},
doi = {10.1145/2487788.2487864},
abstract = {Online reviews are widely adopted in many websites such as Amazon, Yelp, and TripAdvisor. Positive reviews can bring significant financial gains, while negative ones often cause sales loss. This fact, unfortunately, results in strong incentives for opinion spam to mislead readers. Instead of hiring humans to write deceptive reviews, in this work, we bring into attention an automated, low-cost process for generating fake reviews, variations of which could be easily employed by evil attackers in reality. To the best of our knowledge, we are the first to expose the potential risk of machine-generated deceptive reviews. Our simple review synthesis model uses one truthful review as a template, and replaces its sentences with those from other reviews in a repository. The fake reviews generated by this mechanism are extremely hard to detect: Both the state-of-the-art machine detectors and human readers have an error rate of 35%-48%. A novel defense method that leverages the difference of semantic flows between fake and truthful reviews is developed, reducing the detection error rate to approximately 22%. Nevertheless, it is still a challenging research task to further decrease the error rate.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {155–156},
numpages = {2},
keywords = {review spam, classification, spam detection},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487865,
author = {Rachapalli, Jyothsna and Khadilkar, Vaibhav and Kantarcioglu, Murat and Thuraisingham, Bhavani},
title = {REDACT: A Framework for Sanitizing RDF Data},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487865},
doi = {10.1145/2487788.2487865},
abstract = {Resource Description Framework (RDF) is the foundational data model of the Semantic Web, and is essentially designed for integration of heterogeneous data from varying sources. However, lack of security features for managing sensitive RDF data while sharing may result in privacy breaches, which in turn, result in loss of user trust. Therefore, it is imperative to provide an infrastructure to secure RDF data. We present a set of graph sanitization operations that are built as an extension to SPARQL. These operations allow one to sanitize sensitive parts of an RDF graph and further enable one to build more sophisticated security and privacy features, thus allowing RDF data to be shared securely.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {157–158},
numpages = {2},
keywords = {sparql, access control, rdf, provenance, sanitization},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487866,
author = {Thomas, Achint and Punera, Kunal and Kennedy, Lyndon and Tseng, Belle and Chang, Yi},
title = {Framework for Evaluation of Text Captchas},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487866},
doi = {10.1145/2487788.2487866},
abstract = {Interactive websites use text-based Captchas to prevent unauthorized automated interactions. These Captchas must be easy for humans to decipher while being difficult to crack by automated means. In this work we present a framework for the systematic study of Captchas along these two competing objectives. We begin by abstracting a set of distortions that characterize current and past commercial text-based Captchas. By means of user studies, we quantify the way human Captcha solving performance varies with changes in these distortion parameters. To quantify the effect of these distortions on the accuracy of automated solvers (bots), we propose a learning-based algorithm that performs automated Captcha segmentation driven by character recognition. Results show that our proposed algorithm is generic enough to solve text-based Captchas with widely varying distortions without requiring the use of hand-coded image processing or heuristic rules.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {159–160},
numpages = {2},
keywords = {bots, human interactive proofs, captcha},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487867,
author = {Oh, Hyun-Kyo and Kim, Jin-Woo and Kim, Sang-Wook and Lee, Kichun},
title = {A Probability-Based Trust Prediction Model Using Trust-Message Passing},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487867},
doi = {10.1145/2487788.2487867},
abstract = {We propose a probability-based trust prediction model based on trust-message passing which takes advantage of the two kinds of information: an explicit information and an implicit information.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {161–162},
numpages = {2},
keywords = {trust propagation, trust prediction model, message passing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487868,
author = {Shen, Zeqian and Sundaresan, Neel},
title = {RepRank: Reputation in a Peer-to-Peer Online System},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487868},
doi = {10.1145/2487788.2487868},
abstract = {Peer-to-peer e-commerce networks exemplify online lemon markets. Trust is key to sustaining these networks. We present a reputation system named RepRank that approaches trust with an intuition that in the peer-to-peer e-commerce world consisting of buyers and sellers, good buyers are those who buy from good sellers, and good sellers are those from whom good buyers buy. We propagate trust and distrust in a network using this mutually recursive definition. We discuss the algorithms and present the evaluation results.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {163–164},
numpages = {2},
keywords = {reputation, e-commerce},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487869,
author = {Gyrard, Amelie and Bonnet, Christian and Boudaoud, Karima},
title = {The STAC (Security Toolbox: Attacks &amp; Countermeasures) Ontology},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487869},
doi = {10.1145/2487788.2487869},
abstract = {We present a security ontology to help non-security expert software designers or developers to: (1) design secure software and, (2) to understand and be aware of main security concepts and issues. Our security ontology defines the main security concepts such as attacks, countermeasures, security properties and their relationships. Countermeasures can be cryptographic concepts (encryption algorithm, key management, digital signature, hash function), security tools or security protocols. The purpose of this ontology is to be reused in numerous domains such as security of web applications, network management or communication networks (sensor, cellular and wireless). The ontology and a user interface (to use the ontology) are available online.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {165–166},
numpages = {2},
keywords = {ontology, security protocols, wireless communications, countermeasures, security, taxonomy, attacks, osi model, semantic web},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487871,
author = {De Nies, Tom and Coppens, Sam and Mannens, Erik and Van de Walle, Rik},
title = {Modeling Uncertain Provenance and Provenance of Uncertainty in W3C PROV},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487871},
doi = {10.1145/2487788.2487871},
abstract = {This paper describes how to model uncertain provenance and provenance of uncertain things in a flexible and unintrusive manner using PROV, W3C's new standard for provenance. Three new attributes with clearly defined values and semantics are proposed. Modeling this information is an important step towards the modeling and derivation of trust from resources whose provenance is described using PROV.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {167–168},
numpages = {2},
keywords = {standardization, w3c, trust, provenance, uncertainty},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487872,
author = {Ravindra, Padmashree and Anyanwu, Kemafor},
title = {Scalable Processing of Flexible Graph Pattern Queries on the Cloud},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487872},
doi = {10.1145/2487788.2487872},
abstract = {Flexible exploration of large RDF datasets with unknown relationships can be enabled using 'unbound-property' graph pattern queries. Relational-style processing of such queries using normalized relations results in redundant information in intermediate results due to the repetition of adjoining bound (fixed) properties. Such redundancy negatively impacts the disk I/O, network transfer costs, and the required disk space while processing RDF query workloads on MapReduce-based systems. This work proposes packing and lazy unpacking strategies to minimize the redundancy in intermediate results while processing unbound-property queries. In addition to keeping the results compact, this work evaluates RDF queries using the Nested TripleGroup Data Model and Algebra (NTGA) that enables shorter MapReduce execution workflows. Experimental results demonstrate the benefit of this work over RDF query processing using relational-style systems such as Apache Pig and Hive.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {169–170},
numpages = {2},
keywords = {unbound-property, rdf graph pattern matching, mapreduce},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487873,
author = {Singer, Philipp and Niebler, Thomas and Strohmaier, Markus and Hotho, Andreas},
title = {Computing Semantic Relatedness from Human Navigational Paths on Wikipedia},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487873},
doi = {10.1145/2487788.2487873},
abstract = {This paper presents a novel approach for computing semantic relatedness between concepts on Wikipedia by using human navigational paths for this task. Our results suggest that human navigational paths provide a viable source for calculating semantic relatedness between concepts on Wikipedia. We also show that we can improve accuracy by intelligent selection of path corpora based on path characteristics indicating that not all paths are equally useful. Our work makes an argument for expanding the existing arsenal of data sources for calculating semantic relatedness and to consider the utility of human navigational paths for this task.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {171–172},
numpages = {2},
keywords = {navigation, semantic relatedness, wikipedia},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487874,
author = {Zhang, Xiaochen and Jin, Xiaoming and Li, Lianghao and Shen, Dou},
title = {Discovering Multilingual Concepts from Unaligned Web Documents by Exploring Associated Images},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487874},
doi = {10.1145/2487788.2487874},
abstract = {The Internet is experiencing an explosion of information presented in different languages. Though written in different languages, some articles implicitly share common concepts. In this paper, we propose a novel framework to mine cross-language common concepts from unaligned web documents. Specifically, visual words of images are used to bridge articles in different languages and then common concepts of multiple languages are learned by using an existing topic modeling algorithm. We conduct cross-lingual text classification in a real-world data set using the mined multilingual concepts from our method. The experiment results show that our approach is effective to mine cross-lingual common concepts.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {173–174},
numpages = {2},
keywords = {common concepts, multilingual, image},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487875,
author = {Lee, Sanghoon and Lee, Jongwuk and Hwang, Seung-won},
title = {Fria: Fast and Robust Instance Alignment},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487875},
doi = {10.1145/2487788.2487875},
abstract = {This paper proposes Fria, a fast and robust instance alignment framework across two independently built knowledge bases (KBs). Our objective is two-fold: (1) to design an effective instance similarity measure and (2) to build a fast and robust alignment framework. Specifically, Fria consists of two-phases. Fria first achieves high-precision alignment for seed matches which have strong evidence for aligning. To obtain high-recall alignment, Fria then divides non-matched instances according to the types identified from seeds, and gives additional chances to the same-typed instances to be matched. Experimental results show that Fria is fast and robust, by achieving comparable accuracy to state-of-the-arts and a 10-times speed up.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {175–176},
numpages = {2},
keywords = {knowledge base, entity matching, hierarchical partitioning, instance alignment},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487877,
author = {Bao, Peng and Shen, Hua-Wei and Huang, Junming and Cheng, Xue-Qi},
title = {Popularity Prediction in Microblogging Network: A Case Study on Sina Weibo},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487877},
doi = {10.1145/2487788.2487877},
abstract = {Predicting the popularity of content is important for both the host and users of social media sites. The challenge of this problem comes from the inequality of the popularity of content. Existing methods for popularity prediction are mainly based on the quality of content, the interface of social media site to highlight contents, and the collective behavior of users. However, little attention is paid to the structural characteristics of the networks spanned by early adopters, i.e., the users who view or forward the content in the early stage of content dissemination. In this paper, taking the Sina Weibo as a case, we empirically study whether structural characteristics can provide clues for the popularity of short messages. We find that the popularity of content is well reflected by the structural diversity of the early adopters. Experimental results demonstrate that the prediction accuracy is significantly improved by incorporating the factor of structural diversity into existing methods.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {177–178},
numpages = {2},
keywords = {microblogging, popularity prediction, social network, structural diversity, information diffusion},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487878,
author = {Bressan, Marco and Peserico, Enoch and Pretto, Luca},
title = {The Power of Local Information in PageRank},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487878},
doi = {10.1145/2487788.2487878},
abstract = {Can one assess, by visiting only a small portion of a graph, if a given node has a significantly higher PageRank score than another? We show that the answer strongly depends on the interplay between the required correctness guarantees (is one willing to accept a small probability of error?) and the graph exploration model (can one only visit parents and children of already visited nodes?).},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {179–180},
numpages = {2},
keywords = {local computation, pagerank, graph ranking},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487879,
author = {Yang, Cheng-Lun and Kung, Perng-Hwa and Chen, Chun-An and Lin, Shou-De},
title = {Semantically Sampling in Heterogeneous Social Networks},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487879},
doi = {10.1145/2487788.2487879},
abstract = {Online social networks sampling identifies a representative subnetwork that preserves certain graph property given het- erogeneous semantics, with the full network not observed during sampling. This study presents a property, Relational Profile, to account for conditional dependency of node and relation type semantics in a network, and a sampling method to preserve the property. We show the proposed sampling method better preserves Relational Profile. Next, Relational Profile can design features to boost network prediction. Fi- nally, our sampled network trains more accurate prediction models than other sampling baselines.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {181–182},
numpages = {2},
keywords = {network prediction, graph sampling, heterogeneous network},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487880,
author = {Park, Hosung and Moon, Sue},
title = {Sampling Bias in User Attribute Estimation of OSNs},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487880},
doi = {10.1145/2487788.2487880},
abstract = {Recent work on unbiased sampling of OSNs has focused on estimation of the network characteristics such as degree distributions and clustering coefficients. In this work we shift the focus to node attributes. We show that existing sampling methods produce biased outputs and need modifications to alleviate the bias.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {183–184},
numpages = {2},
keywords = {social networks, user attributes, sampling methods},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487881,
author = {Li, Dong and Xu, Zhiming and Li, Sheng and Sun, Xin and Gupta, Anika and Sycara, Katia},
title = {Link Recommendation for Promoting Information Diffusion in Social Networks},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487881},
doi = {10.1145/2487788.2487881},
abstract = {Online social networks mainly have two functions: social interaction and information diffusion. Most of current link recommendation researches only focus on strengthening the social interaction function, but ignore the problem of how to enhance the information diffusion function. For solving this problem, this paper introduces the concept of user diffusion degree and proposes the algorithm for calculating it, then combines it with traditional recommendation methods for reranking recommended links. Experimental results on Email dataset and Amazon dataset under Independent Cascade Model and Linear Threshold Model show that our method noticeably outperforms the traditional methods in terms of promoting information diffusion.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {185–186},
numpages = {2},
keywords = {link recommendation, information diffusion, diffusion degree},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487882,
author = {Miao, Qingliang and Zhang, Shu and Meng, Yao and Yu, Hao},
title = {Domain-Sensitive Opinion Leader Mining from Online Review Communities},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487882},
doi = {10.1145/2487788.2487882},
abstract = {In this paper, we investigate how to identify domain-sensitive opinion leaders in online review communities, and present a model to rank domain-sensitive opinion leaders. To evaluate the effectiveness of the proposed model, we conduct preliminary experiments on a real-world dataset from Amazon.com. Experimental results indicate that the proposed model is effective in identifying domain-sensitive opinion leaders.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {187–188},
numpages = {2},
keywords = {opinion leader, social networks, ranking},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487883,
author = {Contractor, Danish and Faruquie, Tanveer Afzal},
title = {Understanding Election Candidate Approval Ratings Using Social Media Data},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487883},
doi = {10.1145/2487788.2487883},
abstract = {The last few years has seen an exponential increase in the amount of social media data generated daily. Thus, researchers have started exploring the use of social media data in building recommendation systems, prediction models, improving disaster management, discovery trending topics etc. An interesting application of social media is for the prediction of election results. The recently conducted 2012 US Presidential election was the "most tweeted" election in history and provides a rich source of social media posts. Previous work on predicting election outcomes from social media has been largely been based on sentiment about candidates, total volumes of tweets expressing electoral polarity and the like. In this paper we use a collection of tweets to predict the daily approval ratings of the two US presidential candidates and also identify topics that were causal to the approval ratings.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {189–190},
numpages = {2},
keywords = {social network, regression, election prediction, social media, granger causality},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487884,
author = {Liu, Xin and Murata, Tsuyoshi and Wakita, Ken},
title = {Extracting the Multilevel Communities Based on Network Structural and Nonstructural Information},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487884},
doi = {10.1145/2487788.2487884},
abstract = {Many real-world networks contain nonstructural information on nodes, such as the spatial coordinate of a location, profile of a person, or contents of a web page. In this paper, we propose Dist-Modularity, a unified modularity measure, which is useful in extracting the multilevel communities based on network structural and nonstructural information.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {191–192},
numpages = {2},
keywords = {modularity, community structure, social network},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487885,
author = {Yantao, Jia and Yuanzhuo, Wang and Jingyuan, Li and Kai, Feng and Xueqi, Cheng and Jianchen, Li},
title = {Structural-Interaction Link Prediction in Microblogs},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487885},
doi = {10.1145/2487788.2487885},
abstract = {Link prediction in Microblogs by using unsupervised methods aims to find an appropriate similarity measure between users in the network. However, the measures used by existing work lack a simple way to incorporate the structure of the network and the interactions between users. In this work, we define the retweet similarity to measure the interactions between users in Twitter, and propose a structural-interaction based matrix factorization model for following-link prediction. Experiments on the real world Twitter data show our model outperforms state-of-the-art methods.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {193–194},
numpages = {2},
keywords = {microblogs, link prediction, structure-interaction},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487886,
author = {Lee, Jay Yoon and Kang, U. and Koutra, Danai and Faloutsos, Christos},
title = {Fast Anomaly Detection despite the Duplicates},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487886},
doi = {10.1145/2487788.2487886},
abstract = {Given a large cloud of multi-dimensional points, and an off-the shelf outlier detection method, why does it take a week to finish? After careful analysis, we discovered that duplicate points create subtle issues, that the literature has ignored: if dmax is the multiplicity of the most over-plotted point, typical algorithms are quadratic on dmax. We propose several ways to eliminate the problem; we report wall-clock times and our time savings; and we show that our methods give either exact results, or highly accurate approximate ones.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {195–196},
numpages = {2},
keywords = {anomaly detection, large-scale graph, duplicate point problem},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487887,
author = {Soh, Ping-Han and Lin, Yu-Chieh and Chen, Ming-Syan},
title = {Recommendation for Online Social Feeds by Exploiting User Response Behavior},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487887},
doi = {10.1145/2487788.2487887},
abstract = {In recent years, online social networks have been dramatically expanded. Active users spend hours communicating with each other via these networks such that an enormous amount of data is created every second. The tremendous amount of newly created information costs users much time to discover interesting messages from their online social feeds. The problem is even exacerbated if users access these networks via mobile devices. To assist users in discovering interesting messages efficiently, in this paper, we propose a new approach to recommend interesting messages for each user by exploiting the user's response behavior. We extract data from the most popular social network, and the experimental results show that the proposed approach is effective and efficient.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {197–198},
numpages = {2},
keywords = {social feeds, social networks, recommender systems},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487889,
author = {de la Rouviere, Simon and Ehlers, Kobus},
title = {Lists as Coping Strategy for Information Overload on Twitter},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487889},
doi = {10.1145/2487788.2487889},
abstract = {When following too many users on microblogging services, information overload occurs due to increased and varied communication activity. Users then either leave, or employ coping strategies to continue benefiting from the service. Through a crawl of 31 684 random users from Twitter and a qualitative survey with 115 respondents, it has been determined that by using lists as an information management coping strategy (filtering and compartmentalising varied communication activity), users are capable of following more users and experience fewer symptoms of information overload.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {199–200},
numpages = {2},
keywords = {coping strategies, microblogging services, information overload},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487890,
author = {Steiner, Thomas and Chedeau, Christopher},
title = {To Crop, or Not to Crop: Compiling Online Media Galleries},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487890},
doi = {10.1145/2487788.2487890},
abstract = {We have developed an application for the automatic generation of media galleries that visually and audibly summarize events based on media items like videos and photos from multiple social networks. Further, we have evaluated different media gallery styles with online surveys and examined their pros and cons. Besides the survey results, our contribution is also the application itself, where media galleries of different styles can be created on-the-fly. A demo is available at http://social-media-illustrator.herokuapp.com/.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {201–202},
numpages = {2},
keywords = {media galleries, event summarization, social networks},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487891,
author = {Spirin, Nikita and Karahalios, Karrie},
title = {Unsupervised Approach to Generate Informative Structured Snippets for Job Search Engines},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487891},
doi = {10.1145/2487788.2487891},
abstract = {Aiming to improve user experience for a job search engine, in this paper we propose an idea to switch from query-biased snippets used by most web search engines to rich structured snippets associated with the main sections of a job posting page, which are more appropriate for job search due to specific user needs and the structure of job pages. We present a very simple yet actionable approach to generate such snippets in an unsupervised way. The advantages of the proposed approach are two-fold: it doesn't require manual annotation and therefore can be easily deployed to many languages, which is a desirable property for a job search engine operating internationally; it fuses naturally with the trend towards Mobile Web where the content needs to be optimized for small screen devices and informativeness.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {203–204},
numpages = {2},
keywords = {information extraction, summarization, search snippet},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487892,
author = {Guo, Lei and Ma, Jun and Chen, Zhumin},
title = {Learning to Recommend with Multi-Faceted Trust in Social Networks},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487892},
doi = {10.1145/2487788.2487892},
abstract = {Traditionally, trust-aware recommendation methods that utilize trust relations for recommender systems assume a single type of trust between users. However, this assumption ignores the fact that trust as a social concept inherently has many aspects. A user may place trust differently to different people. Motivated by this observation, we propose a novel probabilistic factor analysis method, which learns the multi-faceted trust relations and user profiles through a shared user latent feature space. Experimental results on the real product rating data set show that our approach outperforms state-of-the-art methods on the RMSE measure.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {205–206},
numpages = {2},
keywords = {social recommendation, probabilistic matrix factorization, multi-faceted trust},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487893,
author = {Lee, Jongin and Kim, John and Lee, KwanHong},
title = {Hidden View Game: Designing Human Computation Games to Update Maps and Street Views},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487893},
doi = {10.1145/2487788.2487893},
abstract = {Although the Web has abundant information, it does not necessarily contain the latest, most recently updated information. In particular, interactive map websites and the accompanying street view applications often contain information that is a few years old and are somewhat outdated because street views can change quickly. In this work, we propose Hidden View - a human computation mobile game that enables the updating of maps and street views with the latest information. The preliminary implementation of the game is described and some results collected from a sample user study are presented. This work is the first step towards leveraging human computation and an individual's familiarity with different points-of-interest to keep maps and street views up to date.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {207–208},
numpages = {2},
keywords = {human computation, game with a purpose, crowdsourcing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487894,
author = {Triglianos, Vasileios and Pautasso, Cesare},
title = {ASQ: Interactive Web Presentations for Hybrid MOOCs},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487894},
doi = {10.1145/2487788.2487894},
abstract = {ASQ is a Web application for creating and delivering interactive HTML5 presentations. It is designed to support teachers that need to gather real-time feedback from the students while delivering their lectures. Presentation slides are delivered to viewers that can answer the questions embedded in the slides. The objective is to maximize the efficiency of bi-directional communication between the lecturer and a large audience. More specifically, in the context of a hybrid MOOC classroom, a teacher can use ASQ to get feedback in real time about the level of comprehension of the presented material while reducing the time for gathering survey data, monitoring attendance and assessing solutions.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {209–210},
numpages = {2},
keywords = {software clicker, hybrid teaching, html5, impress.js},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487896,
author = {Xu, Yingzhong and Hu, Songlin},
title = {QMapper: A Tool for SQL Optimization on Hive Using Query Rewriting},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487896},
doi = {10.1145/2487788.2487896},
abstract = {Although HiveQL offers similar features with SQL, it is still difficult to map complex SQL queries into HiveQL and manual translation often leads to poor performance. A tool named QMapper is developed to address this problem by utilizing query rewriting rules and cost-based MapReduce flow evaluation on the basis of column statistics. Evaluation demonstrates that while assuring the correctness, QMapper improves the performance up to 42% in terms of execution time.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {211–212},
numpages = {2},
keywords = {hive, sql, query rewriting, mapreduce},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487897,
author = {Schroeder, Rebeca and Penteado, Raqueline and Hara, Carmem Satie},
title = {Partitioning RDF Exploiting Workload Information},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487897},
doi = {10.1145/2487788.2487897},
abstract = {One approach to leverage scalable systems for RDF management is partitioning large datasets across distributed servers. In this paper we consider workload data, given in the form of query patterns and their frequencies, for determining how to partition RDF datasets. Our experimental study shows that our workload-aware method is an effective way to cluster related data and provides better query response times compared to an elementary fragmentation method.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {213–214},
numpages = {2},
keywords = {rdf, data fragmentation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487898,
author = {Yao, Lina and Sheng, Quan Z.},
title = {Correlation Discovery in Web of Things},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487898},
doi = {10.1145/2487788.2487898},
abstract = {With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web services, Web of Things (WoT) is gaining a considerable momentum as an emerging paradigm where billions of physical objects will be interconnected and present over the World Wide Web. One inevitable challenge in the new era of WoT lies in how to efficiently and effectively manage things, which is critical for a number of important applications such as object search, recommendation, and composition. In this paper, we propose a novel approach to discover the correlations of things by constructing a relational network of things (RNT) where similar things are linked via virtual edges according to their latent correlations by mining three dimensional information in the things usage events in terms of user, temporality and spatiality. With RNT, many problems centered around things management such as objects classification, discovery and recommendation can be solved by exploiting graph-based algorithms. We conducted experiments using real-world data collected over a period of four months to verify and evaluate our model and the results demonstrate the feasibility of our approach.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {215–216},
numpages = {2},
keywords = {correlation discovery, web of things, random walk with restart},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487899,
author = {Pautasso, Cesare and Babazadeh, Masiar},
title = {The Atomic Web Browser},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487899},
doi = {10.1145/2487788.2487899},
abstract = {The Atomic Web Browser achieves atomicity for distributed transactions across multiple RESTful APIs. Assuming that the participant APIs feature support for the Try-Confirm/Cancel pattern, the user may navigate with the Atomic Web Browser among multiple Web sites to perform local resource state transitions (e.g., reservations or bookings). Once the user indicates that the navigation has successfully completed, the Atomic Web browser takes care of confirming the local transitions to achieve the atomicity of the global transaction.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {217–218},
numpages = {2},
keywords = {distributed systems, atomic transactions, rest},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487900,
author = {Geneves, Pierre and Layaida, Nabil},
title = {XML Validation: Looking Backward - Strongly Typed and Flexible XML Processing Are Not Incompatible},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487900},
doi = {10.1145/2487788.2487900},
abstract = {One major concept in web development using XML is validation: checking whether some document instance fulfills structural constraints described by some schema. Over the last few years, there has been a growing debate about XML validation, and two main schools of thought emerged about the way it should be done. On the one hand, some advocate the use of validation with respect to complete grammar-based descriptions such as DTDs and XML Schemas. On the other hand, motivated by a need for greater flexibility, others argue for no validation at all, or prefer the use of lightweight constraint languages such as Schematron with the aim of validating only required constraints, while making schema descriptions more compositional and more reusable.Owing to a logical compilation, we show that validators used in each of these approaches share the same theoretical foundations, meaning that the two approaches are far from being incompatible. Our findings include that the logic in [2] can be seen as a unifying formal ground for the construction of robust and efficient validators and static analyzers using any of these schema description techniques. This reconciles the two approaches from both a theoretical and a practical perspective, therefore facilitating any combination of them.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {219–220},
numpages = {2},
keywords = {schemas, validation, xml, schematron, foundations},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487901,
author = {Dubey, Ayush and De, Pradipta and Dey, Kuntal and Mittal, Sumit and Agarwal, Vikas and Chetlur, Malolan and Mukherjea, Sougata},
title = {Co-Operative Content Adaptation Framework: Satisfying Consumer and Content Creator in Resource Constrained Browsing},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487901},
doi = {10.1145/2487788.2487901},
abstract = {Mobile Web is characterized by two salient features, ubiquitous access to content and limited resources, like bandwidth and battery. Since most web pages are designed for the wired Internet, it is challenging to adapt the pages seamlessly to ensure a satisfactory mobile web experience. Content heavy web pages lead to longer load time on mobile browsers. Pre-defined load order of items in a page does not adapt to mobile browsing habits, where user looks for different snippets of a page to load under different contexts. Web content adaptation for mobile web has mainly focused on the user to define her preferences for content. We propose a framework where content creator is additionally included in guiding the adaptation. Allowing content creator to specify importance of items in a page also helps in factoring her incentives by pushing revenue generating content. We present mechanisms to enable cooperative content adaptation. Preliminary results show the efficacy of cooperative content adaptation in resource constrained mobile browsing scenario.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {221–222},
numpages = {2},
keywords = {content adaptation, mobile browsing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487903,
author = {Pang, Guansong and Jin, Huidong and Jiang, Shengyi},
title = {An Effective Class-Centroid-Based Dimension Reduction Method for Text Classification},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487903},
doi = {10.1145/2487788.2487903},
abstract = {Motivated by the effectiveness of centroid-based text classification techniques, we propose a classification-oriented class-centroid-based dimension reduction (DR) method, called CentroidDR. Basically, CentroidDR projects high-dimensional documents into a low-dimensional space spanned by class centroids. On this class-centroid-based space, the centroid-based classifier essentially becomes CentroidDR plus a simple linear classifier. Other classification techniques, such as K-Nearest Neighbor (KNN) classifiers, can be used to replace the simple linear classifier to form much more effective text classification algorithms. Though CentroidDR is simple, non-parametric and runs in linear time, preliminary experimental results show that it can improve the accuracy of the classifiers and perform better than general DR methods such as Latent Semantic Indexing (LSI).},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {223–224},
numpages = {2},
keywords = {dimension reduction, class centroid, text classification},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487904,
author = {Zubiaga, Arkaitz and Ji, Heng},
title = {Harnessing Web Page Directories for Large-Scale Classification of Tweets},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487904},
doi = {10.1145/2487788.2487904},
abstract = {Classification is paramount for an optimal processing of tweets, albeit performance of classifiers is hindered by the need of large sets of training data to encompass the diversity of contents one can find on Twitter. In this paper, we introduce an inexpensive way of labeling large sets of tweets, which can be easily regenerated or updated when needed. We use human-edited web page directories to infer categories from URLs contained in tweets. By experimenting with a large set of more than 5 million tweets categorized accordingly, we show that our proposed model for tweet classification can achieve 82% in accuracy, performing only 12.2% worse than for web page classification.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {225–226},
numpages = {2},
keywords = {classification, large-scale, tweets, distant},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487905,
author = {Park, Youngki and Park, Sungchan and Lee, Sang-goo and Jung, Woosung},
title = {Scalable K-Nearest Neighbor Graph Construction Based on Greedy Filtering},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487905},
doi = {10.1145/2487788.2487905},
abstract = {K-Nearest Neighbor Graph (K-NNG) construction is a primitive operation in the field of Information Retrieval and Recommender Systems. However, existing approaches to K-NNG construction do not perform well as the number of nodes or dimensions scales up. In this paper, we present greedy filtering, an effcient and scalable algorithm for selecting the candidates for nearest neighbors by matching only the dimensions of large values. The experimental results show that our K-NNG construction scheme, based on greedy filtering, guarantees a high recall while also being 5 to 6 times faster than state-of-the-art algorithms for large, high-dimensional data.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {227–228},
numpages = {2},
keywords = {k-nearest neighbor graphs, similarity join, greedy filtering},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487906,
author = {Wu, Jie and Liu, Yi and Wen, Ji-Rong},
title = {Numeric Query Ranking Approach},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487906},
doi = {10.1145/2487788.2487906},
abstract = {We handle a special category of Web queries, queries containing numeric terms. We call them numeric queries. Motivated by some issues in ranking of numeric queries, we detect numeric sensitive queries by mining from retrieved documents using phrase operator. We also propose features based on numeric terms by extracting reliable numeric terms for each document. Finally, a ranking model is trained for numeric sensitive queries, combining proposed numeric-related features and traditional features. Experiments show that our model can significantly improve relevance for numeric sensitive queries.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {229–230},
numpages = {2},
keywords = {web search, numeric sensitive queries, numeric queries},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487907,
author = {Lian, Defu and Zheng, Vincent W. and Xie, Xing},
title = {Collaborative Filtering Meets next Check-in Location Prediction},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487907},
doi = {10.1145/2487788.2487907},
abstract = {With the increasing popularity of Location-based Social Networks, a vast amount of location check-ins have been accumulated. Though location prediction in terms of check-ins has been recently studied, the phenomena that users often check in novel locations has not been addressed. To this end, in this paper, we leveraged collaborative filtering techniques for check-in location prediction and proposed a short- and long-term preference model. We extensively evaluated it on two large-scale check-in datasets from Gowalla and Dianping with 6M and 1M check-ins, respectively, and showed that the proposed model can outperform the competing baselines.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {231–232},
numpages = {2},
keywords = {lbsns, collaborative filtering, location prediction},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487908,
author = {Jiang, Yu and Liu, Jing and Zhang, Xi and Li, Zechao and Lu, Hanqing},
title = {TCRec: Product Recommendation via Exploiting Social-Trust Network and Product Category Information},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487908},
doi = {10.1145/2487788.2487908},
abstract = {In this paper, we develop a novel product recommendation method called TCRec, which takes advantage of consumer rating history record, social-trust network and product category information simultaneously. Compared experiments are conducted on two real-world datasets and outstanding performance is achieved, which demonstrates the effectiveness of TCRec.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {233–234},
numpages = {2},
keywords = {recommendation, social-trust network, product category},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487909,
author = {Sakaki, Takeshi and Toriumi, Fujio and Shinoda, Kosuke and Kazama, Kazuhiro and Kurihara, Satoshi and Noda, Itsuki and Matsuo, Yutaka},
title = {Regional Analysis of User Interactions on Social Media in Times of Disaster},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487909},
doi = {10.1145/2487788.2487909},
abstract = {Social media attract attention for sharing information, especially Twitter, which is now being used in times of disasters. In this paper, we perform regional analysis of user interactions on Twittter during the Great East Japan Earthquake and arrived at the following two conclusions:People diffused much more information after the earthquake, especially in the heavily-damaged areas; People communicated with nearby users but diffused information posted by distant users. We conclude that social media users changed their behavior to widely diffuse information.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {235–236},
numpages = {2},
keywords = {online social network, disaster situation, information diffusion, earthquake, twitter},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487910,
author = {Marcacini, Ricardo M. and Domingues, Marcos A. and Rezende, Solange O.},
title = {Improving Consensus Clustering of Texts Using Interactive Feature Selection},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487910},
doi = {10.1145/2487788.2487910},
abstract = {Consensus clustering and interactive feature selection are very useful methods to extract and manage knowledge from texts. While consensus clustering allows the aggregation of different clustering solutions into a single robust clustering solution, the interactive feature selection facilitates the incorporation of the users experience in text clustering tasks by selecting a set of high-level features. In this paper, we propose an approach to improve the robustness of consensus clustering using interactive feature selection. We have reported some experimental results on real-world datasets that show the effectiveness of our approach.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {237–238},
numpages = {2},
keywords = {consensus clustering, interactive feature selection},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487912,
author = {Lo, James and Wohlstadter, Eric and Mesbah, Ali},
title = {Live Migration of JavaScript Web Apps},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487912},
doi = {10.1145/2487788.2487912},
abstract = {Due to the increasing complexity of web applications and emerging HTML5 standards, a large amount of runtime state is created and managed in the user's browser. While such complexity is desirable for user experience, it makes it hard for developers to implement mechanisms that provide users ubiquitous access to the data they create during application use. This work showcases Imagen, our implemented platform for browser session migration of JavaScript-based web applications. Session migration is the act of transferring a session between browsers at runtime. Without burden to developers, Imagen allows users to create a snapshot image that captures the runtime state needed to resume the session elsewhere. Our approach works completely in the JavaScript layer and we demonstrate that snapshots can be transferred between different browser vendors and hardware devices. The demo will illustrate our system's performance and interoperability using two HTML5 apps, four different browsers and three different devices.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {241–244},
numpages = {4},
keywords = {javascript, json, dom, html5, session migration},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487913,
author = {Le Breton, Gabriel and Maronnaud, Fabien and Hall\'{e}, Sylvain},
title = {Automated Exploration and Analysis of Ajax Web Applications with WebMole},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487913},
doi = {10.1145/2487788.2487913},
abstract = {WebMole is a browser-based tool that automatically and exhaustively explores all pages inside a web application. Contrarily to classical web crawlers, which only explore pages accessible through regular anchors, WebMole can find its way through Ajax applications that use JavaScript-triggered links, and handles state changes that do not involve a page reload. User-defined functions called oracles can be used to bound the range of pages explored by WebMole to specific parts of an application, as well as to evaluate Boolean test conditions on all visited pages. Overall, WebMole can prove a more flexible alternative to automated testing suites such as Selenium WebDriver.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {245–248},
numpages = {4},
keywords = {navigation, web applications, reverse engineering},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487914,
author = {Heinrich, Matthias and Lehmann, Franz and Gr\"{u}neberger, Franz Josef and Springer, Thomas and Gaedke, Martin},
title = {Analyzing the Suitability of Web Applications for a Single-User to Multi-User Transformation},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487914},
doi = {10.1145/2487788.2487914},
abstract = {Multi-user web applications like Google Docs or Etherpad are crucial to efficiently support collaborative work (e.g. jointly create texts, graphics, or presentations). Nevertheless, enhancing single-user web applications with multi-user capabilities (i.e. document synchronization and conflict resolution) is a time-consuming and intricate task since traditional approaches adopting concurrency control libraries (e.g. Apache Wave) require numerous scattered source code changes. Therefore, we devised the Generic Collaboration Infrastructure (GCI) [8] that is capable of converting single-user web applications non-invasively into collaborative ones, i.e. no source code changes are required. In this paper, we present a catalog of vital application properties that allows determining if a web application is suitable for a GCI transformation. On the basis of the introduced catalog, we analyze 12 single-user web applications and show that 6 are eligible for a GCI transformation. Moreover, we demonstrate (1) the transformation of one qualified application, namely, the prominent text editor TinyMCE, and (2) showcase the resulting multi-user capabilities. Both demo parts are illustrated in a dedicated screencast that is available at http://vsr.informatik.tu-chemnitz.de/demo/TinyMCE/.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {249–252},
numpages = {4},
keywords = {groupware, web applications, web engineering, shared editing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487915,
author = {Langhans, Philipp and Wieser, Christoph and Bry, Fran\c{c}ois},
title = {Crowdsourcing MapReduce: JSMapReduce},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487915},
doi = {10.1145/2487788.2487915},
abstract = {JSMapReduce is an implementation of MapReduce which exploits the computing power available in the computers of the users of a web platform by giving tasks to the JavaScript engines of their web browsers. This article describes the implementation of JSMapReduce exploiting HTML 5 features, the heuristics it uses for distributing tasks to workers, and reports on an experimental evaluation of JSMapReduce.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {253–256},
numpages = {4},
keywords = {javascript, crowdsourcing, mapreduce},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487916,
author = {Boden, Christoph and Karnstedt, Marcel and Fernandez, Miriam and Markl, Volker},
title = {Large-Scale Social-Media Analytics on Stratosphere},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487916},
doi = {10.1145/2487788.2487916},
abstract = {The importance of social-media platforms and online communities - in business as well as public context - is more and more acknowledged and appreciated by industry and researchers alike. Consequently, a wide range of analytics has been proposed to understand, steer, and exploit the mechanics and laws driving their functionality and creating the resulting benefits. However, analysts usually face significant problems in scaling existing and novel approaches to match the data volume and size of modern online communities. In this work, we propose and demonstrate the usage of the massively parallel data processing system Stratosphere, based on second order functions as an extended notion of the MapReduce paradigm, to provide a new level of scalability to such social-media analytics. Based on the popular example of role analysis, we present and illustrate how this massively parallel approach can be leveraged to scale out complex data-mining tasks, while providing a programming approach that eases the formulation of complete analytical workflows.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {257–260},
numpages = {4},
keywords = {online communities, community analysis, scalability, role analysis, boards.ie, stratosphere, behaviour analysis},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487917,
author = {Kim, HyeongSik and Ravindra, Padmashree and Anyanwu, Kemafor},
title = {Optimizing RDF(S) Queries on Cloud Platforms},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487917},
doi = {10.1145/2487788.2487917},
abstract = {Scalable processing of Semantic Web queries has become a critical need given the rapid upward trend in availability of Semantic Web data. The MapReduce paradigm is emerging as a platform of choice for large scale data processing and analytics due to its ease of use, cost effectiveness, and potential for unlimited scaling. Processing queries on Semantic Web triple models is a challenge on the mainstream MapReduce platform called Apache Hadoop, and its extensions such as Pig and Hive. This is because such queries require numerous joins which leads to lengthy and expensive MapReduce workflows. Further, in this paradigm, cloud resources are acquired on demand and the traditional join optimization machinery such as statistics and indexes are often absent or not easily supported.In this demonstration, we will present RAPID+, an extended Apache Pig system that uses an algebraic approach for optimizing queries on RDF data models including queries involving inferencing. The basic idea is that by using logical and physical operators that are more natural to MapReduce processing, we can reinterpret such queries in a way that leads to more concise execution workflows and small intermediate data footprints that minimize disk I/Os and network transfer overhead. RAPID+ evaluates queries using the Nested TripleGroup Data Model and Algebra(NTGA). The demo will show comparative performance of NTGA query plans vs. relational algebra-like query plans used by Apache Pig and Hive.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {261–264},
numpages = {4},
keywords = {mapreduce, rdf(s), hadoop, sparql},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487918,
author = {Galli, Marcio dos Santos and Santos, Eduardo Pezutti Beletato},
title = {TagVisor: Extending Web Pages with Interaction Events to Support Presentation in Digital Signage},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487918},
doi = {10.1145/2487788.2487918},
abstract = {New interaction experiences are fundamentally changing the way we interact with the web. Emerging touch-based devices and a variety of web-connected appliances represents challenges that prevents the seamless reach of web resources originally tailored for the standard browser experience. This paper explores how web pages can be re-purposed and become interactive presentations that effectively supports communication in scenarios such as digital signage and other presentation use cases. We will cover the TagVisor project which is a JavaScript run-time that uses modern animation effects and provides an HTML5 extension approach to support the authoring of visual narratives using plain web pages.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {265–268},
numpages = {4},
keywords = {javascript, digital signage, html5, w3c, web, boot2gecko, css3, dom},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487919,
author = {Roy Chowdhury, Soudip and Chudnovskyy, Olexiy and Niederhausen, Matthias and Pietschmann, Stefan and Sharples, Paul and Daniel, Florian and Gaedke, Martin},
title = {Complementary Assistance Mechanisms for End User Mashup Composition},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487919},
doi = {10.1145/2487788.2487919},
abstract = {Despite several efforts for simplifying the composition process, learning efforts required for using existing mashup editors to develop mashups remain still high. In this paper, we describe how this barrier can be lowered by means of an assisted development approach that seamlessly integrates automatic composition and interactive pattern recommendation techniques into existing mashup platforms for supporting easy mashup development by end users. We showcase the use of such an assisted development environment in the context of an open-source mashup platform Apache Rave. Results of our user studies demonstrate the benefits of our approach for end user mashup development.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {269–272},
numpages = {4},
keywords = {crisis mashup, automated compostion, assisted mashup development, interactive pattern recommendation, end user development},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487921,
author = {Rodrigues, Tiago and Dewan, Prateek and Kumaraguru, Ponnurangam and Minardi, Raquel Melo and Almeida, Virg\'{\i}lio},
title = {UTrack: Track Yourself! Monitoring Information on Online Social Media},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487921},
doi = {10.1145/2487788.2487921},
abstract = {The past one decade has witnessed an astounding outburst in the number of online social media (OSM) services, and a lot of these services have enthralled millions of users across the globe. With such tremendous number of users, the amount of content being generated and shared on OSM services is also enormous. As a result, trying to visualize all this overwhelming amount of content, and gain useful insights from it has become a challenge. In this work, we present uTrack, a personalized web service to analyze and visualize the diffusion of content shared by users across multiple OSM platforms. To the best of our knowledge, there exists no work which concentrates on monitoring information diffusion for personal accounts. Currently, uTrack monitors and supports logging in from Facebook, Twitter, and Google+. Once granted permissions by the user, uTrack monitors all URLs (like videos, photos, news articles) the user has shared in all OSM services supported, and generates useful visualizations and statistics from the collected data.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {273–276},
numpages = {4},
keywords = {visualization, information diffusion, online social media, tracking},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487922,
author = {Wei, Bifan and Liu, Jun and Ma, Jian and Zheng, Qinghua and Zhang, Wei and Feng, Boqin},
title = {DFT-Extractor: A System to Extract Domain-Specific Faceted Taxonomies from Wikipedia},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487922},
doi = {10.1145/2487788.2487922},
abstract = {Extracting faceted taxonomies from the Web has received increasing attention in recent years from the web mining community. We demonstrate in this study a novel system called DFT-Extractor, which automatically constructs domain-specific faceted taxonomies from Wikipedia in three steps: 1) It crawls domain terms from Wikipedia by using a modified topical crawler. 2) Then it exploits a classification model to extract hyponym relations with the use of motif-based features. 3) Finally, it constructs a faceted taxonomy by applying a community detection algorithm and a group of heuristic rules. DFT-Extractor also provides a graphical user interface to visualize the learned hyponym relations and the tree structure of taxonomies.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {277–280},
numpages = {4},
keywords = {faceted taxonomy, network motif, wikipedia},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487923,
author = {Georgescu, Mihai and Pham, Dang Duc and Kanhabua, Nattiya and Zerr, Sergej and Siersdorfer, Stefan and Nejdl, Wolfgang},
title = {Temporal Summarization of Event-Related Updates in Wikipedia},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487923},
doi = {10.1145/2487788.2487923},
abstract = {Wikipedia is a free multilingual online encyclopedia covering a wide range of general and specific knowledge. Its content is continuously maintained up-to-date and extended by a supporting community. In many cases, real-world events influence the collaborative editing of Wikipedia articles of the involved or affected entities. In this paper, we present Wikipedia Event Reporter, a web-based system that supports the entity-centric, temporal analytics of event-related information in Wikipedia by analyzing the whole history of article updates. For a given entity, the system first identifies peaks of update activities for the entity using burst detection and automatically extracts event-related updates using a machine-learning approach. Further, the system determines distinct events through the clustering of updates by exploiting different types of information such as update time, textual similarity, and the position of the updates within an article. Finally, the system generates the meaningful temporal summarization of event-related updates and automatically annotates the identified events in a timeline.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {281–284},
numpages = {4},
keywords = {temporal summarization, entity timeline, wikipedia updates, event detection},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487924,
author = {Milicic, Vuk and Rizzo, Giuseppe and Redondo Garcia, Jos\'{e} Luis and Troncy, Rapha\"{e}l and Steiner, Thomas},
title = {Live Topic Generation from Event Streams},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487924},
doi = {10.1145/2487788.2487924},
abstract = {Social platforms constantly record streams of heterogeneous data about human's activities, feelings, emotions and conversations opening a window to the world in real-time. Trends can be computed but making sense out of them is an extremely challenging task due to the heterogeneity of the data and its dynamics making often short-lived phenomena. We develop a framework which collects microposts shared on social platforms that contain media items as a result of a query, for example a trending event. It automatically creates different visual storyboards that reflect what users have shared about this particular event. More precisely it leverages on: (i) visual features from media items for near-deduplication, and (ii) textual features from status updates to interpret, cluster, and visualize media items. A screencast showing an example of these functionalities is published at: http://youtu.be/8iRiwz7cDYY while the prototype is publicly available at http://mediafinder.eurecom.fr.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {285–288},
numpages = {4},
keywords = {topic generation, storytelling, visual summarizatio, storyboard identification, social media},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487925,
author = {Verma, Pramod},
title = {Serefind: A Social Networking Website for Classifieds},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487925},
doi = {10.1145/2487788.2487925},
abstract = {This paper presents the design and implementation of a social networking website for classifieds, called Serefind. We designed search interfaces with focus on security, privacy, usability, design, ranking, and communications. We deployed this site at the Johns Hopkins University, and the results show it can be used as a self-sustaining classifieds site for public or private communities.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {289–292},
numpages = {4},
keywords = {classifieds, search interfaces, security and privacy},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487926,
author = {Cleveland, Seth B. and Gao, Byron J.},
title = {MASFA: Mass-Collaborative Faceted Search for Online Communities},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487926},
doi = {10.1145/2487788.2487926},
abstract = {Faceted search combines faceted navigation with direct keyword search, providing exploratory search capacities allowing progressive query refinement. It has become the de facto standard for e-commerce and product-related websites such as amazon.com and ebay.com. However, faceted search has not been effectively incorporated into non-commercial online community portals such as craigslist.org. This is mainly because unlike keyword search, faceted search systems require metadata that constantly evolve, making them very costly to build and maintain. In this paper, we propose a framework MASFA that utilizes a set of non-domain-specific techniques to build and maintain effective, portable, and cost-free faceted search systems in a mass-collaborative manner. We have implemented and deployed the framework on selected categories of Craigslist to demonstrate its utility.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {293–296},
numpages = {4},
keywords = {interactive information retrieval, mass-collaboration, crowdsoursing, faceted search, named entity recognition, human computation, taxonomy, faceted navigation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487927,
author = {Crescenzi, Valter and Merialdo, Paolo and Qiu, Disheng},
title = {ALFRED: Crowd Assisted Data Extraction},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487927},
doi = {10.1145/2487788.2487927},
abstract = {The development of solutions to scale the extraction of data from Web sources is still a challenging issue. High accuracy can be achieved by supervised approaches, but the costs of training data, i.e., annotations over a set of sample pages, limit their scalability. Crowdsourcing platforms are making the manual annotation process more affordable. However, the tasks demanded to these platforms should be extremely simple, to be performed by non-expert people, and their number should be minimized, to contain the costs. We demonstrate ALFRED, a wrapper inference system supervised by the workers of a crowdsourcing platform. Training data are labeled values generated by means of membership queries, the simplest form of queries, posed to the crowd. ALFRED includes several original features: it automatically selects a representative sample set from the input collection of pages; in order to minimize the wrapper inference costs, it dynamically sets the expressiveness of the wrapper formalism and it adopts an active learning algorithm to select the queries posed to the crowd; it is able to manage inaccurate answers that can be provided by the workers engaged by crowdsourcing platforms.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {297–300},
numpages = {4},
keywords = {wrapper generation, crowdsourcing, data extraction, active learning},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487928,
author = {Yus, Roberto and Mena, Eduardo and Ilarri, Sergio and Illarramendi, Arantza},
title = {SHERLOCK: A System for Location-Based Services in Wireless Environments Using Semantics},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487928},
doi = {10.1145/2487788.2487928},
abstract = {Nowadays people are exposed to huge amounts of information that are generated continuously. However, current mobile applications, Web pages, and Location-Based Services (LBSs) are designed for specific scenarios and goals. In this demo we show the system SHERLOCK, which searches and shares up-to-date knowledge from nearby devices to relieve the user from knowing and managing such knowledge directly. Besides, the system guides the user in the process of selecting the service that best fits his/her needs in the given context.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {301–304},
numpages = {4},
keywords = {mobile computing, knowledge representation and reasoning, real-time query processing, location-based services},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487930,
author = {Tavakolifard, Mozhgan and Gulla, Jon Atle and Almeroth, Kevin C. and Ingvaldesn, Jon Espen and Nygreen, Gaute and Berg, Erik},
title = {Tailored News in the Palm of Your Hand: A Multi-Perspective Transparent Approach to News Recommendation},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487930},
doi = {10.1145/2487788.2487930},
abstract = {Mobile news recommender systems help users retrieve news that is relevant in their particular context and can be presented in ways that require minimal user interaction. In spite of the availability of contextual information about mobile users, though, current mobile news applications employ rather simple strategies for news recommendation. Our multi-perspective approach unifies temporal, locational, and preferential information to provide a more fine-grained recommendation strategy. This demo paper presents the implementation of our solution to efficiently recommend specific news articles from a large corpus of newly-published press releases in a way that closely matches a reader's reading preferences.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {305–308},
numpages = {4},
keywords = {news, recommender systems, mobile, user modeling},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487931,
author = {Nixon, Lyndon and Bauer, Matthias and Bara, Cristian},
title = {Connected Media Experiences: Web Based Interactive Video Using Linked Data},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487931},
doi = {10.1145/2487788.2487931},
abstract = {This demo submission presents a set of tools and an extended framework with API for enabling the semantically empowered enrichment of online video with Web content. As audiovisual media is increasingly transmitted online, new services deriving added value from such material can be imagined. For example, combining it with other material elsewhere on the Web which is related to it or enhances it in a meaningful way, to the benefit of the owner of the original content, the providers of the content enhancing it and the end consumer who can access and interact with these new services. Since the services are built around providing new experiences through connecting different related media together, we consider such services to be Connected Media Experiences (ConnectME). This paper presents a toolset for ConnectME - an online annotation tool for video and a HTML5-based enriched video player - as well as the ConnectME framework which enables these media experiences to be generated on the server side with semantic technology.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {309–312},
numpages = {4},
keywords = {hypervideo, annotation, enrichment, linked data, web media, media linking, clickable video},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487932,
author = {Pereira, \'{A}lvaro R. and Dutra, Diego and Stiilpen, Milton and Dutra, Alex Amorim and Melo, Felipe Martins and Mendon\c{c}a, Paulo H. C. and de Jesus, \^{A}ngelo Magno and Ferreira, Kledilson},
title = {Radialize: A Tool for Social Listening Experience on the Web Based on Radio Station Programs},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487932},
doi = {10.1145/2487788.2487932},
abstract = {Radialize represents a service for listening to music and radio programs through the web. The service allows the discovery of the content being played by radio stations on the web, either by managing explicit information made available by those stations or by means of our technology for automatic recognition of audio content in a stream. Radialize then offers a service in which the user can search, be recommended, and provide feedback on artists and songs being played in traditional radio stations, either explicitly or implicitly, in order to compose an individual profile. The recommender system utilizes every user interaction as a data source, as well as the similarity abstraction extracted out of the radios' musical programs, making use of the wisdom of crowds implicitly present in the radio programs.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {313–316},
numpages = {4},
keywords = {recommendation, crawling, music, search, signal processing, software architecture, radio},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487933,
author = {Hoi, Steven C.H. and Wang, Dayong and Cheng, I. Yeu and Lin, Elmer Weijie and Zhu, Jianke and He, Ying and Miao, Chunyan},
title = {FANS: Face Annotation by Searching Large-Scale Web Facial Images},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487933},
doi = {10.1145/2487788.2487933},
abstract = {Auto face annotation is an important technique for many real-world applications, such as online photo album management, new video summarization, and so on. It aims to automatically detect human faces from a photo image and further name the faces with the corresponding human names. Recently, mining web facial images on the internet has emerged as a promising paradigm towards auto face annotation. In this paper, we present a demonstration system of search-based face annotation: FANS - Face ANnotation by Searching large-scale web facial images. Given a query facial image for annotation, we first retrieve a short list of the most similar facial images from a web facial image database, and then annotate the query facial image by mining the top-ranking facial images and their corresponding labels with sparse representation techniques. Our demo system was built upon a large-scale real-world web facial image database with a total of 6,025 persons and about 1 million facial images. This paper demonstrates the potential of searching and mining web-scale weakly labeled facial images on the internet to tackle the challenging face annotation problem, and addresses some open problems for future exploration by researchers in web community. The live demo of FANS is available online at http://msm.cais.ntu.edu.sg/FANS/.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {317–320},
numpages = {4},
keywords = {web facial images, web data mining, search-based face annotation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487934,
author = {Gomes, Daniel and Cruz, David and Miranda, Jo\~{a}o and Costa, Miguel and Fontes, Sim\~{a}o},
title = {Search the Past with the Portuguese Web Archive},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487934},
doi = {10.1145/2487788.2487934},
abstract = {The web was invented to quickly exchange data between scientists, but it became a crucial communication tool to connect the world. However, the web is extremely ephemeral. Most of the information published online becomes quickly unavailable and is lost forever. There are several initiatives worldwide that struggle to archive information from the web before it vanishes. However, search mechanisms to access this information are still limited and do not satisfy their users who demand performance similar to live-web search engines.This demo presents the Portuguese Web Archive, which enables search over 1.2 billion files archived from 1996 to 2012. It is the largest full-text searchable web archive publicly available [17]. The software developed to support this service is also publicly available as a free open source project at Google Code, so that it can be reused and enhanced by other web archivists. A short video about the Portuguese Web Archive is available at vimeo.com/59507267. The service can be tried live at archive.pt.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {321–324},
numpages = {4},
keywords = {temporal search, web archiving, digital preservation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487935,
author = {Biega, Joanna and Kuzey, Erdal and Suchanek, Fabian M.},
title = {Inside YAGO2s: A Transparent Information Extraction Architecture},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487935},
doi = {10.1145/2487788.2487935},
abstract = {YAGO [9, 6] is one of the largest public ontologies constructed by information extraction. In a recent refactoring called YAGO2s, the system has been given a modular and completely transparent architecture. In this demo, users can see how more than 30 individual modules of YAGO work in parallel to extract facts, to check facts for their correctness, to deduce facts, and to merge facts from different sources. A GUI allows users to play with different input files, to trace the provenance of individual facts to their sources, to change deduction rules, and to run individual extractors. Users can see step by step how the extractors work together to combine the individual facts to the coherent whole of the YAGO ontology.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {325–328},
numpages = {4},
keywords = {information extraction, yago, ontologies},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487936,
author = {Ngonga Ngomo, Axel-Cyrille and B\"{u}hmann, Lorenz and Unger, Christina and Lehmann, Jens and Gerber, Daniel},
title = {SPARQL2NL: Verbalizing Sparql Queries},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487936},
doi = {10.1145/2487788.2487936},
abstract = {Linked Data technologies are now being employed by a large number of applications. While experts can query the backend of these applications using the standard query language SPARQL, most lay users lack the expertise necessary to proficiently interact with these applications. Consequently, non-expert users usually have to rely on forms, query builders, question answering or keyword search tools to access RDF data. Yet, these tools are usually unable to make the meaning of the queries they generate plain to lay users, making it difficult for these users to i) assess the correctness of the query generated out of their input, and ii) to adapt their queries or iii) to choose in an informed manner between possible interpretations of their input.We present SPARQL2NL, a generic approach that allows verbalizing SPARQL queries, i.e., converting them into natural language. In addition to generating verbalizations, our approach can also explain the output of queries by providing a natural-language description of the reasons that led to each element of the result set being selected. Our evaluation of SPARQL2NL within a large-scale user survey shows that SPARQL2NL generates complete and easily understandable natural language descriptions. In addition, our results suggest that even SPARQL experts can process the natural language representation of SPARQL queries computed by our approach more efficiently than the corresponding SPARQL queries. Moreover, non-experts are enabled to reliably understand the content of SPARQL queries. Within the demo, we present the results generated by our approach on arbitrary questions to the DBpedia and MusicBrainz datasets. Moreover, we present how our framework can be used to explain results of SPARQL queries in natural language},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {329–332},
numpages = {4},
keywords = {query verbalization, sparql, natural language generation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487937,
author = {Bai, Yiyuan and Wang, Chaokun and Ning, Yuanchi and Wu, Hanzhao and Wang, Hao},
title = {G-Path: Flexible Path Pattern Query on Large Graphs},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487937},
doi = {10.1145/2487788.2487937},
abstract = {With the socialization trend of web sites and applications, the techniques of effective management of graph-structured data have become one of the most important modern web technologies. In this paper, we present a system of path query on large graphs, known as G-Path. Based on Hadoop distributed framework and bulk synchronized parallel model, the system can process generic queries without preprocessing or building indices. To demonstrate the system, we developed a web-based application which allows searching entities and relationships on a large social network, e.g., DBLP publication network or Twitter dataset. With the flexibility of G-Path, the application is able to handle different kinds of queries. For example, a user may want to search for a publication graph of an author while another user may want to search for all publications of the author's co-authors. All these queries can be done by an interactive user interface and the results will be shown in a visual graph.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {333–336},
numpages = {4},
keywords = {graph query language, regular path pattern, g-path, path pattern query, social network},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487939,
author = {Benson, Edward},
title = {Mockup Driven Web Development},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487939},
doi = {10.1145/2487788.2487939},
abstract = {Dynamic web development still borrows heavily from its origins in CGI scripts: modern web applications are largely designed and developed as programs that happen to output HTML. This thesis proposes to investigate the idea taking a mockup-centric approach instead, in which self-contained, full page web mockups are the central artifact driving the application development process. In some cases, these mockups are sufficient to infer the dynamic application structure completely.This approach to mockup driven development is made possible by the development of a language the thesis develops, called Cascading Tree Sheets (CTS), that enables a mockup to be annotated with enough information so that many common web development tasks and workflows can be eliminated or vastly simplified. CTS describes and encapsulates a web page's design structure the same way CSS describes its styles. This enables mockups to serve as the input of a web application rather than simply a design artifact. Using this capability, I will study the feasibility and usability of mockup driven development for a range of novice and expert authorship tasks. The thesis aims to finish by demonstrating that the functionality of a domain-specific content management system can be inferred automatically from site mockups.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {337–342},
numpages = {6},
keywords = {application synthesis, web authoring usability, web architecture, content management},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487940,
author = {Binh Tran, Giang},
title = {Structured Summarization for News Events},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487940},
doi = {10.1145/2487788.2487940},
abstract = {Helping users to understand the news is an acute problem nowadays as the users are struggling to keep up with tremendous amount of information published every day in the Internet. In this research, we focus on modelling the content of news events by their semantic relations with other events, and generating structured summarization.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {343–348},
numpages = {6},
keywords = {hierarchical, structured summarization, relation extraction, temporal, spatial, news events, causal},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487941,
author = {Bracamonte, Teresa},
title = {Multimedia Information Retrieval on the Social Web},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487941},
doi = {10.1145/2487788.2487941},
abstract = {Efforts have been made to obtain more accurate results for multimedia searches on the Web. Nevertheless, not all multimedia objects have related text descriptions available. This makes bridging the semantic gap more difficult. Approaches that combine context and content information of multimedia objects are the most popular for indexing and later retrieving these objects. However, scaling these techniques to Web environments is still an open problem. In this thesis, we propose the use of user-generated content (UGC) from the Web and social platforms as well as multimedia content information to describe the context of multimedia objects. We aim to design tag-oriented algorithms to automatically tag multimedia objects, filter irrelevant tags, and cluster tags in semantically-related groups. The novelty of our proposal is centered on the design of Web-scalable algorithms that enrich multimedia context using the social information provided by users as a result of their interaction with multimedia objects. We validate the results of our proposal with a large-scale evaluation in crowdsourcing platforms.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {349–354},
numpages = {6},
keywords = {multimedia content analysis, social media analysis, web mining, multimedia information retrieval},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487942,
author = {Eshete, Birhanu},
title = {Effective Analysis, Characterization, and Detection of Malicious Web Pages},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487942},
doi = {10.1145/2487788.2487942},
abstract = {The steady evolution of the Web has paved the way for miscreants to take advantage of vulnerabilities to embed malicious content into web pages. Up on a visit, malicious web pages steal sensitive data, redirect victims to other malicious targets, or cease control of victim's system to mount future attacks. Approaches to detect malicious web pages have been reactively effective at special classes of attacks like drive-by-downloads. However, the prevalence and complexity of attacks by malicious web pages is still worrisome. The main challenges in this problem domain are (1) fine-grained capturing and characterization of attack payloads (2) evolution of web page artifacts and (3) exibility and scalability of detection techniques with a fast-changing threat landscape. To this end, we proposed a holistic approach that leverages static analysis, dynamic analysis, machine learning, and evolutionary searching and optimization to effectively analyze and detect malicious web pages. We do so by: introducing novel features to capture fine-grained snapshot of malicious web pages, holistic characterization of malicious web pages, and application of evolutionary techniques to fine-tune learning-based detection models pertinent to evolution of attack payloads. In this paper, we present key intuition and details of our approach, results obtained so far, and future work.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {355–360},
numpages = {6},
keywords = {machine learning, static analysis, dynamic analysis, web-based attacks, malicious web pages, effective detection},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487943,
author = {Fl\"{o}ck, Fabian},
title = {Identifying, Understanding and Detecting Recurring, Harmful Behavior Patterns in Collaborative Wikipedia Editing: Doctoral Proposal},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487943},
doi = {10.1145/2487788.2487943},
abstract = {In this doctoral proposal, we describe an approach to identify recurring, collective behavioral mechanisms in the collaborative interactions of Wikipedia editors that have the potential to undermine the ideals of quality, neutrality and completeness of article content. We outline how we plan to parametrize these patterns in order to understand their emergence and evolution and measure their effective impact on content production in Wikipedia. On top of these results we intend to build end-user tools to increase the transparency of the evolution of articles and equip editors with more elaborated quality monitors. We also sketch out our evaluation plans and report on already accomplished tasks.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {361–366},
numpages = {6},
keywords = {collaboration systems, collective intelligence, editing behavior, social dynamics, user modeling, web science, wikipedia},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487944,
author = {Freitas, Larissa A. and Vieira, Renata},
title = {Ontology Based Feature Level Opinion Mining for Portuguese Reviews},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487944},
doi = {10.1145/2487788.2487944},
abstract = {This paper presents a thesis whose goal is to propose and evaluate methods to identify polarity in Portuguese user generated reviews according to features described in domain ontologies (experiments will consider movie and hotel ontologies Movie Ontology1 and Hontology2).},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {367–370},
numpages = {4},
keywords = {ontology, sentiment analysis, opinion mining, feature level},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487945,
author = {Gyrard, Amelie},
title = {A Machine-to-Machine Architecture to Merge Semantic Sensor Measurements},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487945},
doi = {10.1145/2487788.2487945},
abstract = {The emerging field Machine-to-Machine (M2M) enables machines to communicate with each other without human intervention. Existing semantic sensor networks are domain-specific and add semantics to the context. We design a Machine-to-Machine (M2M) architecture to merge heterogeneous sensor networks and we propose to add semantics to the measured data rather than to the context. This architecture enables to: (1) get sensor measurements, (2) enrich sensor measurements with semantic web technologies, domain ontologies and the Link Open Data, and (3) reason on these semantic measurements with semantic tools, machine learning algorithms and recommender systems to provide promising applications.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {371–376},
numpages = {6},
keywords = {semantic sensor networks, machine-to-machine (m2m), ontology, m2m applications, semantic web technologies, m2m gateways, resource description framework (rdf)},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487946,
author = {Khelghati, Mohamamdreza and Hiemstra, Djoerd and Van Keulen, Maurice},
title = {Deep Web Entity Monitoring},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487946},
doi = {10.1145/2487788.2487946},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {377–382},
numpages = {6},
keywords = {entity monitoring, web harvesting, crawling, deep web},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487947,
author = {Kiseleva, Julia},
title = {Context Mining and Integration into Predictive Web Analytics},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487947},
doi = {10.1145/2487788.2487947},
abstract = {Predictive Web Analytics is aimed at understanding behavioural patterns of users of various web-based applications: e-commerce, ubiquitous and mobile computing, and computational advertising. Within these applications business decisions often rely on two types of predictions: an overall or particular user segment demand predictions and individualised recommendations for visitors. Visitor behaviour is inherently sensitive to the context, which can be defined as a collection of external factors. Context-awareness allows integrating external explanatory information into the learning process and adapting user behaviour accordingly. The importance of context-awareness has been recognised by researchers and practitioners in many disciplines, including recommendation systems, information retrieval, personalisation, data mining, and marketing. We focus on studying ways of context discovery and its integration into predictive analytics.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {383–388},
numpages = {6},
keywords = {advertising, user modeling, context},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487948,
author = {Myung, Jaeseok},
title = {A Proximity-Based Fallback Model for Hybrid Web Recommender Systems},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487948},
doi = {10.1145/2487788.2487948},
abstract = {Although there are numerous websites that provide recommendation services for various items such as movies, music, and books, most of studies on recommender systems only focus on one specific item type. As recommender sites expand to cover several types of items, though, it is important to build a hybrid web recommender system that can handle multiple types of items.The switch hybrid recommender model provides a solution to this problem by choosing an appropriate recommender system according to given selection criteria, thereby facilitating cross-domain recommendations supported by individual recommender systems. This paper seeks to answer the question of how to deal with situations where no appropriate recommender system exists to deal with a required type of item. In such cases, the switch model cannot generate recommendation results, leading to the need for a fallback model that can satisfy most users most of the time.Our fallback model exploits a graph-based proximity search, ranking every entity on the graph according to a given proximity measure. We study how to incorporate the fallback model into the switch model, and propose a general architecture and simple algorithms for implementing these ideas. Finally, we present the results of our research result and discuss remaining challenges and possibilities for future research.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {389–394},
numpages = {6},
keywords = {recommender systems, proximity search, fallback model},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487949,
author = {Saha Roy, Rishiraj},
title = {Analyzing Linguistic Structure of Web Search Queries},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487949},
doi = {10.1145/2487788.2487949},
abstract = {It is believed that Web search queries are becoming more structurally complex over time. However, there has been no systematic study that quantifies such characteristics. In this thesis, we propose that queries are evolving into a unique linguistic system. We demonstrate proof of this hypothesis by examining the structure of Web queries by applying well-established techniques from natural language understanding. Preliminary results of these experiments show quantitative and qualitative proof that queries are not just some form of text between random sequences of words and natural language - they have distinct properties of their own.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {395–400},
numpages = {6},
keywords = {query intent, word co-occurrence networks, query understanding, query segmentation, query structure},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487950,
author = {Yanardag Delul, Pinar},
title = {Understanding and Analysing Microblogs},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487950},
doi = {10.1145/2487788.2487950},
abstract = {Microblogging is a form of blogging where posts typically consist of short content such as quick comments, phrases, URLs, or media, like images and videos. Because of the fast and compact nature of microblogs, users have adopted them for novel purposes, including sharing personal updates, spreading breaking news, promoting political views, marketing and tracking real time events. Thus, finding relevant information sources out of the rapidly growing content is an essential task.In this paper, we study the problem of understanding and analysing microblogs. We present a novel 2-stage framework to find potentially relevant content by extracting topics from the tweets and by taking advantage of submodularity.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {401–406},
numpages = {6},
keywords = {submodularity, personalization, twitter, social media, topic models, recommendation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254932,
author = {Dietze, Stefan and d'Aquin, Mathieu and Gasevic, Dragan},
title = {Session Details: LILE'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254932},
doi = {10.1145/3254932},
abstract = {LILE2013 Welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {4},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254933,
author = {Dietze, Stefan},
title = {Session Details: LILE'13 Keynote Talk},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254933},
doi = {10.1145/3254933},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487953,
author = {Roffel, Sweitze},
title = {Linking Data in and Outside a Scientific Publishing House},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487953},
doi = {10.1145/2487788.2487953},
abstract = {Publishing has undergone many changes since the 1960's, often driven by rapid technological development. Technology impacts the creation and dissemination of knowledge only to a certain extent, and in this talk I'll try to give a publisher's perspective of some technological drivers impacting Academic publishing today, and how the many actors involved are learning to cooperate as well as compete in an increasingly distributed environment to better turn information into knowledge. Technically, organizationally, and with regard to shared standards and infrastructure.Publishing has been called many different things by many different people. A simple definition could be that publishing is 'organizing content', so the focus of this talk will be on Elsevier's current use of Linked Data &amp; Semantic technology in organizing scientific content, including some early lessons learned.This view from a publisher aims to help the discussion on how we can all contribute to better disseminate and promote the enormous creativity made through core research contributions.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {411–412},
numpages = {2},
keywords = {academic, knowledge management, ecosystem, linking data, electronic publishing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254934,
author = {Dietze, Stefan},
title = {Session Details: LILE'13 Session 1},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254934},
doi = {10.1145/3254934},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487955,
author = {Sarker, Farhana and Tiropanis, Thanassis and Davis, Hugh C},
title = {Exploring Student Predictive Model That Relies on Institutional Databases and Open Data Instead of Traditional Questionnaires},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487955},
doi = {10.1145/2487788.2487955},
abstract = {Research in student retention and progression to completion is traditionally survey-based, where researchers collect data through questionnaires and interviewing students. The major issues with survey-based study are the potentially low response rates and cost. Nevertheless, a large number of datasets that could inform the questions that students are explicitly asked in surveys is commonly available in the external open datasets. This paper describes a new student predictive model for student progression that relies on the data available in institutional internal databases and external open data, without the need for surveys. The results of empirical study for undergraduate students in their first year of study shows that this model can perform as well as or even out-perform traditional survey-based ones.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {413–418},
numpages = {6},
keywords = {logistic regression, predictive models, progression to completion, categorical principal component analysis, institutional internal/external data sources, linked data, student retention, survey/questionnaire data},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487956,
author = {Taibi, Davide and Fetahu, Besnik and Dietze, Stefan},
title = {Towards Integration of Web Data into a Coherent Educational Data Graph},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487956},
doi = {10.1145/2487788.2487956},
abstract = {Personalisation, adaptation and recommendation are central aims of Technology Enhanced Learning (TEL) environments. In this context, information retrieval and clustering techniques are more and more often applied to filter and deliver learning resources according to user preferences and requirements. However, the suitability and scope of possible recommendations is fundamentally dependent on the available data, such as metadata about learning resources as well as users. However, quantity and quality of both is still limited. On the other hand, throughout the last years, the Linked Data (LD) movement has succeeded to provide a vast body of well-interlinked and publicly accessible Web data. This in particular includes Linked Data of explicit or implicit educational nature. In this paper, we propose a large-scale educational dataset which has been generated by exploiting Linked Data methods together with clustering and interlinking techniques to extract import and interlink a wide range of educationally relevant data. We also introduce a set of reusable techniques which were developed to realise scalable integration and alignment of Web data in educational settings.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {419–424},
numpages = {6},
keywords = {tel, semantic web, recommender system, linked data},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487957,
author = {Siehndel, Patrick and Kawase, Ricardo and Hadgu, Asmelash Teka and Herder, Eelco},
title = {Finding Relevant Missing References in Learning Courses},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487957},
doi = {10.1145/2487788.2487957},
abstract = {Reference sites play an increasingly important role in learning processes. Teachers use these sites in order to identify topics that should be covered by a course or a lecture. Learners visit online encyclopedias and dictionaries to find alternative explanations of concepts, to learn more about a topic, or to better understand the context of a concept. Ideally, a course or lecture should cover all key concepts of the topic that it encompasses, but often time constraints prevent complete coverage. In this paper, we propose an approach to identify missing references and key concepts in a corpus of educational lectures. For this purpose, we link concepts in educational material to the organizational and linking structure of Wikipedia. Identifying missing resources enables learners to improve their understanding of a topic, and allows teachers to investigate whether their learning material covers all necessary concepts.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {425–430},
numpages = {6},
keywords = {linked data, education, wikipedia},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254935,
author = {Dietze, Stefan},
title = {Session Details: LILE'13 Session 2},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254935},
doi = {10.1145/3254935},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487959,
author = {Mikroyannidis, Alexander and Domingue, John},
title = {Interactive Learning Resources and Linked Data for Online Scientific Experimentation},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487959},
doi = {10.1145/2487788.2487959},
abstract = {There is currently a huge potential for eLearning in several new online learning initiatives like Massive Open Online Courses (MOOCs) and Open Educational Resources (OERs). These initiatives enable learners to self-regulate their learning by providing them with an abundant amount of free learning materials of high quality. This paper presents FORGE, a new European initiative for online learning using Future Internet Research and Experimentation (FIRE) facilities. FORGE is a step towards turning FIRE into a pan-European educational platform for Future Internet through Linked Data. This will benefit learners and educators by giving them both access to world class facilities in order to carry out experiments on e.g. new internet protocols. In turn, this supports constructivist and self-regulated learning approaches, through the use of interactive learning resources, such as eBooks.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {431–434},
numpages = {4},
keywords = {open educational resources, interactive ebooks, widgets, linked data, self-regulated learning, massive open online courses},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487960,
author = {Damljanovic, Danica and Miller, David and O'Sullivan, Daniel},
title = {Learning from Quizzes Using Intelligent Learning Companions},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487960},
doi = {10.1145/2487788.2487960},
abstract = {It is widely recognised that engaging games can have a profound impact on learning. Integrating a conversational Artificial Intelligence (AI) into the mix makes the experience of learning even more engaging and enriching. In this paper we describe a conversational agent which is built with the purpose of acting as a personal tutor. The tutor can prompt, question, stimulate and guide a learner and then adapt exercises and challenges to specific needs. We illustrate how automatic generation of quizzes can be used to build learning exercises and activities.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {435–438},
numpages = {4},
keywords = {e-learning, ontologies, quizzes, linked data},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487961,
author = {H\"{o}ver, Kai Michael and M\"{u}hlh\"{a}user, Max},
title = {Linked Data Selectors},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487961},
doi = {10.1145/2487788.2487961},
abstract = {In the world of Linked Data, HTTP URIs are names. A URI is dereferenced to obtain a copy or description of the referred resource. If only a fragment of a resource should be referred, pointing to the whole resource is not sufficient. Therefore, it is necessary to be able to refer to fragments of resources, and to name them with URIs to interlink them in the Web of Data. This is especially helpful in the educational context where learning processes including discussion and social interaction demand for exact references and granular selections of media. This paper presents the specification of Linked Data Selectors, an OWL ontology for describing dereferenceable fragments of Web resources.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {439–444},
numpages = {6},
keywords = {anchors, e-learning, ontology, linked data},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487962,
author = {Kawase, Ricardo and Fisichella, Marco and Niemann, Katja and Pitsilis, Vassilis and Vidalis, Aristides and Holtkamp, Philipp and Nunes, Bernardo},
title = {OpenScout: Harvesting Business and Management Learning Objects from the Web of Data},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487962},
doi = {10.1145/2487788.2487962},
abstract = {Already existing open educational resources in the field of Business and Management have a high potential for enterprises to address the increasing training needs of their employees. However, it is difficult to act on OERs as some data is hidden. In the meanwhile, numerous repositories provide Linked Open Data on this field. Though, users have to search a number of repositories with heterogeneous interfaces in order to retrieve the desired content. In this paper, we present the strategies to gather heterogeneous learning objects from the Web of Data, and we provide an overview of the benefits of the OpenScout platform. Despite the fact that not all data repositories strictly follow Linked Data principles, OpenScout addressed individual variations in order to harvest, align, and provide a single end-point. In the end, OpenScout provides a full-fledged environment that leverages on the Linked Open Data available on the Web and additionally exposes it in an homogeneous format.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {445–450},
numpages = {6},
keywords = {linked data, metadata, open content, sharing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254936,
author = {Oomen, Johan and Troncy, Rapha\"{e}l and Mezaris, Vasileios},
title = {Session Details: LiME'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254936},
doi = {10.1145/3254936},
abstract = {LiME'13 Welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {3},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487965,
author = {Nixon, Lyndon},
title = {The Importance of Linked Media to the Future Web: Lime 2013 Keynote Talk -- a Proposal for the Linked Media Research Agenda},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487965},
doi = {10.1145/2487788.2487965},
abstract = {If the future Web will be able to fully leverage the scale and quality of online media, a Web scale layer of structured, interlinked media annotations is needed, which we will call Linked Media, inspired by the Linked Data movement for making structured, interlinked descriptions of resources better available online. Mobile and tablet devices, as well as connected TVs, introduce novel application domains that will benefit from broad understanding and acceptance of Linked Media standards. In the keynote, I will provide an overview of current practices and specification efforts in the domain of video and Web content integration, drawing from the LinkedTV1 and MediaMixer2 projects. From this, I will present a vision for a Linked Media layer on the future Web will can empower new media-centric applications in a world of ubiquitous online multimedia.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {455–456},
numpages = {2},
keywords = {web multimedia, hypervideo, media metadata, online media, linked media, hypermedia, media semantics, interactive video, media descriptions},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487967,
author = {Aly, Robin and Ordelman, Roeland J.F. and Eskevich, Maria and Jones, Gareth J.F. and Chen, Shu},
title = {Linking inside a Video Collection: What and How to Measure?},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487967},
doi = {10.1145/2487788.2487967},
abstract = {Although linking video to additional information sources seems to be a sensible approach to satisfy information needs of user, the perspective of users is not yet analyzed on a fundamental level in real-life scenarios. However, a better understanding of the motivation of users to follow links in video, which anchors users prefer to link from within a video, and what type of link targets users are typically interested in, is important to be able to model automatic linking of audiovisual content appropriately. In this paper we report on our methodology towards eliciting user requirements with respect to video linking in the course of a broader study on user requirements in searching and a series of benchmark evaluations on searching and linking.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {457–460},
numpages = {4},
keywords = {user study, video linking},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487968,
author = {Hildebrand, Michiel and Hardman, Lynda},
title = {Using Explicit Discourse Rules to Guide Video Enrichment},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487968},
doi = {10.1145/2487788.2487968},
abstract = {Video content analysis and named entity extraction are increasingly used to automatically generate content annotations for TV programs. A potential use of these annotations is to provide an entry point to background information that users can consume on a second screen. Automatic enrichments are, however, meaningless when it is unclear to the user what they can do with them and why they would. We propose to contextualize the annotations by an explicit representation of discourse in the form of scene templates. Through content rules these templates are populated with the relevant annotations. We illustrate this idea with an example video and annotations generated in the LinkedTV1 project.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {461–464},
numpages = {4},
keywords = {video annotation, scene templates, second screen, discourse, rule-based},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487969,
author = {Leroy, Julien and Rocca, Fran\c{c}ois and Mancas, Matei and Gosselin, Bernard},
title = {Second Screen Interaction: An Approach to Infer Tv Watcher's Interest Using 3d Head Pose Estimation},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487969},
doi = {10.1145/2487788.2487969},
abstract = {In this paper, we present our "work-in-progress" approach to implicitly track user interaction and infer the interest a user can have for TV media. The aim is to identify moments of attentive focus, noninvasively and continuously, to dynamicaly improve the user profile by detecting which annotated media have drawn the user attention. Our method is based on the detection and estimation of face pose in 3D using a consumer depth camera. This allows us to determine when a user is or not looking at his television. This study is realized in the scenario of second screen interaction (tablet, smartphone), a behavior that has become common for spectators. We present our progress on the system and its integration in the LinkedTV project.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {465–468},
numpages = {4},
keywords = {second screen interaction, attention, head pose estimation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487970,
author = {Li, Yunjia and Rizzo, Giuseppe and Redondo Garc\'{\i}a, Jos\'{e} Luis and Troncy, Rapha\"{e}l and Wald, Mike and Wills, Gary},
title = {Enriching Media Fragments with Named Entities for Video Classification},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487970},
doi = {10.1145/2487788.2487970},
abstract = {With the steady increase of videos published on media sharing platforms such as Dailymotion and YouTube, more and more efforts are spent to automatically annotate and organize these videos. In this paper, we propose a framework for classifying video items using both textual features such as named entities extracted from subtitles, and temporal features such as the duration of the media fragments where particular entities are spotted. We implement four automatic machine learning algorithms for multiclass classification problems, namely Logistic Regression (LG), K-Nearest Neighbour (KNN), Naive Bayes (NB) and Support Vector Machine (SVM). We study the temporal distribution patterns of named entities extracted from 805 Dailymotion videos. The results show that the best performance using the entity distribution is obtained with KNN (overall accuracy of 46.58%) while the best performance using the temporal distribution of named entities for each type is obtained with SVM (overall accuracy of 43.60%). We conclude that this approach is promising for automatically classifying online videos.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {469–476},
numpages = {8},
keywords = {nerd, video classification, named entity extraction, media annotation, concept extraction, media fragment},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487972,
author = {M\'{e}dini, Lionel and B\^{a}cle, Florian and Nguyen, Hoang Duy Tan},
title = {DataConf: Enriching Conference Publications with a Mobile Mashup Application},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487972},
doi = {10.1145/2487788.2487972},
abstract = {This paper describes a mobile Web application that allows browsing conference publications, their authors, authors' organizations, and even authors' other publications or publications related to the same keywords. It queries a main SPARQL endpoint that serves the conference metadata set, as well as other endpoints to enrich and explore data. It provides extra functions, such as flashing a publication QR code from the Web browser, accessing external resources about the publications, and it can be linked to external Web services. This application exploits the Linked Data paradigm and performs client-side reasoning. It follows recent W3C technical advances and as a mashup, requires few server resources. It can easily be deployed for any conference with available metadata on the Web.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {477–478},
numpages = {2},
keywords = {publication browsing, mobile web, mobile reasoning, mashup, linked data},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487973,
author = {Oehme, Philipp and Krug, Michael and Wiedemann, Fabian and Gaedke, Martin},
title = {The Chrooma+ Approach to Enrich Video Content Using HTML5},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487973},
doi = {10.1145/2487788.2487973},
abstract = {The Internet has become an important source for media content. Content types are not limited to text and pictures but also include video and audio. Currently audiovisual media is presented as it is. However, these media do not integrate the huge amount of related information, which is available on the Web. In this paper we present the Chrooma+ approach to improve the user experience of media consumption by enriching media content with additional information from various sources in the Web. Our approach focuses on the aggregation and combination of this related information with audiovisual media. This approach involves using new HTML5 technologies and with WebVTT a new annotation format to display relevant information at definite times. Some of the advantages of this approach are the usage of a rich annotation format and extensibility to include heterogeneous information sources.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {479–480},
numpages = {2},
keywords = {media enrichment, mashup, streaming, video mashup, video streaming, media aggregation, html5, widgets},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487974,
author = {Oomen, Johan and Tzouvaras, Vassilis and Hyyppa\"{a}, Kati},
title = {Linking and Visualizing Television Heritage: The EUscreen Virtual Exhibitions and the Linked Open Data Pilot},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487974},
doi = {10.1145/2487788.2487974},
abstract = {The EUscreen initiative represents the European television archives and acts as a domain aggregator for Europeana, Europe's digital library, which provides access to over 20 million digitized cultural objects. The main motivation for the initiative is to provide unified access to a representative collection of television programs, secondary sources and articles, and in this way to allow students, scholars and the general public to study the history of television in its wider context. This paper explores the EUscreen activities related to novel ways to present curated content and publishing EUscreen metadata as Linked Open Data.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {481–484},
numpages = {4},
keywords = {visualization, linked open data, tv on the web, interoperability, metadata, linked media, europeana},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254937,
author = {He, Qi and Tian, Yuanyuan and Tong, Hanghang and McPherson, John and Konopnicki, David and Sun, Jimeng and Appel, Ana Paula},
title = {Session Details: LSNA'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254937},
doi = {10.1145/3254937},
abstract = {LSNA'13 Welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {2},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487977,
author = {Baeza-Yates, Ricardo and Saez-Trumper, Diego},
title = {Online Social Networks: Beyond Popularity},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487977},
doi = {10.1145/2487788.2487977},
abstract = {One of the main differences between traditional Web analysis and online Social Networks (OSNs) studies, is that in the first case the information is organized around content, while in the second case it is organized around people. While search engines have done a good job finding relevant content across billions of pages, nowadays we do not have an equivalent tool to find relevant people in OSNs. Even though an impressive amount of research has been done in this direction, there are still a lot of gaps to cover. Although the first intuition could be (and was!) search for popular people, previous research have shown that users' in-degree (e.g. number of friends or followers) is important but not enough to represent the importance and reputation of a person. Another approach is to study the content of the messages exchanged between users, trying to identify topical experts. However the computational cost of such approach - including language diversity - is a big limitation. In our work we take a content-agnostic approach, focusing in frequency, type, and time properties of user actions rather than content, mixing their static characteristics (social graph) and their activities (dynamic graphs). Our goal is to understand the role of popular users in OSNs, and also find "hidden important users": do popular users create new trends and cascades? Do they add value to the network? And, if they don't, who does it? Our research provides preliminary answers for these questions.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {489–490},
numpages = {2},
keywords = {influence, social networks, information networks, web advertising},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487978,
author = {Dasgupta, Anirban},
title = {Aggregating Information from the Crowd and the Network},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487978},
doi = {10.1145/2487788.2487978},
abstract = {In social systems, information often exists in a dispersed manner, as individual opinions, local insights and preferences. In order to make a global decision however, we need to be able to aggregate such local pieces of information into a global description of the system. Such information aggregation problems are key in setting up crowdsourcing or human computation systems. How do we formally build and analyze such information aggregation systems? In this talk we will discuss three different vignettes based on the particular information aggregation problem and the "social system" that we are extracting the information from.In our first result, we will analyze a crowdsourcing system consisting of a set of users and binary choice questions. Each user has a specific reliability that determines the user's error rate in answering the questions. We show how to give an unsupervised algorithm for aggregating the user answers in order to simultaneously derive the user expertise as well as the truth values of the questions.Our second result will deal with the case when there is an interacting user community on a question answer forum. User preferences of quality are now expressed in terms of ("best answer" and "thumbs up/down") votes cast on each other's content. We will analyze a set of possible factors that indicate bias in user voting behavior - these factors encompass different gaming behavior, as well as other eccentricities. We address the problem of aggregating user preferences (votes) using a supervised machine learning framework to calibrate such votes. We will see that this supervised learning method of content-agnostic vote calibration can significantly improve the performance of answer ranking and expert ranking.The last part of the talk will describe how it is possible to exploit local insights that users have about their friends in order to improve the efficiency of surveying in a (networked) population. We will describe the notion of "social sampling", where participants in a poll respond with a summary of their friends' putative responses to the poll. The analysis of social sampling leads to novel trade-off questions: the savings in the number of samples(roughly the average size of neighborhood of participants) vs. the systematic bias in the poll due to the network structure. We show bounds on the variances of few such estimators - experiments on real world networks show this to be a useful paradigm in obtaining accurate information with small number of samples.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {491–492},
numpages = {2},
keywords = {crowdsourcing, information aggregation, networks},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487979,
author = {de Paula, Rog\'{e}rio},
title = {The Social Meanings of Social Networks: Integrating SNA and Ethnography of Social Networking},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487979},
doi = {10.1145/2487788.2487979},
abstract = {In this talk, I examine the manifest, emic meanings of social networking in the context of social network analysis and it uses this to discuss how the confluence of social science and computational sociology can contribute to a richer understanding of how emerging social technologies shape and are shaped by people's everyday practices.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {493–494},
numpages = {2},
keywords = {social network analysis, ethnography, computational socilogy, ethno-mining},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487980,
author = {Faloutsos, Michalis},
title = {Detecting Malware with Graph-Based Methods: Traffic Classification, Botnets, and Facebook Scams},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487980},
doi = {10.1145/2487788.2487980},
abstract = {In this talk, we highlight two topics on security from our lab. First, we address the problem of Internet traffic classification (e.g. web, filesharing, or botnet?). We present a fundamentally different approach to classifying traffic that studies the network wide behavior by modeling the interactions of users as a graph. By contrast, most previous approaches use statistics such as packet sizes and inter-packet delays. We show how our approach gives rise to novel and powerful ways to: (a) visualize the traffic, (b) model the behavior of applications, and (c) detect abnormalities and attacks. Extending this approach, we develop ENTELECHEIA, a botnet-detection method. Tests with real data suggests that our graph-based approach is very promising.Second, we present, MyPageKeeper, a security Facebook app, with 13K downloads, which we deployed to: (a) quantify the presence of malware on Facebook, and (b) protect end-users. We designed MyPageKeeper in a way that strikes the balance between accuracy and scalability. Our initial results are scary and interesting: (a) malware is widespread, with 49% of our users are exposed to at least one malicious post from a friend, and (b) roughly 74% of all malicious posts contain links that point back to Facebook, and thus would evade any of the current web-based filtering approaches.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {495–496},
numpages = {2},
keywords = {socware, traffic classification, botnets, Malware detection},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487981,
author = {Guy, Ido},
title = {Mining and Analyzing the Enterprise Knowledge Graph},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487981},
doi = {10.1145/2487788.2487981},
abstract = {Today's enterprises hold ever-growing amounts of public data, stemming from different organizational systems, such as development environments, CRM systems, business intelligence systems, and enterprise social media. This data unlocks rich and diverse information about entities, people, terms, and the relationships among them. A lot of insight can be gained through analyzing this knowledge graph, both by individual employees and by the organization as a whole. In this talk, I will review recent work done by the Social Technologies &amp; Analytics group at IBM Research-Haifa to mine these relationships, represent them in a generalized model, and use the model for different aims within the enterprise, including social search [5], expertise location [1], social recommendation [2, 3], and network analysis [4].},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {497–498},
numpages = {2},
keywords = {social media, knowledge graph, social business, enterprise},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487982,
author = {Ugander, Johan},
title = {Scaling Graph Computations at Facebook},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487982},
doi = {10.1145/2487788.2487982},
abstract = {With over a billion nodes and hundreds of billions of edges, scalability is at the forefront of concerns when dealing with the Facebook social graph. This talk will focus on two recent advances in graph computations at Facebook. The first focus concerns the development of a novel graph sharding algorithm - Balanced Label Propagation - for load-balancing distributed graph computations. Using Balanced Label Propagation, we were able to reduce by 50% the query time of Facebook's 'People You May Know' service, the realtime distributed system responsible for the feature extraction and ranking of the friends-of-friends of all active Facebook users. The second focus concerns the 2011 computation of the average distance distribution between all active Facebook users. This computation, which produced an average distance of 4.74, was made possible by two recent computational advances: Hyper-ANF, a modern probabilistic algorithm for computing distance distributions, and Layered Label Propagation, a modern compression scheme suited for social graphs. The details of how this computation was coordinated will be described. The talk describes joint work with Lars Backstrom, Paolo Boldi, Marco Rosa, and Sebastiano Vigna.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {499–500},
numpages = {2},
keywords = {social networks, degrees of separation, graph partitioning},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487984,
author = {Bao, Nguyen Thien and Suzumura, Toyotaro},
title = {Towards Highly Scalable Pregel-Based Graph Processing Platform with X10},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487984},
doi = {10.1145/2487788.2487984},
abstract = {Many practical computing problems concern large graph. Standard problems include web graph analysis and social networks analysis like Facebook, Twitter. The scale of these graph poses challenge to their efficient processing. To efficiently process large-scale graph, we create X-Pregel, a graph processing system based on Google's Computing Pregel model [1], by using the state-of-the-art PGAS programming language X10. We do not purely implement Google Pregel by using X10 language, but we also introduce two new features that do not exists in the original model to optimize the performance: (1) an optimization to reduce the number of messages which is exchanged among workers, (2) a dynamic re-partitioning scheme that effectively reassign vertices to different workers during the computation. Our performance evaluation demonstrates that our optimization method of sending messages achieves up to 200% speed up on Pagerank by reducing the network I/O to 10 times in comparison with the default method of sending messages when processing SCALE20 Kronecker graph [2](vertices = 1,048,576, edges = 33,554,432). It also demonstrates that our system processes large graph faster than prior implementation of Pregel such as GPS [3](stands for graph processing system) and Giraph [4].},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {501–508},
numpages = {8},
keywords = {distributed computing, graph analysis system, parallel graph processing system},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487985,
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
title = {A First View of Exedra: A Domain-Specific Language for Large Graph Analytics Workflows},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487985},
doi = {10.1145/2487788.2487985},
abstract = {In recent years, many programming models, software libraries, and middleware have appeared for processing large graphs of various forms. However, there exists a significant usability gap between the graph analysis scientists, and High Performance Computing (HPC) application programmers due to the complexity of HPC graph analysis software. In this paper we provide a basic view of Exedra, a domain-specific language (DSL) for large graph analysis in which we aim to eliminate the aforementioned complexities. Exedra consists of high level language constructs for specifying different graph analysis tasks on distributed environments. We implemented Exedra DSL on a scalable graph analysis platform called Dipper. Dipper uses Igraph/R interface for creating graph analysis workflows which in turn gets translated to Exedra statements. Exedra statements are interpreted by Dipper interpreter, and gets mapped to user specified libraries/middleware. Exedra DSL allows for synthesize of graph algorithms that are more efficient compared to bare use of graph libraries while maintaining a standard interface that could use even future graph analysis software. We evaluated Exedra's feasibility for expressing graph analysis tasks by running Dipper on a cluster of four nodes. We observed that Dipper has the ability of reducing the time taken for graph analysis when the workflow was distributed on all four nodes despite the communication, and data format conversion overhead of the Dipper framework.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {509–516},
numpages = {8},
keywords = {workflow, large graph data analysis, programming techniques, exascale, program synthesis, domain-specific language},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487986,
author = {Nunes, Santiago A. and Romani, Luciana A.S. and Avila, Ana M.H. and Coltri, Priscila P. and Traina, Caetano and Cordeiro, Robson L.F. and de Sousa, Elaine P.M. and Traina, Agma J.M.},
title = {Analysis of Large Scale Climate Data: How Well Climate Change Models and Data from Real Sensor Networks Agree?},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487986},
doi = {10.1145/2487788.2487986},
abstract = {Research on global warming and climate changes has attracted a huge attention of the scientific community and of the media in general, mainly due to the social and economic impacts they pose over the entire planet. Climate change simulation models have been developed and improved to provide reliable data, which are employed to forecast effects of increasing emissions of greenhouse gases on a future global climate. The data generated by each model simulation amount to Terabytes of data, and demand fast and scalable methods to process them. In this context, we propose a new process of analysis aimed at discriminating between the temporal behavior of the data generated by climate models and the real climate observations gathered from ground-based meteorological station networks. Our approach combines fractal data analysis and the monitoring of real and model-generated data streams to detect deviations on the intrinsic correlation among the time series defined by different climate variables. Our measurements were made using series from a regional climate model and the corresponding real data from a network of sensors from meteorological stations existing in the analyzed region. The results show that our approach can correctly discriminate the data either as real or as simulated, even when statistical tests fail. Those results suggest that there is still room for improvement of the state-of-the-art climate change models, and that the fractal-based concepts may contribute for their improvement, besides being a fast, parallelizable, and scalable approach.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {517–526},
numpages = {10},
keywords = {fractal analysis, data streams, sensor networks, climate data},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487987,
author = {\v{S}ubelj, Lovro and Bajec, Marko},
title = {Model of Complex Networks Based on Citation Dynamics},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487987},
doi = {10.1145/2487788.2487987},
abstract = {Complex networks of real-world systems are believed to be controlled by common phenomena, producing structures far from regular or random. These include scale-free degree distributions, small-world structure and assortative mixing by degree, which are also the properties captured by different random graph models proposed in the literature. However, many (non-social) real-world networks are in fact disassortative by degree. Thus, we here propose a simple evolving model that generates networks with most common properties of real-world networks including degree disassortativity. Furthermore, the model has a natural interpretation for citation networks with different practical applications.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {527–530},
numpages = {4},
keywords = {clustering, citation networks, complex networks, degree mixing, graph models},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487988,
author = {Watanabe, Masaru and Suzumura, Toyotaro},
title = {How Social Network is Evolving? A Preliminary Study on Billion-Scale Twitter Network},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487988},
doi = {10.1145/2487788.2487988},
abstract = {Recently, social network services such as Twitter, Facebook, MySpace, LinkedIn have been remarkably growing. There are various studies about social networks analysis. Haewoon Kwak performed the analysis of the Twitter network on 2009 and shows the degree of separation. However, the number of users on 2009 is about 41.7 million, the graph scale is not very large compared with the current graph. In this paper, we conduct a Twitter network analysis in terms growth by region, scale-free, reciprocity, degree of separation and diameter using Twitter user data with 469.9 million users and 28.7 billion relationships. We report that the value of degree of separation is 4.59 in current Twitter network through our experiments.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {531–534},
numpages = {4},
keywords = {reciprocity, social network analysis, degree of separation, diameter, degree distribution, twitter},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254938,
author = {Cambria, Erik and Xia, Yunqing and Howard, Newton},
title = {Session Details: MABSDA'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254938},
doi = {10.1145/3254938},
abstract = {MABSDA'13 Welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {2},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487991,
author = {White, Bebo},
title = {The Web as a Laboratory},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487991},
doi = {10.1145/2487788.2487991},
abstract = {Insights from Web Science and Big Data Analysis have led many researchers to the conclusion that the Web not only represents an almost unlimited data store but also a remarkable multi-disciplinary laboratory environment. A new challenge is how to best leverage the potential of this experimental space. What are the procedures for defining, implementing and evaluating "Web-scale" experiments? What are acceptable measures of robustness and repeatability? What are the opportunities for experimental collaboration? What disciplines are likely to benefit from this new research model? The Web Laboratory model provides an exciting new and fertile model for future research.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {539–540},
numpages = {2},
keywords = {analytics, security, web science, big data, crowdsourcing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487992,
author = {Ohsawa, Shohei and Matsuo, Yutaka},
title = {Like Prediction: Modeling like Counts by Bridging Facebook Pages with Linked Data},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487992},
doi = {10.1145/2487788.2487992},
abstract = {Recent growth of social media has produced a new market for branding of people and businesses. Facebook provides Facebook Pages (Pages in short) for public figures and businesses (we call entities) to communicate with their fans through a Like button. Because Like counts sometimes reflect the popularity of entities, techniques to increase the Like count can be a matter of interest, and might be known as social media marketing. From an academic perspective, Like counts of Pages depend not only on the popularity of the entity, but also on the popularity of semantically related entities. For example, Lady Gaga's Page has many Likes; her song "Poker Face" does too. We can infer that her next song will acquire many Likes immediately. Important questions are these: How does the Like count of Lady Gaga affect the Like count of her song? Alternatively, how does the Like count of her song constitute some fraction of the Like count of Lady Gaga herself?As described in this paper, we strive to reveal the mutual influences of Like counts among semantically related entities. To measure the influence of related entities, we propose a problem called the Like prediction problem (LPP). It models Like counts of a given entity using information of related entities. The semantic relations among entities, expressed as RDF predicates, are obtained by linking each Page with the most similar DBpedia entity. Using the model learned by support vector regression (SVR) on LPP, we can estimate the Like count of a new entity e.g., Lady Gaga's new song. More importantly, we can analyze which RDF predicates are important to infer Like counts, providing a mutual influence network among entities. Our study comprises three parts: (1) crawling the Pages and their Like counts, (2) linking Pages to DBpedia, and (3) constructing features to solve the LPP. Our study, based on 20 million Pages with 30 billion Likes, is the largest-scale study of Facebook Likes ever reported. This research constitutes a new attempt to integrate unstructured emotional data such as Likes, with Linked data, and to provide new insights for branding with social media.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {541–548},
numpages = {8},
keywords = {feature construction, facebook, entity linking, linked data, link-based prediction},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487993,
author = {Hong, Yoonsung and Kwak, Haewoon and Baek, Youngmin and Moon, Sue},
title = {Tower of Babel: A Crowdsourcing Game Building Sentiment Lexicons for Resource-Scarce Languages},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487993},
doi = {10.1145/2487788.2487993},
abstract = {With the growing amount of textual data produced by online social media today, the demands for sentiment analysis are also rapidly increasing; and, this is true for worldwide. However, non-English languages often lack sentiment lexicons, a core resource in performing sentiment analysis. Our solution, Tower of Babel (ToB), is a language-independent sentiment-lexicon-generating crowdsourcing game. We conducted an experiment with 135 participants to explore the difference between our solution and a conventional manual annotation method. We evaluated ToB in terms of effectiveness, efficiency, and satisfactions. Based on the result of the evaluation, we conclude that sentiment classification via ToB is accurate, productive and enjoyable.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {549–556},
numpages = {8},
keywords = {online games, world wide web, sentiment labeling, lexicon construction, distributed knowledge acquisition},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487994,
author = {Gindl, Stefan and Weichselbraun, Albert and Scharl, Arno},
title = {Rule-Based Opinion Target and Aspect Extraction to Acquire Affective Knowledge},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487994},
doi = {10.1145/2487788.2487994},
abstract = {Opinion holder and opinion target extraction are among the most popular and challenging problems tackled by opinion mining researchers, recognizing the significant business value of such components and their importance for applications such as media monitoring and Web intelligence. This paper describes an approach that combines opinion target extraction with aspect extraction using syntactic patterns. It expands previous work limited by sentence boundaries and includes a heuristic for anaphora resolution to identify targets across sentences. Furthermore, it demonstrates the application of concepts known from research on open information extraction to the identification of relevant opinion aspects. Qualitative analyses performed on a corpus of 100,000 Amazon product reviews show that the approach is promising. The extracted opinion targets and aspects are useful for enriching common knowledge resources and opinion mining ontologies, and support practitioners and researchers to identify opinions in document collections.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {557–564},
numpages = {8},
keywords = {opinion aspect extraction, opinion target extraction, opinion mining},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487995,
author = {Rajagopal, Dheeraj and Cambria, Erik and Olsher, Daniel and Kwok, Kenneth},
title = {A Graph-Based Approach to Commonsense Concept Extraction and Semantic Similarity Detection},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487995},
doi = {10.1145/2487788.2487995},
abstract = {Commonsense knowledge representation and reasoning support a wide variety of potential applications in fields such as document auto-categorization, Web search enhancement, topic gisting, social process modeling, and concept-level opinion and sentiment analysis. Solutions to these problems, however, demand robust knowledge bases capable of supporting flexible, nuanced reasoning. Populating such knowledge bases is highly time-consuming, making it necessary to develop techniques for deconstructing natural language texts into commonsense concepts. In this work, we propose an approach for effective multi-word commonsense expression extraction from unrestricted English text, in addition to a semantic similarity detection technique allowing additional matches to be found for specific concepts not already present in knowledge bases.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {565–570},
numpages = {6},
keywords = {natural language processing, ai, commonsense knowledge representation and reasoning, semantic similarity},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487996,
author = {Montejo-R\'{a}ez, Arturo and D\'{\i}az-Galiano, Manuel Carlos and Perea-Ortega, Jos\'{e} Manuel and Ure\~{n}a-L\'{o}pez, Luis Alfonso},
title = {Spanish Knowledge Base Generation for Polarity Classification from Masses},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487996},
doi = {10.1145/2487788.2487996},
abstract = {This work presents a novel method for the generation of a knowledge base oriented to Sentiment Analysis from the continuous stream of published micro-blogs in social media services like Twitter. The method is simple in its approach and has shown to be effective compared to other knowledge based methods for Polarity Classification. Due to independence from language, the method has been tested on different Spanish corpora, with a minimal effort in the lexical resources involved. Although for two of the three studied corpora the obtained results did not improve those officially obtained on the same corpora, it should be noted that this is an unsupervised approach and the accuracy levels achieved were close to those levels obtained with well-known supervised algorithms.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {571–578},
numpages = {8},
keywords = {sentiment analysis, polarity classification, social media, knowledge-base generation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2487997,
author = {Saad, Farag and Mathiak, Brigitte},
title = {Revised Mutual Information Approach for German Text Sentiment Classification},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487997},
doi = {10.1145/2487788.2487997},
abstract = {The significant increase in content of online social media such as product reviews, blogs, forums etc., have led to an increasing attention to sentiment analysis tools and approaches that make use of mining this substantially growing content. The aim of this paper is to develop a robust classification approach of customer reviews based on a self-annotated domain-specific corpus by applying a statistical approach i.e., mutual information. First, subjective words in each test sentence are identified. Second, ambiguous adjectives such as high, low, large, many etc., are disambiguated based on their accompanying noun using a conditional mutual information approach. Third, a mutual information approach is applied to find the sentiment orientation (polarity) of the identified subjective words based on analyzing their statistical relationship with the manually annotated sentiment labels within a sizeable sentiment training data. Fourth, since negation plays a significant role in flipping the sentiment polarity of an identified sentiment word, we estimate the role of negation in affecting the classification accuracy. Finally, the identified polarity for each test sentence is evaluated against experts' annotation.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {579–586},
numpages = {8},
keywords = {sentiment analysis, mutual information, disambiguation, negation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254939,
author = {Rowe, Matthew and Stankovic, Milan and Dadzie, Aba-Sah},
title = {Session Details: MSM'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254939},
doi = {10.1145/3254939},
abstract = {MSM'13 Welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {4},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254940,
author = {Dadzie, Aba-Sah},
title = {Session Details: MSM'13 Keynote Talk},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254940},
doi = {10.1145/3254940},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488000,
author = {Quercia, Daniele},
title = {Urban: Crowdsourcing for the Good of London},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488000},
doi = {10.1145/2487788.2488000},
abstract = {For the last few years, we have been studying existing social media sites and created new ones in the context of London. By combining what Twitter users in a variety of London neighborhoods talk about with census data, we showed that neighborhood deprivation was associated (positively and negatively) with use of emotion words (sentiment) [2] and with specific topics [5]. Users in more deprived neighborhoods tweeted about wedding parties, matters expressed in Spanish/Portuguese, and celebrity gossips. By contrast, those in less deprived neighborhoods tweeted about vacations, professional use of social media, environmental issues, sports, and health issues. Also, upon data about 76 million London underground and overground rail journeys, we found that people from deprived areas visited both other deprived areas and prosperous areas, while residents of better-off communities tended to only visit other privileged neighborhoods - suggesting a geographic segregation effect [1, 6]. More recently, we created and launched two crowdsourcing websites. First, we launched urbanopticon.org, which extracts Londoners' mental images of the city. By testing which places are remarkable and unmistakable and which places represent faceless sprawl, we were able to draw the recognizability map of London. We found that areas with low recognizability did not fare any worse on the economic indicators of income, education, and employment, but they did significantly suffer from social problems of housing deprivation, poor living conditions, and crime [4]. Second, we launched urbangems.org. This crowdsources visual perceptions of quiet, beauty and happiness across the city using Google Street View pictures.The aim is to identify the visual cues that are generally associated with concepts difficult to define such beauty, happiness, quietness, or even deprivation. By using state-of-the-art image processing techniques, we determined the visual cues that make a place appear beautiful, quiet, and happy [3]: the amount of greenery was the most positively associated visual cue with each of three qualities; by contrast, broad streets, fortress-like buildings, and council houses tended to be negatively associated. These two sites offer the ability to conduct specific urban sociological experiments at scale. More generally, this line of work is at the crossroad of two emerging themes in computing research - a crossroad where "web science" meets the "smart city" agenda.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {591–592},
numpages = {2},
keywords = {web science, social media, urban informatics},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254941,
author = {Dadzie, Aba-Sah},
title = {Session Details: MSM'13 Machine Learning &amp; Statistical Analysis},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254941},
doi = {10.1145/3254941},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488002,
author = {Godin, Fr\'{e}deric and Slavkovikj, Viktor and De Neve, Wesley and Schrauwen, Benjamin and Van de Walle, Rik},
title = {Using Topic Models for Twitter Hashtag Recommendation},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488002},
doi = {10.1145/2487788.2488002},
abstract = {Since the introduction of microblogging services, there has been a continuous growth of short-text social networking on the Internet. With the generation of large amounts of microposts, there is a need for effective categorization and search of the data. Twitter, one of the largest microblogging sites, allows users to make use of hashtags to categorize their posts. However, the majority of tweets do not contain tags, which hinders the quality of the search results. In this paper, we propose a novel method for unsupervised and content-based hashtag recommendation for tweets. Our approach relies on Latent Dirichlet Allocation (LDA) to model the underlying topic assignment of language classified tweets. The advantage of our approach is the use of a topic distribution to recommend general hashtags.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {593–596},
numpages = {4},
keywords = {hashtag prediction, twitter, short-text classification, microposts, topic models},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488003,
author = {de Oliveira, Diego Marinho and Laender, Alberto H.F. and Veloso, Adriano and da Silva, Altigran S.},
title = {FS-NER: A Lightweight Filter-Stream Approach to Named Entity Recognition on Twitter Data},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488003},
doi = {10.1145/2487788.2488003},
abstract = {Microblog platforms such as Twitter are being increasingly adopted by Web users, yielding an important source of data for web search and mining applications. Tasks such as Named Entity Recognition are at the core of many of these applications, but the effectiveness of existing tools is seriously compromised when applied to Twitter data, since messages are terse, poorly worded and posted in many different languages. Also, Twitter follows a streaming paradigm, imposing that entities must be recognized in real-time. In view of these challenges and the inappropriateness of existing tools, we propose a novel approach for Named Entity Recognition on Twitter data called FS-NER (Filter-Stream Named Entity Recognition). FS-NER is characterized by the use of filters that process unlabeled Twitter messages, being much more practical than existing supervised CRF-based approaches. Such filters can be combined either in sequence or in parallel in a flexible way. Moreover, because these filters are not language dependent, FS-NER can be applied to different languages without requiring a laborious adaptation. Through a systematic evaluation using three Twitter collections and considering seven types of entity, we show that FS-NER performs 3% better than a CRF-based baseline, besides being orders of magnitude faster and much more practical.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {597–604},
numpages = {8},
keywords = {crf, fs-ner, named entity recognition, twitter},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254942,
author = {Dadzie, Aba-Sah},
title = {Session Details: MSM'13 Trend &amp; Topic Detection in Microposts},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254942},
doi = {10.1145/3254942},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488005,
author = {Uren, Victoria and Dadzie, Aba-Sah},
title = {Nerding out on Twitter: Fun, Patriotism and #curiosity},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488005},
doi = {10.1145/2487788.2488005},
abstract = {This paper presents an analysis of tweets collected over six days before, during and after the landing of the Mars Science Laboratory, known as Curiosity, in the Gale Crater on the 6th of August 2012. A sociological application of web science is demonstrated by use of parallel coordinate visualization as part of a mixed methods study. The results show strong, predominantly positive, international interest in the event. Scientific details dominated the stream, but, following the successful landing, other themes emerged such as fun, and national pride.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {605–612},
numpages = {8},
keywords = {high-dimensional visualization, parallel coordinates visualization, public engagement with science, web science},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488006,
author = {Parikh, Ruchi and Karlapalem, Kamalakar},
title = {ET: Events from Tweets},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488006},
doi = {10.1145/2487788.2488006},
abstract = {Social media sites such as Twitter and Facebook have emerged as popular tools for people to express their opinions on various topics. The large amount of data provided by these media is extremely valuable for mining trending topics and events. In this paper, we build an efficient, scalable system to detect events from tweets (ET). Our approach detects events by exploring their textual and temporal components. ET does not require any target entity or domain knowledge to be specified; it automatically detects events from a set of tweets. The key components of ET are (1) an extraction scheme for event representative keywords, (2) an efficient storage mechanism to store their appearance patterns, and (3) a hierarchical clustering technique based on the common co-occurring features of keywords. The events are determined through the hierarchical clustering process. We evaluate our system on two data-sets; one is provided by VAST challenge 2011, and the other published by US based users in January 2013. Our results show that we are able to detect events of relevance efficiently.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {613–620},
numpages = {8},
keywords = {event detection, text analytics, data mining, hierarchical clustering, tweets processing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254943,
author = {Dadzie, Aba-Sah},
title = {Session Details: MSM'13 Filtering &amp; Cassification of Microposts},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254943},
doi = {10.1145/3254943},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488008,
author = {Posch, Lisa and Wagner, Claudia and Singer, Philipp and Strohmaier, Markus},
title = {Meaning as Collective Use: Predicting Semantic Hashtag Categories on Twitter},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488008},
doi = {10.1145/2487788.2488008},
abstract = {This paper sets out to explore whether data about the usage of hashtags on Twitter contains information about their semantics. Towards that end, we perform initial statistical hypothesis tests to quantify the association between usage patterns and semantics of hashtags. To assess the utility of pragmatic features - which describe how a hashtag is used over time - for semantic analysis of hashtags, we conduct various hashtag stream classification experiments and compare their utility with the utility of lexical features. Our results indicate that pragmatic features indeed contain valuable information for classifying hashtags into semantic categories. Although pragmatic features do not outperform lexical features in our experiments, we argue that pragmatic features are important and relevant for settings in which textual information might be sparse or absent (e.g., in social video streams).},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {621–628},
numpages = {8},
keywords = {twitter, hashtags, semantics, social structure},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488009,
author = {Hollerit, Bernd and Kr\"{o}ll, Mark and Strohmaier, Markus},
title = {Towards Linking Buyers and Sellers: Detecting Commercial Intent on Twitter},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488009},
doi = {10.1145/2487788.2488009},
abstract = {Since more and more people use the micro-blogging platform Twitter to convey their needs and desires, it has become a particularly interesting medium for the task of identifying commercial activities. Potential buyers and sellers can be contacted directly thereby opening up novel perspectives and economic possibilities. By detecting commercial intent in tweets, this work is considered a first step to bring together buyers and sellers. In this work, we present an automatic method for detecting commercial intent in tweets where we achieve reasonable precision 57% and recall 77% scores. In addition, we provide insights into the nature and characteristics of tweets exhibiting commercial intent thereby contributing to our understanding of how people express commercial activities on Twitter.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {629–632},
numpages = {4},
keywords = {commercial intent, knowledge acquisition, twitter},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254944,
author = {Dadzie, Aba-Sah},
title = {Session Details: MSM'13 Posters &amp; Demonstrations},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254944},
doi = {10.1145/3254944},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488011,
author = {Dahimene, Ryadh and du Mouza, C\'{e}dric},
title = {MicroFilter: Real Time Filtering of Microblogging Content},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488011},
doi = {10.1145/2487788.2488011},
abstract = {Microblogging systems have become a major trend over the Web. After only 7 years of existence, Twitter for instance claims more than 500 million users with more than 350 billion delivered update each day. As a consequence the user must today manage possibly extremely large feeds, resulting in poor data readability and loss of valuable information and the system must face a huge network load. In this demonstration, we present and illustrate the features of MicroFilter (MF in the the following), an inverted list-based filtering engine that nicely extends existing centralized microblogging systems by adding a real-time filtering feature. The demonstration proposed illustrates how the user experience is improved, the impact on the traffic for the overall system, and how the characteristics of microblogs drove the design of the indexing structures.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {633–634},
numpages = {2},
keywords = {microblogging, twitter, filtering},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488012,
author = {Vanin, Aline A. and Freitas, Larissa A. and Vieira, Renata and Bochernitsan, Marco},
title = {Some Clues on Irony Detection in Tweets},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488012},
doi = {10.1145/2487788.2488012},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {635–636},
numpages = {2},
keywords = {irony detection, twitter, microposts},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254945,
author = {Hacid, Hakim and Guo, Shengbo and Vakali, Athena},
title = {Session Details: MSND'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254945},
doi = {10.1145/3254945},
abstract = {MSND'13 Welcome},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {2},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488015,
author = {Aggarwal, Anupama and Almeida, Jussara and Kumaraguru, Ponnurangam},
title = {Detection of Spam Tipping Behaviour on Foursquare},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488015},
doi = {10.1145/2487788.2488015},
abstract = {In Foursquare, one of the currently most popular online location based social networking sites (LBSNs), users may not only check-in at specific venues but also post comments (or tips), sharing their opinions and previous experiences at the corresponding physical places. Foursquare tips, which are visible to everyone, provide venue owners with valuable user feedback besides helping other users to make an opinion about the specific venue. However, they have been the target of spamming activity by users who exploit this feature to spread tips with unrelated content.In this paper, we present what, to our knowledge, is the first effort to identify and analyze different patterns of tip spamming activity in Foursquare, with the goal of developing automatic tools to detect users who post spam tips - tip spammers. A manual investigation of a real dataset collected from Foursquare led us to identify four categories of spamming behavior, viz. Advertising/Spam, Self-promotion, Abusive and Malicious. We then applied machine learning techniques, jointly with a selected set of user, social and tip's content features associated with each user, to develop automatic detection tools. Our experimental results indicate that we are able to not only correctly distinguish legitimate users from tip spammers with high accuracy (89.76%) but also correctly identify a large fraction (at least 78.88%) of spammers in each identified category.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {641–648},
numpages = {8},
keywords = {tip spam, social networks, user behaviour, location-based social networks},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488016,
author = {Alves, Bruno Leite and Benevenuto, Fabr\'{\i}cio and Laender, Alberto H.F.},
title = {The Role of Research Leaders on the Evolution of Scientific Communities},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488016},
doi = {10.1145/2487788.2488016},
abstract = {There have been considerable efforts in the literature towards understanding and modeling dynamic aspects of scientific communities. Despite the great interest, little is known about the role that different members play in the formation of the underlying network structure of such communities. In this paper, we provide a wide investigation of the roles that members of the core of scientific communities play in the collaboration network structure formation and evolution. To do that, we define a community core based on an individual metric, core score, which is an h-index derived metric that captures both, the prolificness and the involvement of researchers in a community. Our results provide a number of key observations related to community formation and evolving patterns. Particularly, we show that members of the community core work as bridges that connect smaller clustered research groups. Furthermore, these members are responsible for an increase in the average degree of the whole community underlying network and a decrease on the overall network assortativeness. More important, we note that variations on the members of the community core tend to be strongly correlated with variations on these metrics. We argue that our observations are important for shedding a light on the role of key members on community formation and structure.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {649–656},
numpages = {8},
keywords = {community evolution, scientific communities, core community},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488017,
author = {Jenders, Maximilian and Kasneci, Gjergji and Naumann, Felix},
title = {Analyzing and Predicting Viral Tweets},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488017},
doi = {10.1145/2487788.2488017},
abstract = {Twitter and other microblogging services have become indispensable sources of information in today's web. Understanding the main factors that make certain pieces of information spread quickly in these platforms can be decisive for the analysis of opinion formation and many other opinion mining tasks.This paper addresses important questions concerning the spread of information on Twitter. What makes Twitter users retweet a tweet? Is it possible to predict whether a tweet will become "viral", i.e., will be frequently retweeted? To answer these questions we provide an extensive analysis of a wide range of tweet and user features regarding their influence on the spread of tweets. The most impactful features are chosen to build a learning model that predicts viral tweets with high accuracy. All experiments are performed on a real-world dataset, extracted through a public Twitter API based on user IDs from the TREC 2011 microblog corpus.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {657–664},
numpages = {8},
keywords = {spread, microblog, twitter, retweet, analysis, tweet, model, prediction},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488018,
author = {Ju, Jeongin and Park, Hosung and Moon, Sue},
title = {Resolving Homonymy with Correlation Clustering in Scholarly Digital Libraries},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488018},
doi = {10.1145/2487788.2488018},
abstract = {As scholarly data increases rapidly, scholarly digital libraries, supplying publication data through convenient online interfaces, become popular and important tools for researchers. Researchers use SDLs for various purposes, including searching the publications of an author, assessing one's impact by the citations, and identifying one's research topics. However, common names among authors cause difficulties in correctly identifying one's works among a large number of scholarly publications. Abbreviated first and middle names make it even harder to identify and distinguish authors with the same representation (i.e. spelling) of names. Several disambiguation methods have solved the problem under their own assumptions. The assumptions are usually that inputs such as the number of same-named authors, training sets, or rich and clear information about papers are given. Considering the size of scholarship records today and their inconsistent formats, we expect their assumptions be very hard to be met. We use common assumption that coauthors are likely to write more than one paper together and propose an unsupervised approach to group papers from the same author only using the most common information, author lists. We represent each paper as a point in an author name space, take dimension reduction to find author names shown frequently together in papers, and cluster papers with vector similarity measure well fitted for name disambiguation task. The main advantage of our approach is to use only coauthor information as input. We evaluate our method using publication records collected from DBLP, and show that our approach results in better disambiguation compared to other five clustering methods in terms of cluster purity and fragmentation.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {665–672},
numpages = {8},
keywords = {correlation clustering, name disambiguation, scholarly digital libraries},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488019,
author = {Velichety, Srikar and Ram, Sudha},
title = {Examining Lists on Twitter to Uncover Relationships between Following, Membership and Subscription},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488019},
doi = {10.1145/2487788.2488019},
abstract = {We report on an exploratory analysis of pairwise relationships between three different forms of information consumption on Twitter viz., following, listing and subscribing. We develop a systematic framework to examine the relationships between these three forms. Using our framework, we conducted an empirical analysis of a dataset from Twitter. Our results show that people not only consume information by explicitly following others, but also by listing and subscribing to lists and that the people they list or subscribe to are not the same as the ones they follow. Our work has implications for understanding information propagation and diffusion via Twitter and for generating recommendations for adding users to lists, subscribing and merging or splitting them.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {673–676},
numpages = {4},
keywords = {twitter, subscription, membership, lists, descriptive modeling},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254946,
author = {Kostkova, Patty and Paolotti, Daniela and Brownstein, John},
title = {Session Details: PHDA'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254946},
doi = {10.1145/3254946},
abstract = {PHDA'13 Welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {2},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488022,
author = {Oliveira, Allisson D. and Cabral, Giordano and L\'{o}pez, D. and Firmo, Caetano and Serrat, F. Zarzuela and Albuquerque, J.},
title = {A Proposal for Automatic Diagnosis of Malaria: Extended Abstract},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488022},
doi = {10.1145/2487788.2488022},
abstract = {This paper presents a methodology for automatic diagnosis of malaria using computer vision techniques combined with artificial intelligence. We had obtained an accuracy rate of 74% in the detection system.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {681–682},
numpages = {2},
keywords = {computer vision techniques, artificial intelligence, malaria, haar, detecting malaria, automatic diagnostic},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488023,
author = {Brien, Stephanie and Naderi, Nona and Shaban-Nejad, Arash and Mondor, Luke and Kroemker, Doerthe and Buckeridge, David L.},
title = {Vaccine Attitude Surveillance Using Semantic Analysis: Constructing a Semantically Annotated Corpus},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488023},
doi = {10.1145/2487788.2488023},
abstract = {This paper reports work in progress to semantically annotate blog posts about vaccines to use in the Vaccine Attitude Surveillance using Semantic Analysis (VASSA) framework. The VASSA framework combines semantic web and natural language processing (NLP) tools and techniques to provide a coherent semantic layer across online social media for assessment and analysis of vaccination attitudes and beliefs. We describe how the blog posts were sampled and selected, our schema to semantically annotate concepts defined in our ontology, details of the annotation process, and inter-annotator agreement on a sample of blog posts.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {683–686},
numpages = {4},
keywords = {social network, vaccine sentiment, ontologies, semantic analysis},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488024,
author = {Kostkova, Patty},
title = {A Roadmap to Integrated Digital Public Health Surveillance: The Vision and the Challenges},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488024},
doi = {10.1145/2487788.2488024},
abstract = {The exponentially increasing stream of real time big data produced by Web 2.0 Internet and mobile networks created radically new interdisciplinary challenges for public health and computer science. Traditional public health disease surveillance systems have to utilize the potential created by new situation-aware realtime signals from social media, mobile/sensor networks and citizens? participatory surveillance systems providing invaluable free realtime event-based signals for epidemic intelligence. However, rather than improving existing isolated systems, an integrated solution bringing together existing epidemic intelligence systems scanning news media (e.g., GPHIN, MedISys) with real-time social media intelligence (e.g., Twitter, participatory systems) is required to substantially improve and automate early warning, outbreak detection and preparedness operations. However, automatic monitoring and novel verification methods for these multichannel event-based real time signals has to be integrated with traditional case-based surveillance systems from microbiological laboratories and clinical reporting. Finally, the system needs effectively support coordination of epidemiological teams, risk communication with citizens and implementation of prevention measures.However, from computational perspective, signal detection, analysis and verification of very high noise realtime big data provide a number of interdisciplinary challenges for computer science. Novel approaches integrating current systems into a digital public health dashboard can enhance signal verification methods and automate the processes assisting public health experts in providing better informed and more timely response. In this paper, we describe the roadmap to such a system, components of an integrated public health surveillance services and computing challenges to be resolved to create an integrated real world solution.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {687–694},
numpages = {8},
keywords = {outbreak detection, risk communication, epidemic intelligence},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488025,
author = {Johansson, Michael and Wojcik, Oktawia and Chunara, Rumi and Smolinski, Mark and Brownstein, John},
title = {Participatory Disease Surveillance in Latin America},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488025},
doi = {10.1145/2487788.2488025},
abstract = {Participatory disease surveillance systems are dynamic, sensitive, and accurate. They also offer an opportunity to directly connect the public to public health. Implementing them in Latin America requires targeting multiple acute febrile illnesses, designing a system that is appropriate and scalable, and developing local strategies for encouraging participation.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {695–696},
numpages = {2},
keywords = {participatory surveillance, dengue, public health, influenza, puerto rico, acute febrile illness},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488026,
author = {Mantilla-Beniers, Natalia Barbara and Rodriguez-Ramirez, Rocio and Stephens, Christopher Rhodes},
title = {Crowdsourced Risk Factors of Influenza-like-Illness in Mexico},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488026},
doi = {10.1145/2487788.2488026},
abstract = {Monitoring of influenza like illnesses (ILI) using the Internet has become more common since its beginnings nearly a decade ago. The initial project of Der Grote Griep Meting was launched in 2003 in the Netherlands and Belgium. It was designed as a means of engaging people in matters of scientific and public health importance, and indeed attracted participation from over 30,000 people in its first year. Its success thus gathered a wealth of potentially valuable epidemiological data complementary to those obtained through the established disease surveillance networks, and linked to rich background information on each participant. Since then, there has been an accelerated increase in the number of countries hosting similar websites, and many of these have generated rather promising resultsIn this talk, an analysis of the data from the Mexican monitoring website, "Reporta" is presented, and the risk factors that are linked to reporting of ILI symptoms among its participants are determined and analyzed. The data base gathered from the launching of Reporta in May 2009 to September 2011 is used for this purpose. The definition of suspect ILI case employed by the Mexican Health Ministry is applied to distinguish a class C of participants; the traits gathered in the background questionnaire are labeled Xi. Risk associated to any given trait Xi is evaluated by considering the difference between the frequency with which C occurs among participants with trait Xi and in the general population. This difference is then normalized to assess its statistical significanceInterestingly, while some of the results confirm the suspected importance of certain traits indicative of enhanced susceptibility or a large contact network, others are unexpected and must be interpreted within an adequate framework. Thus, a taxonomy of background traits is proposed to aid interpretation, and tested through a new assessment of the associated risks. This work illustrates a way in which Internet-based monitoring can contribute to our understanding of disease spread.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {697–698},
numpages = {2},
keywords = {monitoring, epidemiology, data mining, influenza-like-illness, ili, risk factor, respiratory diseases, modeling, crowdsourcing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488027,
author = {Bodnar, Todd and Salath\'{e}, Marcel},
title = {Validating Models for Disease Detection Using Twitter},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488027},
doi = {10.1145/2487788.2488027},
abstract = {Data mining social media has become a valuable resource for infectious disease surveillance. However, there are considerable risks associated with incorrectly predicting an epidemic. The large amount of social media data combined with the small amount of ground truth data and the general dynamics of infectious diseases present unique challenges when evaluating model performance. In this paper, we look at several methods that have been used to assess influenza prevalence using Twitter. We then validate them with tests that are designed to avoid and illustrate issues with the standard k-fold cross validation method. We also find that small modifications to the way that data are partitioned can have major effects on a model's reported performance.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {699–702},
numpages = {4},
keywords = {machine learning, twitter, data mining, regression},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488028,
author = {van der Goot, Erik and Tanev, Hristo and Linge, Jens P.},
title = {Combining Twitter and Media Reports on Public Health Events in Medisys},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488028},
doi = {10.1145/2487788.2488028},
abstract = {We describe the harvesting and subsequent analysis of tweets that are linked to media reports on public health events in order to identify which Internet resources are being referred to in these tweets. The aim was to automatically detect resources that are traditionally not considered mainstream media, but play a role in the discussion of public health events on the Internet. Interestingly, our initial evaluation of the results showed that most references related to public health events lead to traditional news media sites, even though URLs to non-traditional media receive a higher rank. We will briefly describe the Medical Information System (MedISys) and the methodology used to obtain and analyse tweets.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {703–718},
numpages = {16},
keywords = {event-based surveillance, epidemic intelligence, medisys, twitter analysis},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254947,
author = {Kumaraguru, Ponnurangam and Almeida, Virgilio},
title = {Session Details: PSOM'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254947},
doi = {10.1145/3254947},
abstract = {PSOM'13 Welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {3},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488032,
author = {Cheng, Yuan and Park, Jaehong and Sandhu, Ravi},
title = {Preserving User Privacy from Third-Party Applications in Online Social Networks},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488032},
doi = {10.1145/2487788.2488032},
abstract = {Online social networks (OSNs) facilitate many third-party applications (TPAs) that offer users additional functionality and services. However, they also pose serious user privacy risk as current OSNs provide little control over disclosure of user data to TPAs. Addressing the privacy and security issues related to TPAs (and the underlying social networking platforms) requires solutions beyond a simple all-or-nothing strategy. In this paper, we outline an access control framework that provides users flexible controls over how TPAs can access user data and activities in OSNs while still retaining the functionality of TPAs. The proposed framework specifically allows TPAs to utilize some private data without actually transmitting this data to TPAs. Our approach determines access from TPAs based on user-specified policies in terms of relationships between the user and the application.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {723–728},
numpages = {6},
keywords = {privacy, social applications, online social networks},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488033,
author = {Gupta, Aditi and Lamba, Hemank and Kumaraguru, Ponnurangam and Joshi, Anupam},
title = {Faking Sandy: Characterizing and Identifying Fake Images on Twitter during Hurricane Sandy},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488033},
doi = {10.1145/2487788.2488033},
abstract = {In today's world, online social media plays a vital role during real world events, especially crisis events. There are both positive and negative effects of social media coverage of events, it can be used by authorities for effective disaster management or by malicious entities to spread rumors and fake news. The aim of this paper, is to highlight the role of Twitter, during Hurricane Sandy (2012) to spread fake images about the disaster. We identified 10,350 unique tweets containing fake images that were circulated on Twitter, during Hurricane Sandy. We performed a characterization analysis, to understand the temporal, social reputation and influence patterns for the spread of fake images. Eighty six percent of tweets spreading the fake images were retweets, hence very few were original tweets. Our results showed that top thirty users out of 10,215 users (0.3%) resulted in 90% of the retweets of fake images; also network links such as follower relationships of Twitter, contributed very less (only 11%) to the spread of these fake photos URLs. Next, we used classification models, to distinguish fake images from real images of Hurricane Sandy. Best results were obtained from Decision Tree classifier, we got 97% accuracy in predicting fake images from real. Also, tweet based features were very effective in distinguishing fake images tweets from real, while the performance of user based features was very poor. Our results, showed that, automated techniques can be used in identifying real images from fake images posted on Twitter.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {729–736},
numpages = {8},
keywords = {online social networks},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488034,
author = {Halevi, Tzipora and Lewis, James and Memon, Nasir},
title = {A Pilot Study of Cyber Security and Privacy Related Behavior and Personality Traits},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488034},
doi = {10.1145/2487788.2488034},
abstract = {Recent research has begun to focus on the factors that cause people to respond to phishing attacks as well as affect user behavior on social networks. This study examines the correlation between the Big Five personality traits and email phishing response. Another aspect examined is how these factors relate to users' tendency to share information and protect their privacy on Facebook (which is one of the most popular social networking sites).This research shows that when using a prize phishing email, neuroticism is the factor most correlated to responding to this email, in addition to a gender-based difference in the response. This study also found that people who score high on the openness factor tend to both post more information on Facebook as well as have less strict privacy settings, which may cause them to be susceptible to privacy attacks. In addition, this work detected no correlation between the participants estimate of being vulnerable to phishing attacks and actually being phished, which suggests susceptibility to phishing is not due to lack of awareness of the phishing risks and that real-time response to phishing is hard to predict in advance by online users.The goal of this study is to better understand the traits that contribute to online vulnerability, for the purpose of developing customized user interfaces and secure awareness education, designed to increase users' privacy and security in the future.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {737–744},
numpages = {8},
keywords = {privacy, facebook, personality traits, phishing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488035,
author = {Edwards, Lilian and Matwyshyn, Andrea M.},
title = {Twitter (R)Evolution: Privacy, Free Speech and Disclosure},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488035},
doi = {10.1145/2487788.2488035},
abstract = {Using Twitter as a case study, this paper sets forth the legal tensions faced by social networks that seek to defend privacy interests of users. Recent EC and UN initiatives have begun to suggest an increased role for corporations as protectors of human rights. But, as yet, binding rather than voluntary obligations of this kind under international human rights law seem either non-existent or highly conflicted, and structural limitations to such a shift may currently exist under both U.S. and UK law. Companies do not face decisions regarding disclosure in a vacuum, rather they face them constrained by existing obligations under (sometimes conflicting) legal demands. Yet, companies such as Twitter are well-positioned to be advocates for consumers' interests in these legal debates. Using several recent corporate disclosure decisions regarding user identity as illustration, this paper places questions of privacy, free speech and disclosure in broader legal context. More scholarship is needed on the mechanics of how online intermediaries, especially social media, manage their position as crucial speech platforms in democratic as well as less democratic regimes.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {745–750},
numpages = {6},
keywords = {social networks, privacy, pseudonymity, anonymity, twitter},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488036,
author = {Parwani, Tarun and Kholoussi, Ramin and Karras, Panagiotis},
title = {How to Hack into Facebook without Being a Hacker},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488036},
doi = {10.1145/2487788.2488036},
abstract = {The proliferation of online social networking services has aroused privacy concerns among the general public. The focus of such concerns has typically revolved around providing explicit privacy guarantees to users and letting users take control of the privacy-threatening aspects of their online behavior, so as to ensure that private personal information and materials are not made available to other parties and not used for unintended purposes without the user's consent. As such protective features are usually opt-in, users have to explicitly opt-in for them in order to avoid compromising their privacy. Besides, third-party applications may acquire a user's personal information, but only after they have been granted consent by the user. If we also consider potential network security attacks that intercept or misdirect a user's online communication, it would appear that the discussion of user vulnerability has accurately delimited the ways in which a user may be exposed to privacy threats.In this paper, we expose and discuss a previously unconsidered avenue by which a user's privacy can be gravely exposed. Using this exploit, we were able to gain complete access to some popular online social network accounts without using any conventional method like phishing, brute force, or trojans. Our attack merely involves a legitimate exploitation of the vulnerability created by the existence of obsolete web-based email addresses. We present the results of an experimental study on the spread that such an attack can reach, and the ethical dilemmas we faced in the process. Last, we outline our suggestions for defense mechanisms that can be employed to enhance online security and thwart the kind of attacks that we expose.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {751–754},
numpages = {4},
keywords = {identity, online social networking, facebook, brute force, media, phishing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488037,
author = {Ur, Blase and Wang, Yang},
title = {A Cross-Cultural Framework for Protecting User Privacy in Online Social Media},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488037},
doi = {10.1145/2487788.2488037},
abstract = {Social media has become truly global in recent years. We argue that support for users' privacy, however, has not been extended equally to all users from around the world. In this paper, we survey existing literature on cross-cultural privacy issues, giving particular weight to work specific to online social networking sites. We then propose a framework for evaluating the extent to which social networking sites' privacy options are offered and communicated in a manner that supports diverse users from around the world. One aspect of our framework focuses on cultural issues, such as norms regarding the use of pseudonyms or posting of photographs. A second aspect of our framework discusses legal issues in cross-cultural privacy, including data-protection requirements and questions of jurisdiction. The final part of our framework delves into user expectations regarding the data-sharing practices and the communication of privacy information. The framework can enable service providers to identify potential gaps in support for user privacy. It can also help researchers, regulators, or consumer advocates reason systematically about cultural differences related to privacy in social media.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {755–762},
numpages = {8},
keywords = {privacy, social media, culture, cross-cultural, social networks},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488038,
author = {Wang, Yang and Leon, Pedro Giovanni and Scott, Kevin and Chen, Xiaoxuan and Acquisti, Alessandro and Cranor, Lorrie Faith},
title = {Privacy Nudges for Social Media: An Exploratory Facebook Study},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488038},
doi = {10.1145/2487788.2488038},
abstract = {Anecdotal evidence and scholarly research have shown that a significant portion of Internet users experience regrets over their online disclosures. To help individuals avoid regrettable online disclosures, we employed lessons from behavioral decision research and research on soft paternalism to design mechanisms that "nudge" users to consider the content and context of their online disclosures before posting them. We developed three such privacy nudges on Facebook. The first nudge provides visual cues about the audience for a post. The second nudge introduces time delays before a post is published. The third nudge gives users feedback about their posts. We tested the nudges in a three-week exploratory field trial with 21 Facebook users, and conducted 13 follow-up interviews. Our system logs, results from exit surveys, and interviews suggest that privacy nudges could be a promising way to prevent unintended disclosure. We discuss limitations of the current nudge designs and future directions for improvement.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {763–770},
numpages = {8},
keywords = {facebook, nudge, privacy, online disclosure, soft paternalism},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254948,
author = {Zubiaga, Arkaitz and Spina, Damiano and de Rijke, Maarten and Strohmaier, Markus},
title = {Session Details: RAMSS'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254948},
doi = {10.1145/3254948},
abstract = {RAMSS'13 Welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {3},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488041,
author = {Sarukkai, Ramesh R.},
title = {Real-Time User Modeling and Prediction: Examples from Youtube},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488041},
doi = {10.1145/2487788.2488041},
abstract = {Real-time analysis and modeling of users for improving engagement, and interaction is a burgeoning area of interest with applications to web sites, social networks and mobile applications. Apart from scalability issues, this domain poses a number of modeling and algorithmic challenges. In this talk, as an illustrative example, we present DAL, a system that leverages real-time user activity/signals for dynamic ad loads, and designed to improve the overall user experience on YouTube. This system uses machine learning to optimize for user activity during a visit and helps decide on real-time advertising policies dynamically for the user. We conclude the talk with challenges and opportunities in this important area of real-time user analysis and social modeling.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {775–776},
numpages = {2},
keywords = {optimization, user modeling, advertising, real-time analysis},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488042,
author = {De Francisci Morales, Gianmarco},
title = {SAMOA: A Platform for Mining Big Data Streams},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488042},
doi = {10.1145/2487788.2488042},
abstract = {Social media and user generated content are causing an ever growing data deluge. The rate at which we produce data is growing steadily, thus creating larger and larger streams of continuously evolving data. Online news, micro-blogs, search queries are just a few examples of these continuous streams of user activities. The value of these streams relies in their freshness and relatedness to ongoing events. However, current (de-facto standard) solutions for big data analysis are not designed to deal with evolving streams.In this talk, we offer a sneak preview of SAMOA, an upcoming platform for mining dig data streams. SAMOA is a platform for online mining in a cluster/cloud environment. It features a pluggable architecture that allows it to run on several distributed stream processing engines such as S4 and Storm. SAMOA includes algorithms for the most common machine learning tasks such as classification and clustering. Finally, SAMOA will soon be open sourced in order to foster collaboration and research on big data stream mining.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {777–778},
numpages = {2},
keywords = {stream mining, big data, machine learning, open source, data streams, distributed computing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488044,
author = {Diaz-Aviles, Ernesto and Nejdl, Wolfgang and Drumond, Lucas and Schmidt-Thieme, Lars},
title = {Towards Real-Time Collaborative Filtering for Big Fast Data},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488044},
doi = {10.1145/2487788.2488044},
abstract = {The Web of people is highly dynamic and the life experiences between our on-line and "real-world" interactions are increasingly interconnected. For example, users engaged in the Social Web more and more rely upon continuous social streams for real-time access to information and fresh knowledge about current affairs. However, given the deluge of data items, it is a challenge for individuals to find relevant and appropriately ranked information at the right time. Having Twitter as test bed, we tackle this information overload problem by following an online collaborative approach. That is, we go beyond the general perspective of information finding in Twitter, that asks: "What is happening right now?", towards an individual user perspective, and ask: "What is interesting to me right now within the social media stream?". In this paper, we review our recently proposed online collaborative filtering algorithms and outline potential research directions.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {779–780},
numpages = {2},
keywords = {online ranking, twitter, collaborative filtering},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488045,
author = {Zhang, Lumin and Jia, Yan and Zhou, Bin and Han, Yi},
title = {Detecting Real-Time Burst Topics in Microblog Streams: How Sentiment Can Help},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488045},
doi = {10.1145/2487788.2488045},
abstract = {Microblog has become an increasing valuable resource of up-to-date topics about what is happening in the world. In this paper, we propose a novel approach of detecting real-time events in microblog streams based on bursty sentiments detection. Instead of traditional sentiment orientation like positive, negative and neutral, we use sentiment vector as our sentiment model to abstract subjective messages which are then used to detect bursts and clustered into new events. Experimental evaluations show that our approach could perform effectively for online event detection. Although we worked with Chinese in our research, the technique can be used with any other language.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {781–782},
numpages = {2},
keywords = {burst, microblog, event detection, sentiment vector},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488046,
author = {Abhik, Dhekar and Toshniwal, Durga},
title = {Sub-Event Detection during Natural Hazards Using Features of Social Media Data},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488046},
doi = {10.1145/2487788.2488046},
abstract = {Social networking sites such as Flickr, YouTube, Facebook, etc. contain a huge amount of user-contributed data for a variety of real-world events. These events can be some natural calamities such as earthquakes, floods, forest fires, etc. or some man-made hazards like riots. This work focuses on getting better knowledge about a natural hazard event using the data available from social networking sites. Rescue and relief activities in emergency situations can be enhanced by identifying sub-events of a particular event. Traditional topic discovery techniques used for event identification in news data cannot be used for social media data because social network data may be unstructured. To address this problem the features or metadata associated with social media data can be exploited. These features can be user-provided annotations (e.g., title, description) and automatically generated information (e.g., content creation time). Considerable improvement in performance is observed by using multiple features of social media data for sub-event detection rather than using individual feature. Proposed here is a two-step process. In the first step, clusters are formed from social network data using relevant features individually. Based on the significance of features weights are assigned to them. And in the second step all the clustering solutions formed in the first step are combined in a principal weighted manner to give the final clustering solution. Each cluster represents a sub-event for a particular natural hazard.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {783–788},
numpages = {6},
keywords = {natural-hazards, social-media, emergency-situation awareness, sub-event detection},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488048,
author = {Troncy, Rapha\"{e}l and Milicic, Vuk and Rizzo, Giuseppe and Garc\'{\i}a, Jos\'{e} Luis Redondo},
title = {MediaFinder: Collect, Enrich and Visualize Media Memes Shared by the Crowd},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488048},
doi = {10.1145/2487788.2488048},
abstract = {Social networks play an increasingly important role for sharing media items related to human's activities, feelings, emotions and conversations opening a window to the world in real-time. However, these images and videos are spread over multiple social networks. In this paper, we first describe a so-called media server that collect recent images and videos which can be potentially attached to an event. These media items can then be used for the automatic generation of visual summaries. However, making sense out of the resulting media galleries is an extremely challenging task. We present a framework that leverages on: (i) visual features from media items for near-deduplication and (ii) textual features from status updates to enrich, cluster and generate storyboards. A prototype is publicly available at http://mediafinder.eurecom.fr.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {789–790},
numpages = {2},
keywords = {visual summarization, topic generation, storytelling, social media, storyboard identification},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488049,
author = {Steiner, Thomas and van Hooland, Seth and Summers, Ed},
title = {MJ No More: Using Concurrent Wikipedia Edit Spikes with Social Network Plausibility Checks for Breaking News Detection},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488049},
doi = {10.1145/2487788.2488049},
abstract = {We have developed an application called Wikipedia Live Monitor that monitors article edits on different language versions of Wikipedia--as they happen in realtime. Wikipedia articles in different languages are highly interlinked. For example, the English article "en:2013_Russian_meteor_event" on the topic of the February 15 meteoroid that exploded over the region of Chelyabinsk Oblast, Russia, is interlinked with "ru:ПaДehne_meteopnta_ha_Ypajie_B_2013_roДy?, the Russian article on the same topic. As we monitor multiple language versions of Wikipedia in parallel, we can exploit this fact to detect concurrent edit spikes of Wikipedia articles covering the same topics, both in only one, and in different languages. We treat such concurrent edit spikes as signals for potential breaking news events, whose plausibility we then check with full-text cross-language searches on multiple social networks. Unlike the reverse approach of monitoring social networks first, and potentially checking plausibility on Wikipedia second, the approach proposed in this paper has the advantage of being less prone to false-positive alerts, while being equally sensitive to true-positive events, however, at only a fraction of the processing cost. A live demo of our application is available online at the URL http://wikipedia-irc.herokuapp.com/, the source code is available under the terms of the Apache 2.0 license at https://github.com/tomayac/wikipedia-irc.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {791–794},
numpages = {4},
keywords = {breaking news detection, internet relay chat, event detection, wikipedia, social networks},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488050,
author = {Milajevs, Dmitrijs and Bouma, Gosse},
title = {Real Time Discussion Retrieval from Twitter},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488050},
doi = {10.1145/2487788.2488050},
abstract = {While social media receive a lot of attention from the scientific community in general, there is little work on high recall retrieval of messages relevant to a discussion. Hash tag based search is widely used for data retrieval from social media. This work shows limitations of this approach, because the majority of the relevant messages do not even contain any hash tag, and unpredictable hash tags are used as the conversation evolves in time. To overcome these limitations, we propose an alternative retrieval method. Given an input stream of messages as an example of the discussion, our method extracts the most relevant words from it and queries the social network for more messages with these words. Our method filters messages that do not belong to the discussion using an LDA topic model. We demonstrate this concept on manually built collections of tweets about major sport and music events.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {795–800},
numpages = {6},
keywords = {discussion retrieval, social media, event data},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254949,
author = {Mohaisen, Abedelaziz and Ferretti, Stefano and Benevenuto, Fabricio},
title = {Session Details: SIMPLEX'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254949},
doi = {10.1145/3254949},
abstract = {SIMPLEX'13 Welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {2},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254950,
author = {Sastry, Nishanth},
title = {Session Details: SIMPLEX'13 Technical Session 1},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254950},
doi = {10.1145/3254950},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488053,
author = {Murai, Fabricio and Ribeiro, Bruno and Towsley, Donald and Gile, Krista},
title = {Characterizing Branching Processes from Sampled Data},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488053},
doi = {10.1145/2487788.2488053},
abstract = {Branching processes model the evolution of populations of agents that randomly generate offspring (children). These processes, more patently Galton-Watson processes, are widely used to model biological, social, cognitive, and technological phenomena, such as the diffusion of ideas, knowledge, chain letters, viruses, and the evolution of humans through their Y-chromosome DNA or mitochondrial RNA. A practical challenge of modeling real phenomena using a Galton-Watson process is the choice of the offspring distribution, which must be measured from the population. In most cases, however, directly measuring the offspring distribution is unrealistic due to lack of resources or the death of agents. So far, researchers have relied on informed guesses to guide their choice of offspring distribution. In this work we propose two methods to estimate the offspring distribution from real sampled data. Using a small sampled fraction of the agents and instrumented with the identity of the ancestors of the sampled agents, we show that accurate offspring distribution estimates can be obtained by sampling as little as 14% of the population.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {805–812},
numpages = {8},
keywords = {sampling and estimation, branching processes, graph characterization, mcmc},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488054,
author = {Ferretti, Stefano},
title = {Resilience of Dynamic Overlays through Local Interactions},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488054},
doi = {10.1145/2487788.2488054},
abstract = {This paper presents a self-organizing protocol for dynamic (unstructured P2P) overlay networks, which allows to react to the variability of node arrivals and departures. Through local interactions, the protocol avoids that the departure of nodes causes a partitioning of the overlay. We show that it is sufficient to have knowledge about 1st and 2nd neighbours, plus a simple interaction P2P protocol, to make unstructured networks resilient to node faults. A simulation assessment over different kinds of overlay networks demonstrates the viability of the proposal.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {813–820},
numpages = {8},
keywords = {simulation, complex networks, unstructured overlays},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488055,
author = {Guimar\~{a}es, Abra\~{a}o and Vieira, Alex B. and Silva, Ana Paula Couto and Ziviani, Artur},
title = {Fast Centrality-Driven Diffusion in Dynamic Networks},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488055},
doi = {10.1145/2487788.2488055},
abstract = {Diffusion processes in complex dynamic networks can arise, for instance, on data search, data routing, and information spreading. Therefore, understanding how to speed up the diffusion process is an important topic in the study of complex dynamic networks. In this paper, we shed light on how centrality measures and node dynamics coupled with simple diffusion models can help on accelerating the cover time in dynamic networks. Using data from systems with different characteristics, we show that if dynamics is disregarded, network cover time is highly underestimated. Moreover, using centrality accelerates the diffusion process over a different set of complex dynamic networks when compared with the random walk approach. For the best case, in order to cover 80% of nodes, fast centrality-driven diffusion reaches an improvement of 60%, i.e. when next-hop nodes are selected by using centrality measures. Additionally, we also propose and present the first results on how link prediction can help on speeding up the diffusion process in dynamic networks.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {821–828},
numpages = {8},
keywords = {complex networks, centrality, dynamic networks, diffusion processes},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488056,
author = {Mohaisen, Abedelaziz and Alrawi, Omar},
title = {Unveiling Zeus: Automated Classification of Malware Samples},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488056},
doi = {10.1145/2487788.2488056},
abstract = {Malware family classification is an age old problem that many Anti-Virus (AV) companies have tackled. There are two common techniques used for classification, signature based and behavior based. Signature based classification uses a common sequence of bytes that appears in the binary code to identify and detect a family of malware. Behavior based classification uses artifacts created by malware during execution for identification. In this paper we report on a unique dataset we obtained from our operations and classified using several machine learning techniques using the behavior-based approach. Our main class of malware we are interested in classifying is the popular Zeus malware. For its classification we identify 65 features that are unique and robust for identifying malware families. We show that artifacts like file system, registry, and network features can be used to identify distinct malware families with high accuracy - in some cases as high as 95 percent.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {829–832},
numpages = {4},
keywords = {automatic analysis, classification, malware},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254951,
author = {Benevenuto, Fabricio},
title = {Session Details: SIMPLEX'13 Technical Session 2},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254951},
doi = {10.1145/3254951},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488058,
author = {Brand\~{a}o, Michele A. and Moro, Mirella M. and Lopes, Giseli Rabello and Oliveira, Jos\'{e} P.M.},
title = {Using Link Semantics to Recommend Collaborations in Academic Social Networks},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488058},
doi = {10.1145/2487788.2488058},
abstract = {Social network analysis (SNA) has been explored in many contexts with different goals. Here, we use concepts from SNA for recommending collaborations in academic networks. Recent work shows that research groups with well connected academic networks tend to be more prolific. Hence, recommending collaborations is useful for increasing a group's connections, then boosting the group research as a collateral advantage. In this work, we propose two new metrics for recommending new collaborations or intensification of existing ones. Each metric considers a social principle (homophily and proximity) that is relevant within the academic context. The focus is to verify how these metrics influence in the resulting recommendations. We also propose new metrics for evaluating the recommendations based on social concepts (novelty, diversity and coverage) that have never been used for such a goal. Our experimental evaluation shows that considering our new metrics improves the quality of the recommendations when compared to the state-of-the-art.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {833–840},
numpages = {8},
keywords = {link prediction, social network, collaboration recommendation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488059,
author = {Gummadi, Krishna P.},
title = {Addressing the Privacy Management Crisis in Online Social Networks},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488059},
doi = {10.1145/2487788.2488059},
abstract = {The sharing of personal data has emerged as a popular activity over online social networking sites like Facebook. As a result, the issue of online social network privacy has received significant attention in both the research literature and the mainstream media. Our overarching goal is to improve defaults and provide better tools for managing privacy, but we are limited by the fact that the full extent of the privacy problem remains unknown; there is little quantification of the incidence of incorrect privacy settings or the difficulty users face when managing their privacy. In this talk, I will first focus on measuring the disparity between the desired and actual privacy settings, quantifying the magnitude of the problem of managing privacy. Later, I will discuss how social network analysis techniques can be leveraged towards addressing the privacy management crisis.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {841–842},
numpages = {2},
keywords = {online social networks, privacy},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254952,
author = {De Francisci Morales, Gianmarco and Gionis, Aristides and Silvestri, Fabrizio},
title = {Session Details: SNOW'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254952},
doi = {10.1145/3254952},
abstract = {SNOW'13 Welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {2},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254953,
author = {Silvestri, Fabrizio},
title = {Session Details: SNOW'13 Opening},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254953},
doi = {10.1145/3254953},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488062,
author = {Schifferes, Steve},
title = {Social Media, Journalism and the Public},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488062},
doi = {10.1145/2487788.2488062},
abstract = {This paper draws on the parallels between the current period and other periods of historic change in journalism to examine what is new in today's world of social media and what continuities there are with the past. It examines the changing relationship between the public and the press and how it is being continuously reinterpreted. It addresses the questions of whether we are the beginning or end of a process of revolutionary media change.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {847–848},
numpages = {2},
keywords = {journalism, social media, mass media, media history, press},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488063,
author = {Kiscuitwala, Kanak and Bult, Willem and L\'{e}cuyer, Mathias and Purtell, T.J. and Ross, Madeline K.B. and Chaintreau, Augustin and Haseman, Chris and Lam, Monica S. and McGregor, Susan E.},
title = {Weaving a Safe Web of News},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488063},
doi = {10.1145/2487788.2488063},
abstract = {The rise of social media and data-capable mobile devices in recent years has transformed the face of global journalism, supplanting the broadcast news anchor with a new source for breaking news: the citizen reporter. Social media's decentralized networks and instant re-broadcasting mechanisms mean that the reach of a single tweet can easily trump that of the most powerful broadcast satellite. Brief, text-based and easy to translate, social messages allow news audiences to skip the middleman and get news "straight from the source."Whether used by "citizen" or professional reporters, however, social media technologies can also pose risks that endanger these individuals and, by extension, the press as a whole. First, social media platforms are usually proprietary, leaving users' data and activities on the system open to scrutiny by collaborating companies and/or governments. Second, the networks upon which social media reporting relies are inherently fragile, consisting of easily targeted devices and relatively centralized message-routing systems that authorities may block or simply shut down. Finally, this same privileged access can be used to flood the network with inaccurate or discrediting messages, drowning the signal of real events in misleading noise.A citizen journalist can be anyone who is simply in the right place at the right time. Typically untrained and unevenly tech-savvy, citizen reporters are unaccustomed to thinking of their social media activities as high-risk, and may not consider the need to defend themselves against potential threats. Though often part of a crowd, they may have no formal affiliations; if targeted for retaliation, they may have nowhere to turn for help. The dangers citizen journalists face are personal and physical. They may be targeted in the act of reporting, and/or online through the tracking of their digital communications. Addressing their needs for protection, resilience, and recognition requires a move away from the major assumptions of in vitro communication security. For citizen journalists using social networks, the adversary is already inside, as the network itself may be controlled or influenced by the threatening party, while "outside" nodes, such as public figures, protest organizers, and other journalists can be trusted to handle content appropriately. In these circumstances there can be no seamless, guaranteed solution. Yet the need remains for technologies that improve the security of these journalists who in many cases may constitute a region's only independent press.In this paper, we argue that a comprehensive and collaborative effort is required to make publishing and interacting with news websites more secure. Journalists typically enjoy stronger legal protection at least in some countries, such as the United States. However, this protection may prove ineffective, as many online tools compromise source protection. In the remaining sections, we identify a set of discussion topics and challenges to encourage a broader research agenda aiming to address jointly the need for social features and security for citizens journalists and readers alike. We believe communication technologies should embrace the methods and possibilities of social news rather than treating this as a pure security problem. We briefly touch upon a related initiative, Dispatch, that focuses on providing security to citizen journalists for publisihing content.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {849–852},
numpages = {4},
keywords = {mobile publishing, disconnection resilience},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254954,
author = {De Francisci Morales, Gianmarco},
title = {Session Details: SNOW'13 Breaking the News},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254954},
doi = {10.1145/3254954},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488065,
author = {Castillo, Carlos},
title = {Traffic Prediction and Discovery of News via News Crowds},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488065},
doi = {10.1145/2487788.2488065},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {853–854},
numpages = {2},
keywords = {social media news, mass media},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488066,
author = {Gall\'{e}, Matthias and Renders, Jean-Michel and Karstens, Eric},
title = {Who Broke the News? An Analysis on First Reports of News Events},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488066},
doi = {10.1145/2487788.2488066},
abstract = {We present a data-driven study on which sources were the first to report on news events. For this, we implemented a news-aggregator that included a large number of established news sources and covered one year of data. We present a novel framework that is able to retrieve a large number of events and not only the most salient ones, while at the same time making sure that they are not exclusively of local impact.Our analysis then focuses on different aspects of the news cycle. In particular we analyze which are the sources to break most of the news. By looking when certain events become bursty, we are able to perform a finer analysis on those events and the associated sources that dominate the global news-attention. Finally we study the time it takes news outlet to report on these events and how this reects different strategies of which news to report.A general finding of our study is that big news agencies remain an important threshold to cross to bring global attention to particular news, but it also shows the importance of focused (by region or topic) outlets.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {855–862},
numpages = {8},
keywords = {breaking news, data journalism, topic detection and tracking},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254955,
author = {Silvestri, Fabrizio},
title = {Session Details: SNOW'13 Social News},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254955},
doi = {10.1145/3254955},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488068,
author = {Lehmann, Janette and Castillo, Carlos and Lalmas, Mounia and Zuckerman, Ethan},
title = {Finding News Curators in Twitter},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488068},
doi = {10.1145/2487788.2488068},
abstract = {Users interact with online news in many ways, one of them being sharing content through online social networking sites such as Twitter. There is a small but important group of users that devote a substantial amount of effort and care to this activity. These users monitor a large variety of sources on a topic or around a story, carefully select interesting material on this topic, and disseminate it to an interested audience ranging from thousands to millions. These users are news curators, and are the main subject of study of this paper. We adopt the perspective of a journalist or news editor who wants to discover news curators among the audience engaged with a news site.We look at the users who shared a news story on Twitter and attempt to identify news curators who may provide more information related to that story. In this paper we describe how to find this specific class of curators, which we refer to as news story curators. Hence, we proceed to compute a set of features for each user, and demonstrate that they can be used to automatically find relevant curators among the audience of two large news organizations.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {863–870},
numpages = {8},
keywords = {digital curator, journalism, automatic learning, social media, news story},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488069,
author = {De Nies, Tom and Haesendonck, Gerald and Godin, Fr\'{e}deric and De Neve, Wesley and Mannens, Erik and Van de Walle, Rik},
title = {Towards Automatic Assessment of the Social Media Impact of News Content},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488069},
doi = {10.1145/2487788.2488069},
abstract = {In this paper, we investigate the possibilities to estimate the impact the content of a news article has on social media, and in particular on Twitter. We propose an approach that makes use of captured and temporarily stored microposts found in social media, and compares their relevance to an arbitrary news article. These results are used to derive key indicators of the social media impact of the specified content. We describe each step of our approach, provide a first implementation, and discuss the most imminent challenges and discussion points.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {871–874},
numpages = {4},
keywords = {relevance, search and retrieval, social media, content analysis, news},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488070,
author = {Schifferes, Steve and Newman, Nic},
title = {Verifying News on the Social Web: Challenges and Prospects},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488070},
doi = {10.1145/2487788.2488070},
abstract = {The problem of verification is the key issue for journalists who use social media. This paper argues for the importance of a user-centered approach in finding solutions to this problem. Because journalists have different needs for different types of stories, there is no one magic bullet that can verify social media. Any tool will need to have a multi-faceted approach to the problem, and will have to be adjustable to suit the particular needs of individual journalists and news organizations.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {875–878},
numpages = {4},
keywords = {social media, journalism, verification, breaking news},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488071,
author = {Zubiaga, Arkaitz},
title = {Newspaper Editors vs the Crowd: On the Appropriateness of Front Page News Selection},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488071},
doi = {10.1145/2487788.2488071},
abstract = {The front page is the showcase that might condition whether one buys a newspaper, and so editors carefully select the news of the day that they believe will attract as many readers as possible. Little is known about the extent to which editors' criteria for front page news selection are appropriate so as to matching the actual interests of the crowd. In this paper, we compare the news stories in The New York Times over the period of a year to their popularity on Twitter and Facebook. Our study questions the current news selection criteria, revealing that while editors focus on picking hard news such as politics for the front page, social media users are rather into soft news such as science and fashion.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {879–880},
numpages = {2},
keywords = {facebook, front page, twitter, social media, news},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254956,
author = {Shadbolt, Nigel and De Roure, David},
title = {Session Details: SOCM'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254956},
doi = {10.1145/3254956},
abstract = {SOCM'13 welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {2},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488074,
author = {Buregio, Vanilson and Meira, Silvio and Rosa, Nelson},
title = {Social Machines: A Unified Paradigm to Describe Social Web-Oriented Systems},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488074},
doi = {10.1145/2487788.2488074},
abstract = {Blending computational and social elements into software has gained significant attention in key conferences and journals. In this context, "Social Machines" appears as a promising model for unifying both computational and social processes. However, it is a fresh topic, with concepts and definitions coming from different research fields, making a unified understanding of the concept a somewhat challenging endeavor. This paper aims to investigate efforts related to this topic and build a preliminary classification scheme to structure the science of Social Machines. We provide a preliminary overview of this research area through the identification of the main visions, concepts, and approaches; we additionally examine the result of the convergence of existing contributions. With the field still in its early stage, we believe that this work can collaborate to the process of providing a more common and coherent conceptual basis for understanding Social Machines as a paradigm. Furthermore, this study helps detect important research issues and gaps in the area.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {885–890},
numpages = {6},
keywords = {social machines, web-oriented systems, social software},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488075,
author = {Byrne Evans, Maire and O'Hara, Kieron and Tiropanis, Thanassis and Webber, Craig},
title = {Crime Applications and Social Machines: Crowdsourcing Sensitive Data},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488075},
doi = {10.1145/2487788.2488075},
abstract = {The authors explore some issues with the United Kingdom (U.K.) crime reporting and recording systems which currently produce Open Crime Data. The availability of Open Crime Data seems to create a potential data ecosystem which would encourage crowdsourcing, or the creation of social machines, in order to counter some of these issues. While such solutions are enticing, we suggest that in fact the theoretical solution brings to light fairly compelling problems, which highlight some limitations of crowdsourcing as a means of addressing Berners-Lee's "social constraint." The authors present a thought experiment -- a Gendankenexperiment - in order to explore the implications, both good and bad, of a social machine in such a sensitive space and suggest a Web Science perspective to pick apart the ramifications of this thought experiment as a theoretical approach to the characterisation of social machines.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {891–896},
numpages = {6},
keywords = {transparency, crime data, open data, trust, network science, social machines},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488076,
author = {Dalton, Ben},
title = {Pseudonymity in Social Machines},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488076},
doi = {10.1145/2487788.2488076},
abstract = {This paper describes the potential of systems in which many people collectively control a single constructed identity mediated by socio-technical networks. By looking to examples of identities that have spontaneously emerged from anonymous communities online, a model for pseudonym design in social machines is proposed. A framework of identity dimensions is presented as a means of exploring the functional types of identity encountered in social machines, and design guidelines are outlined that suggest possible approaches to this task.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {897–900},
numpages = {4},
keywords = {emergent, design, identity, social machines, pseudonymity},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488077,
author = {De Roure, David and Hooper, Clare and Meredith-Lobay, Megan and Page, Kevin and Tarte, S\'{e}gol\`{e}ne and Cruickshank, Don and De Roure, Catherine},
title = {Observing Social Machines Part 1: What to Observe?},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488077},
doi = {10.1145/2487788.2488077},
abstract = {As a scoping exercise in the design of our Social Machines Observatory we consider the observation of Social Machines "in the wild", as illustrated through two scenarios. More than identifying and classifying individual machines, we argue that we need to study interactions between machines and observe them throughout their lifecycle. We suggest that purpose may be a key notion to help identify individual Social Machines in composed systems, and that mixed observation methods will be required. This exercise provides a basis for later work on how we instrument and observe the ecosystem.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {901–904},
numpages = {4},
keywords = {web science, web observatories, social machines},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488078,
author = {Shadbolt, Nigel R. and Smith, Daniel A. and Simperl, Elena and Van Kleek, Max and Yang, Yang and Hall, Wendy},
title = {Towards a Classification Framework for Social Machines},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488078},
doi = {10.1145/2487788.2488078},
abstract = {The state of the art in human interaction with computational systems blurs the line between computations performed by machine logic and algorithms, and those that result from input by humans, arising from their own psychological processes and life experience. Current socio-technical systems, known as "social machines" exploit the large-scale interaction of humans with machines. Interactions that are motivated by numerous goals and purposes including financial gain, charitable aid, and simply for fun. In this paper we explore the landscape of social machines, both past and present, with the aim of defining an initial classificatory framework. Through a number of knowledge elicitation and refinement exercises we have identified the polyarchical relationship between infrastructure, social machines, and large-scale social initiatives. Our initial framework describes classification constructs in the areas of contributions, participants, and motivation. We present an initial characterisation of some of the most popular social machines, as demonstration of the use of the identified constructs. We believe that it is important to undertake an analysis of the behaviour and phenomenology of social machines, and of their growth and evolution over time. Our future work will seek to elicit additional opinions, classifications and validation from a wider audience, to produce a comprehensive framework for the description, analysis and comparison of social machines.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {905–912},
numpages = {8},
keywords = {social machines, web science},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488079,
author = {Singh, Priyanka and Shadbolt, Nigel},
title = {Linked Data in Crowdsourcing Purposive Social Network},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488079},
doi = {10.1145/2487788.2488079},
abstract = {Internet is an easy medium for people to collaborate and crowdsourcing is an efficient feature of social web where people with common interest and expertise come together to solve specific problems by collective thinking and create a community. It can also be used to filter out important information from large data, remove spams, and gamification techniques are used to reward the users for their contribution and keep a sustainable environment for the growth of the community. Semantic web technologies can be used to structure the community data so it can be combined, decentralized and be used across platform. Using such tools knowledge can be enhanced and easily discovered and merged together. This paper discusses the concept of a purposive social network where people with similar interest and varied expertise come together, use crowdsourcing technique to solve a common problem and build tools for common purpose. The StackOverflow website is chosen to study the purposive network, different network ties and roles of user is studied. Linked Data is used for name disambiguation of keywords and topics for easier search and discovery of experts in a field and provide useful information that is otherwise unavailable in the website.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {913–918},
numpages = {6},
keywords = {q&amp;a, name entity disambiguation, social media, crowdsourcing, social machine, linked data},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488080,
author = {Strohmaier, Markus},
title = {A Few Thoughts on Engineering Social Machines: Extended Abstract},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488080},
doi = {10.1145/2487788.2488080},
abstract = {Social machines are integrated systems of people and computers. What distinguishes social machines from other types of software systems - such as software for cars or air planes - is the unprecedented involvement of data about user behavior, -goals and -motivations into the software system's structure. In social machines, the interaction between a user and the system is mediated by the aggregation of explicit or implicit data from other users. This is the case with systems where, for example, user data is used to suggest search terms (e.g. Google Autosuggest), to recommend products (e.g. Amazon recommendations), to aid navigation (e.g. tag-based navigation) or to filter content (e.g. Digg.com). This makes social machines a novel class of software systems (as opposed to for example safety-related software that is being used in cars) and unique in a sense that potentially essential system properties and functions - such as navigability - are dynamically influenced by aggregate user behavior. Such properties can not be satisfied through the implementation of requirements alone, what is needed is regulation, i.e. a dynamic integration of users' goals and behavior into the continuous process of engineering.Functional and non-functional properties of software systems have been the subject of software engineering research for decades [1]. The notion of non-functional requirements (softgoals) captures a recognition by the software engineering community that software requirements can be subjective and interdependent, they can lack a clear-cut success criteria, exhibit different priorities and can require decomposition or operationalization. Resulting approaches to analyzing and designing software systems emphasize the role of users (or more general: agents) in this process (such as [1]). i* for example has been used to capture and represent user goals during system design and run time.With the emergence of social machines, such as the WWW, and social-focussed applications running on top of the web, such as facebook.com, delicious.com and others, social machines and their emergent properties have become a crucial infrastructure for many aspects of our daily lives. To give an example: the navigability of the web depends on the behavior of web editors who are interlinking documents, or the usefulness of tags for classification depends on the tagging behavior of users [2]. The rise of social machines can be expected to fundamentally change the way in which such properties and functions of software systems are designed and maintained. Rather than planning for certain system properties (such as navigability, usefulness for certain tasks) and functions at design time, the task of engineers is to build a platform which allows to influence and regulate emergent user behavior in such a way that desired system attributes are achieved at run time. It is through the process of social computation, i.e. the combination of social behavior and algorithmic computation, that desired system properties and functions emerge.For a science of social machines, specifically understanding the relationship between individual and social behavior on one hand, and desired system properties and functions on the other is crucial. In order to maintain control, research must focus on understanding a wide variety of social machine properties such as semantic, intentional and navigational properties across different systems and applications including - but not limited to - social media. Summarizing, the full implications of the genesis of social machines for related domains including software engineering, knowledge acquisition or peer production systems are far from being well understood, and warrant future work. For example, the interactions between the pragmatics of such systems (how they are used) and the semantics emerging in those systems (what the words, symbols, etc mean) is a fundamental issue that deserves greater attention. Equipping engineers of social machines with the right tools to achieve and maintain desirable system properties is a problem of practical relevance that needs to be addressed by future research.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {919–920},
numpages = {2},
keywords = {software engineering, social machines},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488081,
author = {Tinati, Ramine and Carr, Leslie and Halford, Susan and Pope, Catherine J.},
title = {The HTP Model: Understanding the Development of Social Machines},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488081},
doi = {10.1145/2487788.2488081},
abstract = {The Web represents a collection of socio-technical activities inter-operating using a set of common protocols and standards. Online banking, web TV, internet shopping, e-government and social networking are all different kinds of human interaction that have recently leveraged the capabilities of the Web architecture. Activities that have human and computer components are referred to as social machines. This paper introduces HTP, a socio-technical model to understand, describe and analyze the formation and development of social machines and other web activities. HTP comprises three components: heterogeneous networks of actors involved in a social machine; the iterative process of translation of the actors' activities into a temporarily stable and sustainable social machine; and the different phases of this machine's adaptation from one stable state to another as the surrounding networks restructure and global agendas ebb and flow. The HTP components are drawn from an interdisciplinary range of theoretical positions and concepts. HTP provides an analytical framework to explain why different Web activities remain stable and functional, whilst others fail. We illustrate the use of HTP by examining the formation of a classic social machine (Wikipedia), and the stabilization points corresponding to its different phases of development.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {921–926},
numpages = {6},
keywords = {web engineering, social machines, web science},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488082,
author = {Van Kleek, Max and Smith, Daniel A. and Hall, Wendy and Shadbolt, Nigel},
title = {"the Crowd Keeps Me in Shape": Social Psychology and the Present and Future of Health Social Machines},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488082},
doi = {10.1145/2487788.2488082},
abstract = {Can the Web help people live healthier lives? This paper seeks to answer this question through an examination of sites, apps and online communities designed to help people improve their fitness, better manage their disease(s) and conditions, and to solve the often elusive connections between the symptoms they experience, diseases and treatments. These health social machines employ a combination of both simple and complex social and computational processes to provide such support. We first provide a descriptive classification of the kinds of machines currently available, and the support each class offers. We then describe the limitations exhibited by these systems and potential ways around them, towards the design of more effective machines in the future.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {927–932},
numpages = {6},
keywords = {social computing, gameification, social machines, health management},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254957,
author = {Guy, Ido and Chen, Li and Zhou, Michelle X.},
title = {Session Details: SRS'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254957},
doi = {10.1145/3254957},
abstract = {SRS'13 welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {2},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488085,
author = {Leskovec, Jure},
title = {How Status and Reputation Shape Human Evaluations: Consequences for Recommender Systems},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488085},
doi = {10.1145/2487788.2488085},
abstract = {Recommender systems are inherently driven by evaluations and reviews provided by the users of these systems. Understanding ways in which users form judgments and produce evaluations can provide insights for modern recommendation systems. Many online social applications include mechanisms for users to express evaluations of one another, or of the content they create. In a variety of domains, mechanisms for evaluation allow one user to say whether he or she trusts another user, or likes the content they produced, or wants to confer special levels of authority or responsibility on them. We investigate a number of fundamental ways in which user and item characteristics affect the evaluations in online settings. For example, evaluations are not unidimensional but include multiple aspects that all together contribute to user's overall rating. We investigate methods for modeling attitudes and attributes from online reviews that help us better understand user's individual preferences. We also examine how to create a composite description of evaluations that accurately reflects some type of cumulative opinion of a community. Natural applications of these investigations include predicting the evaluation outcomes based on user characteristics and to estimate the chance of a favorable overall evaluation from a group knowing only the attributes of the group's members, but not their expressed opinions.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {937–938},
numpages = {2},
keywords = {social recommender systems},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488086,
author = {Tiwari, Mitul},
title = {Large-Scale Social Recommender Systems: Challenges and Opportunities},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488086},
doi = {10.1145/2487788.2488086},
abstract = {Online social networks have become very important for networking, communication, sharing, and content discovery. Recommender systems play a significant role on any online social network for engaging members, recruiting new members, and recommending other members to connect with. This talk presents challenges in recommender systems, graph analysis, social stream relevance and virality on a large-scale social networks such as LinkedIn, the largest professional network with more than 200M members.First, social recommender systems for recommending jobs, groups, companies to follow, other members to connect with, are very important part of a professional network like LinkedIn [1, 6, 7, 9]. Each one of these entity recommender systems present novel challenges to use social and member generated data. Second, various problems, such as, link prediction, visualizing connection network, finding the strength of each connection, and the best path among members, require large-scale social graph analysis, and present unique research opportunities [2, 5]. Third, social stream relevance and capturing virality in social products are crucial for engaging users on any online social network [4]. Final, systems challenges must be addressed in scaling recommender systems on a large-scale social networks [3, 8, 10]. This talk presents challenges and interesting problems in large-scale social recommender systems, and describes some of the solutions.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {939–940},
numpages = {2},
keywords = {relevance, recommender systems, social networks, link prediction},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488088,
author = {Arru, Giuliano and Feltoni Gurini, Davide and Gasparetti, Fabio and Micarelli, Alessandro and Sansonetti, Giuseppe},
title = {Signal-Based User Recommendation on Twitter},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488088},
doi = {10.1145/2487788.2488088},
abstract = {In recent years, social networks have become one of the best ways to access information. The ease with which users connect to each other and the opportunity provided by Twitter and other social tools in order to follow person activities are increasing the use of such platforms for gathering information. The amount of available digital data is the core of the new challenges we now face. Social recommender systems can suggest both relevant content and users with common social interests. Our approach relies on a signal-based model, which explicitly includes a time dimension in the representation of the user interests. Specifically, this model takes advantage of a signal processing technique, namely, the wavelet transform, for defining an efficient pattern-based similarity function among users. Experimental comparisons with other approaches show the benefits of the proposed approach.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {941–944},
numpages = {4},
keywords = {social network, signal processing, twitter, wavelet, user recommendation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488089,
author = {Carvalho, Lucas Augusto M.C. and Macedo, Hendrik T.},
title = {Generation of Coalition Structures to Provide Proper Groups' Formation in Group Recommender Systems},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488089},
doi = {10.1145/2487788.2488089},
abstract = {Group recommender systems usually provide recommendations to a fixed and predetermined set of members. In some situations, however, there is a set of people (N) that should be organized into smaller and cohesive groups, so it is possible to provide more effective recommendations to each of them. This is not a trivial task. In this paper we propose an innovative approach for grouping people within the recommendation problem context. The problem is modeled as a coalitional game from Game Theory. The goal is to group people into exhaustive and disjoint coalitions so as to maximize the social welfare function of the group. The optimal coalition structure is that with highest summation over all social welfare values. Similarities between recommendation system users are used to define the social welfare function. We compare our approach with K-Means clustering for a dataset from Movielens. Results have shown that the proposed approach performs better than K-Means for both average group satisfaction and Davies-Bouldin index metrics when the number of coalitions found is not greater than 4 (K &lt;= 4) for a population size of 12 (N = 12).},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {945–950},
numpages = {6},
keywords = {coalitional games, group recommendation, social welfare, game theory, group formation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488090,
author = {Carvalho, Lucas Augusto Montalv\~{a}o Costa and Macedo, Hendrik Teixeira},
title = {Users' Satisfaction in Recommendation Systems for Groups: An Approach Based on Noncooperative Games},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488090},
doi = {10.1145/2487788.2488090},
abstract = {A major difficulty in a recommendation system for groups is to use a group aggregation strategy to ensure, among other things, the maximization of the average satisfaction of group members. This paper presents an approach based on the theory of noncooperative games to solve this problem. While group members can be seen as game players, the items for potential recommendation for the group comprise the set of possible actions. Achieving group satisfaction as a whole becomes, then, a problem of finding the Nash equilibrium. Experiments with a MovieLens dataset and a function of arithmetic mean to compute the prediction of group satisfaction for the generated recommendation have shown statistically significant results when compared to state-of-the-art aggregation strategies, in particular, when evaluation among group members are more heterogeneous. The feasibility of this unique approach is shown by the development of an application for Facebook, which recommends movies to groups of friends.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {951–958},
numpages = {8},
keywords = {nash equilibrium, game theory, group recommendation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488091,
author = {Cohen, Sara and Ebel, Lior},
title = {Recommending Collaborators Using Keywords},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488091},
doi = {10.1145/2487788.2488091},
abstract = {This paper studies the problem of recommending collaborators in a social network, given a set of keywords. Formally, given a query q, consisting of a researcher s (who is a member of a social network) and a set of keywords k (e.g., an article name or topic of future work), the collaborator recommendation problem is to return a high-quality ranked list of possible collaborators for s on the topic k. Extensive effort was expended to define ranking functions that take into consideration a variety of properties, including structural proximity to s, textual relevance to k, and importance. The effectiveness of our methods have been experimentally proven over two large subsets of the social network determined by DBLP co-authorship data. The results show that the ranking methods developed in this paper work well in practice.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {959–962},
numpages = {4},
keywords = {social network, collaborator recommendation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488092,
author = {Lu, Yao and El Helou, Sandy and Gillet, Denis},
title = {A Recommender System for Job Seeking and Recruiting Website},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488092},
doi = {10.1145/2487788.2488092},
abstract = {In this paper, a hybrid recommender system for job seeking and recruiting websites is presented. The various interaction features designed on the website help the users organize the resources they need as well as express their interest. The hybrid recommender system exploits the job and user profiles and the actions undertaken by users in order to generate personalized recommendations of candidates and jobs. The data collected from the website is modeled using a directed, weighted, and multi-relational graph, and the 3A ranking algorithm is exploited to rank items according to their relevance to the target user. A preliminary evaluation is conducted based on simulated data and production data from a job hunting website in Switzerland.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {963–966},
numpages = {4},
keywords = {multi-relational graph, recommender system, job seeking and recruiting websites},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488093,
author = {Menezes, Danilo and Lacerda, Anisio and Silva, Leila and Veloso, Adriano and Ziviani, Nivio},
title = {Weighted Slope One Predictors Revisited},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488093},
doi = {10.1145/2487788.2488093},
abstract = {Recommender systems are used to help people in specific life choices, like what items to buy, what news to read or what movies to watch. A relevant work in this context is the Slope One algorithm, which is based on the concept of differential popularity between items (i.e., how much better one item is liked than another). This paper proposes new approaches to extend Slope One based predictors for collaborative filtering, in which the predictions are weighted based on the number of users that co-rated items. We propose to improve collaborative filtering by exploiting the web of trust concept, as well as an item utility measure based on the error of predictions based on specific items to specific users. We performed experiments using three application scenarios, namely Movielens, Epinions, and Flixter. Our results demonstrate that, in most cases, exploiting the web of trust is benefitial to prediction performance, and improvements are reported when comparing the proposed approaches against the original Weighted Slope One algorithm.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {967–972},
numpages = {6},
keywords = {recommender systems, collaborative filtering, slope one, trust-aware},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488094,
author = {Servajean, Maximilien and Pacitti, Esther and Amer-Yahia, Sihem and Neveu, Pascal},
title = {Profile Diversity in Search and Recommendation},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488094},
doi = {10.1145/2487788.2488094},
abstract = {We investigate profile diversity, a novel idea in searching scientic documents. Combining keyword relevance with popularity in a scoring function has been the subject of different forms of social relevance [2, 6, 9]. Content diversity has been thoroughly studied in search and advertising [4, 11], database queries [16, 5, 8], and recommendations [17, 10, 18]. We believe our work is the first to investigate profile diversity to address the problem of returning highly popular but too-focused documents. We show how to adapt Fagin's threshold-based algorithms to return the most relevant and most popular documents that satisfy content and profile diversities and run preliminary experiments on two benchmarks to validate our scoring function.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {973–980},
numpages = {8},
keywords = {top-k, diversity, recommendation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488095,
author = {Tavakolifard, Mozhgan and Almeroth, Kevin C. and Gulla, Jon Atle},
title = {Does Social Contact Matter? Modelling the Hidden Web of Trust Underlying Twitter},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488095},
doi = {10.1145/2487788.2488095},
abstract = {Social recommender systems aim to alleviate the information overload problem on social network sites. The social network structure is often an important input to these recommender systems. Typically, this structure cannot be inferred directly from declared relationships among users. The goal of our work is to extract an underlying hidden and sparse network which more strongly represents the actual interactions among users. We study how to leverage Twitter activities like micro-blogging and the network structure to find a simple, efficient, but accurate method to infer and expand this hidden network. We measure and compare the performance of several different modeling strategies using a crawled data set from Twitter. Our results reveal that the structural similarity in the network generated by users' retweeting behavior outweighs the other discussed methods.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {981–988},
numpages = {8},
keywords = {recommender systems, trust, twitter, social networks, similarity},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488096,
author = {Zhang, Jun and Teng, Chun-yuen and Qu, Yan},
title = {Understanding User Spatial Behaviors for Location-Based Recommendations},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488096},
doi = {10.1145/2487788.2488096},
abstract = {In this paper, we introduce a network-based method to study user spatial behaviors based on check-in histories. The results of this study have direct implications for location-based recommendation systems.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {989–992},
numpages = {4},
keywords = {spatial behavior, location-based recommendations, network analysis, location-based services, human mobility},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254958,
author = {Vukovic, Maja and Kumara, Soundar and Meier, Patrick},
title = {Session Details: SWDM'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254958},
doi = {10.1145/3254958},
abstract = {SWDM'13 welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {2},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254959,
author = {Kumara, Soundar},
title = {Session Details: SWDM'13 Keynote},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254959},
doi = {10.1145/3254959},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488099,
author = {Jain, Ramesh C.},
title = {Disasters Response Using Social Life Networks},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488099},
doi = {10.1145/2487788.2488099},
abstract = {Connecting people to required resources efficiently, effectively and promptly is one of the most important challenges for our society. Disasters make it the challenge for life and death. During disasters many normal sources of information to assess situations as well as distributing vital information to individuals break down. Unfortunately, during disastrous situations, most current practices are forced to follow bureaucratic processes and procedures that may delay help in critical life and death moments. Social media brings together different media as well as modes of distribution - focused, narrowcast, and broadcast -- and has revolutionized communication among people. Mobile phones, equipped with myriads of sensors are bringing the next generation of social networks not only to connect people with other people, but also to connect people with other people and essential life resources based on the disaster situation and personal context. We believe that such Social Life Networks (SLN) may play very important role for solving some essential human problems, including providing vital help to people during disasters. We will present early design of such systems and use a few examples of such systems explored in our group during disasters. Focused Micro Blogs (FMBs) will be discussed as an alternative to less noisy and more direct versions of current microblogs, such as Tweets and Status Updates. An important part of our discussion will be to list challenges and opportunities in this area.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {997–998},
numpages = {2},
keywords = {social life network, disaster management},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254960,
author = {Vukovic, Maja},
title = {Session Details: SWDM'13 Twitter in Action},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254960},
doi = {10.1145/3254960},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488101,
author = {Robinson, Bella and Power, Robert and Cameron, Mark},
title = {A Sensitive Twitter Earthquake Detector},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488101},
doi = {10.1145/2487788.2488101},
abstract = {This paper describes early work at developing an earthquake detector for Australia and New Zealand using Twitter. The system is based on the Emergency Situation Awareness (ESA) platform which provides all-hazard information captured, filtered and analysed from Twitter. The detector sends email notifications of evidence of earthquakes from Tweets to the Joint Australian Tsunami Warning Centre.The earthquake detector uses the ESA platform to monitor Tweets and checks for specific earthquake related alerts. The Tweets that contribute to an alert are then examined to determine their locations: when the Tweets are identified as being geographically close and the retweet percentage is low an email notification is generated.The earthquake detector has been in operation since December 2012 with 31 notifications generated where 17 corresponded with real, although minor, earthquake events. The remaining 14 were a result of discussions about earthquakes but not prompted by an event. A simple modification to our algorithm results in 20 notifications identifying the same 17 real events and reducing the false positives to 3. Our detector is sensitive in that it can generate alerts from only a few Tweets when they are determined to be geographically close.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {999–1002},
numpages = {4},
keywords = {social media, crisis management, disaster management, situation awareness},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488102,
author = {Liang, Yuan and Caverlee, James and Mander, John},
title = {Text vs. Images: On the Viability of Social Media to Assess Earthquake Damage},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488102},
doi = {10.1145/2487788.2488102},
abstract = {In this paper, we investigate the potential of social media to provide rapid insights into the location and extent of damage associated with two recent earthquakes - the 2011 Tohoku earthquake in Japan and the 2011 Christchurch earthquake in New Zealand. Concretely, we (i) assess and model the spatial coverage of social media; and (ii) study the density and dynamics of social media in the aftermath of these two earthquakes. We examine the difference between text tweets and media tweets (containing links to images and videos), and investigate tweet density, re-tweet density, and user tweeting count to estimate the epicenter and to model the intensity attenuation of each earthquake. We find that media tweets provide more valuable location information, and that the relationship between social media activity vs. loss/damage attenuation suggests that social media following a catastrophic event can provide rapid insight into the extent of damage.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1003–1006},
numpages = {4},
keywords = {social media, damage assessment, attenuation pattern},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488103,
author = {Power, Robert and Robinson, Bella and Wise, Catherine},
title = {Comparing Web Feeds and Tweets for Emergency Management},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488103},
doi = {10.1145/2487788.2488103},
abstract = {This paper describes ongoing work with the Australian Government to assemble information from a collection of web feeds describing emergency incidents of interest for emergency managers. The developed system, the Emergency Response Intelligence Capability (ERIC) tool, has been used to gather information about emergency events during the Australian summer of 2012/13. The web feeds are an authoritative source of structured information summarising incidents that includes links to emergency services web sites containing further details about the events underway.The intelligence obtained using ERIC for a specific fire event has been compared with information that was available in Twitter using the Emergency Situation Awareness (ESA) platform. This information would have been useful as a new source of intelligence: it was reported faster than via the web feed, contained more specific event information, included details of impact to the community, was updated more frequently, included information from the public and remains available as a source of information long after the web feed contents have been removed.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1007–1010},
numpages = {4},
keywords = {social media, disaster management, situation awareness},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254961,
author = {Meier, Patrick},
title = {Session Details: SWDM'13 Keynote 2},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254961},
doi = {10.1145/3254961},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488105,
author = {Stevens, David},
title = {Leveraging on Social Media to Support the Global Building Resilient Cities Campaign},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488105},
doi = {10.1145/2487788.2488105},
abstract = {This paper presents a summary of the main points put forward during the presentation delivered at the 2nd International Workshop on Social Web for Disaster Management which was held in conjunction with WWW 2013 on May 14th 2013 in Rio de Janeiro, Brazil.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1011–1012},
numpages = {2},
keywords = {social media, disaster risk reduction, crisis mapping},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254962,
author = {Kumara, Soundar},
title = {Session Details: SWDM'13 Insights from Social Web},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254962},
doi = {10.1145/3254962},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488107,
author = {Ikawa, Yohei and Vukovic, Maja and Rogstadius, Jakob and Murakami, Akiko},
title = {Location-Based Insights from the Social Web},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488107},
doi = {10.1145/2487788.2488107},
abstract = {Citizens, news reporters, relief organizations, and governments are increasingly relying on the Social Web to report on and respond to disasters as they occur. The capability to rapidly react to important events, which can be identified from high-volume streams even when the sources are unknown, still requires precise localization of the events and verification of the reports. In this paper, we propose a framework for classifying location elements and a method for their extraction from Social Web data. We describe the framework in the context of existing Social Web systems used for disaster management. We present a new location-inferencing architecture and evaluate its performance with a data set from a real-world disaster.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1013–1016},
numpages = {4},
keywords = {text analysis, geolocation analysis, microblog},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488108,
author = {Lingad, John and Karimi, Sarvnaz and Yin, Jie},
title = {Location Extraction from Disaster-Related Microblogs},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488108},
doi = {10.1145/2487788.2488108},
abstract = {Location information is critical to understanding the impact of a disaster, including where the damage is, where people need assistance and where help is available. We investigate the feasibility of applying Named Entity Recognizers to extract locations from microblogs, at the level of both geo-location and point-of-interest. Our experimental results show that such tools once retrained on microblog data have great potential to detect the where information, even at the granularity of point-of-interest.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1017–1020},
numpages = {4},
keywords = {location extraction, named entity recognition, social media mining},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488109,
author = {Imran, Muhammad and Elbassuoni, Shady and Castillo, Carlos and Diaz, Fernando and Meier, Patrick},
title = {Practical Extraction of Disaster-Relevant Information from Social Media},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488109},
doi = {10.1145/2487788.2488109},
abstract = {During times of disasters online users generate a significant amount of data, some of which are extremely valuable for relief efforts. In this paper, we study the nature of social-media content generated during two different natural disasters. We also train a model based on conditional random fields to extract valuable information from such content. We evaluate our techniques over our two datasets through a set of carefully designed experiments. We also test our methods over a non-disaster dataset to show that our extraction model is useful for extracting information from socially-generated content in general.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1021–1024},
numpages = {4},
keywords = {information extraction, social media, information filtering},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488110,
author = {Toriumi, Fujio and Sakaki, Takeshi and Shinoda, Kosuke and Kazama, Kazuhiro and Kurihara, Satoshi and Noda, Itsuki},
title = {Information Sharing on Twitter during the 2011 Catastrophic Earthquake},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488110},
doi = {10.1145/2487788.2488110},
abstract = {Such large disasters as earthquakes and hurricanes are very unpredictable. During a disaster, we must collect information to save lives. However, in time disaster, it is difficult to collect information which is useful for ourselves from such traditional mass media as TV and newspapers that contain information for the general public. Social media attract attention for sharing information, especially Twitter, which is a hugely popular social medium that is now being used during disasters. In this paper, we focus on the information sharing behaviors on Twitter during disasters. We collected data before and during the Great East Japan Earthquake and arrived at the following conclusions: Many users with little experience with such specific functions as reply and retweet did not continuously use them after the disaster.Retweets were well used to share information on Twitter. Retweets were used not only for sharing the information provided by general users but used for relaying the information from the mass media.We conclude that social media users changed their behavior to widely diffuse important information and decreased non-emergency tweets to avoid interrupting critical information.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1025–1028},
numpages = {4},
keywords = {social networks, information diffusion, information sharings, earthquake, twitter},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488111,
author = {Popoola, Abdulfatai and Krasnoshtan, Dmytro and Toth, Attila-Peter and Naroditskiy, Victor and Castillo, Carlos and Meier, Patrick and Rahwan, Iyad},
title = {Information Verification during Natural Disasters},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488111},
doi = {10.1145/2487788.2488111},
abstract = {Large amounts of unverified and at times contradictory information often appear on social media following natural disasters. Timely verification of this information can be crucial to saving lives and for coordinating relief efforts. Our goal is to enable this verification by developing an online platform that involves ordinary citizens in the evidence gathering and evaluation process. The output of this platform will provide reliable information to humanitarian organizations, journalists, and decision makers involved in relief efforts.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1029–1032},
numpages = {4},
keywords = {crowdsourcing, data verification},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254963,
author = {Baeza-Yates, Ricardo and Masan\`{a}s, Julien and Spaniol, Marc},
title = {Session Details: TempWeb'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254963},
doi = {10.1145/3254963},
abstract = {TempWeb'13 welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {2},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488114,
author = {Alonso, Omar and Shiells, Kyle},
title = {Timelines as Summaries of Popular Scheduled Events},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488114},
doi = {10.1145/2487788.2488114},
abstract = {Known events that are scheduled in advance, such as popular sports games, usually get a lot of attention from the public. Communications media like TV, radio, and newspapers will report the salient aspects of such events live or post-hoc for general consumption. However, certain actions, facts, and opinions would likely be omitted from those objective summaries. Our approach is to construct a particular game's timeline in such a way that it can be used as a quick summary of the main events that happened along with popular subjective and opinionated items that the public inject. Peaks in the volume of posts discussing the event reflect both objectively recognizable events in the game - in the sports example, a change in score - and subjective events such as a referee making a call fans disagree with. In this work, we introduce a novel timeline design that captures a more complete story of the event by placing the volume of Twitter posts alongside keywords that are driving the additional traffic. We demonstrate our approach using events of major international social impact from the World Cup 2010 and evaluate against professional liveblog coverage of the same events.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1037–1044},
numpages = {8},
keywords = {sports, football, world cup, soccer, summaries, timelines, rugby},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488116,
author = {Costa, Miguel and Gomes, Daniel and Couto, Francisco and Silva, M\'{a}rio},
title = {A Survey of Web Archive Search Architectures},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488116},
doi = {10.1145/2487788.2488116},
abstract = {Web archives already hold more than 282 billion documents and users demand full-text search to explore this historical information. This survey provides an overview of web archive search architectures designed for time-travel search, i.e. full-text search on the web within a user-specified time interval. Performance, scalability and ease of management are important aspects to take in consideration when choosing a system architecture. We compare these aspects and initialize the discussion of which search architecture is more suitable for a large-scale web archive.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1045–1050},
numpages = {6},
keywords = {temporal search, portuguese web archive},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488117,
author = {AlSum, Ahmed and Nelson, Michael L. and Sanderson, Robert and Van de Sompel, Herbert},
title = {Archival HTTP Redirection Retrieval Policies},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488117},
doi = {10.1145/2487788.2488117},
abstract = {When retrieving archived copies of web resources (mementos) from web archives, the original resource's URI-R is typically used as the lookup key in the web archive. This is straightforward until the resource on the live web issues a redirect: R -&gt;R`. Then it is not clear if R or R` should be used as the lookup key to the web archive. In this paper, we report on a quantitative study to evaluate a set of policies to help the client discover the correct memento when faced with redirection. We studied the stability of 10,000 resources and found that 48% of the sample URIs tested were not stable, with respect to their status and redirection location. 27% of the resources were not perfectly reliable in terms of the number of mementos of successful responses over the total number of mementos, and 2% had a reliability score of less than 0.5. We tested two retrieval policies. The first policy covered the resources which currently issue redirects and successfully resolved 17 out of 77 URIs that did not have mementos of the original URI, but did of the resource that was being redirected to. The second policy covered archived copies with HTTP redirection and helped the client in 58% of the cases tested to discover the nearest memento to the requested datetime.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1051–1058},
numpages = {8},
keywords = {memento, web archive},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488118,
author = {Gomes, Daniel and Costa, Miguel and Cruz, David and Miranda, Jo\~{a}o and Fontes, Sim\~{a}o},
title = {Creating a Billion-Scale Searchable Web Archive},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488118},
doi = {10.1145/2487788.2488118},
abstract = {Web information is ephemeral. Several organizations around the world are struggling to archive information from the web before it vanishes. However, users demand efficient and effective search mechanisms to access the already vast collections of historical information held by web archives. The Portuguese Web Archive is the largest full-text searchable web archive publicly available. It supports search over 1.2 billion files archived from the web since 1996. This study contributes with an overview of the lessons learned while developing the Portuguese Web Archive, focusing on web data acquisition, ranking search results and user interface design. The developed software is freely available as an open source project. We believe that sharing our experience obtained while developing and operating a running service will enable other organizations to start or improve their web archives.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1059–1066},
numpages = {8},
keywords = {portuguese web archive, temporal search},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488120,
author = {Kiseleva, Julia and Thanh Lam, Hoang and Pechenizkiy, Mykola and Calders, Toon},
title = {Discovering Temporal Hidden Contexts in Web Sessions for User Trail Prediction},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488120},
doi = {10.1145/2487788.2488120},
abstract = {In many web information systems such as e-shops and information portals, predictive modeling is used to understand user's intentions based on their browsing behaviour. User behavior is inherently sensitive to various hidden contexts. It has been shown in different experimental studies that exploitation of contextual information can help in improving prediction performance significantly. It is reasonable to assume that users may change their intents during one web session and that changes are influenced by some external factors such as switch in temporal context e.g. 'users want to find information about a specific product' and after a while 'they want to buy this product'. A web session can be represented as a sequence of user's actions where actions are ordered by time. The generation of a web session might be influenced by several hidden temporal contexts. Each session can be represented as a concatenation of independent segments, each of which is influenced by one corresponding context. We show how to learn how to apply different predictive models for each segment in this work. We define the problem of discovering temporal hidden contexts in such way that we optimize directly the accuracy of predictive models (e.g. users' trails prediction) during the process of context acquisition. Our empirical study on a real dataset demonstrates the effectiveness of our method.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1067–1074},
numpages = {8},
keywords = {temporal web analytics, browsing behaviour, context-awareness},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488121,
author = {SalahEldeen, Hany M. and Nelson, Michael L.},
title = {Carbon Dating the Web: Estimating the Age of Web Resources},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488121},
doi = {10.1145/2487788.2488121},
abstract = {In the course of web research it is often necessary to estimate the creation datetime for web resources (in the general case, this value can only be estimated). While it is feasible to manually establish likely datetime values for small numbers of resources, this becomes infeasible if the collection is large. We present "carbon date", a simple web application that estimates the creation date for a URI by polling a number of sources of evidence and returning a machine-readable structure with their respective values. To establish a likely datetime, we poll bitly for the first time someone shortened the URI, topsy for the first time someone tweeted the URI, a Memento aggregator for the first time it appeared in a public web archive, Google's time of last crawl, and the Last-Modified HTTP response header of the resource itself. We also examine the backlinks of the URI as reported by Google and apply the same techniques for the resources that link to the URI. We evaluated our tool on a gold standard data set of 1200 URIs in which the creation date was manually verified. We were able to estimate a creation date for 75.90% of the resources, with 32.78% having the correct value. Given the different nature of the URIs, the union of the various methods produces the best results. While the Google last crawl date and topsy account for nearly 66% of the closest answers, eliminating the web archives or Last-Modified from the results produces the largest overall negative impact on the results. The carbon date application is available for download or use via a web API.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1075–1082},
numpages = {8},
keywords = {archiving, creation dates, memento, social media},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488122,
author = {Alonso, Omar},
title = {Stuff Happens Continuously: Exploring Web Contents with Temporal Information},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488122},
doi = {10.1145/2487788.2488122},
abstract = {In the last few years there has been an increased interest from researchers and practitioners in exploring time as a dimension that can benefit several information retrieval tasks. There is exciting work in analyzing and exploiting temporal information embedded in documents as relevance cues for the presentation, organization, and the exploration of search results in a temporal context.Most of the current approaches focus on leveraging the temporal information available in document sources like web pages or news articles. However, the Web keeps evolving beyond simple web pages and new information sources and services are adopted very rapidly. For example, the incredible amount of content that is generated by users in social networks offers another aspect to examine how people produce and consume content over time.We review the current activities centered on identifying and extracting time information from document collections and the applications to the information seeking process. We outline the potential of new sources for studying temporal information by presenting new problems. Finally, we discuss a number of scenarios where a temporal perspective can provide insights when exploring Web contents.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1083–1084},
numpages = {2},
keywords = {temporal information, timelines, information retrieval, social},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488124,
author = {Miranda, Lucas C.O. and Santos, Rodrygo L.T. and Laender, Alberto H.F.},
title = {Characterizing Video Access Patterns in Mainstream Media Portals},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488124},
doi = {10.1145/2487788.2488124},
abstract = {Watching online videos is part of the daily routine of a considerable fraction of Internet users nowadays. Understanding the patterns of access to these videos is paramount for improving the capacity planning for video providers, the conversion rate for advertisers, and the relevance of the whole online video watching experience for end users. While much research has been conducted to analyze video access patterns in user-generated content (UGC), little is known of how such patterns manifest in mainstream media (MSM) portals. In this paper, we perform the first large-scale analysis of video access patterns in MSM portals. As a case study, we analyze interaction logs across a total of 38 Brazilian MSM portals, including six of the largest portals in the country, over a period of eight weeks. Our analysis reveals interesting static and temporal video access patterns in MSM portals, which we compare and contrast to the access patterns reported for UGC websites. Overall, our analysis provides several insights for an improved understanding of video access on the Internet beyond UGC websites.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1085–1092},
numpages = {8},
keywords = {user-generated content, temporal analysis, online video, mainstream media, video access patterns},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488125,
author = {Celis, L. Elisa and Dasgupta, Koustuv and Rajan, Vaibhav},
title = {Adaptive Crowdsourcing for Temporal Crowds},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488125},
doi = {10.1145/2487788.2488125},
abstract = {Crowdsourcing is rapidly emerging as a computing paradigm that can employ the collective intelligence of a distributed human population to solve a wide variety of tasks. However, unlike organizational environments where workers have set work hours, known skill sets and performance indicators that can be monitored and controlled, most crowdsourcing platforms leverage the capabilities of fleeting workers who exhibit changing work patterns, expertise, and quality of work. Consequently, platforms exhibit significant variability in terms of performance characteristics (like response time, accuracy, and completion rate). While this variability has been folklore in the crowdsourcing community, we are the first to show data that displays this kind of changing behavior. Notably, these changes are not due to a distribution with high variance; rather, the distribution itself is changing over time.Deciding which platform is most suitable given the requirements of a task is of critical importance in order to optimize performance; further, making the decision(s) adaptively to accommodate the dynamically changing crowd characteristics is a problem that has largely been ignored. In this paper, we address the changing crowds problem and, specifically, propose a multi-armed bandit based framework. We introduce the simple epsilon-smart algorithm that performs robustly. Counterfactual results based on real-life data from two popular crowd platforms demonstrate the efficacy of the proposed approach. Further simulations using a random-walk model for crowd performance demonstrate its scalability and adaptability to more general scenarios.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1093–1100},
numpages = {8},
keywords = {crowdsourcing, online learning, temporal behavior, multi-armed bandit},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488126,
author = {Joho, Hideo and Jatowt, Adam and Roi, Blanco},
title = {A Survey of Temporal Web Search Experience},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488126},
doi = {10.1145/2487788.2488126},
abstract = {Temporal aspects of web search have gained a great level of attention in the recent years. However, many of the research attempts either focused on technical development of various tools or behavioral analysis based on log data. This paper presents the results of user survey carried out to investigate the practice and experience of temporal web search. A total of 110 people was recruited and answered 18 questions regarding their recent experience of web search. Our results suggest that an interplay of seasonal interests, technicality of information needs, target time of information, re-finding behaviour, and freshness of information can be important factors for the application of temporal search. These findings should be complementary to log analyses for further development of temporally aware search engines.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1101–1108},
numpages = {8},
keywords = {temporal web search, survey, user experience},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254964,
author = {Jatowt, Adam and Castillo, Carlos and Gyongyi, Zoltan and Tanaka, Katsumi},
title = {Session Details: WebQuality'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254964},
doi = {10.1145/3254964},
abstract = {WebQuality'13 welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {2},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254965,
author = {Jatowt, Adam},
title = {Session Details: WEBQUALITY'13 Keynote Talk},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254965},
doi = {10.1145/3254965},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488129,
author = {Baeza-Yates, Ricardo},
title = {Measuring Web Quality},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488129},
doi = {10.1145/2487788.2488129},
abstract = {Measuring the quality of web content, either at page level or website level, is at the heart of several key challenges in the Web. Without doubt, the main one is web search, to be able to rank results. However, there are other important problems such as web reputation or trust, and web spam detection and filtering. However, measuring intrinsic web quality is a hard problem, because of our limited (automatic) understanding of text semantics, which is even worse for other media. Hence, similarly to human trust assessing, where we use past actions, face expressions, body language, etc; in the Web we need to use indirect signals that serve as surrogates for web quality. In this keynote we attempt to present the most important signals as well as new signals that are or can be used to measure quality in the Web. We divide them using the traditional web content, structure, and usage trilogy. We also characterize them according to how easy is to measure these signals, who can measure them, and how well they scale to the whole Web.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1113–1114},
numpages = {2},
keywords = {ranking, web quality, web search, web spam, web trust},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254966,
author = {Jatowt, Adam},
title = {Session Details: WEBQUALITY'13 Web Content Quality Session},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254966},
doi = {10.1145/3254966},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488131,
author = {Liu, Xin and Nielek, Radoslaw and Wierzbicki, Adam and Aberer, Karl},
title = {Defending Imitating Attacks in Web Credibility Evaluation Systems},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488131},
doi = {10.1145/2487788.2488131},
abstract = {Unlike traditional media such as television and newspapers, web contents are relatively easy to be published without being rigorously fact-checked. This seriously influences people's daily life if non-credible web contents are utilized for decision making. Recently, web credibility evaluation systems have emerged where web credibility is derived by aggregating ratings from the community (e.g., MyWOT). In this paper, We focus on the robustness of such systems by identifying a new type of attack scenario where an attacker imitates the behavior of trustworthy experts by copying system's credibility ratings to quickly build high reputation and then attack certain web contents. In order to defend this attack, we propose a two-stage defence algorithm. At stage 1, our algorithm applies supervised learning algorithm to predict the credibility of a web content and compare it with a user's rating to estimate whether this user is malicious or not. In case the user's maliciousness can not be determined with high confidence, the algorithm goes to stage 2 where we investigate users' past rating patterns and detect the malicious one by applying hierarchical clustering algorithm. Evaluation using real datasets demonstrates the efficacy of our approach.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1115–1122},
numpages = {8},
keywords = {machine learning, imitating attack, web credibility, robustness},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488132,
author = {Pattanaphanchai, Jarutas and O'Hara, Kieron and Hall, Wendy},
title = {Trustworthiness Criteria for Supporting Users to Assess the Credibility of Web Information},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488132},
doi = {10.1145/2487788.2488132},
abstract = {Assessing the quality of information on the Web is a challenging issue for at least two reasons. First, as a decentralized data publishing platform in which anyone can share nearly anything, the Web has no inherent quality control mechanisms to ensure that content published is valid, legitimate, or even just interesting. Second, when assessing the trustworthiness of web pages, users tend to base their judgments upon descriptive criteria such as the visual presentation of the website rather than more robust normative criteria such as the author's reputation and the source's review process. As a result, Web users are liable to make incorrect assessments, particularly when making quick judgments on a large scale. Therefore, Web users need credibility criteria and tools to help them assess the trustworthiness of Web information in order to place trust in it. In this paper, we investigate the criteria that can be used to collect supportive data about a piece of information in order to improve a person's ability to quickly judge the trustworthiness of the information. We propose the normative trustworthiness criteria namely, authority, currency, accuracy and relevance which can be used to support users' assessments of the trustworthiness of Web information. In addition, we validate these criteria using an expert panel. The results show that the proposed criteria are helpful. Moreover, we obtain weighting scores for criteria which can be used to calculate the trustworthiness of information and suggest a piece of information that is more likely to be trustworthy to Web users.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1123–1130},
numpages = {8},
keywords = {information quality, trust, web credibility, trustworthiness},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488133,
author = {K\k{a}kol, Micha\l{} and Jankowski-Lorek, Micha\l{} and Abramczuk, Katarzyna and Wierzbicki, Adam and Catasta, Michele},
title = {On the Subjectivity and Bias of Web Content Credibility Evaluations},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488133},
doi = {10.1145/2487788.2488133},
abstract = {In this paper we describe the initial outcomes of the Reconcile1 study concerning Web content credibility evaluations. The study was run with a balanced sample of 1503 respondents who independently evaluated 154 web pages from several thematic categories. Users taking part in the study not only evaluated credibility, but also filled a questionnaire covering additional respondents' traits. Using the gathered information about socio-economic status and psychological features of the users, we studied the influence of subjectivity and bias in the credibility ratings. Subjectivity and bias, in fact, represent a key design issue for Web Credibility systems, to the extent that they could jeopardize the system performance if not taken into account.We found out that evaluations of Web content credibility are slightly subjective. On the other hand, the evaluations exhibit a strong acquiescence bias.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1131–1136},
numpages = {6},
keywords = {credibility, credibility ratings, subjectivity, bias, world wide web, reliability, user based},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254967,
author = {Gyongyi, Zoltan},
title = {Session Details: WEBQUALITY'13 Industry Experience Session},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254967},
doi = {10.1145/3254967},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488135,
author = {Pevtsov, Sergey and Volkov, Sergey},
title = {Russian Web Spam Evolution: Yandex Experience},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488135},
doi = {10.1145/2487788.2488135},
abstract = {Web spam has a negative impact on the search quality and users' satisfaction and forces search engines to waste resources to crawl, index, and rank it. Thus search engines are compelled to make significant efforts in order to fight web spam. Traffic from search engines plays a great role in online economics. It causes a tough competition for high positions in search results and increases the motivation of spammers to invent new spam techniques. At the same time, ranking algorithms become more complicated, as well as web spam detection methods. So, web spam constantly evolves which makes the problem of web spam detection always relevant and challenging.As the most popular search engine in Russia Yandex faces the problem of web spam and has some expertise in this matter. This article describes our experience in detection different types of web spam based on content, links, clicks, and user behavior. We also review aggressive advertising and fraud because they affect the user experience. Besides, we demonstrate the connection between classic web spam and modern social engineering approaches in fraud.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1137–1140},
numpages = {4},
keywords = {aggressive advertizing, web spam detection, user behaviour},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@dataset{10.1145/review-2487788.2488135_R50242,
author = {Wang, De},
title = {Review ID:R50242 for DOI: 10.1145/2487788.2488135},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-2487788.2488135_R50242}
}

@inproceedings{10.1145/2487788.2488136,
author = {Venzhega, Andrei and Zhinalieva, Polina and Suboch, Nikolay},
title = {Graph-Based Malware Distributors Detection},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488136},
doi = {10.1145/2487788.2488136},
abstract = {Search engines are currently facing a problem of websites that distribute malware. In this paper we present a novel efficient algorithm that learns to detect such kind of spam. We have used a bipartite graph with two types of nodes, each representing a layer in the graph: web-sites and file hostings (FH), connected with edges representing the fact that a file can be downloaded from the hosting via a link on the web-site. The performance of this spam detection method was verified using two set of ground truth labels: manual assessments of antivirus analysts and automatically generated assessments obtained from antivirus companies. We demonstrate that the proposed method is able to detect new types of malware even before the best known antivirus solutions are able to detect them.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1141–1144},
numpages = {4},
keywords = {malware detection, webspam, graph mining, large data, search engine security},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488137,
author = {Shishkin, Alexander and Zhinalieva, Polina and Nikolaev, Kirill},
title = {Quality-Biased Ranking for Queries with Commercial Intent},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488137},
doi = {10.1145/2487788.2488137},
abstract = {Modern search engines are good enough to answer popular commercial queries with mainly highly relevant documents. However, our experiments show that users behavior on such relevant commercial sites may differ from one to another web-site with the same relevance label. Thus search engines face the challenge of ranking results that are equally relevant from the perspective of the traditional relevance grading approach. To solve this problem we propose to consider additional facets of relevance, such as trustability, usability, design quality and the quality of service. In order to let a ranking algorithm take these facets in account, we proposed a number of features, capturing the quality of a web page along the proposed dimensions. We aggregated new facets into the single label, commercial relevance, that represents cumulative quality of the site. We extrapolated commercial relevance labels for the entire learning-to-rank dataset and used weighted sum of commercial and topical relevance instead of default relevance labels. For evaluating our method we created new DCG-like metrics and conducted off-line evaluation as well as on-line interleaving experiments demonstrating that a ranking algorithm taking the proposed facets of relevance into account is better aligned with user preferences.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1145–1148},
numpages = {4},
keywords = {learning to rank, web search, relevance measures},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254968,
author = {Castillo, Carlos},
title = {Session Details: WEBQUALITY'13 Web Spam Detection Session},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254968},
doi = {10.1145/3254968},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488139,
author = {Garz\'{o}, Andr\'{a}s and Dar\'{o}czy, B\'{a}lint and Kiss, Tam\'{a}s and Sikl\'{o}si, D\'{a}vid and Bencz\'{u}r, Andr\'{a}s A.},
title = {Cross-Lingual Web Spam Classification},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488139},
doi = {10.1145/2487788.2488139},
abstract = {While Web spam training data exists in English, we face an expensive human labeling procedure if we want to filter a Web domain in a different language. In this paper we overview how existing content and link based classification techniques work, how models can be "translated" from English into another language, and how language-dependent and independent methods combine. In particular we show that simple bag-of-words translation works very well and in this procedure we may also rely on mixed language Web hosts, i.e. those that contain an English translation of part of the local language text. Our experiments are conducted on the ClueWeb09 corpus as the training English collection and a large Portuguese crawl of the Portuguese Web Archive. To foster further research, we provide labels and precomputed values of term frequencies, content and link based features for both ClueWeb09 and the Portuguese data.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1149–1156},
numpages = {8},
keywords = {web spam, content analysis, cross-lingual text processing, link analysis, web classification},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488140,
author = {Suhara, Yoshihiko and Toda, Hiroyuki and Nishioka, Shuichi and Susaki, Seiji},
title = {Automatically Generated Spam Detection Based on Sentence-Level Topic Information},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488140},
doi = {10.1145/2487788.2488140},
abstract = {Spammers use a wide range of content generation techniques with low quality pages known as content spam to achieve their goals. We argue that content spam must be tackled using a wide range of content quality features. In this paper, we propose novel sentence-level diversity features based on the probabilistic topic model. We combine them with other content features to build a content spam classifier. Our experiments show that our method outperforms the conventional methods.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1157–1160},
numpages = {4},
keywords = {topic model, spam feature, spam detection},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254969,
author = {Akerkar, Rajendra and Maret, Pierre and Vercouter, Laurent},
title = {Session Details: WI&amp;C'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254969},
doi = {10.1145/3254969},
abstract = {WI&amp;C'13 welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {2},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254970,
author = {Maret, Pierre},
title = {Session Details: WI&amp;C'13 Keynote Talk},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254970},
doi = {10.1145/3254970},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488143,
author = {Almeida, Virgilio},
title = {Exploring Very Large Data Sets from Online Social Networks},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488143},
doi = {10.1145/2487788.2488143},
abstract = {The explosion in the volume of digital data currently available in social networks has created new opportunities for scientific discoveries in the realm of social media. In particular, I show our recent progress in user preference understanding, data mining, summarization and explorative analysis of very large data sets. In information networks where users send messages to one another, the issue of information overload naturally arises: which are the most important messages? Based on a very large dataset with more 54 million user accounts and with all tweets ever posted by the collected users - more than 1.7 billion tweets, I discuss the problem of understanding the importance of messages in Twitter.In another work based on large-scale crawls of over 27 million user profiles that represented nearly 50% of the entire network in 2011, I show a detailed analysis of the Google+ social network. I discuss the key differences and similarities with other popular networks like Facebook and Twitter, in order to determine whether Google+ is a new paradigm or yet another social network.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1165–1166},
numpages = {2},
keywords = {data mining, social networks, big data, user preferences, content analysis},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254971,
author = {Maret, Pierre},
title = {Session Details: WI&amp;C'13 Session 1},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254971},
doi = {10.1145/3254971},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488145,
author = {Aggarwal, Suhas},
title = {Animated CAPTCHAs and Games for Advertising},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488145},
doi = {10.1145/2487788.2488145},
abstract = {In this paper, we discuss animated captcha systems which can be very useful for advertising. They are hardly any Animated Advertisement Captchas, available these days which are more secure than single image based Captchas and more fun as well. Some solutions are available such as yo!Captcha, NLPcaptcha. Solve Media TYPE-IN Captchas. These Captchas are single image based Captchas which ask users to type in Brand message to solve Captcha for brand recall. In this paper, we discuss some more appealing media which can be used for Captcha advertising. We also present Interactive Environment/Game Captcha which provide a more powerful medium for advertising. Finally, we showcase a Game with a purpose, named 'Pick Brands' which promote advertising and and can be used to obtain feedback/reviews, collect user questions concerning products/Advertisements.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1167–1174},
numpages = {8},
keywords = {game with a purpose for advertisements, advertisement captchas, interactive environments/game captchas, animation, visual effects, random captcha system, brand recall, secure captchas},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488146,
author = {Bell, Jonathan and Sheth, Swapneel and Kaiser, Gail},
title = {A Large-Scale, Longitudinal Study of User Profiles in World of Warcraft},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488146},
doi = {10.1145/2487788.2488146},
abstract = {We present a survey of usage of the popular Massively Multiplayer Online Role Playing Game, World of Warcraft. Players within this game often self-organize into communities with similar interests and/or styles of play. By mining publicly available data, we collected a dataset consisting of the complete player history for approximately six million characters, with partial data for another six million characters. The paper provides a thorough description of the distributed approach used to collect this massive community data set, and then focuses on an analysis of player achievement data in particular, exposing trends in play from this highly successful game. From this data, we present several findings regarding player profiles. We correlate achievements with motivations based upon a previously-defined motivation model, and then classify players based on the categories of achievements that they pursued. Experiments show players who fall within each of these buckets can play differently, and that as players progress through game content, their play style evolves as well.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1175–1184},
numpages = {10},
keywords = {virtual worlds, video games, world of warcraft, web information mining, user profiles},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488147,
author = {Pobiedina, Nataliia and Neidhardt, Julia and Calatrava Moreno, Maria del Carmen and Werthner, Hannes},
title = {Ranking Factors of Team Success},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488147},
doi = {10.1145/2487788.2488147},
abstract = {As an increasing number of human activities are moving to the Web, more and more teams are predominantly virtual. Therefore, formation and success of virtual teams is an important issue in a wide range of fields. In this paper we model social behavior patterns of team work using data from virtual communities. In particular, we use data about the Web community of the multiplayer online game Dota 2 to study cooperation within teams. By applying statistical analysis we investigate how and to which extent different factors of the team in the game, such as role distribution, experience, number of friends and national diversity, have an influence on the team's success. In order to complete the picture we also rank the factors according to their influence. The results of our study imply that cooperation within the team is better than competition.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1185–1194},
numpages = {10},
keywords = {national diversity, team formation, statistical analysis, virtual community, online game},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254972,
author = {Maret, Pierre},
title = {Session Details: WI&amp;C'13 Session 2},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254972},
doi = {10.1145/3254972},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {1},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488149,
author = {Pedro, Saulo D.S. and Appel, Ana Paula and Hruschka, Estevam R.},
title = {Autonomously Reviewing and Validating the Knowledge Base of a Never-Ending Learning System},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488149},
doi = {10.1145/2487788.2488149},
abstract = {The amount of information available on the Web has been increasing daily. However, how one might know what is right or wrong? Does the Web itself can be used as a source for verification of information? NELL (Never-Ending Language Learner) is a computer system that gathers knowledge from Web. Prophet is a link prediction component on NELL that has been successfully used to help populate its knowledge database. However, during link prediction task performance Prophet classify some edges as misplaced edges, that is, edges that we can not assure if they are right or not. In this paper we use the Web itself, using question answer (QA) systems, as a Prophet extension to validate these edges. This is an important issue when working with a self-supervised system where inserted errors might be propagate and generate dangerous concept drifting.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1195–1204},
numpages = {10},
keywords = {question answering, graph mining, active learning, never-ending-learning, anomaly link detection},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488150,
author = {Rana, Juwel and Morshed, Sarwar and Synnes, K\r{a}re},
title = {End-User Creation of Social Apps by Utilizing Web-Based Social Components and Visual App Composition},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488150},
doi = {10.1145/2487788.2488150},
abstract = {This paper presents a social component framework for the SatinII App Development Environment. The environment provides a systematic way of designing, developing and deploying personalized apps and enables end-users to develop their own apps without requiring prior knowledge of programming. A wide range of social components based on the framework have been deployed in the SatinII Editor, including components that utilize aggregated social graphs to automatically create groups or recommending/filtering information. The resulting social apps are web-based and target primarily mobile clients such as smartphones. The paper also presents a classification of social components and provides an initial user-evaluation with a small group of users. Initial results indicate that social apps can be built and deployed by end-users within 17 minutes on average after 20 to 30 minutes of being introduced to the SatinII Editor.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1205–1214},
numpages = {10},
keywords = {mobile social app, social data, component-based social app development, tools for social app development},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488151,
author = {Jelassi, Mohamed Nader and Ben Yahia, Sadok and Mephu Nguifo, Engelbert},
title = {A Personalized Recommender System Based on Users' Information in Folksonomies},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488151},
doi = {10.1145/2487788.2488151},
abstract = {Thanks to the high popularity and simplicity of folksonomies, many users tend to share objects (movies, songs, bookmarks, etc.) by annotating them with a set of tags of their own choice. Users represent the core of the system since they are both the contributors and the creators of the information. Yet, each user has its own profile and its own ideas making thereby the strength as well as the weakness of folksonomies. Indeed, it would be helpful to take account of users' profile when suggesting a list of tags and resources or even a list of friends, in order to make a more personal recommandation. The goal is to suggest tags (or resources) which may correspond to a user's vocabulary or interests rather than a list of most used and popular tags in folksonomies. In this paper, we consider users' profile as a new dimension of a folksonomy classically composed of three dimensions "users, tags, ressources" and we propose an approach to group users with equivalent profiles and equivalent interests as quadratic concepts. Then, we use quadratic concepts in order to propose our personalized recommendation system of users, tags and resources according to each user's profile. Carried out experiments on the large-scale real-world filmography dataset MovieLens highlight encouraging results in terms of precision.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1215–1224},
numpages = {10},
keywords = {folksonomy, recommender system, profile, users, quadratic concepts, precision},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254973,
author = {Mendes, Pablo and Rizzo, Giuseppe and Charton, Eric},
title = {Session Details: WOLE'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254973},
doi = {10.1145/3254973},
abstract = {WOLE'13 welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {4},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488154,
author = {Mika, Peter},
title = {Entity Search on the Web},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488154},
doi = {10.1145/2487788.2488154},
abstract = {More than the half of queries in the logs of a web search engine refer directly to a single named entity or a named set of entities [1]. To support entity search queries, search engines have begun developing targeted functionality, such as rich displays of factual information, question-answering and related entity recommendations. In this talk, we will provide an overview of recent work in the field of entity search, illustrated by the example of the Spark system, a large-scale system currently in use at Yahoo! for related entity recommendations in web search. Spark combines various knowledge bases and collects evidence from query logs and social media to provide the most relevant related entities for every web query with an entity intent. We discuss the methods used in Spark as well as how the system is evaluated in daily use.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1231–1232},
numpages = {2},
keywords = {semantics, web search, entity search, semantic web},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488156,
author = {Anderson, Neil and Hong, Jun},
title = {Visually Extracting Data Records from the Deep Web},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488156},
doi = {10.1145/2487788.2488156},
abstract = {Web sites that rely on databases for their content are now ubiquitous. Query result pages are dynamically generated from these databases in response to user-submitted queries. Automatically extracting structured data from query result pages is a challenging problem, as the structure of the data is not explicitly represented. While humans have shown good intuition in visually understanding data records on a query result page as displayed by a web browser, no existing approach to data record extraction has made full use of this intuition. We propose a novel approach, in which we make use of the common sources of evidence that humans use to understand data records on a displayed query result page. These include structural regularity, and visual and content similarity between data records displayed on a query result page. Based on these observations we propose new techniques that can identify each data record individually, while ignoring noise items, such as navigation bars and adverts. We have implemented these techniques in a software prototype, rExtractor, and tested it using two datasets. Our experimental results show that our approach achieves significantly higher accuracy than previous approaches. Furthermore, it establishes the case for use of vision-based algorithms in the context of data extraction from web sites.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1233–1238},
numpages = {6},
keywords = {data extraction, deep web data integration, vision-based data extraction},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488157,
author = {Gagnon, Michel and Zouaq, Amal and Jean-Louis, Ludovic},
title = {Can We Use Linked Data Semantic Annotators for the Extraction of Domain-Relevant Expressions?},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488157},
doi = {10.1145/2487788.2488157},
abstract = {Semantic annotation is the process of identifying expressions in texts and linking them to some semantic structure. In particular, Linked data-based Semantic Annotators are now becoming the new Holy Grail for meaning extraction from unstructured documents. This paper presents an evaluation of the main linked data-based annotators available with a focus on domain topics and named entities. In particular, we compare the ability of each tool to annotate relevant domain expressions in text. The paper also proposes a combination of annotators through voting methods and machine learning. Our results show that some linked-data annotators, especially Alchemy, can be considered as a useful resource for topic extraction. They also show that a substantial increase in recall can be achieved by combining the annotators with a weighted voting scheme. Finally, an interesting result is that by removing Alchemy from the combination, or by combining only the more precise annotators, we get a significant increase in precision, at the cost of a lower recall.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1239–1246},
numpages = {8},
keywords = {semantic annotation, topic extraction, evaluation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488158,
author = {Guha, Neel and Wytock, Matt},
title = {Course-Specific Search Engines: Semi-Automated Methods for Identifying High Quality Topic-Specific Corpora},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488158},
doi = {10.1145/2487788.2488158},
abstract = {Web search is an important research tool for many high school courses. However, generic search engines have a number of problems that arise out of not understanding the context of search (the high school course), leading to results that are off-topic or inappropriate as reference material. In this paper, we introduce the concept of a course-specific search engine and build such a search engine for the Advanced Placement US History (APUSH) course; the results of which are preferred by subject matter experts (high school teachers) over existing search engines. This reference search engine for APUSH relies on a hand-curated set of sites picked specifically for this educational context. In order to automate this expensive process, we describe two algorithms for indentifying high quality topical sites using an authoritative source such as a textbook: one based on textual similarity and another using structured data from knowledge bases. Initial experimental results indicate that these algorithms can successfully classify high quality documents leading to the automatic creation of topic-specific corpora for any course.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1247–1252},
numpages = {6},
keywords = {semantic web, knowledge bases, automation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488159,
author = {Haslhofer, Bernhard and Martins, Fl\'{a}vio and Magalh\~{a}es, Jo\~{a}o},
title = {Using SKOS Vocabularies for Improving Web Search},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488159},
doi = {10.1145/2487788.2488159},
abstract = {Knowledge organization systems such as thesauri or taxonomies are increasingly being expressed using the Simple Knowledge Organization System (SKOS) and published as structured data on the Web. Search engines can exploit these vocabularies and improve search by expanding terms at query or document indexing time. We propose a SKOS-based term expansion and scoring technique that leverages labels and semantic relationships of SKOS concept definitions. We also implemented this technique for Apache Lucene and Solr. Experiments with the Medical Subject Headings vocabulary and an early evaluation with Library of Congress Subject Headings indicated gains in precision when using SKOS-based expansion compared to pseudo relevance feedback and no expansion. Our findings are important for publishers and consumer of Web vocabularies who want to use them for improving search over Web documents.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1253–1258},
numpages = {6},
keywords = {skos, search, query expansion, linked data, thesauri},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488160,
author = {Jain, Paridhi and Kumaraguru, Ponnurangam and Joshi, Anupam},
title = {@i Seek 'Fb.Me': Identifying Users across Multiple Online Social Networks},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488160},
doi = {10.1145/2487788.2488160},
abstract = {An online user joins multiple social networks in order to enjoy different services. On each joined social network, she creates an identity and constitutes its three major dimensions namely profile, content and connection network. She largely governs her identity formulation on any social network and therefore can manipulate multiple aspects of it. With no global identifier to mark her presence uniquely in the online domain, her online identities remain unlinked, isolated and difficult to search. Literature has proposed identity search methods on the basis of profile attributes, but has left the other identity dimensions e.g. content and network, unexplored. In this work, we introduce two novel identity search algorithms based on content and network attributes and improve on traditional identity search algorithm based on profile attributes of a user. We apply proposed identity search algorithms to find a user's identity on Facebook, given her identity on Twitter. We report that a combination of proposed identity search algorithms found Facebook identity for 39% of Twitter users searched while traditional method based on profile attributes found Facebook identity for only 27.4%. Each proposed identity search algorithm access publicly accessible attributes of a user on any social network. We deploy an identity resolution system, Finding Nemo, which uses proposed identity search methods to find a Twitter user's identity on Facebook. We conclude that inclusion of more than one identity search algorithm, each exploiting distinct dimensional attributes of an identity, helps in improving the accuracy of an identity resolution process.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1259–1268},
numpages = {10},
keywords = {identity resolution, privacy, online social networks, digital footprint, identity search},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488161,
author = {Keller, Matthias and M\"{u}hlschlegel, Patrick and Hartenstein, Hannes},
title = {Search Result Presentation: Supporting Post-Search Navigation by Integration of Taxonomy Data},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488161},
doi = {10.1145/2487788.2488161},
abstract = {As a result of additional semantic annotations and novel mining methods, Web site taxonomies are more and more available to machines, including search engines. Recent research shows that after a search result is clicked, users often continue navigating on the destination site because in many cases a single document cannot satisfy the information need. The role Web site taxonomies play in this post-search navigation phase has not yet been researched. In this paper we analyze in an empirical study of three highly-frequented Web sites how Web site taxonomies influence the next browsing steps of users arriving from a search engine. The study reveals that users not randomly explore the destination site, but proceed to the direct child nodes of the landing page with significantly higher frequency compared to the other linked pages. We conclude that the common post-search navigation strategy in taxonomies is to descend towards more specific results. The study has interesting implications for the presentation of search results. Current search engines focus on summarizing the linked document only. In doing so, search engines ignore the fact the linked documents are in many cases just the starting point for further navigation. Based on the observed post-search navigation strategy, we propose to include information about child nodes of linked documents in the presentation of search results. Users would benefit by saving clicks, because they could not only estimate whether the linked document provides useful information, but also whether post-search navigation is promising.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1269–1274},
numpages = {6},
keywords = {taxonomies, clickstreams, search result presentation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488162,
author = {Murnane, Elizabeth L. and Haslhofer, Bernhard and Lagoze, Carl},
title = {RESLVE: Leveraging User Interest to Improve Entity Disambiguation on Short Text},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488162},
doi = {10.1145/2487788.2488162},
abstract = {We address the Named Entity Disambiguation (NED) problem for short, user-generated texts on the social Web. In such settings, the lack of linguistic features and sparse lexical context result in a high degree of ambiguity and sharp performance drops of nearly 50% in the accuracy of conventional NED systems. We handle these challenges by developing a model of user-interest with respect to a personal knowledge context; and Wikipedia, a particularly well-established and reliable knowledge base, is used to instantiate the procedure. We conduct systematic evaluations using individuals' posts from Twitter, YouTube, and Flickr and demonstrate that our novel technique is able to achieve substantial performance gains beyond state-of-the-art NED methods.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1275–1284},
numpages = {10},
keywords = {personalized ir, social web, user interest modeling, semantic knowledge graph, entity resolution},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488163,
author = {Orlando, Salvatore and Pizzolon, Francesco and Tolomei, Gabriele},
title = {SEED: A Framework for Extracting Social Events from Press News},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488163},
doi = {10.1145/2487788.2488163},
abstract = {Everyday people are exchanging a huge amount of data through the Internet. Mostly, such data consist of unstructured texts, which often contain references to structured information (e.g., person names, contact records, etc.). In this work, we propose a novel solution to discover social events from actual press news edited by humans. Concretely, our method is divided in two steps, each one addressing a specific Information Extraction (IE) task: first, we use a technique to automatically recognize four classes of named-entities from press news: DATE, LOCATION, PLACE, and ARTIST. Furthermore, we detect social events by extracting ternary relations between such entities, also exploiting evidence from external sources (i.e., the Web). Finally, we evaluate both stages of our proposed solution on a real-world dataset. Experimental results highlight the quality of our first-step Named-Entity Recognition (NER) approach, which indeed performs consistently with state-of-the-art solutions. Eventually, we show how to precisely select true events from the list of all candidate events (i.e., all the ternary relations), which result from our second-step Relation Extraction (RE) method. Indeed, we discover that true social events can be detected if enough evidence of those is found in the result list of Web search engines.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1285–1294},
numpages = {10},
keywords = {information extraction, relation extraction, social event discovery, named-entity recognition},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488164,
author = {Simonet, Vincent},
title = {Classifying YouTube Channels: A Practical System},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488164},
doi = {10.1145/2487788.2488164},
abstract = {This paper presents a framework for categorizing channels of videos in a thematic taxonomy with high precision and coverage. The proposed approach consists of three main steps.First, videos are annotated by semantic entities describing their central topics. Second, semantic entities are mapped to categories using a combination of classifiers.Last, the categorization of channels is obtained by combining the results of both previous steps.This framework has been deployed on the whole corpus of YouTube, in 8 languages, and used to build several user facing products. Beyond the description of the framework, this paper gives insight into practical aspects and experience: rationale from product requirements to the choice of the solution, spam filtering, human-based evaluations of the quality of the results, and measured metrics on the live site.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1295–1304},
numpages = {10},
keywords = {video, semantic entity, taxonomy classification},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254974,
author = {De Roure, David and Nejdl, Wolfgang},
title = {Session Details: WOW'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254974},
doi = {10.1145/3254974},
abstract = {WOW'13 welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {2},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488167,
author = {Booth, Paul and Gaskell, Paul and Hughes, Chris},
title = {The Economics of Data: Quality, Value &amp; Exchange in Web Observatories},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488167},
doi = {10.1145/2487788.2488167},
abstract = {The aim of this paper is to present a requirement for assessing the quality of data and the development of efficient methods of valuing and exchanging data among Web Observatories. Using economic and business theory a range of concepts are explored which include a brief review of existing business structures related to the exchange of goods, data or otherwise. The paper calls for a wider discussion by the Web Observatory community to begin to define relevant criteria by which data can be assessed and improved over time. The economic incentives are addressed as part of a price by proxy framework we introduce, which is supported by the need to strive for clear pricing signals and the reduction of information asymmetries. What is presented here is a way of establishing and improving data quality with a view to valuing data exchanges that does not require the presence of money in the transaction, yet it remains tied to revenue generation models as they exist online.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1309–1316},
numpages = {8},
keywords = {valuation, transaction costs, data exchange, web observatory, data quality},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488168,
author = {Brown, Ian and Hall, Wendy and Harris, Lisa},
title = {From Search to Observation},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488168},
doi = {10.1145/2487788.2488168},
abstract = {In this paper, we propose a set of concepts underlying the process and requirements of observation: that is, the process of employing web observatories for research. We refer to observation as a new concept, distinct from search, which we believe is worthy of study in its own right and note that the process of observation moves the focus of information retrieval away from universal coverage and towards improved quality of results and thus has many potential facets not necessarily present in traditional search.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1317–1320},
numpages = {4},
keywords = {web science, observatory models, web observatory},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488169,
author = {Diaz-Aviles, Ernesto},
title = {Living Analytics Methods for the Web Observatory},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488169},
doi = {10.1145/2487788.2488169},
abstract = {The collective effervescence of social media production has been enjoying a great deal of success in recent years. The hundred of millions of users who are actively participating in the Social Web are exposed to ever-growing amounts of sites, relationships, and information.In this paper, we report part of the efforts towards the realization of a Web Observatory at the L3S Research Center (www.L3S.de). In particular, we present our approach based on Living Analytics methods, whose main goal is to capture people interactions in real-time and to analyze multidimensional relationships, metadata, and other data becoming ubiquitous in the social web, in order to discover the most relevant and attractive information to support observation, understanding and analysis of the Web. We center the discussion on two areas: (i) Recommender Systems for Big Fast Data and (ii) Collective Intelligence, both key components towards an analytics toolbox for our Web Observatory.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1321–1324},
numpages = {4},
keywords = {recommender systems, collective intelligence, big fast data, web observatory},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488170,
author = {Gloria, Marie Joan Kristine and McGuinness, Deborah L. and Luciano, Joanne S. and Zhang, Qingpeng},
title = {Exploration in Web Science: Instruments for Web Observatories},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488170},
doi = {10.1145/2487788.2488170},
abstract = {The following contribution highlights selected work conducted by Rensselaer Polytechnic Institute's Web Science Research Center. (RPI WSRC). Specifically, it brings to light four different themed Web Observatories - Science Data, Health and Life Sciences, Open Government, and Social Spaces. Each of these observatories serves as a repository of data, tools, and methods that help answer complicated questions in each of these research areas. We present six case studies featuring tools and methods developed by RPI WSRC to aide in the exploration, discovery, and analysis of large data sets. These case studies along with our web observatory developments are aimed to increase our understanding of web science in general and to serve as test beds for our research.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1325–1328},
numpages = {4},
keywords = {linked data, methods, web observatory, semantic technologies},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488171,
author = {Jain, Ramesh and Jalali, Laleh and Fan, Mingming},
title = {From Health-Persona to Societal Health},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488171},
doi = {10.1145/2487788.2488171},
abstract = {In this position paper, we propose an approach for Web Observatories that builds on using social media, personal data, and sensors to build Persona for an individual, but also use this data and the concept of Focused Micro Blogs (FMB) for situation detection, helping individual using situation action rules, and finally gaining insights for obtaining insights about society. We demonstrate this in a concrete use case of fitness and health care related sensors for building health persona and using this for understanding societal health issues.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1329–1334},
numpages = {6},
keywords = {focused micro blogs, eventshop, situation detection, event streams, health and fitness, persona},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488172,
author = {Kanhabua, Nattiya and Nejdl, Wolfgang},
title = {Understanding the Diversity of Tweets in the Time of Outbreaks},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488172},
doi = {10.1145/2487788.2488172},
abstract = {A microblogging service like Twitter continues to surge in importance as a means of sharing information in social networks. In the medical domain, several works have shown the potential of detecting public health events (i.e., infectious disease outbreaks) using Twitter messages or tweets. Given its real-time nature, Twitter can enhance early outbreak warning for public health authorities in order that a rapid response can take place. Most of previous works on detecting outbreaks in Twitter simply analyze tweets matched disease names and/or locations of interests. However, the effectiveness of such method is limited for two main reasons. First, disease names are highly ambiguous, i.e., referring slangs or non health-related contexts. Second, the characteristics of infectious diseases are highly dynamic in time and place, namely, strongly time-dependent and vary greatly among different regions. In this paper, we propose to analyze the temporal diversity of tweets during the known periods of real-world outbreaks in order to gain insight into a temporary focus on specific events. More precisely, our objective is to understand whether the temporal diversity of tweets can be used as indicators of outbreak events, and to which extent. We employ an efficient algorithm based on sampling to compute the diversity statistics of tweets at particular time. To this end, we conduct experiments by correlating temporal diversity with the estimated event magnitude of 14 real-world outbreak events manually created as ground truth. Our analysis shows that correlation results are diverse among different outbreaks, which can reflect the characteristics (severity and duration) of outbreaks.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1335–1342},
numpages = {8},
keywords = {event detection, temporal diversity, outbreak events, web observatory, twitter},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488173,
author = {Kunegis, J\'{e}r\^{o}me},
title = {KONECT: The Koblenz Network Collection},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488173},
doi = {10.1145/2487788.2488173},
abstract = {We present the Koblenz Network Collection (KONECT), a project to collect network datasets in the areas of web science, network science and related areas, as well as provide tools for their analysis. In the cited areas, a surprisingly large number of very heterogeneous data can be modeled as networks and consequently, a unified representation of networks can be used to gain insight into many kinds of problems. Due to the emergence of the World Wide Web in the last decades many such datasets are now openly available. The KONECT project thus has the goal of collecting many diverse network datasets from the Web, and providing a way for their systematic study. The main parts of KONECT are (1) a collection of over 160 network datasets, consisting of directed, undirected, unipartite, bipartite, weighted, unweighted, signed and temporal networks collected from the Web, (2) a Matlab toolbox for network analysis and (3) a website giving a compact overview the various computed statistics and plots. In this paper, we describe KONECT's taxonomy of networks datasets, give an overview of the datasets included, review the supported statistics and plots, and briefly discuss KONECT's role in the area of web science and network science.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1343–1350},
numpages = {8},
keywords = {web observatory, network analysis},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488174,
author = {McKelvey, Karissa and Menczer, Filippo},
title = {Design and Prototyping of a Social Media Observatory},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488174},
doi = {10.1145/2487788.2488174},
abstract = {The broad adoption of online social networking platforms has made it possible to study communication networks at an unprecedented scale. With social media and micro-blogging platforms such as Twitter, we can observe high-volume data streams of online discourse. However, it is a challenge to collect, manage, analyze, visualize, and deliver large amounts of data, even by experts in the computational sciences. In this paper, we describe our recent extensions to Truthy, a social media observatory that collects and analyzes discourse on Twitter dating from August 2010. We introduce several interactive visualizations and analytical tools with the goal of enabling researchers to study online social networks with mixed methods at multiple scales. We present design considerations and a prototype for integrating social media observatories as important components of a web observatory framework.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1351–1358},
numpages = {8},
keywords = {api, web observatory, visualization, resource data management, social media observatory},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488175,
author = {Pongpaichet, Siripen and Singh, Vivek K. and Gao, Mingyan and Jain, Ramesh},
title = {EventShop: Recognizing Situations in Web Data Streams},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488175},
doi = {10.1145/2487788.2488175},
abstract = {Web Observatories must address fundamental societal challenges using enormous volumes of data being created due to the significant progress in technology. The proliferation of heterogeneous data streams generated by social media, sensor networks, internet of things, and digitalization of transactions in all aspect of humans? life presents an opportunity to establish a new era of networks called Social Life Networks (SLN). The main goal of SLN is to connect People to Resources effectively, efficiently, and promptly in given Situations. Towards this goal, we present a computing framework, called EventShop, to recognize evolving situations from massive web streams in real-time. These web streams can be fundamentally considered as spatio-temporal-thematic streams and can be combined using a set of generic spatio-temporal analysis operators to recognize evolving situations. Based on the detected situations, the relevant information and alerts can be provided to both individuals and organizations. Several examples from the real world problems have been developed to test the efficacy of EventShop framework.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1359–1368},
numpages = {10},
keywords = {web data streams, eventshop, web observatory, social life networks, events processing framework, situation recognition},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488176,
author = {Seyed, A. Patrice and Lebo, Tim and Patton, Evan and McCusker, Jim and McGuinness, Deborah},
title = {SemantEco: A next-Generation Web Observatory},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488176},
doi = {10.1145/2487788.2488176},
abstract = {A web observatory for empirical research of Web data benefits from software frameworks that are modular, has a clear underlying semantic model, and that includes metadata enabling a trace and inspection of the source data and justifications for derived datasets. We present SemantEco as an architecture that can serve as an exemplar abstraction for infrastructure design and metadata based on best practices in Semantic Web, Provenance, and Software Engineering, that can be employed in any Web Observatory, that may grow out of a community. We will describe how the SemantEco framework allows for searching, visualizing, and tracing a wide variety of data.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1369–1372},
numpages = {4},
keywords = {environmental sciences, provenance, semantic web},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488177,
author = {Tinati, Ramine and Tiropanis, Thanassis and Carr, Lesie},
title = {An Approach for Using Wikipedia to Measure the Flow of Trends across Countries},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488177},
doi = {10.1145/2487788.2488177},
abstract = {Wikipedia has grown to become the most successful online encyclopedia on the Web, containing over 24 million articles, offered in over 240 languages. In just over 10 years Wikipedia has transformed from being just an encyclopedia of knowledge, to a wealth of facts and information, from articles discussing trivia, political issues, geographies and demographics, to popular culture, news articles, and social events. In this paper we explore the use of Wikipedia for identifying the flow of information and trends across the world. We start with the hypothesis that, given that Wikipedia is a resource that is globally available in different languages across countries, access to its articles could be a reflection human activity. To explore this hypothesis we try to establish metrics on the use of Wikipedia in order to identify potential trends and to establish whether or how those trends flow from one county to another. We subsequently compare the outcome of this analysis to that of more established methods that are based on online social media or traditional media. We explore this hypothesis by applying our approach to a subset of Wikipedia articles and also a specific worldwide social phenomenon that occurred during 2012; we investigate whether access to relevant Wikipedia articles correlates to the viral success of the South Korean pop song, "Gangnam Style" and the associated artist "PSY" as evidenced by traditional and online social media. Our analysis demonstrates that Wikipedia can indeed provide a useful measure for detecting social trends and events, and in the case that we studied; it could have been possible to identify the specific trend quicker in comparison to other established trend identification services such as Google Trends.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1373–1378},
numpages = {6},
keywords = {wikipedia, web science, web observatories, social machines},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488178,
author = {Trestian, Ionut and Xiao, Chunjing and Kuzmanovic, Aleksandar},
title = {A Glance at an Overlooked Part of the World Wide Web},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488178},
doi = {10.1145/2487788.2488178},
abstract = {Although according to surveys related to internet user activity it is considered one of the most popular aspects, few studies are actually concerned with internet pornography. This paper is aimed at rectifying that overlook. In particular, we study user activity related to internet pornography by looking at two main behaviors: (i) watching pornography, and (ii) providing feedback on pornography items in the form of ratings and comments.By using appropriate datasets that we collect, we make contributions related to the study of both behaviors pointed out above. With regards to viewing, we observe that views are highly dependent on pornography category and video size. By studying the feedback system of pornography video websites, we observe differences in the way users rate items across websites popular in different parts of the world. Finally, we employ sentiment analysis to study the comments that users leave on pornography websites and we find surprising similarities across the analyzed websites. Our results pave the way to understanding more about human behavior related to internet pornography and can impact, among others, fields such as content personalization, video content delivery, recommender systems},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1379–1386},
numpages = {8},
keywords = {internet behavior, search behavior, sentiment analysis},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3254975,
author = {Alarcon, Rosa and Pautasso, Cesare and Wilde, Erik},
title = {Session Details: WS-REST'13 Workshop},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254975},
doi = {10.1145/3254975},
abstract = {WS-REST'13 welcome and organization},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
numpages = {3},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488181,
author = {Gulden, Markus and Kugele, Stefan},
title = {A Concept for Generating Simplified RESTful Interfaces},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488181},
doi = {10.1145/2487788.2488181},
abstract = {Today, innovative companies are forced to evolve their software systems faster and faster, either for providing customer services and products or for supporting internal processes. At the same time, already existing, maybe even legacy systems are crucial for different reasons and by that cannot be abolished easily. While integrating legacy software into new systems in general is considered by well-known approaches like SOA (service-oriented architecture), at the best of our knowledge, it lacks of ways to make legacy systems available for remote clients like smart phones or embedded devices.In this paper, we propose an approach to leverage heterogeneous (legacy) applications by adding RESTful web-based interfaces in a model-driven way. We introduce an additional application layer, which encapsulates services of one or several existing applications, and provides a unified, web-based, and seamless interface. This interface is modelled in our own DSL (domain-specific language), the belonging code generator produces productive Java code. Finally, we report on an case study proving our concept by means of an e-bike sharing service.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1391–1398},
numpages = {8},
keywords = {representational state transfer, service-orientated architecture, domain-specific language, data transfer object},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488182,
author = {Verborgh, Ruben and Hausenblas, Michael and Steiner, Thomas and Mannens, Erik and Van de Walle, Rik},
title = {Distributed Affordance: An Open-World Assumption for Hypermedia},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488182},
doi = {10.1145/2487788.2488182},
abstract = {Hypermedia links and controls drive the Web by transforming information into affordances through which users can choose actions. However, publishers of information cannot predict all actions their users might want to perform and therefore, hypermedia can only serve as the engine of application state to the extent the user's intentions align with those envisioned by the publisher. In this paper, we introduce distributed affordance, a concept and architecture that extends application state to the entire Web. It combines information inside the representation with knowledge of action providers to generate affordance from the user's perspective. Unlike similar approaches such as Web Intents, distributed affordance scales both in the number of actions and the number of action providers, because it is resource-oriented instead of action-oriented. A proof-of-concept shows that distributed affordance is a feasible strategy on today's Web.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1399–1406},
numpages = {8},
keywords = {distributed systems, rest, affordance, web, hypermedia},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488183,
author = {Panziera, Luca and De Paoli, Flavio},
title = {A Framework for Self-Descriptive RESTful Services},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488183},
doi = {10.1145/2487788.2488183},
abstract = {REST principles define services as resources that can be manipulated by a set of well-known methods. The same approach is suitable to define service descriptions as resources. In this paper, we try to unify the two concepts (services and their descriptions) by proposing a set of best practices to build self-descriptive RESTful services accessible by both humans and machines. Moreover, to make those practices usable with little manual effort, we provide a software framework that extracts compliant descriptions from documents published on the Web, and makes them available to clients as resources.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1407–1414},
numpages = {8},
keywords = {web services, information extraction, rest, service description, semantic web},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2487788.2488184,
author = {Lanthaler, Markus and G\"{u}tl, Christian},
title = {Model Your Application Domain, Not Your JSON Structures},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488184},
doi = {10.1145/2487788.2488184},
abstract = {Creating truly RESTful Web APIs is still more an art than a science. Developers have to struggle with a number of complex design decisions because concrete guidelines and processes are missing. Consequently, often it is decided to implement the simplest solution which is, most of the time, to rely on out-of-band contracts between the client and the server. Instead of properly modeling the application domain, all the effort is put in the design of proprietary JSON structures and URLs. This then forms the base for the contract which is communicated in natural-language (with all its ambiguity) to client developers. Since it is the server who owns the contract it may be changed at any point, which, more often than not, results in broken clients. In this position paper, we discuss some of the challenges and choices that need to be made when designing RESTful Web APIs. In particular, we compare how contracts are supposed to be established and how they are defined in practice. We illustrate the problems that are the cause of these divergences. As a first step to address these issues we describe and motivate an alternative, domain-driven approach to design Web APIs.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1415–1420},
numpages = {6},
keywords = {web services, json-ld, hydra, web apis, web, http, domain-driven design, rest, linked data, distributed systems, contracts},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

