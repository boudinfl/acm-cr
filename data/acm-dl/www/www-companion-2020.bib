@inproceedings{10.1145/3366424.3382670,
author = {Tiwary, Mayank and Mishra, Pritish and Jain, Shashank and Puthal, Deepak},
title = {Data Aware Web-Assembly Function Placement},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382670},
doi = {10.1145/3366424.3382670},
abstract = {Existing container based serverless computing systems are limited by cold-start problems and complex architecture for stateful services, multi-tenancy, etc. This paper presents serverless functions to be placed as per data locality and executed as a web-assembly sandbox, which results better execution latency and reduced network usage as compared to the existing architectures. The designed serverless runtime features resource isolation in terms of CPU, Memory, and file-system isolation and falicitates multi-tenancy executions. The proposed architecture is evaluated using IoT workloads with different performance metrics.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {4–5},
numpages = {2},
keywords = {Servelress, Web-Assembly, Multi-Tenancy, Data Locality},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382671,
author = {Song, Shuangyong and Wang, Chao and Xie, Qianqian and Zu, Xinxing and Chen, Huan and Chen, Haiqing},
title = {A Two-Stage Conversational Query Rewriting Model with Multi-Task Learning},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382671},
doi = {10.1145/3366424.3382671},
abstract = {Conversational context understanding aims to recognize the real intention of user from the conversation history, which is critical for building the dialogue system. However, the multi-turn conversation understanding in open domain is still quite challenging, which requires the system extracting the important information and resolving the dependencies in contexts among a variety of open topics. In this paper, we propose the conversational query rewriting model, reformulating the multi-turn conversational queries into a single turn query, which conveys the true intention of users concisely and alleviates the difficulty of the multi-turn dialogue modeling. In the model, we formulate the query rewriting as a sequence generation problem and introduce word category information via the auxiliary word category label predicting task. To train our model, we construct a new Chinese query rewriting dataset and conduct experiments on it. The experimental results show that our model outperforms compared models, and prove the effectiveness of the word category information in improving the rewriting performance.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {6–7},
numpages = {2},
keywords = {Query rewriting, QA system, sequence labeling, Multi-task learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382672,
author = {Bao, Peng and Wang, Jiahui},
title = {Metapath-Guided Credit Allocation for Identifying Representative Works},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382672},
doi = {10.1145/3366424.3382672},
abstract = {The ability to identify the representative works of a researcher has important implications in a wide range of areas, including hiring, funding, and promotion systems. In this paper, we propose a metapath-guided credit allocation method (MGCA) for identifying the representative works of individual researchers. MGCA utilizes metapath-guided neighbours to exploit rich semantic information in heterogeneous information network for locating a researcher’s field of expertise, and explicitly captures the importance of a paper, its relevance to other papers, and the unequally distributed contribution of each citation via a two-step credit allocation. We validate MGCA by applying it on the American Physical Society dataset in the scenario of identifying the Nobel prize winning papers of the Nobel laureates. Experiments demonstrate that the proposed method can significantly outperform the existing approaches.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {8–9},
numpages = {2},
keywords = {credit allocation, representative work, metapath},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382673,
author = {Rossi, Ryan A. and Ahmed, Nesreen K. and Koh, Eunyee and Kim, Sungchul},
title = {Fast Hierarchical Graph Clustering in Linear-Time},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382673},
doi = {10.1145/3366424.3382673},
abstract = {While there has been a lot of research on graph clustering (community detection), most work (i) does not address the hierarchical community detection problem or are (ii) inefficient for large networks. In this work, we describe an approach called hLP that addresses both these limitations. Notably, hLP is fast and efficient for discovering a hierarchy of communities in large networks with a worst-case time and space complexity that is linear in the number of edges and nodes, respectively. The experiments demonstrate the effectiveness of hLP. Finally, we show an application for visualizing large networks with hundreds of thousands of nodes and edges.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {10–12},
numpages = {3},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382674,
author = {Song, Shuangyong and Wang, Chao and Chen, Haiqing and Chen, Huan},
title = {Supervised Term Weight Training for Improving Question-Knowledge Matching in Chatbots},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382674},
doi = {10.1145/3366424.3382674},
abstract = {We design several supervised methods of training term weights for improving results of question-knowledge matching task on a chatbot system-AliMe. Considering demand of high QPS (Query Per Second), we take a traditional regression model as the matching model rather than deep models.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {13–14},
numpages = {2},
keywords = {Semantic matching, Question answering, Chatbot},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382675,
author = {Song, Shuangyong and Wang, Chao and Chen, Haiqing and Chen, Huan},
title = {A Model for Quality Control of Customer Service},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382675},
doi = {10.1145/3366424.3382675},
abstract = {In E-commerce industry, customer service is with a very high quality, because of comprehensive and strict training of customer service staffs. With statistics on manual sampling results in abundant history logs, just about 0.2% answers of customer service staffs show attitudinal problems. Furthermore, even for those 0.2% answers, they can hardly be detected by some obvious rules. Therefore, present manual sampling method is very inefficient and time-consuming. In this paper we proposed a problem detection model for quality control of customer service. Experimental results on a dataset from a real-world industrial customer service Alime Assist show the model’s effect.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {15–16},
numpages = {2},
keywords = {Context-based classification, Quality control},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382676,
author = {Chu, Jui and Chen, Chung-Chi and Huang, Hen-Hsen and Chen, Hsin-Hsi},
title = {Learning to Generate Correct Numeric Values in News Headlines},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382676},
doi = {10.1145/3366424.3382676},
abstract = {Motivated by the significant role of numeric values to convey concise and accurate information in news headlines, we focus the headline generation task on displaying correct numbers. We propose various ways to present the numeric values to the generative model. In the end, we come up with a simple but effective pre-train task to guide the generator to correctly process the values, which outperforms other base models even if the numbers in the headline are newly generated from the article.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {17–18},
numpages = {2},
keywords = {numeracy, headline generation, Summarization},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382677,
author = {Huang, Zhenhua and Wang, Zhenyu and Zhang, Rui and Zhao, Yangyang and Zheng, Fadong},
title = {Learning Bi-Directional Social Influence in Information Cascades Using Graph Sequence Attention Networks},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382677},
doi = {10.1145/3366424.3382677},
abstract = {The online information cascades have spatial and temporal characteristics. Retweets in cascades have bi-directional social influence and temporal delay, e.g., a hub node influenced by a root node will further increase the exposure of information, enhance the influence of the root node, and cause subsequent retweets after a certain time. Existing deep learning approaches mostly consider only the decay effects of social influence, ignoring the bi-directional dependency and delay effects between graphs at different moments. Therefore, a novel method is presented here, namely the Graph Sequence Attention Networks (GSAN) , which addresses the bi-directional attention mechanism to learn temporal dynamics of social influence, as well as the cascading structure. A graph transformer block is designed to learn the complicated dependencies of spatial and temporal features in cascades. The proposed method could achieve state-of-the-art performance and large improvements in popularity prediction compared to strong baselines in real-world datasets. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {19–21},
numpages = {3},
keywords = {social influence, information cascades, popularity prediction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382678,
author = {Li, Quanzhi and Zhang, Qiong},
title = {Abstractive Event Summarization on Twitter},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382678},
doi = {10.1145/3366424.3382678},
abstract = {This paper presents a new approach for automatically summarizing a social media event. It utilizes the BERT model as the encoder and a Transformer architecture as the decoder. The framework also includes an event topic prediction component, and the predicted event topic will help the decoder focus more on the specific aspects of the topic category when generating summary. To make the summary more succinct and coherent, the most important messages from an event cluster are selected by a message selection model and encoded by the BERT model. Our preliminary experiment shows that our approach outperforms the baseline methods.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {22–23},
numpages = {2},
keywords = {event topic, Twitter, BERT, social media, event summary},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382679,
author = {Yu, Dung-Ru and Chu, Chiao-Chuan and Lai, Hsu-Chao and Huang, Jiun-Long},
title = {Social Attentive Network for Live Stream Recommendation},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382679},
doi = {10.1145/3366424.3382679},
abstract = {Live streaming platforms not only provide live videos but also allow social interactions between viewers via real-time chatting. However, none of existing research has studied the social impact for recommending live streams. In this work, we formulate a new personalized recommendation problem by factoring in both video and social contents (chats). Accordingly, we 1) design a new attention network ANSWER to identify viewers’ attention on video and social contents, and 2) rank the channels based on the attentive features. We collect a real dataset from Twitch for evaluation. The experimental results manifest that ANSWER outperforms baselines by at least 26.6% in terms of NDCG@5.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {24–25},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382680,
author = {Liu, Feng and Guo, Wei and Guo, Huifeng and Tang, Ruiming and Ye, Yunming and He, Xiuqiang},
title = {Dual-Attentional Factorization-Machines Based Neural Network for User Response Prediction},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382680},
doi = {10.1145/3366424.3382680},
abstract = {This paper proposes Dual-attentional Factorization-Machines (DFM), which incorporates global-wise attention and element-wise attention with FM for user response prediction. We further extend DFM with a deep neural network and name this new model Dual-attentional Factorization-machines based Network (DFNet). Comprehensive experiments are conducted on two real-world datasets to demonstrate the effectiveness of DFM and DFNet over the state-of-the-art models for user response prediction.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {26–27},
numpages = {2},
keywords = {Factorization-Machines, Feature Interactions, Attention Network},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382681,
author = {Shraga, Roee and Roitman, Haggai and Feigenblat, Guy and Weiner, Bar},
title = {Projection-Based Relevance Model for Table Retrieval},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382681},
doi = {10.1145/3366424.3382681},
abstract = {We propose a novel relevance model for table-retrieval using table columns (“projections”) as pseudo-relevance feedback. To demonstrate the merits of our proposed approach, we re-rank tables that were initially retrieved from a Wikipedia corpus by state-of-the-art table retrieval methods. Overall, our proposed projection-based relevance model (PTRM) results in significant performance gains.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {28–29},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382682,
author = {Lu Jia, Adele and Rao, Yuanxing and Li, Hongru and Tian, Ran and Shen, Siqi},
title = {Revealing Donation Dynamics in Social Live Video Streaming},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382682},
doi = {10.1145/3366424.3382682},
abstract = {Social live video streaming has become a global economic and social phenomenon with the rise of platforms like Facebook-Live, Youtube-Live, and Twitch. The phenomenon of user donation in these communities is rapidly emerging, towards which however we have very limited understandings. In this preliminary work, we reveal the dynamics of user donations based on a publicly available (anonymized) dataset with detailed information on over 2 million users and worth in total over 200 million US dollars. Among other results, we find that (i) both the donations received and the donations made are highly skewed, (ii) user donation is strongly correlated with the atmosphere (the volume and the sentiment of real-time user chats) and in the long run, the loss of broadcasters, and (iii) donors are loyal and very generous to their favorite broadcasters while in the mean time they also support others moderately. Our findings represent a first step towards understanding user donations which will shed lights on the donor retention problem and the design of social live video streaming services. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {30–31},
numpages = {2},
keywords = {User Donation, User Behavior, Social Live Video Streaming},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382683,
author = {Rossi, Ryan A. and Rao, Anup and Mai, Tung and Ahmed, Nesreen K.},
title = {Fast and Accurate Estimation of Typed Graphlets},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382683},
doi = {10.1145/3366424.3382683},
abstract = {Typed graphlets are small typed (labeled, colored) induced subgraphs and were recently shown to be the fundamental building blocks of rich complex heterogeneous networks. In many applications, speed is more important than accuracy, and it is sufficient to trade-off a tiny amount of accuracy for a significantly faster method. In this work, we propose fast and accurate estimators for typed graphlets. The typed graphlet estimation techniques naturally support general heterogeneous graphs with any arbitrary number of types, which include bipartite, k-partite, k-star, labeled graphs, and attributed networks as special cases. The experiments demonstrate the effectiveness of the typed graphlet estimation techniques.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {32–34},
numpages = {3},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382685,
author = {Fang, Zhihan and Wang, Guang and Zhang, Desheng},
title = {Modeling Fine-Grained Human Mobility on Cellular Networks},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382685},
doi = {10.1145/3366424.3382685},
abstract = {Cellular network data has been proved as one of the most promising ways to understand large-scale human mobility due to its high penetration of cellphones and low collection cost. Most existing mobility models driven by cellular network data are based on either CDR (Call Detail Records) or data connection records. However, estimated mobility is biased with coarse granularities due to the insufficient data quality. Mobility modeling on cellular networks always suffer from the sparse spatial-temporal observations since user locations are recorded with cellphone activities. In this paper, to solve the issue, we design a system named FineCell to model fine-grained human mobility based on sparse cellular network data. The key challenge we address in FineCell is to achieve fine-grained mobility modeling with sparse cellular network data. In contrast to the existing works on human mobility, the novelty of the FineCell is to infer missing spatial and temporal observations caused by sensing gaps in cellular networks. More importantly, we evaluate FineCell with large-scale fine-grained ground truth data from the same cellular network. The evaluation results show FineCell achieve 9.8% lower error compared with state-of-the-art models.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {35–37},
numpages = {3},
keywords = {Fine Grained, Human Mobility, Location Inference},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382686,
author = {Wang, Caleb and Chen, Hsi and Huang, Polly},
title = {Towards Cost Effective Server Population Estimation: A Case Study of Twitch},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382686},
doi = {10.1145/3366424.3382686},
abstract = {Internet services such as email and video streaming are daily necessity for many of us. To scale to the demand of service quality and continuity, major services today operate on server clusters. Growingly large, long-lasting, and dynamic, these server clusters are pretty much ecosystems themselves. Drawing from the observation, we see a window of opportunity applying techniques in ecology, e.g., Capture-Mark-Recapture (i.e., CMR) for server population estimation, a problem fundamental to cluster monitoring.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {38–39},
numpages = {2},
keywords = {Server Cluster, Network Measurement, Population Estimation, Twitch},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382687,
author = {Qin, Xiaoyu and Zhang, Xiaowang and Feng, Zhiyong},
title = {Optimizing Ontology Materialization with Equivalent Role and Inverse Role Rewriting},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382687},
doi = {10.1145/3366424.3382687},
abstract = {Materialization, as a basic approach to ontology-mediated query answering, extends an ontology by exploiting the semantic knowledge expressed in the ontology to improve query answering over data. However, those existing materialization approaches are often inefficient, especially when ontologies consist of complex roles. In this paper, we propose a new rewriting algorithm for equivalent roles and inverse roles to both improve materialization efficiency and reduce memory consumption. Moreover, we develop a role scoring algorithm to optimize role rewriting. The preliminary experiments show that our approach can significantly reduce materialization times and memory consumption.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {40–41},
numpages = {2},
keywords = {Role rewriting, Ontology-mediated querying, Role scoring},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382688,
author = {Rossi, Ryan A. and Rao, Anup and Kim, Sungchul and Koh, Eunyee and Ahmed, Nesreen},
title = {From Closing Triangles to Closing Higher-Order Motifs},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382688},
doi = {10.1145/3366424.3382688},
abstract = {This work introduces higher-order ranking and link prediction methods based on closing higher-order network motifs. In particular, we propose the general notion of a motif closure that goes beyond simple triangle closures and demonstrate that these new motif closures often outperform triangle-based methods. This result implies that one should consider other motif closures beyond simple triangles. We also find that the “best” motif closure depends highly on the underlying network and its structural properties. Furthermore, the methods are fast and efficient for real-time applications such as online visitor stitching, web search, and recommendation. The experimental results indicate the importance of these new motif closures. Finally, the new motif closures can serve as a basis for developing better (un)supervised ranking/link prediction methods.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {42–43},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382689,
author = {Yue, Yi and Cheng, Bo and Liu, Xuan and Wang, Meng and Li, Biyi},
title = {Dynamic VNF Placement for Mapping Service Function Chain Requests in NFV-Enabled Networks},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382689},
doi = {10.1145/3366424.3382689},
abstract = {Network function virtualization (NFV) brings significant performance and management benefits for the enterprises to place virtual network functions (VNFs) while reducing the operating expenses and capital expenditures. In this paper, we study the dynamic VNF placement problem for mapping users’ service function requests (SFCRs) in NFV-enabled networks. The problem is formulated as a heuristic cost function aiming at minimizing the cost of mapping each SFCR. Then we propose SFCR-Mapping and Dynamic VNF-Releasing algorithms to efficiently map SFCRs and timely release the redundant VNF instances. The simulation results demonstrate that our method efficiently reduces the total activated VNF instances and improves VNF utilization compared with the benchmarks. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {44–45},
numpages = {2},
keywords = {resource optimization, Dynamic VNF placement},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382690,
author = {Imani, Shima and Keogh, Eamonn},
title = {Natura: Towards Conversational Analytics for Comparing and Contrasting Time Series},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382690},
doi = {10.1145/3366424.3382690},
abstract = {Comparing and contrasting two things is one of the most basic ideas in science. While existing tools allow such comparisons for pairs (e.g. texts, trees, graphs, histograms), there are currently no such tools for time series data. This is somewhat surprising, given the ubiquity of time series data in modern life. One could imagine a tool for comparing and contrasting two time series by reporting various summary statistics. However, scientists, engineers, and physicians typically communicate such findings in natural language. In this work we present Natura, a domain agnostic natural language framework for comparing and contrasting two time series, that aims to duplicate this human skill. Through case study, we demonstrate the effectiveness and utility of our framework.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {46–47},
numpages = {2},
keywords = {Natural language, Contrasting time series, Time series, Conversational analytics},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382691,
author = {Huang, Yun-Ya and Kuo, Tong-Yi and Chen, Hung-Hsuan},
title = {Selecting Representative Thumbnail Image and Video Clip from a Video via Bullet Screen},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382691},
doi = {10.1145/3366424.3382691},
abstract = {Finding representative thumbnails or key video clips in a long video usually requires laborious manual editing and compilation. Although there have been deep-learning-based methods that aim to understand videos and pictures, these approaches rely on a large number of computing resources and training data, and the results may still be unsatisfactory. This paper tackles the task of thumbnail selection and highlighting video selection from a different perspective — we leverage on the bullet screen, an emerging new feature on the online video streaming sites that are popular in East Asia, to select thumbnails and video clips. We compared the proposed method with a thumbnail and video clip selecting tool developed by KKStream, a leading streaming service provider in East Asia. We recruited 100 individuals to conduct subjective tests. The experimental results show that most participants are satisfied with the thumbnails and video clips selected by our method, suggesting that the bullet screen could be a valuable resource for video understanding. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {48–49},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382692,
author = {Argyriou, Andreas and Gonz\'{a}lez-Fierro, Miguel and Zhang, Le},
title = {Microsoft Recommenders: Best Practices for Production-Ready Recommendation Systems},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382692},
doi = {10.1145/3366424.3382692},
abstract = {Recommendation algorithms have been widely applied in various contemporary business areas, however the process of implementing them in production systems is complex and has to address significant challenges. We present Microsoft Recommenders, an open-source Github repository for helping researchers, developers and non-experts in general to prototype, experiment with and bring to production both classic and state-of-the-art recommendation algorithms. A focus of this repository is on best practices in development of recommendation systems. We have also incorporated learnings from our experience with recommendation systems in production, in order to enhance ease of use; speed of implementation and deployment; scalability and performance.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {50–51},
numpages = {2},
keywords = {Recommender systems, Libraries, Algorithms},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382693,
author = {Zhang, Ao and Chen, Shizhan and Zhang, Xiaowang and Li, Rui and Zhang, Xinzhi},
title = {A Knowledge-Enriched Model for Emotional Conversation Generation},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382693},
doi = {10.1145/3366424.3382693},
abstract = {In this poster, we propose a knowledge-enriched emotional conversation generation model (KE-EGM) that can ensure high quality content and focus on the impact of emotional factors during the conversation. First, we apply a multi-embedding fusion layer to provide this model with the token-level and sentence-level understanding. Then, the emotion flow attention mechanism combines flow emotion state and attention mechanism to learn and capture emotional information during the conversation dynamically. Finally, the multi-objective optimization mechanism is introduced to detect and generate fine-grained emotional responses. The experimental results show that KE-EGM outperforms several baselines not only in the content aspect but also in the emotional aspect.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {52–53},
numpages = {2},
keywords = {Emotion State, Conversation, Content, Attention},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382694,
author = {Kang, Jiwon and Yoon, Jeewoo and Han, Jinyoung},
title = {Why Do Instagram Users Tag Friends in Comments?},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382694},
doi = {10.1145/3366424.3382694},
abstract = {Tagging a friend in a comment is one of the well-known mechanisms that leads user interaction in social media. This paper investigates the current practice of user tagging in Instagram by collecting a large-scale data that includes 9&nbsp;K posts and their 4&nbsp;M comments shared by 3&nbsp;M users. Our analysis reveals that 54.8% of the comments contain user tagging, meaning that user tagging is widely-used in Instagram. To shed light on why Instagram users tag friends in comments, we develop a learning-based model that classifies the motivation of user tagging into one of the following motivations: (i) information-oriented, (ii) relationship-oriented, and (iii) discussion-oriented. We then apply our model to the comments with user tagging in our data, and reveal that user tagging is often used for interpersonal communication with friends.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {54–56},
numpages = {3},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382695,
author = {Pang, Jinhui and Jiao, Jie and Ji, Guangxi and Wu, Yunjie and Zhang, Ding and Wang, Shujun},
title = {Towards Incomplete SPARQL Query in RDF Question Answering - A Semantic Completion Approach},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382695},
doi = {10.1145/3366424.3382695},
abstract = {RDF question/answering(Q/A) system allows users to ask questions in natural language on a knowledge base represented by RDF and retrieve answers. A common problem in RDF Q/A is that existing works tend to translate a natural language question into an incomplete SPARQL query, which means that SPARQL queries may not fully understand user’s ideas. For example, some triple patterns may be missing in the question translation stage. In this poster, we first present a siamese adaptation of the Long Short-Term Memory(LSTM) network to detect whether the SPARQL query generated by the RDF Q/A system is complete. Then, for incomplete queries, we propose a Markov-based method to supplement SPARQL queries. Finally, we compare our approach with some state-of-the-art RDF Q/A systems in the benchmark dataset. Extensive experiments confirm that our method improves the precision significantly.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {57–58},
numpages = {2},
keywords = {Question Answering, Siamese Neural Networks, LSTM, RDF},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382696,
author = {Verma, Gaurav and Srinivasan, Balaji Vasan and Saini, Shiv Kumar and Chhaya, Niyati},
title = {Modeling Causal Impact of Textual Style on a Targeted Goal},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382696},
doi = {10.1145/3366424.3382696},
abstract = {The consumption characteristics of a textual piece are influenced by both the core-content (i.e., what is being conveyed) and its stylistic attributes (i.e., how it is being conveyed). We present an approach to model stylistic attributes in text and leverage a multi-cause deconfounder model to estimate the causal effect of stylistic attributes towards a targeted goal. We show that our approach can identify causally significant attributes along with the ones considered important by conventional supervised approaches. Furthermore, we demonstrate using performance comparison on classification tasks that our approach does not compromise on the modeling capabilities. We believe that such a model can be valuable towards providing statistical feedback to an author to improve on certain style attributes to better achieve a target objective. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {59–60},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382697,
author = {Chen, Chi-Hua and Zhang, Yizhuo and Guo, Wenzhong and Pan, Mingyang and Lyu, Lingjuan and Lin, Chia-Yu},
title = {Contour Accentuation for Transfer Learning-Based Ship Recognition Method},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382697},
doi = {10.1145/3366424.3382697},
abstract = {This study proposes a ship recognition system which includes intelligent bridge piers and a ship recognition server. The ship recognition server can analyse the contour features of ship images from intelligent bridge piers by the proposed contour accentuation method; the ship image with contour accentuation can be adopted as the inputs of transfer learning-based neural network for ship classification by the proposed transfer learning-based ship recognition method. In practical experiments, the results showed that the proposed transfer learning-based ship recognition method with contour accentuation can obtain higher accuracy, and the accuracy of the proposed method was 97.79%.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {61–62},
numpages = {2},
keywords = {convolutional neural network, contour accentuation, transfer learning, ship recognition},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382698,
author = {Wang, Shujun and Jiao, Jie and Zhang, Xiaowang},
title = {A Semantic Similarity-Based Subgraph Matching Method for Improving Question Answering over RDF},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382698},
doi = {10.1145/3366424.3382698},
abstract = {RDF question/answering (Q/A) system can explore RDF data by translating natural language questions into SPARQL queries. In this poster, we design a generation-and-ranking approach to translate natural language questions into SPARQL queries based on semantic similarity between questions and SPARQL queries. In the generation stage, we employ a directed super semantic query graph to extract the structural query intention of the question, based on which Q/A on RDF is reduced to the graph matching problem. After building the query graph, we generate a set of candidate queries of the question. In the ranking stage, we rank the query in the candidate query set according to the semantic similarity between the query and question. Finally, we pick the query with the highest semantic similarity to the original question.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {63–64},
numpages = {2},
keywords = {Semantic Query Graph, RDF, SPARQL, Question Answering},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382699,
author = {Wang, Chunpei and Zhang, Xiaowang},
title = {Q-BERT: A BERT-Based Framework for Computing SPARQL Similarity in Natural Language},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382699},
doi = {10.1145/3366424.3382699},
abstract = {In this paper, we present a pre-trained transformer network Q-BERT, in which siamese network architecture is employed to produce semantically meaningful embeddings of SPARQL queries to be compared via cosine-similarity. A core idea of Q-BERT is to put SPARQL query into the category of natural language to ensure that each entity mention or relation phrase in different knowledge bases has the same vector representation. Moreover, based on Q-BERT, we present a practical approach for the SPARQL query-empty-answer problem by accessing the RDF repository. The experiments on real datasets show the effectiveness and efficiency of Q-BERT.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {65–66},
numpages = {2},
keywords = {Siamese, SPARQL, Semantic Similarity, BERT},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382700,
author = {Lim, Dongwoo and Toriumi, Fujio and Hayashi, Kaori},
title = {Network Analysis on Emotional Responses in International Disputes},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382700},
doi = {10.1145/3366424.3382700},
abstract = {In this study, we describe different aspects of nationalism revealed on Twitter in Japan and Korea during a recent international dispute on the trade conflict. We conducted a network analysis of 23 million tweets collected in July 2019, during a bitter economic dispute between Japan and South Korea. As a result, we could identify different emotional responses between two countries. Japan’s retweeted relationship was divided into two large groups – conservatives and liberals – in terms of political orientation, and Korea’s follow-up relationship was consisted of one large group that shared a sense of nationalism regardless of political orientation. Furthermore, we created a co-occurrence network to identify which words were frequently used in each distinctive group.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {67–68},
numpages = {2},
keywords = {network analysis, social media, public opinion, nationalism, international dispute},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382701,
author = {Hsu, Ting-Wei and Chen, Chung-Chi and Huang, Hen-Hsen and Chang Chen, Meng and Chen, Hsin-Hsi},
title = {Hedging via Opinion-Based Pair Trading Strategy},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382701},
doi = {10.1145/3366424.3382701},
abstract = {Risk is an important component when constructing a trading strategy. However, most of the previous works that make the price movement prediction on the basis of the opinions on social media platforms do not take the risk into consideration. In order to hedge the market-risk, we propose an idea of an opinion-based pair trading strategy. Comparing with the task setting of the previous works, our experimental results show that the neural network models with the pair-wise task setting perform better in both accuracy and profitability metrics. That introduces a new research direction for future researches on opinion-based price movement predictions. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {69–70},
numpages = {2},
keywords = {financial social media, Pair trading, text mining},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382702,
author = {Wang, Wei and Chen, Junyang and Sun, Weiwei and Gong, Zhiguo},
title = {Scientific Collaboration Sustainability Prediction Based on H-Index Reciprocity},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382702},
doi = {10.1145/3366424.3382702},
abstract = {Despite the increasing interest to investigate and promote scientific collaborations, we know little about scientific collaboration sustainability. In this work-in-progress, we propose to investigate the extent to which scientific collaboration sustainability can be predicted. For this purpose, we design a reciprocity-based collaboration sustainability prediction model by injecting h-index reciprocity. Experiments indicate considering early-stage reciprocity can improve prediction performance compared with state-of-the-art methods.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {71–72},
numpages = {2},
keywords = {H-index, Collaboration Network, Collaborator Sustainability},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382703,
author = {Chen, Cen and Zhang, Ya-Lin and Qiu, Minghui and Wu, Bingzhe and Wang, Li and Li, Longfei and Zhou, Jun},
title = {Automatic Knowledge Fusion in Transferrable Networks for Semantic Text Matching},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382703},
doi = {10.1145/3366424.3382703},
abstract = {Transfer learning has been widely studied in many real-world applications to help the model performance of data-deficient domains. However, the design choice, such as what, where and how to share, can greatly influence the model performance. To save the effort in the transfer learning model design, we propose a novel method to automatically fuse transferable network architecture. Extensive experiments on public datasets of semantic text matching tasks show that our proposed method has better performance than the state-of-the-art transfer learning architectures.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {73–74},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382704,
author = {Pant, Kartikey and Dadu, Tanvi and Mamidi, Radhika},
title = {Towards Detection of Subjective Bias Using Contextualized Word Embeddings},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382704},
doi = {10.1145/3366424.3382704},
abstract = {Subjective bias detection is critical for applications like propaganda detection, content recommendation, sentiment analysis, and bias neutralization. This bias is introduced in natural language via inflammatory words and phrases, casting doubt over facts, and presupposing the truth. In this work, we perform comprehensive experiments for detecting subjective bias using BERT-based models on the Wiki Neutrality Corpus(WNC). The dataset consists of 360k labeled instances, from Wikipedia edits that remove various instances of the bias. We further propose BERT-based ensembles that outperform state-of-the-art methods like BERTlarge by a margin of 5.6 F1 score.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {75–76},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382705,
author = {Gupta, Parth and Dreossi, Tommaso and Bakus, Jan and Lin, Yu-Hsiang and Salaka, Vamsi},
title = {Treating Cold Start in Product Search by Priors},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382705},
doi = {10.1145/3366424.3382705},
abstract = {New products in e-commerce platforms suffer from cold start, both in recommendation and search. In this study, we present experiments to deal with cold start in search by predicting priors for behavioral features in learning to rank set up. The offline results show that our technique generates priors for behavioral features which closely track posterior values. The online A/B test on 140MM queries shows that treatment with priors improves new products impressions and increased customers engagement pointing to their relevance and quality.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {77–78},
numpages = {2},
keywords = {product search, learning to rank, cold start},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382706,
author = {Tian, Lin and Zhang, Xiuzhen and Peng, Min},
title = {FakeFinder: Twitter Fake News Detection on Mobile},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382706},
doi = {10.1145/3366424.3382706},
abstract = {Misinformation, or fake news, spreads quickly on the social media platform Twitter. Mobile devices are widely used to read Twitter posts. A mobile app that can detect fake news from the live Twitter stream and alert users in real time is an effective way to contain the spread of misinformation on Twitter. Towards this objective, the prediction model needs to be small to achieve fast prediction. In this paper, we design and develop a fake news detection mobile app with a device-based prediction model based on the small language model ALBERT. Experiments show that it can achieve real-time, accurate detection of fake news.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {79–80},
numpages = {2},
keywords = {Mobile, Fake news detection, ALBERT},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382707,
author = {Guo, Hao and Xing, Zhenchang and Li, Xiaohong},
title = {Predicting Missing Information of Vulnerability Reports},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382707},
doi = {10.1145/3366424.3382707},
abstract = {We have found that there is a certain degree of missing information through studying the current exposure of software vulnerabilities. This problem is caused by incomplete information submitted by the vulnerability report submitter. In this paper, we extract the knowledge of software vulnerability in a fine-grained way, and design a machine learning method to complete the missing information through the other information of the vulnerability itself. Our method can predict and complete the missing types and causes information of vulnerability reports.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {81–82},
numpages = {2},
keywords = {Common Vulnerabilities and Exposures database, software security, vulnerability information reasoning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382708,
author = {Wang, Ping-Lun and Yang, Shao-Hong and Hsiao, Hsu-Chun},
title = {Hybrid-Voting: A Hybrid Structured Electronic Voting System},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382708},
doi = {10.1145/3366424.3382708},
abstract = {Recent development of electronic voting (e-voting) systems has been focusing on blockchain-based design&nbsp;[4]. Despite blockchain’s advantages in public verifiability, existing blockchain-based voting systems are impractical due to the high block time and transaction costs of the underlying blockchain. To achieve verifiability and efficiency simultaneously, we propose Hybrid-Voting, a hybrid structured e-voting system that combines an untrusted centralized server with smart contracts on Ethereum blockchain. In addition, voter anonymity and privacy are guaranteed by using short linkable ring signature and ElGamal encryption. Our evaluation shows that Hybrid-Voting can support 10k voters by using one commodity computer, and the cost per voter is less than one US dollar, much lower than the cost per voter in today’s elections&nbsp;[1].},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {83–84},
numpages = {2},
keywords = {e-voting, blockchain, ring signature},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382709,
author = {Lin, Fandel and Hsieh, Hsun-Ping},
title = {A Convolutional Approach for Estimating Popularity of New Branch Stores},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382709},
doi = {10.1145/3366424.3382709},
abstract = {Thousands of shops are opened and closed down constantly every day. Determining the location of a new branch store is a critical issue for retail business. Making decisions for locations of new stores has been widely studied in many fields considering cost saving and profits. In this study, we propose the “Grid-liked Graph” (GLG), a graph-based approach for spatial data division to model the spatial distribution of urban features. Based on the proposed GLG, four footprint features are extracted from location-based social network data and treated as image features. Convolution Neural Network (CNN)-based models including LeNet and AlexNet are then adopted to infer the popularity of each candidate store for optimizing location selection. According to our experimental results on two kinds of retail stores, the proposed methods can increase usage of data, and can also perform well for datasets with fewer features compared with previous studies.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {85–87},
numpages = {3},
keywords = {Popular Location Prediction, Convolutional Neural Network (CNN), Feature Engineering, Optimal Retail Store Location},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382711,
author = {Li, Xurong and Yu, Kun and Ji, Shouling and Wang, Yan and Wu, Chunming and Xue, Hui},
title = {Fighting Against Deepfake: Patch&amp;Pair Convolutional Neural Networks (PPCNN)},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382711},
doi = {10.1145/3366424.3382711},
abstract = {In this paper, we propose a novel Patch&amp;Pair Convolutional Neural Networks (PPCNN) to distinguish Deepfake videos or images from real ones. Through the comprehensive evaluations on public datasets, we demonstrate that our model performs better than existing detection methods and show better generalization. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {88–89},
numpages = {2},
keywords = {Tampering detection, Multi-task learning, Deepfake videos},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382712,
author = {Shafiei, Hossein and Khonsari, Ahmad and Kashipazha, Sina and Dadlani, Aresh},
title = {The Tongue Can Paint: Understanding Popularity in Podcasting Societies},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382712},
doi = {10.1145/3366424.3382712},
abstract = {Podcast listeners have tripled over the past 5 years to over 100 million subscribers inside the US alone. Yet, no research study has focused on obtaining insights into the growth and popularity of contents among the subscribers. In this paper, we investigate the main factors that contribute to the popularity of podcasts. Our study helps content producers, service providers, and investors to better understand the rationales behind the popularity of podcasts.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {90–91},
numpages = {2},
keywords = {Content Popularity Analysis, Online Podcasting Services},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382713,
author = {Sinha, Manjira and Dasgupta, Tirthankar},
title = {Detecting Uncertainty in Text Using Multi-Channel CNN-TreeBiLSTM Network},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382713},
doi = {10.1145/3366424.3382713},
abstract = {Uncertainty in a text refers to the propositions that exhibit fuzziness in meaning. In this paper, we present a novel Multi-Channel Tree-LSTM model that integrates a relation aware self-attention along with multiple embeddings to automatically detect uncertainty cues in texts. We have evaluated the models with data sources across multiple domains that include bio-medical texts, privacy policies, and consumer reviews. Our preliminary analysis showed that the proposed model outperforms the existing baseline systems across all the domains.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {92–93},
numpages = {2},
keywords = {Vagueness in text, Uncertainty detection, Neural networks},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382714,
author = {Tulabandhula, Theja and Sinha, Deeksha and Patidar, Prasoon},
title = {Recommendations under Multi-Product Purchase Behavior},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382714},
doi = {10.1145/3366424.3382714},
abstract = {We model consumer behavior wherein users select multiple items from the offered recommendations. We then formulate and solve an optimization problem that computes the best recommendation sets that the e-commerce platform should display to maximize its expected revenues. Numerical simulations show the scalability of our approach in real-world settings.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {94–95},
numpages = {2},
keywords = {combinatorial optimization, e-commerce, choice models},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382715,
author = {Aslanyan, Grigor and Mandal, Aritra and Senthil Kumar, Prathyusha and Jaiswal, Amit and Rangasamy Kannadasan, Manojkumar},
title = {Personalized Ranking in ECommerce Search},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382715},
doi = {10.1145/3366424.3382715},
abstract = {We address the problem of personalization in the context of eCommerce search. Specifically, we develop personalization ranking features that use in-session context to augment a generic ranker optimized for conversion and relevance. We use a combination of latent features learned from item co-clicks in historic sessions and content based features that use item title and price. Personalization in search has been discussed extensively in the existing literature. The novelty of our work is combining and comparing content based and content agnostic features and showing that they complement each other to result in a significant improvement of the ranker. We experimentally show that our technique significantly outperforms a generic ranker in terms of Mean Reciprocal Rank (MRR). We also provide anecdotal evidence for the semantic similarity captured by the item embeddings on the eBay search engine.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {96–97},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382716,
author = {Lee, Kuan-Lin and Cheng, Yu-Chung and Chen, Pai-Lin and Huang, Hen-Hsen},
title = {Keeping Their Words: Direct and Indirect Chinese Quote Attribution from Newspapers},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382716},
doi = {10.1145/3366424.3382716},
abstract = {Quote attribution plays an important role in emerging research topics such as fact checking, stance detection, and argument mining. This work explores Chinese quote attribution from newspapers. Both direct and indirect quotes are addressed by a text-encoder based sequence labeling model. We create a dataset for empirical analysis. Experimental results show the effectiveness of our model.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {98–99},
numpages = {2},
keywords = {quote attribution, computational journalism, quote extraction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382717,
author = {Lin, Fandel and Lin, Shiuan-Tyng and Fang, Jie-Yu and Hsieh, Hsun-Ping},
title = {Traffic Light Control with Real-Time Vehicle License Plate Recognition},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382717},
doi = {10.1145/3366424.3382717},
abstract = {Traffic light control is one of crucial issues in the traffic deployment. However, the current procedure in generating traffic light schedule that relies on either empirical law manually or simulating software is time-consuming and lack of both flexibility and immediacy. Given real-time traffic flow information from vehicle license plate recognition sensors, the proposed framework can immediately generate a traffic light schedule to relieve arising or probable traffic congestion which was not expected at first by management authorities. The novelty of our system is that it combines the instant traffic flow transition estimation and an adaptation of Gaussian Mixture Model. Preliminary evaluations show that proposed method outperforms the current procedure that the traffic management authority in Tainan City takes.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {100–102},
numpages = {3},
keywords = {Gaussian Mixture Model (GMM), Traffic Light Control, Traffic Flow Prediction, Markov Chain},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382719,
author = {Xu, Rong-Qin and Zhou, Mingyang and Liao, Hao},
title = {RNR: A Generic Bayesian-Based Framework for Enhancing Top-N Recommender Systems},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382719},
doi = {10.1145/3366424.3382719},
abstract = {In personalized top-N recommender systems, a core task is to design effective methods to measure user-item preference scores and then to suggest, for each user, a small set of personalized items with high scores. However, little attention was paid to the recommendation of low-score user-item links. In this work, based on the Bayesian estimation theory, we propose a novel metric RNR (Recall-to-Noise Ratio) to characterize the ability to recommend both high-score and low-score user-item links. Then we propose a generic framework that leverages RNR to transfer the link scores of the state-of-the-art recommendation methods. Empirical experiments show that the proposed framework could optimally improve the recommendation accuracy.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {103–104},
numpages = {2},
keywords = {Bayesian theory, Accuracy, Top-N recommender systems},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382720,
author = {Chhabra, Anamika and R. S. Iyengar, S.},
title = {Activity-Selection Behavior of Users in StackExchange Websites},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382720},
doi = {10.1145/3366424.3382720},
abstract = {In this work, we examine the disparity in contribution behavior of users with respect to the activities that they choose to perform on Q&amp;A websites. We collect the data of 156 websites of StackExchange and analyze the contribution made by over 5.3 million users. We find that most of the users tend to contribute predominantly to one of the primary activities. Such a behavior yields a high-level distribution of users across the activities, which provides useful insights into the user base composition of these websites. In this work, we show how this distribution varies for StackExchange websites and discuss how it can be a valuable maintenance parameter for these websites.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {105–106},
numpages = {2},
keywords = {Role-playing, Q&amp;A portals, StackExchange, Activity-selection},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382721,
author = {Zhao, Hu and Al-Masri, Eyhab},
title = {A User-Centered Handoff Procedure in 5G Cellular Networks},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382721},
doi = {10.1145/3366424.3382721},
abstract = {The 5G cellular network is being deployed across many countries and is becoming an integral technology for mobile communication. The ubiquity of the 5G services, however, will require the dissemination of a large number of base stations which translates into a significant number of handoff operations compared to earlier generations (e.g. 4G/LTE). Furthermore, Narrowband Internet of Things (NB-IoT) is a 3GPP specification designed for enabling the wireless communication of low powered IoT devices. However, NB-IoT does not provide complete support for handoff operations and is intended for fixed IoT devices. In this paper, we investigate critical handoff attributes that may affect the end-to-end service in NB-IoT and present a user-centered handoff optimization model. Results show that our proposed user-centered model significantly outperforms the generic uniform preferences across 96% of tested handoff operations.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {107–108},
numpages = {2},
keywords = {NB-IoT. Edge, Handoff, User-Centered, 5G, Internet of Things},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382722,
author = {Tobiyama, Shun and Hu, Bo and Kamiya, Kazunori and Takahashi, Kenji},
title = {Large-Scale Network-Traffic-Identification Method with Domain Adaptation},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382722},
doi = {10.1145/3366424.3382722},
abstract = {With the continuous evolution of the Internet, a variety of web applications, such as video streaming and social network services, are widely used, significantly increasing the amount of traffic. Classifying the types of network traffic in detail is becoming more important from the perspectives of resource management and security analysis. In an Internet service provider (ISP) level large-scale network, details of packet payload cannot be collected due to the huge amount of traffic. Alternatively, network flow data representing the statistics of a sequence of packets are collected from devices. However, due to the lack of packet details, the granularity of classification is limited to the protocol level. For better understanding of the types of traffic in detail, we propose an application-level traffic identification method combining both packet data of small-scale networks and flow data of large-scale networks. With our proposed method, we adapt an identification model trained from small-scale network data to large-scale network data with domain adaptation.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {109–110},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382723,
author = {Momma, Michinari and Bagheri Garakani, Alireza and Ma, Nanxun and Sun, Yi},
title = {Multi-Objective Ranking via Constrained Optimization},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382723},
doi = {10.1145/3366424.3382723},
abstract = {In this paper, we introduce an Augmented Lagrangian based method to incorporate the multiple objectives (MO) in a search ranking algorithm. Optimizing MOs is an essential and realistic requirement for building ranking models in production. The proposed method formulates MO in constrained optimization and solves the problem in the popular Boosting framework – a novel contribution of our work. Furthermore, we propose a procedure to set up all optimization parameters in the problem. The experimental results show that the method successfully achieves MO criteria much more efficiently than existing methods.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {111–112},
numpages = {2},
keywords = {Multi-objective ranking, Product/web search, Learning to rank},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382724,
author = {Agarwal, Manoj and Vasavada, Jaldeep},
title = {Identifying Expert Intent Queries},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382724},
doi = {10.1145/3366424.3382724},
abstract = {Search engines assess the quality of their results using nDCG@k for the queries in regular judgement sets. Since these sets are human annotated, the judgement sets are limited in size. Hence, these sets may not have proper representation from “Expert Topics”, e.g., <machine learning>, if an Expert Topic represents a tail segment. Therefore, query sets specific to expert topics are prepared to assess the search quality on such topics, which are judged by the domain experts. Such expert judgements are typically 5X-10X more expensive compared to regular judgements. Hence, “Expert Query Sets” must be prepared such that these queries are likely to be judged differently by an expert and a regular judge.In this paper, we define a novel problem, to identify a sub set of users queries, from a topic specific query set, such that the judgement difference between expert judges and regular judges, called yield rate, is high. Our results, across different expert topics, show that the expert query sets identified through our methodology have ∼15% more yield rate.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {113–115},
numpages = {3},
keywords = {Expert Queries, Search, Web Queries},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382726,
author = {Ming Wong, Ka and Chuang, Chia-Min and Wu, Chun-Kai and Wang, Yu-Chun and Tzong-Han Tsai, Richard},
title = {DeepEncyclolink: A Cross-Encyclopedia, Cross-Language Article-Linking System Based on Deep Learning},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382726},
doi = {10.1145/3366424.3382726},
abstract = {DeepEncyclolink is a web-based system for linking pairs of corresponding articles from different encyclopedias in different languages. The core technology of DeepEncyclolink is based on paragraph embeddings calculated by a long-short-term-memory network with attention. Compared to our previous feature-based machine learning model, DeepEncyclolink has made great strides in coverage and accuracy. DeepEncyclolink offers an easy-to-use user interface and is supported by a powerful service backend for retrieving and selecting equivalent articles between English Wikipedia and Chinese Baidu Encyclopedia. DeepEncyclolink breaks the barriers of different encyclopedias and different languages, making it easy for users to access knowledge written in various languages. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {116–117},
numpages = {2},
keywords = {article linking, neural networks, representation learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382727,
author = {Kobayashi, Masaki and Wakabayashi, Kei and Morishima, Atsuyuki},
title = {Quality-Aware Dynamic Task Assignment in Human+AI Crowd},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382727},
doi = {10.1145/3366424.3382727},
abstract = {Today, crowdsourcing the creation of AI programs is a common practice. However, combining the created programs with other programs and human computations to obtain efficient human-in-the-loop solutions is not trivial. Our paper proposes a framework to address the problem of dynamically assigning tasks to a crowd of AI and human workers, and presents the results of a preliminary experiment.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {118–119},
numpages = {2},
keywords = {Task assignment, Human-AI collaboration, Crowdsourcing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382728,
author = {Chauhan, Ayush and Prasad, Archiki and Gupta, Parth and Prashanth Reddy, Amiredddy and Kumar Saini, Shiv},
title = {Time Series Forecasting for Cold-Start Items by Learning from Related Items Using Memory Networks},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382728},
doi = {10.1145/3366424.3382728},
abstract = {Time series forecasting for new items is very important in a wide variety of applications. Existing solutions for time series forecasting, however, do not address this cold start problem. The underlying machine learning models in these solutions rely heavily on the availability of the past data points of the time series. Here, we propose to use a modified Dynamic Key-Value Memory Network (DKVMN) that enables knowledge sharing across items. The network is conventionally used for binary tasks in knowledge tracing. We modify it for our regression-based forecasting use-case. Specifically, we change the output layer, include feedback for error correction, add a mechanism to handle scale across items. We test our solution on the SKU level data of a large e-commerce company and compare the results to the widely used LSTM model, outperforming it by over 25% across multiple metrics.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {120–121},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382729,
author = {Almerekhi, Hind and Kwak, Haewoon and Jansen, Bernard},
title = {Statistical Modeling of Harassment against Reddit Moderators},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382729},
doi = {10.1145/3366424.3382729},
abstract = {Despite the dedication that some volunteer moderators of online communities display when performing their moderation duties, they become targets of hate and harassment by other users. To understand what causes the change in moderator role from heroes to victims, we analyze the responses of 1,818 moderators on Reddit to an online survey about moderation practices and harassment. We built a statistical model and found 6 significant independent variables that affect harassment on moderators, such as the knowledge of community norms, which increases harassment on moderators the most. Our findings imply that vulnerable moderators in toxic communities need countermeasures against harassment.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {122–123},
numpages = {2},
keywords = {survey, Reddit, Online communities, toxicity, moderation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382730,
author = {Lin, Pin-Yen and Zhou, Zhong-Yi and Chang, Chun-Ming and Chen, Hung-Wei and Tung, Shu-Po and Hsiao, Hsu-Chun},
title = {Keeping Passwords In Your Pocket: Managing Password Locally With Mobile Fingerprint Sensors},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382730},
doi = {10.1145/3366424.3382730},
abstract = {Password managers allow users to use strong and unique passwords online. However, users have to either trust a third-party service to manage their passwords or manually synchronize passwords among local devices. To relax the trust assumption while maintaining usability, in this work, we develop a password manager that stores passwords on mobile phones. The passwords can be easily unlocked via the user’s fingerprint and securely sent to other devices such as a desktop browser. Our implementation uses the iOS local authentication API for biological authentication, and uses firebase cloud messaging to send the password to the browser.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {124–125},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382731,
author = {Patil, Sangameshwar},
title = {Domain-Specific Noisy Query Correction Using Linguistic Network Community Detection},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382731},
doi = {10.1145/3366424.3382731},
abstract = {Noisy queries pose an important challenge for retrieving relevant search results. The importance for query correction increases with increasing use of hand-held devices and technologies such as SMS, tweets to search and access information. The task is further complicated for domain-specific search engines as the amount of query logs may be significantly smaller than general purpose search engines. In this paper, we propose to use the community detection technique from social network analysis for spelling correction of a set of noisy queries such as SMS messages. We focus on the task of identifying relevant questions from a set of Frequently Asked Questions (FAQ) for different domains for a set of incoming noisy queries. Experimental validation shows that the proposed CD-Speller method performs significantly better than Hunspell, the popular and industry-strength spelling correction tool.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {126–127},
numpages = {2},
keywords = {Social network analysis, Linguistic Networks, Community detection, Noisy query processing, Natural language processing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382732,
author = {Wang, Jiaqi and Lu, Zac and Al-Masri, Eyhab},
title = {Edgify: Resource Allocation Optimization for Edge Clouds Using Stable Matching},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382732},
doi = {10.1145/3366424.3382732},
abstract = {As more Internet of Things (IoT) devices become increasingly ubiquitous, dynamic resource allocation for edge computing environments becomes extremely time consuming and challenging task. To overcome these challenges, edge computing environments need to dynamically scale based on the availability of accessible edge clouds within existing IoT infrastructure. In this paper, we introduce Edgify: a dynamic resource provisioning model that can efficiently allocate resources across a distributed edge computing environment. We evaluate Edgify through a number of experiments that demonstrate usefulness and effectiveness of the proposed approach.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {128–130},
numpages = {3},
keywords = {Edge computing, IoT, resource allocation, cloud computing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383525,
author = {Agrawal, Parag and Menon, Tulasi and Kam, Aya and Naim, Michel and Chouragade, Chaikesh and Singh, Gurvinder and Kulkarni, Rohan and Suri, Anshuman and Katakam, Sahithi and Pratik, Vineet and Bansal, Prakul and Kaur, Simerpreet and Duggal, Anand and Chalabi, Achraf and Choudhari, Prashant and Satti, Somi Reddy and Nayak, Niranjan and Rajput, Neha},
title = {QnAMaker: Data to Bot in 2 Minutes},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383525},
doi = {10.1145/3366424.3383525},
abstract = {Having a bot for seamless conversations is a much-desired feature that products and services today seek for their websites and mobile apps. These bots help reduce traffic received by human support significantly by handling frequent and directly answerable known questions. Many such services have huge reference documents such as FAQ pages, which makes it hard for users to browse through this data. A conversation layer over such raw data can lower traffic to human support by a great margin. We demonstrate QnAMaker, a service that creates a conversational layer over semi-structured data such as FAQ pages, product manuals, and support documents. QnAMaker is the popular choice for Extraction and Question-Answering as a service and is used by over 15,000 bots in production. It is also used by search interfaces and not just bots. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {131–134},
numpages = {4},
keywords = {Information Retrieval, Multilingual, Online learning, Question Answering, ChatBots, Bot Persona, Democratizing AI},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383526,
author = {Steiner, Thomas and LePage, Pete and Nattestad, Thomas and McClelland, Rory and Russell, Alex and Ng, Dominick},
title = {From Fugu With Love: New Capabilities for the Web},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383526},
doi = {10.1145/3366424.3383526},
abstract = {We present a&nbsp;greeting card web application that is useful in all modern browsers, but sees its experience progressively enhanced through new and upcoming browser capabilities such as native file system access, system clipboard access, contacts retrieval, periodic background sync, screen wake lock, web sharing features, and more.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {135–138},
numpages = {4},
keywords = {Web Incubator Community Group, Web apis, Progressive Web Apps},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383527,
author = {Li, Quanzhi and Avadhanam, Satish and Zhang, Qiong},
title = {An End-to-End Tool for News Processing and Semantic Search},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383527},
doi = {10.1145/3366424.3383527},
abstract = {In this demonstration, we present an intelligent system for business analysis and market research that can produce actionable competitive and strategic insight. It has two sub systems, a news content processing pipeline and a semantic search engine. The pipeline collects global business news and processes them to establish context, themes, topics, entities, sentiment and relationships. The semantic search system integrates all the information produced by the pipeline and provides users the semantic search and information exploration ability. This paper introduces all the components of this system.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {139–142},
numpages = {4},
keywords = {semantic search, entity extraction, content generation, content integration, News processing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383528,
author = {De Brouwer, Mathias and Bonte, Pieter and Arndt, D\"{o}rthe and Vander Sande, Miel and Heyvaert, Pieter and Dimou, Anastasia and Verborgh, Ruben and De Turck, Filip and Ongenae, Femke},
title = {Distributed Continuous Home Care Provisioning through Personalized Monitoring &amp; Treatment Planning},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383528},
doi = {10.1145/3366424.3383528},
abstract = {In healthcare, the aging of the population is resulting in a gradual shift from residential care to home care, requiring reliable follow-up of elderly people by a whole network of care providers. The environment of these patients is increasingly being equipped with different monitoring devices, which allow to obtain insight into the current condition of patient &amp; environment. However, current monitoring platforms that support care providers are centralized and not personalized, reducing performance, scalability, autonomy and privacy. Because the available data is only exposed through custom APIs, profile knowledge cannot be efficiently exchanged, which is required to provide optimal care. Therefore, this paper presents a distributed data-driven platform, built on Semantic Web technologies, that enables the integration of all profile knowledge in order to deliver personalized continuous home care. It provides a distributed smart monitoring service, which allows to locally monitor only the relevant sensors according to the patient’s profile, and infer personalized decisions when analyzing events. Moreover, it provides a medical treatment planning service, which composes treatment plans tailored to each patient, including personalized quality of service parameters allowing a doctor to select the optimal plan. To illustrate how the platform delivers these services, the paper also presents a demonstrator using a realistic home care&nbsp;scenario.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {143–147},
numpages = {5},
keywords = {personalized healthcare, semantic reasoning, smart monitoring, Semantic Web, continuous home care, data-driven, distributed},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383529,
author = {Hong, Shenda and Fu, Zhaoji and Zhou, Rongbo and Yu, Jie and Li, Yongkui and Wang, Kai and Cheng, Guanlin},
title = {CardioLearn: A Cloud Deep Learning Service for Cardiac Disease Detection from Electrocardiogram},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383529},
doi = {10.1145/3366424.3383529},
abstract = {Electrocardiogram (ECG) is one of the most convenient and non-invasive tools for monitoring peoples’ heart condition, which can use for diagnosing a wide range of heart diseases, including Cardiac Arrhythmia, Acute Coronary Syndrome, et al. However, traditional ECG disease detection models show substantial rates of misdiagnosis due to the limitations of the abilities of extracted features. Recent deep learning methods have shown significant advantages, but they do not provide publicly available services for those who have no training data or computational resources. In this paper, we demonstrate our work on building, training, and serving such out-of-the-box cloud deep learning service for cardiac disease detection from ECG named CardioLearn. The analytic ability of any other ECG recording devices can be enhanced by connecting to the Internet and invoke our open API. As a practical example, we also design a portable smart hardware device along with an interactive mobile program, which can collect ECG and detect potential cardiac diseases anytime and anywhere. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {148–152},
numpages = {5},
keywords = {Electrocardiogram, Healthcare, Deep learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383530,
author = {Li, Yuening and Zha, Daochen and Venugopal, Praveen and Zou, Na and Hu, Xia},
title = {PyODDS: An End-to-End Outlier Detection System with Automated Machine Learning},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383530},
doi = {10.1145/3366424.3383530},
abstract = {Outlier detection is an important task for various data mining applications. Current outlier detection techniques are often manually designed for specific domains, requiring large human efforts of database setup, algorithm selection, and hyper-parameter tuning. To fill this gap, we present PyODDS, an automated end-to-end Python system for Outlier Detection with Database Support, which automatically optimizes an outlier detection pipeline for a new data source at hand. Specifically, we define the search space in the outlier detection pipeline, and produce a search strategy within the given search space. PyODDS enables end-to-end executions based on an Apache Spark backend server and a light-weight database. It also provides unified interfaces and visualizations for users with or without data science or machine learning background. In particular, we demonstrate PyODDS on several real-world datasets, with quantification analysis and visualization results.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {153–157},
numpages = {5},
keywords = {Outlier Detection, Open Source Package, End-to-end System, Automated Machine Learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383531,
author = {Cheng, Yong and Ma, Sijia and Chen, Jun and Zhao, Yingying and Chen, Yaolin and Wang, Jingtao and Xing, Jia and Guo, Wenhui and Jing, Yushi and Wu, Genze and Zhu, Wenli},
title = {Towards Web-Based Etymological Hanzi Learning},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383531},
doi = {10.1145/3366424.3383531},
abstract = {Modern-day Chinese characters, or Hanzi, originate from the ancient oracle-bone scripts (甲骨文). Such etymological relationship creates unique opportunities for Hanzi learning. This work proposes to use Web-based tools and the latest machine learning techniques to scale-up and enhance etymological Hanzi learning. By sharing our implementation details from launching an interactive sketch-based learning exhibition, we hope education-AI becomes more widely incorporated into today’s commercial Web applications. Our demo video can be found at: https://youtube/1met8Uk5pA0},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {158–162},
numpages = {5},
keywords = {neural representation, sketch-based interface, Cloud computing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383532,
author = {Krishna, Ajay and Pallec, Michel Le and Martinez, Alejandro and Mateescu, Radu and Sala\"{u}n, Gwen},
title = {MOZART: Design and Deployment of Advanced IoT Applications},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383532},
doi = {10.1145/3366424.3383532},
abstract = {The Internet of Things (IoT) aims at sensing and altering our surrounding environment through connected objects to improve everyday life. IoT applications are built using interconnected objects with a goal to provide added-value services. However, there are still challenges in providing a secure, robust and easy-to-use end-user platform for development of such applications. In this paper, we present a end-user tool for supporting the design and deployment of smart home IoT applications. The tool first provides a graphical user interface to specify an IoT application using a rule-based composition language. Automated analysis techniques can then be called for verifying that the designed application is correct (e.g., free of deadlocks). Finally, the tool provides a rule execution engine to support application deployment. The tool is built by implementing a set of components on top of Mozilla WebThings platform, which is a concrete implementation of W3C’s Web of Things specification.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {163–166},
numpages = {4},
keywords = {IoT, web of things, composition, formal verification},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383533,
author = {Deb Nath, Rudra Pratap and Hose, Katja and Pedersen, Torben Bach and Romero, Oscar and Bhattacharjee, Amrit},
title = {SETLBI: An Integrated Platform for Semantic Business Intelligence},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383533},
doi = {10.1145/3366424.3383533},
abstract = {With the growing popularity of Semantic Web technologies, more and more organizations natively manage data using Semantic Web standards, in particular RDF. This development gives rise to new requirements for Business Intelligence tools to enable analyses in the style of On-Line Analytical Processing (OLAP) over RDF data. In this demonstration, we therefore present the SETLBI (Semantic Extract-Transform-Load and Business Intelligence) integration platform that brings together the Semantic Web and Business Intelligence technologies. SETLBI covers all phases of integration: target definition, source to target mappings generation, semantic and non-semantic source extraction, data transformation, and target population and update. It facilitates Data Warehouse designers to build a semantic Data Warehouse, either from scratch or by defining a multi-dimensional view over existing RDF data sources, and further enables OLAP-style analyses.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {167–171},
numpages = {5},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383534,
author = {Zehlike, Meike and S\"{u}hr, Tom and Castillo, Carlos and Kitanovski, Ivan},
title = {FairSearch: A Tool For Fairness in Ranked Search Results},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383534},
doi = {10.1145/3366424.3383534},
abstract = {Ranked search results and recommendations have become the main mechanism by which we find content, products, places, and people online. With hiring, selecting, purchasing, and dating being increasingly mediated by algorithms, rankings may determine business opportunities, education, access to benefits, and even social success. It is therefore of societal and ethical importance to ask whether search results can demote, marginalize, or exclude individuals of unprivileged groups or promote products with undesired features. In this paper we present FairSearch, the first fair open source search API to provide fairness notions in ranked search results. We implement two well-known algorithms from the literature, namely FA*IR &nbsp;(Zehlike et&nbsp;al., 9) and DELTR &nbsp;(Zehlike and Castillo, 10) and provide them as stand-alone libraries in Python and Java. Additionally we implement interfaces to Elasticsearch for both algorithms, a well-known search engine API based on Apache Lucene. The interfaces use the aforementioned Java libraries and enable search engine developers who wish to ensure fair search results of different styles to easily integrate DELTR and FA*IR into their existing Elasticsearch environment.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {172–175},
numpages = {4},
keywords = {Ranking, Algorithmic Fairness, Disparate Impact},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383535,
author = {Wang, Xiaolan and Suhara, Yoshihiko and Nuno, Natalie and Li, Yuliang and Li, Jinfeng and Carmeli, Nofar and Angelidis, Stefanos and Kandogann, Eser and Tan, Wang-Chiew},
title = {ExtremeReader: An Interactive Explorer for Customizable and Explainable Review Summarization},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383535},
doi = {10.1145/3366424.3383535},
abstract = {Building summarization systems have become a necessity due to the extensive volume and growth of online reviews. Despite extensive research on this topic, existing summarization systems generally fall short on two aspects. First, existing techniques generate static summaries which cannot be tailored to specific user needs. Second, most existing systems generate extractive summaries which selects only certain salient aspects from the summaries. Hence, they do not completely depict the overall opinion of the reviews. In this paper, we demonstrate a novel summarization system, ExtremeReader, that overcomes the limitations of existing summarization systems described above. ExtremeReader allows summaries to be tailored and explored interactively so that users can quickly find the desired information. In addition, ExtremeReader generates abstractive summaries with an underlying structure that helps users understand, explore, and seek explanations to the generated summaries.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {176–180},
numpages = {5},
keywords = {Explanation mining, Opinion summarization},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383536,
author = {H. Lee, Helena and Shu, Ke and Achananuparp, Palakorn and Prasetyo, Philips Kokoh and Liu, Yue and Lim, Ee-Peng and Varshney, Lav R.},
title = {RecipeGPT: Generative Pre-Training Based Cooking Recipe Generation and Evaluation System},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383536},
doi = {10.1145/3366424.3383536},
abstract = {Interests in the automatic generation of cooking recipes have been growing steadily over the past few years thanks to a large amount of online cooking recipes. We present RecipeGPT, a novel online recipe generation and evaluation system. The system provides two modes of text generations: (1) instruction generation from given recipe title and ingredients; and (2) ingredient generation from recipe title and cooking instructions. Its back-end text generation module comprises a generative pre-trained language model GPT-2 fine-tuned on a large cooking recipe dataset. Moreover, the recipe evaluation module allows the users to conveniently inspect the quality of the generated recipe contents and store the results for future reference. RecipeGPT can be accessed online at &nbsp;https://recipegpt.org/ },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {181–184},
numpages = {4},
keywords = {web application, natural language generation, recipe generation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383537,
author = {Vachtsevanou, Danai and Junker, Philip and Ciortea, Andrei and Mizutani, Iori and Mayer, Simon},
title = {Long-Lived Agents on the Web: Continuous Acquisition of Behaviors in Hypermedia Environments},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383537},
doi = {10.1145/3366424.3383537},
abstract = {We demonstrate how autonomous goal-directed agents can exploit hypermedia to acquire and execute new behaviors at run time. In addition to behaviors programmed into the agents, in our system agents can discover and reuse behaviors extracted from machine-readable resource manuals. Such manuals can be published by developers, synthesized by agents through automated planning, or even specified by human users at run time. Agents can then discover and use physical and virtual resources in flexible ways, which allows them to better cope with the rapid evolution of open and dynamic Web environments.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {185–189},
numpages = {5},
keywords = {Hypermedia Multi-agent Systems, Interoperability, Web of Things},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383538,
author = {Nica-Avram, Georgiana and Harvey, John and Goulding, James and Lucas, Benjamin and Smith, Andrew and Smith, Gavin and Perrat, Bertrand},
title = {FIMS: Identifying, Predicting and Visualising Food Insecurity},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383538},
doi = {10.1145/3366424.3383538},
abstract = {Food insecurity is a persistent and pernicious problem in the UK. Due to logistical challenges, national food insecurity statistics are unmeasured by government bodies - and this lack of data leads to any local estimates that do exist being routinely questioned by policymakers. We demonstrate a data-driven approach to address this issue, deriving national estimates of food insecurity via combination of supervised machine learning with network analysis of user behaviour, extracted from the world’s most popular peer-to-peer food sharing application (OLIO). Despite long-standing theoretical links between social graph topologies and physical neighbourhoods, prior research has not considered dimensions of geography, network interactions and behaviours in the digital/analogue space simultaneously. In addressing this oversight, we produce a browser-based, interactive and rapidly updateable visualisation, which can be used to analyse the spatial distribution of food insecurity across the UK, and provide new perspective for policy research.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {190–193},
numpages = {4},
keywords = {Visualization, Poverty, Hunger, Food Insecurity, Geospatial, Computational Social Science, Data},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383539,
author = {Luo, Feng and Wang, Xiaoli and Wu, Qingfeng and Liang, Jiaying and Qiu, Xueliang and Bao, Zhifeng},
title = {HQADeepHelper: A Deep Learning System for Healthcare Question Answering},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383539},
doi = {10.1145/3366424.3383539},
abstract = {It is challenging to generate high quality answers for healthcare queries in online platforms. Recent studies proposed deep models for healthcare question answering (HQA) tasks. However, these models have not been thoroughly compared, and they were only tested on self-created datasets. This paper demonstrates a novel system, denoted by HQADeepHelper, to facilitate the learning and practicing of deep models for HQA. We have implemented a wide spectrum of state-of-the-art deep models for HQA retrieval. Users can upload self-collected HQA datasets and knowledge graphs, and do simple configurations by selecting datasets, knowledge graphs, neural network models, and evaluation metrics. Based on user’s configuration specified, the system can automatically train and test the model, conduct extensive experimental evaluation of the models selected, and report comprehensive findings. The reports provide new insights about the strengths and weaknesses of deep models that can guide practitioners to select appropriate models for various scenarios. Moreover, users can download the datasets, knowledge graphs, experimental reports and source codes of neural network models for their own practice and evaluations further.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {194–197},
numpages = {4},
keywords = {Healthcare question answering, Neural network models},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383540,
author = {Li, Lei and Chen, Li and Zhang, Yongfeng},
title = {Towards Controllable Explanation Generation for Recommender Systems via Neural Template},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383540},
doi = {10.1145/3366424.3383540},
abstract = {It has been commonly agreed that the explanation associated with recommendation can be effective in increasing the recommender systems (RS)’s transparency and thus users’ satisfaction and acceptance. Among the various types of explanation in RS, the commonly used textual explanation can be roughly classified into two categories, i.e., template-based and generation-based. As for the former, the fixed template may lose flexibility, while, though the latter may enrich the explanation, it may produce less useful content due to the lack of controllability. In this work, we combine the advantages of the two types of method by developing a neural generation approach named Neural Template (NETE) whose explanations are not only flexible but also controllable and useful. Our human evaluation results confirm that the explanations from our model are perceived helpful by users. Furthermore, our case study illustrates that the explanation generation process is controllable. To demonstrate the controllability of our model, we present a demo that can be easily viewed on a Web browser.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {198–202},
numpages = {5},
keywords = {neural networks, Explainable recommendation, natural language generation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383541,
author = {Enshaeifar, Shirin and Barnaghi, Payam and Skillman, Severin and Sharp, David and Nilforooshan, Ramin and Rostill, Helen},
title = {A Digital Platform for Remote Healthcare Monitoring},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383541},
doi = {10.1145/3366424.3383541},
abstract = {We describe a digital platform developed in collaboration with clinicians and user groups to provide remote healthcare monitoring and support in a dementia care application. The platform uses data from sensory devices that are deployed in participants’ homes and utilises a set machine learning and analytical algorithms to identify risks of adverse health conditions such as Urinary Tract Infections (UTIs) and hypertension in people with dementia. The platform includes a clinical interface that is used by a monitoring team to view alerts and notifications that are generated by the algorithms and to also browse the in-home activity and physiological data in a secure and privacy-aware system. The platform complies to the information governance requirements of the UK National Healthcare Service (NHS) and is registered as a class 1 medical device. The platform has been deployed and tested in over 150 homes.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {203–206},
numpages = {4},
keywords = {Digital healthcare, digital platform, dementia, early intervention},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383542,
author = {Aken, Betty van and Winter, Benjamin and L\"{o}ser, Alexander and Gers, Felix A.},
title = {VisBERT: Hidden-State Visualizations for Transformers},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383542},
doi = {10.1145/3366424.3383542},
abstract = {Explainability and interpretability are two important concepts, the absence of which can and should impede the application of well-performing neural networks to real-world problems. At the same time, they are difficult to incorporate into the large, black-box models that achieve state-of-the-art results in a multitude of NLP tasks. Bidirectional Encoder Representations from Transformers (BERT) is one such black-box model. It has become a staple architecture to solve many different NLP tasks and has inspired a number of related Transformer models. Understanding how these models draw conclusions is crucial for both their improvement and application. We contribute to this challenge by presenting VisBERT, a tool for visualizing the contextual token representations within BERT for the task of (multi-hop) Question Answering. Instead of analyzing attention weights, we focus on the hidden states resulting from each encoder block within the BERT model. This way we can observe how the semantic representations are transformed throughout the layers of the model. VisBERT enables users to get insights about the model’s internal state and to explore its inference steps or potential shortcomings. The tool allows us to identify distinct phases in BERT’s transformations that are similar to a traditional NLP pipeline and offer insights during failed predictions.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {207–211},
numpages = {5},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383543,
author = {Wang, Xueting and Zhang, Yiwei and Yamasaki, Toshihiko},
title = {Earn More Social Attention: User Popularity Based Tag Recommendation System},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383543},
doi = {10.1145/3366424.3383543},
abstract = {Enhancing social popularity of a post (i.e., the number of views or likes) on social network services is important for both ordinary users and companies who want to promote themselves. In this paper, we have implemented an online tagging support system to achieve this using an algorithm that recommends appropriate hashtags considering not only content popularity but also user popularity. The effectiveness of this technology has been verified by actually uploading photos with recommended hashtags on a real social network service.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {212–216},
numpages = {5},
keywords = {social media, social popularity, tag ranking, tagging system, tag recommendation, user-aware},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383545,
author = {Chatterjee, Urbi and Sadhukhan, Rajat and Mukhopadhyay, Debdeep and Subhra Chakraborty, Rajat and Mahata, Debashis and M. Prabhu, Mukesh.},
title = {Stupify: A Hardware Countermeasure of KRACKs in WPA2 Using Physically Unclonable Functions},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383545},
doi = {10.1145/3366424.3383545},
abstract = {A digital communication network is typically the backbone of world wide web and web based applications. Security protocols, specifically in wireless network has undergone several rounds of modifications and upgrades in order to prevent supplicants (or clients) or authenticators (or access points) from attackers either sitting physically around the wireless coverage area or being hooked up to a wired network connected to wireless clients. The latest security protocol in the series is WPA2 (Wi-Fi Protected Access II) which has been implemented in most of the Wi-Fi stations (clients or access points) that are being used in traditional wireless networking as well as recent IoT and CPS devices. Recently a severe replay attack named Key Reinstallation AttaCK (KRACK) has shown that the handshake in WPA2 protocol suite can be compromised and enforce the stations to reuse an old set of initialization vectors (IVs). In this work, we propose to use an unconventional hardware security primitive named Physically Unclonable Functions (PUFs) to nullify the impact of KRACK attack by ensuring a mutual authentication before establishing the communication between the authenticators and the supplicants. In this demo, we show i) how the hardware intrinsic properties of a device can be leveraged to embed a PUF instance in each device, ii) a working prototype of PUF based authentication protocol using Z-Turn board integrated with dual-core ARM Cortex-A9 processor and Artix-7 FPGA, iii) how this protocol can be integrated with existing handshake protocol in WiFi network to resist against KRACK attacks.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {217–221},
numpages = {5},
keywords = {Key Reinstallation Attack, Physically Unclonable Functions, Wi-Fi Protected Access II.},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383546,
author = {Qiu, Sihang and Bozzon, Alessandro and Houben, Geert-Jan},
title = {VirtualCrowd: A Simulation Platform for Microtask Crowdsourcing Campaigns},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383546},
doi = {10.1145/3366424.3383546},
abstract = {This demo presents VirtualCrowd, a simulation platform for crowdsourcing campaigns. The platform allows the design, configuration, step-by-step execution, and analysis of customized tasks, worker profiles, and crowdsourcing strategies. The platform will be demonstrated through a crowd-mapping example in two cities, which will highlight the utility of VirtualCrowd for complex crowdsourcing tasks in real world settings.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {222–225},
numpages = {4},
keywords = {Crowdsourcing, Simulation, Discrete Event System},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383547,
author = {Leonhardt, Jurek and Anand, Avishek and Khosla, Megha},
title = {Boilerplate Removal Using a Neural Sequence Labeling Model},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383547},
doi = {10.1145/3366424.3383547},
abstract = {The extraction of main content from web pages is an important task for numerous applications, ranging from usability aspects, like reader views for news articles in web browsers, to information retrieval or natural language processing. Existing approaches are lacking as they rely on large amounts of hand-crafted features for classification. This results in models that are tailored to a specific distribution of web pages, e.g. from a certain time frame, but lack in generalization power. We propose a neural sequence labeling model that does not rely on any hand-crafted features but takes only the HTML tags and words that appear in a web page as input. This allows us to present a browser extension which highlights the content of arbitrary web pages directly within the browser using our model. In addition, we create a new, more current dataset to show that our model is able to adapt to changes in the structure of web pages and outperform the state-of-the-art model.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {226–229},
numpages = {4},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383548,
author = {Lin, Fandel and Fang, Jie-Yu and Hsieh, Hsun-Ping},
title = {Customizing Your Own Route with QQIP. A Quantitative and Qualitative Itinerary Planner for New Transportation Routes},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383548},
doi = {10.1145/3366424.3383548},
abstract = {Public transportation route planning is crucial for both traffic management authority and residents. Current procedures for deciding new routes are time-consuming and ineffective due to the complicate simulation process or overwhelming numbers of opinions from stockholders. In this paper, we propose a novel decision supporting tool, Quantitative and Qualitative Itinerary Planner (QQIP), to help governments pre-evaluate new route services in the city in a timely manner. The function of QQIP is three-fold: visualization of urban informatics, a flexible interface for sketching designate routes, and passenger flows estimation in certain time intervals. With acquired relevant urban information, user can pre-estimate the effectiveness of designed routes using QQIP. To capture the spatial-temporal factors correlated with passenger flows, we propose route-affecting region (RAR) and adopt Deep Neural Network (DNN) framework to combine several dynamic and static features. According to our experimental results on bus-ticket data of Tainan city, the proposed RAR-based feature engineering methods are effective for handling and combining high-correlated dynamic and static data; meanwhile, QQIP can help decision makers infer the passenger flow effectively and efficiently for given designated routes. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {230–234},
numpages = {5},
keywords = {Urban Planning, Itinerary Planner, Feature Engineering, Passenger Volume Estimation, Interactive Route Sketch},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383549,
author = {Fernando Monsores Passos Maia, Lu\'{\i}s and Lenzi, Marcia and Rabello, Elaine and Oliveira, Jonice},
title = {A Web Tool to Map Research Impacts Via Altmetrics},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383549},
doi = {10.1145/3366424.3383549},
abstract = {Currently, there is a big concern on governments and research institutes on assessing the population awareness about the scientific innovations, such as new food production technologies and the development of drugs against emerging and neglected diseases. There is an unmet demand for new ways to show the impact of scientific research on social media (population’s main communication vehicles) and assess the social outreach of the scientific output. This article presents a novel web tool to map research impacts via Altmetrics, wich are alternative metrics based on the exchange of scientific knowledge on social media and online environments.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {235–239},
numpages = {5},
keywords = {altmetrics, zika, bibliometrics, social network analysis, chikungunya},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383550,
author = {Zha, Zhichao},
title = {TaxAA: A Reliable Tax Auditor Assistant for Exploring Suspicious Transactions},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383550},
doi = {10.1145/3366424.3383550},
abstract = {We present TaxAA, a reliable tax auditor assistant that helps tax auditors explore suspicious transactions and get reliable evidence. We construct a Tax Audit Network(TAN) and use extended algorithms based on the semi-supervised Graph Convolutional Network(GCN) to build detection models for calculating taxpayers’ suspicion score, we choose Hierarchical Graph Convolutional Network(H-GCN) as our basic model according to the experimental results. Then the visual analytic system allows tax auditors to customize suspicious indicators to observe the suspicious relationships among taxpayers by the ”wheel” chart. Meanwhile, it can provide detailed transactions and individual information for reference. We have evaluated the assistant based on the tax data of a province, our detection model can achieve high accuracy and the visual analytic system can provide useful guidance for tax auditors.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {240–244},
numpages = {5},
keywords = {H-GCN, visual analytic system, tax audit},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383551,
author = {Yaghoub-Zadeh-Fard, Mohammad-Ali and Zamanirad, Shayan and Benatallah, Boualem and Casati, Fabio},
title = {REST2Bot: Bridging the Gap between Bot Platforms and REST APIs},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383551},
doi = {10.1145/3366424.3383551},
abstract = {With the development of REST (REpresentational State Transfer) APIs, many applications have been designed to harness their potential. As such, bots emerged recently as natural interfaces to facilitate conversations between humans and API-accessible services. Existing bot development platforms (e.g., Dialogflow, Wit.ai) facilitate building bots, but bot developers are still required to provide training data by defining corresponding intents (user’s intention such as booking a hotel) and entities (e.g., hotel location) for each API. Moreover, bot developers are required to build and deploy webhook functions to invoke APIs on intents detection. In this paper, we introduce REST2Bot, a tool that addresses these shortcomings (e.g., translating APIs to Intents, and invoking APIs based on detected Intents) in bot development frameworks to automate several tasks in the life cycle of the bot development process. REST2Bot relies on automated approaches for parsing OpenAPI specifications, generating training data, building bots on desired bot development frameworks, and generating deployable webhook functions to map intents and entities to APIs.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {245–248},
numpages = {4},
keywords = {Chatbots, Automated Bot Development, REST APIs, Paraphrasing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383552,
author = {Qiu, Siyuan and Xu, Binxia and Zhang, Jie and Wang, Yafang and Shen, Xiaoyu and de Melo, Gerard and Long, Chong and Li, Xiaolong},
title = {EasyAug: An Automatic Textual Data Augmentation Platform for Classification Tasks},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383552},
doi = {10.1145/3366424.3383552},
abstract = {Imbalanced data is a perennial problem that impedes the learning abilities of current machine learning-based classification models. One approach to address it is to leverage data augmentation to expand the training set. For image data, there are a number of suitable augmentation techniques that have proven effective in previous work. For textual data, however, due to the discrete units inherent in natural language, techniques that randomly perturb the signal may be ineffective. Additionally, due to the substantial discrepancy between different textual datasets (e.g., different domains), an augmentation approach that facilitates the classification on one dataset may be detrimental on another dataset. For practitioners, comparing different data augmentation techniques is non-trivial, as the corresponding methods might need to be incorporated into different system architectures, and the implementation of some approaches, such as generative models, is laborious. To address these challenges, we develop EasyAug, a data augmentation platform that provides several augmentation approaches. Users can conveniently compare the classification results and can easily choose the most suitable one for their own dataset. In addition, the system is extensible and can incorporate further augmentation approaches, such that with minimal effort a new method can comprehensively be compared with the baselines. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {249–252},
numpages = {4},
keywords = {data augmentation, text classification, text generation, model fusion, imbalanced data},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383553,
author = {Beheshti, Amin and Tabebordbar, Alireza and Benatallah, Boualem},
title = {IStory: Intelligent Storytelling with Social Data},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383553},
doi = {10.1145/3366424.3383553},
abstract = {The production of knowledge from ever increasing amount of social data is seen by many organizations as an increasingly important capability that can complement the traditional analytics sources. Examples include extracting knowledge and deriving insights from social data to improve government services, predict intelligence activities, personalize the advertisements in elections and improve national security and public health. Understanding social data can be challenging as the analysis goal can be subjective. In this context, storytelling is considered as an appropriate metaphor as it facilitates understanding and surfacing insights which is embedded within the data. In this paper, we focus on the research problem of ‘understanding the social data’ in general and more particularly the curation, summarization and presentation of large amounts of social data. The goal is to enable intelligent narrative construction based on the important features (extracted and ranked automatically) and enable storytelling at multiple levels and from different views using novel summarization techniques. We implement an interactive storytelling dashboard, namely iStory, and focus on a motivating scenario for analyzing Urban Social Issues from Twitter as it relates to the Australian Government Budget, to highlight how storytelling can significantly facilitate understanding social data. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {253–256},
numpages = {4},
keywords = {Data Curation, Data Lake, Summarization, Knowledge Lake, Storytelling},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382084,
author = {Mossie, Zewdie},
title = {Social Media Dark Side Content Detection Using Transfer Learning Emphasis on Hate and Conflict},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382084},
doi = {10.1145/3366424.3382084},
abstract = {Although online content continues to grow, the prevalence of dark side content such as hate, misinformation, disinformation, conflicting, fake, and so on continues to grow and has become a problem for online and offline society. Consequently, work into automated analytical and detection methods has gained much attention. The scarcity of the labeled dataset has, however, become one of the major challenges in both machine and deep learning to develop an effective supervised learning model. As a result, most State-of-the-Art (SOTA) approaches focus on English languages for the detection of such content. The identification task of such content has become a problem due to the diversity of languages used on social media platforms. We propose transfer learning since it needs only access to a large unlabeled text available on social media platforms. Since we use data from Amharic Language, which is in the low-resource language family for machine leaarning, transfer learning is found effective. First, we prepare a topic and word embedding models using Facebook data as a task-specific and a general corpus from different web domains respectively. Second, we combine topic embedding and word embedding and then send the features to a fully-connected Recurrent Neural Networks (RNNs). Our preliminary experimental results from the newly proposed attention-based topic model combined with word embedding outperform the baselines.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {259–263},
numpages = {5},
keywords = {Pre-trained model, Low-resource language, Transfer learning, Word-topic embeddings, Social media dark side},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382085,
author = {Jenkins, Porter},
title = {Structured Paragraph Embeddings of Financial Earnings Calls},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382085},
doi = {10.1145/3366424.3382085},
abstract = {Financial earnings calls contain rich information about the quarterly performance and future projections of public companies. Such information is highly relevant to developing trading strategies and understanding economic trends. However, due to the unstructured nature of call transcripts important signals can be difficult to extract. In this preliminary work, we propose a novel paragraph embedding method that leverages the structure inherent in the Q&amp;A format of earnings calls. We show that the proposed method improves classification performance over more general methods and provides a useful measure of similarity between paragraphs.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {264–268},
numpages = {5},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382086,
author = {Lee, Jieh-Sheng},
title = {Measuring and Controlling Text Generation by Semantic Search},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382086},
doi = {10.1145/3366424.3382086},
abstract = {Our motivation in this work is to measure patent text generation by semantic search, particularly by textual similarity in high dimensional space for neural network models. The objective is to control patent text generation by semantic search. Conceptually it is an attempt to integrate two subfields in NLP: text generation and semantic search. In our previous milestone of the PatentTransformer project, a prototype based on GPT-2 is capable of generating fluent patent title, abstract, independent claim, and dependent claim. However, beneath the surface form, the quality issue in the generated patent text was less explored. How to control text generation is also a hard problem in NLP field. We would like to address these issues in this work and experiment with different approaches. On the measurement side, this work will address the quality measurement issue from the perspective of textual similarity. Based on that, the approaches we propose include two embedding spaces, span-based textual similarity, and language model for patent claim spans. One the control side, we propose a knob-turning approach for controlling text generation based on measuring a range of textual similarity. In this way, we can search for a Goldilocks zone in which the similarity of generated patent text is close to but not too far from prior patents. We hypothesize that patent novelty may exist in such a zone.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {269–273},
numpages = {5},
keywords = {patent, semantic search, GPT-2., textual similarity, natural language generation, natural language processing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382087,
author = {An, Kijin},
title = {Enhancing Web App Execution with Automated Reengineering},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382087},
doi = {10.1145/3366424.3382087},
abstract = {The execution of modern web applications is affected by distribution, mobility, and heterogeneity. The design-time assumptions of web applications rarely correspond to their runtime conditions. As a result, the efficiency, performance, and reliability of web app execution can suffer. This dissertation research addresses this problem by introducing novel automated reengineering techniques. In particular, we put forward a series of novel domain-specific refactorings, semantics-preserving program transformations, that behind-the-scenes improve the efficiency, performance, and reliability of extant web applications. To that end, we apply state-of-the-art program analysis and transformation techniques, extending and adapting them for the domain of web applications. Our ultimate objective is to enhance the execution of real-world web apps to meet the requirements of modern users, so the web can remain the most versatile computing application deployment and delivery infrastructure. The intermediate results of this dissertation research have been accepted for publication as a full research paper in the proceedings of the Web Conference 2020&nbsp;[3]. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {274–278},
numpages = {5},
keywords = {Reengineering, Middleware, Web Applications, Mobile Apps, Program Analysis &amp; Transformation, JavaScript, Software Engineering},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382088,
author = {Yang, Hui-Kuo},
title = {Learning Topic Map from Large Scale Social Media Data},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382088},
doi = {10.1145/3366424.3382088},
abstract = {Geo-tagged social media data are hugely generated every day. Those content provide rich sources to explore keywords and topics in any region of the world thanks to the widely popularized mobile internet services. The association between geographic regions and their keywords/topics in social media has raised a lot of research attentions. Such association provides important information to event prediction, source detection, and news propagation in various applications. For instance, the disaster control and management, and evaluation of sales marketing campaign are just two of the many examples. The association between regions and keywords/topics are analogous to that between geographic coordinates and spatial features, such as streets intersections, building compounds and so on in a conventional street map, and we propose a new model, called “topic map” to mimic such an analogy and to encode and represent text features with geographic regions that reside in Geo-tagged social media data. Applications based on topic map will be explored during my research, and extensions to temporal data will be investigated further.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {279–283},
numpages = {5},
keywords = {Social Media Data, Spatial Model, Geo-tagging, Topic Model},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382089,
author = {Tigunova, Anna},
title = {Extracting Personal Information from Conversations},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382089},
doi = {10.1145/3366424.3382089},
abstract = {In my doctoral research, I focus on inferring personal information from conversational texts. Such unstructured input does not provide sufficient explicit assertions, hence, personal facts have to be inferred from latent cues or speech style. In my work I investigate what salient attributes can be extracted from dialogues and which tools may be applicable for that.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {284–288},
numpages = {5},
keywords = {neural networks, user profiling, personal knowledge, Information extraction, conversational text},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382090,
author = {Tseng, Yun-Chien},
title = {PKE: A Model for Recommender Systems in Online Service Platform},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382090},
doi = {10.1145/3366424.3382090},
abstract = {Graph embedding is a technique that has grown attention in recent years. Apart from mining implicit information in a graph representation data, graph embedding can be used in recommender system. Recommender system is a tool that can help e-sellers collect users’ information more easily. This is beneficial for platform providers to mine users’ interests with more data. However, it is easy to be distracted by other unrelated information when too many data are collected. In this paper, we proposed an idea called information matrix to combine information from different data. This idea considers data as graphics; hence, connects data with common nodes and extends to vectors through keyword embedding. In addition, we constructed a model that illustrates the efficiency and ability of the information matrix. This model was tested on data provided by e-sellers. Our aim was to combine browsing data and order data, and transfer these data into information matrix with high accuracy. Although the accuracy rate drops after transferring information into embedded type, it provides an idea for potential solution of cold start, a common problem in recommender systems. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {289–293},
numpages = {5},
keywords = {Recommendation, Graph embedding, Keywords analysis},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382091,
author = {Almerekhi, Hind and Jansen, Supervised by Bernard J. and Kwak, co-supervised by Haewoon},
title = {Investigating Toxicity Across Multiple Reddit Communities, Users, and Moderators},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382091},
doi = {10.1145/3366424.3382091},
abstract = {Online platforms like Reddit enable users to build communities and converse about diverse topics and interests. However, with the increasing number of users that post disturbing comments containing profanity, harassment, and hate speech, otherwise known as toxic comments. Moderators often struggle with managing the safety of discussions in online communities. To address these issues, we need to detect toxic comments and the root causes of toxicity in discussion threads, i.e., toxicity triggers. Additionally, we need to investigate the toxic posting behavior of users to understand how it differs across online communities and consolidate our findings with moderators from Reddit. In this work, we present our approach, which builds on state-of-the-art methods of toxic comment and toxicity trigger detection. Lastly, we present our research findings of investigating toxicity across users and moderators on Reddit.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {294–298},
numpages = {5},
keywords = {online communities, toxicity, discussion threads, Reddit, trigger detection},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383107,
author = {Oosterhuis, Harrie and Jagerman, Rolf and de Rijke, Maarten},
title = {Unbiased Learning to Rank: Counterfactual and Online Approaches},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383107},
doi = {10.1145/3366424.3383107},
abstract = {This tutorial is about Unbiased Learning to Rank, a recent research field that aims to learn unbiased user preferences from biased user interactions. We will provide an overview of the two main families of methods in Unbiased Learning to Rank: Counterfactual Learning to Rank (CLTR) and Online Learning to Rank (OLTR) and their underlying theory. First, the tutorial will start with a brief introduction to the general Learning to Rank (LTR) field and the difficulties user interactions pose for traditional supervised LTR methods. The second part will cover Counterfactual Learning to Rank (CLTR), a LTR field that sprung out of click models. Using an explicit model of user biases, CLTR methods correct for them in their learning process and can learn from historical data. Besides these methods, we will also cover practical considerations, such as how certain biases can be estimated. In the third part of the tutorial we focus on Online Learning to Rank (OLTR), methods that learn by directly interacting with users and dealing with biases by adding stochasticity to displayed results. We will cover cascading bandits, dueling bandit techniques and the most recent pairwise differentiable approach. Finally, in the concluding part of the tutorial, both approaches are contrasted, highlighting their relative strengths and weaknesses, and presenting future directions of research. For LTR practitioners our comparison gives guidance on how the choice between methods should be made. For the field of Information Retrieval (IR) we aim to provide an essential guide on unbiased LTR to understanding and choosing between methodologies.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {299–300},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383108,
author = {Cheng, Gong and Gunaratna, Kalpa and Kharlamov, Evgeny},
title = {Entity Summarization in Knowledge Graphs: Algorithms, Evaluation, and Applications},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383108},
doi = {10.1145/3366424.3383108},
abstract = {Knowledge graphs (KGs) encapsulate entities and relationships that describe the entities. The concise representation format and graph nature of KGs have resulted in creating many novel Web and industrial applications and enhancing existing ones. However, in a KG, dozens or hundreds of facts describing an entity could exceed the capacity of a typical user interface and overload users with excessive amounts of information. This has motivated fruitful research on entity summarization—automated generation of compact summaries for entities to satisfy users’ information needs efficiently and effectively. Over the recent years, researchers have contributed to this problem by proposing approaches ranging from pure ranking and mining techniques to machine and deep learning techniques. The state of the art has continuously improved and at the same time made it harder for the community and new comers to the problem to keep up with the recent contributions and basic building blocks in the space. This tutorial aims to fill this gap.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {301–302},
numpages = {2},
keywords = {semantic web, entity summarization, knowledge graph},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383110,
author = {Gade, Krishna and Geyik, Sahin and Kenthapadi, Krishnaram and Mithal, Varun and Taly, Ankur},
title = {Explainable AI in Industry: Practical Challenges and Lessons Learned},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383110},
doi = {10.1145/3366424.3383110},
abstract = {Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with the proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI have become far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability [11]. Model explainability is considered a prerequisite for building trust and adoption of AI systems in high stakes domains such as lending and healthcare&nbsp;[1] which require reliability, safety, and fairness. It is also critical to automated transportation, and other industrial applications with significant socio-economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling. As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale [14, 15, 25]. The challenges for the research community include: (i) defining model explainability, (ii) formulating explainability tasks for understanding model behavior and developing solutions for these tasks, and finally (iii) designing measures for evaluating the performance of models in explainability tasks. In this tutorial, we will first motivate the need for model interpretability and explainability in AI [6] from societal, legal, enterprise, end-user, and model developer perspectives, and present techniques &amp; tools for providing explainability as part of AI/ML systems [13]. Then, we will focus on the real-world application of explainability techniques in industry, wherein we present practical challenges &amp; implications for using explainability techniques effectively and lessons learned from deploying explainable models for several web-scale machine learning and data mining applications. We will present case studies across different companies, spanning application domains such as search and recommendation systems, hiring, lending, sales, and fraud detection. Finally, based on our experiences in industry, we will identify open problems and research directions for the WWW community. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {303–304},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383111,
author = {Zheng, Da and Wang, Minjie and Gan, Quan and Zhang, Zheng and Karypis, George},
title = {Learning Graph Neural Networks with Deep Graph Library},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383111},
doi = {10.1145/3366424.3383111},
abstract = {Learning from graph and relational data plays a major role in many applications including social network analysis, marketing, e-commerce, information retrieval, knowledge modeling, medical and biological sciences, engineering, and others. In the last few years, Graph Neural Networks (GNNs) have emerged as a promising new supervised learning framework capable of bringing the power of deep representation learning to graph and relational data. This ever-growing body of research has shown that GNNs achieve state-of-the-art performance for problems such as link prediction, fraud detection, target-ligand binding activity prediction, knowledge-graph completion, and product recommendations. The objective of this tutorial is twofold. First, it will provide an overview of the theory behind GNNs, discuss the types of problems that GNNs are well suited for, and introduce some of the most widely used GNN model architectures and problems/applications that are designed to solve. Second, it will introduce the Deep Graph Library (DGL), a new software framework that simplifies the development of efficient GNN-based training and inference programs. To make things concrete, the tutorial will provide hands-on sessions using DGL. This hands-on part will cover both basic graph applications (e.g., node classification and link prediction), as well as more advanced topics including training GNNs on large graphs and in a distributed setting. In addition, it will provide hands-on tutorials on using GNNs and DGL for real-world applications such as recommendation and fraud detection. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {305–306},
numpages = {2},
keywords = {applications, graph neural networks, Deep Graph Library},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383112,
author = {He, Qi and Yang, Jaewon and Shi, Baoxu},
title = {Constructing Knowledge Graph for Social Networks in A Deep and Holistic Way},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383112},
doi = {10.1145/3366424.3383112},
abstract = {Online social networks such as Facebook and LinkedIn have been an integrated part of everyday life. To improve the user experience and power the products around the social network, Knowledge Graphs (KG) are used as a standard way to extract and organize the knowledge in social networks. This tutorial focuses on how to build KGs for social networks by developing deep NLP models, and holistic optimization of KGs and the social network. Building KG for social networks poses two challenges: 1) input data for each member in the social network is noisy, implicit and in multilingual, so a deep understanding of the input data is needed; 2) KG and the social network influence each other via multiple organic feedback loops, so a holistic view on both networks is needed. We will share the lessons we learned from tackling the above challenges in the past seven years on building the Knowledge Graph for the LinkedIn social network. To address the first challenge of noisy and implicit input data, we present how to train high precision language understanding models by adding small clean data to the noisy data. By doing so, we enhance the-state-of-the-art NLP models such as BERT for building KG. To address multilingual aspect of the input data, we explain how to expand a single-language KG to multilingual KGs by applying transfer learning. For the second challenge of modeling interactions between social network and KG, we launch new products to get explicit feedback on KG from users, and refine KG by learning deep embeddings from the social network. Lastly, we present how we use our KG to empower more than 20+ products at LinkedIn with high business impacts.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {307–308},
numpages = {2},
keywords = {Social Network, Knowledge Graph Construction, Knowledge Graph},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383113,
author = {Gionis, Aristides and Matakos, Antonis and Ordozgoiti, Bruno and Xiao, Han},
title = {Mining Signed Networks: Theory and Applications: Tutorial Proposal for the Web Conference 2020},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383113},
doi = {10.1145/3366424.3383113},
abstract = {Signed networks transform the information encoded by conventional graphs by attaching either a positive or a negative sign to every edge. This subtle modification vastly enhances the modelling capabilities of graphs. For instance, in a social network, where edges might represent interactions between users, the sign may determine whether an exchange was friendly or hostile. However, the introduction of edge signs invalidates many established methods and results from the graph-mining toolbox, and thus, problem formulations and algorithmic techniques must be studied anew. In this tutorial we aim to provide an overview of the literature in mining signed networks. We will present the most important theoretical results since their inception to the present day, we will discuss some of the most common applications, and we will reflect on emerging applications and directions for future work.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {309–310},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383114,
author = {Saez-Trumper, Diego and Redi, Miriam},
title = {Wikimedia Public (Research) Resources},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383114},
doi = {10.1145/3366424.3383114},
abstract = {The Wikimedia Foundation’s mission is to disseminate open knowledge effectively and globally. In keeping with this mission, we support research in areas that benefit the free knowledge community. We aim to make any work with our support openly available to the public. At the same time that we do a minimalist user data collection, all the material (text and multimedia) available in our projects is public and reusable by everybody under a creative commons license. Moreover, all the article’s history and interactions among users are also public, and we offer a set of tools for accessing such data. In this tutorial we are going to give an overview on all the data sources, and a detailed explanation of how to interact with this content including data and tools such as the Wikipedia Dumps, Quarry (SQL Replicas), Pageviews, PAWS (Wikimedia hosted Jupyter Notebooks), WikiData and put special attention to the multimedia content hosted by Wikimedia Commons project.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {311–312},
numpages = {2},
keywords = {gaze detection, text tagging, datasets, neural networks},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383115,
author = {Li, Yanen and Yang, Yang and Zhou, Sen and Qiao, Jian and Long, Bo},
title = {Deep Transfer Learning for Search and Recommendation},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383115},
doi = {10.1145/3366424.3383115},
abstract = {Training data sparsity is a common problem for many real-world applications in Search and Recommendation domains. Even for applications with a lot of training data, in the cold-start scenario we usually do not get enough labeled data. Transfer Learning is a promising approach for addressing this problem. In addition, features might interact with each other in a complex way that traditional approaches might not be able to represent, Deep Transfer Learning, which leverages Deep Neural Networks for Transfer Learning, might be able to catch such deep patterns hidden in complex feature interactions. Due to these reasons, recently Deep Transfer Learning research has gained a lot of attention and has been successfully applied to many real-world applications. This tutorial offers an overview of Deep Transfer Learning approaches in Search and Recommendation domains from the industry perspective. In this tutorial We first introduce the basic concepts and major categories of Deep Transfer Learning. Then we focus on recent developments of Deep Transfer Learning approaches in the Search and Recommendation domains. After that we will introduce two real-world examples of how to apply Deep Transfer Learning methods to improve Search and Recommendation performance at LinkedIn. Finally we will conclude the tutorial with discussion of future directions.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {313–314},
numpages = {2},
keywords = {Search and Recommendation, Unified Embeddings, Deep Transfer Learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383116,
author = {Wagner, Isabel},
title = {Methods of Corporate Surveillance: A Primer on Experimental Transparency Research},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383116},
doi = {10.1145/3366424.3383116},
abstract = {News headlines about privacy invasions, discrimination, and biases discovered in the platforms of big technology companies are commonplace today. The headlines – ranging from comprehensive profiling of users, to microtargeting of political messages, to discrimination based on gender, race, and age – show that big tech’s operations can cause real-world harm. However, big tech companies are reluctant to disclose how they operate and typically do not give out specific information, such as what data they collect or how their algorithms make decisions. This secretive operation counteracts ideals of transparency, openness, and accountability. This tutorial will present research methods and findings from the last 5-10 years that use large-scale experiments to systematically interact with the publicly accessible elements of big tech’s platforms. These experiments allow inferring details about big tech’s hidden operations – in essence conducting meta-surveillance against big tech. For example, findings from these experiments have documented how user tracking works, the extent of tracking on the web today, and how the collected information is used for algorithmic decision-making and ad targeting.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {315–316},
numpages = {2},
keywords = {advertising, profiling, black-box analysis, algorithm auditing, tracking, research methods, information flow, corporate surveillance, blackbox experiments, privacy, discrimination, transparency},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383117,
author = {Gupta, Somit and Shi, Xiaolin and Dmitriev, Pavel and Fu, Xin},
title = {Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383117},
doi = {10.1145/3366424.3383117},
abstract = {A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the “why” behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program 11, 14. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions. A version of this tutorial was also present at KDD 2019 23. It was attended by around 150 participants. This tutorial has also been accepted for the WSDM 2020 conference.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {317–319},
numpages = {3},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383118,
author = {Faloutsos, Christos and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim and Wang, Yuyang},
title = {Forecasting Big Time Series: Theory and Practice},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383118},
doi = {10.1145/3366424.3383118},
abstract = {Time series forecasting is a key ingredient in the automation and optimization of business processes: in retail, deciding which products to order and where to store them depends on the forecasts of future demand in different regions; in cloud computing, the estimated future usage of services and infrastructure components guides capacity planning; and workforce scheduling in warehouses and factories requires forecasts of the future workload. Recent years have witnessed a paradigm shift in forecasting techniques and applications, from computer-assisted model- and assumption-based to data-driven and fully-automated. This shift can be attributed to the availability of large, rich, and diverse time series data sources and result in a set of challenges that need to be addressed such as the following. How can we build statistical models to efficiently and effectively learn to forecast from large and diverse data sources? How can we leverage the statistical power of “similar” time series to improve forecasts in the case of limited observations? What are the implications for building forecasting systems that can handle large data volumes? The objective of this tutorial is to provide a concise and intuitive overview of the most important methods and tools available for solving large-scale forecasting problems. We review the state of the art in both: (1) classical modeling of time series, (2) deep learning for forecasting. We also discuss the practical aspects of building a large scale forecasting system, including data integration, feature generation, backtesting framework, error tracking and analysis, etc. Accompanied with the practice side is a hands-on session, where we would engage the audience with Jupyter notebooks that demonstrates the key concepts in the theory part. Furthermore, we provides interactive demos, showing various avenues to solve business problems with AWS Forecasting offerings such as GluonTS, DeepAR (SageMaker), and Amazon Forecast.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {320–321},
numpages = {2},
keywords = {Time Series, Forecasting, Neural Network},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382183,
author = {Gandhi, Sahaj and Mansouri, Behrooz and Campos, Ricardo and Jatowt, Adam},
title = {Event-Related Query Classification with Deep Neural Networks},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382183},
doi = {10.1145/3366424.3382183},
abstract = {Users tend to search over the Internet to get the most updated news when an event occurs. Search engines should then be capable of effectively retrieving relevant documents for event-related queries. As the previous studies have shown, different retrieval models are needed for different types of events. Therefore, the first step for improving effectiveness is identifying the event-related queries and determining their types. In this paper, we propose a novel model based on deep neural networks to classify event-related queries into four categories: periodic, aperiodic, one-time-only, and non-event. The proposed model combines recurrent neural networks (by feeding two LSTM layers with query frequencies) and visual recognition models (by transforming time-series data from a 1D signal to a 2D image - later passed to a CNN model) for effective query type estimation. Worth noting is that our method uses only the time-series data of query frequencies, without the need to resort to any external sources such as contextual data, which makes it language and domain-independent with regards to the query issued. For evaluation, we build upon the previous datasets on event-related queries to create a new dataset that fits the purpose of our experiments. The obtained results show that our proposed model can achieve an F1-score of 0.87.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {324–330},
numpages = {7},
keywords = {Temporal Queries, Event Queries, Temporal Information Retrieval},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382184,
author = {Hernandez, Anthony and Ng, Kin and Iamnitchi, Adriana},
title = {Using Deep Learning for Temporal Forecasting of User Activity on Social Media: Challenges and Limitations},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382184},
doi = {10.1145/3366424.3382184},
abstract = {The recent advances in neural network-based machine learning algorithms promise a revolution in prediction-based tasks in a variety of domains. Of these, forecasting user activity in social media is particularly relevant for problems such as modeling and predicting information diffusion and designing intervention techniques to mitigate disinformation campaigns. Social media seems an ideal context for applying neural network techniques, as they provide large datasets and challenging prediction objectives. Yet, our experiments find a number of limitations in the power of deep neural networks and traditional machine learning approaches in predicting user activity on social media platforms. These limitations are related to dataset characteristics due to temporal aspects of user behavior. This work describes the challenges we encountered while attempting to forecast user activity on two popular social interaction sites: Twitter and GitHub.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {331–336},
numpages = {6},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382185,
author = {Macha, Meghanath and Venkitachalam, Shankar and Pai, Deepak},
title = {CrEOS: Identifying Critical Events in Online Sessions},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382185},
doi = {10.1145/3366424.3382185},
abstract = {Analyzing a consumer’s experience in an online session is a fundamental task of a marketer to enrich, personalize and enhance their future interactions with a firm. Click (web) or tap (mobile) event streams efficiently capture a consumer’s experience in an online session. In this work, we propose CrEOS contributing to the extant literature in the deployment of deep learning techniques to model online consumer behaviour. CrEOS models consumer conversion behaviour using a multi-variable LSTM, enriching the consumer event stream with a series of contextual features. In addition to the predictions, CrEOS identifies events that either hamper or help a conversion at three levels - consumer, consumer segment and transitions in marketing funnel. We validate CrEOS on click-stream data of a large US based e-commerce firm, compare it with single variable LSTM and discuss the insights derived from the critical event identification considering a series of marketing tasks.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {337–342},
numpages = {6},
keywords = {marketing, consumer behavior, counter factual analysis, deep learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382186,
author = {Martinc, Matej and Montariol, Syrielle and Zosa, Elaine and Pivovarova, Lidia},
title = {Capturing Evolution in Word Usage: Just Add More Clusters?},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382186},
doi = {10.1145/3366424.3382186},
abstract = {The way the words are used evolves through time, mirroring cultural or technological evolution of society. Semantic change detection is the task of detecting and analysing word evolution in textual data, even in short periods of time. In this paper we focus on a new set of methods relying on contextualised embeddings, a type of semantic modelling that revolutionised the NLP field recently. We leverage the ability of the transformer-based BERT model to generate contextualised embeddings capable of detecting semantic change of words across time. Several approaches are compared in a common setting in order to establish strengths and weaknesses for each of them. We also propose several ideas for improvements, managing to drastically improve the performance of existing approaches. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {343–349},
numpages = {7},
keywords = {Contextualised Embeddings, Clustering, Semantic Change},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383296,
author = {Sinha, Priyanka and Dey, Lipika and Mitra, Pabitra and Thomas, Dilys},
title = {A Hierarchical Clustering Algorithm for Characterizing Social Media Users},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383296},
doi = {10.1145/3366424.3383296},
abstract = {In this paper we propose a method to characterize user behavior from their engagement with enterprise social media. Content analysis often suffers challenges due to noise. Here we study behavior using temporal activity, i.e., the number of posts per month represented as a time series. User posting volume on social media has a long tailed nature. It causes time series clustering algorithms to result in unbalanced clusters with either very few users or almost all users. Thus we propose a hierarchical time series clustering algorithm to group users according to their behavioral homogeneity and provide interpretable characterizations to the resulting clusters. Users in distinct clusters deviate significantly in their topics of interest while being homophilic (near identical or similar minded) within the cluster. Goodness of the clustering is observed over Enterprise Social Media (ESM); Stackexchange; and Linux Kernel Mailing List (LKML) datasets as opposed to existing clustering techniques.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {353–362},
numpages = {10},
keywords = {clustering, time series, enterprise social media, employee behavior},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383297,
author = {Saxena, Akrati and Hsu, Wynne and Lee, Mong Li and Leong Chieu, Hai and Ng, Lynette and Teow, Loo Nin},
title = {Mitigating Misinformation in Online Social Network with Top-k Debunkers and Evolving User Opinions},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383297},
doi = {10.1145/3366424.3383297},
abstract = {Online social networks provide an easy platform to share the information, and the spread of fake news and rumors has become prevalent, with severe consequences on major events including the US and Jakarta elections. Existing works have designed methods to find a set of top-k users to launch truth-campaigns and mitigate the negative influence of misinformation. The assumption is that these top-k users are open and willing to disseminate fact-checked content. Further, these methods assume that as misinformation and counter messages propagate in the network, user opinions once formed do not change. In this work, we address a more realistic scenario where users’ opinion can fluctuate before some deadline, and the goal is to find a good seed set of users from a set of debunkers to minimize the impact of misinformation. We propose a new opinion model that takes into account users’ biases and their social neighbours’ opinions. Based on this model, we design a mitigation solution to identify a subset of debunkers that maximizes the number of users who have been exposed to the misinformation but choose to believe the counter message. Experiments on Facebook and Twitter datasets demonstrate that our proposed solution is effective in mitigating the negative influences of misinformation. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {363–370},
numpages = {8},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383298,
author = {Chowdhury, Farhan Asif and Allen, Lawrence and Yousuf, Mohammad and Mueen, Abdullah},
title = {On Twitter Purge: A Retrospective Analysis of Suspended Users},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383298},
doi = {10.1145/3366424.3383298},
abstract = {Abuse and spam in Twitter have long been a pressing issue, and in response, Twitter regularly purges (i.e., suspends in mass) accounts that violate Twitter Rules. However, there is no available information about the characteristics and activities of these regularly purged users. We have developed a novel and comprehensive measurement mechanism to identify millions of purged Twitter users and collect their tweets. We have identified 2.4M purged users and collected 1M tweets made by them over eight months. Using our dataset, we perform a retrospective analysis to characterize their account properties and behavioral activities. We analyze their tweet content to identify their role and abuse strategy over-time. Our analysis shows that the abuse on Twitter is pervasive globally and not confined in mere spamming. Alarmingly, more than 60% of the purged users survived on Twitter for more than two years. We observe that politics is a major theme among the purged users irrespective of language and location, and these politically motivated users spread controversial content consistently over time. However, the spammers reorient their agenda across time to participate in multiple marketing campaigns. We also discover interaction and associated communities among purged users. Our analysis sheds new light on the evolving nature of abuse in Twitter that can help researchers understanding the characteristics and behavior of emerging malicious users to develop an effective defense system. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {371–378},
numpages = {8},
keywords = {Abuse, Suspension, Social Networks},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383299,
author = {Bo, Hongbo and McConville, Ryan and Hong, Jun and Liu, Weiru},
title = {Social Network Influence Ranking via Embedding Network Interactions for User Recommendation},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383299},
doi = {10.1145/3366424.3383299},
abstract = {Within social networks user influence may be modelled based on user interactions. Further, it is typical to recommend users to others. What is the role of user influence in user recommendation? In this paper, we first propose to use a node embedding approach to integrate many types of interaction into embedded spaces where we then define a novel closeness measure to quantify the closeness of users based on interactions. We then propose a new influence ranking algorithm based on PageRank by incorporating the closeness measure into the ranking mechanism. We evaluate our algorithm, EIRank, using a dataset collected from Twitter. Our experimental results show that our algorithm measures user influence better by way of a user recommendation task, where our algorithm outperforms TwitterRank across a range of experimental network settings.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {379–384},
numpages = {6},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383300,
author = {Cenikj, Gjorgjina and Gievska, Sonja},
title = {Boosting Recommender Systems with Advanced Embedding Models},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383300},
doi = {10.1145/3366424.3383300},
abstract = {Recommender systems are paramount in providing personalized content and intelligent content filtering on any social media platform, web portal, and online application. In line with the current trends in the field directed towards mapping problem and data encoding representations from other fields, this research investigates the feasibility of augmenting a graph-based recommender system for Amazon products with two state-of-the-art representation models. In particular, the potential benefits of using the language representation model BERT and GraphSage based representations of nodes and edges for improving the quality of the recommendations were investigated. Link prediction and link attribute inference were used to identify the products that the users will buy and predict the rating they will give to a product, respectively. The initial results of our exploratory study are encouraging and point to potential directions for future research.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {385–389},
numpages = {5},
keywords = {word embeddings, graph embeddings, recommender systems, link prediction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383301,
author = {Zeno, Giselle and Fond, Timothy La and Neville, Jennifer},
title = {Dynamic Network Modeling from Motif-Activity},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383301},
doi = {10.1145/3366424.3383301},
abstract = {Graph structure in dynamic networks changes rapidly. Using temporal information about their connections, models for dynamic networks can be developed and used to understand the process of how their structure changes over time. Additionally, higher-order motifs have been established as building blocks for the structure of networks. In this paper, we first demonstrate empirically in three dynamic network datasets, that motifs with edges: (1) do not transition from one motif type to another (e.g, wedges becoming triangles and vice-versa); (2) motifs re-appear in other time periods and the rate depends on their configuration. We propose the Dynamic Motif-Activity Model (DMA) for sampling synthetic dynamic graphs with parameters learned from an observed network. We evaluate our DMA model, with two dynamic graph generative model baselines, by measuring different graph structure metrics in the generated synthetic graphs and comparing with the graph used as input. Our results show that employing motifs captures the underlying graph structure and modeling their activity recreates the fast changes seen in dynamic networks.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {390–397},
numpages = {8},
keywords = {temporal graphs, motifs, dynamic networks, network evolution, motif evolution},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3384366,
author = {Li, Cheng-Te},
title = {Explainable Detection of Fake News and Cyberbullying on Social Media},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3384366},
doi = {10.1145/3366424.3384366},
abstract = {While social media had ubiquitously penetrate into into people’s daily life, where allows interactions between people, user-generated text data not only enables novel applications, but also provides user digital footprints for us to analyze a variety of human behaviors. In this talk, we will share two of our recent studies on combating anti-social behaviors: detecting fake news and identifying cyberbullying behaviors on social media. We will reveal three important insights. First, it is possible to predict anti-social behaviors without social network information. Second, graph neural networks (GNN) is effective in improving the performance of such two tasks. Third, our models can provide model explainability to understand the language use of anti-social behaviors. In the end of this talk, we will point out future directions on fighting with fake news and cyberbullying in social media.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {398},
numpages = {1},
keywords = {social media, anti-social behaviors, graph neural networks., Explainable model, fake news detection, cyberbullying detection},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383755,
author = {Geng, Qian and Deng, Siyu and Jia, Danping and Jin, Jian},
title = {Cross-Domain Ontology Construction and Alignment from Online Product Reviews},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383755},
doi = {10.1145/3366424.3383755},
abstract = {Online customer product reviews often contain detailed sentiment attitudes towards different aspects of products and these online opinions help potential consumers to be familiar with products. The introduction of well-constructed domain ontology from online product reviews helps potential consumers to obtain relevant information about products quickly. Nonetheless, they may compare products in multiple domains for purchase decisions. On this basis, the comparison of products in different domains induces that ontology alignment becomes a fundamental task to form a cross-domain ontology. In this paper, a series of natural language processing approaches are applied to construct two domain ontologies from online product reviews automatically. Next, a new ontology alignment method is proposed for consumers to make purchase decisions regarding cross-domain product comparisons, in which a semantic-based algorithm and a structure-based algorithm are integrated to form a cross-domain ontology. Categories of experiments were conducted on reviews of smartphone and digital camera. Compared with benchmarked alignment tools, the proposed method performed 5% better in terms of F1 on ontology alignment. Finally, a case study with a customer friendly website is illustrated to present how the alignment of cross-domain ontology help consumers on purchase decision support.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {401–408},
numpages = {8},
keywords = {purchase decision making., product comparison, product review, ontology construction, ontology alignment},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383756,
author = {Wang, Jianyu and Wen, Rui and Wu, Chunming and Xiong, Jian},
title = {Analyzing and Detecting Adversarial Spam on a Large-Scale Online APP Review System},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383756},
doi = {10.1145/3366424.3383756},
abstract = {The online review system provides a platform for consumers to express their opinions and make decisions. However, the spam review has become a widespread problem recently. Existing detection systems are mostly designed for fraudsters who write fake reviews to promote their products or mislead consumers in E-commerce (e.g., Amazon, Yelp, Alibaba). In this paper, we focus on spams in the large-scale online app review platform. Through an in-depth analysis of 5 million reviews from the Tencent App store, we find almost half of the reviews are spams. Even worse, most of the reviews are deliberately designed by fraud reviewers for misleading classifiers. Specially, we conclude three popular patterns of generating adversarial spams by attackers. Considering the characteristics of spam reviews are adversarial, irregular, and distributed. Human labeling is costly. We build our detection system by utilizing a collective method to spot suspicious users and reviews. First, we trained our two models based on the Behavior-Based Module (BBM) and Content-Based Module (CBM) individually. Then, they are co-trained by getting the feedback from each other. Experiment results show that our method outperforms the state-of-the-art baselines in identifying adversarial examples. Besides, the scalability and effectiveness of our approach have also been demonstrated by deploying it on Tencent Beacon Anti-Fraud System.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {409–417},
numpages = {9},
keywords = {Online APP Review System, Adversarial Detection},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383757,
author = {Alrashdi, Reem and O'Keefe, Simon},
title = {Automatic Labeling of Tweets for Crisis Response Using Distant Supervision},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383757},
doi = {10.1145/3366424.3383757},
abstract = {Current tweet classification models aimed at enhancing crisis response are based on supervised deep learning. They rely on the quality and quantity of human-labeled training data. Still, the available training data is small in size and imbalanced in coverage of crisis types, which prevents the models from generalization, and as it is manually labeled, it is also expensive to produce. To overcome these problems, distant supervision can be applied to automatically generate large-scale labeled data for tweet classification for crisis response. Experimental results on different crisis events show that our work can produce good quality labeled data from past and recent events. Substituting automatically labeled training data for part of the manually labeled training data has a minimal impact on the model performance, indicating that automatically labeled data can be used when no hand-labeled data is available.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {418–425},
numpages = {8},
keywords = {FrameNet, large- scale data, crisis response, distant supervision, Tweet classification},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383758,
author = {Debnath, Alok and Pinnaparaju, Nikhil and Shrivastava, Manish and Varma, Vasudeva and Augenstein, Isabelle},
title = {Semantic Textual Similarity of Sentences with Emojis},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383758},
doi = {10.1145/3366424.3383758},
abstract = {In this paper, we extend the task of semantic textual similarity to include sentences which contain emojis. Emojis are ubiquitous on social media today, but are often removed in the pre-processing stage of curating datasets for NLP tasks. In this paper, we qualitatively ascertain the amount of semantic information lost by discounting emojis, as well as show a mechanism of accounting for emojis in a semantic task. We create a sentence similarity dataset of 4000 pairs of tweets with emojis, which have been annotated for relatedness. The corpus contains tweets curated based on common topic as well as by replacement of emojis. The latter was done to analyze the difference in semantics associated with different emojis. We aim to provide an understanding of the information lost by removing emojis by providing a qualitative analysis of the dataset. We also aim to present a method of using both emojis and words for downstream NLP tasks beyond sentiment analysis.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {426–430},
numpages = {5},
keywords = {datasets, emoji, sentence similarity},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3384389,
author = {Yun-Nung Chen, Dr.},
title = {Robust and Scalable Conversational AI},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3384389},
doi = {10.1145/3366424.3384389},
abstract = {Even conversational systems have attracted a lot of attention recently, the current systems sometimes fail due to the errors from different components. This keynote talk presents following research directions for improving robustness and scalability of conversational systems: 1) we first focus on learning language embeddings specifically for spoken scenarios such as more noisy inputs during inference, and 2) secondly we extend the dialogue systems to access not only structured but unstructured knowledge and propose a novel learning framework for natural language understanding and generation on top of duality towards better scalability. The enhanced robustness and scalability shows the great potential of guiding future research directions.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {431–432},
numpages = {2},
keywords = {dialogue systmes, natural language understanding, conversational question answering, conversational systems, speech recognition, natural language generation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3386194,
author = {Liu, Yang and Wang, Chu and Zhang, Zuohua and Wu, Yongning},
title = {Study on Price Consistency Regarding Pack Size via Product Variant Retrieval and Pack Size Extraction},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3386194},
doi = {10.1145/3366424.3386194},
abstract = {Price perception is extremely important for retailers. Customers assess the price of a product not only from the product’s own price history, but also from the prices of the product’s close variants. One particular kind of variant considered is the same product sold in different sizes, where a reduced unit price is generally expected for the ones sold in large quantities. Such price consistency between product variants could be important for customer experience, yet very challenging for retailers which carry millions of products with possibly missing and noisy catalog information. We propose a framework to measure pricing consistency between product size variants by retrieving product variants via search and extracting product size information with natural language processing methods. We evaluate three monotonic regression models that regularize the unit price instead of simple heuristics. To quantify the extent of price inconsistency, we define new metrics and demonstrate that one method can lower the inconsistency measure by up to 45% on the experiment sample set.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {435–440},
numpages = {6},
keywords = {Regression, Price, Information Retrieval, Named Entity Recognition, Information Extraction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3386195,
author = {Yang, Ji and Yi, Xinyang and Zhiyuan Cheng, Derek and Hong, Lichan and Li, Yang and Xiaoming Wang, Simon and Xu, Taibai and Chi, Ed H.},
title = {Mixed Negative Sampling for Learning Two-Tower Neural Networks in Recommendations},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3386195},
doi = {10.1145/3366424.3386195},
abstract = {Learning query and item representations is important for building large scale recommendation systems. In many real applications where there is a huge catalog of items to recommend, the problem of efficiently retrieving top k items given user’s query from deep corpus leads to a family of factorized modeling approaches where queries and items are jointly embedded into a low-dimensional space. In this paper, we first showcase how to apply a two-tower neural network framework, which is also known as dual encoder in the natural language community, to improve a large-scale, production app recommendation system. Furthermore, we offer a novel negative sampling approach called Mixed Negative Sampling (MNS). In particular, different from commonly used batch or unigram sampling methods, MNS uses a mixture of batch and uniformly sampled negatives to tackle the selection bias of implicit user feedback. We conduct extensive offline experiments using large-scale production dataset and show that MNS outperforms other baseline sampling methods. We also conduct online A/B testing and demonstrate that the two-tower retrieval model based on MNS significantly improves retrieval quality by encouraging more high-quality app installs.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {441–447},
numpages = {7},
keywords = {Information Retrieval, Context-aware Recommender Systems, Extreme Classification, Neural Networks},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3386196,
author = {West, Rebecca and Jadda, Khalifeh Al and Ahsan, Unaiza and Qu, Huiming and Cui, Xiquan},
title = {Interpretable Methods for Identifying Product Variants},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3386196},
doi = {10.1145/3366424.3386196},
abstract = {For e-commerce companies with large product selections, the organization and grouping of products in meaningful ways is important for creating great customer shopping experiences and cultivating an authoritative brand image. One important way of grouping products is to identify a family of product variants, where the variants are mostly the same with slight and yet distinct differences (e.g. color or pack size). In this paper, we introduce a novel approach to identifying product variants. It combines both constrained clustering and tailored NLP techniques (e.g. extraction of product family name from unstructured product title and identification of products with similar model numbers) to achieve superior performance compared with an existing baseline using a vanilla classification approach. In addition, we design the algorithm to meet certain business criteria, including meeting high accuracy requirements on a wide range of categories (e.g. appliances, decor, tools, and building materials, etc.) as well as prioritizing the interpretability of the model to make it accessible and understandable to all business partners.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {448–453},
numpages = {6},
keywords = {natural language processing, product variants, constrained clustering},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3386197,
author = {Zhao, Xiaoting and Louca, Raphael and Hu, Diane and Hong, Liangjie},
title = {The Difference Between a Click and a Cart-Add: Learning Interaction-Specific Embeddings},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3386197},
doi = {10.1145/3366424.3386197},
abstract = {For large-scale online marketplaces with over millions of items, users come to rely on personalized recommendations to find relevant items from their massive inventory. One hallmark of the shopping experience in such online marketplaces is the many ways a user can interact with an item they are interested in: they can click it, favorite it, add it to cart, purchase it, etc. We hypothesize that the different ways in which a user interacts with an item indicates different kinds of intent. Consequently, a user’s recommendations should be based not only on items from their past activity, but also the way in which they interacted with these items. Co-occurrence based methods have been successfully used to give recommendations that incorporate interaction types, such as the popular “Because you purchased X, you may also purchase Y”. In this paper, we propose a novel method that generalizes upon the co-occurrence methods to learn interaction-based item embeddings that encode the co-occurrence patterns of not only the item itself, but also the interaction type. The learned embeddings provide a convenient way of approximating the likelihood that one item-interaction pair would co-occur with another by way of a simple inner product. To show their effectiveness, we deploy the interaction-based embeddings in an industry-scale recommendation system that serves live traffic on Etsy.com. We find that taking interaction types into account shows significant improvements in accurately modeling user shopping behavior for both online and offline metrics.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {454–460},
numpages = {7},
keywords = {Embeddings, Recommendation, Candidate Set Selection},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3386198,
author = {Yu, Bingqing and Tagliabue, Jacopo and Greco, Ciro and Bianchi, Federico},
title = {“An Image is Worth a Thousand Features”: Scalable Product Representations for In-Session Type-Ahead Personalization},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3386198},
doi = {10.1145/3366424.3386198},
abstract = {We address the problem of personalizing query completion in a digital commerce setting, in which the bounce rate is typically high and recurring users are rare. We focus on in-session personalization and improve a standard noisy channel model by injecting dense vectors computed from product images at query time. We argue that image-based personalization displays several advantages over alternative proposals (from data availability to business scalability), and provide quantitative evidence and qualitative support on the effectiveness of the proposed methods. Finally, we show how a shared vector space between similar shops can be used to improve the experience of users browsing across sites, opening up the possibility of applying zero-shot unsupervised personalization to increase conversions. This will prove to be particularly relevant to retail groups that manage multiple brands and/or websites and to multi-tenant SaaS providers that serve multiple clients in the same space.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {461–470},
numpages = {10},
keywords = {zero-shot learning, conditional language model, neural networks, transfer learning, e-commerce query auto-completion},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3386200,
author = {Das Gollapalli, Sujatha and Jung-Jae, Kim},
title = {Effective Identification of Distinctive Wordmarks},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3386200},
doi = {10.1145/3366424.3386200},
abstract = {A wordmark or a logotype string is a stylized text used by companies or businesses for the purposes of identification and branding products. Pie face under the product category “baked goods and pastries” and airbus smarter fleet under the product category “data processing and aircraft maintenance software” are examples of wordmarks. The wordmark strings are manually examined by patent officers for their distinctiveness and uniqueness to be accepted as “protected intellectual property” for specific businesses. We address the problem of automatically identifying acceptable English wordmarks based on their textual content. Different from most text mining tasks, our problem involves the classification of a small set of words (often less than five) in context of a specific product description. We handle this sparsity challenge by designing a range of features for characterizing wordmarks using the syntactic and linguistic properties of words as well as by incorporating co-occurrence and similarity information from external resources such as WordNet, Wikipedia, and Word Embeddings. We investigate machine learning models for this novel task and study their classification effectiveness on a large dataset of about 71K wordmarks.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {471–477},
numpages = {7},
keywords = {co-occurrence statistics, short-text classification, word embeddings},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3384369,
author = {Hershkovitch Neiterman, Evgeny and Bitan, Moshe and Azaria, Amos},
title = {Multilingual Deception Detection by Autonomous Agents},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3384369},
doi = {10.1145/3366424.3384369},
abstract = {In this work we present the development of a multilingual deception detection model based on speech. In addition, we also develop a model that detects whether a statement will be perceived as a lie or not by human subjects. To this end, we developed a game for collecting a large scale and high quality labeled data-set in a controlled environments in English and Hebrew. We developed a model that can detect deception based only on a vocal statement from the participants of the experiment. The data-set will be released to the community. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {480–484},
numpages = {5},
keywords = {Lie detection, Agents, Voice, Deception detection},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3384370,
author = {Graells-Garrido, Eduardo and Meta, Irene and Serra-Buriel, Feliu and Reyes, Patricio and Cucchietti, Fernando M.},
title = {Measuring Spatial Subdivisions in Urban Mobility with Mobile Phone Data},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3384370},
doi = {10.1145/3366424.3384370},
abstract = {Urban population grows constantly. By 2050 two thirds of the world population will reside in urban areas. This growth is faster and more complex than the ability of cities to measure and plan for their sustainability. To understand what makes a city inclusive for all, we define a methodology to identify and characterize spatial subdivisions: areas with over- and under-representation of specific population groups, named hot and cold spots respectively. Using aggregated mobile phone data, we apply this methodology to the city of Barcelona to assess the mobility of three groups of people: women, elders, and tourists. We find that, within the three groups, cold spots have a lower diversity of amenities and services than hot spots. Also, cold spots of women and tourists tend to have lower population income. These insights apply to the floating population of Barcelona, thus augmenting the scope of how inclusiveness can be analyzed in the city.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {485–494},
numpages = {10},
keywords = {Spatial Analysis, Mobile Phone Data, Urban Mobility},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3384371,
author = {Santhosh, Roshan and Chopra, Shweta and Baghel, Nupur and Huang, Yi-Nung and Rauckhorst, Marc and Annadate, Shubham},
title = {Investigating Cash Bail Patterns in Pennsylvania},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3384371},
doi = {10.1145/3366424.3384371},
abstract = {A key component of pretrial justice reform has been to overhaul the cash bail system, which has been known to unfairly and disproportionately affect poor communities of color. The lack of clear standards and the variability in decision making by magistrate judges has resulted in a system that is inconsistent in determining who must pay cash bail as well as the amount of bail. Together with the lack of any metrics or transparency associated with this process, the problem has escalated. The American Civil Liberties Union (ACLU) has been at the forefront of this fight to reform the bail determination system. To support their efforts, we utilized a dataset containing all criminal offenses charged within the state of Pennsylvania in 2016-17 in order to derive analytical insights, and perform a series of statistical tests to evaluate and quantify biases in the bail determination system in Pennsylvania. In addition to this, we also developed a predictive model and utilised model interpretability tools to evaluate the factors affecting bail type assignment. One of the key focus areas of our work was the determination of racial bias, particularly between the defendants identified as Black and White in the court records. Through our work, we were able to develop a pipeline for identifying racial bias and conclusively establish that bail assignment patterns do vary across judges.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {495–503},
numpages = {9},
keywords = {judicial reform, bail prediction, judge bias, cash bail, racial discrimination},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3384372,
author = {Graells-Garrido, Eduardo and Pe\~{n}a-Araya, Vanessa},
title = {Toward An Interdisciplinary Methodology to Solve New (Old) Transportation Problems},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3384372},
doi = {10.1145/3366424.3384372},
abstract = {The rising availability of digital traces provides a fertile ground for new solutions to both, new and old problems in cities. Even though a massive data set analyzed with Data Science methods may provide a powerful solution to a problem, its adoption by relevant stakeholders is not guaranteed, due to adoption blockers such as lack of interpretability and transparency. In this context, this paper proposes a preliminary methodology toward bridging two disciplines, Data Science and Transportation, to solve urban problems with methods that are suitable for adoption. The methodology is defined by four steps where people from both disciplines go from algorithm and model definition to the building of a potentially adoptable solution. As case study, we describe how this methodology was applied to define a model to infer commuting trips with mode of transportation from mobile phone data.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {504–509},
numpages = {6},
keywords = {Urban Mobility, Transportation, Data Science},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3384373,
author = {Fard, Amir Ebrahimi and Lingeswaran, Shajeeshan},
title = {Misinformation Battle Revisited: Counter Strategies from Clinics to Artificial Intelligence},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3384373},
doi = {10.1145/3366424.3384373},
abstract = {The spread of misinformation is one of the severe challenges that societies have been dealing with for many years. However, the rapid growth of social media has accelerated the creation and circulation of such information and turned it into a potential threat to the main societal institutions such as peace and democracy. Although many of iconic figures, policymakers, business leaders and researchers have warned us of serious repercussions of misinformation, a clear course of action is not yet visible. To tackle such an issue, the preliminary step would be the evaluation of the as-is situation, which allows us to identify the deficiencies of existing solutions. This issue has been addressed in this study by a comprehensive analysis over decades of societal efforts against misinformation. In this analysis, quelling strategies from organisational and government perspectives are explained. Then they are investigated from efficacy level and governance mode. Our analyses show that, despite a seemingly suitable setting for confronting misinformation, there is a major shortcoming in governance mode of current quelling strategies. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {510–519},
numpages = {10},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3384374,
author = {Chu, Eric and Gillani, Nabeel and Priscilla Makini, Sneha},
title = {Games for Fairness and Interpretability},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3384374},
doi = {10.1145/3366424.3384374},
abstract = {As Machine Learning (ML) systems becomes more ubiquitous, ensuring the fair and equitable application of their underlying algorithms is of paramount importance. We argue that one way to achieve this is to proactively cultivate public pressure for ML developers to design and develop fairer algorithms — and that one way to cultivate public pressure while simultaneously serving the interests and objectives of algorithm developers is through gameplay. We propose a new class of games — “games for fairness and interpretability” — as one example of an incentive-aligned approach for producing fairer and more equitable algorithms. Games for fairness and interpretability are carefully-designed games with mass appeal. They are inherently engaging, provide insights into how machine learning models work, and ultimately produce data that helps researchers and developers improve their algorithms. We highlight several possible examples of games, their implications for fairness and interpretability, how their proliferation could creative positive public pressure by narrowing the gap between algorithm developers and the general public, and why the machine learning community could benefit from them.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {520–524},
numpages = {5},
keywords = {machine learning, interpretability, fairness, games, crowdsourcing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382118,
author = {Matsuno, Shogo and Mizuki, Sakae and Sakaki, Takeshi},
title = {Improved Advertisement Targeting via Fine-Grained Location Prediction Using Twitter},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382118},
doi = {10.1145/3366424.3382118},
abstract = {With the growing demand for social network service advertisements, more accurate targeting methods are required. Therefore, the authors of this study try to predict the location of Twitter users in a more fine-grained manner with regard to area marketing. Specifically, the visit probability to each segment (e.g., prefecture and city) is predicted by a label propagation algorithm, and the user’s location information is obtained from a geo-tagged tweet and an user profile as a label. The proposed method predicts which Twitter users may visit the corresponding area with improved granularity, to the level of an oaza or a block. As a verification experiment, we construct a system that outputs a list of users who are likely to visit the location when an oaza name is entered based on the proposed method, and we also try to predict the possibility of users visiting each segment. The results show that the average accuracy is 73% at the prefecture level, 42% at the city level, and 25% at the oaza level. In addition, the prediction accuracy in Tokyo alone is 31% at the oaza level. These results indicate the effectiveness of the proposed method. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {527–532},
numpages = {6},
keywords = {Twitter, social media, label propagation, social graph, geo-marketing, location prediction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382119,
author = {Zhou, Wenjie and Yang Zhou, Zhe},
title = {E-Sports Report Path and Mode: A Case Study on China Youth Online},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382119},
doi = {10.1145/3366424.3382119},
abstract = {In recent years, the development of China's e-sports has received more and more attention. Many influential events have caused a sensation on social media and became media events for the national carnival. As an important channel for the public to obtain news information, online media plays an indispensable role in reporting e-sports news. This article has researched China Youth Online, collected the news report focusing on e-sports from 2002 to 2019. Based on the social network analysis method and content analysis method, this study analyzed the report topics and report attitudes about e-sports. It was found that in the aspect of report topics, China Youth Online concerned about the impact of e-sports on the development of youth. This paper pointed out the deficiencies in government supervision, acknowledged the positive effects of e-sports culture, and supported the development of e-sports technology. Also, in terms of reporting attitudes, the majority of them were negative towards youth, social evaluation, e-sports industry chain and other issues, mostly e-sports issues about technology and education had positive attitudes, and mostly e-sports games were found to be neutral.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {533–539},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382120,
author = {Pramanik, Prithviraj and Mondal, Tamal and Nandi, Subrata and Saha, Mousumi},
title = {AirCalypse: Can Twitter Help in Urban Air Quality Measurement and Who Are the Influential Users?},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382120},
doi = {10.1145/3366424.3382120},
abstract = {In this digital age, Online Social Media’s ubiquity has led it to it’s role as a ”Sensor”. Starting from disaster response to political predictions, online social media like Twitter, have been instrumental and are actively researched areas. In this work, we have focused on something quite insidious in the current context, i.e., air pollution in developing regions. Starting as an empirical study on using Twitter as a ”Sensor” to measure air quality, the focal point of this work is to identify the users who have been actively tweeting in the air pollution events in Delhi, the capital of India. From these users, we try to identify the influential ones, who play a significant role in creating the initial awareness and hence act as ”Sensors”. We have utilized a tailored ”TRank” algorithm for finding out the influential users by considering Retweet, Favorite, and Follower influence of the users. After ranking the users based on their social influence, we further study the behavior, i.e., perception of pollution from those users’ posts with respect to the actual air pollution levels using the physical sensors. The tracking of influential users in air quality monitoring assists in developing a crowd sensed air quality measurement framework, which can augment the physical air quality sensors for raising awareness against air pollution.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {540–545},
numpages = {6},
keywords = {Social Media, Air Quality, Participatory Sensing, Influential User},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382121,
author = {Zhang, Desheng},
title = {Mobile Cyber-Physical Systems for Smart Cities},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382121},
doi = {10.1145/3366424.3382121},
abstract = {Nowadays, rapid urbanization leads to severe urban challenges, e.g., congestion and energy consumption, related to human mobility. To address them, it is essential to (i) measure and predict human mobility based on data from urban infrastructure, and (ii) intervene and alter human mobility with novel services on urban infrastructure, i.e., a Cyber-Physical System approach. However, both existing mobility models and resultant services are built upon the interaction of residents with single infrastructure (e.g., taxis) or multiple infrastructures from single domains (e.g., transportation), which are limited by their homogeneous nature. Fortunately, cross-domain infrastructures and their data from the latest infrastructure expansion enable us to explore real-time interactions between residents and infrastructures across domains. In this talk, we will introduce some of our recent work under cross-domain interactions. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {546–548},
numpages = {3},
keywords = {Smart Cities, Cyber-Physical Systems, Human Mobility},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382122,
author = {Savage, Saiph},
title = {Citizens as More Than Sensors, Citizens as Agents for Change.},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382122},
doi = {10.1145/3366424.3382122},
abstract = {How might we tap into citizens’ knowledge, context, and passion on social media to have them be much more than just sensors, but rather, agents of change? What would it take to be able to orchestrate citizens on social media to execute ambitious goals, e.g., creating new infrastructure to support the indoor navigation of the visually impaired? The main challenge that is blocking these type of interactions at scale is that citizens have heterogeneous knowledge and usually lack the skills needed to complete the work needed to reach the collective goals. To address this problem, I introduce a novel architecture conformed of Intelligent Civic Blocks that use machine learning to: 1) help citizens on social media to build at execution time the skills they need to complete work associated with the large-scale collective effort; 2) empower citizens to develop their creativity and entrepreneurship to propose creative solutions to build improved societies. In this talk, I will present case-studies showcasing how this proposed architecture can lead to collective action organized on social media in a range of areas (see Fig. 1 for an example). In difference to prior work that focused primarily on using citizens as simple sensors, this research pushes towards new intelligent systems that enable the orchestration of collective action on social media to create improved societies. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {549},
numpages = {1},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383414,
author = {Gefen, Gilie and Ben-Porat, Omer and Tennenholtz, Moshe and Yom-Tov, Elad},
title = {Privacy, Altruism, and Experience: Estimating the Perceived Value of Internet Data for Medical Uses},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383414},
doi = {10.1145/3366424.3383414},
abstract = {People increasingly turn to the Internet when they have a medical condition. The data they create during this process is a valuable source for medical research and future health services. When used for these purposes, it is imperative to balance use with user privacy. One way to understand how to harmonize these requirements is to match the perceived value that users assign to their data with the value of the services derived from them. Here we describe novel experiments where methods from Mechanism Design, Crowdsourcing and Data Science were used together to elicit truthful valuations from users for their Internet data and for services derived from these data, specifically for medical screening. In these experiments, 880 people from around the world were asked to participate in an auction to provide their data for uses differing in their contribution to the participant and to society, and in the disease they addressed. Some users were offered monetary compensation for their participation, while others were asked to pay to participate. Our findings show that 99% of people were willing to contribute their data in exchange for monetary compensation and an analysis of their data, while 53% were willing to pay to have their data analyzed. The average perceived value users assigned to their data was estimated at US$49. Their value for services which offer personal benefit to them was US$22, while the value of this service offered to the general public was US$20. Our findings show that it is possible to place a monetary value on health-related uses of highly personal data. Our methodology can be extended to other areas where sensitive data may be exchanged for services to individuals and to society. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {552–556},
numpages = {5},
keywords = {web search, medicine, health, privacy, web browsing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383415,
author = {Li, Ang and J. Chen, Suming and Qin, Jingzheng and Qin, Zhen},
title = {Training Machine Learning Models With Causal Logic},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383415},
doi = {10.1145/3366424.3383415},
abstract = {Machine-learning (ML) models are ubiquitously used to make a variety of inferences, a common application being to predict and categorize user behavior. However, ML models often suffer from only being exposed to biased data – for instance, a search ranking model that uses clicks to determine how to rank will suffer from position bias. The difficulty arises due to user feedback only being observed for one treatment and not existing counterfactually for other potential treatments. In this work, we discuss a real-world situation in which a binary classification model is used in production in order to make decisions about how to treat users. We introduce the model and discuss the limitations of our modeling approach. We show that by using unit selection criterion we can do a better job classifying users. Following, we propose a causal modeling method in which we can take the existing data and use it to derive bounds that can be used to modify the objective function in order to incorporate causal learning into our training process. We demonstrate the effectiveness of this approach in a real-world setting. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {557–561},
numpages = {5},
keywords = {counterfactual learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383416,
author = {Kang, Wang-Cheng and Cheng, Derek Zhiyuan and Chen, Ting and Yi, Xinyang and Lin, Dong and Hong, Lichan and Chi, Ed H.},
title = {Learning Multi-Granular Quantized Embeddings for Large-Vocab Categorical Features in Recommender Systems},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383416},
doi = {10.1145/3366424.3383416},
abstract = {Recommender system models often represent various sparse features like users, items, and categorical features via embeddings. A standard approach is to map each unique feature value to an embedding vector. The size of the produced embedding table grows linearly with the size of the vocabulary. Therefore, a large vocabulary inevitably leads to a gigantic embedding table, creating two severe problems: (i) making model serving intractable in resource-constrained environments; (ii) causing overfitting problems. In this paper, we seek to learn highly compact embeddings for large-vocab sparse features in recommender systems (recsys). First, we show that the novel Differentiable Product Quantization (DPQ) approach can generalize to recsys problems. In addition, to better handle the power-law data distribution commonly seen in recsys, we propose a Multi-Granular Quantized Embeddings (MGQE) technique which learns more compact embeddings for infrequent items. We seek to provide a new angle to improve recommendation performance with compact model sizes. Extensive experiments on three recommendation tasks and two datasets show that we can achieve on par or better performance, with only ∼ 20% of the original model size.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {562–566},
numpages = {5},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383417,
author = {Hofman, Jake M. and Goldstein, Daniel G. and Sen, Siddhartha and Poursabzi-Sandegh, Forough},
title = {Expanding the Scope of Reproducibility Research Through Data Analysis Replications},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383417},
doi = {10.1145/3366424.3383417},
abstract = {In recent years, researchers in several scientific disciplines have become concerned with published studies replicating less often than expected. A positive side effect of this concern is an increased appreciation for replicating other researchers’ work as a vital part of the scientific process. To date, many such efforts have come from the experimental sciences, where replication entails running new experiments, generating new data, and analyzing it. In this article, we emphasize not experimental replication but data analysis replication. We do so for three reasons. First, experimental replication excludes entire classes of publications that do not run experiments or even collect original data (for example, papers that make use of economic data, census data, municipal data, and the like). Second, experimental replication may in some cases be a needlessly high bar: there is great value in replicating the data analyses of published experimental work. As analytical replications require a lower investment of time and money than experimental replications, their adoption should expand the number and variety of scientific reproducibility studies undertaken. Third, we propose educating undergraduate students to perform data analysis replications, which has scalable benefits for both the students themselves and the broader research community. In our talk we will provide details of a pilot program we created to teach undergraduates the skills necessary to conduct data analysis replications, and include a case study of the first set of students who completed this program and attempted to replicate a widely-cited social science paper on policing.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {567–571},
numpages = {5},
keywords = {replication, education, robustness, data analysis, reproducibility},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383418,
author = {G. Harris, Christopher},
title = {Methods to Evaluate Temporal Cognitive Biases in Machine Learning Prediction Models},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383418},
doi = {10.1145/3366424.3383418},
abstract = {When asked to rank or rate a list of items, humans are often affected by cognitive biases, which may lead to inconsistent decisions over time. These inconsistencies become part of machine learning prediction algorithms trained on human judgments, leading to misalignment and consequently affecting the metrics used to evaluate their correctness. In this paper, we propose new accuracy metrics, built upon commonly used statistics- and decision support-based metrics. Each of these metrics is designed to address the varying nature of human judgment and to evaluate the importance of decisions that change over time due to cognitive biases.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {572–575},
numpages = {4},
keywords = {Fairness, Machine Learning, Decision Making, Temporal Evaluation, Cognitive Bias, Data Science},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3385771,
author = {Gupta, Aabhaas and Yang, Wenxi and Sivakumar, Divya and Silva, Yasin and Hall, Deborah and Nardini Barioni, Maria},
title = {Temporal Properties of Cyberbullying on Instagram},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3385771},
doi = {10.1145/3366424.3385771},
abstract = {Concurrent with the growth and widespread use of social networking platforms has been a rise in the prevalence of cyberbullying and cyberharassment, particularly among youth. Although cyberbullying is frequently defined as hostile communication or interactions that occur repetitively via electronic media, little is known about the temporal aspects of cyberbullying on social media, such as how the number, frequency, and timing of posts may vary systematically between cyberbullying and non-cyberbullying social media sessions. In this paper, we aim to contribute to the understanding of temporal properties of cyberbullying through the analysis of Instagram data. That is, the paper presents key temporal characteristics of cyberbullying and trends obtained from descriptive and burst analysis tasks. Our results have the potential to inform the development of more effective cyberbullying detection models.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {576–583},
numpages = {8},
keywords = {Instagram, burst analysis, social media, temporal properties, cyberbullying},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3385772,
author = {Chen, Zhouhan and Freire, Juliana},
title = {Proactive Discovery of Fake News Domains from Real-Time Social Media Feeds},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3385772},
doi = {10.1145/3366424.3385772},
abstract = {The proliferation of web sites that disseminate fake news is a growing problem in our society. Not surprisingly, the problem of identifying whether a web page contains fake news has attracted substantial attention. However, the problem of discovering new sources of fake news has been largely unexplored. Timely discovery of such sources is critical to combat misinformation and minimize its potential harm. In this paper, we present an automatic discovery system that proactively surfaces fake news domains before they are flagged by humans. Our system operates in two-steps: first, it uses Twitter feeds to uncover user co-sharing structures to discover political websites; then it uses a topic-agnostic classifier to score and rank newly discovered domains. To demonstrate the effectiveness of our system, we conduct an experimental evaluation in which we collect tweets related to the 2020 presidential impeachment process in the United States, and show that not only our system is able to discover new sites, but that a large percentage of these sites are indeed publishing fake news. We also design an integrated user interface to support fact-checkers and leverage their knowledge. Through this interface, fact-checkers can visualize domain interaction networks, query domain fakeness score, and tag incorrectly predicted results. Our proactive discovery system will expedite fact-checking process and can be a powerful weapon in the toolbox to combat misinformation. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {584–592},
numpages = {9},
keywords = {social network analysis, misinformation, fake news discovery},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3385773,
author = {Jiyoung Whang, Joyce and Jung, Yeonsung and Kang, Seonggoo and Yoo, Dongho and S. Dhillon, Inderjit},
title = {Scalable Anti-TrustRank with Qualified Site-Level Seeds for Link-Based Web Spam Detection},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3385773},
doi = {10.1145/3366424.3385773},
abstract = {Web spam detection is one of the most important and challenging tasks in web search. Since web spam pages tend to have a lot of spurious links, many web spam detection algorithms exploit the hyperlink structure between the web pages to detect the spam pages. In this paper, we conduct a comprehensive analysis of the link structure of web spam using real-world web graphs to systemically investigate the characteristics of the link-based web spam. By exploring the structure of the page-level graph as well as the site-level graph, we propose a scalable site-level seeding methodology for the Anti-TrustRank (ATR) algorithm. The key idea is to map a website into a feature space where we learn a classifier to prioritize the websites so that we can effectively select a set of good seeds for the ATR algorithm. This seeding method enables the ATR algorithm to detect the largest number of spam pages among the competitive baseline methods. Furthermore, we design work-efficient asynchronous ATR algorithms which are able to significantly reduce the computational cost of the traditional ATR algorithm without degrading the performance in detecting spam pages while guaranteeing the convergence.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {593–602},
numpages = {10},
keywords = {Web Spam Detection, Link Analysis., Seeds, Anti-TrustRank},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3385774,
author = {Tavabi, Nazgol and Abeliuk, Andres and Mokhberian, Negar and Abramson, Jeremy and Lerman, Kristina},
title = {Challenges in Forecasting Malicious Events from Incomplete Data},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3385774},
doi = {10.1145/3366424.3385774},
abstract = {The ability to accurately predict cyber-attacks would enable organizations to mitigate their growing threat and avert the financial losses and disruptions they cause. But how predictable are cyber-attacks? Researchers have attempted to combine external data – ranging from vulnerability disclosures to discussions on Twitter and the darkweb – with machine learning algorithms to learn indicators of impending cyber-attacks. However, successful cyber-attacks represent a tiny fraction of all attempted attacks: the vast majority are stopped, or filtered by the security appliances deployed at the target. As we show in this paper, the process of filtering reduces the predictability of cyber-attacks. The small number of attacks that do penetrate the target’s defenses follow a different generative process compared to the whole data which is much harder to learn for predictive models. This could be caused by the fact that the resulting time series also depends on the filtering process in addition to all the different factors that the original time series depended on. We empirically quantify the loss of predictability due to filtering using real-world data from two organizations. Our work identifies the limits to forecasting cyber-attacks from highly filtered data.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {603–610},
numpages = {8},
keywords = {time-series, predictability, cyber-attack, permutation entropy, forecasting},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3385775,
author = {Pacheco, Diogo and Flammini, Alessandro and Menczer, Filippo},
title = {Unveiling Coordinated Groups Behind White Helmets Disinformation},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3385775},
doi = {10.1145/3366424.3385775},
abstract = {Propaganda, disinformation, manipulation, and polarization are the modern illnesses of a society increasingly dependent on social media as a source of news. In this paper, we explore the disinformation campaign, sponsored by Russia and allies, against the Syria Civil Defense (a.k.a. the White Helmets). We unveil coordinated groups using automatic retweets and content duplication to promote narratives and/or accounts. The results also reveal distinct promoting strategies, ranging from the small groups sharing the exact same text repeatedly, to complex “news website factories” where dozens of accounts synchronously spread the same news from multiple sites.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {611–616},
numpages = {6},
keywords = {Twitter, White Helmets, Misinformation, coordinated groups},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3385776,
author = {Pierri, Francesco},
title = {The Diffusion of Mainstream and Disinformation News on Twitter: The Case of Italy and France},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3385776},
doi = {10.1145/3366424.3385776},
abstract = {In this work we provide preliminary results from an ongoing investigation on the Twitter diffusion of news pertaining to two classes of sources, namely websites which notably produce disinformation, i.e. misleading and harmful information, opposed to more traditional and mainstream websites which instead publish credible information. We used the Twitter Streaming API to collect a large-scale dataset of thousands of tweets containing links to news articles in two different countries, Italy and France. We show that mainstream news outlets generate a much larger engagement in both settings, with a larger discrepancy between the two news domains in France. We also show that only a handful of Italian outlets actively engage with Twitter users, whereas in France there is a larger number of outlets sharing misleading information which exhibit a non-negligible volume of shares. We observe a strong tendency towards sharing mainstream news in those users who also share non-credible information in both countries. Analyzing the diffusion networks of distinct news domains and countries, we observed that disinformation networks are more clustered and connected, but much smaller than the mainstream ones (with the largest discrepancy in the French scenario).},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {617–622},
numpages = {6},
keywords = {Twitter, network science, disinformation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3385777,
author = {Schwartz, Christopher and Overdorf, Rebekah},
title = {Disinformation from the Inside: Combining Machine Learning and Journalism to Investigate Sockpuppet Campaigns},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3385777},
doi = {10.1145/3366424.3385777},
abstract = {This paper brings together machine learning and investigative journalism to examine sockpuppets accounts, a historical breed of fake accounts that are non-automated and human-controlled. Due to their flexible and human-centered nature, sockpuppets pose a complication for purely technological approaches to detecting and studying fake accounts. We find that as machine learning-based detection methods of bots slowly grow stronger, adversaries engaging in disinformation are turning to such sockpuppets accounts, and in particular a subset of sockpuppets that we call “infiltrators” — those that aim to integrate into a community in order spread disinformation. This represents a new stage in the evolution of the sockpuppet concept: where bots seek to simulate audiences and drown online social media platforms with a particular point of view, infiltrators seek to persuade and assimilate genuine audiences from within. In addition to these insights into infiltrator sockpuppets, combining machine learning and investigative journalism enables learning something more than detection and important patterns of activity: it can also gain a sense of the motivations and reasoning of adversaries who engage in disinformation. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {623–628},
numpages = {6},
keywords = {Social Networks, Sock Puppets, Disinformation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3385778,
author = {Ashraf, Noman and Mustafa, Rabia and Sidorov, Grigori and Gelbukh, Alexander},
title = {Individual vs. Group Violent Threats Classification in Online Discussions},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3385778},
doi = {10.1145/3366424.3385778},
abstract = {Violent threat is a serious crime affecting the targeted individuals or groups. It is essential for media providers to block the users that post such threats. In this paper, we focused on detection of violent threat language in YouTube comments. We categorized the threatening comments into those targeting an individual or a group. We started from an existing dataset with violent threat language identified, but without any categorization into comments targeting individuals or groups. We adopted a binary classification approach for the prediction of individual- vs. group-targeting threats. We compared two text representations: bag of words (BOW) and pre-trained word embedding such as GloVe and fastText. We used deep-learning classifiers such as 1D-CNN, LSTM, and bidirectional LSTM (BiLSTM). GloVe embedding showed the worst results, fastText performed much better, and BiLSTM on BOW with term frequency-inverse document frequency (TF-IDF) weighting scheme gave the best results, achieving 0.94% ROC-AUC and Macro-F1 score of 0.85%. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {629–633},
numpages = {5},
keywords = {Violent threat, NLP, social media, individual and group threats, deep learning},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3385758,
author = {Lange, Peter de and S\l{}upczy\'{n}ski, Micha\l{} and Klamma, Ralf},
title = {Incentivizing Contribution in DecentralizedCommunity Information Systems},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3385758},
doi = {10.1145/3366424.3385758},
abstract = {Decentralized peer-to-peer architectures provide significant advantages in comparison to centralized solutions, especially for smaller Communities of Practice. They take into account the constant change of these communities and mitigate the need for central coordination or facilitation. Using blockchain technology, decentralized architectures can provide verifiable and secure collaboration, just like centralized solutions do, but without shifting the control away from the community to a single entity. However, blockchain transactions are not for free. This leads to the problem that in addition to contributing to the community, users have to pay the blockchain transaction fees. In this contribution, we try to solve this double-spending issue by introducing a reputation-based reward system for decentralized infrastructures. This system takes into account several aspects of a user’s contribution and her reputation, and on this basis calculates the reimbursement for her efforts. Our first evaluation shows promising results and this contribution provides a first step towards developing incentivizing mechanisms for decentralized community information systems.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {636–644},
numpages = {9},
keywords = {incentivizing community contributions, reputation systems, peer-to-peer architectures, decentralized architectures},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3385759,
author = {Ramachandran, Manoharan and Chowdhury, Niaz and Third, Allan and Domingue, John and Quick, Kevin and Bachler, Michelle},
title = {Towards Complete Decentralised Verification of Data with Confidentiality: Different Ways to Connect Solid Pods and Blockchain},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3385759},
doi = {10.1145/3366424.3385759},
abstract = {Over-centralisation of data leads to tampering and sharing user information without the consent of the owners. This problem has been studied extensively in recent times providing separate solutions involving distributed storage, Blockchain technology and Solid Pods. Individually these solutions are not sufficient to build realistic applications in a decentralised environment; however, a combination of them can effectively provide more powerful and useful use-cases. In this paper, we propose the methods of combining Solid Pods and distributed ledgers in introducing complete decentralisation of data with total user-control, keeping the integrity of the stored information intact through Blockchain-based verification. We demonstrated multiple configurations of our solutions, offering several new use-cases in various sectors. These configurations introduce new dimensions on the Web and mobile applications’ data storage that developers can benefit from building Distributed Applications (DApps) in a complete decentralised environment.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {645–649},
numpages = {5},
keywords = {Blockchain, data verification, Linked Data, decentralisation},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3385770,
author = {Wild, Antonia and Ciortea, Andrei and Mayer, Simon},
title = {Designing Social Machines for Tackling Online Disinformation},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3385770},
doi = {10.1145/3366424.3385770},
abstract = {Traditional news outlets as carriers and distributors of information have been challenged by online social networks with regards to their gate-keeping function. We believe that only a combined effort of people and machines will be able to curb so-called “fake news” at scale in a decentralized Web. In this position paper, we propose an approach to design social machines that coordinate human- and machine-driven credibility assessment of information on a decentralized Web. To this end, we defined a fact-checking process that draws upon ongoing efforts for tackling disinformation on the Web, and we formalized this process as a multi-agent organisation for curating W3C Web Annotations. We present the current state of our prototypical implementation in the form of a browser plugin that builds on the Hypothesis annotation platform and the JaCaMo multi-agent platform. Our social machines would span across the Web to enable collaboration in form of public discourse, thereby increasing the transparency and accountability of information.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {650–654},
numpages = {5},
keywords = {Multi-Agent Systems, Decentralization, Web Architecture, Social Machines, Disinformation, Linked Data},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3391263,
author = {Huang, Di and He, Zihao and Huang, Yuzhong and Sun, Kexuan and Abu-El-Haija, Sami and Perozzi, Bryan and Lerman, Kristina and Morstatter, Fred and Galstyan, Aram},
title = {Graph Embedding with Personalized Context Distribution},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3391263},
doi = {10.1145/3366424.3391263},
abstract = {Graph representation learning embeds graph nodes in a low-dimensional latent space, which allows for mathematical operations on nodes using low-dimensional vectors for downstream tasks, such as link prediction, node classification, and recommendation. Traditional graph embedding methods rely on hyper-parameters to capture the rich variation hidden in the structure of real-world graphs. In many applications, it may not be computationally feasible to search for optimal hyper-parameters. In this work, built on WatchYourStep which a graph embedding method leveraging graph attention, we propose a method that utilizes node-personalized context attention to capture the local variation in a graph structure. Specifically, we replace the shared context distribution among nodes with learnable personalized context distribution for each node. We evaluate our model on seven real-world graphs and show that our method outperforms the state-of-the-art baselines on both link prediction and node classification tasks. We further analyze the learned node context distribution to provide insights into its connection to graph structural properties. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {655–661},
numpages = {7},
keywords = {graph representation learning, social networks, random walk, node classification, link prediction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3391264,
author = {Shekhar, Shubhranshu and Pai, Deepak and Ravindran, Sriram},
title = {Entity Resolution in Dynamic Heterogeneous Networks},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3391264},
doi = {10.1145/3366424.3391264},
abstract = {Networks evolve continuously over time not only with the addition and deletion of links and nodes but also with changes in the importance of edges. Even though many networks contain this type of temporal weightings, vast majority of research in network representation learning and classification has focused on static snapshots of the graph, while largely ignoring the temporal dynamics. In this work, we describe two approaches for incorporating weighted temporal information into network embedding methods such as Graph Convolutional Networks (GCNs). While the first approach aggregates time-weighted edges and nodes, the second approach uses temporal random walks to find relevant convolution nodes. With experiments on public and proprietary datasets, we demonstrate the effectiveness of the proposed TimeSage for link prediction tasks. By applying these predictions, we show improvements in our task of identifying fraudulent actors on a large e-commerce website selling software as subscriptions.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {662–668},
numpages = {7},
keywords = {neural networks, graph representation, entity resolution},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3391265,
author = {Harsha Vardhan, L Vivek and Jia, Guo and Kok, Stanley},
title = {Probabilistic Logic Graph Attention Networks for Reasoning},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3391265},
doi = {10.1145/3366424.3391265},
abstract = {Knowledge base completion, which involves the prediction of missing relations between entities in a knowledge graph, has been an active area of research. Markov logic networks, which combine probabilistic graphical models and first order logic, have proven to be effective on knowledge graph tasks like link prediction and question answering. However, their intractable inference limits their scalability and wider applicability across various tasks. In recent times, graph attention neural networks, which capture features of neighbouring entities, have achieved superior results on highly complex graph problems like node classification and link prediction. Combining the best of both worlds, we propose Probabilistic Logic Graph Attention Network (pGAT) for reasoning. In the proposed model, the joint distribution of all possible triplets defined by a Markov logic network is optimized with a variational EM algorithm. This helps us to efficiently combine first-order logic and graph attention networks. With the goal of establishing strong baselines for future research on link prediction, we evaluate our model on various standard link prediction benchmarks, and obtain competitive results.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {669–673},
numpages = {5},
keywords = {Graph attention networks, Knowledge graphs, Link prediction, Markov logic networks},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3391266,
author = {Wen, Rui and Wang, Jianyu and Wu, Chunming and Xiong, Jian},
title = {ASA: Adversary Situation Awareness via Heterogeneous Graph Convolutional Networks},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3391266},
doi = {10.1145/3366424.3391266},
abstract = {Given a large graph with millions of vertices and different types, how can we spot anomalies and find potential adversaries in time? Most graph-based fraud detection algorithms focus on finding dense blocks, discovering local subgraphs, designing belief propagation, and latent factor models. However, as fraudsters in online social networks gradually become adversarial, distributed, and invisible, it is easy for them to evade traditional detection methods. Even worse, existing detection systems are fragile to the new attacks. Once attackers update their tragedies, they will be inefficient. Our focus is to detect potential adversaries as soon as possible. We design and propose ASA (Adversary Situation Awareness), a system that (a) uncovers potential adversaries in time, (b) utilizes heterogeneous information in the graph, and (c)is effective in real-world data. We do experiments on a heterogeneous graph including 198,541 nodes with five types and 224,384 edges. The result shows that ASA can spot potential adversaries and successfully recovers 60 of potential abnormal users within a few minutes. Additionally, after two months we deployed it, ASA could achieve 90 recall, which means ASA can well discover a small number of adversaries with a latent period in time.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {674–678},
numpages = {5},
keywords = {Graph Neural Networks, Heterogeneous Graph, Adversary Situation Awareness},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3391267,
author = {Liberti, Leo},
title = {A New Distance Geometry Method for Constructing Word and Sentence Vectors},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3391267},
doi = {10.1145/3366424.3391267},
abstract = {We present a new methodology for producing low-dimensional word vectors based on distance geometry and dimensional reduction techniques. We use these word vectors in order to construct sentence vectors. We evaluate their usefulness in a sentence classification task performed by a simple artificial neural network. Compared to n-gram incidence vectors, the new methodology is shown to yield lower loss values.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {679–685},
numpages = {7},
keywords = {graph embeddings, clustering, neural networks, natural language processing, word vectors},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3384361,
author = {Opitz, Daniela and Graells-Garrido, Eduardo and P\'{e}rez-Messina, Ignacio},
title = {Toward Characterizing Cities with Social Media Images Using Activity Recognition, Topic Modeling and Visualization},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3384361},
doi = {10.1145/3366424.3384361},
abstract = {Images are at the heart of the digital life, registering what, where, and when people do activities. This rich visual information may allow to characterize cities at spatial granularities unseen with traditional methods. To advance in that direction, here we propose a methodology to explore the activities held in cities, by means of activity recognition (what happens in images), topic modeling (what activities are relevant), and visualization (how to choose the number of topics, interpret them, and where do they appear in the city). We apply this methodology to Santiago, Chile, using images from Flickr, and find promising results for planners and urbanites.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {688–693},
numpages = {6},
keywords = {Topic Modeling, Cities, Activities, Visualization, Flickr},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3384362,
author = {Roitero, Kevin and Carterrete, Ben and Mehrotra, Rishabh and Lalmas, Mounia},
title = {Leveraging Behavioral Heterogeneity Across Markets for Cross-Market Training of Recommender Systems},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3384362},
doi = {10.1145/3366424.3384362},
abstract = {Modern recommender systems are optimised to deliver personalised recommendations to millions of users spread across different geographic regions exhibiting various forms of heterogeneity, including behavioural-, content- and trend specific heterogeneity. System designers often face the challenge of deploying either a single global model across all markets, or developing custom models for different markets. In this work, we focus on the specific case of music recommendation across 21 different markets, and consider the trade-off between developing global model versus market specific models. We begin by investigating behavioural differences across users of different markets, and motivate the need for considering market as an important factor when training models. We propose five different training styles, covering the entire spectrum of models: from a single global model to individual market specific models, and in the process, propose ways to identify and leverage users abroad, and data from similar markets. Based on a large scale experimentation with data for 100M users across 21 different markets, we present insights which highlight that markets play a key role, and describe models that leverage market specific data in serving personalised recommendations.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {694–702},
numpages = {9},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3384363,
author = {Degbelo, Auriol and Sherpa, Ang},
title = {Open Geodata Reuse: Towards Natural Language Interfaces to Web APIs},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3384363},
doi = {10.1145/3366424.3384363},
abstract = {Open Government datasets have been flooding the Web recently, and Application Programming Interfaces (APIs) are key to the development of services on top of these datasets. An issue of current APIs worldwide is that their learnability is limited. This work has explored the potential of querying APIs using natural language terms to mitigate that issue. A user study with 20 participants has demonstrated that a natural-language-based, along with an order-agnostic approach to API design can produce easily learnable APIs for both novice and experienced API users. These insights can pave the way for a paradigm change on Web-API design for open geodata retrieval and beyond.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {703–710},
numpages = {8},
keywords = {API Usability, Open Government Data, API Learnability, Geographic Information Retrieval, API Design Conventions, Smart Cities},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383555,
author = {Farnad, Golnoosh and Babaki, Behrouz and Gendreau, Michel},
title = {A Unifying Framework for Fairness-Aware Influence Maximization},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383555},
doi = {10.1145/3366424.3383555},
abstract = {The problem of selecting a subset of nodes with greatest influence in a graph, commonly known as influence maximization, has been well studied over the past decade. This problem has real world applications which can potentially affect lives of individuals. Algorithmic decision making in such domains raises concerns about their societal implications. One of these concerns, which surprisingly has only received limited attention so far, is algorithmic bias and fairness. We propose a flexible framework that extends and unifies the existing works in fairness-aware influence maximization. This framework is based on an integer programming formulation of the influence maximization problem. The fairness requirements are enforced by adding linear constraints or modifying the objective function. Contrary to the previous work which designs specific algorithms for each variant, we develop a formalism which is general enough for specifying different notions of fairness. A problem defined in this formalism can be then solved using efficient mixed integer programming solvers. The experimental evaluation indicates that our framework not only is general but also is competitive with existing algorithms. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {714–722},
numpages = {9},
keywords = {Influence Maximization, Mixed Integer Programming, Group Fairness},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383556,
author = {Ogura, Hikaru and Takeda, Akiko},
title = {Convex Fairness Constrained Model Using Causal Effect Estimators},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383556},
doi = {10.1145/3366424.3383556},
abstract = {Recent years have seen much research on fairness in machine learning. Here, mean difference (MD) or demographic parity is one of the most popular measures of fairness. However, MD quantifies not only discrimination but also explanatory bias which is the difference of outcomes justified by explanatory features. In this paper, we devise novel models, called FairCEEs, which remove discrimination while keeping explanatory bias. The models are based on estimators of causal effect utilizing propensity score analysis. We prove that FairCEEs with the squared loss theoretically outperform a naive MD constraint model. We provide an efficient algorithm for solving FairCEEs in regression and binary classification tasks. In our experiment on synthetic and real-world data in these two tasks, FairCEEs outperformed an existing model that considers explanatory bias in specific cases.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {723–732},
numpages = {10},
keywords = {supervised learning, explanatory bias, fairness, causality, propensity score},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383557,
author = {Li, Yanying and Ning, Yue and Liu, Rong and Wu, Ying and Hui Wang, Wendy},
title = {Fairness of Classification Using Users’ Social Relationships in Online Peer-To-Peer Lending},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383557},
doi = {10.1145/3366424.3383557},
abstract = {Peer-to-peer (P2P) lending marketplaces on the Web have been growing over the last decade. By providing online platforms, P2P lending enables individuals to borrow and lend money directly from and to one another. Since the applicants on P2P lending platforms may lack sufficient financial history for assessment, quite a few P2P lending service providers have been utilizing the applicants’ social relationships to improve the risk prediction accuracy of loan applications. However, utilizing the information of applicants’ social relationships may introduce discrimination in prediction. In this paper, we analyze and evaluate the impact of the applicants’ social relationships on the fairness of risk prediction for P2P lending. We investigate over a million loan records collected from Prosper.com, one of the leading P2P lending companies in the world. We construct the Prosper social network of loan borrowers and lenders, and generate the social features of applicants by adapting a state-of-the-art social credit scoring scheme to the Prosper social network. We consider two types of fairness notions in the literature, namely individual fairness and counterfactual fairness. Our results demonstrate that the social score harms both individual and counterfactual fairness of classification. To address this issue, we design two new algorithms that mitigate bias by generalizing social features. Our experimental results show that our mitigation algorithms can reduce bias while utilizing social scores effectively.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {733–742},
numpages = {10},
keywords = {machine learning, Algorithmic fairness, social network},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383558,
author = {Huan, Wen and Wu, Yongkai and Zhang, Lu and Wu, Xintao},
title = {Fairness through Equality of Effort},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383558},
doi = {10.1145/3366424.3383558},
abstract = {Fair machine learning is receiving an increasing attention in machine learning fields. Researchers in fair learning have developed correlation or association-based measures such as demographic disparity, mistreatment disparity, calibration, causal-based measures such as total effect, direct and indirect discrimination, and counterfactual fairness, and fairness notions such as equality of opportunity and equalized odds that consider both decisions in the training data and decisions made by predictive models. In this paper, we develop a new causal-based fairness notation, called equality of effort. Different from existing fairness notions which mainly focus on discovering the disparity of decisions between two groups of individuals, the proposed equality of effort notation helps answer questions like to what extend a legitimate variable should change to make a particular individual achieve a certain outcome level and addresses the concerns whether the efforts made to achieve the same outcome level for individuals from the protected group and that from the unprotected group are different. We develop algorithms for determining whether an individual or a group of individuals is discriminated in terms of equality of effort. We also develop an optimization-based method for removing discriminatory effects from the data if discrimination is detected. We conduct empirical evaluations to compare the equality of effort and existing fairness notion and show the effectiveness of our proposed algorithms.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {743–751},
numpages = {9},
keywords = {Causality, Fairness, Equality of Effort},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383559,
author = {Babaeianjelodar, Marzieh and Lorenz, Stephen and Gordon, Josh and Matthews, Jeanna and Freitag, Evan},
title = {Quantifying Gender Bias in Different Corpora},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383559},
doi = {10.1145/3366424.3383559},
abstract = {Word embedding models have been shown to be effective in performing a wide variety of Natural Language Processing (NLP) tasks such as identifying audiences for web advertisements, parsing resum\'{e}s to select promising job candidates, and translating documents from one language to another. However, it has been demonstrated that NLP systems learn gender bias from the corpora of documents on which they are trained. It is increasingly common for pre-trained models to be used as a starting point for building applications in a wide range of areas including critical decision making applications. It is also very easy to use a pre-trained model as the basis for a new application without careful consideration of the original nature of the training set. In this paper, we quantify the degree to which gender bias differs with the corpora used for training. We look especially at the impact of starting with a pre-trained model and fine-tuning with additional data. Specifically, we calculate a measure of direct gender bias on several pre-trained models including BERT’s Wikipedia and Book corpus models as well as on several fine-tuned General Language Understanding Evaluation (GLUE) benchmarks. In addition, we evaluate the bias from several more extreme corpora including the Jigsaw identity toxic dataset that includes toxic speech biased against race, gender, religion, and disability and the RtGender dataset that includes speech specifically labelled by gender. Our results reveal that the direct gender bias of the Jigsaw toxic identity dataset is surprisingly close to that of the base pre-trained Google model, but the RtGender dataset has significantly higher direct gender bias than the base model. When the bias learned by an NLP system can vary significantly with the corpora used for training, it becomes important to consider and report these details, especially for use in critical decision-making applications. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {752–759},
numpages = {8},
keywords = {BERT, datasets, natural language processing, gender bias},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383560,
author = {Gordon, Joshua and Babaeianjelodar, Marzieh and Matthews, Jeanna},
title = {Studying Political Bias via Word Embeddings},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383560},
doi = {10.1145/3366424.3383560},
abstract = {Machine Learning systems learn bias in addition to other patterns from input data on which they are trained. Bolukbasi et al. pioneered a method for quantifying gender bias learned from a corpus of text. Specifically, they compute a gender subspace into which words, represented as word vectors, can be placed and compared with one another. In this paper, we apply a similar methodology to a different type of bias, political bias. Unlike with gender bias, it is not obvious how to choose a set of definitional word pairs to compute a political bias subspace. We propose a methodology for doing so that could be used for modeling other types of bias as well. We collect and examine a 26 GB corpus of tweets from Republican and Democratic politicians in the United States (presidential candidates and members of Congress). With our definition of a political bias subspace, we observe several interesting and intuitive trends including that tweets from presidential candidates, both Republican and Democratic, show more political bias than tweets from other politicians of the same party. This work models political bias as a binary choice along one axis, as Bolukbasi et al. did for gender. However, most kinds of bias - political, racial and even gender bias itself - are much more complicated than two binary extremes along one axis. In this paper, we also discuss what might be required to model bias along multiple axes (e.g. liberal/conservative and authoritarian/libertarian for political bias) or as a range of points along a single axis (e.g. a gender spectrum). },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {760–764},
numpages = {5},
keywords = {Twitter dataset, political bias, natural language processing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383561,
author = {Graells-Garrido, Eduardo and Baeza-Yates, Ricardo and Lalmas, Mounia},
title = {Representativeness of Abortion Legislation Debate on Twitter: A Case Study in Argentina and Chile},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383561},
doi = {10.1145/3366424.3383561},
abstract = {The role of the Web in political exchange has been crucial for society. Its platforms have connected people and allowed manifestation, organization, and access to information; however, they have also produced negative outcomes, such as increased polarization and fast disinformation spreading. These types of phenomena are not completely understood in the context of continuous technological change. Here we propose to grow knowledge in these issues by focusing on representativeness, through the following question: How demographic groups are represented in the discussion on micro-blogging platforms? Our aim is to answer this question on the discussion about a specific topic, abortion, as observed on one of the most popular micro-blogging platforms. As a case study, we followed the abortion discussion on Twitter in two Spanish-speaking countries from 2015 to 2018. Our results indicate differences in representativeness with respect to country, stance, and time of publication, a process that affects to on-going legislation. These findings show that demographic groups differ in how they generate content, and that under- and over-represented groups are not the same between countries, implying that single-country outcomes are not generalizable.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {765–774},
numpages = {10},
keywords = {Data Bias, Social Networks, Stance Prediction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383562,
author = {G. Harris, Christopher},
title = {Mitigating Cognitive Biases in Machine Learning Algorithms for Decision Making},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383562},
doi = {10.1145/3366424.3383562},
abstract = {Cognitive biases are an ingrained part of the human decision-making process. Nearly all machine learning algorithms that mimic human decision-making use human judgments as training data, which propagates these biases. In this paper, we conduct an empirical study in which 150 applicants are rated for suitability for three separate job openings. We develop an algorithm that learns from human judgments and consequently develops biases based on these human-generated inputs. Next, we explore and apply techniques to mitigate these algorithmic biases, using a combination of pre-processing, in-processing, and post-processing algorithms. The results from our study show that biases can be mitigated using these approaches but involve a tradeoff between complexity and effectiveness.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {775–781},
numpages = {7},
keywords = {Data Science, Machine Learning, Fairness, Artificial Intelligence, Human Resources Tasks, Algorithmic Bias, Decision Making, Cognitive Bias},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383564,
author = {Baeza-Yates, Ricardo},
title = {Biases on Social Media Data: (Keynote Extended Abstract)},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383564},
doi = {10.1145/3366424.3383564},
abstract = {Social media data is often used to pulse the opinion of online communities, either by predicting sentiment or stances (e.g., political), to mention just two typical use cases. However, those analysis assume that the data samples really represent the underlying demographics of the overall community, both, in number and characteristics, which in most cases is not true. As a result, extrapolating these results to larger populations usually do not work. This happens because social media data is inherently biased, mainly due to two facts: (1) not all people is equally active in social media platforms and most of them are really passive; and (2) there are demographic biases in gender and age, among other attributes. Hence, the questions of how representative is the data and if is possible to make it representative are of crucial importance. We also discuss related issues such as using public samples of mostly private platforms as well as typical errors in the analysis of such data.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {782–783},
numpages = {2},
keywords = {representativeness, gender and age prediction., Bias, data samples},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383565,
author = {Chuai, Ziang and Geng, Qian and Jin, Jian},
title = {Domain-Specific Automatic Scholar ProfilingBased on Wikipedia},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383565},
doi = {10.1145/3366424.3383565},
abstract = {Scholar profiling is usually invited to provide junior researchers with domain-specific concepts systematically, but the organization of domain literatures is usually time-consuming. Then, an automatic scholar profiling method is preferred. Firstly, to extract some properties of a given scholar, structured data, like infobox in Wikipedia, are often used as training datasets. But it may lead to serious mis-labeling problems, such as institutions and alma maters, and a Fine-Grained Entity Typing method is expected. Thus, a novel Relation Embedding method based on local context is proposed to enhance the typing performance. Also, to highlight critical concepts in selective bibliographies of scholars, a novel Keyword Extraction method based on Learning to Rank is proposed to bridge the gap that conventional supervised methods fail to provide junior scholars with relative importance of keywords. Categories of experiments were conducted to evaluate the Relation Embedding method and the Keyword Extraction method, which show that the proposed approaches outperform existing baseline methods notably.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {786–793},
numpages = {8},
keywords = {named entity typing, keyword extraction, relation embedding, scholar profiling, learning to rank},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383567,
author = {Miz, Volodymyr and Hanna, Jo\"{e}lle and Aspert, Nicolas and Ricaud, Benjamin and Vandergheynst, Pierre},
title = {What is Trending on Wikipedia? Capturing Trends and Language Biases Across Wikipedia Editions},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383567},
doi = {10.1145/3366424.3383567},
abstract = {In this work, we propose an automatic evaluation and comparison of the browsing behavior of Wikipedia readers that can be applied to any language editions of Wikipedia. As an example, we focus on English, French, and Russian languages during the last four months of 2018. The proposed method has three steps. Firstly, it extracts the most trending articles over a chosen period of time. Secondly, it performs a semi-supervised topic extraction and thirdly, it compares topics across languages. The automated processing works with the data that combines Wikipedia’s graph of hyperlinks, pageview statistics and summaries of the pages. The results show that people share a common interest and curiosity for entertainment, e.g. movies, music, sports independently of their language. Differences appear in topics related to local events or about cultural particularities. Interactive visualizations showing clusters of trending pages in each language edition are available online https://wiki-insights.epfl.ch/wikitrendshttps://wiki-insights.epfl.ch/wikitrends},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {794–801},
numpages = {8},
keywords = {data mining, languages, networks, society, Wikipedia},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383568,
author = {Li, Ang and Farzan, Rosta},
title = {Collaboration of Open Content News in Wikipedia: The Role and Impact of Gatekeepers},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383568},
doi = {10.1145/3366424.3383568},
abstract = {With the advances of social technologies, open content in social media has become an important place where people gather and communicate news. Prior studies in conventional mass media has long studied the news reporting process, and suggest that gatekeepers – editors or journalists who control the information and have a unique power to determine what gets published to the public – play an important role in the news reporting process. However, as the process of how open content are created by contributors in social media platforms is different, what we understand about content publication process in traditional mass media can not directly apply in the context of social media. Especially, it is unclear who are the gatekeepers and how do they influence the content creation and spread of information in social media. In the current proposed study, I aim to understand this new model of content generation process through the lens of gatekeepers in social media platforms such as Wikipedia. Specifically, I aim to discover ways to identify gatekeepers and assess their impact on information quality and content polarization.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {802–805},
numpages = {4},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383569,
author = {Beyt\'{\i}a, Pablo},
title = {The Positioning Matters: Estimating Geographical Bias in the Multilingual Record of Biographies on Wikipedia},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383569},
doi = {10.1145/3366424.3383569},
abstract = {This article proposes that an appropriate assessment of the geographical bias in multilingual Wikipedia's content should consider not only the number of articles linked to places, but also their internal positioning –i.e. their location in different languages and their centrality in the network of references between articles–. This idea is studied empirically, systematically evaluating the geographic concentration in the biographical coverage of globally recognized individuals (those whose biographies are found in more than 25 language versions of Wikipedia). Considering the internal positioning levels of these biographies, only 5 countries account for more than 62% of Wikipedia's biographical coverage. In turn, the inequality in coverage between countries reaches very high levels, estimated with a Gini coefficient of .84 and a Palma ratio of 207. In all the tests carried out, the inclusion of the linguistic and/or relational positioning of the articles increases the estimate of inequality in biographical coverage. This suggests that previous estimates of geographical bias, which do not consider differences in internal positioning, have underestimated the degree of inequality in the distribution of information.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {806–810},
numpages = {5},
keywords = {geo-tagged information, Wikipedia, information inequality, geographical bias},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383570,
author = {Ni, Chien-Chun and Sum Liu, Kin and Torzec, Nicolas},
title = {Layered Graph Embedding for Entity Recommendation Using Wikipedia in the Yahoo! Knowledge Graph},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383570},
doi = {10.1145/3366424.3383570},
abstract = {In this paper, we describe an embedding-based entity recommendation framework for Wikipedia that organizes Wikipedia into a collection of graphs layered on top of each others, learns complementary entity representations from their topology and content, and combines them with a lightweight learning-to-rank approach to recommend related entities on Wikipedia. Through offline and online evaluations, we show that the resulting embeddings and recommendations perform well in terms of quality and user engagement. Balancing simplicity and quality, this framework provides default entity recommendations for English and other languages in the Yahoo! Knowledge Graph, which Wikipedia is a core subset of. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {811–818},
numpages = {8},
keywords = {graph embedding, representation learning, entity recommendation, knowledge graph, learning-to-rank},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383571,
author = {Liubonko, Kateryna and S\'{a}ez-Trumper, Diego},
title = {Matching Ukrainian Wikipedia Red Links with English Wikipedia’s Articles},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383571},
doi = {10.1145/3366424.3383571},
abstract = {This work tackles the problem of matching Wikipedia red links with existing articles. Links in Wikipedia pages are considered red when lead to nonexistent articles. In other Wikipedia editions could exist articles that correspond to such red links. In our work, we propose a way to match red links in one Wikipedia edition to existent pages in another edition. We define the task as a Named Entity Linking problem because red link titles are mostly named entities. We solve it in a context of Ukrainian red links and English existing pages. We created a dataset of 3171 most frequent Ukrainian red links and a dataset of almost 3 million pairs of red links and the most probable candidates for the correspondent pages in English Wikipedia. This dataset is publicly released1. In this work we define conceptual characteristics of the data — word and graph properties — based on its analysis and exploit these properties in entity resolution. BabelNet knowledge base was applied to this task and was regarded as a baseline for our approach (F1 score = 32 %). To improve the result we introduced several similarity metrics based on mentioned red links characteristics. Combined in a linear model they resulted in F1 score = 85 %. To the best of our knowledge, we are the first to state the problem and propose a solution for red links in Ukrainian Wikipedia edition.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {819–826},
numpages = {8},
keywords = {Wikipedia, Entity linking, Red links},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3386584,
author = {Moustaka, Vaia and Maitis, Antonis and Vakali, Athena and Anthopoulos, Leonidas G.},
title = {CityDNA Dynamics: A Model for Smart City Maturity and Performance Benchmarking},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3386584},
doi = {10.1145/3366424.3386584},
abstract = {Smart cities have emerged significantly since their initial appearance in 1990s, more and more cities around the world are striving to gain intelligence and in this regard the need for standardization and performance measurement grows. Given the current challenges in the field of smart cities, this work revisits the proposed “cityDNA” framework which has been designed to detect the interrelations between smart city dimensions and form city profile. This work contribute to further enable benchmarking and measuring the maturity of smart cities through the exploitation of international standards and civil engagement. The proposed framework along with the design of a web application will handle urban data effectively. “cityDNA” that will be advanced to receive and utilize urban data and visualize cities’ health and maturity metrics is presented, while potential constraints on its implementation are highlighted.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {829–833},
numpages = {5},
keywords = {smart cities, citizen engagement, cityDNA, standardization, city profile, maturity, conceptual model, artificial intelligence, city benchmarking},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3386586,
author = {Degbelo, Auriol},
title = {Open Data User Needs: A Preliminary Synthesis},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3386586},
doi = {10.1145/3366424.3386586},
abstract = {Open city data is critical for smart cities and is becoming increasingly available, thanks to open government initiatives. Yet, we still know little about the needs of users concerning open data reuse. Taking fragmented and scattered issues reported by users in several works as a starting point, this article formulates user needs statements for research at the intersection of the topics of web, city, and data reuse. The 27 user needs statements proposed can inform the design and evaluation of tools that facilitate open data reuse.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {834–839},
numpages = {6},
keywords = {dataset search, open government data, user needs statements, web intelligence, Smart city, human-computer interaction theory},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3386585,
author = {Cantuarias-Villessuzanne, Carmen and Weigel, Romain},
title = {Mapping of Smart Cities in Order to Understand the Cities’ Sustainability},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3386585},
doi = {10.1145/3366424.3386585},
abstract = {This paper aims at building a mapping of smart cities based on the concept of core capabilities defined by Teece et&nbsp;al. [11]. We pursue two objectives: improving on the analysis of the practices of the smart cities, and understanding the sustainability of city strategies according to their economic, geographic and demographic characteristics.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {842–843},
numpages = {2},
keywords = {principal component analysis, smart cities, dynamic capabilities approach, knowledge management},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382684,
author = {Song, Shuangyong and Wang, Chao and Chen, Haiqing and Chen, Huan},
title = {TCNN: Triple Convolutional Neural Network Models for Retrieval-Based Question Answering System in E-Commerce},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382684},
doi = {10.1145/3366424.3382684},
abstract = {A key solution to the Information Retrieval (IR)-based question answering (QA) models is to retrieve the most similar knowledge entries of a given query from a QA knowledge base, and then rerank those knowledge entries with semantic matching models. In this paper, we aim to improve an IR based e-commerce QA system-AliMe with proposed text matching models, including a basic Triple Convolutional Neural Network (TCNN) model and two Attention-based TCNN (ATCNN) models.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {844–845},
numpages = {2},
keywords = {Semantic matching, Chatbot, Convolutional Neural Network},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382725,
author = {Ren, Yankun and Lin, Jianbin and Zhou, Jun},
title = {Neural Zero-Shot Fine-Grained Entity Typing},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382725},
doi = {10.1145/3366424.3382725},
abstract = {Fine-grained entity typing is a task to assign types to entity mentions dependent on mentions’ context. Due to the heavy work of human annotation, high quality training data is always not enough, zero-shot fine-grained entity typing becomes important. Previous zero-shot works rely on hand-crafted features and suffers from noisy distant supervision induced training data. In this paper, we propose a Neural Zero-Shot Fine-Grained Entity Typing (NZFET) model. NZFET is an end-to-end neural model free from hand-crafted features. The entity type attention in NZFET makes model focus on information relevent to the entity type. In our experiments, NZFET obtains better results on popular datasets than previous works. And we show experimentally that our method is robust against noisy training data.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {846–847},
numpages = {2},
keywords = {natural language processing, zero-shot entity typing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3383295,
author = {Neve, James and Palomares, Iv\'{a}n},
title = {Hybrid Reciprocal Recommender Systems: Integrating Item-to-User Principles in Reciprocal Recommendation},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383295},
doi = {10.1145/3366424.3383295},
abstract = {Reciprocal Recommender Systems (RRS) recommend users to other users in a personalised manner, in scenarios where both sides of the preference relation must be considered. Existing RRS approaches based on collaborative filtering or content-based filtering, have been used for enhancing user experience in online dating and other online services aimed at connecting users with each other. However, some of these services e.g. skill sharing platforms, are still pervaded by content published, shared and consumed by users, consequently there is a valuable source of item-to-user preferential information not captured by existing RRS models. We present a novel hybrid RRS framework that integrates user preferences towards content in reciprocal recommendation, and we instantiate and evaluate it using data from Cookpad, a recipe sharing social media platform. As part of our model, we also implement a novel content-based extension of Jaccard similarity measure that operates on word embeddings. },
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {848–853},
numpages = {6},
keywords = {Latent Factor Models, Reciprocal Recommender Systems, Word Embeddings},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366424.3382710,
author = {Kang, Jiaojian and Zhang, Wen and Kong, Hao and Zhang, Wei and Chen, Huajun},
title = {Learning Rule Embeddings over Knowledge Graphs: A Case Study from E-Commerce Entity Alignment},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382710},
doi = {10.1145/3366424.3382710},
abstract = {E-Commerce platforms such as Alibaba and Amazon make people's life more convenient and play an important role in people's life. One critical issue for these platforms is entity alignment over item knowledge graph aiming at detecting if two items belong to the same product. In knowledge graph, rule learning methods and embedding-based methods are either inefficient or unexplainable. In this paper, we combine these two methods together and propose an embedding-based rule learning method that regard alignment rules as embeddings and learn these embeddings in the model training process. The experimental results on the real world E-Commerce dataset indicate that our proposed method is more effective and can reach better performance than other related methods.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {854–855},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

