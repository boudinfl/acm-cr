@inproceedings{10.1145/3249238,
author = {Gravano, Luis},
title = {Session Details: Search Engineering 1},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249238},
doi = {10.1145/3249238},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988674,
author = {Ntoulas, Alexandros and Cho, Junghoo and Olston, Christopher},
title = {What's New on the Web? The Evolution of the Web from a Search Engine Perspective},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988674},
doi = {10.1145/988672.988674},
abstract = {We seek to gain improved insight into how Web search engines shouldcope with the evolving Web, in an attempt to provide users with themost up-to-date results possible. For this purpose we collectedweekly snapshots of some 150 Web sites over the course of one year,and measured the evolution of content and link structure. Our measurements focus on aspects of potential interest to search engine designers: the evolution of link structure over time, the rate ofcreation of new pages and new distinct content on the Web, and the rate of change of the content of existing pages under search-centric measures of degree of change.Our findings indicate a rapid turnover rate of Web pages, i.e.,high rates of birth and death, coupled with an even higher rate ofturnover in the hyperlinks that connect them. For pages that persistover time we found that, perhaps surprisingly, the degree of contentshift as measured using TF.IDF cosine distance does not appear to beconsistently correlated with the frequency of contentupdating. Despite this apparent non-correlation, the rate of content shift of a given page is likely to remain consistent over time. That is, pages that change a great deal in one week will likely change by a similarly large degree in the following week. Conversely, pages that experience little change will continue to experience little change. We conclude the paper with a discussion of the potential implications ofour results for the design of effective Web search engines.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {1–12},
numpages = {12},
keywords = {web pages, search engines, degree of change, web characterization, rate of change, link structure evolution, web evolution, change prediction},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988675,
author = {Rose, Daniel E. and Levinson, Danny},
title = {Understanding User Goals in Web Search},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988675},
doi = {10.1145/988672.988675},
abstract = {Previous work on understanding user web search behavior has focused on how people search and what they are searching for, but not why they are searching. In this paper, we describe a framework for understanding the underlying goals of user searches, and our experience in using the framework to manually classify queries from a web search engine. Our analysis suggests that so-called navigational" searches are less prevalent than generally believed while a previously unexplored "resource-seeking" goal may account for a large fraction of web searches. We also illustrate how this knowledge of user search goals might be used to improve future web search engines.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {13–19},
numpages = {7},
keywords = {web search, query classification, user goals, information retrieval, user behavior},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988676,
author = {Cho, Junghoo and Roy, Sourashis},
title = {Impact of Search Engines on Page Popularity},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988676},
doi = {10.1145/988672.988676},
abstract = {Recent studies show that a majority of Web page accesses are referred by search engines. In this paper we study the widespread use of Web search engines and its impact on the ecology of the Web. In particular, we study how much impact search engines have on the popularity evolution of Web pages. For example, given that search engines return currently popular" pages at the top of search results, are we somehow penalizing newly created pages that are not very well known yet? Are popular pages getting even more popular and new pages completely ignored? We first show that this unfortunate trend indeed exists on the Web through an experimental study based on real Web data. We then analytically estimate how much longer it takes for a new page to attract a large number of Web users when search engines return only popular pages at the top of search results. Our result shows that search engines can have an immensely worrisome impact on the discovery of new Web pages.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {20–29},
numpages = {10},
keywords = {pagerank, search engine's impact, random surfer model, change in pagerank, web evolution},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249239,
author = {MacDaniel, Patrick},
title = {Session Details: Security and Privacy},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249239},
doi = {10.1145/3249239},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988678,
author = {Novak, Jasmine and Raghavan, Prabhakar and Tomkins, Andrew},
title = {Anti-Aliasing on the Web},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988678},
doi = {10.1145/988672.988678},
abstract = {It is increasingly common for users to interact with the web using a number of different aliases. This trend is a double-edged sword. On one hand, it is a fundamental building block in approaches to online privacy. On the other hand, there are economic and social consequences to allowing each user an arbitrary number of free aliases. Thus, there is great interest in understanding the fundamental issues in obscuring the identities behind aliases.However, most work in the area has focused on linking aliases through analysis of lower-level properties of interactions such as network routes. We show that aliases that actively post text on the web can be linked together through analysis of that text. We study a large number of users posting on bulletin boards, and develop algorithms to anti-alias those users: we can with a high degree of success identify when two aliases belong to the same individual.Our results show that such techniques are surprisingly effective, leading us to conclude that guaranteeing privacy among aliases that post actively requires mechanisms that do not yet exist.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {30–39},
numpages = {10},
keywords = {pseudonyms, privacy, personas, bulletin boards, aliases, alias detection},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988679,
author = {Huang, Yao-Wen and Yu, Fang and Hang, Christian and Tsai, Chung-Hung and Lee, Der-Tsai and Kuo, Sy-Yen},
title = {Securing Web Application Code by Static Analysis and Runtime Protection},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988679},
doi = {10.1145/988672.988679},
abstract = {Security remains a major roadblock to universal acceptance of the Web for many kinds of transactions, especially since the recent sharp increase in remotely exploitable vulnerabilities have been attributed to Web application bugs. Many verification tools are discovering previously unknown vulnerabilities in legacy C programs, raising hopes that the same success can be achieved with Web applications. In this paper, we describe a sound and holistic approach to ensuring Web application security. Viewing Web application vulnerabilities as a secure information flow problem, we created a lattice-based static analysis algorithm derived from type systems and typestate, and addressed its soundness. During the analysis, sections of code considered vulnerable are instrumented with runtime guards, thus securing Web applications in the absence of user intervention. With sufficient annotations, runtime overhead can be reduced to zero. We also created a tool named.WebSSARI (Web application Security by Static Analysis and Runtime Inspection) to test our algorithm, and used it to verify 230 open-source Web application projects on SourceForge.net, which were selected to represent projects of different maturity, popularity, and scale. 69 contained vulnerabilities. After notifying the developers, 38 acknowledged our findings and stated their plans to provide patches. Our statistics also show that static analysis reduced potential runtime overhead by 98.4%.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {40–52},
numpages = {13},
keywords = {security vulnerabilities, information flow, web application security, type systems, noninterference, program security, verification},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988680,
author = {Skogsrud, Halvard and Benatallah, Boualem and Casati, Fabio},
title = {Trust-Serv: Model-Driven Lifecycle Management of Trust Negotiation Policies for Web Services},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988680},
doi = {10.1145/988672.988680},
abstract = {A scalable approach to trust negotiation is required in Web service environments that have large and dynamic requester populations. We introduce Trust-Serv, a model-driven trust negotiation framework for Web services. The framework employs a model for trust negotiation that is based on state machines, extended with security abstractions. Our policy model supports lifecycle management, an important trait in the dynamic environments that characterize Web services. In particular, we provide a set of change operations to modify policies, and migration strategies that permit ongoing negotiations to be migrated to new policies without being disrupted. Experimental results show the performance benefit of these strategies. The proposed approach has been implemented as a container-centric mechanism that is transparent to the Web services and to the developers of Web services, simplifying Web service development and management as well as enabling scalable deployments.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {53–62},
numpages = {10},
keywords = {web services, trust negotiation, lifecycle management, conceptual modeling},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249240,
author = {Chang, Bay-Wei},
title = {Session Details: Usability and Accessibility},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249240},
doi = {10.1145/3249240},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988682,
author = {Milic-Frayling, Natasa and Jones, Rachel and Rodden, Kerry and Smyth, Gavin and Blackwell, Alan and Sommerer, Ralph},
title = {Smartback: Supporting Users in Back Navigation},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988682},
doi = {10.1145/988672.988682},
abstract = {This paper presents the design and user evaluation of SmartBack, a feature that complements the standard Back button by enabling users to jump directly to key pages in their navigation session, making common navigation activities more efficient. Defining key pages was informed by the findings of a user study that involved detailed monitoring of Web usage and analysis of Web browsing in terms of navigation trails. The pages accessible through SmartBack are determined automatically based on the structure of the user's navigation trails or page association with specific user's activities, such as search or browsing bookmarked sites. We discuss implementation decisions and present results of a usability study in which we deployed the SmartBack prototype and monitored usage for a month in both corporate and home settings. The results show that the feature brings qualitative improvement to the browsing experience of individuals who use it.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {63–71},
numpages = {9},
keywords = {browsing, web trails, revisitation, web usage, usability study, navigation, back navigation},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988683,
author = {Richards, John T. and Hanson, Vicki L.},
title = {Web Accessibility: A Broader View},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988683},
doi = {10.1145/988672.988683},
abstract = {Web accessibility is an important goal. However, most approaches to its attainment are based on unrealistic economic models in which Web content developers are required to spend too much for which they receive too little. We believe this situation is due, in part, to the overly narrow definitions given both to those who stand to benefit from enhanced access to the Web and what is meant by this enhanced access. In this paper, we take a broader view, discussing a complementary approach that costs developers less and provides greater advantages to a larger community of users. While we have quite specific aims in our technical work, we hope it can also serve as an example of how the technical conversation regarding Web accessibility can move beyond the narrow confines of limited adaptations for small populations.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {72–79},
numpages = {8},
keywords = {standards, user interface, web accessibility},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988684,
author = {Ramakrishnan, I. V. and Stent, Amanda and Yang, Guizhen},
title = {Hearsay: Enabling Audio Browsing on Hypertext Content},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988684},
doi = {10.1145/988672.988684},
abstract = {In this paper we present HearSay, a system for browsing hypertext Web documents via audio. The HearSay system is based on our novel approach to automatically creating audio browsable content from hypertext Web documents. It combines two key technologies: (1) automatic partitioning of Web documents through tightly coupled structural and semantic analysis, which transforms raw HTML documents into semantic structures so as to facilitate audio browsing; and (2) VoiceXML, an already standardized technology which we adopt to represent voice dialogs automatically created from the XML output of partitioning. This paper describes the software components of HearSay and presents an initial system evaluation.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {80–89},
numpages = {10},
keywords = {HTML, World Wide Web, user interface, audio browser, VoiceXML, structural analysis, semantic analysis},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249241,
author = {Bayardo, Roberto},
title = {Session Details: Information Extraction},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249241},
doi = {10.1145/3249241},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988686,
author = {Cui, Hang and Kan, Min-Yen and Chua, Tat-Seng},
title = {Unsupervised Learning of Soft Patterns for Generating Definitions from Online News},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988686},
doi = {10.1145/988672.988686},
abstract = {Breaking news often contains timely definitions and descriptions of current terms, organizations and personalities. We utilize such web sources to construct definitions for such terms. Previous work has identified definitions using hand-crafted rules or supervised learning that constructs rigid, hard text patterns. In contrast, we demonstrate a new approach that uses flexible, soft matching patterns to characterize definition sentences. Our soft patterns are able to effectively accommodate the diversity of definition sentence structure exhibited in news. We use pseudo-relevance feedback to automatically label sentences for use in soft pattern generation. The application of our unsupervised method significantly improves baseline systems on both the standardized TREC corpus as well as crawled online news articles by 27% and 30%, respectively, in terms of F measure. When applied to a state-of-art definition generation system recently fielded in the TREC 2003 definitional question answering task, it improves the performance by 14%.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {90–99},
numpages = {10},
keywords = {unsupervised learning, definitional question answering, pseudo-relevance feedback, soft patterns, definition generation},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988687,
author = {Etzioni, Oren and Cafarella, Michael and Downey, Doug and Kok, Stanley and Popescu, Ana-Maria and Shaked, Tal and Soderland, Stephen and Weld, Daniel S. and Yates, Alexander},
title = {Web-Scale Information Extraction in Knowitall: (Preliminary Results)},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988687},
doi = {10.1145/988672.988687},
abstract = {Manually querying search engines in order to accumulate a large bodyof factual information is a tedious, error-prone process of piecemealsearch. Search engines retrieve and rank potentially relevantdocuments for human perusal, but do not extract facts, assessconfidence, or fuse information from multiple documents. This paperintroduces KnowItAll, a system that aims to automate the tedious process ofextracting large collections of facts from the web in an autonomous,domain-independent, and scalable manner.The paper describes preliminary experiments in which an instance of KnowItAll, running for four days on a single machine, was able to automatically extract 54,753 facts. KnowItAll associates a probability with each fact enabling it to trade off precision and recall. The paper analyzes KnowItAll's architecture and reports on lessons learned for the design of large-scale information extraction systems.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {100–110},
numpages = {11},
keywords = {information extraction, mutual information, pmi, search},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988688,
author = {Ramakrishnan, Ganesh and Chakrabarti, Soumen and Paranjpe, Deepa and Bhattacharya, Pushpak},
title = {Is Question Answering an Acquired Skill?},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988688},
doi = {10.1145/988672.988688},
abstract = {We present a question answering (QA) system which learns how to detect and rank answer passages by analyzing questions and their answers (QA pairs) provided as training data. We built our system in only a few person-months using off-the-shelf components: a part-of-speech tagger, a shallow parser, a lexical network, and a few well-known supervised learning algorithms. In contrast, many of the top TREC QA systems are large group efforts, using customized ontologies, question classifiers, and highly tuned ranking functions. Our ease of deployment arises from using generic, trainable algorithms that exploit simple feature extractors on QA pairs. With TREC QA data, our system achieves mean reciprocal rank (MRR) that compares favorably with the best scores in recent years, and generalizes from one corpus to another. Our key technique is to recover, from the question, fragments of what might have been posed as a structured query, had a suitable schema been available. comprises selectors: tokens that are likely to appear (almost) unchanged in an answer passage. The other fragment contains question tokens which give clues about the answer type, and are expected to be replaced in the answer passage by tokens which specialize or instantiate the desired answer type. Selectors are like constants in where-clauses in relational queries, and answer types are like column names. We present new algorithms for locating selectors and answer type clues and using them in scoring passages with respect to a question.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {111–120},
numpages = {10},
keywords = {question answering, machine learning},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249242,
author = {Douglis, Fred},
title = {Session Details: Mobility},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249242},
doi = {10.1145/3249242},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988690,
author = {Rodriguez, Pablo and Mukherjee, Sarit and Ramgarajan, Sampath},
title = {Session Level Techniques for Improving Web Browsing Performance on Wireless Links},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988690},
doi = {10.1145/988672.988690},
abstract = {Recent observations through experiments that we have performed incurrent third generation wireless networks have revealed that the achieved throughput over wireless links varies widely depending on the application. In particular, the throughput achieved by file transfer application (FTP) and web browsing application (HTTP) are quite different. The throughput achieved over a HTTP session is much lower than that achieved over an FTP session. The reason for the lower HTTP throughput is that the HTTP protocol is affected by the large Round-Trip Time (RTT) across Wireless links. HTTP transfers require multiple TCP connections and DNS lookups before a HTTP page can be displayed. Each TCP connection requires several RTTs to fully open the TCP send window and each DNS lookup requires several RTTs before resolving the domain name to IP mapping. These TCP/DNS RTTs significantly degrade the performance of HTTP over wireless links. To overcome these problems, we have developed session level optimization techniques to enhance HTTP download mechanisms. These techniques (a) minimize the number of DNS lookups over the wireless link and (b) minimize the number of TCP connections opened by the browser. These optimizations bridge the mismatch caused by wireless links between application-level protocols (such as HTTP) and transport-level protocols (such asTCP). Our solutions do not require any client-side software and can be deployed transparently on a service provider network toprovide 30-50% decrease in end-to-end user perceived latency and 50-100% increase in data throughput across wireless links for HTTP sessions.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {121–130},
numpages = {10},
keywords = {web, wireless, optimizations},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988691,
author = {Zhou, Dong and Islam, Nayeem and Ismael, Ali},
title = {Flexible On-Device Service Object Replication with Replets},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988691},
doi = {10.1145/988672.988691},
abstract = {An increasingly large amount of Web applications employ service
objects such as Servlets to generate dynamic and personalized
content. Existing caching infrastructures are not well suited for
caching such content in mobile environments because of
disconnection and weak connection. One possible approach to this
problem is to replicate Web-related application logic to client
devices. The challenges to this approach are to deal with client
devices that exhibit huge divergence in resource availabilities, to
support applications that have different data sharing and coherency
requirements, and to accommodate the same application under
different deployment environments.The Replet system targets these challenges. It uses client,
server and application capability and preference information (CPI)
to direct the replication of service objects to client devices:
from the selection of a device for replication and populating the
device with client-specific data, to choosing an appropriate
replica to serve a given request and maintaining the desired state
consistency among replicas. The Replet system exploits on-device
replication to enable client-, server- and application-specific
cost metrics for replica invocation and synchronization. We have
implemented a prototype in the context of Servlet-based Web
applications. Our experiment and simulation results demonstrate the
viability and significant benefits of CPI-driven on-device service
object replication.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {131–142},
numpages = {12},
keywords = {preference, capability, reconfiguration, replication, service, synchronization},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988692,
author = {Lai, Albert M. and Nieh, Jason and Bohra, Bhagyashree and Nandikonda, Vijayarka and Surana, Abhishek P. and Varshneya, Suchita},
title = {Improving Web Browsing Performance on Wireless Pdas Using Thin-Client Computing},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988692},
doi = {10.1145/988672.988692},
abstract = {Web applications are becoming increasingly popular for mobile wireless PDAs. However, web browsing on these systems can be quite slow. An alternative approach is handheld thin-client computing, in which the web browser and associated application logic run on a server, which then sends simple screen updates to thePDA for display. To assess the viability of this thin-client approach, we compare the web browsing performance of thin clients against fat clients that run the web browser locally on a PDA. Our results show that thin clients can provide better web browsing performance compared to fat clients, both in terms of speed and ability to correctly display web content. Surprisingly, thin clients are faster even when having to send more data over the network. We characterize and analyze different design choices in various thin-client systems and explain why these approaches can yield superior web browsing performance on mobile wireless PDAs.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {143–154},
numpages = {12},
keywords = {wireless and mobility, thin-client computing, web performance},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249243,
author = {White, Bebo},
title = {Session Details: XML},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249243},
doi = {10.1145/3249243},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988694,
author = {Li, Quanzhong and Kim, Michelle Y. and So, Edward and Wood, Steve},
title = {XVM: A Bridge between Xml Data and Its Behavior},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988694},
doi = {10.1145/988672.988694},
abstract = {XML has become one of the core technologies for contemporary business applications, especially web-based applications. To facilitate processing of diverse XML data, we propose an extensible, integrated XML processing architecture, the XML Virtual Machine (XVM), which connects XML data with their behaviors. At the same time, the XVM is also a framework for developing and deploying XML-based applications. Using component-based techniques, the XVM supports arbitrary granularity and provides a high degree of modularity and reusability. XVM components are dynamically loaded and composed during XML data processing. Using the XVM, both client-side and server-side XML applications can be developed and deployed in an integrated way. We also present an XML application container built on top of the XVM along with several sample applications to demonstrate the applicability of the XVM framework.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {155–163},
numpages = {9},
keywords = {XML applications, XML, components, XML processing, web applications, XVM},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988695,
author = {Coen, Claudio Sacerdoti and Marinelli, Paolo and Vitali, Fabio},
title = {Schemapath, a Minimal Extension to Xml Schema for Conditional Constraints},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988695},
doi = {10.1145/988672.988695},
abstract = {In the past few years, a number of constraint languages for XML documents has been proposed. They are cumulatively called schema languages or validation languages and they comprise, among others, DTD, XML Schema, RELAX NG, Schematron, DSD, xlinkit. One major point of discrimination among schema languages is the support of co-constraints, or co-occurrence constraints, e.g., requiring that attribute A is present if and only if attribute B is (or is not) presentin the same element. Although there is no way in XML Schema to express these requirements, they are in fact frequently used in many XML document types, usually only expressed in plain human-readable text, and validated by means of special code modules by the relevant applications. In this paper we propose SchemaPath, a light extension of XML Schema to handle conditional constraints on XML documents. Two new constructs have been added to XML Schema: conditions -- based on XPath patterns -- on type assignments for elements and attributes; and a new simple type, xsd:error, for the direct expression of negative constraints (e.g. it is prohibited for attribute A to be present if attribute B is also present). A proof-of-concept implementation is provided. A Web interface is publicly accessible for experiments and assessments of the real expressiveness of the proposed extension.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {164–174},
numpages = {11},
keywords = {schema languages, xml, schemapath, co-constraints},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988696,
author = {Bernauer, Martin and Kappel, Gerti and Kramler, Gerhard},
title = {Composite Events for Xml},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988696},
doi = {10.1145/988672.988696},
abstract = {Recently, active behavior has received attention in the XML field to automatically react to occurred events. Aside from proprietary approaches for enriching XML with active behavior, the W3C standardized the Document Object Model (DOM) Event Module for the detection of events in XML documents. When using any of these approaches, however, it is often impossible to decide which event to react upon because not a single event but a combination of multiple events, i.e., a composite event determines a situation to react upon. The paper presents the first approach for detecting composite events in XML documents by addressing the peculiarities of XML events which are caused by their hierarchical order in addition to their temporal order. It also provides for the detection of satisfied multiplicity constraints defined by XML schemas. Thereby the approach enables applications operating on XML documents to react to composite events which have richer semantics.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {175–183},
numpages = {9},
keywords = {xml, event algebra, active behavior, event-condition-action rule, composite event},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249244,
author = {Liu, Bing},
title = {Session Details: Learning Classifiers},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249244},
doi = {10.1145/3249244},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988698,
author = {Huang, Chien-Chung and Chuang, Shui-Lung and Chien, Lee-Feng},
title = {Liveclassifier: Creating Hierarchical Text Classifiers through Web Corpora},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988698},
doi = {10.1145/988672.988698},
abstract = {Many Web information services utilize techniques of information extraction(IE) to collect important facts from the Web. To create more advanced services, one possible method is to discover thematic information from the collected facts through text classification. However, most conventional text classification techniques rely on manual-labelled corpora and are thus ill-suited to cooperate with Web information services with open domains. In this work, we present a system named LiveClassifier that can automatically train classifiersthrough Web corpora based on user-defined topic hierarchies. Due to its flexibility and convenience, LiveClassifier can be easily adapted for various purposes. New Web information services can be created to fully exploit it; human users can use it to create classifiers for their personal applications. The effectiveness of classifiers created by LiveClassifier is well supportedby empirical evidence.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {184–192},
numpages = {9},
keywords = {topic hierarchy, text classification, web mining},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988699,
author = {Shih, L. K. and Karger, D. R.},
title = {Using Urls and Table Layout for Web Classification Tasks},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988699},
doi = {10.1145/988672.988699},
abstract = {We propose new features and algorithms for automating Web-page classification tasks such as content recommendation and ad blocking. We show that the automated classification of Web pages can be much improved if, instead of looking at their textual content, we consider each links's URL and the visual placement of those links on a referring page. These features are unusual: rather than being scalar measurements like word counts they are tree structured---describing the position of the item in a tree. We develop a model and algorithm for machine learning using such tree-structured features. We apply our methods in automated tools for recognizing and blocking Web advertisements and for recommending "interesting" news stories to a reader. Experiments show that our algorithms are both faster and more accurate than those based on the text content of Web documents.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {193–202},
numpages = {10},
keywords = {news recommendation, classification, web applications, tree structures},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988700,
author = {Song, Ruihua and Liu, Haifeng and Wen, Ji-Rong and Ma, Wei-Ying},
title = {Learning Block Importance Models for Web Pages},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988700},
doi = {10.1145/988672.988700},
abstract = {Previous work shows that a web page can be partitioned into multiple segments or blocks, and often the importance of those blocks in a page is not equivalent. Also, it has been proven that differentiating noisy or unimportant blocks from pages can facilitate web mining, search and accessibility. However, no uniform approach and model has been presented to measure the importance of different segments in web pages. Through a user study, we found that people do have a consistent view about the importance of blocks in web pages. In this paper, we investigate how to find a model to automatically assign importance values to blocks in a web page. We define the block importance estimation as a learning problem. First, we use a vision-based page segmentation algorithm to partition a web page into semantic blocks with a hierarchical structure. Then spatial features (such as position and size) and content features (such as the number of images and links) are extracted to construct a feature vector for each block. Based on these features, learning algorithms are used to train a model to assign importance to different segments in the web page. In our experiments, the best model can achieve the performance with Micro-F1 79% and Micro-Accuracy 85.9%, which is quite close to a person's view.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {203–211},
numpages = {9},
keywords = {classification, web mining, block importance model, page segmentation},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249245,
author = {Paepcke, Andreas},
title = {Session Details: Web Site Engineering},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249245},
doi = {10.1145/3249245},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988702,
author = {Narayan, Michael and Williams, Christopher and Perugini, Saverio and Ramakrishnan, Naren},
title = {Staging Transformations for Multimodal Web Interaction Management},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988702},
doi = {10.1145/988672.988702},
abstract = {Multimodal interfaces are becoming increasingly ubiquitous with the advent of mobile devices, accessibility considerations, and novel software technologies that combine diverse interaction media. In addition to improving access and delivery capabilities, such interfaces enable flexible and personalized dialogs with websites, much like a conversation between humans. In this paper, we present a software framework for multimodal web interaction management that supports mixed-initiative dialogs between users and websites. A mixed-initiative dialog is one where the user and the website take turns changing the flow of interaction. The framework supports the functional specification and realization of such dialogs using staging transformations -- a theory for representing and reasoning about dialogs based on partial input. It supports multiple interaction interfaces, and offers sessioning, caching, and co-ordination functions through the use of an interaction manager. Two case studies are presented to illustrate the promise of this approach.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {212–223},
numpages = {12},
keywords = {mixed-initiative interaction, partial evaluation, web dialogs, out-of-turn interaction, program transformations},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988703,
author = {Parr, Terence John},
title = {Enforcing Strict Model-View Separation in Template Engines},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988703},
doi = {10.1145/988672.988703},
abstract = {The mantra of every experienced web application developer is the same: thou shalt separate business logic from display. Ironically, almost all template engines allow violation of this separation principle, which is the very impetus for HTML template engine development. This situation is due mostly to a lack of formal definition of separation and fear that enforcing separation emasculates a template's power. I show that not only is strict separation a worthy design principle, but that we can enforce separation while providing a potent template engine. I demonstrate my StringTemplate engine, used to build jGuru.com and other commercial sites, at work solving some nontrivial generational tasks.My goal is to formalize the study of template engines, thus, providing a common nomenclature, a means of classifying template generational power, and a way to leverage interesting results from formal language theory. I classify three types of restricted templates analogous to Chomsky's type 1..3 grammar classes and formally define separation including the rules that embody separation.Because this paper provides a clear definition of model-view separation, template engine designers may no longer blindly claim enforcement of separation. Moreover, given theoretical arguments and empirical evidence, programmers no longer have an excuse to entangle model and view.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {224–233},
numpages = {10},
keywords = {template engine, model-view-controller, web application},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988704,
author = {Bellas, Fernando and Fern\'{a}ndez, Daniel and Mui\~{n}o, Abel},
title = {A Flexible Framework for Engineering "My" Portals},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988704},
doi = {10.1145/988672.988704},
abstract = {There exist many portal servers that support the construction of "My" portals that is portals that allow the user to have one or more personal pages composed of a number of personalizable services. The main drawback of current portal servers is their lack of generality and adaptability. This paper presents the design of MyPersonalizer a J2EE-based framework for engineering My portals. The framework is structured according to the Model-View-Controller and Layers architectural patterns providing generic adaptable model and controller layers that implement the typical use cases of a My portal. MyPersonalizer allows for a good separation of roles in the development team: graphical designers (without programming skills) develop the portal view by writing JSP pages while software engineers implement service plugins and specify framework configuration.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {234–243},
numpages = {10},
keywords = {web engineering, portal technology, web application frameworks and architectures, design patterns, j2ee},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249246,
author = {Patel-Schneider, Peter},
title = {Session Details: Semantic Interfaces and OWL Tools},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249246},
doi = {10.1145/3249246},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988706,
author = {McDowell, Luke and Etzioni, Oren and Halevy, Alon and Levy, Henry},
title = {Semantic Email},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988706},
doi = {10.1145/988672.988706},
abstract = {This paper investigates how the vision of the Semantic Web can be carried overto the realm of email. We introduce a general notion of semantice mail, in which an email message consists of an RDF query or update coupled with corresponding explanatory text. Semantic email opens the door to a wide range of automated, email-mediated applications with formally guaranteed properties. In particular, this paper introduces a broad class of semantic email processes. For example consider the process of sending an email to a program committee asking who will attend the PC dinner automatically collecting the responses and tallying them up. We define bothlogical and decision-theoretic models where an email process ismodeled as a set of updates to a data set on which we specify goals via certain constraints or utilities. We then describe a set ofinference problems that arise while trying to satisfy these goals and analyze their computational tractability. In particular weshow that for the logical model it is possible to automatically infer which email responses are acceptable w.r.t. a set ofconstraints in polynomial time and for the decision-theoreticmodel it is possible to compute the optimal message-handling policy in polynomial time. Finally we discuss our publicly available implementation of semantic email and outline research challenges inthis realm.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {244–254},
numpages = {11},
keywords = {decision-theoretic, semantic web, formal model, satisfiability},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988707,
author = {Quan, D. A. and Karger, R.},
title = {How to Make a Semantic Web Browser},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988707},
doi = {10.1145/988672.988707},
abstract = {Two important architectural choices underlie the success of the Web: numerous, independently operated servers speak a common protocol, and a single type of client the Web browser provides point-and-click access to the content and services on these decentralized servers. However, because HTML marries content and presentation into a single representation, end users are often stuck with inappropriate choices made by the Web site designer of how to work with and view the content. RDF metadata on the Semantic Web does not have this limitation: users can gain direct access to information and control over how it is presented. This principle forms the basis for our Semantic Web browser an end user application that automatically locates metadata and assembles point-and-click interfaces from a combination of relevant information, ontological specifications, and presentation knowledge, all described in RDF and retrieved dynamically from the Semantic Web. Because data and services are accessed directly through a standalone client and not through a central point of access (e.g., a portal), new content and services can be consumed as soon as they become available. In this way we take advantage of an important sociological force that encourages the production of new Semantic Web content while remaining faithful to the decentralized nature of the Web.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {255–265},
numpages = {11},
keywords = {bioinformatics, rdf, user interface, semantic web, web services},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988708,
author = {Bechhofer, Sean K. and Carroll, Jeremy J.},
title = {Parsing Owl Dl: Trees or Triples?},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988708},
doi = {10.1145/988672.988708},
abstract = {The Web Ontology Language (OWL) defines three classes of documents: Lite, DL, and Full. All RDF/XML documents are OWL Full documents, some OWL Full documents are also OWL DL documents, and some OWL DL documents are also OWL Lite documents. This paper discusses parsing and species recognition -- that is the process of determining whether a given document falls into the OWL Lite, DL or Full class. Wedescribe two alternative approaches to this task, one based on abstract syntax trees, the other on RDF triples, and compare their key characteristics.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {266–275},
numpages = {10},
keywords = {parsing, owl, semantic web, rdf},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249247,
author = {King, Irwin},
title = {Session Details: Server Performance and Scalability},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249247},
doi = {10.1145/3249247},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988710,
author = {Elnikety, Sameh and Nahum, Erich and Tracey, John and Zwaenepoel, Willy},
title = {A Method for Transparent Admission Control and Request Scheduling in E-Commerce Web Sites},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988710},
doi = {10.1145/988672.988710},
abstract = {This paper presents a method for admission control and request scheduling for multiply-tiered e-commerce Web sites, achieving both stable behavior during overload and improved response times. Our method externally observes execution costs of requests online, distinguishing different request types, and performs overload protection and preferential scheduling using relatively simple measurements and a straight forward control mechanism. Unlike previous proposals, which require extensive changes to the server or operating system, our method requires no modifications to the host O.S., Web server, application server or database. Since our method is external, it can be implemented in a proxy. We present such an implementation, called Gatekeeper, using it with standard software components on the Linux operating system. We evaluate the proxy using the industry standard TPC-W workload generator in a typical three-tiered e-commerce environment. We show consistent performance during overload and throughput increases of up to 10 percent. Response time improves by up to a factor of 14, with only a 15 percent penalty to large jobs.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {276–286},
numpages = {11},
keywords = {admission control, web servers, request scheduling, load control, dynamic web content},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988711,
author = {Xi, Bowei and Liu, Zhen and Raghavachari, Mukund and Xia, Cathy H. and Zhang, Li},
title = {A Smart Hill-Climbing Algorithm for Application Server Configuration},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988711},
doi = {10.1145/988672.988711},
abstract = {The overwhelming success of the Web as a mechanism for facilitating information retrieval and for conducting business transactions has ledto an increase in the deployment of complex enterprise applications. These applications typically run on Web Application Servers, which assume the burden of managing many tasks, such as concurrency, memory management, database access, etc., required by these applications. The performance of an Application Server depends heavily on appropriate configuration. Configuration is a difficult and error-prone task dueto the large number of configuration parameters and complex interactions between them. We formulate the problem of finding an optimal configuration for a given application as a black-box optimization problem. We propose a smart hill-climbing algorithm using ideas of importance sampling and Latin Hypercube Sampling (LHS). The algorithm is efficient in both searching and random sampling. It consists of estimating a local function, and then, hill-climbing in the steepest descent direction. The algorithm also learns from past searches and restarts in a smart and selective fashion using the idea of importance sampling. We have carried out extensive experiments with an on-line brokerage application running in a WebSphere environment. Empirical results demonstrate that our algorithm is more efficient than and superior to traditional heuristic methods.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {287–296},
numpages = {10},
keywords = {simulated annealing, automatic tuning, importance sampling, system configuration, gradient method},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988712,
author = {Li, Wen-Syan and Hsiung, Wang-Pin and Po, Oliver and Hino, Koji and Candan, Kasim Selcuk and Agrawal, Divyakant},
title = {Challenges and Practices in Deploying Web Acceleration Solutions for Distributed Enterprise Systems},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988712},
doi = {10.1145/988672.988712},
abstract = {For most Web-based applications, contents are created dynamically based on the current state of a business, such as product prices and inventory, stored in database systems. These applications demand personalized content and track user behavior while maintaining application integrity. Many of such practices are not compatible with Web acceleration solutions. Consequently, although many web acceleration solutions have shown promising performance improvement and scalability, architecting and engineering distributed enterprise Web applications to utilize available content delivery networks remains a challenge. In this paper, we examine the challenge to accelerate J2EE-based enterprise web applications. We list obstacles and recommend some practices to transform typical database-driven J2EE applications to cache friendly Web applications where Web acceleration solutions can be applied. Furthermore, such transformation should be done without modification to the underlying application business logic and without sacrificing functions that are essential to e-commerce. We take the J2EE reference software, the Java PetStore, as a case study. By using the proposed guideline, we are able to cache more than 90% of the content in the PetStore and scale up the Web site more than 20 times.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {297–308},
numpages = {12},
keywords = {scalability, application server, j2ee, edge server, web acceleration, dynamic content, reliability, fragment},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249248,
author = {Cho, Junghoo},
title = {Session Details: Link Analysis},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249248},
doi = {10.1145/3249248},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988714,
author = {Eiron, Nadav and McCurley, Kevin S. and Tomlin, John A.},
title = {Ranking the Web Frontier},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988714},
doi = {10.1145/988672.988714},
abstract = {The celebrated PageRank algorithm has proved to be a very effective paradigm for ranking results of web search algorithms. In this paper we refine this basic paradigm to take into account several evolving prominent features of the web, and propose several algorithmic innovations. First, we analyze features of the rapidly growing "frontier" of the web, namely the part of the web that crawlers are unable to cover for one reason or another. We analyze the effect of these pages and find it to be significant. We suggest ways to improve the quality of ranking by modeling the growing presence of "link rot" on the web as more sites and pages fall out of maintenance. Finally we suggest new methods of ranking that are motivated by the hierarchical structure of the web, are more efficient than PageRank, and may be more resistant to direct manipulation.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {309–318},
numpages = {10},
keywords = {hypertext, pagerank, ranking},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988715,
author = {Xi, Wensi and Zhang, Benyu and Chen, Zheng and Lu, Yizhou and Yan, Shuicheng and Ma, Wei-Ying and Fox, Edward Allan},
title = {Link Fusion: A Unified Link Analysis Framework for Multi-Type Interrelated Data Objects},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988715},
doi = {10.1145/988672.988715},
abstract = {Web link analysis has proven to be a significant enhancement for quality based web search. Most existing links can be classified into two categories: intra-type links (e.g., web hyperlinks), which represent the relationship of data objects within a homogeneous data type (web pages), and inter-type links (e.g., user browsing log) which represent the relationship of data objects across different data types (users and web pages). Unfortunately, most link analysis research only considers one type of link. In this paper, we propose a unified link analysis framework, called "link fusion", which considers both the inter- and intra- type link structure among multiple-type inter-related data objects and brings order to objects in each data type at the same time. The PageRank and HITS algorithms are shown to be special cases of our unified link analysis framework. Experiments on an instantiation of the framework that makes use of the user data and web pages extracted from a proxy log show that our proposed algorithm could improve the search effectiveness over the HITS and DirectHit algorithms by 24.6% and 38.2% respectively.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {319–327},
numpages = {9},
keywords = {information retrieval, link fusion, link analysis algorithms, data fusion},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988716,
author = {Bar-Yossef, Ziv and Broder, Andrei Z. and Kumar, Ravi and Tomkins, Andrew},
title = {Sic Transit Gloria Telae: Towards an Understanding of the Web's Decay},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988716},
doi = {10.1145/988672.988716},
abstract = {The rapid growth of the web has been noted and tracked extensively. Recent studies have however documented the dual phenomenon: web pages have small half lives, and thus the web exhibits rapid death as well. Consequently, page creators are faced with an increasingly burdensome task of keeping links up-to-date, and many are falling behind. In addition to just individual pages, collections of pages or even entire neighborhoods of the web exhibit significant decay, rendering them less effective as information resources. Such neighborhoods are identified only by frustrated searchers, seeking a way out of these stale neighborhoods, back to more up-to-date sections of the web; measuring the decay of a page purely on the basis of dead links on the page is too naive to reflect this frustration. In this paper we formalize a strong notion of a decay measure and present algorithms for computing it efficiently. We explore this measure by presenting a number of validations, and use it to identify interesting artifacts on today's web. We then describe a number of applications of such a measure to search engines, web page maintainers, ontologists, and individual users.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {328–337},
numpages = {10},
keywords = {web information retrieval, 404 return code, web decay, link analysis, dead links},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249249,
author = {Nieh, Jason},
title = {Session Details: Optimizing Encoding},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249249},
doi = {10.1145/3249249},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988718,
author = {Yin, Xinyi and Lee, Wee Sun},
title = {Using Link Analysis to Improve Layout on Mobile Devices},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988718},
doi = {10.1145/988672.988718},
abstract = {Delivering web pages to mobile phones or personal digital assistants has become possible with the latest wireless technology. However, mobile devices have very small screen sizes and memory capacities. Converting web pages for delivery to a mobile device is an exciting new problem. In this paper, we propose to use a ranking algorithm similar to Google's PageRank algorithm to rank the content objects within a web page. This allows the extraction of only important parts of web pages for delivery to mobile devices. Experiments show that the new method is effective. In experiments on pages from randomly selected websites, the system needed to extract and deliver only 39% of the objects in a web page in order to provide 85% of a viewer's desired viewing content. This provides significant savings in the wireless traffic and downloading time while providing a satisfactory reading experience on the mobile device.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {338–344},
numpages = {7},
keywords = {html, pda (personal digital assistant), www (world wide web), link analysis},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988719,
author = {Bayardo, R. J. and Gruhl, D. and Josifovski, V. and Myllymaki, J.},
title = {An Evaluation of Binary Xml Encoding Optimizations for Fast Stream Based Xml Processing},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988719},
doi = {10.1145/988672.988719},
abstract = {This paper provides an objective evaluation of the performance impacts of binary XML encodings, using a fast stream-based XQuery processor as our representative application. Instead of proposing one binary format and comparing it against standard XML parsers, we investigate the individual effects of several binary encoding techniques that are shared by many proposals. Our goal is to provide a deeper understanding of the performance impacts of binary XML encodings in order to clarify the ongoing and often contentious debate over their merits, particularly in the domain of high performance XML stream processing.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {345–354},
numpages = {10},
keywords = {XPath processing, XML binary formats},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988720,
author = {Spiesser, Jacqueline and Kitchen, Les},
title = {Optimization of Html Automatically Generated by Wysiwyg Programs},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988720},
doi = {10.1145/988672.988720},
abstract = {Automatically generated HTML, as produced by WYSIWYG programs, typically contains much repetitive and unnecessary markup. Thispaper identifies aspects of such HTML that may be altered whileleaving a semantically equivalent document, and proposes techniques to achieve optimizing modifications. These techniques include attribute re-arrangement via dynamic programming, the use of style classes, and dead-coderemoval. These techniques produce documents as small as 33% of original size. The size decreases obtained are still significant when the techniques are used in combination with conventional text-based compression.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {355–364},
numpages = {10},
keywords = {dynamic programming, html optimization, haskell, wysiwyg},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249250,
author = {Sheth, Amit},
title = {Session Details: Semantic Web Applications},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249250},
doi = {10.1145/3249250},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988722,
author = {Miles-Board, Timothy J. and Bailey, Christopher P. and Hall, Wendy and Carr, Leslie A.},
title = {Building a Companion Website in the Semantic Web},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988722},
doi = {10.1145/988672.988722},
abstract = {A problem facing many textbook authors (including one of the authors of this paper) is the inevitable delay between new advances in the subject area and their incorporation in a new (paper) edition of the textbook. This means that some textbooks are quickly considered out of date, particularly in active technological areas such as the Web, even though the ideas presented in the textbook are still valid and important to the community. This paper describes our approach to building a companion website for the textbook Hypermedia and the Web: An Engineering Approach. We use Bloom's taxonomy of educational objectives to critically evaluate a number of authoring and presentation techniques used in existing companion websites, and adapt these techniques to create our own companion website using Semantic Web technologies in order to overcome the identified weaknesses. Finally, we discuss a potential model of future companion websites, in the context of an e-publishing, e-commerce Semantic Web services scenario.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {365–373},
numpages = {9},
keywords = {bloom's taxonomy, semantic web, electronic publishing, companion website, textbook},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988723,
author = {Rocha, Cristiano and Schwabe, Daniel and Aragao, Marcus Poggi},
title = {A Hybrid Approach for Searching in the Semantic Web},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988723},
doi = {10.1145/988672.988723},
abstract = {This paper presents a search architecture that combines classical search techniques with spread activation techniques applied to a semantic model of a given domain. Given an ontology, weights are assigned to links based on certain properties of the ontology, so that they measure the strength of the relation. Spread activation techniques are used to find related concepts in the ontology given an initial set of concepts and corresponding initial activation values. These initial values are obtained from the results of classical search applied to the data associated with the concepts in the ontology. Two test cases were implemented, with very positive results. It was also observed that the proposed hybrid spread activation, combining the symbolic and the sub-symbolic approaches, achieved better results when compared to each of the approaches alone.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {374–383},
numpages = {10},
keywords = {ontologies, semantic search, semantic web, spread activation algorithms, semantic associations, network analysis},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988724,
author = {schraefel, m. c. and Shadbolt, Nigel R. and Gibbins, Nicholas and Harris, Stephen and Glaser, Hugh},
title = {CS AKTive Space: Representing Computer Science in the Semantic Web},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988724},
doi = {10.1145/988672.988724},
abstract = {We present a Semantic Web application that we callCS AKTive Space. The application exploits a wide range of semantically heterogeneousand distributed content relating to Computer Science research in theUK. This content is gathered on a continuous basis using a variety of methods including harvesting and scraping as well as adopting a range models for content acquisition. The content currently comprises aroundten million RDF triples and we have developed storage, retrieval andmaintenance methods to support its management. The content is mediated through an ontology constructed for the application domainand incorporates components from other published ontologies. CS AKTive Spacesupports the exploration of patterns and implications inherent in the content and exploits a variety of visualisations and multi dimensional representations. Knowledge services supported in the applicationinclude investigating communities of practice: who is working, researching or publishing with whom. This work illustrates a number ofsubstantial challenges for the Semantic Web. These include problems of referential integrity, tractable inference and interaction support. Wereview our approaches to these issues and discuss relevant related work.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {384–392},
numpages = {9},
keywords = {semantic web challenge groups, semantic web, ontologies},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249251,
author = {Pennock, David},
title = {Session Details: Reputation Networks},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249251},
doi = {10.1145/3249251},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988726,
author = {Lam, Shyong K. and Riedl, John},
title = {Shilling Recommender Systems for Fun and Profit},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988726},
doi = {10.1145/988672.988726},
abstract = {Recommender systems have emerged in the past several years as an effective way to help people cope with the problem of information overload. One application in which they have become particularly common is in e-commerce, where recommendation of items can often help a customer find what she is interested in and, therefore can help drive sales. Unscrupulous producers in the never-ending quest for market penetration may find it profitable to shill recommender systems by lying to the systems in order to have their products recommended more often than those of their competitors. This paper explores four open questions that may affect the effectiveness of such shilling attacks: which recommender algorithm is being used, whether the application is producing recommendations or predictions, how detectable the attacks are by the operator of the system, and what the properties are of the items being attacked. The questions are explored experimentally on a large data set of movie ratings. Taken together, the results of the paper suggest that new ways must be used to evaluate and detect shilling attacks on recommender systems.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {393–402},
numpages = {10},
keywords = {recommender systems, collaborative filtering, shilling},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988727,
author = {Guha, R. and Kumar, Ravi and Raghavan, Prabhakar and Tomkins, Andrew},
title = {Propagation of Trust and Distrust},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988727},
doi = {10.1145/988672.988727},
abstract = {A (directed) network of people connected by ratings or trust scores, and a model for propagating those trust scores, is a fundamental building block in many of today's most successful e-commerce and recommendation systems. We develop a framework of trust propagation schemes, each of which may be appropriate in certain circumstances, and evaluate the schemes on a large trust network consisting of 800K trust scores expressed among 130K people. We show that a small number of expressed trusts/distrust per individual allows us to predict trust between any two people in the system with high accuracy. Our work appears to be the first to incorporate distrust in a computational trust propagation setting.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {403–412},
numpages = {10},
keywords = {web of trust, distrust, trust propagation},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988728,
author = {Almeida, Rodrigo B. and Almeida, Virgilio A. F.},
title = {A Community-Aware Search Engine},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988728},
doi = {10.1145/988672.988728},
abstract = {Current search technologies work in a "one size fits all" fashion. Therefore, the answer to a query is independent of specific user information need. In this paper we describe a novel ranking technique for personalized search servicesthat combines content-based and community-based evidences. The community-based information is used in order to provide context for queries andis influenced by the current interaction of the user with the service. Ouralgorithm is evaluated using data derived from an actual service available on the Web an online bookstore. We show that the quality of content-based ranking strategies can be improved by the use of communityinformation as another evidential source of relevance. In our experiments the improvements reach up to 48% in terms of average precision.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {413–421},
numpages = {9},
keywords = {searching and ranking, data mining},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249252,
author = {Anderson, Corey},
title = {Session Details: Versioning and Fragmentation},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249252},
doi = {10.1145/3249252},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988730,
author = {Dyreson, Curtis E. and Lin, Hui-ling and Wang, Yingxia},
title = {Managing Versions of Web Documents in a Transaction-Time Web Server},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988730},
doi = {10.1145/988672.988730},
abstract = {This paper presents a transaction-time HTTP server, called TTApache that supports document versioning. A document often consists of a main file formatted in HTML or XML and several included files such as images and stylesheets. A change to any of the files associated with a document creates a new version of that document. To construct a document version history, snapshots of the document's files are obtained over time. Transaction times are associated with each file version to record the version's lifetime. The transaction time is the system time of the edit that created the version. Accounting for transaction time is essential to supporting audit queries that delve into past document versions and differential queries that pinpoint differences between two versions. TTApache performs automatic versioning when a document is read thereby removing the burden of versioning from document authors. Since some versions may be created but never read, TTApache distinguishes between known and assumed versions of a document. TTApache has a simple query language to retrieve desired versions. A browser can request a specific version, or the entire history of a document. Queries can also rewrite links and references to point to current or past versions. Over time, the version history of a document continually grows. To free space, some versions can be vacuumed. Vacuuming a version however changes the semantics of requests for that version. This paper presents several policies for vacuuming versions and strategies for accounting for vacuumed versions in queries.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {422–432},
numpages = {11},
keywords = {observant system, transaction time, versioning},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988731,
author = {Nguyen, Tien Nhut and Munson, Ethan Vincent and Thao, Cheng},
title = {Fine-Grained, Structured Configuration Management for Web Projects},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988731},
doi = {10.1145/988672.988731},
abstract = {Researchers in Web engineering have regularly noted that existing Web application development environments provide little support for managing the evolution of Web applications. Key limitations of Web development environments include line-oriented change models that inadequately represent Web document semantics and in ability to model changes to link structure or the set of objects making up the Webapplication. Developers may find it difficult to grasp how theoverall structure of the Web application has changed over time and may respond by using ad hoc solutions that lead to problems of maintain ability, quality and reliability. Web applications are software artifacts, and as such, can benefit from advanced version control and software configuration management (SCM)technologies from software engineering. We have modified an integrated development environment to manage the evolution and maintenance of Web applications. The resulting environment is distinguished by itsfine-grained version control framework, fine-grained Web contentchange management, and product versioning configuration management, in which a Web project can be organized at the logical level and itsstructure and components are versioned in a fine-grained manner aswell. This paper describes the motivation for this environment as well as its user interfaces, features, and implementation.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {433–442},
numpages = {10},
keywords = {version control, web engineering, software configuration management},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988732,
author = {Ramaswamy, Lakshmish and Iyengar, Arun and Liu, Ling and Douglis, Fred},
title = {Automatic Detection of Fragments in Dynamically Generated Web Pages},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988732},
doi = {10.1145/988672.988732},
abstract = {Dividing web pages into fragments has been shown to provide significant benefits for both content generation and caching. In order for a web site to use fragment-based content generation, however, good methods are needed for dividing web pages into fragments. Manual fragmentation of web pages is expensive, error prone, and unscalable. This paper proposes a novel scheme to automatically detect and flag fragments that are cost-effective cache units in web sites serving dynamic content. We consider the fragments to be interesting if they are shared among multiple documents or they have different lifetime or personalization characteristics. Our approach has three unique features. First, we propose a hierarchical and fragment-aware model of the dynamic web pages and a data structure that is compact and effective for fragment detection. Second, we present an efficient algorithm to detect maximal fragments that are shared among multiple documents. Third, we develop a practical algorithm that effectively detects fragments based on their lifetime and personalization characteristics. We evaluate the proposed scheme through a series of experiments, showing the benefits and costs of the algorithms. We also study the impact of adopting the fragments detected by our system on disk space utilization and network bandwidth consumption.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {443–454},
numpages = {12},
keywords = {fragment-based caching, L-P fragments, dynamic content caching, shared fragments, fragment detection},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249253,
author = {Goble, Carole},
title = {Session Details: Semantic Annotation and Integration},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249253},
doi = {10.1145/3249253},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988734,
author = {Blythe, Jim and Gil, Yolanda},
title = {Incremental Formalization of Document Annotations through Ontology-Based Paraphrasing},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988734},
doi = {10.1145/988672.988734},
abstract = {For the manual semantic markup of documents to become wide-spread, usersmust be able to express annotations that conform to ontologies (orschemas) that have shared meaning. However, a typical user is unlikelyto be familiar with the details of the terms as defined by the ontology authors. In addition, the idea to be expressed may not fit perfectly within a pre-defined ontology. The ideal tool should help users find apartial formalization that closely follows the ontology where possiblebut deviates from the formal representation where needed. We describe animplemented approach to help users create semi-structured semantic annotations for a document according to an extensible OWL ontology. In our approach, users enter a short sentence in free text to describe allor part of a document, and the system presents a set of potential paraphrases of the sentence that are generated from valid expressions inthe ontology, from which the user chooses the closest match. We use a combination of off-the-shelf parsing tools and breadth-first search of expressions in the ontology to help users create valid annotations starting from free text. The user can also define new terms to augmentthe ontology, so the potential matches can improve over time.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {455–461},
numpages = {7},
keywords = {semantic markup, document annotation, knowledge acquisition},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988735,
author = {Cimiano, Philipp and Handschuh, Siegfried and Staab, Steffen},
title = {Towards the Self-Annotating Web},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988735},
doi = {10.1145/988672.988735},
abstract = {The success of the Semantic Web depends on the availability of ontologies as well as on the proliferation of web pages annotated with metadata conforming to these ontologies. Thus, a crucial question is where to acquire these metadata from. In this paper wepropose PANKOW (Pattern-based Annotation through Knowledge on theWeb), a method which employs an unsupervised, pattern-based approach to categorize instances with regard to an ontology. The approach is evaluated against the manual annotations of two human subjects. The approach is implemented in OntoMat, an annotation tool for the Semantic Web and shows very promising results.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {462–471},
numpages = {10},
keywords = {semantic annotation, metadata, semantic web, information extraction},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988736,
author = {Zhang, Dell and Lee, Wee Sun},
title = {Web Taxonomy Integration Using Support Vector Machines},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988736},
doi = {10.1145/988672.988736},
abstract = {We address the problem of integrating objects from a source taxonomy into a master taxonomy. This problem is not only currently pervasive on the web, but also important to the emerging semantic web. A straightforward approach to automating this process would be to train a classifier for each category in the master taxonomy, and then classify objects from the source taxonomy into these categories. In this paper we attempt to use a powerful classification method, Support Vector Machine (SVM), to attack this problem. Our key insight is that the availability of the source taxonomy data could be helpful to build better classifiers in this scenario, therefore it would be beneficial to do transductive learning rather than inductive learning, i.e., learning to optimize classification performance on a particular set of test examples. Noticing that the categorizations of the master and source taxonomies often have some semantic overlap, we propose a method, Cluster Shrinkage (CS), to further enhance the classification by exploiting such implicit knowledge. Our experiments with real-world web data show substantial improvements in the performance of taxonomy integration.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {472–481},
numpages = {10},
keywords = {transductive learning, taxonomy integration, support vector machines, semantic web, ontology mapping, classification},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249254,
author = {Bharat, Krishna},
title = {Session Details: Mining New Media},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249254},
doi = {10.1145/3249254},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988738,
author = {Gabrilovich, Evgeniy and Dumais, Susan and Horvitz, Eric},
title = {Newsjunkie: Providing Personalized Newsfeeds via Analysis of Information Novelty},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988738},
doi = {10.1145/988672.988738},
abstract = {We present a principled methodology for filtering news stories by formal measures of information novelty, and show how the techniques can be usedto custom-tailor news feeds based on information that a user has already reviewed. We review methods for analyzing novelty and then describe Newsjunkie, a system that personalizes news for users by identifying the novelty of stories in the context of stories they have already reviewed. Newsjunkie employs novelty-analysis algorithms that represent articles as words and named entities. The algorithms analyze inter-andintra-document dynamics by considering how information evolves over timefrom article to article, as well as within individual articles. We review the results of a user study undertaken to gauge the value of the approachover legacy time-based review of newsfeeds, and also to compare the performance of alternate distance metrics that are used to estimate the dissimilarity between candidate new articles and sets of previously reviewed articles.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {482–490},
numpages = {9},
keywords = {news, personalization, novelty detection},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988739,
author = {Gruhl, Daniel and Guha, R. and Liben-Nowell, David and Tomkins, Andrew},
title = {Information Diffusion through Blogspace},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988739},
doi = {10.1145/988672.988739},
abstract = {We study the dynamics of information propagation in environments of low-overhead personal publishing, using a large collection of weblogs over time as our example domain. We characterize and model this collection at two levels. First, we present a macroscopic characterization of topic propagation through our corpus, formalizing the notion of long-running "chatter" topics consisting recursively of "spike" topics generated by outside world events, or more rarely, by resonances within the community. Second, we present a microscopic characterization of propagation from individual to individual, drawing on the theory of infectious diseases to model the flow. We propose, validate, and employ an algorithm to induce the underlying propagation network from a sequence of posts, and report on the results.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {491–501},
numpages = {11},
keywords = {information propagation, viral propagation, topic structure, viruses, memes, topic characterization, blogs},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988740,
author = {Reis, D. C. and Golgher, P. B. and Silva, A. S. and Laender, A. F.},
title = {Automatic Web News Extraction Using Tree Edit Distance},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988740},
doi = {10.1145/988672.988740},
abstract = {The Web poses itself as the largest data repository ever available in the history of humankind. Major efforts have been made in order to provide efficient access to relevant information within this huge repository of data. Although several techniques have been developed to the problem of Web data extraction, their use is still not spread, mostly because of the need for high human intervention and the low quality of the extraction results.In this paper, we present a domain-oriented approach to Web data extraction and discuss its application to automatically extracting news from Web sites. Our approach is based on a highly efficient tree structure analysis that produces very effective results. We have tested our approach with several important Brazilian on-line news sites and achieved very precise results, correctly extracting 87.71% of the news in a set of 4088 pages distributed among 35 different sites.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {502–511},
numpages = {10},
keywords = {schema inference, edit distance, web, data extraction},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249255,
author = {Wolman, Alec},
title = {Session Details: Workload Analysis},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249255},
doi = {10.1145/3249255},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988742,
author = {Sen, Subhabrata and Spatscheck, Oliver and Wang, Dongmei},
title = {Accurate, Scalable in-Network Identification of P2p Traffic Using Application Signatures},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988742},
doi = {10.1145/988672.988742},
abstract = {The ability to accurately identify the network traffic associated with different P2P applications is important to a broad range of network operations including application-specific traffic engineering, capacity planning, provisioning, service differentiation,etc. However, traditional traffic to higher-level application mapping techniques such as default server TCP or UDP network-port baseddisambiguation is highly inaccurate for some P2P applications.In this paper, we provide an efficient approach for identifying the P2P application traffic through application level signatures. We firstidentify the application level signatures by examining some available documentations, and packet-level traces. We then utilize the identified signatures to develop online filters that can efficiently and accurately track the P2P traffic even on high-speed network links.We examine the performance of our application-level identification approach using five popular P2P protocols. Our measurements show thatour technique achieves less than 5% false positive and false negative ratios in most cases. We also show that our approach only requires the examination of the very first few packets (less than 10packets) to identify a P2P connection, which makes our approach highly scalable. Our technique can significantly improve the P2P traffic volume estimates over what pure network port based approaches provide. For instance, we were able to identify 3 times as much traffic for the popular Kazaa P2P protocol, compared to the traditional port-based approach.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {512–521},
numpages = {10},
keywords = {application-level signatures, traffic analysis, p2p, online application classification},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988743,
author = {Bent, L. and Rabinovich, M. and Voelker, G. M. and Xiao, Z.},
title = {Characterization of a Large Web Site Population with Implications for Content Delivery},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988743},
doi = {10.1145/988672.988743},
abstract = {This paper presents a systematic study of the properties of a large number of Web sites hosted by a major ISP. To our knowledge, ours is the first comprehensive study of a large server farm that contains thousands of commercial Web sites. We also perform a simulation analysis to estimate potential performance benefits of content delivery networks (CDNs) for these Web sites. We make several interesting observations about the current usage of Web technologies and Web site performance characteristics. First, compared with previous client workload studies, the Web server farm workload contains a much higher degree of uncacheable responses and responses that require mandatory cache validations. A significant reason for this is that cookie use is prevalent among our population, especially among more popular sites. However, we found an indication of wide-spread indiscriminate usage of cookies, which unnecessarily impedes the use of many content delivery optimizations. We also found that most Web sites do not utilize the cache-control features ofthe HTTP 1.1 protocol, resulting in suboptimal performance. Moreover, the implicit expiration time in client caches for responses is constrained by the maximum values allowed in the Squid proxy. Finally, our simulation results indicate that most Web sites benefit from the use of a CDN. The amount of the benefit depends on site popularity, and, somewhat surprisingly, a CDN may increase the peak to average request ratio at the origin server because the CDN can decrease the average request rate more than the peak request rate.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {522–533},
numpages = {12},
keywords = {workload characterization, content distribution, performance, cookie, web caching, measurement, http},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988744,
author = {Costa, Cristiano P. and Cunha, Italo S. and Borges, Alex and Ramos, Claudiney V. and Rocha, Marcus M. and Almeida, Jussara M. and Ribeiro-Neto, Berthier},
title = {Analyzing Client Interactivity in Streaming Media},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988744},
doi = {10.1145/988672.988744},
abstract = {This paper provides an extensive analysis of pre-stored streaming media workloads, focusing on the client interactive behavior. We analyze four workloads that fall into three different domains, namely, education, entertainment video and entertainment audio. Our main goals are: (a) to identify qualitative similarities and differences in the typical client behavior for the three workload classes and (b) to provide data for generating realistic synthetic workloads.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {534–543},
numpages = {10},
keywords = {workload characterization, streaming media},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249256,
author = {Staab, Steffen},
title = {Session Details: Semantic Web Services},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249256},
doi = {10.1145/3249256},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988746,
author = {Solanki, Monika and Cau, Antonio and Zedan, Hussein},
title = {Augmenting Semantic Web Service Descriptions with Compositional Specification},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988746},
doi = {10.1145/988672.988746},
abstract = {Current ontological specifications for semantically describing properties of Web services are limited to their static interface description. Normally for proving properties of service compositions, mapping input/output parameters and specifying the pre/post conditions are found to be sufficient. However these properties are assertions only on the initial and final states of the service respectively. They do not help in specifying/verifying ongoing behaviour of an individual service or a composed system. We propose a framework for enriching semantic service descriptions with two compositional assertions: assumption and commitment that facilitate reasoning about service composition and verification of their integration. The technique is based on Interval Temporal Logic(ITL): a sound formalism for specifying and proving temporal properties of systems. Our approach utilizes the recently proposed Semantic Web Rule Language.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {544–552},
numpages = {9},
keywords = {web services, swrl, owl, interval temporal logics, assumption, commitment, semantic web services, owl-s},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988747,
author = {Patil, Abhijit A. and Oundhakar, Swapna A. and Sheth, Amit P. and Verma, Kunal},
title = {Meteor-s Web Service Annotation Framework},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988747},
doi = {10.1145/988672.988747},
abstract = {The World Wide Web is emerging not only as an infrastructure for data, but also for a broader variety of resources that are increasingly being made available as Web services. Relevant current standards like UDDI, WSDL, and SOAP are in their fledgling years and form the basis of making Web services a workable and broadly adopted technology. However, realizing the fuller scope of the promise of Web services and associated service oriented architecture will requite further technological advances in the areas of service interoperation, service discovery, service composition, and process orchestration. Semantics, especially as supported by the use of ontologies, and related Semantic Web technologies, are likely to provide better qualitative and scalable solutions to these requirements. Just as semantic annotation of data in the Semantic Web is the first critical step to better search, integration and analytics over heterogeneous data, semantic annotation of Web services is an equally critical first step to achieving the above promise. Our approach is to work with existing Web services technologies and combine them with ideas from the Semantic Web to create a better framework for Web service discovery and composition. In this paper we present MWSAF (METEOR-S Web Service Annotation Framework), a framework for semi-automatically marking up Web service descriptions with ontologies. We have developed algorithms to match and annotate WSDL files with relevant ontologies. We use domain ontologies to categorize Web services into domains. An empirical study of our approach is presented to help evaluate its performance.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {553–562},
numpages = {10},
keywords = {web services discovery, wsdl, ontology, semantic annotation of web services, semantic web services},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988748,
author = {Mika, Peter and Oberle, Daniel and Gangemi, Aldo and Sabou, Marta},
title = {Foundations for Service Ontologies: Aligning OWL-S to Dolce},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988748},
doi = {10.1145/988672.988748},
abstract = {Clarity in semantics and a rich formalization of this semantics are important requirements for ontologies designed to be deployed in large-scale, open, distributed systems such as the envisioned Semantic Web This is especially important for the description of Web Services, which should enable complex tasks involving multiple agents. As one of the first initiatives of the Semantic Webcommunity for describing Web Services, OWL-S attracts a lot of interest even though it is still under development. We identify problematic aspects of OWL-S and suggest enhancements through alignment to a foundational ontology. Another contribution of ourwork is the Core Ontology of Services that tries to fill the epistemological gap between the foundational ontology and OWL-S. It can be reused to align other Web Service description languages as well. Finally, we demonstrate the applicability of our work byaligning OWL-S' standard example called CongoBuy.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {563–572},
numpages = {10},
keywords = {daml-s, semantic web, web services, descriptions and situations, owl-s, dolce, core ontology of services},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249257,
author = {Koudas, Nick},
title = {Session Details: Search Engineering 2},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249257},
doi = {10.1145/3249257},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988750,
author = {Perkowitz, Mike and Philipose, Matthai and Fishkin, Kenneth and Patterson, Donald J.},
title = {Mining Models of Human Activities from the Web},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988750},
doi = {10.1145/988672.988750},
abstract = {The ability to determine what day-to-day activity (such as cooking pasta, taking a pill, or watching a video) a person is performing is of interest in many application domains. A system that can do this requires models of the activities of interest, but model construction does not scale well: humans must specify low-level details, such as segmentation and feature selection of sensor data, and high-level structure, such as spatio-temporal relations between states of the model, for each and every activity. As a result, previous practical activity recognition systems have been content to model a tiny fraction of the thousands of human activities that are potentially useful to detect. In this paper, we present an approach to sensing and modeling activities that scales to a much larger class of activities than before. We show how a new class of sensors, based on Radio Frequency Identification (RFID) tags, can directly yield semantic terms that describe the state of the physical world. These sensors allow us to formulate activity models by translating labeled activities, such as 'cooking pasta', into probabilistic collections of object terms, such as 'pot'. Given this view of activity models as text translations, we show how to mine definitions of activities in an unsupervised manner from the web. We have used our technique to mine definitions for over 20,000 activities. We experimentally validate our approach using data gathered from actual human activity as well as simulated data.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {573–582},
numpages = {10},
keywords = {web mining, rfid, activity inference, activity models},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988751,
author = {Amer-Yahia, S. and Botev, C. and Shanmugasundaram, J.},
title = {Texquery: A Full-Text Search Extension to Xquery},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988751},
doi = {10.1145/988672.988751},
abstract = {One of the key benefits of XML is its ability to represent a mix of structured and unstructured (text) data. Although current XML query languages such as XPath and XQuery can express rich queries over structured data, they can only express very rudimentary queries over text data. We thus propose TeXQuery, which is a powerful full-text search extension to XQuery. TeXQuery provides a rich set of fully composable full-text search primitives,such as Boolean connectives, phrase matching, proximity distance, stemming and thesauri. TeXQuery also enables users to seamlessly query over both structured and text data by embedding TeXQuery primitives in XQuery, and vice versa. Finally, TeXQuery supports a flexible scoring construct that can be used toscore query results based on full-text predicates. TeXQuery is the precursor ofthe full-text language extensions to XPath 2.0 and XQuery 1.0 currently being developed by the W3C.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {583–594},
numpages = {12},
keywords = {xquery, full-text search},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988752,
author = {Boldi, P. and Vigna, S.},
title = {The Webgraph Framework I: Compression Techniques},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988752},
doi = {10.1145/988672.988752},
abstract = {Studying web graphs is often difficult due to their large size. Recently,several proposals have been published about various techniques that allow tostore a web graph in memory in a limited space, exploiting the inner redundancies of the web. The WebGraph framework is a suite of codes, algorithms and tools that aims at making it easy to manipulate large web graphs. This papers presents the compression techniques used in WebGraph, which are centred around referentiation and intervalisation (which in turn are dual to each other). WebGraph can compress the WebBase graph (118 Mnodes, 1 Glinks)in as little as 3.08 bits per link, and its transposed version in as littleas 2.89 bits per link.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {595–602},
numpages = {8},
keywords = {web graph, compression},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249258,
author = {Gaedke, Martin},
title = {Session Details: Infastructure for Implementation},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249258},
doi = {10.1145/3249258},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988754,
author = {Onose, Nicola and Simeon, Jerome},
title = {XQuery at Your Web Service},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988754},
doi = {10.1145/988672.988754},
abstract = {XML messaging is at the heart of Web services, providing the flexibility required for their deployment, composition, and maintenance. Yet, current approaches to Web services development hide the messaging layer behind Java or C# APIs, preventing the application to get direct access to the underlying XML information. To address this problem, we advocate the use of a native XML language, namely XQuery, as an integral part of the Web services development infrastructure. The main contribution of the paper is a binding between WSDL, the Web Services Description Language, and XQuery. The approach enables the use of XQuery for both Web services deployment and composition. We present a simple command-line tool that can be used to automatically deploy a Web service from a given XQuery module, and extend the XQuery language itself with a statement for accessing one or more Web services. The binding provides tight-coupling between WSDL and XQuery, yielding additional benefits, notably: the ability to use WSDL as an interface language for XQuery, and the ability to perform static typing on XQuery programs that include Web service calls. Last but not least, the proposal requires only minimal changes to the existing infrastructure. We report on our experience implementing this approach in the Galax XQuery processor.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {603–611},
numpages = {9},
keywords = {web services, interface, XQuery, wsdl, modules, XML},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988755,
author = {Shadgar, Bita and Holyer, Ian},
title = {Adapting Databases and WebDAV Protocol},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988755},
doi = {10.1145/988672.988755},
abstract = {The ability of the Web to share data regardless of geographical location raises a new issue called remote authoring. With the Internet and Web browsers being independent of hardware, it becomes possible to build Web-enabled database applications. Many approaches are provided to integrate databases into the Web environment, which use the Web's protocol i.e. HTTP to transfer the data between clients and servers. However, those methods are affected by the HTTP shortfalls with regard to remote authoring. This paper introduces and discusses a new methodology for remote authoring of databases, which is based on the WebDAV protocol. It is a seamless and effective methodology for accessing and authoring databases, particularly in that it naturally benefits from the WebDAV advantages such as metadata and access control. These features establish a standard way of accessing database metadata, and increase the database security, while speeding up the database connection.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {612–620},
numpages = {9},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988756,
author = {Fu, Xiang and Bultan, Tevfik and Su, Jianwen},
title = {Analysis of Interacting BPEL Web Services},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988756},
doi = {10.1145/988672.988756},
abstract = {This paper presents a set of tools and techniques for analyzing interactions of composite web services which are specified in BPEL and communicate through asynchronous XML messages. We model the interactions of composite web services as conversations, the global sequence of messages exchanged by the web services. As opposed to earlier work, our tool-set handles rich data manipulation via XPath expressions. This allows us to verify designs at a more detailed level and check properties about message content. We present a framework where BPEL specifications of web services are translated to an intermediate representation, followed by the translation of the intermediate representation to a verification language. As an intermediate representation we use guarded automata augmented with unbounded queues for incoming messages, where the guards are expressed as XPath expressions. As the target verification language we use Promela, input language of the model checker SPIN. Since SPIN model checker is a finite-state verification tool we can only achieve partial verification by fixing the sizes of the input queues in the translation. We propose the concept of synchronizability to address this problem. We show that if a composite web service is synchronizable, then its conversation set remains same when asynchronous communication is replaced with synchronous communication. We give a set of sufficient conditions that guarantee synchronizability and that can be checked statically. Based on our synchronizability results, we show that a large class of composite web services with unbounded input queues can be completely verified using a finite state model checker such as SPIN.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {621–630},
numpages = {10},
keywords = {synchronizability, spin, conversation, xpath, web service, model checking, asynchronous communication, BPEL},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249259,
author = {van Harmelen, Frank},
title = {Session Details: Distributed Semantic Query},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249259},
doi = {10.1145/3249259},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988758,
author = {Stuckenschmidt, Heiner and Vdovjak, Richard and Houben, Geert-Jan and Broekstra, Jeen},
title = {Index Structures and Algorithms for Querying Distributed RDF Repositories},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988758},
doi = {10.1145/988672.988758},
abstract = {A technical infrastructure for storing, querying and managing RDFdata is a key element in the current semantic web development. Systems like Jena, Sesame or the ICS-FORTH RDF Suite are widelyused for building semantic web applications. Currently, none ofthese systems supports the integrated querying of distributed RDF repositories. We consider this a major shortcoming since the semanticweb is distributed by nature. In this paper we present an architecture for querying distributed RDF repositories by extending the existing Sesame system. We discuss the implications of our architectureand propose an index structure as well as algorithms forquery processing and optimization in such a distributed context.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {631–639},
numpages = {9},
keywords = {optimization, RDF querying, index structures},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988759,
author = {Tempich, Christoph and Staab, Steffen and Wranik, Adrian},
title = {Remindin': Semantic Query Routing in Peer-to-Peer Networks Based on Social Metaphors},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988759},
doi = {10.1145/988672.988759},
abstract = {In peer-to-peer networks, finding the appropriate answer for an information request, such as the answer to a query for RDF(S) data, depends on selecting the right peer in the network. We hereinvestigate how social metaphors can be exploited effectively andefficiently to solve this task. To this end, we define a method for query routing, REMINDIN', that lets <em>(i)</em> peers observewhich queries are successfully answered by other peers,<em>(ii)</em>, memorizes this observation, and, <em>(iii)</em>,subsequently uses this information in order to select peers to forward requests to.REMINDIN' has been implemented for the SWAP peer-to-peer platformas well as for a simulation environment. We have used the simulation environment in order to investigate how successfulvariations of REMINDIN' are and how they compare to baseline strategies in terms of number of messages forwarded in the networkand statements appropriately retrieved.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {640–649},
numpages = {10},
keywords = {peer selection, peer-to-peer, query routing, ontologies},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988760,
author = {Cai, Min and Frank, Martin},
title = {RDFPeers: A Scalable Distributed RDF Repository Based on a Structured Peer-to-Peer Network},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988760},
doi = {10.1145/988672.988760},
abstract = {Centralized Resource Description Framework (RDF) repositories have limitations both in their failure tolerance and in their scalability. Existing Peer-to-Peer (P2P) RDF repositories either cannot guarantee to find query results, even if these results exist in the network, or require up-front definition of RDF schemas and designation of super peers. We present a scalable distributed RDF repository (RDFPeers) that stores each triple at three places in a multi-attribute addressable network by applying globally known hash functions to its subject predicate and object. Thus all nodes know which node is responsible for storing triple values they are looking for and both exact-match and range queries can be efficiently routed to those nodes. RDFPeers has no single point of failure nor elevated peers and does not require the prior definition of RDF schemas. Queries are guaranteed to find matched triples in the network if the triples exist. In RDFPeers both the number of neighbors per node and the number of routing hops for inserting RDF triples and for resolving most queries are logarithmic to the number of nodes in the network. We further performed experiments that show that the triple-storing load in RDFPeers differs by less than an order of magnitude between the most and the least loaded nodes for real-world RDF data.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {650–657},
numpages = {8},
keywords = {semantic web, distributed RDF repositories, peer-to-peer},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249260,
author = {Broder, Andrei},
title = {Session Details: Query Result Processing},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249260},
doi = {10.1145/3249260},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988762,
author = {Kummamuru, Krishna and Lotlikar, Rohit and Roy, Shourya and Singal, Karan and Krishnapuram, Raghu},
title = {A Hierarchical Monothetic Document Clustering Algorithm for Summarization and Browsing Search Results},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988762},
doi = {10.1145/988672.988762},
abstract = {Organizing Web search results into a hierarchy of topics and sub-topics facilitates browsing the collection and locating results of interest. In this paper, we propose a new hierarchical monothetic clustering algorithm to build a topic hierarchy for a collection of search results retrieved in response to a query. At every level of the hierarchy, the new algorithm progressively identifies topics in a way that maximizes the coverage while maintaining distinctiveness of the topics. We refer the proposed algorithm to as DisCover. Evaluating the quality of a topic hierarchy is a non-trivial task, the ultimate test being user judgment. We use several objective measures such as coverage and reach time for an empirical comparison of the proposed algorithm with two other monothetic clustering algorithms to demonstrate its superiority. Even though our algorithm is slightly more computationally intensive than one of the algorithms, it generates better hierarchies. Our user studies also show that the proposed algorithm is superior to the other algorithms as a summarizing and browsing tool.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {658–665},
numpages = {8},
keywords = {search, automatic taxonomy generation, summarization, clustering, data mining},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988763,
author = {Kraft, Reiner and Zien, Jason},
title = {Mining Anchor Text for Query Refinement},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988763},
doi = {10.1145/988672.988763},
abstract = {When searching large hypertext document collections, it is often possible that there are too many results available for ambiguous queries. Query refinement is an interactive process of query modification that can be used to narrow down the scope of search results. We propose a new method for automatically generating refinements or related terms to queries by mining anchor text for a large hypertext document collection. We show that the usage of anchor text as a basis for query refinement produces high quality refinement suggestions that are significantly better in terms of perceived usefulness compared to refinements that are derived using the document content. Furthermore, our study suggests that anchor text refinements can also be used to augment traditional query refinement algorithms based on query logs, since they typically differ in coverage and produce different refinements. Our results are based on experiments on an anchor text collection of a large corporate intranet.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {666–674},
numpages = {9},
keywords = {anchor text, web search, query refinement, rank},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988764,
author = {Sugiyama, Kazunari and Hatano, Kenji and Yoshikawa, Masatoshi},
title = {Adaptive Web Search Based on User Profile Constructed without Any Effort from Users},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988764},
doi = {10.1145/988672.988764},
abstract = {Web search engines help users find useful information on the World Wide Web (WWW). However, when the same query is submitted by different users, typical search engines return the same result regardless of who submitted the query. Generally, each user has different information needs for his/her query. Therefore, the search result should be adapted to users with different information needs. In this paper, we first propose several approaches to adapting search results according to each user's need for relevant information without any user effort, and then verify the effectiveness of our proposed approaches. Experimental results show that search systems that adapt to each user's preferences can be achieved by constructing user profiles based on modified collaborative filtering with detailed analysis of user's browsing history in one day.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {675–684},
numpages = {10},
keywords = {WWW, user modeling, information retrieval},
location = {New York, NY, USA},
series = {WWW '04}
}

@dataset{10.1145/review-988672.988764_R37670,
author = {Kraft, Donald Harris},
title = {Review ID:R37670 for DOI: 10.1145/988672.988764},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-988672.988764_R37670}
}

@inproceedings{10.1145/3249261,
author = {Schwabe, Daniel},
title = {Session Details: Web Site Analysis and Customization},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249261},
doi = {10.1145/3249261},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988766,
author = {Despeyroux, Thierry},
title = {Practical Semantic Analysis of Web Sites and Documents},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988766},
doi = {10.1145/988672.988766},
abstract = {As Web sites are now ordinary products, it is necessary to explicit the notion of quality of a Web site. The quality of a site may belinked to the easiness of accessibility and also to other criteria such as the fact that the site is up to date and coherent. This last quality is difficult to insure because sites may be updated very frequently, may have many authors, may be partially generated and inthis context proof-reading is very difficult. The same piece of information may be found in different occurrences, but also in data ormeta-data, leading to the need for consistency checking. In this paper we make a parallel between programs and Web sites. We present some examples of semantic constraints that one would like to specify (constraints between the meaning of categories and sub-categories in a thematic directory, consistency between the organization chart and the rest of the site in an academic site). We present quickly the Natural Semantics a way to specify the semantics of programming languages that inspires ourworks. Natural Semantics itself comes from both an operational semantics and from logic programming and its implementation uses Prolog. Then we propose a specification language for semantic constraints in Web sites that, in conjunction with the well known "make" program, permits to generate some site verification tools by compiling the specification into Prolog code. We apply our method to alarge XML document which is the scientific part of our instituteactivity report, tracking errors or inconsistencies and alsoconstructing some indicators that can be used by the management of theinstitute.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {685–693},
numpages = {9},
keywords = {formal semantics, web engineering, content management, quality, logic programming, web sites, knowledge management, information system, web site evolution, consistency, XML},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988767,
author = {Hung, Eugene and Pasquale, Joseph},
title = {Web Customization Using Behavior-Based Remote Executing Agents},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988767},
doi = {10.1145/988672.988767},
abstract = {ReAgents are remotely executing agents that customize Web browsing for non-standard clients. A reAgent is essentially a one-shot" mobile agent that acts as an extension of a client dynamically launched by the client to run on its behalf at a remote more advantageous location. ReAgents simplify the use of mobile agent technology by transparently handling data migration and run-time network communications and provide a general interface for programmers to more easily implement their application-specific customizing logic. This is made possible by the identification of useful remote behaviors i.e. common patterns of actions that exploit the ability to process and communicate remotely. Examples of such behaviors are transformers monitors cachers and collators. In this paper we identify a set ofuseful reAgent behaviors for interacting with Web services via astandard browser describe how to program and use reAgents and show that the overhead of using reAgents is low and outweighed by its benefits.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {694–703},
numpages = {10},
keywords = {dynamic deployment, web customization, remote agents},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/3249262,
author = {Carroll, Jeremy},
title = {Session Details: Semantic Web Foundations},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249262},
doi = {10.1145/3249262},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
numpages = {1},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988769,
author = {Grau, Bernardo Cuenca},
title = {A Possible Simplification of the Semantic Web Architecture},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988769},
doi = {10.1145/988672.988769},
abstract = {In the semantic Web architecture, Web ontology languages arebuilt on top of RDF(S). However, serious difficulties have arisen when trying to layer expressive ontology languages, like OWL, on top of RDF-Schema. Although these problems can be avoided, OWL (andthe whole semantic Web architecture) becomes much more complex than it should be. In this paper, a possible simplification of thesemantic Web architecture is suggested, which has several import antadvantages with respect to the layering currently accepted by the W3C Ontology Working Group.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {704–713},
numpages = {10},
keywords = {ontology web language (OWL), resource description framework schema (RDF-schema), description logics, semantic web, resource description framework (RDF)},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988770,
author = {Dong, J. S. and Lee, C. H. and Lee, H. B. and Li, Y. F. and Wang, H.},
title = {A Combined Approach to Checking Web Ontologies},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988770},
doi = {10.1145/988672.988770},
abstract = {The understanding of Semantic Web documents is built upon ontologies that define concepts and relationships of data. Hence, the correctness of ontologies is vital. Ontology reasoners such as RACER and FaCT have been developed to reason ontologies with a high degree of automation. However, complex ontology-related properties may not be expressible within the current web ontology languages, consequently they may not be checkable by RACER and FaCT. We propose to use the software engineering techniques and tools, i.e., Z/EVES and Alloy Analyzer, to complement the ontology tools for checking Semantic Web documents.In this approach, Z/EVES is first applied to remove trivial syntax and type errors of the ontologies. Next, RACER is used to identify any ontological inconsistencies, whose origins can be traced by Alloy Analyzer. Finally Z/EVES is used again to express complex ontology-related properties and reveal errors beyond the modeling capabilities of the current web ontology languages. We have successfully applied this approach to checking a set of military plan ontologies.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {714–722},
numpages = {9},
keywords = {semantic web, daml+oil, racer, alloy, ontologies, z},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/988672.988771,
author = {Horrocks, Ian and Patel-Schneider, Peter F.},
title = {A Proposal for an Owl Rules Language},
year = {2004},
isbn = {158113844X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/988672.988771},
doi = {10.1145/988672.988771},
abstract = {Although the OWLWeb Ontology Language adds considerable expressive power to the Semantic Web it does have expressive limitations, particularly with respect to what can be said about properties. Wepresent ORL (OWL Rules Language), a Horn clause rules extension to OWL that overcomes many of these limitations. ORL extends OWL in a syntactically and semantically coherent manner: the basic syntax for ORL rules is an extension of the abstract syntax for OWL DL and OWLLite; ORL rules are given formal meaning via an extension of the OWLDL model-theoretic semantics; ORL rules are given an XML syntax basedon the OWL XML presentation syntax; and a mapping from ORL rules to RDF graphs is given based on the OWL RDF/XML exchange syntax. Wediscuss the expressive power of ORL, showing that the ontology consistency problem is undecidable, provide several examples of ORLusage, and discuss how reasoning support for ORL might be provided.},
booktitle = {Proceedings of the 13th International Conference on World Wide Web},
pages = {723–731},
numpages = {9},
keywords = {model-theoretic semantics, semantic web, representation},
location = {New York, NY, USA},
series = {WWW '04}
}

@inproceedings{10.1145/1013367.1013369,
author = {Tane, Julien and Schmitz, Christoph and Stumme, Gerd},
title = {Semantic Resource Management for the Web: An e-Learning Application},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013369},
doi = {10.1145/1013367.1013369},
abstract = {Topics in education are changing with an ever faster pace. ELearning resources tend to be more and more decentralized. Users increasingly need to be able to use the resources of the web. For this, they should have tools for finding and organizing information in a decentralized way. In this paper, we show how an ontologybased tool suite allows to make the most of the resources available on the web.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {1–10},
numpages = {10},
keywords = {semantic web, e-learning, knowledge management},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/3244262,
author = {Wade, Vincent},
title = {Session Details: Sharing Educational Resources},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244262},
doi = {10.1145/3244262},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
numpages = {1},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013370,
author = {Quemada, Juan and Huecas, Gabriel and de-Miguel, Tom\"{y}s and Salvach\`{u}a, Joaqu\'{\i}n and Fernandez, Blanca and Simon, Bernd and Maillet, Katherine and Lai-Cong, Efiie},
title = {Educanext: A Framework for Sharing Live Educational Resources with Isabel},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013370},
doi = {10.1145/1013367.1013370},
abstract = {EducaNext is an educational mediator created within the UNIVERSAL IST Project which supports both, the exchange of reusable educational materials based on open standards, as well as the collaboration of educators over the network in the realization of educational activities. The Isabel CSCW application is a group collaboration tool for the Internet supporting audience interconnection over the network, such as distributed classrooms, conferences or meetings. This paper describes the conclusions and feedback obtained from the integration of Isabel into EducaNext, it's use for the realization of collaborative educational activities involving distributed classrooms, lectures or workshops, as well as the general conclusions obtained about the integration of synchronous collaboration applications into educational mediators.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {11–18},
numpages = {8},
keywords = {educational activity, educational mediators, Isabel application, IEEE standard, videoconferencing, LOM, educaNext, learning resource, live collaboration over the internet},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013371,
author = {Hatala, Marek and Richards, Griff and Eap, Timmy and Willms, Jordan},
title = {The Interoperability of Learning Object Repositories and Services: Standards, Implementations and Lessons Learned},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013371},
doi = {10.1145/1013367.1013371},
abstract = {Interoperability is one of the main issues in creating a networked system of repositories. The eduSource project in its holisticapproach to building a network of learning object repositories in Canada is implementing an open network for learning services. Itsopenness is supported by a communication protocol called theeduSource Communications Layer (ECL) which closely implements the IMS Digital Repository Interoperability (DRI)specification and architecture. The ECL in conjunction withconnection middleware enables any service providers to join thenetwork. EduSource is open to external initiatives as it explicitlysupports an extensible bridging mechanism between eduSource and other major initiatives. This paper discusses interoperability in general and then focuses on the design of ECL as animplementation of IMS DRI with supporting infrastructure andmiddleware. The eduSource implementation is in the mature stateof its development as being deployed in different settings withdifferent partners. Two applications used in evaluating ourapproach are described: a gateway for connecting betweeneduSource and the NSDL initiative, and a federated searchconnecting eduSource, EdNA and SMETE.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {19–27},
numpages = {9},
keywords = {learning object repositories, interoperability},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/3244263,
author = {Neville, Liddy},
title = {Session Details: Web of Communities},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244263},
doi = {10.1145/3244263},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
numpages = {1},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013373,
author = {Bar-Ilan, J.},
title = {An Outsider's View on "Topic-Oriented Blogging"},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013373},
doi = {10.1145/1013367.1013373},
abstract = {The number of Web blogs is growing extremely fast, thus this phenomenon cannot be ignored. This paper discusses the issue through monitoring a set of blogs for a two months period in September-October 2003 and characterizing these blogs based on descriptive statistics and content analysis.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {28–34},
numpages = {7},
keywords = {blogs, bloggers},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013374,
author = {Martin, Kathi C.},
title = {The Role of Standards in Creating Community},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013374},
doi = {10.1145/1013367.1013374},
abstract = {Participation in the web of communities requires a common language, a common technological structure and development of content that is relevant and captivating. This paper reports on a project that both conserves a rich regional cultural heritage and has structured the content developed during this conservation to be fluidly shared with both the domain and the broader communities. It also examines the varied degrees of acceptance within these communities.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {35–40},
numpages = {6},
keywords = {ontology, museums online archive California, historic costume collection, XML, Dublin core, thesaurus, semantic web, open archive initiative},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013375,
author = {Shamma, David A. and Owsley, Sara and Hammond, Kristian J. and Bradshaw, Shannon and Budzik, Jay},
title = {Network Arts: Exposing Cultural Reality},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013375},
doi = {10.1145/1013367.1013375},
abstract = {In this article, we explore a new role for the computer in art as a reflector of popular culture. Moving away from the static audio-visual installations of other artistic endeavors and from the traditional role of the machine as a computational tool, we fuse art and the Internet to expose cultural connections people draw implicitly but rarely consider directly. We describe several art installations that use the World Wide Web as a reflection of cultural reality to highlight and explore the relations between ideas that compose the fabric of our every day lives.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {41–47},
numpages = {7},
keywords = {network arts, media arts, software agents, world wide web, information retrieval, culture},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/3244264,
author = {Curbera, Francisco},
title = {Session Details: Quality of Service},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244264},
doi = {10.1145/3244264},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
numpages = {1},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013377,
author = {Marchetti, Carlo and Pernici, Barbara and Plebani, Pierluigi},
title = {A Quality Model for Multichannel Adaptive Information},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013377},
doi = {10.1145/1013367.1013377},
abstract = {The ongoing diffusion of novel and mobile devices offers new ways to provide services across a growing set of network technologies. As a consequence, traditional information systems evolve to multichannel systems in which services are provided through different channels, being a channel the abstraction of a device and a network. This work proposes a quality model suitable for capturing and reasoning about quality aspects of multichannel information systems. In particular, the model enables a clear separation of modeling aspects of services, networks, and devices. Further, it embeds rules enabling the evaluation of end-to-end quality, which can be used to select services according to the actual quality perceived by users.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {48–54},
numpages = {7},
keywords = {quality of service, model},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013378,
author = {Keidl, Markus and Kemper, Alfons},
title = {Towards Context-Aware Adaptable Web Services},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013378},
doi = {10.1145/1013367.1013378},
abstract = {In this paper, we present a context framework that facilitates the development and deployment of context-aware adaptable Web services. Web services are provided with context information about clients that may be utilized to provide a personalized behavior. Context is extensible with new types of information at any time without any changes to the underlying infrastructure. Context processing is done by Web services, context plugins, or context services. Context plugins and context services pre- and post-process Web service messages based on the available contextinformation. Both are essential for automatic context processing and automatic adaption of Web services to new context types without the necessity to adjust the Web services themselves. We implemented the context framework within the ServiceGlobe system, our open and distributed Web service platform.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {55–65},
numpages = {11},
keywords = {extensible framework, context, web services, information services, service platform, extensibility, automatic context processing},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013379,
author = {Liu, Yutu and Ngu, Anne H. and Zeng, Liang Z.},
title = {QoS Computation and Policing in Dynamic Web Service Selection},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013379},
doi = {10.1145/1013367.1013379},
abstract = {The emerging Service-Oriented Computing (SOC) paradigm promises to enable businesses and organizations to collaborate in an unprecedented way by means of standard web services. To support rapid and dynamic composition of services in this paradigm, web services that meet requesters' functional requirements must be able to be located and bounded dynamically from a large and constantly changing number of service providers based on their Quality of Service (QoS). In order to enable quality-driven web service selection, we need an open, fair, dynamic and secure framework to evaluate the QoS of a vast number of web services. The fair computation and enforcing of QoS of web services should have minimal overhead but yet able to achieve sufficient trust by both service requesters and providers. In this paper, we presented our open, fair and dynamic QoS computation model for web services selection through implementation of and experimentation with a QoS registry in a hypothetical phone service provisioning market place application.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {66–73},
numpages = {8},
keywords = {ranking of QoS, QoS, web services, extensible QoS model},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/3244265,
author = {Stata, Raymie},
title = {Session Details: Industrial Practice I},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244265},
doi = {10.1145/3244265},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
numpages = {1},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013381,
author = {Carroll, Jeremy J. and Dickinson, Ian and Dollin, Chris and Reynolds, Dave and Seaborne, Andy and Wilkinson, Kevin},
title = {Jena: Implementing the Semantic Web Recommendations},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013381},
doi = {10.1145/1013367.1013381},
abstract = {The new Semantic Web recommendations for RDF, RDFS and OWL have, at their heart, the RDF graph. Jena2, a second-generation RDF toolkit, is similarly centered on the RDF graph. RDFS and OWL reasoning are seen as graph-to-graph transforms, producing graphs of virtual triples. Rich APIs are provided. The Model API includes support for other aspects of the RDF recommendations, such as containers and reification. The Ontology API includes support for RDFS and OWL, including advanced OWL Full support. Jena includes the de facto reference RDF/XML parser, and provides RDF/XML output using the full range of the rich RDF/XML grammar. N3 I/O is supported. RDF graphs can be stored in-memory or in databases. Jena's query language, RDQL, and the Web API are both offered for the next round of standardization.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {74–83},
numpages = {10},
keywords = {RDF, RDQL, semantic web, OWL, Jena},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013382,
author = {Katikaneni, Udaykiran and Ladner, Roy and Petry, Frederick},
title = {Internet Delivery of Meteorological and Oceanographic Data in Wide Area Naval Usage Environments},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013382},
doi = {10.1145/1013367.1013382},
abstract = {Access and retrieval of meteorological and oceanographic data from heterogeneous sources in a distributed system presents many issues. Effective bandwidth utilization is important for any distributed system. In addition, specific issues need to be addressed in order to assimilate spatio-temporal data from multiple sources. These issues include resolution of differences in datum, map-projection and time coordinate. Reduction in the complexity of data formats is a significant factor for fostering interoperability. Simplification of training is important to promote usage of the distributed system. We describe techniques that revolutionize Web-based delivery of meteorological and oceanographic data to address needs of the Naval/Marine user.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {84–88},
numpages = {5},
keywords = {resumable object streams, meteorological and oceanographic data},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013383,
author = {Gong, Leiguang},
title = {Can Web-Based Recommendation Systems Afford Deep Models: A Context-Based Approach for Efficient Model-Based Reasoning},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013383},
doi = {10.1145/1013367.1013383},
abstract = {Web-based product and service recommendation systems have become ever popular on-line business practice with increasing emphasis on modeling customer needs and providing them with targeted or personalized service solutions in real-time interaction. Almost all the commercial web service systems adopt some kind of simple customer segmentation models and shallow pattern matching or rule-based techniques for high performance. The models built based on these techniques though very efficient have a fundamental limitation in their ability to capture and explain the reasoning in the process of determining and selecting appropriate services or products. However, using deep models (e.g. semantic networks), though desirable for their expressive power, may require significantly more computational resources (e.g. time) for reasoning. This can compromise the system performance. This paper reports on a new approach that represents and uses contextual information in semantic net-based models to constrain and prune potentially very large search space, which results in more efficient reasoning and much improved performance in terms of speed and selectivity as evidenced by the evaluation results.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {89–93},
numpages = {5},
keywords = {recommendation systems, context, reasoning, semantic network, model},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/3244266,
author = {Nejdl, Wolfgang},
title = {Session Details: Adaptive e-Learning Systems},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244266},
doi = {10.1145/3244266},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
numpages = {1},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013385,
author = {Nodenot, Thierry and Marquesuza\'{a}, Christophe and Laforcade, Pierre and Sallaberry, Christian},
title = {Model Based Engineering of Learning Situations for Adaptive Web Based Educational Systems},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013385},
doi = {10.1145/1013367.1013385},
abstract = {In this paper, we propose an approach for the engineering of web based educational applications. The applications that we focus require advanced functionality for regulating and tutoring learners' activities (dynamics of learning). Our approach aims at proposing models, not only to describe details of such learning situations, but also to characterize the constraints that the Learning Management System exploiting such situations must satisfy in this sense, this approach also contributes to the specification of the Adaptive Web Based Educational System (AWBES) fitted to a particular learning situation. Moreover, this approach for the engineering of learning situations conforms to current software engineering research works.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {94–103},
numpages = {10},
keywords = {UML language, architectures and designs for web-based learning delivery environments, models and metamodels, specification of educational applications},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013386,
author = {Brusilovsky, Peter},
title = {KnowledgeTree: A Distributed Architecture for Adaptive e-Learning},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013386},
doi = {10.1145/1013367.1013386},
abstract = {This paper presents KnowledgeTree, an architecture for adaptive E-Learning based on distributed reusable intelligent learning activities. The goal of KnowledgeTree is to bridge the gap between the currently popular approach to Web-based education, which is centered on learning management systems vs. the powerful but underused technologies in intelligent tutoring and adaptive hypermedia. This integrative architecture attempts to address both the component-based assembly of adaptive systems and teacher-level reusability.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {104–113},
numpages = {10},
keywords = {student model server, adaptive content service, e-learning, learning portal, learning object metadata, content re-use, adaptive web},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013387,
author = {Stash, Natalia Victorovna and Cristea, Alexandra Ioana and De Bra, Paul M.},
title = {Authoring of Learning Styles in Adaptive Hypermedia: Problems and Solutions},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013387},
doi = {10.1145/1013367.1013387},
abstract = {Learning styles, as well as the best ways of responding with corresponding instructional strategies, have been intensively studied in the classical educational (classroom) setting. There is much less research of application of learning styles in the new educational space, created by the Web. Moreover, authoring applications are scarce, and they do not provide explicit choices and creation of instructional strategies for specific learning styles. The main objective of the research described in this paper is to provide the authors with a tool which will allow them to incorporate different learning styles in their adaptive educational hypermedia applications. In this way, we are creating a semantically significant interface between classical learning styles and instructional strategies and the modern field of adaptive educational hypermedia.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {114–123},
numpages = {10},
keywords = {adaptive hypermedia, user modeling, authoring of adaptive hypermedia, learning styles},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/3244267,
author = {Dumais, Susan},
title = {Session Details: Business Processes and Conversations},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244267},
doi = {10.1145/3244267},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
numpages = {1},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013389,
author = {Ardissono, Liliana and Cardinio, Davide and Petrone, Giovanna and Segnan, Marino},
title = {A Framework for the Server-Side Management of Conversations with Web Services},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013389},
doi = {10.1145/1013367.1013389},
abstract = {The emerging standards for the publication of Web Services are focused on the specification of the static interfaces of the operations to be invoked, or on the service composition. Few efforts have been made to specify the interaction between a Web Service and the individual consumer, although this aspect is essential to the successful service execution.In fact, while "one-shot" services may be invoked in a straight forward way, the invocation of services requiring complex interactions, where multiple messages are needed to complete the service, depends on the fact that the consumer respects the business logic of the Web Service.In this paper, we propose a framework for the server-side management of the interaction between a Web Service and its consumers. In our approach, the Web Service is in charge of assisting the consumer during the service invocation, by managing the interaction context and instructing the consumer about the operations that can be invoked and their actual parameters, at each step of the conversation. Our framework is based on the exchange of SOAP messages specifying the invocation of Java-based operations. Moreover, in order to support the interoperability with other software environments, the conversation flow specification is exported to a WSDL format that enables heterogeneous consumers to invoke the Web Service in a seamless way.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {124–133},
numpages = {10},
keywords = {tools and technologies for web services development, service oriented architectures},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013390,
author = {Chafle, Girish B. and Chandra, Sunil and Mann, Vijay and Nanda, Mangala Gowri},
title = {Decentralized Orchestration of Composite Web Services},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013390},
doi = {10.1145/1013367.1013390},
abstract = {Web services make information and software available programmatically via the Internet and may be used as building blocks for applications. A composite web service is one that is built using multiple component web services and is typically specified using a language such as BPEL4WS or WSIPL. Once its specification has been developed, the composite service may be orchestrated either in a centralized or in a decentralized fashion. Decentralized orchestration offers performance improvements in terms of increased throughput and scalability and lower response time. However, decentralized orchestration also brings additional complexity to the system in terms of error recovery and fault handling. Further, incorrect design of a decentralized system can lead to potential deadlock or non-optimal usage of system resources. This paper investigates build time and runtime issues related to decentralized orchestration of composite web services. We support our design decisions with performance results obtained on a decentralized setup using BPEL4WS to describe the composite web services and BPWS4J as the underlying runtime environment to orchestrate them.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {134–143},
numpages = {10},
keywords = {decentralized orchestration, BPEL4WS, composite web services, code partitioning},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013391,
author = {Davulcu, Hasan and Kifer, Michael and Ramakrishnan, I. V.},
title = {CTR-S: A Logic for Specifying Contracts in Semantic Web Services},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013391},
doi = {10.1145/1013367.1013391},
abstract = {A requirements analysis in the emerging field of Semantic Web Services (SWS) (see http://daml.org/services/swsl/requirements/) has identified four major areas of research: intelligent service discovery, automated contracting of services, process modeling, and service enactment. This paper deals with the intersection of two of these areas: process modeling as it pertains to automated contracting. Specifically, we propose a logic, called CTR-S,which captures the dynamic aspects of contracting for services.Since CTR-S is an extension of the classical first-order logic, it is well-suited to model the static aspects of contracting as well. A distinctive feature of contracting is that it involves two or more parties in a potentially adversarial situation. CTR-S is designed to model this adversarial situation through its novel model theory, which incorporates certain game-theoretic concepts. In addition to the model theory, we develop a proof theory for CTR-S and demonstrate the use of the logic formodeling and reasoning about Web service contracts.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {144–153},
numpages = {10},
keywords = {web services, services composition, contracts},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/3244269,
author = {Brusilovsky, Peter},
title = {Session Details: Student Tracking and Personalization},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244269},
doi = {10.1145/3244269},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
numpages = {1},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013393,
author = {Mazza, Riccardo and Dimitrova, Vania},
title = {Visualising Student Tracking Data to Support Instructors in Web-Based Distance Education},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013393},
doi = {10.1145/1013367.1013393},
abstract = {This paper presents a novel approach of using web log data generated by course management systems (CMS) to help instructors become aware of what is happening in distance learning classes. Specifically, techniques from Information Visualization are used to graphically render complex, multidimensional student tracking data collected by CMS. A system, called CourseVis, illustrates the proposed approach. Graphical representations from the use of CourseVis to visualise data from a java on-line distance course ran with WebCT are presented. Findings from the evaluation of CourseVis are presented, and it is argued that CourseVis can help teachers become aware of some social, behavioural, and cognitive aspects related to distance learners. Using graphical representations of student tracking data, instructors can identify tendencies in their classes, or quickly discover individuals that need special attention.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {154–161},
numpages = {8},
keywords = {student tracking, information visualization, Web-based distance education},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013394,
author = {Farrell, Robert G. and Liburd, Soyini D. and Thomas, John C.},
title = {Dynamic Assembly of Learning Objects},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013394},
doi = {10.1145/1013367.1013394},
abstract = {This paper describes one solution to the problem of how to select sequence, and link Web resources into a coherent, focused organization for instruction that addresses a user's immediate and focused learning need. A system is described that automatically generates individualized learning paths from a repository of XML Web resources. Each Web resource has an XML Learning Object Metadata (LOM) description consisting of General, Educational, and Classification metadata. Dynamic assembly of these learning objects is based on the relative match of the learning object content and metadata to the learner's needs, preferences, context, and constraints. Learning objects are connected into coherent paths based on their LOM topic classifications and the proximity of these topics in a Resource Description Framework (RDF) graph. An instructional sequencing policy specifies how to arrange the objects on the path into a particular learning sequence. The system has been deployed and evaluated within a corporate setting.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {162–169},
numpages = {8},
keywords = {information retrieval, assembly, LOM, organization, learning object, data retrieval, instruction, linking, metadata, content management, RDF, semantic web},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013395,
author = {Dolog, Peter and Henze, Nicola and Nejdl, Wolfgang and Sintek, Michael},
title = {Personalization in Distributed E-Learning Environments},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013395},
doi = {10.1145/1013367.1013395},
abstract = {Personalized support for learners becomes even more important, when e-Learning takes place in open and dynamic learning and information networks. This paper shows how to realize personalized learning support in distributed learning environments based on Semantic Web technologies. Our approach fills the existing gap between current adaptive educational systems with well-established personalization functionality, and open, dynamic learning repository networks. We propose a service-based architecture for establishing personalized e-Learning, where personalization functionality is provided by various web-services. A Personal Learning Assistant integrates personalization services and other supporting services, and provides the personalized access to learning resources in an e-Learning network.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {170–179},
numpages = {10},
keywords = {web services, adaptation, learning repositories, ontologies, standards, P2P, personalization},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/3244270,
author = {Manase, Mark},
title = {Session Details: Industrial Practice 2},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244270},
doi = {10.1145/3244270},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
numpages = {1},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013397,
author = {Davis, A. and Parikh, J. and Weihl, W. E.},
title = {Edgecomputing: Extending Enterprise Applications to the Edge of the Internet},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013397},
doi = {10.1145/1013367.1013397},
abstract = {Content delivery networks have evolved beyond traditional distributed caching. With services such as Akamai's EdgeComputing it is now possible to deploy and run enterprise business Web applications on a globally distributed computing platform, to provide subsecond response time to end users anywhere in the world. Additionally, this distributed application platform provides high levels of fault-tolerance and scalability on-demand to meet virtually any need. Application resources can be provisioned dynamically in seconds to respond automatically to changes in load on a given application.In some cases, an application can be deployed completely on the global platform without any central enterprise infrastructure. Other applications can require centralizing core business logic and transactional databases at the enterprise data center while the presentation layer and some business logic and database functionality move onto the edge platform.Implementing a distributed application service on the Internet's edge requires overcoming numerous challenges, including sandboxing for security, distributed load-balancing and resource management, accounting and billing, deployment, testing, debugging, and monitoring. Our current implementation of Akamai EdgeComputing supports application programming platforms such as Java 2 Enterprise Edition (J2EE) and Microsoft's .NET Framework, in large part because they make it easier to address some of these challenges. In the near future we will also support environments for other application languages such as C, PHP, and Perl.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {180–187},
numpages = {8},
keywords = {Web services, utility computing, grid computing, web applications, split-tier applications, Internet applications, N-tier applications, distributed applications, edge computing},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013398,
author = {Damodaran, Suresh},
title = {B2B Integration over the Internet with XML: RosettaNet Successes and Challenges},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013398},
doi = {10.1145/1013367.1013398},
abstract = {The practical experience of RosettaNet in using Web technologies for B2B integration illustrates the transformative power of Web technologies and also highlights challenges for the future. This paper provides an overview of RosettaNet technical standards and discusses the lessons learned from the standardization efforts, in particular, what works and what doesn't. This paper also describes the effort to increase automation of B2B software integration, and thereby to reduce cost.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {188–195},
numpages = {8},
keywords = {B2B integration, XML, messaging services, business process, PIP},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/3244268,
author = {Tomkins, Andrew},
title = {Session Details: Semantics and Discovery},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244268},
doi = {10.1145/3244268},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
numpages = {1},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013400,
author = {Balke, Wolf-Tilo and Wagner, Matthias},
title = {Through Different Eyes: Assessing Multiple Conceptual Views for Querying Web Services},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013400},
doi = {10.1145/1013367.1013400},
abstract = {We present enhancements for UDDI / DAML-S registries allow-ing cooperative discovery and selection of Web services with a focus on personalization. To find the most useful service in each instance of a request, not only explicit parameters of the request have to be matched against the service offers. Also user preferences or implicit assumptions of a user with respect to common knowledge in a certain domain have to be considered to improve the quality of service provisioning. In the area of Web services the notion of service ontologies together with cooperative answering techniques can take a lot of this responsibility. However, without quality assessments for the relaxation of service requests and queries a personalized service discovery and selection is virtually impossible. This paper focuses on assessing the semantic meaning of query relaxation plans over multiple conceptual views of the service ontology, each one representing a soft query constraint of the user request. Our focus is on the question what constitutes a minimum amount of necessary relaxation to answer each individual request in a cooperative manner. Incorporating such assessments as early as possible we propose to integrate ontology-based discovery directly into UDDI directories or query facilities in service provisioning portals. Using the quality assessments presented here, this integration promises to propel today's Web services towards an intuitive user-centered service provisioning.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {196–205},
numpages = {10},
keywords = {personalization, cooperative service discovery, semantic web, web services, preference-based service provisioning, user profiling},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013401,
author = {Mukhi, Nirmal K. and Konuru, Ravi and Curbera, Francisco},
title = {Cooperative Middleware Specialization for Service Oriented Architectures},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013401},
doi = {10.1145/1013367.1013401},
abstract = {Service-oriented architectures (SOA) will provide the basis of thenext generation of distributed software systems, and have already gained enormous traction in the industry through an XML--based instantiation, Web services. A central aspect of SOAs is the looser coupling between applications (services) that is achieved when services publish their functional and non-functional behavioral characteristics in a standardized, machine readable format. In this paper we argue that in the basic SOA model access to metadata is too static and results in inflexible interactions between requesters and providers. We propose specific extensions to the SOA model to allow service providers and requestors to dynamically expose and negotiate their public behavior, resulting in the ability to specialize and optimize the middleware supporting an interaction. We introduce a middleware architecture supporting this extended SOA functionality, and describe a conformant implementation based on standard Web services middleware. Finally, we demonstrate the advantages of this approach with a detailed real world scenario.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {206–215},
numpages = {10},
keywords = {service-oriented architecture, web services, metadata exchange, middleware reconfiguration},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013403,
author = {Avoyan, Hovhannes and Levine, Barry},
title = {Web Engineering with the Visual Software Circuit Board},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013403},
doi = {10.1145/1013367.1013403},
abstract = {The Visual Software Circuit Board (VSCB) platform supports a component based development methodology towards the development of software systems. The circuit board design techniques and methodologies have evolved for electronic device and component engineering for decades. The circuit board approach, now applied for software systems and applications, makes the component based development process easy to visualize and comprehend. This paper describes the VSCB based design methodology with a specific focus on usage of VSCB for web application engineering.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {216–217},
numpages = {2},
keywords = {web engineering, circuit based software development, web application development, visual programming, component based development, rapid application development},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@dataset{10.1145/review-1013367.1013403_R38436,
author = {Dave, Maulik A},
title = {Review ID:R38436 for DOI: 10.1145/1013367.1013403},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1013367.1013403_R38436}
}

@inproceedings{10.1145/1013367.1013404,
author = {Kwok, Thomas and Nguyen, Thao and Lam, Linh and Roy, Kakan},
title = {An Efficient and Systematic Method to Generate Xslt Stylesheets for Different Wireless Pervasive Devices},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013404},
doi = {10.1145/1013367.1013404},
abstract = {It is a tedious and cumbersome process to update directly a WML document for the wireless Web because its content composes of both data and presentation. Thus, XML is used to handle the data while its XSLT stylesheet is used to extract and format the data for presentation. However, different stylesheets have to be used for different devices. An efficient and systematic method based on the idea of generating two separate sets of rules corresponding to content extracting and formatting parts of the stylesheet is described in this paper. The data extraction part is constructed from content rules while the formatting part is constructed from presentation rules. They are then combined together to form a stylesheet by an XSLT generator. A large number of stylesheets corresponding to different devices and a number of standard DTD documents or XML schemas can be generated in this way and stored in the pool during application setup stage. They will be individually selected from the pool by an XSLT engine to produce different WML documents for different devices during run time.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {218–219},
numpages = {2},
keywords = {PDA, pervasive devices, XSLT, XML, WML},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013405,
author = {Oberle, Daniel and Staab, Steffen and Volz, Raphael},
title = {An Application Server for the Semantic Web},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013405},
doi = {10.1145/1013367.1013405},
abstract = {The Semantic Web relies on the complex interaction of several technologies involving ontologies. Therefore, sophisticated Semantic Web applications typically comprise more than one software module. Instead of coming up with proprietary solutions, developers should be able to rely on a generic infrastructure for application development in this context. We call such an infrastructure Application Server for the Semantic Web whose design and development are based on existing Application Servers. However, we apply and augment their underlying concepts for use in the Semantic Web and integrate semantic technology within the server itself. We provide a short overview of requirements and design issues of such a server and present our implementation and ongoing work KAON SERVER.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {220–221},
numpages = {2},
keywords = {application server, ontology, semantic web},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013406,
author = {Braslavski, Pavel and Alshanski, Gleb and Shishkin, Anton},
title = {ProThes: Thesaurus-Based Meta-Search Engine for a Specific Application Domain},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013406},
doi = {10.1145/1013367.1013406},
abstract = {In this poster we introduce ProThes, a pilot meta-search engine (MSE) for a specific application domain. ProThes combines three approaches: meta-search, graphical user interface (GUI) for query specification, and thesaurus-based query techniques. ProThes attempts to employ domain-specific knowledge, which is represented by both a conceptual thesaurus and results ranking heuristics. Since the knowledge representation is separated from the MSE core, adjusting the system to a specific domain is trouble free. Thesaurus allows for manual query building and automatic query techniques. This poster outlines the overall system architecture, thesaurus representation format, and query operations. ProThes is implemented on J2EE platform as a Web service.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {222–223},
numpages = {2},
keywords = {meta-search, web services, thesaurus, information retrieval, user interface, query operations},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013407,
author = {Xie, Bo and Han, Peng and Shen, Ruimin},
title = {PipeCF: A Scalable DHT-Based Collaborative Filtering Recommendation System},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013407},
doi = {10.1145/1013367.1013407},
abstract = {Collaborative Filtering (CF) technique has proved to be one of the most successful techniques in recommendation systems in recent years. However, traditional centralized CF system has suffered from its shortage in scalability as their calculation complexity increases quickly both in time and space when the record in user database increases. In this paper, we propose a decentralized CF algorithm, called PipeCF, based on distributed hash table (DHT) method. We also propose two novel approaches to improve the scalability and prediction accuracy of DHT-based CF algorithm. The experimental data show that our DHT-based CF system has better prediction accuracy, efficiency and scalability than traditional CF systems.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {224–225},
numpages = {2},
keywords = {collaborative filtering, distributed hash table},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013408,
author = {Ferretti, Stefano and Roccetti, Marco},
title = {Event Synchronization for Interactive Cyberdrama Generation on the Web: A Distributed Approach},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013408},
doi = {10.1145/1013367.1013408},
abstract = {The digital generation of a story in which users have influence over the narrative is emerging as an exciting example of computer-based interactive entertainment. Interactive storytelling has existed in non digital versions for thousand of years, but with the advent of the Web the demand for enabling distributed cyberdrama generation is becoming increasingly common. To govern the complexity stemming from the distributed generation of complex plots, we have devised an event synchronization service that may be exploited to support the distribution of interactive storytelling activities over the Web. The main novelty of our approach is that the semantics of the cyberdrama is exploited to discard obsolete events. This brings to the positive result of speeding up the activity of drama generation, thus enabling an augmented interactivity among dispersed players.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {226–227},
numpages = {2},
keywords = {cyberdrama generation, web-based multiplayer games, computer-based entertainment, interactive storytelling},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013409,
author = {Bizer, Christian and Oldakowski, Radoslaw},
title = {Using Context- and Content-Based Trust Policies on the Semantic Web},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013409},
doi = {10.1145/1013367.1013409},
abstract = {The current discussion about a future Semantic Web trust architecture is focused on reputational trust mechanisms based on explicit trust ratings. What is often overlooked is the fact that, besides of ratings, huge parts of the application-specific data published on the Semantic Web are also trust relevant and therefore can be used for flexible, fine-grained trust evaluations. In this poster we propose the usage of context- and content-based trust mechanisms and outline a trust architecture which allows the formulation of subjective and task-specific trust policies as a combination of reputation-, context- and content-based trust mechanisms.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {228–229},
numpages = {2},
keywords = {named graphs, trust policies, semantic web, trust mechanisms},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013410,
author = {Tambag, Yusuf},
title = {EIOP: An e-Commerce Interoperability Platform},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013410},
doi = {10.1145/1013367.1013410},
abstract = {Interoperability has become one of the big problems of e-commerce since it was born. A number of B2B standards like ebXML, UDDI, RosettaNet, xCBL, etc. emerged recently to solve the interoperability problem.Currently, there exists many B2B standards each provide competing and complementary solutions to B2B interoperability. So, there is a need for serving implementation of these standards from a single, central store to ease the use and management of the implementations. This paper presents EIOP, an E-commerce Interoperability Platform. EIOP is designed to provide a central store for implementations of e-commerce specifications to be able to use and configure these implementations from a single, central point. It defines the term EIOP Component which corresponds to plug&amp;play e-commerce applications that are stored in the EIOP.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {230–231},
numpages = {2},
keywords = {UDDI, ebXML, interoperability, e-commerce, ebIOP},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013411,
author = {Adi, Asaf and Etzion, Opher and Gilat, Dagan and Ronen, Royi and Sharon, Guy and Skarbovsky, Inna},
title = {Reactive Rules Inference from Dynamic Dependency Models},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013411},
doi = {10.1145/1013367.1013411},
abstract = {Defining dependency models is sometimes an easier, more intuitive way for ontology representation than defining reactive rules directly, as it provides a higher level of abstraction. We will shortly introduce the ADI (Active Dependency Integration) model capabilities, emphasizing new developments: 1. Support of automatic dependencies instantiation from an abstract definition that expresses a general dependency in the ontology, namely a "template". 2. Inference of rules for dynamic dependency models where dependencies and entities may be inserted deleted and updated. We use the eTrade example in order to exemplify those capabilities.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {232–233},
numpages = {2},
keywords = {reactive rules, dependency models, event correlation, relationships between entities, active databases, rule engine, active systems},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013412,
author = {Wang, Szu-Chi and Wei, David S. L. and Kuo, Sy-Yen},
title = {SPT-Based Topology Algorithm for Constructing Power Efficient Wireless Ad Hoc Networks},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013412},
doi = {10.1145/1013367.1013412},
abstract = {In this paper, we present a localized Shortest Path Tree (SPT) based algorithm for constructing a sub-network with the minimum-energy property for a given wireless ad hoc network. Each mobile node determines its own transmission power based only on its local information. The proposed algorithm constructs local shortest path trees from the unit disk graph. The performance improvements of our algorithm are demonstrated through simulations.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {234–235},
numpages = {2},
keywords = {wireless ad hoc networks, topology control, power consumption},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013413,
author = {Aiber, Sarel and Gilat, Dagan and Landau, Ariel and Razinkov, Natalia and Sela, Aviad and Wasserkrug, Segev},
title = {Business Objective Based Resource Management},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013413},
doi = {10.1145/1013367.1013413},
abstract = {Enterprises today wish to manage their IT resources so as to optimize business objectives, such as income, rather than IT metrics, such as response times. Therefore, we introduce a new paradigm, which focuses on such business objective oriented resource management. Additionally, we define a general simulation-based autonomous process enabling such optimizations, and describe a case study, demonstrating the usefulness of such a process.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {236–237},
numpages = {2},
keywords = {modeling techniques, optimization, economic considerations, IT policy, simulation, business objective},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013414,
author = {Sim\~{o}es, D. and Lu\'{\i}s, R. and Horta, N.},
title = {Enhancing the SCORM Metadata Model},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013414},
doi = {10.1145/1013367.1013414},
abstract = {Nowadays, the leading e-learning platforms are converging towards standardization. This paper presents an extension to the SCORM, today's most well acclaimed e-learning standard, enabling the modelling of course related entities that surround learning objects and content aggregations, therefore increasing the standard's modelling scope and allowing for gains in efficiency in knowledge dissemination. A prototype is being implemented and tested on VIANET, an original e-learning platform with extensible support for the SCORM. content aggregations.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {238–239},
numpages = {2},
keywords = {e-learning, modelling, metadata, SCORM, standards},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013415,
author = {Tanaka, Hiroya and Tanaka, Katsumi},
title = {Continuous Web: A New Image-Based Hypermedia and Scape-Oriented Browsing},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013415},
doi = {10.1145/1013367.1013415},
abstract = {Conventionally, Web pages have been recognized as documents described by HTML. Image data, such as photographs, logos, maps, illustrations, and decorated text, have been treated as sub-components of Web documents. However, we can alternatively recognize all Web pages as images on the screen. When a Web page is treated as an image, its HTML data is considered to be metadata which describes the image content. Taking such a viewpoint, we propose a new image-based hypermedia which we call continuous web. In our model, there is no distinction between Web images and other images such as photographs.Regarding everything on the Web as images leads us to consider a new style of browsing and navigating. We use the term scape-oriented browsing. We define a scape as a collection of continuously accumulated images. For example, whenever we walk in the real world, we can perceive and remember various forms of information through a scape process. Here, we describe new methods for scape-oriented browsing, such as see-through anchors, parallel navigation, and peripheral scape presentation. We have designed and implemented a prototype system based on our model. Our system offers continuous browsing and navigation to users. We explain our concepts and discuss the effectiveness and potential of this approach.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {240–241},
numpages = {2},
keywords = {hyperimage, images, scape},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013416,
author = {Rutten, Bas and Barna, Peter and Frasincar, Flavius and Houben, Geert-Jan and Vdovjak, Richard},
title = {HPG: A Tool for Presentation Generation in WIS},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013416},
doi = {10.1145/1013367.1013416},
abstract = {Web Information Systems (WIS) support the process of retrieving information from sources on the Web and of presenting them as a hypermedia presentation. Most WIS design methodologies focus on the engineering of the abstract navigation (hyperlinks). The actual presentation generation is less supported. Hera is one of the few WIS methodologies that offer a tool for presentation generation (HPG). The HPG transforms RDF data obtained as the result of a query into a Web presentation suited to the user (in HTML or WML).},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {242–243},
numpages = {2},
keywords = {hypermedia, RDF(S), WIS, presentation generation, XSLT},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013417,
author = {Naaman, Mor and Song, Yee Jiun and Paepcke, Andreas and Garcia-Molina, Hector},
title = {Automatically Generating Metadata for Digital Photographs with Geographic Coordinates},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013417},
doi = {10.1145/1013367.1013417},
abstract = {Given location information on digital photographs, we can automatically generate an abundance of photo-related metadata using off-the-shelf and web-based data sources. These metadata can serve as additional memory cues and filters when browsing a personal or global collection of photos.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {244–245},
numpages = {2},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013418,
author = {Zhuge, Hai and Li, Yanyan},
title = {Active E-Course for Constructivist Learning},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013418},
doi = {10.1145/1013367.1013418},
abstract = {An active e-course is a self-representable and self-organizable document mechanism with a flexible structure. The kernel of the active e-course is to organize learning materials into a "concept space" rather than a "page space". Besides highly interactive service it supports adaptive learning by dynamically selecting organizing and presenting the learning materials for different students. During the learning progress it also provides assessments on students' learning performances and gives suggestions to guide them in further learning. We have implemented an authoring tool and a course prototype to support the constructivist learning.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {246–247},
numpages = {2},
keywords = {constructivist learning, course ontology, semantic link network, active e-course},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013419,
author = {Murayama, Norifumi and Saito, Suguru and Okumura, Manabu},
title = {Are Web Pages Characterized by Color?},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013419},
doi = {10.1145/1013367.1013419},
abstract = {When human guess the content of a web page,not only the text on the page but also its appearance is an important factor.However, there have been few studies on the relationshipbetween the content and visual appearance of a web page.We investigating the tendencybetween them, especially web content and color use, we found a tendency to use color for some kinds of content pages. We think this result opens the way to estimating web content using color information.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {248–249},
numpages = {2},
keywords = {contents of web page, color},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013420,
author = {Zhang, Xinyan and Liu, Jiangchuan},
title = {Gossip Based Streaming},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013420},
doi = {10.1145/1013367.1013420},
abstract = {In this paper, we propose a novel multicast streaming protocol foroverlay networks, called Gossip Based Streaming (GBS). In GBS,streaming contents are not come from a single upstream source,but delivered from several sources to a client. Though being similarto existing gossip protocols, the unique requirements forstreaming, such as continuous playback, are addressed in our design.Preliminary results show that GBS performs much better indynamic user environments.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {250–251},
numpages = {2},
keywords = {multicast, overlay networks, streaming},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013421,
author = {Kanjo, Daisuke and Kawai, Yukiko and Tanaka, Katsumi},
title = {A3: Framework for User Adaptation Using Xslt},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013421},
doi = {10.1145/1013367.1013421},
abstract = {We propose a system called "Adaptation Anywhere &amp; Anytime(A3)", which is a framework for making web sites/applications adaptable to user's needs or interests, and we describe the implement of a web site on A3 by using XSLT. Web sites/applications built on A3 construct user ontologies for each user automatically and share them between sites/applications. Each site/application uses the user ontology to select an appropriate resource for the user and to present such resources in a suitable form. And A3 offers the method for constructing the adaptable web sites using XSLT. The author of web sites can easily make their sites adaptable by using XSLT.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {252–253},
numpages = {2},
keywords = {user adaptation, ontology, semantic web, XSLT},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013422,
author = {Lu, Yizhou and Zhang, Benyu and Xi, Wensi and Chen, Zheng and Liu, Yi and Lyu, Michael R. and Ma, Wei-ying},
title = {The Powerrank Web Link Analysis Algorithm},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013422},
doi = {10.1145/1013367.1013422},
abstract = {The web graph follows the power law distribution and has a hierarchy structure. But neither the PageRank algorithm nor any of its improvements leverage these attributes. In this paper, we propose a novel link analysis algorithm "the PowerRank algorithm", which makes use of the power law distribution attribute and the hierarchy structure of the web graph. The algorithm consists two parts. In the first part, special treatment is applied to the web pages with low "importance" score. In the second part, the global "importance" score for each web page is obtained by combining those scores together. Our experimental results show that: 1) The PowerRank algorithm computes 10%-30% faster than PageRank algorithm. 2) Top web pages in PowerRank algorithm remain similar to that of the PageRank algorithm.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {254–255},
numpages = {2},
keywords = {hierarchy structure, power distribution, page rank algorithm},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013423,
author = {Tashiro, Noriharu and Hattori, Hiromitsu and Ito, Takayuki and Shintani, Toramatsu},
title = {Implementing a Proxy Agent Based Writable Web for a Dynamic Information Sharing System},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013423},
doi = {10.1145/1013367.1013423},
abstract = {In this paper, we propose a Web based information sharing system called the Proxy Agent-based Information Sharing(PAIS).We also developed a writable Web mechanism called Web browser-based Direct Editing (Wedit), that is a major component of PAIS. Wedit enables public users to effectively edit HTML text on an existing Web browser. Since Wedit was developed with conventional technologies, users quickly learn how to use it. PAIS is implemented by using Wedit and a proxy agent. PAIS enables users to share information via Web pages using Wedit. The proxy agent maintainsusers' editing data. The agent autonomously sends its user's modification data to other agents in the same community. In PAIS, certain confidential information in the community is not publicly shared by using the proxy agent.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {256–257},
numpages = {2},
keywords = {browsing support, information system, multiagent system},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013424,
author = {Sartiani, Carlo},
title = {A Query Algebra for Xml P2p Databases},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013424},
doi = {10.1145/1013367.1013424},
abstract = {This paper describes a query algebra for queries over XML p2p databases that provides explicit mechanisms for modeling data dissemination, replication constraints, and for capturing the transient nature of data and replicas.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {258–259},
numpages = {2},
keywords = {peer data management systems, XML, query algebras},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013425,
author = {Fern\'{a}ndez-Garcia, Norbeto and S\'{a}nchez-Fernandez, Luis and Villamor-Lugo, Jes\'{u}s},
title = {Next Generation Web Technologies in Content Management},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013425},
doi = {10.1145/1013367.1013425},
abstract = {The development of information and communication technologies and the expansion of the Internet means that, nowadays, there arehuge amounts of information available via these emergent media. A number of content management systems have appeared which aim to support the management of these large amounts of content. Most of these systems do not support collaboration among several, distributed sources of managed content. In this paper we present a proposal for an architecture, Infoflex, for the efficient and flexible management of distributed content using Next Generation Web Technologies: Web Services and Semantic Web facilities.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {260–261},
numpages = {2},
keywords = {web services, content management, semantic web},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013426,
author = {Kan, Min-Yen},
title = {Web Page Classification without the Web Page},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013426},
doi = {10.1145/1013367.1013426},
abstract = {Uniform resource locators (URLs), which mark the address of a resource on the World Wide Web, are often human-readable and can hint at the category of the resource. This paper explores the use of URLs for webpage categorization via a two-phase pipeline of word segmentation/expansion and classification. We quantify its performance against document-based methods, which require the retrieval of the source document.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {262–263},
numpages = {2},
keywords = {word segmentation, text categorization, abbreviation expansion, uniform resource locator},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013427,
author = {Mor, Enric and Minguill\'{o}n, Juli\`{a}},
title = {E-Learning Personalization Based on Itineraries and Long-Term Navigational Behavior},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013427},
doi = {10.1145/1013367.1013427},
abstract = {In this paper we describe a practical framework for studying then a navigational behavior of the users of an e-learning environment integrated in a virtual campus. The students navigate through the web based virtual campus interacting with learning resources which are structured following the SCORM e-learning standard. Our main goal is to design a usage mining tool for analyzing such user navigational behavior and for extracting relevant information that can be used to validate several aspects related to virtual campus design and usability but also to determine the optimal scheduling for each course depending on user profile. We intend to extend these quencing capabilities of the SCORM standard to include the concept of recommended itinerary, by combining teachers expertise with learned experience acquired by system usage analysis.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {264–265},
numpages = {2},
keywords = {data mining, personalization, e-learning, navigational patterns, SCORM},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013428,
author = {Mohamed, Khaireel A. and Belenkaia, Lioudmila and Ottmann, Thomas},
title = {Post-Processing Inkml for Random-Access Navigation of Voluminous Handwritten Ink Documents},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013428},
doi = {10.1145/1013367.1013428},
abstract = {The goal of this research is the improvement of browsing voluminous InkML data in two areas: ease of rendering continuous ink-flow for replay-browsing, and ease of random access navigation in eLearning domains. The notion of real-time random access navigation in ink documents has not yet been fully exploited. Users of existing eLearning browsers are restricted to viewing static annotated slides that are inferior in quality when compared to actively replaying the same slides with sequenced ink-flow of the annotated freehand writings. We are developing a tool to investigate ways of managing massive InkML data for efficient "active visible scrolling" of recorded freehand writings in ink documents. This work will also develop and evaluate new post-processing techniques that take advantage of the relationship between ink volumes and active-rendering times for real-time random access navigation.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {266–267},
numpages = {2},
keywords = {digital ink, InkML, random access, freehand writing},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013429,
author = {Constantinescu, Ion and Faltings, Boi and Binder, Walter},
title = {Type Based Service Composition},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013429},
doi = {10.1145/1013367.1013429},
abstract = {Service matchmaking and composition has recently drawn increasing attention in the research community. Most existing algorithms construct chains of services based on exact matches of input/output types. However, this does not work when the available services only cover a part of the range of the input type. We present an algorithm that also allows partial matches and composes them using switches that decide on the required service at runtime based on the actual data type. We report experiments on randomly generated composition problems that show that using partial matches can decrease the failure rate of the integration algorithm using only complete matches by up to 7 times with no increase in the number of directory accesses required. This shows that composition with partial matches is an essential and useful element of web service composition.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {268–269},
numpages = {2},
keywords = {web services, type based composition, runtime non-determinism, partial matches, large scale discovery},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013430,
author = {Qu, Yuzhong and Gao, Zhiqiang},
title = {Interpreting Distributed Ontologies},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013430},
doi = {10.1145/1013367.1013430},
abstract = {Semantic Web is challenged by the URI meaning issues arising from putting ontologies in open and distributed environments. As a try to clarify some of the meaning issues, this paper proposes a new approach to interpreting distributed ontologies, it's built on the top of local models semantics, and extends it to deal with the URI sharing by harmonizing the local models via agreement on vocabulary provenance. The commitment relationship is presented to allow the URI sharing between ontologies with richer semantics.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {270–271},
numpages = {2},
keywords = {commitment relationship, vocabulary provenance, distributed description logic, OWL},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013431,
author = {Wegscheider, Florian and Dangl, Thomas and Jank, Michael and Simon, Rainer},
title = {A Multimodal Interaction Manager for Device Independent Mobile Applications},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013431},
doi = {10.1145/1013367.1013431},
abstract = {This poster presents an overview of the work on an interaction manager of a platform for multimodal applications in 2.5G and 3G mobile phone networks and WLAN environments. The poster describes the requirements for the interaction manager (IM), its tasks and the resulting structure. We examine the W3C's definition of an interaction manager and compare it to our implementation, which accomplishes some additional tasks.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {272–273},
numpages = {2},
keywords = {session management, interaction manager, device independence, multimodal interface, multi-user applications, mobile network},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013432,
author = {Zhuge, Hai and Chen, Xue and Li, Xiang},
title = {Modeling the Growth of Future Web},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013432},
doi = {10.1145/1013367.1013432},
abstract = {The future Web can be imagined as a life network consisting of resource nodes and semantic relationship links between them. Any node has a life span from birth - adding it to the network - to death - removing it from the network. Through establishing and investigating two types of models for such a network, we obtain the same scale free distribution of semantic links. Simulations and comparisons validate the rationality of the proposed models.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {274–275},
numpages = {2},
keywords = {power law, distribution, web, evolution},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013433,
author = {Sereno, Bertrand and Shum, Simon Buckingham and Motta, Enrico},
title = {Semi-Automatic Annotation of Contested Knowledge on the World Wide Web},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013433},
doi = {10.1145/1013367.1013433},
abstract = {We describe a strategy to support the semantic annotation of contested knowledge, in the context of the Scholarly Ontologies project, which aims at building a network of interpretations enriching a corpus of scholarly papers. To model such knowledge, which does not have 'right' and 'wrong' values, we are building on the notion of active recommendations as a means to sparkle annotators' interest. We finally argue for a different approach to the evaluation of its impact.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {276–277},
numpages = {2},
keywords = {annotation, interface, contesting interpretations, sense-making},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013434,
author = {Zhuge, Hai and Zheng, Liping and Zhang, Nan and Li, Xiang},
title = {An Automatic Semantic Relationships Discovery Approach},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013434},
doi = {10.1145/1013367.1013434},
abstract = {An important obstacle to the success of the Semantic Web is that the establishment of the semantic relationship is labor-intensive. This paper proposes an automatic semantic relationship discovering approach for constructing the semantic link network. The basic premise of this work is that the semantics of a web page can be reflected by a set of keywords, and the semantic relationship between two web pages can be determined by the semantic relationship between their keyword sets. The approach adopts the data mining algorithms to discover the semantic relationships between keyword sets, and then uses deductive and analogical reasoning to enrich the semantic relationships. The proposed algorithms have been implemented. Experiment shows that the approach is feasible.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {278–279},
numpages = {2},
keywords = {data mining, semantic link network, semantic web, algorithm, analogical reasoning},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013435,
author = {Xu, Jianliang and Lee, Wang-Chien and Liu, Jiangchuan},
title = {Scheduling Web Requests in Broadcast Environments},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013435},
doi = {10.1145/1013367.1013435},
abstract = {On-demand broadcast has been supported in the Internet to enhance system scalability. Unfortunately, most of existing on-demand scheduling algorithms did not consider the time constraints associated with web requests. This paper proposes a novel scheduling algorithm, called Slack Inverse Number of requests (SIN), that takes into account the urgency and productivity of serving pending requests. Trace-driven experiments demonstrate that SIN significantly out performs existing algorithms over a wide range of workloads.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {280–281},
numpages = {2},
keywords = {scheduling algorithms, web, on-demand broadcast, time constraints},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013436,
author = {Cordasco, Gennaro and Scarano, Vittorio and Vitolo, Cristiano},
title = {Architecture of a P2p Distributed Adaptive Directory},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013436},
doi = {10.1145/1013367.1013436},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {282–283},
numpages = {2},
keywords = {adaptivity, peer to peer, bookmark sharing},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013437,
author = {Zhao, Jun and Goble, Carole and Stevens, Robert},
title = {Semantic Web Applications to E-Science in Silico Experiments},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013437},
doi = {10.1145/1013367.1013437},
abstract = {This paper explains our research and implementations of manual, automatic and deep annotations of provenance logs for e-Science insilico experiments. Compared to annotating general Web documents, annotations for scientific data require more sophisticated professional knowledge to recognize concepts from documents, and more complex text extraction and mapping mechanisms. A simple automatic annotation approach based on "lexicons" and a deep annotation implemented by semantically populating, translating and annotating provenance logs are introduced in this paper. We used COHSE (Conceptual Open Hypermedia Services Environment) to annotate and browse provenance logs from my Grid project, which are conceptually linked together as a hypertext Web of provenance logs and experiment resources, based on the associated conceptual metadata and reasoning over these metadata.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {284–285},
numpages = {2},
keywords = {ontology, annotation, integration, e-science, semantic web, provenance},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013438,
author = {Gedov, Vassil and Stolz, Carsten and Neuneier, Ralph and Skubacz, Michal and Seipel, Dietmar},
title = {Matching Web Site Structure and Content},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013438},
doi = {10.1145/1013367.1013438},
abstract = {To keep an overview of a complex corporate web sites, it is crucial to understand the relationship of contents, structure and the user's behavior. In this paper, we describe an approach which is allowing us to compare web page content with the information implictly defined by the structure of the web site. We start by describing each web page with a set of key words. We combine this information with the link structure in an algorithm generating a context based description. By comparing both descriptions, we draw conclusions about the semantic relationship of a web page and its neighbourhood. In this way, we indicate whether a page fits in the content of its neighbourhood. Doing this, we implicitly identify topics which span over several connected web pages. With our approach we support redesign processes by assessing the actual structure and content of a web site with designer's concepts.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {286–287},
numpages = {2},
keywords = {web content mining, web structure, semantic description},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013439,
author = {Albanese, Massimiliano and Picariello, Antonio and Sansone, Carlo and Sansone, Lucio},
title = {A Web Personalization System Based on Web Usage Mining Techniques},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013439},
doi = {10.1145/1013367.1013439},
abstract = {In the past few years, web usage mining techniques have grown rapidly together with the explosive growth of the web, both in the research and commercial areas. In this work we present a Web mining strategy for Web personalization based on a novel pattern recognition strategy which analyzes and classifies both static and dynamic features. The results of experiments on the data from a large commercial web site are presented to show the effectiveness of the proposed system.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {288–289},
numpages = {2},
keywords = {clustering, web personalization, web usage mining},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013440,
author = {Reynolds, Dave and Shabajee, Paul and Cayzer, Steve},
title = {Semantic Information Portals},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013440},
doi = {10.1145/1013367.1013440},
abstract = {In this paper, we describe the notion of a semantic information portal. This is a community information portal that exploits the semantic web standards to improve structure, extensibility, customization and sustainability. We are in the process of developing a prototype directory of environmental organizations as a demonstration of the approach and outline the design challenges involved and the current status of the work.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {290–291},
numpages = {2},
keywords = {semantic web, information portals},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013441,
author = {Diligenti, Michelangelo and Maggini, Marco and Pucci, Filippo Maria and Scarselli, Franco},
title = {Design of a Crawler with Bounded Bandwidth},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013441},
doi = {10.1145/1013367.1013441},
abstract = {This paper presents an algorithm to bound the bandwidth of a Web crawler. The crawler collects statistics on the transfer rate of each server to predict the expected bandwidth use for future downloads. The prediction allows us to activate the optimal number of fetcher threads in order to exploit the assigned bandwidth. The experimental results show the effectiveness of the proposed technique.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {292–293},
numpages = {2},
keywords = {parallel web crawlers, bandwidth optimization},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013442,
author = {Hyv\"{o}nen, Eero and Valo, Arttu and Viljanen, Kim and Holi, Markus},
title = {A Logic-Based Semantic Web Html Generator - a Poor Man's Publishing Approach},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013442},
doi = {10.1145/1013367.1013442},
abstract = {This paper presents a method and a tool for publishing semantic web content in RDF(S) for the humans as a static HTML page site.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {294–295},
numpages = {2},
keywords = {logic, semantic web, content publishing, ontology},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013443,
author = {Holi, Markus and Hyv\"{o}nen, Eeru},
title = {A Method for Modeling Uncertainty in Semantic Web Taxonomies},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013443},
doi = {10.1145/1013367.1013443},
abstract = {We present a method for representing and reasoning with uncertainty in RDF(S) and OWL ontologies based on Bayesian networks.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {296–297},
numpages = {2},
keywords = {uncertainty, semantic web, ontology},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013444,
author = {Brodie, Daniel and Gupta, Amrish and Shi, Weisong},
title = {Keyword-Based Fragement Detection for Dynamic Web Content Delivery},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013444},
doi = {10.1145/1013367.1013444},
abstract = {Fragment-based caching has been proposed as a promising technique for dynamic Web content delivery and caching. Most of these approaches either assume the fragment-based content is served by Web server automatically, or look at server-side caching only. There is no method of extracting fragments from an existing dynamic Web content, which is of great importance to thesuccess of fragment-based caching. Also, current technologies for supporting dynamic fragments do not allow to take into account changes in fragment spatiality, which is a popular technique in dynamic and personalized Web site design. This paper describes our effort to address these short comings. The first, DyCA, aDynamic Content Adapter, is a tool for creating fragment-based content from original dynamic content. Our second proposal is an augmentation to the ESI standard that will allow it to support looking up fragment locations in a mapping table that comes attached with the template. This allows the fragments to move across the document without needing to reserve the template.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {298–299},
numpages = {2},
keywords = {dynamic web content delivery, fragment detection},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013445,
author = {Nasraoui, Olfa and Pavuluri, Mrudula},
title = {Accurate Web Recommendations Based on Profile-Specific Url-Predictor Neural Networks},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013445},
doi = {10.1145/1013367.1013445},
abstract = {We present a Context Ultra-Sensitive Approach based on two-step Recommender systems (CUSA-2-step-Rec). Our approach relies on a committee of profile-specific neural networks. This approach provides recommendations that are accurate and fast to train because only the URLs relevant to a specific profile are used to define the architecture of each network. We compare the proposed approach with collaborative filtering showing that our approach achieves higher coverage and precision while being faster, and requiring lower main memory at recommendation time. While most recommenders are inherently context sensitive, our approach is context ultra-sensitive because a different recommendation model is designed for each profile separately.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {300–301},
numpages = {2},
keywords = {web mining, neural networks, collaborative filtering},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013446,
author = {Guo, Yuanbo and Pan, Zhengxiang and Heflin, Jeff},
title = {Choosing the Best Knowledge Base System for Large Semantic Web Applications},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013446},
doi = {10.1145/1013367.1013446},
abstract = {We present an evaluation of four knowledge base systems with respect to use in large Semantic Web applications. We discuss the performance of each system. In particular, we show that existing systems need to place a greater emphasis on scalability.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {302–303},
numpages = {2},
keywords = {evaluation, knowledge base system, benchmark, DAML+OIL, semantic web},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013447,
author = {Yang, Hui and Chua, Tat-Seng},
title = {FADA: Find All Distinct Answers},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013447},
doi = {10.1145/1013367.1013447},
abstract = {The wealth of information available on the web makes it an attractive resource for seeking quick answers. While web-based question answering becomes an emerging topic in recent years, the problem of efficiently locating a complete set of distinct answers on the Web is far from being solved. We introduce our system, FADA, which relies on question event analysis, web page clustering, and natural language parsing, to find reliable distinct answers with high recall. The method has been found to be effective in strengthening state-of-the-art Web question answering techniques by emphasizing on answer completeness and uniqueness.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {304–305},
numpages = {2},
keywords = {question answering, web page classification},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013448,
author = {Parsia, Bijan and Patel-Schneider, Peter F.},
title = {Meaning and the Semantic Web},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013448},
doi = {10.1145/1013367.1013448},
abstract = {The meaning of names (URI references) is a contentious issue in the Semantic Web. Numerous proposals have been given for how to provide meaning for names in the Semantic Web, ranging from a strict localized model-theoretic semantics to proposals for a unified single meaning. We argue that a slight expansion of the standard model-theoretic semantics for names is sufficient for the present, and can easily be augmented where necessary to allow communities of interest to strengthen this spartan theory of meaning.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {306–307},
numpages = {2},
keywords = {semantic web, representation, meaning},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013449,
author = {Mallya, Ashok U. and Singh, Munindar P.},
title = {A Semantic Approach for Designing Business Protocols},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013449},
doi = {10.1145/1013367.1013449},
abstract = {Business processes involve interactions among autonomous partners. We propose that these interactions be specified modularly as protocols. Protocols can be published, enabling implementors to independently develop components that respect published protocols and yet serve diverse interests. A variety of business protocols would be needed to capture subtle business needs. We propose that the same kinds of conceptual abstractions be developed for protocols as for information models. Specifically, we consider (1) refinement: a subprotocol may satisfy the requirements of a superprotocol, but support additional properties and (2) aggregation: a protocol may combine existing protocols. In support of the above, we develop a formal semantics for protocols, an operational characterization of them, and an algebra for protocol composition.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {308–309},
numpages = {2},
keywords = {web services, business process composition, commitments},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013450,
author = {McCormack, Cameron L. and Marriott, Kim and Meyer, Bernd},
title = {Constraint SVG},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013450},
doi = {10.1145/1013367.1013450},
abstract = {We believe it is important for web graphic standards such as SVG to support user interaction and diagrams that can adapt their layout and appearance to their viewing context so as to take into account viewing device charateristics and the viewer's requirements. Previously we suggested that adding expression-based attributes to SVG and using one-way constraints to evaluate these dynamically would considerably improve SVG's support for adaptive layout and user interaction. We describe a minimal backward compatible extension to SVG 1.1, called Constraint SVG (CSVG), that provides such expression-based attributes and its implementation on top of Batik. CSVG also provides another significant extension to SVG 1.1: it allows the author to define new custom elements using XSLT.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {310–311},
numpages = {2},
keywords = {scalable vector graphics, CSVG, interaction, constraints, constraint-based graphics, differential scaling, adaptivity, SVG, document formats, semantic zooming},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013451,
author = {Kim, Su Myeon and Rosu, Marcel Catalin},
title = {A Survey of Public Web Services},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013451},
doi = {10.1145/1013367.1013451},
abstract = {This paper introduces a methodology to provide the first characterization of public Web Services in terms of their evolution, location, complexity, message size, and response time.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {312–313},
numpages = {2},
keywords = {web services, WSDL, web services traffic characteristics, SOAP, measurement, UDDI business registry},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013452,
author = {Nambiar, Ullas and Kambhampati, Subbarao},
title = {Providing Ranked Relevant Results for Web Database Queries},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013452},
doi = {10.1145/1013367.1013452},
abstract = {Often Web database users experience difficulty in articulating their needs using a precise query. Providing ranked set of possible answers would benefit such users. We propose to provide ranked answers to user queries by identifying a set of queries from the query log whose answers are relevant to the given user query. The relevance detection is done using a domain and end-user independent content similarity estimation technique.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {314–315},
numpages = {2},
keywords = {web-enabled database, query suggestion, content similarity},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013453,
author = {Yamaguchi, Toshihiro and Hattori, Hiromitsu and Ito, Takayuki and Shintani, Toramatsu},
title = {On a Web Browsing Support System with 3d Visualization},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013453},
doi = {10.1145/1013367.1013453},
abstract = {Existing commercial Web browsers provide various utilities and functions, e.g., Web bookmarks and a browsing history list. Since the bookmark and history functions only the title and URL of the Web page, users who cannot remember the contents of each Web page have difficulty retracing their steps. In this paper, we propose a bookmark system based on a 3D interface. Additionally, our system offers three main functions a 3D browsing history function, a marker function, and a look-ahead loading function. These functions enable users to browse Web pages more effectively.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {316–317},
numpages = {2},
keywords = {web browser, 3D technology, visualization},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013454,
author = {Carboni, Davide and Piras, Andrea and Sanna, Stefano and Giroux, Sylvain},
title = {The Web around the Corner: Augmenting the Browser with Gps},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013454},
doi = {10.1145/1013367.1013454},
abstract = {As programmable mobile devices (such as high-end cellular phones and Personal Digital Assistants) became widely adopted, users ask for Internet access on-the-road. While upcoming technologies like UMTS and Wi-Fi provide broadband wireless communication, Web services and Web browsers do not provide any sort of location-awareness yet. As GPS receivers get cheaper, positioning devices will be embedded into commercial mobile devices. Thus, the position of the user can be used to filter and tailor the information presented to the user as already done for language preferences and user-agent.This paper describes early results of an ongoing project called GPSWeb, which aims to provide GPS support for Web browsers and an application model for Location-Based Services. It introduces the Location-Based Browsing concept that enhances the classic Webuser-Website interaction.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {318–319},
numpages = {2},
keywords = {browser, javascript, GPS, LBS, location-awareness},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013455,
author = {Nanno, Tomoyuki and Fujiki, Toshiaki and Suzuki, Yasuhiro and Okumura, Manabu},
title = {Automatically Collecting, Monitoring, and Mining Japanese Weblogs},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013455},
doi = {10.1145/1013367.1013455},
abstract = {We present a system that tries to automatically collect and monitor Japanese blog collections that include not only ones made with blog softwares but also ones written as normal web pages. Our approach is based on extraction of date expressions and analysis of HTML documents. Our system also extracts and mines useful information from the collected blog pages.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {320–321},
numpages = {2},
keywords = {monitoring, text mining, weblogs, trend analysis, document analysis},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013456,
author = {Watanabe, Mitsutaka and Takaya, Ken-ichi and Seo, Akishi and Hashimoto, Masatomo and Izumida, Tomonori and Mori, Akira},
title = {A Scheme of Service Discovery and Control on Ubiquitous Devices},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013456},
doi = {10.1145/1013367.1013456},
abstract = {We have developed a set of hardware and software components to realize ubiquitous computing environments, based on two keywords, simple" (easy to implement) and "open"(adopt widely publicized specifications). Then this set has been resulted into UBKit (Ubiquity Building Toolkit). The Micro-Server an instance of UBKitenables existing consumer electronics to join in computer networks. In this paperwe propose a scheme for discovery and control of deviecs attached to micro-servers."},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {322–323},
numpages = {2},
keywords = {ad-hoc network, peer to peer, service discovery, ubiquitous computing},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013457,
author = {Qu, Yuzhong and Zhang, Xiang and Li, Huiying},
title = {OREL: An Ontology-Based Rights Expression Language},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013457},
doi = {10.1145/1013367.1013457},
abstract = {This paper proposes an Ontology-based Rights Expression Language, called OREL. Based on OWL Web Ontology Language, OREL allows not only users but also machines to handle digital rights at semantics level. The ontology-based rights model of OREL is also presented. The usage of OREL and its advantages against existing RELs are discussed.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {324–325},
numpages = {2},
keywords = {OREL, rights expression language, OWL, XrML},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013458,
author = {Harth, Andreas and Decker, Stefan and He, Yu and Tangmunarunkit, Hongsuda and Kesselman, Carl},
title = {A Semantic Matchmaker Service on the Grid},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013458},
doi = {10.1145/1013367.1013458},
abstract = {A fundamental task on the Grid is to decide what jobs to run on what computing resources based on job or application requirements. Our previous work on ontology-based matchmaking discusses a resource matchmaking mechanism using Semantic Web technologies. We extend our previous work to provide dynamic access to such matchmaking capability by building a persistent online matchmaking service. Our implementation uses the Globus Toolkit for the Grid service development, and exploits the monitoring and discovery service in the Grid infrastructure to dynamically discover and update resource information. We describe the architecture of our semantic matchmaker service in the poster.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {326–327},
numpages = {2},
keywords = {semantic web, networking and distributed web applications, resource selection, grid services, resource allocation},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013459,
author = {Baeza-Yates, Ricardo and Davis, Emilio},
title = {Web Page Ranking Using Link Attributes},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013459},
doi = {10.1145/1013367.1013459},
abstract = {We present a variant of PageRank, WLRank, that considers different Web page attributes to give more weight to some links. Our evaluation shows that the precision of the answers can improve significantly.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {328–329},
numpages = {2},
keywords = {web link ranking, PageRank},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013460,
author = {Gao, Song and Ng, Wee Siong and Qian, Weining and Zhou, Ying Ao},
title = {CC-Buddy: An Adaptive Framework for Maintaining Cache Coherency Using Peers},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013460},
doi = {10.1145/1013367.1013460},
abstract = {In this paper, we propose a framework called CC-Buddy, for maintaining dynamic data coherency in peer-to-peer environment. Working on the basis of peer heterogeneity in data coherency requirement, peers in CC-Buddy cooperate with each other to disseminate the updates by pushing. Simulation results show that our solution not only improves the fidelity in data, but also reduces the workload of servers, therefore achieves high-scalability.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {330–331},
numpages = {2},
keywords = {peer-to-peer, cache coherency, dynamic data},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013461,
author = {Wang, Hsinping and Lin, Tsungnan and Chen, Chia Hung and Shen, Yennan},
title = {Dynamic Search in Peer-to-Peer Networks},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013461},
doi = {10.1145/1013367.1013461},
abstract = {This work specifically addresses the search issues in unstructured peer-to-peer (P2P) systems that involve the design of an efficient search algorithm, the proposed dynamic search, and the modeling of P2P systems reflecting real measured P2P networks. Through simulations, we will show dynamic search outperforms other existing ones in terms of performance aspects.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {332–333},
numpages = {2},
keywords = {search algorithm, gnutella, P2P, Modeling},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013462,
author = {Zhuge, Hai and Liu, Jie},
title = {A Novel Heterogeneous Data Integration Approach for P2p Semantic Link Network},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013462},
doi = {10.1145/1013367.1013462},
abstract = {This paper proposes a novel approach to integrate heterogeneous data in P2P networks. The approach includes a tool for building P2P semantic link networks, mechanisms for peer schema mapping, criteria for peer similarity degree measurement, and algorithms for heterogeneous data integration. The approach has three advantages: First, it uses semantic links to describe semantic relationships between peers' data schemas. Second, it deals with the semantic heterogeneity, the structural heterogeneity and the data value inconsistency. Finally, it considers the semantic similarity and structural similarity to forward queries to relevant peers.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {334–335},
numpages = {2},
keywords = {semantic link, semantic web, P2P computing, data integration},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013463,
author = {Feldman, H. J. and Triola, M. M.},
title = {ResEval: A Web-Based Evaluation System for Internal Medicine House Staff},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013463},
doi = {10.1145/1013367.1013463},
abstract = {The evaluation and assessment of physicians-in-training (house staff) is a complex task. Residency training programs are under increasing pressure [1] to provide accurate and comprehensive evaluations of performance of resident physicians [2,3]. For many years, the Internal Medicine training program at NYU School of Medicine used a single standardized paper form for all evaluation scenarios. This strategy was inadequate as physicians train in multiple diverse settings evaluation of physicians in the intensive care unit is quite different from those in the general clinics. The paper system resulted in poor compliance by house staff and faculty in the completion of evaluations. In addition, the data being collected from the paper forms was of poor quality due to the non-specific nature of the questions. A committee was formed in 2001, which created a new strategy for evaluating the core competencies of house staff. Given the ubiquity of web accessible computers in the clinical and non-clinical areas of hospitals and the flexibility a computerized system would provide, a web-based evaluation system was designed and implemented. This system allows for on-the-spot evaluations tailored to the evaluator, evaluatee and the venue of the evaluation. During the 2002 residency year, data was collected on satisfaction and use of the system and compared with the previous paper evaluation.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {336–337},
numpages = {2},
keywords = {education, evaluations, HTML, assessment, python, web, house staff, medicine, oracle},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013464,
author = {Liu, Y. and Zhang, B. and Chen, Z. and Lyu, M. R. and Ma, W.},
title = {Affinity Rank: A New Scheme for Efficient Web Search},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013464},
doi = {10.1145/1013367.1013464},
abstract = {Maximizing only the relevance between queries and documents will not satisfy users if they want the top search results to present a wide coverage of topics by a few representative documents. In this paper, we propose two new metrics to evaluate the performance of information retrieval: diversity, which measures the topic coverage of a group of documents, and information richness, which measures the amount of information contained in a document. Then we present a novel ranking scheme, Affinity Rank, which utilizes these two metrics to improve search results. We demonstrate how Affinity Rank works by a toy data set, and verify our method by experiments on real-world data sets.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {338–339},
numpages = {2},
keywords = {diversity, link analysis, information richness, affinity rank},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013465,
author = {Harren, Matthew and Raghavachari, Mukund and Shmueli, Oded and Burke, Michael G. and Sarkar, Vivek and Bordawekar, Rajesh},
title = {XJ: Integration of XML Processing into Java},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013465},
doi = {10.1145/1013367.1013465},
abstract = {The increased importance of XML as a universal data representation format has led to several proposals for enabling the development of applications that operate on XML data. These proposals range from runtime API-based interfaces to XML-based programming languages. The subject of this paper is XJ, a research language that proposes novel mechanisms for the integration of XML as a first-class construct into JavaTM. The design goals of XJ distinguish it from pastwork on integrating XML support into programming languages ---specifically, the XJ design adheres to the XML Schema and XPathstandards, and supports in-place updates of XML data thereby keeping with the imperative nature of Java. We have also built a prototype compiler for XJ, and our preliminary experimental results demonstrate that the performance of XJ programs can approach that of tradition allow level API-based interfaces, while providing a higher level of abstraction.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {340–341},
numpages = {2},
keywords = {data integration, XML, Java},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013466,
author = {Fraternali, P. and Lanzi, P. L. and Matera, M. and Maurino, A.},
title = {Exploiting Conceptual Modeling for Web Application Quality Evaluation},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013466},
doi = {10.1145/1013367.1013466},
abstract = {This paper presents an approach and a toolset for exploiting the benefits of conceptual modeling in the quality evaluation tasks that take place both before the deployment and during the operational life of a Web application. The full version of the paper is available as a technical report at the address: http://www.elet.polimi.it/upload/fraterna/FLMM2004.pdf.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {342–343},
numpages = {2},
keywords = {web mining, web application quality, conceptual modeling},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013467,
author = {Jatowt, Adam},
title = {Web Page Summarization Using Dynamic Content},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013467},
doi = {10.1145/1013367.1013467},
abstract = {Summarizing web pages have recently gained much attention from researchers. Until now two main types of approaches have been proposed for this task: content- and context-based methods. Both of them assume fixed content and characteristics of web documents without considering their dynamic nature. However the volatility of information published on the Internet argue for the implementation of more time-aware techniques. This paper proposes a new approach towards automatic web page description, which extends the concept of a web page by the temporal dimension. Our method provides a broader view on web document summarization and can complement the existing techniques.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {344–345},
numpages = {2},
keywords = {web page summarization, change detection, web document},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013468,
author = {Yamada, Yasuhiro and Craswell, Nick and Nakatoh, Tetsuya and Hirokawa, Sachio},
title = {Testbed for Information Extraction from Deep Web},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013468},
doi = {10.1145/1013367.1013468},
abstract = {Search results generated by searchable databases are served dynamically and far larger than the static documents on the Web. These results pages have been referred to as the Deep Web. We need to extract the target data in results pages to integrate them on different searchable databases. We propose a test bed for information extraction from search results. We chose 100 databases randomly from 114,540 pages with search forms. Therefore, these databases have a good variety. We selected 51 databases which include URLs in a results pageand manually identify target information to be extracted. We also suggest evaluation measures for comparing extraction methods and methods for extending the target data.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {346–347},
numpages = {2},
keywords = {meta search, testbed, wrapper, deep web},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013469,
author = {Qin, Jian and Hern\'{a}ndez, Naybell},
title = {Ontological Representation of Learning Objects: Building Interoperable Vocabulary and Structures},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013469},
doi = {10.1145/1013367.1013469},
abstract = {The ontological representation of learning objects is a way to deal with the interoperability and reusability of learning objects (including metadata) through providing a semantic infrastructure that will explicitly declare the semantics and forms of concepts used in labeling learning objects. This paper reports the preliminary result from a learning object ontology construction project, which includes an in-depth study of 14 learning objects and over 500 components in these learning objects. An analysis of the types of components and terms used in these objects reveals that most terms fell into the form and subject categories few pedagogical terms were used. Drawing findings from literature and case study, the authors use a matrix to show relationships in learning objects and relevant knowledge and technologies. Strategies and methods in ontology development and implementation are also discussed.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {348–349},
numpages = {2},
keywords = {controlled vocabulary, metadata, ontologies, learning objects, content structures},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013470,
author = {Kawaeme, Noriaki and Suzuki, Hideaki and Mizuno, Osamu},
title = {Query and Content Suggestion Based on Latent Interest and Topic Class},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013470},
doi = {10.1145/1013367.1013470},
abstract = {To improve the process of user information retrieval, we propose the concept of a latent semantic map (LSM), along with a method of generating this map. The novel aspect of the LSM is that it can archive user models and latent semantic analysis on one map to support instantaneous information retrieval. With this characteristic, the LSM can improve search engines in terms of not only user support but also search results.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {350–351},
numpages = {2},
keywords = {query suggestion, document suggestion, information retrieval, latent semantic map, document categorization},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013471,
author = {Sydow, Marcin},
title = {Random Surfer with Back Step},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013471},
doi = {10.1145/1013367.1013471},
abstract = {We present a novel link-based ranking algorithm RBS, which may be viewed as an extension of PageRank by back-step feature.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {352–353},
numpages = {2},
keywords = {PageRank, back step, ranking algorithms},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013472,
author = {Chan, Pat Pik-Wah and Lyu, Michael R. and Chin, Roland T.},
title = {Copyright Protection on the Web: A Hybrid Digital Video Watermarking Scheme},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013472},
doi = {10.1145/1013367.1013472},
abstract = {Video is one of the most popular data shared in the Web, and the protection of video copyright is of vast interest. In this paper, we present a comprehensive approach for protecting and managing video copyrights in the Internet with watermarking techniques. We propose a novel hybrid digital video watermarking scheme with scrambled watermarks and error correction codes. The effectiveness of this scheme is verified through a series of experiments, and the robustness of our approach is demonstrated using the criteria of the latest StirMark test.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {354–355},
numpages = {2},
keywords = {video, hybrid, digital watermarking, scene change},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013473,
author = {Siu, Dexter Chi Wai and Lau, Tak Pang},
title = {Distributed Ranking over Peer-to-Peer Networks},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013473},
doi = {10.1145/1013367.1013473},
abstract = {Query flooding is a problem existing in Peer-to-Peer networks like Gnutella. Firework Query Model solves this problem by Peer Clustering and routes the query message more intelligently. However, it still contains drawbacks like query flooding inside clusters. The condition can be improved if the query message can send directly to the query destination, as the message does not need to send hop by hop. This can be achieved by ranking. By ranking, the network can know the destination and the information quality shared by each peer. We introduce distributed ranking in this paper. We give background of FQM, outline of the proposed method, and conduct a series of experiments that demonstrate the significant reduction of query flooding in a P2P network.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {356–357},
numpages = {2},
keywords = {peer-to-peer networks, distributed peer ranking},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013474,
author = {Damiani, Ernesto and De Capitani di Vimercati, Sabrina and Paraboschi, Stefano and Samarati, Pierangela and Tironi, Andrea and Zaniboni, Luca},
title = {Spam Attacks: P2p to the Rescue},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013474},
doi = {10.1145/1013367.1013474},
abstract = {We propose a decentralized privacy-preserving approach to spam filtering. Our solution exploits robust digests to identify messages that are a slight variation of one another and a peer-to-peer architecture between mail servers to collaboratively share knowledge about spam.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {358–359},
numpages = {2},
keywords = {spam filtering, structured P2P, reputation},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013475,
author = {Wong, Wan Yeung},
title = {Site-to-Site (S2s) Searching Using the P2p Framework with Cgi},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013475},
doi = {10.1145/1013367.1013475},
abstract = {Peer-To-Peer (P2P) networks like Gnutella improve some shortcomings of Conventional Search Engines (CSE) such as centralized and outdated indexing by distributing the search engines over the peers, which maintain their updated local contents. But they are designed for sharing and searching the contents in personal computers instead of websites. In this work, we propose a novel web information retrieval method called Site-To-Site (S2S) searching, which uses the P2P framework with CGI as protocol. It helps the site owners to turn their websites into autonomous search engines without extra hardware and software costs. In this paper, we introduce S2S searching with some related work. We also describe the system architecture and communication protocol. Finally, we summarize the experimental results, and show that S2S searching works well in one thousand sites.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {360–361},
numpages = {2},
keywords = {site-to-site (S2S), web information retrieval, peer-to-peer (P2P), distributed system, search engine},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013476,
author = {Costa, Fabrizio and Frasconi, Paolo},
title = {Distributed Community Crawling},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013476},
doi = {10.1145/1013367.1013476},
abstract = {The massive distribution of the crawling task can lead to inefficient exploration of the same portion of the Web. We propose a technique to guide crawlers exploration based on the notion of Web communities. Thest ability properties of the method can be used as an implicit coordination mechanism to increase the efficiency of the crawling task.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {362–363},
numpages = {2},
keywords = {web communities, distributed crawling, web metrics},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013477,
author = {Huang, Yingping and Madey, Gregory},
title = {Web Data Integration Using Approximate String Join},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013477},
doi = {10.1145/1013367.1013477},
abstract = {Web data integration is an important preprocessing step for web mining. It is highly likely that several records on the web whose textual representations differ may represent the same real world entity. These records are called approximate duplicates. Data integration seeks to identify such approximate duplicates and merge them into integrated records. Many existing data integration algorithms make use of approximate string join, which seeks to (approximately) find all pairs of strings whose distances are less than a certain threshold. In this paper, we propose a new mapping method to detect pairs of strings with similarity above a certain threshold. In our method, each string is first mapped to a point in a high dimensional grid space, then pairs of points whose distances are 1 are identified. We implement it using Oracle SQL and PL/SQL. Finally, we evaluate this method using real data sets. Experimental results suggest that our method is both accurate and efficient.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {364–365},
numpages = {2},
keywords = {data integration, approximate string join},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013478,
author = {Hulten, G. and Goodman, J. and Rounthwaite, R.},
title = {Filtering Spam E-Mail on a Global Scale},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013478},
doi = {10.1145/1013367.1013478},
abstract = {In this paper we analyze a very large junk e-mail corpus which was generated by a hundred thousand volunteer users of the Hotmail e-mail service. We describe how the corpus is being collected, and analyze: the geographic origins of the e-mail who the e-mail is targeting and what the e-mail is selling.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {366–367},
numpages = {2},
keywords = {spam, international e-mail, Junk E-mail},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013479,
author = {Yip, A.},
title = {The Effect of Different Types of Site Maps on User's Performance in an Information-Searching Task},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013479},
doi = {10.1145/1013367.1013479},
abstract = {This study examines the effects of different types of site maps on user's performance in an information-searching task for three web sites. Forty-two participants (22 males and 20 females) participated in the study. The results showed significant effects on the types of site maps used. It was found that participants found the correct answers more often, required less time, visited significantly fewer web pages, and required fewer clicks to complete the task when the site map was visible. However, it was found that the participants had a lower success rate in finding the correct answers when the site map had hyperlinks. In addition, the results showed significant performance differences among the three web sites and the effects of a site map were found to be more prominent for a larger web site.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {368–369},
numpages = {2},
keywords = {site map, web navigation, hypertext},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013480,
author = {Mathieu, Fabien and Bouklit, Mohamed},
title = {The Effect of the Back Button in a Random Walk: Application for Pagerank},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013480},
doi = {10.1145/1013367.1013480},
abstract = {Theoretical analysis of the Web graph is often used to improve the efficiency of search engines. The PageRank algorithm, proposed by Brin and Page, is used by the Google search engine to improve the results of the queries. The purpose of this article is to describe an enhanced version of the PageRank algorithm using a realistic model forthe back button. We introduce a limited history stack model (you cannot click more than m times in a row), and showthat when m=1, the computation of this Back PageRank can be as fast as that of a standard PageRank.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {370–371},
numpages = {2},
keywords = {web analysis, flow, random walk, back button, pagerank},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013481,
author = {Plachouras, V. and Ounis, I.},
title = {Distribution of Relevant Documents in Domain-Level Aggregates for Topic Distillation},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013481},
doi = {10.1145/1013367.1013481},
abstract = {In this paper, we study the distribution of relevant documents in aggregates, formed by grouping the retrieved documents according to their domain. For each aggregate, we take into account its size, and a measure of the correlation between its incoming and outgoing hyperlinks. We report on a preliminary experiment with two TREC topic distillation tasks, where we find that larger aggregates, or those aggregates with correlated hyperlinks, are more likely to contain relevant documents. This result shows that the distribution of domain-level aggregates is potentially useful for finding relevant documents.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {372–373},
numpages = {2},
keywords = {aggregates, web IR, distribution of relevant documents},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013482,
author = {Wollowski, Michael and Nei, Peter and Barrell, Chris},
title = {A Diagrammatic Inference System for the Web},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013482},
doi = {10.1145/1013367.1013482},
abstract = {We developed a diagrammatic inference system for the World Wide Web. Our system enables the creation of diagrams such that the information contained in them can be searched and inference can be performed on it. We developed an XMLSchema for bar, line, and pie charts. Based on it, we developed software that transforms a corresponding XML file into an SVG image, which in turn is rendered by the client as an image. Additionally, we developed a search engine which enables a user to find information explicitly contained in the XML file, and as such in the image. Furthermore, we developed an inference engine which enables a user to locate information that is implicitly contained in the image.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {374–375},
numpages = {2},
keywords = {inference system, search, XML, searchable diagrams, inference},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013483,
author = {Eap, Ty Mey and Hatala, Marek and Richards, Griff},
title = {Digital Repository Interoperability: Design, Implementation and Deployment of the Ecl Protocol and Connecting Middleware},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013483},
doi = {10.1145/1013367.1013483},
abstract = {This paper describes the design and implementation of the eduSource Communication Layer (ECL) protocol. ECL is one outcome of a pan-Canadian project called eduSource Canada to build an open network of interoperable digital repositories. The design goal was to achieve a highly flexible, easy-to-use, and platform independent communication layer protocol that allows new and existing repositories to communicate and share resources across a network. ECL conforms to IMS Digital Repository Interoperability (DRI) specifications and supports four main functions: search/expose, submit/store, gather/expose and request/deliver. The ECL protocol builds on the latest standards and is flexible with respect to metadata schemas and repository contents. To support easy adoption of the protocol we provide middleware components for connecting existing systems. The ECL is currently used in the eduSource network, and we have begun work bridging with other interoperable initiatives such as Open Knowledge Initiative (OKI). Based on our experience, ECL is truly flexible and easy to use.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {376–377},
numpages = {2},
keywords = {protocols, middleware, interoperability},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013484,
author = {Spoerri, Anselm},
title = {Metacrystal: Visualizing the Degree of Overlap between Different Search Engines},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013484},
doi = {10.1145/1013367.1013484},
abstract = {MetaCrystal enables users to visualize and control the degree of overlap between the results returned by different search engines. Several linked overview tools support rapid exploration, facilitate complex filtering operations and guide users toward relevant information. MetaCrystal addresses the problem of the effective fusion of different search results by helping users to visually combine and filter the top results returned by the different engines. Users can apply weights to the search engines to create their own ranking functions. They can control the degree of overlap by modifying the URL directory depth used to match documents or by changing the number of top documents being compared.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {378–379},
numpages = {2},
keywords = {information visualization, meta searching},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013485,
author = {Kerasha, Mohamed A. and Greenshields, Ian},
title = {Huskysim: A Simulation Toolkit for Application Scheduling in Computational Grids},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013485},
doi = {10.1145/1013367.1013485},
abstract = {Grid computing-- the assemblage of heterogeneous distributed clusters of computers viewed as a single virtual machine-- promises to serve as the next major paradigm in distributed computing. Since Grids are assemblages of (usually) autonomous systems (autonomous clusters, supercomputers, or even single workstations) scheduling can become a complex affair which must take into consideration not just the requirements (and scheduling decisions) made at the point of the job's origin, but also the scheduling requirements (and decisions) made at remote points on the fabric, and in particular scheduling decisions made by a remote autonomous system onto which the local job has been scheduled. The current existing scheduling models range from static, where each of the programs is assigned once to a processor before execution of the program commences, to dynamic, where a program may be reassigned to different processors, or a hybrid approach, which combines characteristics of both techniques [1,4,5].To address this issue, we have developed a JAVA based discrete event Grid simulator toolkit called HuskySim. The HuskySim toolkit provides core functionalities (e.g., compute objects, network objects, and scheduling objects) that can be used to simulate a distributed computing environment. Furthermore, it can be used to predict the performance of various classes of Grid scheduling algorithms including: Static scheduling algorithms, Dynamic scheduling, Adaptive Scheduling.In our design, we adopted an object-oriented design, which allows an easy mapping and integration of simulation objects into the simulation program. This approach simplifies the simulation of multitasking, and distributed data processing model. Our model of multitasking processing is based on an interrupt driven mechanism.As shown in Figure 1, the simulator works by relaying messages between the core engine and the simulation modules through the message handling sub-system. Once the architecture, the load distribution, and the scheduling algorithms are defined, the object registration subsystem sends a NEW OBJECT REQUEST MESSAGE to the object class libraries and builds a skeleton for the requested simulation experiment.Workloads traces can be generated using probabilistic models. The currently supported distributions are: Uniform, Poisson, Exponential, Normal, Erlang, and Power Tailed. It is also possible to use real world load traces. Moreover, we augmented the Simulator with a statistical module. Using the statistical module provided with the HuskySim, the core simulation engine can send messages to perform various type of analysis on the performance data including: variance reduction, regression, time series analysis, clustering, and data mining.In order to quantify the system performance, the simulator provides various performance metrics including: CPU utilization, disk utilization, application turnaround time, latency, make span, host to host bandwidth, jammed bandwidth, and TCP/IP traffic data. These measurements are handled through the measurement sub-system.Furthermore, the HuskySim can be used to simulate the classes of algorithmic and parametric adaptive Grid schedulers. In which, the scheduling algorithm may not be fixed in advance. Simply, the scheduling algorithm is selected at run time based on the current workload on the Grid fabric in order to operate at near optimal level.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {380–381},
numpages = {2},
keywords = {performance prediction, discrete event simulation, computational grids, adaptive scheduling},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013486,
author = {Scarselli, Franco and Tsoi, Ah Chung and Hagenbuchner, Markus},
title = {Computing Personalized Pageranks},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013486},
doi = {10.1145/1013367.1013486},
abstract = {A recently published approach to adaptive page rank, using the solution of quadratic optimization methods with a set of simple constraints, is modified to permit classification of web pages according to their page contents, URLs. This modification allows the approach to be more adapted to the needs of focussed crawlers, or personalized search engines.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {382–383},
numpages = {2},
keywords = {interface personalization, search engines, pagerank},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013487,
author = {Lam, Ka Wai and Leung, Chi Ho},
title = {Rank Aggregation for Meta-Search Engines},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013487},
doi = {10.1145/1013367.1013487},
abstract = {In this paper, we present an algorithm for merging results from different data sources in meta-search engine. We further extend one that has developed for ranking players of a round-robin tournament to a more general one when the ranking input is given from multiple sources. The problem in meta-search engine can be represented by a complete directed graph which can be used by the Majority Spanning Tree (MST) algorithm. It is useful especially when the system must integrate and merge the query results that are returned from various search engines in a consistent manner.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {384–385},
numpages = {2},
keywords = {meta-search engines, rank aggregation},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013488,
author = {Kalantari, Leila and Hatala, Marek and Willms, Jordan},
title = {Using Semantic Web Approach in Augmented Audio Reality System for Museum Visitors},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013488},
doi = {10.1145/1013367.1013488},
abstract = {In this paper, we describe our work in progress on the reasoning module of ec(h)o, an augmented audio-reality interface for museum visitors utilizing spatialized soundscapes and a semantic web approach to information. We used ontologies to describe the semantics of sound objects and represent user model. A rule-based system for selecting sound object uses semantic description of objects, visitor's interaction history and heuristics for continuity of the dialogue between user and the system.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {386–387},
numpages = {2},
keywords = {ontologies, augmented-audio reality, user model, inference rules},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013489,
author = {Crainiceanu, Adina and Linga, Prakash and Machanavajjhala, Ashwin and Gehrke, Johannes and Shanmugasundaram, Jayavel},
title = {A Storage and Indexing Framework for P2p Systems},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013489},
doi = {10.1145/1013367.1013489},
abstract = {We present a modularized storage and indexing framework that cleanly separates the functional components of a P2P system, enabling us to tailor the P2P infrastructure to the specific needs of various Internet applications eat, without having to devise completely new storage management and index structures for each application.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {388–389},
numpages = {2},
keywords = {indexing framework, p2p, peer-to-peer},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013490,
author = {Crainiceanu, Adina and Linga, Prakash and Gehrke, Johannes and Shanmugasundaram, Jayavel},
title = {P-Tree: A P2p Index for Resource Discovery Applications},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013490},
doi = {10.1145/1013367.1013490},
abstract = {We propose a new distributed, fault-tolerant Peer-to-Peer index structure for resource discovery applications called the P-tree. P-trees efficiently support range queries in addition to equality queries.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {390–391},
numpages = {2},
keywords = {resource discovery, range queries, peer-to-peer, indexing},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013491,
author = {Langville, Amy Nicole and Meyer, Carl Dean},
title = {Updating Pagerank with Iterative Aggregation},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013491},
doi = {10.1145/1013367.1013491},
abstract = {We present an algorithm for updating the PageRank vector [1]. Due to the scale of the web, Google only updates its famous PageRank vector on a monthly basis. However, the Web changes much more frequently. Drastically speeding the PageRank computation can lead to fresher, more accurate rankings of the webpages retrieved by search engines. It can also make the goal of real-time personalized rankings within reach. On two small subsets of the web, our algorithm updates PageRank using just 25% and 14%, respectively, of the time required by the original PageRank algorithm. Our algorithm uses iterative aggregation techniques [7, 8] to focus on the slow-converging states of the Markov chain. The most exciting feature of this algorithm is that it can be joined with other PageRank acceleration methods, such as the dangling node lumpability algorithm [6], quadratic extrapolation [4], and adaptive PageRank [3], to realize even greater speedups (potentially a factor of 60 or more speedup when all algorithms are combined). every few weeks. Our solution harnesses the power of iterative aggregation principles for Markov chains to allow for much more frequent updates to the valuable ranking vectors.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {392–393},
numpages = {2},
keywords = {updating, power method, disaggregation, Markov chains, pagerank, aggregation, link analysis, stationary vector},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013492,
author = {Youssefi, Amir H. and Duke, David J. and Zaki, Mohammed J.},
title = {Visual Web Mining},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013492},
doi = {10.1145/1013367.1013492},
abstract = {Analysis of web site usage data involves two significant challenges: firstly the volume of data, arising from the growth of the web, and secondly, the structural complexity of web sites. In this paper we apply Data Mining and Information Visualization techniques to the web domain in order to benefit from the power of both human visual perception and computing we term this Visual Web Mining. In response to the two challenges, we propose a generic framework, where we apply Data Mining techniques to large web data sets and use Information Visualization methods on the results. The goal is to correlate the outcomes of mining Web Usage Logs and the extracted Web Structure by visually superimposing the results. We design several new information visualization diagrams.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {394–395},
numpages = {2},
keywords = {data mining, visual data exploration, web usage mining, frequent access patterns, information visualization},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013493,
author = {Akavipat, R. and Wu, L-S. and Menczer, F.},
title = {Small World Peer Networks in Distributed Web Search},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013493},
doi = {10.1145/1013367.1013493},
abstract = {In ongoing research, a collaborative peer network application is being proposed to address the scalability limitations of centralized search engines. Here we introduce a local adaptive routing algorithm used to dynamically change the topology of the peer network based on a simple learning scheme driven by query response interactions among neighbors. We test the algorithm via simulations with 70 model users based on actual Web crawls. We find that the network topology rapidly converges from a random network to a small world network, with emerging clusters that match the user communities with shared interests.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {396–397},
numpages = {2},
keywords = {topical crawlers, peer collaborative search, small world},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013494,
author = {Sumiya, Kazutoshi and Munisamy, Mahendren and Tanaka, Katsumi},
title = {Tv2web: Generating and Browsing Web with Multiple Lod from Video Streams and Their Metadata},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013494},
doi = {10.1145/1013367.1013494},
abstract = {We propose a method of automatically constructing Web content from video streams with metadata that we call TV2Web. The Web content includes thumbnails of video units and caption data generated from metadata. Users can watch TV ona normal Web browser. They can also manipulate Web content with zooming metaphors to seamlessly alter the level of detail (LOD) of the content being viewed. They can search for favorite scenes faster than with analog video equipment, and experience a new cross-media environment. We also developed a prototype of the TV2Web system and discuss its implementation.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {398–399},
numpages = {2},
keywords = {web browser from video streams and their metadat, metadata, generation of Web content, video stream, level of detail},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013495,
author = {Roussinov, Dmitri and Robles, Jose},
title = {Self-Learning Web Question Answering System},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013495},
doi = {10.1145/1013367.1013495},
abstract = {While being quite successful in providing keyword based access to web pages, commercial search portals, such as Google, Yahoo, AltaVista, and AOL, still lack the ability to answer questions expressed in a natural language. In this paper, we present a probabilistic approach to automated question answering on the Web. Our approach is based on pattern matching and answer triangulation. By taking advantage of the redundancy inherent in the Web, each answer found by the system is triangulated (confirmed or disconfirmed) against other possible answers. Our approach is entirely self-learning: it does not involve any linguistic resources, nor it does require any manual tuning. Thus, the propose approach can easily be replicated in other information systems with large redundancy.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {400–401},
numpages = {2},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013496,
author = {Gupta, Vipul and Stebila, Douglas and Shantz, Sheueling Chang},
title = {Integrating Elliptic Curve Cryptography into the Web's Security Infrastructure},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013496},
doi = {10.1145/1013367.1013496},
abstract = {RSA is the most popular public-key cryptosystem on the Web today but long-term trends such as the proliferation of smaller, simpler devices and increasing security needs will make continued reliance on RSA more challenging over time. We offer Elliptic Curve Cryptography (ECC) as a suitable alternative and describe our integration of this technology into several key components of the Web's security infrastructure. We also present experimental results quantifying the benefits of using ECC for secure web transactions.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {402–403},
numpages = {2},
keywords = {apache, elliptic curve cryptography, mozilla, openSSL},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013497,
author = {Li, Hua-Fu and Lee, Suh-Yin and Shan, Man-Kwan},
title = {On Mining Webclick Streams for Path Traversal Patterns},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013497},
doi = {10.1145/1013367.1013497},
abstract = {Mining user access patterns from a continuous stream of Web-clicks presents new challenges over traditional Web usage mining in a large static Web-click database. Modeling user access patterns as maximal forward references, we present a single-pass algorithm StreamPath for online discovering frequent path traversal patterns from an extended prefix tree-based data structure which stores the compressed and essential information about user's moving histories in the stream. Theoretical analysis and performance evaluation show that the space requirement of StreamPath is limited to a logarithmic boundary, and the execution time, compared with previous multiple-pass algorithms [2], is fast.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {404–405},
numpages = {2},
keywords = {path traversal patterns, data stream mining, web-click streams},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013498,
author = {Hoi, Chu-Hong and Lyu, Michael R.},
title = {Web Image Learning for Searching Semantic Concepts in Image Databases},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013498},
doi = {10.1145/1013367.1013498},
abstract = {Without textual descriptions or label information of images, searching semantic concepts in image databases is still a very challenging task. While automatic annotation techniques are yet along way off, we can seek other alternative techniques to solve this difficult issue. In this paper, we propose to learn Web images for searching the semantic concepts in large image databases. To formulate effective algorithms, we suggest to engage the support vector machines for attacking the problem. We evaluate our algorithm in a large image database and demonstrate the preliminary yet promising results.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {406–407},
numpages = {2},
keywords = {image retrieval, web image learning, relevance feedback, semantic searching, support vector machine},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013499,
author = {Di Fabbrizio, Giuseppe and Lewis, Charles},
title = {An Xpath-Based Discourse Analysis Module for Spoken Dialogue Systems},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013499},
doi = {10.1145/1013367.1013499},
abstract = {This paper describes an XPath-based discourse analysis module for Spoken Dialogue Systems that allows the dialogue author to easily manipulate and query both the user input's semantic representation and the dialogue context using a simple and compact formalism. We show that, in managing the human-machine interaction, the discourse context and the dialogue history are effectively represented as Document Object Model (DOM) structures. DOM defines interfaces that dialogue scripts can use to dynamically access and update the content, the structure and the style of the documents. In general, this approach applies also to richer multimedia and multimodal interactions where the interpretation of the user input depends on a combination of input modalitie.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {408–409},
numpages = {2},
keywords = {discourse analysis, spoken dialogue systems, XPath},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013500,
author = {Knapp, Michael B. and Dexter, Sara and McLaughlin, Robert},
title = {Metadata Co-Development: A Process Resulting in Metadata about Technical Assistance to Educators},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013500},
doi = {10.1145/1013367.1013500},
abstract = {Metadata development can be challenging because the vocabulary should be flexible and extensible, widely applicable, interoperable, and both machine and human readable. We describe how we engaged members of organizations in the field of technical assistance to educators in a process of metadata development, and the challenges we faced. The result was a an ontology for the communities of practice that is interoperable and can evolve it was then used to catalogue resources for dissemination via the Semantic Web.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {410–411},
numpages = {2},
keywords = {resource cataloging, metadata, education, RDF, technical assistance, semantic web},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013501,
author = {Carroll, Jeremy J. and Stickler, Patrick},
title = {RDF Triples in XML},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013501},
doi = {10.1145/1013367.1013501},
abstract = {RDF/XML does not layer RDF on top of XML ina useful way. We use a simple direct representation of the RDF abstract syntax in XML. We add the ability to name graphs, noting that in practice this is already widely used. We use XSLT as a general syntactic extensibility mechanism to provide human friendly macros for our syntax. This provides a simple serialization solving a persistent problem in the Semantic Web.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {412–413},
numpages = {2},
keywords = {XML, semantic web, RDF},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013502,
author = {He, Hai and Meng, Weiyi and Yu, Clement and Wu, Zonghuan},
title = {Automatic Extraction of Web Search Interfaces for Interface Schema Integration},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013502},
doi = {10.1145/1013367.1013502},
abstract = {This paper provides an overview of a technique for extracting information from the Web search interfaces of e-commerce search engines that is useful for supporting automatic search interface integration. In particular, we discuss how to group elements and labels on a search interface into attributes and how to derive certain meta-information for each attribute.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {414–415},
numpages = {2},
keywords = {search engine, metasearch engine, search interface extraction, search interface representation},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013503,
author = {Peng, Qian and Meng, Weiyi and He, Hai and Yu, Clement},
title = {Clustering E-Commerce Search Engines},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013503},
doi = {10.1145/1013367.1013503},
abstract = {In this paper, we sketch a method for clustering e-commerce search engines by the type of products/services they sell. This method utilizes the special features of interface pages of such search engines. We also provide an analysis of different types of ESE interface pages.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {416–417},
numpages = {2},
keywords = {search engine categorization, document clustering},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013504,
author = {Hyv\"{o}nen, Eero and Junnila, Miikka and Kettula, Suvi and M\"{a}kel\"{a}, Eetu and Saarela, Samppa and Salminen, Mirva and Syreeni, Ahti and Valo, Arttu and Viljanen, Kim},
title = {Publishing Museum Collections on the Semantic Web: The Museumfinland Portal},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013504},
doi = {10.1145/1013367.1013504},
abstract = {Museum collections contain large amounts of data and semantically rich, mutually interrelated metadatain heterogeneous databases. The publication of museum collections on the web is therefore a very promising application domain for semantic web techniques. We present a semantic web portal called MuseumFinland - Finnish Museums on the Semantic Web1" [3]that contains some 4,000 cultural artifacts from the collections of three museums using three different database schemas and database systems. The system is based on seven RDF(S) ontologies consisting of some 10,000 classes and individuals.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {418–419},
numpages = {2},
keywords = {ontology, semantic web, content publishing},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013505,
author = {Kim, Hak Lae and Kim, Hong Gee and Park, Kyung-Mo},
title = {Ontalk: Ontology-Based Personal Document Management System},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013505},
doi = {10.1145/1013367.1013505},
abstract = {In this paper, we present our development of a document management and retrieval tool, which is named Ontalk. Our system provides a semi-automatic metadata generator and an ontology-based search engine for electronic documents. Ontalk can create or import various ontologies in RDFS or OWL for describing the metadata. Our system that is built upon. NET technology is easily communicated with or flexibly plugged into many different programs.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {420–421},
numpages = {2},
keywords = {document management, knowledge management, inference etc., ontology},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013506,
author = {Attardi, Giuseppe and Esuli, Andrea and Simi, Maria},
title = {Best Bets: Thousands of Queries in Search of a Client},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013506},
doi = {10.1145/1013367.1013506},
abstract = {A number of applications require selecting targets for specific contents on the basis of criteria defined by the contents providers rather than selecting documents in response to user queries, as in ordinary information retrieval. We present a class of retrieval systems, called Best Bets, that generalize Information Filtering and encompass a variety of applications including editorial suggestions, promotional campaigns and targeted advertising, such as Google AdWords™. We developed techniques for implementing Best Bets systems addressing performance issues for large scale deployment as efficient query search, incremental updates and dynamic ranking.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {422–423},
numpages = {2},
keywords = {information filtering, query, search, proactive content delivery, information retrieval},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013507,
author = {Zhou, Nianjun and Mihaila, George and Meliksetian, Dikran},
title = {XML Data Mediator Integrated Solution for Xml Roundtrip from Xml to Relational},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013507},
doi = {10.1145/1013367.1013507},
abstract = {This paper presents a system for efficient data transformations between XML and relational databases, called XML Data Mediator (XDM). XDM enables the transformation by externalizing the specification of the mapping in a script and using an efficient run-time engine that automates the conversion task. The runtime engine is independent from the mapping script. A parser converts a mapping script into an internal conversion object. For the mapping from relational to XML, we use a tagging tree as a conversion object inside the runtime engine, and use an SQL outer-join scheme to combine multiple SQL queries in order to reduce the number of backend relational database accesses. For the mapping from XML to relational, the conversion object is a shredding tree, and we use an innovative algorithm to process the XML as a stream in order to achieve linear complexity with respect to the size of the XML document.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {424–425},
numpages = {2},
keywords = {XSL, relational database, RDBMS, shredding, XML},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013508,
author = {Silver, Mark S. and Ward, Sidne G.},
title = {Browser-Based Applications: Positive Transference or Interference?},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013508},
doi = {10.1145/1013367.1013508},
abstract = {Applications that run on top of web browsers dominate the Internet today. Given the many similarities among these applications' features, positive transference from one to another is often seen as an important source of ease-of-use for such applications. This paper examines the many differences in the way similar features are implemented in different browser-based applications, analyzing the way these inconsistencies can lead to negative transference (interference) that degrades rather than enhances usability.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {426–427},
numpages = {2},
keywords = {transference, interference, browser-based applications, usability},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013509,
author = {Perry, M. S. and Stiles, E.},
title = {SEMPL: A Semantic Portal},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013509},
doi = {10.1145/1013367.1013509},
abstract = {Semantic Web technology is intended for the retrieval, collection, and analysis of meaningful data with significant automation afforded by machine understandability of data [1]. As one illustration of semantic web technology in action, we present SEMPL, a semantic web portal for the Large Scale Distributed Information Systems lab (LSDIS) at the University of Georgia. SEMPL, which is powered by a state of the art commercial system, Semagix Freedom [7], uses an ontology-driven approach to provide semantic browsing, linking, and contextual querying of content within the portal. By using the ontology based information integration technique, SEMPL can specify the context of a particular piece of research information, annotate web pages, and provide links to semantically related areas enabling a rich contextual retrieval of information.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {428–429},
numpages = {2},
keywords = {semantic web, ontology, semantic portal},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013510,
author = {Andronico, Patrizia and Buzzi, Marina and Leporini, Barbara},
title = {Can I Find What I'm Looking For?},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013510},
doi = {10.1145/1013367.1013510},
abstract = {In recent years, search engine research has grown rapidly in areas such as algorithms, strategies and architecture, increasing both effectiveness and quality of results. However, a very important aspect that is often neglected is the user interface. In this work we analyzed the interfaces of several popular search tools from the user's point of view, and collected individual feedback in order to determine whether it is possible to improve interface design.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {430–431},
numpages = {2},
keywords = {accessibility, user interface, search engine, usability},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013511,
author = {Bade, Dirk and N\"{u}ssel, Georg and Wilts, Gerd},
title = {Online Feedback by Tests and Reporting for Elearning and Certification Programs},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013511},
doi = {10.1145/1013367.1013511},
abstract = {The evaluation of eLearning success is an indispensable business requirement of education programs: the easy registration of `visits' to eLearning websites is, however, not sufficient in most cases. Additional metrics from authenticated logins and reports of learning activity and success - as obtained from specific online tests ' are required. The aim is to document the acceptance, progress and return of investment (ROI) of eLearning programs, and set up additional training well tailored to the needs of a specific learning community. An example from a corporate certification program proves the applicability of the proposed processes.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {432–433},
numpages = {2},
keywords = {online tests, eLearning, blended learning},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@dataset{10.1145/review-1013367.1013511_R38343,
author = {Retalis, Symeon D.},
title = {Review ID:R38343 for DOI: 10.1145/1013367.1013511},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1013367.1013511_R38343}
}

@inproceedings{10.1145/1013367.1013512,
author = {Simon, Rainer and Kapsch, Michael Jank and Wegscheider, Florian},
title = {A Generic Uiml Vocabulary for Device- and Modality Independent User Interfaces},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013512},
doi = {10.1145/1013367.1013512},
abstract = {We present in this poster our work on a User Interface Markup Language (UIML) vocabulary for the specification of device- and modality independent user interfaces. The work presented here is part of an application-oriented project. One of the results of the project is a prototype implementation of a generic platform for device independent multimodal mobile applications. The poster presents the requirements for a generic user interface description format and explains our approach on an integrated description of user interfaces for both graphical and voice modality. A basic overview of the vocabulary structure, its language elements and main features is presented.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {434–435},
numpages = {2},
keywords = {UIML, mobile networks, mobile devices, voice interfaces, generic user interface description, multimodality, device-independence, multimodal user interfaces},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013513,
author = {Caragea, Doina and Syeda-Mahmood, Tanveer},
title = {Semantic Api Matching for Automatic Service Composition},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013513},
doi = {10.1145/1013367.1013513},
abstract = {In this paper, we address the problem of matching I/O descriptions of services to enable their automatic service composition. Specifically, we develop a method of semantic schema matching and apply it to the API schemas constituting the I/O descriptions of services. The algorithm assures an optimal match of corresponding entities by obtaining a maximum matching in a bi-partite graph formed from the attributes.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {436–437},
numpages = {2},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013514,
author = {Oinn, Tom and Addis, Matthew and Ferris, Justin and Marvin, Darren and Greenwood, Mark and Goble, Carole and Wipat, Anil and Li, Peter and Carver, Tim},
title = {Delivering Web Service Coordination Capability to Users},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013514},
doi = {10.1145/1013367.1013514},
abstract = {As web service technology matures there is growing interest in exploiting workflow techniques to coordinate web services. Bioinformaticians are a user community who combine web resources to perform in silico experiments. These users are scientists and not information technology experts they require workflow solutions that have a low cost of entry for service users and providers. Problems satisfying these requirements with current techniques led to the development of the Simple conceptual unified flow language (Scufl). Scufl is supported by the Freefluo enactment engine [1], and the Taverna editing workbench [3]. The extensibility of Scufl, supported by these tools, means that workflows coordinating web services can be matched to how users view their problems. The Taverna workbench exploits the web to keep Scufl simple by retrieving detail from URIs when required, and by scavenging the web for services. Scufl and its tools are not bioinformatics specific. They can be exploited by other communities who require user-driven composition and execution of workflows coordinating web resources.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {438–439},
numpages = {2},
keywords = {bioinformatics, web programming, scientific workflows, e-science, web services, web service coordination},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013515,
author = {Li, Xiaoli and Liu, Bing},
title = {Dealing with Different Distributions in Learning From},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013515},
doi = {10.1145/1013367.1013515},
abstract = {In the problem of learning with positive and unlabeled examples, existing research all assumes that positive examples P and the hidden positive examples in the unlabeled set U are generated from the same distribution. This assumption may be violated in practice. In such cases, existing methods perform poorly. This paper proposes a novel technique A-EM to deal with the problem. Experimental results with product page classification demonstrate the effectiveness of the proposed technique.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {440–441},
numpages = {2},
keywords = {classification, positive and unlabeled learning},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013516,
author = {Dong, Jin Song and Li, Yuan Fang and Wang, Hai},
title = {TCOZ Approach to Semantic Web Services Design},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013516},
doi = {10.1145/1013367.1013516},
abstract = {Complex Semantic Web (SW) services may have intricate data state, autonomous process behavior and concurrent interactions. The design of such SW service systems requires precise and powerful modelling techniques to capture not only the ontology domain properties but also the services' process behavior and functionalities. In this paper we apply an integrated formal modeling language, Timed Communicating Object Z (TCOZ), to design SW services. Furthermore, the paper presents the development of the systematic translation rules and tools which can automatically extract the SW ontology and services semantic markup from the formal TCOZ design model.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {442–443},
numpages = {2},
keywords = {formal methods, semantic web, TCOZ, DAML+OIL, DAML-S},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013517,
author = {Fujima, Jun and Lunzer, Aran and Hornb\ae{}k, Kasper and Tanaka, Yuzuru},
title = {C3W: Clipping, Connecting and Cloning for the Web},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013517},
doi = {10.1145/1013367.1013517},
abstract = {Many of today's Web applications support just simple trial-and error retrievals: supply one set of parameters, obtain one set of results. For a user who wants to examine a number of alternative retrievals, this form of interaction is inconvenient and frustrating. It can be hard work to keep finding and adjusting the parameter specification widgets buried in a Web page, and to remember or record each result set. Moreover, when using diverse Web applicationsin combination - transferring result data from one into the parameters for another - the lack of an easy way to automate that transfer merely increases the frustration. Our solution is to integrate techniques for each of three key activities: clipping elements from Web pages to wrap an application; connecting wrapped applications using spreadsheet-like formulas; and cloning the interfaceelements so that several sets of parameters and results may behandled in parallel. We describe a prototype that implements this solution, showing how it enables rapid and flexible exploration ofthe resources accessible through user-chosen combinations of Web applications. Our aim in this work is to contribute to research on making optimal use of the wealth of information on the Web, by providing interaction techniques that address very practical needs.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {444–445},
numpages = {2},
keywords = {Web navigation, interfaces, intelligentPad, subjunctive, Web application linkage},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013518,
author = {Pahl, Claus and Barrett, Ronan},
title = {A Web Services Architecture for Learning Object Discovery and Assembly},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013518},
doi = {10.1145/1013367.1013518},
abstract = {Courseware systems are often based on an assembly of different components, addressing the different needs of storage and delivery functionality. The Learning Technology Standard Architecture LTSA provides a generic architectural framework for these systems. Recent developments in Web technology -- e.g. the Web services framework -- have greatly enhanced the flexible and interoperable implementation of courseware architectures.We argue that in order to make the Web services philosophy work, two enhancements to the LTSA approach are required. Firstly, a combination with metadata annotation is needed to support the discovery of educational Web services. Secondly, if these components are to be provided in form of services, more support is needed for their assembly. Architectural patterns of a finer degree of granularity shall satisfy this need.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {446–447},
numpages = {2},
keywords = {discovery, interface descriptions, architecture, teaching and learning environments, web services, assembly, metadata},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013519,
author = {Yu, Philip S. and Li, Xin and Liu, Bing},
title = {On the Temporal Dimension of Search},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013519},
doi = {10.1145/1013367.1013519},
abstract = {Web search is probably the single most important application on the Internet. The most famous search techniques are perhaps the PageRank and HITS algorithms. These algorithms are motivated by the observation that a hyperlink from a page to another is an implicit conveyance of authority to the target page. They exploit this social phenomenon to identify quality pages, e.g., "authority" pages and "hub" pages. In this paper we argue that these algorithms miss an important dimension of the Web, the temporal dimension. The Web is not a static environment. It changes constantly. Quality pages in the past may not be quality pages now or in the future. These techniques favor older pages because these pages have many in-links accumulated over time. New pages, which may be of high quality, have few or no in-links and are left behind. Bringing new and quality pages to users is important because most users want the latest information. Research publication search has exactly the same problem. This paper studies the temporal dimension of search in the context of research publication search. We propose a number of methods deal with the problem. Our experimental results show that these methods are highly effective.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {448–449},
numpages = {2},
keywords = {web search, publication search, temporal dimension of search},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013520,
author = {Magenheim, Johann and Scheel, Olaf},
title = {Integrating Learning Objects into an Open Learning Environment: Evaluation of Learning Processes in an Informatics Learning Lab},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013520},
doi = {10.1145/1013367.1013520},
abstract = {The Didactics of Informatics research group at the University of Paderborn is involved in efforts to design implement and evaluate a web-based learning laboratory for informatics (ILL). The ILL mainly serves the purpose of an open interactive learning environment for software engineering. The poster presentation shows the main components of an ILL and the types of media that are used. A didactical concept, learning strategies and the efforts to create self-organizing learning communities in the ILL are also topics of the poster. Finally, an evaluation concept will be presented including some basic results of empirical research which was done during a seminar held in the summer term 2003.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {450–451},
numpages = {2},
keywords = {informatics learning lab, blended learning, learning communities, computer-based exploration environment, deconstruction of software, learning objects},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013521,
author = {Menczer, Filippo},
title = {Combining Link and Content Analysis to Estimate Semantic Similarity},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013521},
doi = {10.1145/1013367.1013521},
abstract = {Search engines use content and link information to crawl, index, retrieve, and rank Web pages. The correlations between similarity measures based on these cues and on semantic associations between pages therefore crucially affects the performance of any search tool. Here I begin to quantitatively analyze the relationship between content, link, and semantic similarity measures across a massive number of Web page pairs. Maps of semantic similarity across textual and link similarity highlight the potential and limitations of lexical and link analysis for relevance approximation, and provide us with a way to study whether and how text and link based measures should be combined.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {452–453},
numpages = {2},
keywords = {recall, precision, Web search, content and link similarity, semantic maps},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013522,
author = {Tomita, Junji and Nakawatase, Hidekazu and Ishii, Megumi},
title = {Graph-Based Text Database for Knowledge Discovery},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013522},
doi = {10.1145/1013367.1013522},
abstract = {While we expect to discover knowledge in the texts available on the Web, such discovery usually requires many complex analysis steps, most of which require different text handling operations such as similar text search or text clustering. Drawing an analogy from the relational data model, we propose a text representation model that simplifies the steps. The model represents texts in a formal manner, Subject Graphs, described herein, provides text handling operations whose inputs and outputs are identical in form, i.e. a set of subject graphs. We develop a graph-based text database, which is based on the model, and an interactive knowledge discovery system. Trials of the system show that it allows the user to interactively and intuitively discover knowledge in Web pages by combining text handling operations defined on subject graphs in various orders.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {454–455},
numpages = {2},
keywords = {interactive search, subject graphs, knowledge discovery},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013523,
author = {Helic, Denis and Maurer, Hermann and Scerbakov, Nick},
title = {Combining Individual Tutoring with Automatic Course Sequencing in WBT Systems},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013523},
doi = {10.1145/1013367.1013523},
abstract = {Usually, the success of systems using automatic course sequencing depends strongly on careful authoring and foreseeing of all curriculum alternatives before any learning session even starts. We believe that tutors, starting from a simple generic curriculum, and assuming that they have the proper tools, can much easier create curriculum alternatives as immediate response to the current learning situation. In this paper we present a tool that provides a flexible environment for tutors allowing them to customize, and develop the curriculum on-the-fly. However, since individual tutoring is quite expensive we shortly discuss possibilities for enabling automatic adjustment of course curriculum to learners' needs by combining on-the-fly curriculum alternatives created by tutors with well-known automatic course sequencing techniques.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {456–457},
numpages = {2},
keywords = {course sequencing, WBT, course curriculum, tutoring},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013524,
author = {Nadamoto, Akiyo and Tanaka, Katsumi},
title = {Time-Based Contextualized-News Browser (t-Cnb)},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013524},
doi = {10.1145/1013367.1013524},
abstract = {We propose a new way of browsing contextualized-news articles. Our prototype browser system is called a Time-based Contextualized-News Browser (T-CNB). The T-CNB concurrently and automatically presents a series of related pages for one news source while browsing the user-specified page. It extracts the past related pages from a user-specified news articles on the web. The related pages outline the progress of user-specified news articles. We call the related pages 'contextual pages'.Using the T-CNB, a user only needs to specify one news article on the web. The user then automatically receives past related news articles, which provide a wider understanding of the topic. The T-CNB automatically generates and presents contextualized news articles.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {458–459},
numpages = {2},
keywords = {web browser, contextualized news articles, topic graph},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013525,
author = {Xue, Gui-Rong and Zeng, Hua-Jun and Chen, Zheng and Ma, Wei-Ying and Yu, Yong},
title = {Similarity Spreading: A Unified Framework for Similarity Calculation of Interrelated Objects},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013525},
doi = {10.1145/1013367.1013525},
abstract = {In many Web search applications, similarities between objects of one type (say, queries) can be affected by the similarities between their interrelated objects of another type (say, Web pages), and vice versa. We propose a novel framework called similarity spreading to take account of the interrelationship and improve the similarity calculation. Experiment results show that the proposed framework can significantly improve the accuracy of the similarity measurement of the objects in a search engine.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {460–461},
numpages = {2},
keywords = {mutual reinforcement, interrelated, similarity spreading},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013526,
author = {Zhang, Li and Ardagna, Danilo},
title = {SLA Based Profit Optimization in Web Systems},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013526},
doi = {10.1145/1013367.1013526},
abstract = {With the rapid growth of eBusiness, the Web services are becoming a commodity. To reduce the management cost for the IT infrastructure, companies often outsource their IT services to third party service providers. Large service centers have been setup to provide services to many customers by sharing the IT resources. This leads to the efficient use of resources and a reduction of the operating cost. The service provider and their customers often negotiate utility based Service Level Agreements (SLAs) to determine the cost and penalty based on the achieved performance level. The system is based on a centralized controller which can control the request volumes at various servers and the scheduling policy at each server. The controller can also decide to turn ON or OFF servers depending on the system load. This paper designs a resource allocation scheduler for such web environments so as to maximize the profits associated with multiple class SLAs.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {462–463},
numpages = {2},
keywords = {utility function, resource allocation, load balancing, quality of service, SLA optimization},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013527,
author = {Barrett, Ronan and Delany, Sarah Jane},
title = {OpenMVC: A Non-Proprietry Component-Based Framework for Web Applications},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013527},
doi = {10.1145/1013367.1013527},
abstract = {The lack of standardised approaches in the development of web-based systems is an ongoing issue for the developers of commercial software. To address this issue we proposes a hybrid development framework for web-based solutions that combines much of the best attributes of existing frameworks but utilises open, standardised W3C technologies where possible. This framework called openMVC is an evolution of the Model-View-Controller (MVC) pattern. An implementation of openMVC has been built over a 5-tier architecture using Java and .NET.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {464–465},
numpages = {2},
keywords = {XSLT, frameworks, W3C, XML, XML schema, patterns, MVC, web services},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013528,
author = {Rutledge, Lloyd and van Ossenbruggen, Jacco and Hardman, Lynda},
title = {Structuring and Presenting Annotated Media Repositories},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013528},
doi = {10.1145/1013367.1013528},
abstract = {We generate hypermedia presentations from annotated media repositories using simple document structure as an intermediate phase. This poster applies Web style technologies to this process. Results include style specification for accessing semantically annotated media repositories, for determining document structure from semantic structure and for applying this document structure to the final presentation.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {466–467},
numpages = {2},
keywords = {semantics, XHTML+SMIL, style, XSLT, document structure, RDF},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013529,
author = {Papapetrou, Odysseas and Samaras, George},
title = {Distributed Location Aware Web Crawling},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013529},
doi = {10.1145/1013367.1013529},
abstract = {Distributed crawling has shown that it can overcome important limitations of the today's crawling paradigm. However, the optimal benefits of this approach are usually limited to the sites hosting the crawler. In this work, we propose a location-aware method, called IPMicra, that utilizes an IP address hierarchy, and allows crawling of links in a near optimal location aware manner.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {468–469},
numpages = {2},
keywords = {location aware web crawling, distributed web crawling},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013530,
author = {Tang, Wei and Jones, Kipp and Liu, Ling and Pu, Calton},
title = {BizCQ: Using Continual Queries to Cope with Changes in Business Information Exchange},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013530},
doi = {10.1145/1013367.1013530},
abstract = {In this poster, we propose the framework of BizCQ, a system to apply Continual Queries [7][8] on Web-based content to manage information exchanges between two business partners. In this poster, we describe ways to leverage previous research in Web monitoring techniques applied to the everyday problem of managing change within a business environment, and focus on the difficulties of managing changes that are caused by external parties in business-to-business (B2B) information exchanges.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {470–471},
numpages = {2},
keywords = {continual query, B2B, business-to-business, information quality, change response, semantic web},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013531,
author = {Ding, Dawei and Yang, Jun and Li, Qing and Wang, Liping and Wenyin, Liu},
title = {Towards a Flash Search Engine Based on Expressive Semantics},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013531},
doi = {10.1145/1013367.1013531},
abstract = {Flash, as a multimedia format, becomes more and more popular on the Web. However, previous works on Flash are totally based on low-level features, which make it unpractical to build a content-based Flash search engine. To address this problem, our paper proposes expressive semantics for bridging the gap between low-level features and user queries. To smoothly incorporate expressive semantics into a search engine, an eigenvector-based model is devised to map a user query to expressive semantics with the aid of link analysis method. Our experiment results confirm that expressive semantics is a promising approach to understanding and hence searching Flash movies more efficiently.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {472–473},
numpages = {2},
keywords = {flash retrieval, eigenvector, search engine, expressive semantics, web application, classification},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013532,
author = {Kodaganallur, Viswanathan and Weitz, Rob R. and Rosenthal, David},
title = {VersaTutor: Architecture for a Constraint-Based Intelligent Tutor Generator},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013532},
doi = {10.1145/1013367.1013532},
abstract = {Intelligent tutoring systems have demonstrated their utility in a variety of domains. However, they are notoriously resource intensive to build. We report here on the development of a software tool that enables non-software developers to declaratively create intelligent tutors. This intelligent tutor generator creates applications with rich user interaction and powerful theory-based remediation capabilities. It utilizes the Constraint Based Tutoring paradigm and is generic enough to create tutors in several domains. It is easily extensible through plug-ins.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {474–475},
numpages = {2},
keywords = {constraint based tutors, intelligent tutoring, instructional technology, model tracing tutors, distance learning},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013533,
author = {Buttler, David and Rocco, Daniel and Liu, Ling},
title = {Efficient Web Change Monitoring with Page Digest},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013533},
doi = {10.1145/1013367.1013533},
abstract = {The Internet and the World Wide Web have enabled a publishing explosion of useful online information, which has produced the unfortunate side effect of information overload: it is increasingly difficult for individuals to keep abreast of fresh information. In this paper we describe an approach for building a system for efficiently monitoring changes to Web documents. This paper has three main contributions. First, we present a coherent framework that captures different characteristics of Web documents. The system uses the Page Digest encoding to provide a comprehensive monitoring system for content, structure, and other interesting properties of Web documents. Second, the Page Digest encoding enables improved performance for individual page monitors through mechanisms such as short-circuit evaluation, linear time algorithms for document and structure similarity, and data size reduction. Finally, we develop a collection of sentinel grouping techniques based on the Page Digest encoding to reduce redundant processing in large-scale monitoring systems by grouping similar monitoring requests together. We examine how effective these techniques are over a wide range of parameters and have seen an order of magnitude speed up over existing Web-based information monitoring systems.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {476–477},
numpages = {2},
keywords = {document storage, web document monitoring, scalability},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013534,
author = {Oroumchian, Farhad and Darrudi, Ehsan and Taghiyareh, Fattane and Angoshtari, Neeyaz},
title = {Experiments with Persian Text Compression for Web},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013534},
doi = {10.1145/1013367.1013534},
abstract = {The increasing importance of Unicode for text encoding implies a possible doubling of data storage space and data transmission time, with a corresponding need for data compression. The approach presented in this paper aims to reduce the storage and the transmission time for Persian text files in web-based applications and Internet. The basic idea here is to compute the most repetitive n-grams in the Persian text and replace them by a single character in the user-defined sections of the Unicode. The compression will be done on the server side once and the decompression process is eliminated completely. The rendering process in the browser will do the decompression. There is no need for any additional program or add-ins for decompression to be installed on the browser or client side. The user needs only to download the proper Unicode font once. A genetic algorithm is utilized to select the most appropriate n-grams. In the best case, we have achieved 52.26 % reduction of the file size. The method is general, and applies equally well to English and other languages.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {478–479},
numpages = {2},
keywords = {Farsi, genetic algorithm, n-gram compression, unicode},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013535,
author = {Dewan, Prashant and Dasgupta, Partha},
title = {Pride: Peer-to-Peer Reputation Infrastructure for Decentralized Environments},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013535},
doi = {10.1145/1013367.1013535},
abstract = {Peer-to-peer (P2P) networks use the fundamental assumption that the nodes in the network will cooperate and will not cheat. In the absence of any common goals shared by the nodes of a peer-to-peer network, external motivation to cooperate and be trustworthy is mandated. Digital Reputations can be used to inject trust among the nodes of a network. This paper presents PRIDE, a reputation system for decentralized peer-to-peer networks. PRIDE uses self-certification a scheme for identification of peers using digital certificates similar to SDSI certificates, an elicitation-storage protocol for exchange of recommendations and IP Based Safeguard (IBS) to mitigate a peer's vulnerability to 'liar farms.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {480–481},
numpages = {2},
keywords = {reputation systems, peer-to-peer, security},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013536,
author = {Kalnis, Panos and Ng, Wee Siong and Ooi, Beng Chin and Tan, Kian-Lee},
title = {Answering Similarity Queries in Peer-to-Peer Networks},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013536},
doi = {10.1145/1013367.1013536},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {482–483},
numpages = {2},
keywords = {image, peer-to-peer, similarity},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013537,
author = {Broder, Andrei Z. and Lempel, Ronny and Maghoul, Farzin and Pedersen, Jan},
title = {Efficient Pagerank Approximation via Graph Aggregation},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013537},
doi = {10.1145/1013367.1013537},
abstract = {We present a framework for approximating random-walk based probability distributions over Web pages using graph aggregation. We (1) partition the Web's graph into classes of quasi-equivalent vertices, (2) project the page-based random walk to be approximated onto those classes, and (3) compute the stationary probability distribution of the resulting class-based random walk. From this distribution we can quickly reconstruct a distribution on pages. Inparticular, our framework can approximate the well-known PageRank distribution by setting the classes according to the set of pages on each Web host. We experimented on a Web-graph containing over 1.4 billion pages, and were able to produce a ranking that has Spearman rank-order correlation of 0.95 with respect to PageRank. A simplistic implementation of our method required less than half the running time of a highly optimized implementation of PageRank, implying that larger speedup factors are probably possible.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {484–485},
numpages = {2},
keywords = {web information retrieval, link analysis, search engines},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013538,
author = {Acharyya, Sreangsu and Ghosh, Joydeep},
title = {Outlink Estimation for Pagerank Computation under Missing Data},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013538},
doi = {10.1145/1013367.1013538},
abstract = {The enormity and rapid growth of the web-graph forces quantities such as its pagerank tobe computed under missing information consisting of outlinks of pages that have not yet been crawled. This paper examines the role played by the size and distribution of this missing data in determining the accuracy of the computed pagerank, focusing on questions such as (i) the accuracy of pageranks under missing information, (ii) the size at which a crawl process may be aborted while still ensuring reasonable accuracy of pageranks, and (iii) algorithms to estimate pageranks under such missing information. Thefirst couple of questions are addressed on the basis of certain simple bounds relating the expected distance between the true and computed pageranks and the size of the missing data. The third question is explored by devising algorithms to predict the pageranks when full information is not available. A key feature of the "dangling link estimation" and "clustered link estimation" algorithms proposed is that, they do not need to run the pagerank iteration afresh once the outlinks have been estimated.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {486–487},
numpages = {2},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013539,
author = {Gasevic, Dragan and Djuric, Dragan and Devedzic, Vladan and Damjanovi, Violeta},
title = {Converting UML to OWL Ontologies},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013539},
doi = {10.1145/1013367.1013539},
abstract = {This paper presents automatic generation of the Web Ontology Language (OWL) from an UML model. The solution is based on an MDA-defined architecture for ontology development and the Ontology UML Profile (OUP). A conversion, that we present here, transforms an ontology from its OUP definition (i.e. XML Metadata Interchange -- XMI) into OWL description. Accordingly, we illustrate how an OUP-developed ontology can be shared with ontological engineering tools (i.e. Prot\'{e}g\'{e}).},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {488–489},
numpages = {2},
keywords = {OWL, XSLT, UML profiles, ontology},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013540,
author = {Yesilada, Yeliz and Harper, Simon and Goble, Carole and Stevens, Robert},
title = {DANTE: Annotation and Transformation of Web Pages for Visually Impaired Users},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013540},
doi = {10.1145/1013367.1013540},
abstract = {Most Web pages are designed for visual interaction so the mobility, or ease of travel, of visually impaired Web travellers is reduced [2]. Objects that support travel and mobility are not in an appropriate form for nonvisual interaction. Our goal is to enhance the mobility of visually impaired Web travellers by annotating pages with a travel ontology that aims to encapsulate rich structural and navigational knowledge. We propose a semi-automated tool 'Dante' which aims to analyse Web pages to extract travel objects, discover their roles, annotate them with a travel ontology and transform pages based on the annotations to enhance the provided mobility support. This poster introduces the travel ontology and presents how Web pages are annotated with this ontology to guide the transformations.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {490–491},
numpages = {2},
keywords = {mobility, tool, visual impairment, travel, semantic annotation},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013541,
author = {Ianni, Giovambattista and Ricca, Francesco and Calimeri, Francesco and Lio, Vincenzino and Galizia, Stefania},
title = {An Agent System Reasoning about the Web and the User},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013541},
doi = {10.1145/1013367.1013541},
abstract = {The paper describes some innovations related to the ongoing work on the GSA prototype, an integrated information retrieval agent. In order to improve the original system effectiveness, we propose the GSA2 system, introducing a new internal architecture based on a message-passing framework and on an ontology description formalism (WOLF, Web ontology Framework). GSA2 is conceived in order to describe and easily perform reasoning on "facts about the web and the user". The most innovative aspect of the project is its customizable and flexible reasoning system, based on Answer Set Programming it plays the role of the central decision making module, and allows the Agent to take proactive decisions. The introduction of a logic language allows one to describe, program and plan behaviors of the Agent easily and quickly, and to experiment with a large variety of Information Retrieval strategies. Both the System Architecture and WOLF are general and reusable, and the result constitutes a good example of real implementation of agents based on logics.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {492–493},
numpages = {2},
keywords = {agents, answer set programming, information retrieval, logic programming},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013542,
author = {Wolber, David and Brooks, Christopher H.},
title = {Associative Sources and Agents for Zero-Input Publishing},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013542},
doi = {10.1145/1013367.1013542},
abstract = {This paper presents an associative agent that allows seamless navigation from one's own personal space to third-party associative sources, as well as the personal spaces of other users. The agent provides users with access to a dynamically growing list of information sources, all of which follow a common associative sources API that we have defined. The agent also allows users act as sources themselves and take part in peer-to peer knowledge sharing.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {494–495},
numpages = {2},
keywords = {web services, context, agents, polymorphism, aggregation, reconnaissance, associativity},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013543,
author = {Gibson, David},
title = {Surfing the Web by Site},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013543},
doi = {10.1145/1013367.1013543},
abstract = {We provide a system for surfing the web at a high level of abstraction, which is an analogy of the web browser, but which displays entire sites at a time. It allows a principled investigation of what is present, based on an overview of all available information. We show a site's relation to other sites, the broad nature of the information contained and how it is structured, and how it has changed over time. Our current system maintains a continuously updated archive of 40 million sites representing 1.9 billion web pages, and enables real-time navigation through the sea of web sites.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {496–497},
numpages = {2},
keywords = {large scale systems, novel browsing paradigms, web navigation strategies},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013544,
author = {Lee, Yugyung and Patel, Chintan and Chun, Soon Ae and Geller, James},
title = {Compositional Knowledge Management for Medical Services on Semantic Web},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013544},
doi = {10.1145/1013367.1013544},
abstract = {The vision of the Semantic Web is to reduce manual discovery and usage of Web resources (documents and services) and to allow software agents to automatically identify these Web resources, integrate them and execute them for achieving the intended goals of the user. Such a composed Web service may be represented as a workflow, called service flow. Current Web service standards are not sufficient for automatic composition. This paper presents different types of compositional knowledge required for Web service discovery and composition. As a proof of concept, we have implemented our framework in a cardiovascular domain which requires advanced service discovery and composition across heterogeneous platforms of multiple organizations.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {498–499},
numpages = {2},
keywords = {service composition, pragmatic knowledge},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013545,
author = {Davulcu, Hasan and Vadrevu, Srinivas and Nagarajan, Saravanakumar},
title = {OntoMiner: Bootstrapping Ontologies from Overlapping Domain Specific Web Sites},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013545},
doi = {10.1145/1013367.1013545},
abstract = {In this paper, we present automated techniques for bootstrapping and populating specialized domain ontologies by organizing and mining a set of relevant overlapping Web sites provided by the user. We develop algorithms that detect and utilize HTML regularities in the Web documents to turn them into hierarchical semantic structures encoded as XML. Next, we present tree-mining algorithms that identify key domain concepts and their taxonomical relationships. We also extract semi-structured concept instances annotated with their labels whenever they are available. Experimental evaluation for the News, Travel, and Shopping domains indicates that our algorithms can bootstrap and populate domain specific ontologies with high precision and recall.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {500–501},
numpages = {2},
keywords = {semantic web, web mining, data mining, ontology},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013546,
author = {Davison, Brian D. and Zhang, Wei and Wu, Baoning},
title = {Lessons from a Gnutella-Web Gateway},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013546},
doi = {10.1145/1013367.1013546},
abstract = {We present a gateway between the WWW and the Gnutella peer-to-peer network that permits searchers on one side to be able to search and retrieve files on the other side of the gateway. This work improvesthe accessibility of files across different delivery platforms, making it possible to use a single search modality. We outline our design and implementation, present access statistics from a test deployment and discuss lessons learned.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {502–503},
numpages = {2},
keywords = {peer-to-peer, World Wide Web, search engine, Gnutella},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

@inproceedings{10.1145/1013367.1013547,
author = {Jin, Jingwen and Nahrstedt, Klara},
title = {Hybrid Multicasting in Large-Scale Service Networks},
year = {2004},
isbn = {1581139128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1013367.1013547},
doi = {10.1145/1013367.1013547},
abstract = {The importance of service composition has been widely recognized in the Internet research community due to its high flexibility in allowing development of customized applications. So far little attention has been paid to composite services' runtime performance-related aspects, which are of great importance to wide-area applications. Service composition in the wide area actually creates a new type of routing problem which we call QoS service routing. We study this problem in large networks (e.g., the Web) and provide distributed and scalable routing solutions with various optimization goals. Most importantly, we propose ways to reduce redundancies of data delivery and service execution through explorations of different types of multicast (service multicast and data multicast) in one-to-many application scenarios.},
booktitle = {Proceedings of the 13th International World Wide Web Conference on Alternate Track Papers &amp; Posters},
pages = {504–505},
numpages = {2},
keywords = {service composition, multicast, QoS},
location = {New York, NY, USA},
series = {WWW Alt. '04}
}

