@inproceedings{10.1145/964696.964697,
author = {Brown, Leonard D. and Hua, Hong and Gao, Chunyu},
title = {A Widget Framework for Augmented Interaction in SCAPE},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964697},
doi = {10.1145/964696.964697},
abstract = {We have previously developed a collaborative infrastructure called SCAPE - an acronym for Stereoscopic Collaboration in Augmented and Projective Environments - that integrates the traditionally separate paradigms of virtual and augmented reality. In this paper, we extend SCAPE by formalizing its underlying mathematical framework and detailing three augmented Widgets constructed via this framework: CoCylinder, Magnifier, and CoCube. These devices promote intuitive ways of selecting, examining, and sharing synthetic objects, and retrieving associated documentary text. Finally we present a testbed application to showcase SCAPE's capabilities for interaction in large, augmented virtual environments.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {1–10},
numpages = {10},
keywords = {augmented reality (AR), virtual reality (VR), head-mounted projective display (HMPD), tangible user interface (TUI), human computer Interaction (HCI), head-mounted display (HMD)},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964698,
author = {Begole, James "Bo" and Tang, John C. and Hill, Rosco},
title = {Rhythm Modeling, Visualizations and Applications},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964698},
doi = {10.1145/964696.964698},
abstract = {People use their awareness of others' temporal patterns to plan work activities and communication. This paper presents algorithms for programatically detecting and modeling temporal patterns from a record of online presence data. We describe analytic and end-user visualizations of rhythmic patterns and the tradeoffs between them. We conducted a design study that explored the accuracy of the derived rhythm models compared to user perceptions, user preference among the visualization alternatives, and users' privacy preferences. We also present a prototype application based on the rhythm model that detects when a person is "away" for an extended period and predicts their return. We discuss the implications of this technology on the design of computer-mediated communication.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {11–20},
numpages = {10},
keywords = {visualization, CMC, awareness, instant messaging, context-aware computing, user modeling, rhythms, CSCW},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964699,
author = {Ganoe, Craig H. and Somervell, Jacob P. and Neale, Dennis C. and Isenhour, Philip L. and Carroll, John M. and Rosson, Mary Beth and McCrickard, D. Scott},
title = {Classroom BRIDGE: Using Collaborative Public and Desktop Timelines to Support Activity Awareness},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964699},
doi = {10.1145/964696.964699},
abstract = {Classroom BRIDGE supports activity awareness by facilitating planning and goal revision in collaborative, project-based middle school science. It integrates large-screen and desktop views of project times to support incidental creation of awareness information through routine document transactions, integrated presentation of awareness information as part of workspace views, and public access to subgroup activity. It demonstrates and develops an object replication approach to integrating synchronous and asynchronous distributed work for a platform incorporating both desktop and large-screen devices. This paper describes an implementation of these concepts with preliminary evaluation data, using timeline-based user interfaces.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {21–30},
numpages = {10},
keywords = {CSCL, activity awareness, CSCW, large screen display, timeline interface, multiple-device system},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964700,
author = {Goto, Masataka},
title = {SmartMusicKIOSK: Music Listening Station with Chorus-Search Function},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964700},
doi = {10.1145/964696.964700},
abstract = {This paper describes a new music-playback interface for trial listening, SmartMusicKIOSK. In music stores, short trial listening of CD music is not usually a passive experience -- customers often search out the chorus or "hook" of a song using the fast-forward button. Listening of this type, however, has not been traditionally supported. This research achieves a function for jumping to the chorus section and other key parts of a song plus a function for visualizing song structure. These functions make it easier for a listener to find desired parts of a song and thereby facilitate an active listening experience. The proposed functions are achieved by an automatic chorus-section detecting method, and the results of implementing them as a listening station have demonstrated their usefulness.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {31–40},
numpages = {10},
keywords = {audio visualization, music interaction, chorus detection, music-playback interface, song structure},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964701,
author = {Lakshmipathy, Vidya and Schmandt, Chris and Marmasse, Natalia},
title = {TalkBack: A Conversational Answering Machine},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964701},
doi = {10.1145/964696.964701},
abstract = {Current asynchronous voice messaging interfaces, like voicemail, fail to take advantage of our conversational skills. TalkBack restores conversational turn-taking to voicemail retrieval by dividing voice messages into smaller sections based on the most significant silent and filled pauses and pausing after each to record a response. The responses are composed into a reply, alternating with snippets of the original message for context. TalkBack is built into a digital picture frame; the recipient touches a picture of the caller to hear each segment of the message in turn. The minimal interface models synchronous interaction and facilitates asynchronous voice messaging. TalkBack can also present a voice-annotated slide show which it receives over the Internet.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {41–50},
numpages = {10},
keywords = {voicemail, answering machine, computer mediated communication, conversational interface},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964702,
author = {Guimbreti\`{e}re, Fran\c{c}ois},
title = {Paper Augmented Digital Documents},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964702},
doi = {10.1145/964696.964702},
abstract = {Paper Augmented Digital Documents (PADDs) are digital documents that can be manipulated either on a computer screen or on paper. PADDs, and the infrastructure supporting them, can be seen as a bridge between the digital and the paper worlds. As digital documents, PADDs are easy to edit, distribute and archive; as paper documents, PADDs are easy to navigate, annotate and well accepted in social settings. The chimeric nature of PADDs make them well suited for many tasks such as proofreading, editing, and annotation of large format document like blueprints.We are presenting an architecture which supports the seamless manipulation of PADDs using today's technologies and reports on the lessons we learned while implementing the first PADD system.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {51–60},
numpages = {10},
keywords = {digital pen, anoto, PADD, paper based user interface, paper augmented digital document},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964703,
author = {Wobbrock, Jacob O. and Myers, Brad A. and Kembel, John A.},
title = {EdgeWrite: A Stylus-Based Text Entry Method Designed for High Accuracy and Stability of Motion},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964703},
doi = {10.1145/964696.964703},
abstract = {EdgeWrite is a new unistroke text entry method for handheld devices designed to provide high accuracy and stability of motion for people with motor impairments. It is also effective for able-bodied people. An EdgeWrite user enters text by traversing the edges and diagonals of a square hole imposed over the usual text input area. Gesture recognition is accomplished not through pattern recognition but through the sequence of corners that are hit. This means that the full stroke path is unimportant and recognition is highly deterministic, enabling better accuracy than other gestural alphabets such as Graffiti. A study of able-bodied users showed subjects with no prior experience were 18% more accurate during text entry with Edge Write than with Graffiti (p&gt;.05), with no significant difference in speed. A study of 4 subjects with motor impairments revealed that some of them were unable to do Graffiti, but all of them could do Edge Write. Those who could do both methods had dramatically better accuracy with Edge Write.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {61–70},
numpages = {10},
keywords = {motor impairments, assistive technology, PDAs, gesture recognition, text input, graffiti, unistrokes, pebbles, palm, corners, text entry, edges, computer access, handhelds},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964704,
author = {Fitzmaurice, George and Khan, Azam and Piek\'{e}, Robert and Buxton, Bill and Kurtenbach, Gordon},
title = {Tracking Menus},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964704},
doi = {10.1145/964696.964704},
abstract = {We describe a new type of graphical user interface widget, known as a "tracking menu." A tracking menu consists of a cluster of graphical buttons, and as with traditional menus, the cursor can be moved within the menu to select and interact with items. However, unlike traditional menus, when the cursor hits the edge of the menu, the menu moves to continue tracking the cursor. Thus, the menu always stays under the cursor and close at hand.In this paper we define the behavior of tracking menus, show unique affordances of the widget, present a variety of examples, and discuss design characteristics. We examine one tracking menu design in detail, reporting on usability studies and our experience integrating the technique into a commercial application for the Tablet PC. While user interface issues on the Tablet PC, such as preventing round trips to tool palettes with the pen, inspired tracking menus, the design also works well with a standard mouse and keyboard configuration.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {71–79},
numpages = {9},
keywords = {tablet PC, menu system, floating palette, pen based user interfaces, graphical user interface},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964705,
author = {Wigdor, Daniel and Balakrishnan, Ravin},
title = {<i>TiltText</i>: Using Tilt for Text Input to Mobile Phones},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964705},
doi = {10.1145/964696.964705},
abstract = {TiltText, a new technique for entering text into a mobile phone is described. The standard 12-button text entry keypad of a mobile phone forces ambiguity when the 26- letter Roman alphabet is mapped in the traditional manner onto keys 2-9. The TiltText technique uses the orientation of the phone to resolve this ambiguity, by tilting the phone in one of four directions to choose which character on a particular key to enter. We first discuss implementation strategies, and then present the results of a controlled experiment comparing TiltText to MultiTap, the most common text entry technique. The experiment included 10 participants who each entered a total of 640 phrases of text chosen from a standard corpus, over a period of about five hours. The results show that text entry speed including correction for errors using TiltText was 23% faster than MultiTap by the end of the experiment, despite a higher error rate for TiltText. TiltText is thus amongst the fastest known language-independent techniques for entering text into mobile phones.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {81–90},
numpages = {10},
keywords = {text entry, tilt input, mobile phones},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964706,
author = {Kobayashi, Masatomo and Igarashi, Takeo},
title = {Considering the Direction of Cursor Movement for Efficient Traversal of Cascading Menus},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964706},
doi = {10.1145/964696.964706},
abstract = {Cascading menus are commonly seen in most GUI systems. However, people sometimes choose the wrong items by mistake, or become frustrated when submenus pop up unnecessarily. This paper proposes two methods for improving the usability of cascading menus. The first uses the direction of cursor movement to change the menu behavior: horizontal motion opens/closes submenus, while vertical motion changes the highlight within the current menu. This feature can reduce cursor movement errors. The second causes a submenu to pop up at the position where horizontal motion occurs. This is expected to reduce the length of the movement path for menu traversal. A user study showed that our methods reduce menu selection times, shorten search path lengths, and prevent unexpected submenu appearance and disappearance.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {91–94},
numpages = {4},
keywords = {cascading menus, GUI, pointing devices},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964707,
author = {Suh, Bongwon and Ling, Haibin and Bederson, Benjamin B. and Jacobs, David W.},
title = {Automatic Thumbnail Cropping and Its Effectiveness},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964707},
doi = {10.1145/964696.964707},
abstract = {Thumbnail images provide users of image retrieval and browsing systems with a method for quickly scanning large numbers of images. Recognizing the objects in an image is important in many retrieval tasks, but thumbnails generated by shrinking the original image often render objects illegible. We study the ability of computer vision systems to detect key components of images so that automated cropping, prior to shrinking, can render objects more recognizable. We evaluate automatic cropping techniques 1) based on a general method that detects salient portions of images, and 2) based on automatic face detection. Our user study shows that these methods result in small thumbnails that are substantially more recognizable and easier to find in the context of visual search.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {95–104},
numpages = {10},
keywords = {usability study, saliency map, thumbnail, visual search, face detection, image cropping, zoomable user interfaces},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964708,
author = {Ramos, Gonzalo and Balakrishnan, Ravin},
title = {Fluid Interaction Techniques for the Control and Annotation of Digital Video},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964708},
doi = {10.1145/964696.964708},
abstract = {We explore a variety of interaction and visualization techniques for fluid navigation, segmentation, linking, and annotation of digital videos. These techniques are developed within a concept prototype called LEAN that is designed for use with pressure-sensitive digitizer tablets. These techniques include a transient position+velocity widget that allows users not only to move around a point of interest on a video, but also to rewind or fast forward at a controlled variable speed. We also present a new variation of fish-eye views called twist-lens, and incorporate this into a position control slider designed for the effective navigation and viewing of large sequences of video frames. We also explore a new style of widgets that exploit the use of the pen's pressure-sensing capability, increasing the input vocabulary available to the user. Finally, we elaborate on how annotations referring to objects that are temporal in nature, such as video, may be thought of as links, and fluidly constructed, visualized and navigated.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {105–114},
numpages = {10},
keywords = {pen-based interfaces, video, annotations, fluid interaction techniques},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964709,
author = {Wittenburg, Kent and Forlines, Clifton and Lanning, Tom and Esenther, Alan and Harada, Shigeo and Miyachi, Taizo},
title = {Rapid Serial Visual Presentation Techniques for Consumer Digital Video Devices},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964709},
doi = {10.1145/964696.964709},
abstract = {In this paper we propose a new model for a class of rapid serial visual presentation (RSVP) interfaces [16] in the context of consumer video devices. The basic spatial layout "explodes" a sequence of image frames into a 3D trail in order to provide more context for a spatial/temporal presentation. As the user plays forward or back, the trail advances or recedes while the image in the foreground focus position is replaced. The design is able to incorporate a variety of methods for analyzing or highlighting images in the trail. Our hypotheses are that users can navigate more quickly and precisely to points of interest when compared to conventional consumer-based browsing, channel flipping, or fast-forwarding techniques. We report on an experiment testing our hypotheses in which we found that subjects were more accurate but not faster in browsing to a target of interest in recorded television content with a TV remote.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {115–124},
numpages = {10},
keywords = {video browsing, multimedia interfaces, consumer devices, rapid serial visual presentation, TV interfaces, RSVP},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964710,
author = {Fogarty, James and Hudson, Scott E.},
title = {GADGET: A Toolkit for Optimization-Based Approaches to Interface and Display Generation},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964710},
doi = {10.1145/964696.964710},
abstract = {Recent work is beginning to reveal the potential of numerical optimization as an approach to generating interfaces and displays. Optimization-based approaches can often allow a mix of independent goals and constraints to be blended in ways that would be difficult to describe algorithmically. While optimization-based techniques appear to offer several potential advantages, further research in this area is hampered by the lack of appropriate tools. This paper presents GADGET, an experimental toolkit to support optimization for interface and display generation. GADGET provides convenient abstractions of many optimization concepts. GADGET also provides mechanisms to help programmers quickly create optimizations, including an efficient lazy evaluation framework, a powerful and configurable optimization structure, and a library of reusable components. Together these facilities provide an appropriate tool to enable exploration of a new class of interface and display generation techniques.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {125–134},
numpages = {10},
keywords = {numerical optimization, perceptually optimized displays, display generation, layout algorithms, toolkits},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964711,
author = {Lecolinet, Eric},
title = {A Molecular Architecture for Creating Advanced GUIs},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964711},
doi = {10.1145/964696.964711},
abstract = {This paper presents a new GUI architecture for creating advanced interfaces. This model is based on a limited set of general principles that improve flexibility and provide capabilities for implementing information visualization techniques such as magic lenses, transparent tools or semantic zooming. This architecture also makes it possible to create multiple views and application-sharing systems (by sharing views on multiple computer screens) in a simple and uniform way and to handle bimanual interaction and multiple pointers. An experimental toolkit called Ubit was implemented to test the feasibility of this approach. It is based on a pseudo-declarative C++ API that tries to simplify GUI programming by providing a higher level of abstraction.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {135–144},
numpages = {10},
keywords = {multiple-views, brickgets, Ubit, bi-manual interaction, GUI toolkits, multiple displays, transparent tools, ZUIs, GUI architectures, declarative languages},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964712,
author = {Quan, Dennis and Huynh, David and Karger, David R. and Miller, Robert},
title = {User Interface Continuations},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964712},
doi = {10.1145/964696.964712},
abstract = {Dialog boxes that collect parameters for commands often create ephemeral, unnatural interruptions of a program's normal execution flow, encouraging the user to complete the dialog box as quickly as possible in order for the program to process that command. In this paper we examine the idea of turning the act of collecting parameters from a user into a first class object called a user interface continuation. Programs can create user interface continuations by specifying what information is to be collected from the user and supplying a callback (i.e., a continuation) to be notified with the collected information. A partially completed user interface continuation can be saved as a new command, much as currying and partially evaluating a function with a set of parameters produces a new function. Furthermore, user interface continuations, like other continuation-passing paradigms, can be used to allow program execution to continue uninterrupted while the user determines a command's parameters at his or her leisure.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {145–148},
numpages = {4},
keywords = {continuations, dialog boxes},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964713,
author = {Hinckley, Ken},
title = {Synchronous Gestures for Multiple Persons and Computers},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964713},
doi = {10.1145/964696.964713},
abstract = {This research explores distributed sensing techniques for mobile devices using synchronous gestures. These are patterns of activity, contributed by multiple users (or one user with multiple devices), which take on a new meaning when they occur together in time, or in a specific sequence in time. To explore this new area of inquiry, this work uses tablet computers augmented with touch sensors and two-axis linear accelerometers (tilt sensors). The devices are connected via an 802.11 wireless network and synchronize their time-stamped sensor data. This paper describes a few practical examples of interaction techniques using synchronous gestures such as dynamically tiling together displays by physically bumping them together, discusses implementation issues, and speculates on further possibilities for synchronous gestures.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {149–158},
numpages = {10},
keywords = {context awareness, ubiquitous computing, multi-user interfaces, distributed sensor systems, input devices, sensors},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964714,
author = {Izadi, Shahram and Brignull, Harry and Rodden, Tom and Rogers, Yvonne and Underwood, Mia},
title = {Dynamo: A Public Interactive Surface Supporting the Cooperative Sharing and Exchange of Media},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964714},
doi = {10.1145/964696.964714},
abstract = {In this paper we propose a novel way of supporting occasional meetings that take place in unfamiliar public places, which promotes lightweight, visible and fluid collaboration. Our central idea is that the sharing and exchange of information occurs across public surfaces that users can easily access and interact with. To this end, we designed and implemented Dynamo, a communal multi-user interactive surface. The surface supports the cooperative sharing and exchange of a wide range of media that can be brought to the surface by users that are remote from their familiar organizational settings.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {159–168},
numpages = {10},
keywords = {sharing, collaboration, large interactive surfaces, multi-user interfaces, exchange},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964715,
author = {Denoue, Laurent and Nelson, Les and Churchill, Elizabeth},
title = {A Fast, Interactive 3D Paper-Flier Metaphor for Digital Bulletin Boards},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964715},
doi = {10.1145/964696.964715},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {169–172},
numpages = {4},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964716,
author = {Cao, Xiang and Balakrishnan, Ravin},
title = {VisionWand: Interaction Techniques for Large Displays Using a Passive Wand Tracked in 3D},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964716},
doi = {10.1145/964696.964716},
abstract = {A passive wand tracked in 3D using computer vision techniques is explored as a new input mechanism for interacting with large displays. We demonstrate a variety of interaction techniques that exploit the affordances of the wand, resulting in an effective interface for large scale interaction. The lack of any buttons or other electronics on the wand presents a challenge that we address by developing a set of postures and gestures to track state and enable command input. We also describe the use of multiple wands, and posit designs for more complex wands in the future.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {173–182},
numpages = {10},
keywords = {interaction techniques, input devices, buttonless input, vision tracking, gestures, large displays},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964717,
author = {Saund, Eric and Fleet, David and Larner, Daniel and Mahoney, James},
title = {Perceptually-Supported Image Editing of Text and Graphics},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964717},
doi = {10.1145/964696.964717},
abstract = {This paper presents a novel image editing program emphasizing easy selection and manipulation of material found in informal, casual documents such as sketches, handwritten notes, whiteboard images, screen snapshots, and scanned documents. The program, called ScanScribe, offers four significant advances. First, it presents a new, intuitive model for maintaining image objects and groups, along with underlying logic for updating these in the course of an editing session. Second, ScanScribe takes advantage of newly developed image processing algorithms to separate foreground markings from a white or light background, and thus can automatically render the background transparent so that image material can be rearranged without occlusion by background pixels. Third, ScanScribe introduces new interface techniques for selecting image objects with a pointing device without resorting to a palette of tool modes. Fourth, ScanScribe presents a platform for exploiting image analysis and recognition methods to make perceptually significant structure readily available to the user. As a research prototype, ScanScribe has proven useful in the work of members of our laboratory, and has been released on a limited basis for user testing and evaluation.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {183–192},
numpages = {10},
keywords = {bitmap image, lattice grouping, WYPIWYG, foreground/background, scanscribe, rough document, perceptual document editing},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964718,
author = {Wu, Mike and Balakrishnan, Ravin},
title = {Multi-Finger and Whole Hand Gestural Interaction Techniques for Multi-User Tabletop Displays},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964718},
doi = {10.1145/964696.964718},
abstract = {Recent advances in sensing technology have enabled a new generation of tabletop displays that can sense multiple points of input from several users simultaneously. However, apart from a few demonstration techniques [17], current user interfaces do not take advantage of this increased input bandwidth. We present a variety of multifinger and whole hand gestural interaction techniques for these displays that leverage and extend the types of actions that people perform when interacting on real physical tabletops. Apart from gestural input techniques, we also explore interaction and visualization techniques for supporting shared spaces, awareness, and privacy. These techniques are demonstrated within a prototype room furniture layout application, called RoomPlanner.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {193–202},
numpages = {10},
keywords = {tabletop interaction, gestures, multi degree-of-freedom input, collaborative and competitive applications},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964719,
author = {Rekimoto, Jun and Ishizawa, Takaaki and Schwesig, Carsten and Oba, Haruo},
title = {PreSense: Interaction Techniques for Finger Sensing Input Devices},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964719},
doi = {10.1145/964696.964719},
abstract = {Although graphical user interfaces started as imitations of the physical world, many interaction techniques have since been invented that are not available in the real world. This paper focuses on one of these "previewing", and how a sensory enhanced input device called "PreSense Keypad" can provide a preview for users before they actually execute the commands. Preview important in the real world because it is often not possible to undo an action. This previewable feature helps users to see what will occur next. It is also helpful when the command assignment of the keypad dynamically changes, such as for universal commanders. We present several interaction techniques based on this input device, including menu and map browsing systems and a text input system. We also discuss finger gesture recognition for the PreSense Keypad.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {203–212},
numpages = {10},
keywords = {gesture sensing, input devices, previewable user interfaces},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964720,
author = {Saund, Eric and Lank, Edward},
title = {Stylus Input and Editing without Prior Selection of Mode},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964720},
doi = {10.1145/964696.964720},
abstract = {This paper offers a solution to the mode problem in computer sketch/notetaking programs. Conventionally, the user must specify the intended "draw" or "command" mode prior to performing a stroke. This necessity has proven to be a barrier to the usability of pen/stylus systems. We offer a novel Inferred-Mode interaction protocol that avoids the mode hassles of conventional sketch systems. The system infers the user's intent, if possible, from the properties of the pen trajectory and the context of the trajectory. If the intent is ambiguous, the user is offered a choice mediator in the form of a pop-up button. To maximize the fluidity of drawing, the user is entitled to ignore the mediator and continue drawing. We present decision logic for the inferred mode protocol, and discuss subtleties learned in the course of its development. We also present results of initial user trials validating the usability of this interaction design.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {213–216},
numpages = {4},
keywords = {inferred-Mode protocol, mode, draw, inkscribe, sketch, pen, stylus, command},
location = {Vancouver, Canada},
series = {UIST '03}
}

@inproceedings{10.1145/964696.964721,
author = {Poupyrev, Ivan and Maruyama, Shigeaki},
title = {Tactile Interfaces for Small Touch Screens},
year = {2003},
isbn = {1581136366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/964696.964721},
doi = {10.1145/964696.964721},
abstract = {We present the design, implementation, and informal evaluation of tactile interfaces for small touch screens used in mobile devices. We embedded a tactile apparatus in a Sony PDA touch screen and enhanced its basic GUI elements with tactile feedback. Instead of observing the response of interface controls, users can feel it with their fingers as they press the screen. In informal evaluations, tactile feedback was greeted with enthusiasm. We believe that tactile feedback will become the next step in touch screen interface design and a standard feature of future mobile devices.},
booktitle = {Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology},
pages = {217–220},
numpages = {4},
keywords = {touch screen, tactile feedback, mobile computers},
location = {Vancouver, Canada},
series = {UIST '03}
}

