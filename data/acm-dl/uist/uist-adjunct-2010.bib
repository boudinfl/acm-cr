@inproceedings{10.1145/1866218.1866220,
author = {Bernstein, Michael S.},
title = {Crowd-Powered Interfaces},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866220},
doi = {10.1145/1866218.1866220},
abstract = {We investigate crowd-powered interfaces: interfaces that embed human activity to support high-level conceptual activities such as writing, editing and question-answering. For example, a crowd-ppowered interface using paid crowd workers can compute a series of textual cuts and edits to a paragraph, then provide the user with an interface to condense his or her writing. We map out the design space of interfaces that depend on outsourced, friendsourced, and data mined resources, and report on designs for each of these. We discuss technical and motivational challenges inherent in human-powered interfaces.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {347–350},
numpages = {4},
keywords = {social computing, crowdsourcing, outsourcing},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866221,
author = {Cowan, Lisa G.},
title = {Supporting Self-Expression for Informal Communication},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866221},
doi = {10.1145/1866218.1866221},
abstract = {Mobile phones are becoming the central tools for communicating and can help us keep in touch with friends and family on-the-go. However, they can also place high demands on attention and constrain interaction. My research concerns how to design communication mechanisms that mitigate these problems to support self-expression for informal communication on mobile phones. I will study how people communicate with camera-phone photos, paper-based sketches, and projected information and how this communication impacts social practices.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {351–354},
numpages = {4},
keywords = {communication, mobile, self-expression},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866222,
author = {Patel, Kayur},
title = {Lowering the Barrier to Applying Machine Learning},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866222},
doi = {10.1145/1866218.1866222},
abstract = {Machine learning algorithms are key components in many cutting edge applications of computation. However, the full potential of machine learning has not been realized because using machine learning is hard, even for otherwise tech-savvy developers. This is because developing with machine learning is different than normal programming. My thesis is that developers applying machine learning need new general-purpose tools that provide structure for common processes and common pipelines while remaining flexible to account for variability in problems. In this paper, I describe my efforts to understanding the difficulties that developers face when applying machine learning. I then describe Gestalt, a general-purpose integrated development environment designed the application of machine learning. Finally, I describe work on developing a pattern language for building machine learning systems and creating new techniques that help developers understand the interaction between their data and learning algorithms.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {355–358},
numpages = {4},
keywords = {gestalt, machine learning, integrated development environments},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866223,
author = {Pham, Hubert},
title = {User Interface Models for the Cloud},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866223},
doi = {10.1145/1866218.1866223},
abstract = {The current desktop metaphor is unsuitable for the coming age of cloud-based applications. The desktop was developed in an era that was focused on local resources, and consequently its gestures, semantics, and security model reflect heavy reliance on hierarchy and physical locations. This paper proposes a new user interface model that accounts for cloud applications, incorporating representations of people and new gestures for sharing and access, while minimizing the prominence of location. The model's key feature is a lightweight mechanism to group objects for resource organization, sharing, and access control, towards the goal of providing simple semantics for a wide range of tasks, while also achieving security through greater usability.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {359–362},
numpages = {4},
keywords = {model, desktop, cloud, groups},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866224,
author = {Schmidt, Dominik},
title = {Towards Personalized Surface Computing},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866224},
doi = {10.1145/1866218.1866224},
abstract = {With recent progress in the field of surface computing it becomes foreseeable that interactive surfaces will turn into a commodity in the future, ubiquitously integrated into our everyday environments. At the same time, we can observe a trend towards personal data and whole applications being accessible over the Internet, anytime from anywhere. We envision a future where interactive surfaces surrounding us serve as powerful portals to access these kinds of data and services. In this paper, we contribute two novel interaction techniques supporting parts of this vision: First, HandsDown, a biometric user identification approach based on hand contours and, second, PhoneTouch, a novel technique for using mobile phones in conjunction with interactive surfaces.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {363–366},
numpages = {4},
keywords = {surface computing, mobile devices, user identification},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866225,
author = {Schwarz, Julia},
title = {Towards a Unified Framework for Modeling, Dispatching, and Interpreting Uncertain Input},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866225},
doi = {10.1145/1866218.1866225},
abstract = {Many new input technologies (such as touch and voice) hold the promise of more natural user interfaces. However, many of these technologies create inputs with some uncertainty. Unfortunately, conventional infrastructure lacks a method for easily handling uncertainty, and as a result input produced by these technologies is often converted to conventional events as quickly as possible, leading to a stunted interactive experience. Our ongoing work aims to design a unified framework for modeling uncertain input and dispatching it to interactors. This should allow developers to easily create interactors which can interpret uncertain input, give the user appropriate feedback, and accurately resolve any ambiguity. This abstract presents an overview of the design of a framework for handling input with uncertainty and describes topics we hope to pursue in future work. We also give an example of how we built highly accurate touch buttons using our framework. For examples of what interactors can be built and a more detailed description of our framework we refer the reader to [8].},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {367–370},
numpages = {4},
keywords = {recognition, input handling, ambiguity},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866226,
author = {Vig, Jesse},
title = {Intelligent Tagging Interfaces: Beyond Folksonomy},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866226},
doi = {10.1145/1866218.1866226},
abstract = {This paper summarizes our work on using tags to broaden the dialog between a recommender system and its users. We present two tagging applications that enrich this dialog: tagsplanations are tag-based explanations of recommendations provided by a system to its users, and Movie Tuner is a conversational recommender system that enables users to provide feedback on movie recommendations using tags. We discuss the design of both systems and the experimental methodology used to evaluate the design choices.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {371–374},
numpages = {4},
keywords = {conversational recommenders, recommender systems, tagging, explanations},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866227,
author = {Weiss, Malte},
title = {Bringing Everyday Applications to Interactive Surfaces},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866227},
doi = {10.1145/1866218.1866227},
abstract = {This paper presents ongoing work that intends to simplify the introduction of everyday applications to interactive tabletops. SLAP Widgets bring tangible general-purpose widgets to tabletops while providing the flexibility of on-screen controls. Madgets maintain consistency between physical controls and their digital state. BendDesk represents our vision of a multi-touch enabled office environment. Our pattern language captures knowledge for the design of interactive tabletops. For each project, we describe its technical background, present the current state of research, and discuss future work.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {375–378},
numpages = {4},
keywords = {actuation, tangible user interfaces, interactive tabletops, haptic feedback, curved surface, applications},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866229,
author = {Chung, Keywon and Shilman, Michael and Merrill, Chris and Ishii, Hiroshi},
title = {OnObject: Gestural Play with Tagged Everyday Objects},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866229},
doi = {10.1145/1866218.1866229},
abstract = {Many Tangible User Interface (TUI) systems employ sensor-equipped physical objects. However they do not easily scale to users' actual environments; most everyday objects lack the necessary hardware, and modification requires hardware and software development by skilled individuals. This limits TUI creation by end users, resulting in inflexible interfaces in which the mapping of sensor input and output events cannot be easily modified reflecting the end user's wishes and circumstances. We introduce OnObject, a small device worn on the hand, which can program physical objects to respond to a set of gestural triggers. Users attach RFID tags to situated objects, grab them by the tag, and program their responses to grab, release, shake, swing, and thrust gestures using a built-in button and a microphone. In this paper, we demonstrate how novice end users including preschool children can instantly create engaging gestural object interfaces with sound feedback from toys, drawings, or clay.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {379–380},
numpages = {2},
keywords = {ubiquitous computing, gestural object interfaces, tangible interfaces, end user programming},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866230,
author = {Follmer, Sean and Carr, David and Lovell, Emily and Ishii, Hiroshi},
title = {CopyCAD: Remixing Physical Objects with Copy and Paste from the Real World},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866230},
doi = {10.1145/1866218.1866230},
abstract = {This paper introduces a novel technique for integrating geometry from physical objects into computer aided design (CAD) software. We allow users to copy arbitrary real world object geometry into 2D CAD designs at scale through the use of a camera/projector system. This paper also introduces a system, CopyCAD, that uses this technique, and augments a Computer Controlled (CNC) milling machine. CopyCAD gathers input from physical objects, sketches and interactions directly on a milling machine, allowing novice users to copy parts of real world objects, modify them and then create a new physical part.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {381–382},
numpages = {2},
keywords = {fabrication, design tools, tui, prototyping},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866231,
author = {Hemmert, Fabian and M\"{u}ller, Alexander and Jagodzinski, Ron and Wintergerst, G\"{o}tz and Joost, Gesche},
title = {Reflective Haptics: Haptic Augmentation of GUIs through Frictional Actuation of Stylus-Based Interactions},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866231},
doi = {10.1145/1866218.1866231},
abstract = {In this paper, we present a novel system for stylus-based GUI interactions: Simulated physics through actuated frictional properties of a touch screen stylus. We present a prototype that implements a series of principles which we propose for the design of frictionally augmented GUIs. It is discussed how such actuation could be a potential addition of value for stylus-controlled GUIs, through enabling prioritized content, allowing for inherent confirmation, and leveraging on manual dexterity.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {383–384},
numpages = {2},
keywords = {haptic display, physicality, stylus, touch screen, friction},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866232,
author = {Jansen, Yvonne and Karrer, Thorsten and Borchers, Jan},
title = {MudPad: Localized Tactile Feedback on Touch Surfaces},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866232},
doi = {10.1145/1866218.1866232},
abstract = {We present MudPad, a system that is capable of localized active haptic feedback on multitouch surfaces. An array of electromagnets locally actuates a tablet-sized overlay containing magnetorheological (MR) fluid. The reaction time of the fluid is fast enough for realtime feedback ranging from static levels of surface softness to a broad set of dynamically changeable textures. As each area can be addressed individually, the entire visual interface can be enriched with a multi-touch haptic layer that conveys semantic information as the appropriate counterpart to multi-touch input.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {385–386},
numpages = {2},
keywords = {haptic i/o, tactile feedback, multitouch},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866233,
author = {Kato, Jun and Sakamoto, Daisuke and Igarashi, Takeo},
title = {Surfboard: Keyboard with Microphone as a Low-Cost Interactive Surface},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866233},
doi = {10.1145/1866218.1866233},
abstract = {We introduce a technique to detect simple gestures of "surfing" (moving a hand horizontally) on a standard keyboard by analyzing recorded sounds in real-time with a microphone attached close to the keyboard. This technique allows the user to maintain a focus on the screen while surfing on the keyboard. Since this technique uses a standard keyboard without any modification, the user can take full advantage of the input functionality and tactile quality of his favorite keyboard supplemented with our interface.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {387–388},
numpages = {2},
keywords = {interactive surface, keyboard, low-cost, microphone},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866234,
author = {Koizumi, Naoya and Yasu, Kentaro and Liu, Angela and Sugimoto, Maki and Inami, Masahiko},
title = {Animated Paper: A Moving Prototyping Platform},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866234},
doi = {10.1145/1866218.1866234},
abstract = {We have developed a novel prototyping method that utilizes animated paper, a versatile platform created from paper and shape memory alloy (SMA), which is easy to control using a range of different energy sources from sunlight to lasers. We have further designed a laser point tracking system to improve the precision of the wireless control system by embedding retro-reflective material on the paper to act as light markers. It is possible to change the movement of paper prototypes by varying where to mount the SMA or how to heat it, creating a wide range of applications.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {389–390},
numpages = {2},
keywords = {sma, organic user interfaces, paper, flexible structure},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866235,
author = {Le Pallec, Xaiver and Marvie, Rapha\"{e}l and Rouillard, Jos\'{e} and Tarby, Jean-Claude},
title = {A Support to Multi-Devices Web Application},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866235},
doi = {10.1145/1866218.1866235},
abstract = {Programming an application which uses interactive devices located on different terminals is not easy. Programming such applications with standard Web technologies (HTTP, Javascript, Web browser) is even more difficult. However, Web applications have interesting properties like running on very different terminals, the lack of a specific installation step, the ability to evolve the application code at runtime. Our demonstration presents a support for designing multi-devices Web applications. After introducing the context of this work, we briefly describe some problems related to the design of multi-devices web application. Then, we present the toolkit we have implemented to help the development of applications based upon distant interactive devices.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {391–392},
numpages = {2},
keywords = {interactive devices, toolkit, web application},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866236,
author = {Lee, Jinha and Teerapittayanon, Surat and Ishii, Hiroshi},
title = {Beyond: Collapsible Input Device for Direct 3D Manipulation beyond the Screen},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866236},
doi = {10.1145/1866218.1866236},
abstract = {What would it be like to reach into a screen and manipulate or design virtual objects as in real world? We present Beyond, a collapsible input device for direct 3D manipulation. When pressed against a screen, Beyond collapses in the physical world and extends into the digital space of the screen, such that users can perceive that they are inserting the tool into the virtual space. Beyond allows users to directly interact with 3D media, avoiding separation between the users' input and the displayed 3D graphics without requiring special glasses or wearables, thereby enabling users to select, draw, and sculpt in 3D virtual space unfettered. We describe detailed interaction techniques, implementation and application scenarios focused on 3D geometric design and prototyping.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {393–394},
numpages = {2},
keywords = {3d interaction, pen-based uis, pen and tactile input, interaction design, virtual reality, user interface design, tabletop uis, input and interaction technologies, augmented reality},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866237,
author = {Linder, Natan and Maes, Pattie},
title = {LuminAR: Portable Robotic Augmented Reality Interface Design and Prototype},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866237},
doi = {10.1145/1866218.1866237},
abstract = {In this paper we introduce LuminAR: a prototype for a new portable and compact projector-camera system designed to use the traditional incandescent bulb interface as a power source, and a robotic desk lamp that carries it, enabling it with dynamic motion capabilities. We are exploring how the LuminAR system embodied in a familiar form factor of a classic Angle Poise lamp may evolve into a new class of robotic, digital information devices.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {395–396},
numpages = {2},
keywords = {augmented reality, human robot interaction, actuated ui, robotic lamp, multi-touch interfaces, gestural interfaces},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866238,
author = {Mistry, Pranav and Ishii, Kentaro and Inami, Masahiko and Igarashi, Takeo},
title = {Blinkbot: Look at, Blink and Move},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866238},
doi = {10.1145/1866218.1866238},
abstract = {In this paper we present BlinkBot - a hands free input interface to control and command a robot. BlinkBot explores the natural modality of gaze and blink to direct a robot to move an object from a location to another. The paper also explains detailed hardware and software implementation of the prototype system.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {397–398},
numpages = {2},
keywords = {hands free interaction, blink aware interaction, human-robot interaction, robot},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866239,
author = {Shirokura, Takumi and Sakamoto, Daisuke and Sugiura, Yuta and Ono, Tetsuo and Inami, Masahiko and Igarashi, Takeo},
title = {RoboJockey: Real-Time, Simultaneous, and Continuous Creation of Robot Actions for Everyone},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866239},
doi = {10.1145/1866218.1866239},
abstract = {We developed a RoboJockey (Robot Jockey) interface for coordinating robot actions, such as dancing - similar to "Disc jockey" and "Video jockey". The system enables a user to choreograph a dance for a robot to perform by using a simple visual language. Users can coordinate humanoid robot actions with a combination of arm and leg movements. Every action is automatically performed to background music and beat. The RoboJockey will give a new entertainment experience with robots to the end-users.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {399–400},
numpages = {2},
keywords = {visual language, robot jockey interface, multi-touch interface, creation of robot action},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866240,
author = {Sukan, Mengu and Oda, Ohan and Shi, Xiang and Entrena, Manuel and Sadalgi, Shrenik and Qi, Jie and Feiner, Steven},
title = {ARmonica: A Collaborative Sonic Environment},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866240},
doi = {10.1145/1866218.1866240},
abstract = {ARmonica is a 3D audiovisual augmented reality environment in which players can position and edit virtual bars that play sounds when struck by virtual balls launched under the influence of physics. Players experience ARmonica through head-tracked head-worn displays and tracked hand-held ultramobile personal computers, and interact through tracked Wii remotes and touch-screen taps. The goal is for players to collaborate in the creation and editing of an evolving sonic environment. Research challenges include supporting walk-up usability without sacrificing deeper functionality.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {401–402},
numpages = {2},
keywords = {augmented reality, sound},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866241,
author = {Tsukada, Koji and Kambara, Keisuke},
title = {IODisk: Disk-Type i/o Interface for Browsing Digital Contents},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866241},
doi = {10.1145/1866218.1866241},
abstract = {We propose a disk-type I/O interface, IODisk, which helps users browse various digital contents intuitively in their living environment. IODisk mainly consists of a forcefeedback mechanism integrated in the rotation axis of a disk. Users can control the playing speed/direction contents (e.g., videos or picture slideshows) in proportion to the rotational speed/direction of the disk. We developed a prototype system and some applications.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {403–404},
numpages = {2},
keywords = {force feedback, tangible interface, i/o device, disk},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866242,
author = {Weibel, Nadir and Cowan, Lisa G. and Pina, Laura R. and Griswold, William G. and Hollan, James D.},
title = {Enabling Social Interactions through Real-Time Sketch-Based Communication},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866242},
doi = {10.1145/1866218.1866242},
abstract = {We present UbiSketch, a tool for ubiquitous real-time sketch-based communication. We describe the UbiSketch system, which enables people to create doodles, drawings, and notes with digital pens and paper and publish them quickly and easily via their mobile phones to social communication channels, such as Facebook, Twitter, and email. The natural paper-based social interaction enabled by UbiSketch has the potential to enrich current mobile communication practices.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {405–406},
numpages = {2},
keywords = {communication, mobile phone, interactive paper, digital pen, sketching, social networks},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866243,
author = {Weibel, Nadir and Piper, Anne Marie and Hollan, James D.},
title = {HIPerPaper: Introducing Pen and Paper Interfaces for Ultra-Scale Wall Displays},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866243},
doi = {10.1145/1866218.1866243},
abstract = {While recent advances in graphics, display, and computer hardware support ultra-scale visualizations of a tremendous amount of data sets, mechanisms for interacting with this information on large high-resolution wall displays are still under investigation. Different issues in terms of user interface, ergonomics, multi-user interaction, and system flexibility arise while facing ultra-scale wall displays and none of the introduced approaches fully address them. We introduce HIPerPaper, a novel digital pen and paper interface that enables natural interaction with the HIPerSpace wall, a 31.8 by 7.5 foot tiled wall display of 268,720,000 pixels. HIPerPaper provides a flexible, portable, and inexpensive medium for interacting with large high-resolution wall displays.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {407–408},
numpages = {2},
keywords = {interfaces, pen and paper, wall display},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866244,
author = {White, Samuel and Ji, Hanjie and Bigham, Jeffrey P.},
title = {EasySnap: Real-Time Audio Feedback for Blind Photography},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866244},
doi = {10.1145/1866218.1866244},
abstract = {This demonstration presents EasySnap, an application that enables blind and low-vision users to take high-quality photos by providing real-time audio feedback as they point their existing camera phones. Users can readily follow the audio instructions to adjust their framing, zoom level and subject lighting appropriately. Real-time feedback is achieved on current hardware using computer vision in conjunction with use patterns drawn from current blind photographers.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {409–410},
numpages = {2},
keywords = {blind users, non-visual interfaces, photography},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866245,
author = {Withana, Anusha and Kondo, Makoto and Kakehi, Gota and Makino, Yasutoshi and Sugimoto, Maki and Inami, Masahiko},
title = {ImpAct: Enabling Direct Touch and Manipulation for Surface Computing},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866245},
doi = {10.1145/1866218.1866245},
abstract = {This paper explores direct touch and manipulation techniques for surface computing platforms using a special force feedback stylus named ImpAct(Immersive Haptic Augmentation for Direct Touch). Proposed haptic stylus can change its length when it is pushed against a display surface. Correspondingly, a virtual stem is rendered inside the display area so that user perceives the stylus immersed through to the digital space below the screen. We propose ImpAct as a tool to probe and manipulate digital objects in the shallow region beneath display surface. ImpAct creates a direct touch interface by providing kinesthetic haptic sensations along with continuous visual contact to digital objects below the screen surface.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {411–412},
numpages = {2},
keywords = {6-dof input, direct touch, touch screen, simulated projection rendering, force feedback, haptic display},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866246,
author = {Yu, Zihao and Diakopoulos, Nicholas and Naaman, Mor},
title = {The Multiplayer: Multi-Perspective Social Video Navigation},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866246},
doi = {10.1145/1866218.1866246},
abstract = {We present a multi-perspective video "multiplayer" designed to organize social video aggregated from online sites like YouTube. Our system automatically time-aligns videos using audio fingerprinting, thus bringing them into a unified temporal frame. The interface utilizes social metadata to visually aid navigation and cue users to more interesting portions of an event. We provide details about the visual and interaction design rationale of the multiplayer.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {413–414},
numpages = {2},
keywords = {social media, multi-perspective, video},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866248,
author = {Aou, Kanako and Ishii, Asuka and Furukawa, Masahiro and Fukushima, Shogo and Kajimoto, Hiroyuki},
title = {The Enhancement of Hearing Using a Combination of Sound and Skin Sensation to the Pinna},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866248},
doi = {10.1145/1866218.1866248},
abstract = {Recent development in sound technologies has enabled the realistic replay of real-life sounds. Thanks to these technologies, we can experience a virtual real sound environment. However, there are other types of sound technologies that enhance reality, such as acoustic filters, sound effects, and background music. They are quite effective if carefully prepared, but they also alter the sound itself. Consequently, sound is simultaneously used to reconstruct realistic environments and to enhance emotions, which are actually incompatible functions.With this background, we focused on using tactile modality to enhance emotions and propose a method that enhances the sound experience by a combination of sound and skin sensation to the pinna (earlobe). In this paper, we evaluate the effectiveness of this method.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {415–416},
numpages = {2},
keywords = {emotional amplification, pinna, crossmodal displays, emotion, skin sensation},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866249,
author = {Fourney, Adam and Mann, Richard and Terry, Michael},
title = {What Can Internet Search Engines "Suggest" about the Usage and Usability of Popular Desktop Applications?},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866249},
doi = {10.1145/1866218.1866249},
abstract = {In this paper, we show how Internet search query logs can yield rich, ecologically valid data sets describing the common tasks and issues that people encounter when using software on a day-to-day basis. These data sets can feed directly into standard usability practices. We address challenges in collecting, filtering, and summarizing queries, and show how data can be collected at very low cost, even without direct access to raw query logs.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {417–418},
numpages = {2},
keywords = {query log analysis, internet search},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866250,
author = {Gomez, Steven R.},
title = {Interacting with Live Preview Frames: In-Picture Cues for a Digital Camera Interface},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866250},
doi = {10.1145/1866218.1866250},
abstract = {We present a new interaction paradigm for digital cameras aimed at making interactive imaging algorithms accessible on these devices. In our system, the user creates visual cues in front of the lens during the live preview frames that are continuously processed before the snapshot is taken. These cues are recognized by the camera's image processor to control the lens or other settings. We design and analyze vision-based camera interactions, including focus and zoom controls, and argue that the vision-based paradigm offers a new level of photographer control needed for the next generation of digital cameras.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {419–420},
numpages = {2},
keywords = {digital photography, interaction, computer vision},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866251,
author = {Hartmann, Bj\"{o}rn and Dhillon, Mark},
title = {HyperSource: Bridging the Gap between Source and Code-Related Web Sites},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866251},
doi = {10.1145/1866218.1866251},
abstract = {Programmers frequently use the Web while writing code: they search for libraries, code examples, tutorials, documentation, and engage in discussions on Q&amp;A forums. This link between code and visited Web pages largely remains implicit today. Connecting source code and (selective) browsing history can help programmers maintain context, reduce the cost of Web content re-retrieval, and enhance understanding when code is shared. This paper introduces HyperSource, an IDE augmentation that associates browsing histories with source code edits. HyperSource comprises a browser extension that logs visited pages; a novel source document format that maps visited pages to individual characters; and a user interface that enables interaction with these histories.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {421–422},
numpages = {2},
keywords = {browsing history, augmented source code},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866252,
author = {Higuchi, Hideaki and Nojima, Takuya},
title = {Shoe-Shaped i/o Interface},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866252},
doi = {10.1145/1866218.1866252},
abstract = {In this research, we propose a shoe-shaped I/O interface. The benefits to users of wearable devices are significantly reduced if they are aware of them. Wearable devices should have the ability to be worn without requiring any attention from the user. However, previous wearable systems required users to be careful and be aware of wearing or carrying them. To solve this problem, we propose a shoe-shaped I/O interface. By wearing the shoes throughout the day, users soon cease to be conscious of them. Electromechanical devices are potentially easy to install in shoes. This report describes the concept of a shoe-shaped I/O interface, the development of a prototype system, and possible applications.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {423–424},
numpages = {2},
keywords = {wearable devices, shoe-shaped interface, projectors},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866253,
author = {Ichikawa, Takashi and Nojima, Takuya},
title = {Development of the Motion-Controllable Ball},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866253},
doi = {10.1145/1866218.1866253},
abstract = {In this report, we propose a novel ball type interactive interface device. Balls are one of the most important pieces of equipment used for entertainment and sports. Their motion guides a player's response in terms of, for example, a feint or similar movement. Many kinds of breaking ball throws have been developed for various sports(e.g. baseball). However, acquiring the skill to appropriately react to these breaking balls is often hard to achieve and requires long-term training. Many researchers focus on the ball itself and have developed interactive balls with visual and acoustic feedbacks. However, these balls do not have the ability for motion control. In this paper, we introduce a ball-type motion control interface device. It is composed of a ball and an air-pressure tank to change its vector using gas ejection. We conducted an experiment that measures the ball's flight path while subjected to gas ejection and the results showed that the prototype system had enough power to change the ball's vector while flying},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {425–426},
numpages = {2},
keywords = {ball interface, augmented sports, air pressure},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866254,
author = {Jayatilaka, Lahiru G. and Bertuccelli, Luca F. and Staszewski, James and Gajos, Krzysztof Z.},
title = {PETALS: A Visual Interface for Landmine Detection},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866254},
doi = {10.1145/1866218.1866254},
abstract = {Post-conflict landmines have serious humanitarian repercussions: landmines cost lives, limbs and land. The primary method used to locate these buried devices relies on the inherently dangerous and difficult task of a human listening to audio feedback from a metal detector. Researchers have previously hypothesized that expert operators respond to these challenges by building mental patterns with metal detectors through the identification of object-dependent spatially distributed metallic fields. This paper presents the preliminary stages of a novel interface - Pattern Enhancement Tool for Assisting Landmine Sensing (PETALS) - that aims to assist with building and visualizing these patterns, rather than relying on memory alone. Simulated demining experiments show that the experimental interface decreases classification error from 23% to 5% and reduces localization error by 54%, demonstrating the potential for PETALS to improve novice deminer safety and efficiency.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {427–428},
numpages = {2},
keywords = {landmine detection, assistive visual interface, petals, humanitarian demining, spatial patterns representation},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866255,
author = {Karrer, Thorsten and Wittenhagen, Moritz and Heller, Florian and Borchers, Jan},
title = {Pinstripe: Eyes-Free Continuous Input Anywhere on Interactive Clothing},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866255},
doi = {10.1145/1866218.1866255},
abstract = {We present Pinstripe, a textile user interface element for eyes-free, continuous value input on smart garments that uses pinching and rolling a piece of cloth between your fingers. Input granularity can be controlled by the amount of cloth pinched. Pinstripe input elements are invisible, and can be included across large areas of a garment. Pinstripe thus addresses several problems previously identified in the placement and operation of textile UI elements on smart clothing.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {429–430},
numpages = {2},
keywords = {continuous input, wearable computing, smart textiles, eyes-free interaction},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866256,
author = {Kim, Hyunjung and Lee, Woohun},
title = {Kinetic Tiles: Modular Construction Units for Interactive Kinetic Surfaces},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866256},
doi = {10.1145/1866218.1866256},
abstract = {We propose and demonstrate Kinetic Tiles, modular con-struction units for Interactive Kinetic Surfaces (IKSs). We aimed to design Kinetic Tiles to be accessible and available so that users can construct IKSs easily and rapidly. The components of Kinetic Tiles are inexpensive and easily available. In addition, the use of magnetic force enables the separation of the surface material and actuators so that users only interact with the tile modules as if constructing a tile mosaic. Kinetic Tiles can be utilized as a new design and architectural material that allows the surfaces of everyday objects and spaces to convey ambient and pleasurable kinetic expressions.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {431–432},
numpages = {2},
keywords = {interactive kinetic surface, kinetic organic interfaces, kinetic design material},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866257,
author = {Kr\"{a}mer, Jan-Peter and Karrer, Thorsten and Diehl, Jonathan and Borchers, Jan},
title = {Stacksplorer: Understanding Dynamic Program Behavior},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866257},
doi = {10.1145/1866218.1866257},
abstract = {To thoroughly comprehend application behavior, programmers need to understand the interactions of objects at runtime. Today, these interactions are often poorly visualized in common IDEs except during debugging. Stacksplorer allows visualizing and traversing potential call stacks in an application even when it is not running by showing callers and called methods in two columns next to the code editor. The relevant information is gathered from the source code automatically.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {433–434},
numpages = {2},
keywords = {programming, ide, navigation},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866258,
author = {Kulkarni, Chinmay E. and Raju, Santosh and Udupa, Raghavendra},
title = {Memento: Unifying Content and Context to Aid Webpage Re-Visitation},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866258},
doi = {10.1145/1866218.1866258},
abstract = {While users often revisit pages on the Web, tool support for such re-visitation is still lacking. Current tools (such as browser histories) only provide users with basic information such as the date of the last visit and title of the page visited. In this paper, we describe a system that provides users with descriptive topic-phrases that aid re-finding. Unlike prior work, our system considers both the content of a webpage and the context in which the page was visited. Preliminary evaluation of this system suggests users find this approach of combining content with context useful.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {435–436},
numpages = {2},
keywords = {topic phrases, internet search, browsing history},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866259,
author = {Lai, Alessandro and Soro, Alessandro and Scateni, Riccardo},
title = {Interactive Calibration of a Multi-Projector System in a Video-Wall Multi-Touch Environment},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866259},
doi = {10.1145/1866218.1866259},
abstract = {Wall-sized interactive displays gain more and more attention as a valuable tool for multiuser applications, but typically require the adoption of projectors tiles. Projectors tend to display deformed images, due to lens distortion and/or imperfection, and because they are almost never perfectly aligned to the projection surface. Multi-projector video-walls are typically bounded to the video architecture and to the specific application to be displayed. This makes it harder to develop interactive applications, in which a fine grained control of the coordinate transformations (to and from user space and model space) is required. This paper presents a solution to such issues: implementing the blending functionalities at an application level allows seamless development of multi-display interactive applications with multi-touch capabilities. The description of the multi-touch interaction, guaranteed by an array of cameras on the baseline of the wall, is beyond the scope of this work which focuses on calibration.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {437–438},
numpages = {2},
keywords = {multi-touch, video-walls},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866260,
author = {Lichtschlag, Leonhard and Borchers, Jan},
title = {CodeGraffiti: Communication by Sketching for Pair Programmers},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866260},
doi = {10.1145/1866218.1866260},
abstract = {In pair programming, two software developers work on their code together in front of a single workstation, one typing, the other commenting. This frequently involves pointing to code on the screen, annotating it verbally, or sketching on paper or a nearby whiteboard, little of which is captured in the source code for later reference. CodeGraffiti lets pair programmers simultaneously write their code, and annotate it with ephemeral and persistent sketches on screen using touch or pen input. We integrated CodeGraffiti into the Xcode software development environment, to study how these techniques may improve the pair programming workflow.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {439–440},
numpages = {2},
keywords = {pair programming, code annotation, pen input},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866261,
author = {Mistry, Pranav and Maes, Patricia},
title = {Mouseless},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866261},
doi = {10.1145/1866218.1866261},
abstract = {In this short paper we present Mouseless - a novel input device that provides the familiarity of interaction of a physical computer mouse without actually requiring a real hardware mouse. The paper also briefly describes hardware and software implementation of the prototype system and discusses interactions supported.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {441–442},
numpages = {2},
keywords = {desktop computing, mouse, multi-touch, gestural interaction, input device},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866262,
author = {Mujibiya, Adiyan and Miyaki, Takashi and Rekimoto, Jun},
title = {Anywhere Touchtyping: Text Input on Arbitrary Surface Using Depth Sensing},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866262},
doi = {10.1145/1866218.1866262},
abstract = {In this paper, touch typing enabled virtual keyboard system using depth sensing on arbitrary surface is proposed. Keystroke event detection is conducted using 3-dimensional hand appearance database matching combined with fingertip's surface touch sensing. Our prototype system acquired hand posture depth map by implementing phase shift algorithm for Digital Light Processor (DLP) fringe projection on arbitrary flat surface. The system robustly detects hand postures on the sensible surface with no requirement of hand position alignment on virtual keyboard frame. The keystroke feedback is the physical touch to the surface, thus no specific hardware must be worn. The system works real-time in average of 20 frames per second.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {443–444},
numpages = {2},
keywords = {virtual keyboard, touch typing, depth sensing},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866263,
author = {M\"{u}ller, Stefanie and Miller, Gregor and Fels, Sidney},
title = {Using Temporal Video Annotation as a Navigational Aid for Video Browsing},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866263},
doi = {10.1145/1866218.1866263},
abstract = {Video is a complex information space that requires advanced navigational aids for effective browsing. The increasing number of temporal video annotations offers new opportunities to provide video navigation according to a user's needs. We present a novel video browsing interface called TAV (Temporal Annotation Viewing) that provides the user with a visual overview of temporal video annotations. TAV enables the user to quickly determine the general content of a video, the location of scenes of interest and the type of annotations that are displayed while watching the video. An ongoing user study will evaluate our novel approach.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {445–446},
numpages = {2},
keywords = {video browsing, video navigation, video search, video annotation},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866264,
author = {Ng, Wai Shan (Florence) and Sharlin, Ehud},
title = {Tweeting Halo: Clothing That Tweets},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866264},
doi = {10.1145/1866218.1866264},
abstract = {People often like to express their unique personalities, interests, and opinions. This poster explores new ways that allow a user to express her feelings in both physical and virtual settings. With our Tweeting Halo, we demonstrate how a wearable lightweight projector can be used for self-expression very much like a hairstyle, makeup or a T-shirt imprint. Our current prototype allows a user to post a message physically above their head and virtually on Twitter at the same time. We also explore simple ways that will allow physical followers of the Tweeting Halo user to easily become virtual followers by simply taking a snapshot of her projected tweet with a mobile device such as a camera phone. In this extended abstract we present our current prototype, and the results of a design critique we performed using it.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {447–448},
numpages = {2},
keywords = {personal halo, wearable interfaces, social networking, microblogging, personal projector},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866265,
author = {Ruiz, Jaime and Li, Yang},
title = {DoubleFlip: A Motion Gesture Delimiter for Interaction},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866265},
doi = {10.1145/1866218.1866265},
abstract = {In order to use motion gestures with mobile devices it is imperative that the device be able to distinguish between input motion and everyday motion. In this abstract we present DoubleFlip, a unique motion gesture designed to act as an input delimiter for mobile motion gestures. We demonstrate that the DoubleFlip gesture is extremely resistant to false positive conditions, while still achieving high recognition accuracy. Since DoubleFlip is easy to perform and less likely to be accidentally invoked, it provides an always-active input event for mobile interaction.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {449–450},
numpages = {2},
keywords = {mobile interaction, motion gestures, sensors},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866266,
author = {Smith, Daniel A. and Lambert, Joe and schraefel, mc and Bretherton, David},
title = {QWIC: Performance Heuristics for Large Scale Exploratory User Interfaces},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866266},
doi = {10.1145/1866218.1866266},
abstract = {Faceted browsers offer an effective way to explore relationships and build new knowledge across data sets. So far, web-based faceted browsers have been hampered by limited feature performance and scale. QWIC, Quick Web Interface Control, describes a set of design heuristics to address performance speed both at the interface and the backend to operate on large-scale sources.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {451–452},
numpages = {2},
keywords = {performance, faceted browsing, scalability},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866267,
author = {Teitelbaum, Louis-Jean},
title = {What Interfaces Mean: A History and Sociology of Computer Windows},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866267},
doi = {10.1145/1866218.1866267},
abstract = {This poster presents a cursory look at the history of windows in Graphical User Interfaces. It examines the controversy between tiling and overlapping window managers and explains that controversy's sociological importance: windows are control devices, enabling their users to manage their activity and attention. It then explores a few possible reasons for the relative disappearance of windowing in recent computing devices. It concludes with a recapitulative typology.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {453–454},
numpages = {2},
keywords = {windows, history, sociology, activity},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866268,
author = {Weibel, Nadir and Piper, Anne Marie and Hollan, James D.},
title = {Exploring Pen and Paper Interaction with High-Resolution Wall Displays},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866268},
doi = {10.1145/1866218.1866268},
abstract = {We introduce HIPerPaper, a novel digital pen and paper interface that enables natural interaction with a 31.8 by 7.5 foot tiled wall display of 268,720,000 pixels. HIPerPaper provides a flexible, portable, and inexpensive medium for interacting with large high-resolution wall displays. While the size and resolution of such displays allow visualization of data sets of a scale not previously possible, mechanisms for interacting with wall displays remain challenging. HIPerPaper enables multiple concurrent users to select, move, scale, and rotate objects on a high-dimension wall display.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {455–456},
numpages = {2},
keywords = {wall display, paper, digital pen},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866269,
author = {Yu, Neng-Hao and Chan, Li-Wei and Cheng, Lung-Pan and Chen, Mike Y. and Hung, Yi-Ping},
title = {Enabling Tangible Interaction on Capacitive Touch Panels},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866269},
doi = {10.1145/1866218.1866269},
abstract = {We propose two approaches to sense tangible objects on capacitive touch screens, which are used in off-the-shelf multi-touch devices such as Apple iPad, iPhone, and 3M's multi-touch displays. We seek for the approaches that do not require modifications to the panels: spatial tag and frequency tag. Spatial tag is similar to fiducial tag used by tangible tabletop surface interaction, and uses multi-point, geometric patterns to encode object IDs. Frequency tag simulates high-frequency touches in the time domain to encode object IDs, using modulation circuits embedded inside tangible objects to simulate high-speed touches in varying frequency. We will show several demo applications. The first combines simultaneous tangible + touch input system. This explores how tangible inputs (e.g., pen, easer, etc.) and some simple gestures work together on capacitive touch panels.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {457–458},
numpages = {2},
keywords = {tangible, physical interaction, markers, interactive surface},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866218.1866270,
author = {Zhao, Ji and Liu, Hujia and Zhang, Chunhui and Zhang, Zhengyou},
title = {MobileSurface: Interaction in the Air for Mobile Computing},
year = {2010},
isbn = {9781450304627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866218.1866270},
doi = {10.1145/1866218.1866270},
abstract = {We describe a virtual interactive surface technology based on a projector-camera system connected to a mobile device. This system, named mobile surface, can project images on any free surfaces and enable interaction in the air within the projection area. The projector used in the system scans a laser beam very quickly across the projection area to produce a stable image at 60 fps. The camera-projector synchronization is applied to obtain the image of the appointed scanning line. So our system can project what is perceived as a stable image onto the display surface, while simulta neously working as a structured light 3D scanning system.},
booktitle = {Adjunct Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {459–460},
numpages = {2},
keywords = {pico-projector, anywhere interaction, projector-camera system, mobile},
location = {New York, New York, USA},
series = {UIST '10}
}

