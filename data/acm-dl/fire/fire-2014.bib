@inproceedings{10.1145/2824864.2824890,
author = {Clough, Paul},
title = {Evaluation: Thinking Outside the (Search) Box},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824890},
doi = {10.1145/2824864.2824890},
abstract = {Evaluation of IR systems has typically focused on the system and specifically assessing the quality of a ranked list of results with respect to a query. However, IR functionality is typically just one component amongst many that are used to help support users' wider information seeking activities. Many systems that include a search box also provide features, such as faceted lists, subject hierarchies, visualizations and recommendations to help users find information. In this paper I discuss experiences gained from developing a system to support exploration and discovery in digital cultural heritage. In particular I focus on the development of system components to support search and navigation and how the different components were evaluated within the development life-cycle of the project. The importance of taking a holistic approach to evaluation, as well as utilising evaluation approaches from domains other than IR, is emphasized. In short, we need to be thinking outside the (search) box when it comes to evaluation in IR.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {1–9},
numpages = {9},
keywords = {case studies, component-based evaluation, IR evaluation},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824886,
author = {Hoque, Md Moinul and Quaresma, Paulo},
title = {SEMONTOQA: A Semantic Understanding-Based Ontological Framework for Factoid Question Answering},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824886},
doi = {10.1145/2824864.2824886},
abstract = {This paper presents an outline of an Ontological and Semantic understanding-based model (SEMONTOQA) for an open-domain factoid Question Answering (QA) system. The outlined model analyses unstructured English natural language texts to a vast extent and represents the inherent contents in an ontological manner. The model locates and extracts useful information from the text for various question types and builds a semantically rich knowledge-base that is capable of answering different categories of factoid questions. The system model converts the unstructured texts into a minimalistic, labelled, directed graph that we call a Syntactic Sentence Graph (SSG). An Automatic Text Interpreter using a set of pre-learnt Text Interpretation Subgraphs and patterns tries to understand the contents of the SSG in a semantic way. The system proposes a new feature and action based Cognitive Entity-Relationship Network designed to extend the text understanding process to an in-depth level. Application of supervised learning allows the system to gradually grow its capability to understand the text in a more fruitful manner. The system incorporates an effective Text Inference Engine which takes the responsibility of inferring the text contents and isolating entities, their features, actions, objects, associated contexts and other properties, required for answering questions. A similar understanding-based question processing module interprets the user's need in a semantic way. An Ontological Mapping Module, with the help of a set of pre-defined strategies designed for different classes of questions, is able to perform a mapping between a question's ontology with the set of ontologies stored in the background knowledge-base. Empirical verification is performed to show the usability of the proposed model. The results achieved show that, this model can be used effectively as a semantic understanding based alternative QA system.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {10–20},
numpages = {11},
keywords = {Text understanding, Syntactic Sentence Graph, Text inference, Text ontology, Semantic mapping, Natural language processing, Deep question answering system},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824878,
author = {Flores, Enrique and Rosso, Paolo and Moreno, Lidia and Villatoro-Tello, Esa\'{u}},
title = {On the Detection of SOurce COde Re-Use},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824878},
doi = {10.1145/2824864.2824878},
abstract = {This paper summarizes the goals, organization and results of the first SOCO competitive evaluation campaign for systems that automatically detect the source code re-use phenomenon. The detection of source code re-use is an important research field for both software industry and academia fields. Accordingly, PAN@FIRE track, named SOurce COde Re-use (SOCO) focused on the detection of re-used source codes in C/C++ and Java programming languages. Participant systems were asked to annotate several source codes whether or not they represent cases of source code re-use. In total five teams submitted 17 runs. The training set consisted of annotations made by several experts, a feature which turns the SOCO 2014 collection in a useful data set for future evaluations and, at the same time, it establishes a standard evaluation framework for future research works on the posed shared task.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {21–30},
numpages = {10},
keywords = {Test collections, Source code re-use, Evaluation framework, SOCO, Plagiarism detection},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824879,
author = {Ram\'{\i}rez-de-la-Cruz, Aar\'{o}n and Ram\'{\i}rez-de-la-Rosa, Gabriela and S\'{a}nchez-S\'{a}nchez, Christian and Jim\'{e}nez-Salazar, H\'{e}ctor},
title = {On the Importance of Lexicon, Structure and Style for Identifying Source Code Plagiarism},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824879},
doi = {10.1145/2824864.2824879},
abstract = {Source code plagiarism can be identified by analyzing similarities of several and diverse aspects of a pair of source code. In this paper we present three types of similarity features that account for three aspects of source code documents, particularly: i) lexical, ii) structural, and iii) stylistics. From the lexical view, we used a character 3-gram model without considering reserved words for the programming language in revision. For the structural view, we proposed two similarity metrics that take into account the function's signatures within a source code, namely the data types and the identifier's names of the function's signature. The third view consists on accounting for several stylistics' features, such as the number of white spaces, lines of code, upper letters, etc. Accordingly, we proposed 8 similarity features to represent pairs of source code in order to, under a supervised approach, identify plagiarized pairs of source codes. We use a set of more than 32000 source code documents from Java and C to perform our experiments. The results show the pertinence of our set of features to identify plagiarism for source code documents that satisfy particular conditions, such as, source code that solve difficult problems.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {31–38},
numpages = {8},
keywords = {structural and stylistic features, Lexical, Plagiarism detection, Document representation, Source code plagiarism},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824887,
author = {Ganguly, Debasis and Jones, Gareth J. F.},
title = {DCU@FIRE-2014: An Information Retrieval Approach for Source Code Plagiarism Detection},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824887},
doi = {10.1145/2824864.2824887},
abstract = {We investigate an information retrieval (IR) based approach to source code plagiarism detection. The standard method plagiarism detection by extensively checking pairwise similarities between documents is not scalable to large collections of source code documents. To make the task of source code plagiarism detection fast and scalable in practice, we propose an IR based approach. In this method each document is treated as a pseudo-query which retrieves a list of documents which are potential candidate for containing plagiarised material in decreasing order of their similarity to the query. A threshold is then applied on the relative similarity decrement ratios to create a set of documents as potential cases of source-code reuse. Instead of treating a source code as an unstructured text document, we explore term extraction from the annotated parse tree of a source code and also make use of a field-based language model for indexing and retrieval of source code documents. Results confirm that source code parsing plays a vital role in improving the plagiarism prediction accuracy.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {39–42},
numpages = {4},
keywords = {Source Code Plagiarism Detection, Field Search},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824870,
author = {Patel, Shraddha and Desai, Vaibhavi},
title = {LIGA and Syllabification Approach for Language Identification and Back Transliteration: A Shared Task Report by DA-IICT},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824870},
doi = {10.1145/2824864.2824870},
abstract = {A huge chunk of user-generated content belonging to languages, based on non-Roman scripts is found written in a Roman script, on the Web. Due to this, search engines face a problem of information retrieval in the transliterated space, where the transliterated content can have a lot of spelling variations. In this paper, as a solution for the aforementioned problem, we discuss our approach for word-level query labeling and subsequent transliteration of Indian language words into their native scripts. We have considered Language Identification Graph Approach to label the words with their language markers; and transliteration using rule based syllabification to obtain transliterated word in its native script. The proposed system is submitted to the FIRE-2014 shared task on Transliterated Search Subtask 1, where it showed the best performing results for the language Gujarati.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {43–47},
numpages = {5},
keywords = {transliteration, LIGA, rule based syllabification},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824872,
author = {Bhat, Irshad Ahmad and Mujadia, Vandan and Tammewar, Aniruddha and Bhat, Riyaz Ahmad and Shrivastava, Manish},
title = {IIIT-H System Submission for FIRE2014 Shared Task on Transliterated Search},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824872},
doi = {10.1145/2824864.2824872},
abstract = {This paper describes our submission for FIRE 2014 Shared Task on Transliterated Search. The shared task features two sub-tasks: Query word labeling and Mixed-script Ad hoc retrieval for Hindi Song Lyrics.Query Word Labeling is on token level language identification of query words in code-mixed queries and back-transliteration of identified Indian language words into their native scripts. We have developed letter based language models for the token level language identification of query words and a structured perceptron model for back-transliteration of Indic words.The second subtask for Mixed-script Ad hoc retrieval for Hindi Song Lyrics is to retrieve a ranked list of songs from a corpus of Hindi song lyrics given an input query in Devanagari or transliterated Roman script. We have used edit distance based query expansion and language modeling followed by relevance based reranking for the retrieval of relevant Hindi Song lyrics for a given query.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {48–53},
numpages = {6},
keywords = {Perplexity, Language Modeling, Transliteration, Information Retrieval, Language Identification},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824876,
author = {Banerjee, Somnath and Kuila, Alapan and Roy, Aniruddha and Naskar, Sudip Kumar and Rosso, Paolo and Bandyopadhyay, Sivaji},
title = {A Hybrid Approach for Transliterated Word-Level Language Identification: CRF with Post-Processing Heuristics},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824876},
doi = {10.1145/2824864.2824876},
abstract = {In this paper, we describe a hybrid approach for word-level language (WLL) identification of Bangla words written in Roman script and mixed with English words as part of our participation in the shared task on transliterated search at Forum for Information Retrieval Evaluation (FIRE) in 2014. A CRF based machine learning model and post-processing heuristics are employed for the WLL identification task. In addition to language identification, two transliteration systems were built to transliterate detected Bangla words written in Roman script into native Bangla script. The system demonstrated an overall token level language identification accuracy of 0.905. The token level Bangla and English language identification F-scores are 0.899, 0.920 respectively. The two transliteration systems achieved accuracies of 0.062 and 0.037. The word-level language identification system presented in this paper resulted in the best scores across almost all metrics among all the participating systems for the Bangla-English language pair.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {54–59},
numpages = {6},
keywords = {Transliteration, Word level language identification, code switch},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824877,
author = {Gupta, Deepak Kumar and Kumar, Shubham and Ekbal, Asif},
title = {Machine Learning Approach for Language Identification &amp; Transliteration},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824877},
doi = {10.1145/2824864.2824877},
abstract = {In this paper, we describe the system that we developed as part of our participation to the FIRE-2014 Shared Task on Transliterated Search. We participated only for Subtask 1 that focused on labeling the query words. The entire process consists of the following components: language identification of each word in text, named entity recognition and classification (NERC) and transliteration of Indian language words written in non-native scripts to the corresponding native Indic scripts. The proposed methods of language identification and NERC are based on supervised approaches, where we use several machine learning algorithms. Our transliteration framework is based on modified joint source channel model. Experiments on benchmark setup show that we achieve quite encouraging performance for both the pairs of languages, viz. Bangla-English and Hindi-English. It is also to be noted that we did not make use of heavy domain-specific resources and/or tools, and therefore this can be easily adapted to the other domains and/or languages.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {60–64},
numpages = {5},
keywords = {NERC, Transliteration, Modified Joint-Source Channel Model, Language Identification, Ensemble},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824880,
author = {Prabhakar, Dinesh Kumar and Pal, Sukomal},
title = {ISM@FIRE-2014: Shared Task on Transliterated Search},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824880},
doi = {10.1145/2824864.2824880},
abstract = {This paper describe approaches we used for the Shared Task on Transliterated Search in FIRE-2014. The approaches solve identification of native languages of given terms/words and their labeling. MaxEnt a supervised classifier is used for the classification and labeling. The word labeling completion is followed by back-transliteration of Hindi H labeled terms. For the back-transliteration we have used generative and GEN-EXT(combination of generative and extraction) approaches. From the evaluation it has been seen that our Runs has performed best in some metrics like LA, LP, LF, EF etc. For some metrics runs performed closed to 2nd and 3rd best, and in some other performance was poor as well.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {65–69},
numpages = {5},
keywords = {Word classification, Transliteration, Word labeling},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824871,
author = {Sequiera, Royal Denzil and Rao, Shashank S and Shambavi, B. R.},
title = {Word-Level Language Identification and Back Transliteration of Romanized Text},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824871},
doi = {10.1145/2824864.2824871},
abstract = {This paper presents the BMSCE team's participation in `FIRE Shared Task on Transliterated Search subtask-1'. Our Language Identification system is based on the n-grams approach and uses a tri-gram language identifier trained over a shared and collected training set to classify the language of a word at the. We use a rule based approach blended with simple dictionary search to back transliterate the Romanized Kannada word. We participated in the Bengali-English, Guajarati-English, Kannada-English, Malayalam-English and Tamil-English language tracks and have obtained 70-80% accuracy for the language pairs.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {70–73},
numpages = {4},
keywords = {Word level language classification, transliteration, n-grams},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824884,
author = {Raj, Abhinav},
title = {Word Level Language Identification and Back-Transliteration},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824884},
doi = {10.1145/2824864.2824884},
abstract = {In this paper, I describe a Rule based and List-Searching system for Word-Level Language Identification and Named Entity recognition in bilingual text. My method uses dictionary search, rules for LI and CRF++, character n-gram for NER. The model does not use any Language specific rules, therefore can easily be replicated on most languages having mixed pair with English. The model also does back-transliteration of words into native language script and recognizes named entity. The model performance is carried on the test sets provided by the shared task on language Identification for English Hindi (En-Hi) Pair, Microsoft Research India. The experimental results show a consistent performance with high precision.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {74–79},
numpages = {6},
keywords = {N-gram, Mixed Script, NLP, NER, bilingual text, Hindi Words, CRF, Language Identification, Back-Transliteration, Indian Language},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824888,
author = {Ganguly, Debasis and Pal, Santanu and Jones, Gareth J. F.},
title = {DCU@FIRE-2014: Fuzzy Queries with Rule-Based Normalization for Mixed Script Information Retrieval},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824888},
doi = {10.1145/2824864.2824888},
abstract = {We describe the participation of Dublin City University (DCU) in the FIRE-2014 shared task on transliteration search, hereby referred to as the TST (Transliteration Search Task). The TST involves an ad-hoc search over a collection of Hindi film song lyrics. The Hindi language content of each document of the collection is either written in the native script or transliterated in Roman script or both. The queries may also be in mixed script. The task is challenging primarily because of the vocabulary mismatch which may arise due to the multiple transliteration alternatives. We attempt to address the vocabulary mismatch problem both during the indexing and retrieval. During indexing, we apply a rule-based normalization on some character sequences of the transliterated words in order to have a single representation in the index for the multiple transliteration alternatives. During the retrieval phase, we make use of prefix matched fuzzy query terms to account for the morphological variations of the transliterated words. The results show significant improvement of about 56% over a standard baseline query likelihood language modelling (LM) approach. Additionally, we also apply statistical machine transliteration to train a transliteration model in order to predict the transliteration of out-of-vocabulary words. Surprisingly, even with satisfactory transliteration accuracy, we find that automatic transliteration of query terms degrades retrieval effectiveness.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {80–85},
numpages = {6},
keywords = {Statistical Machine Transliteration, Fuzzy Query, Rule-based Normalization},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824873,
author = {Mukherjee, Abhinav and Ravi, Anirudh and Datta, Kaustav},
title = {Mixed-Script Query Labelling Using Supervised Learning and Ad Hoc Retrieval Using Sub Word Indexing},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824873},
doi = {10.1145/2824864.2824873},
abstract = {Much of the user generated content on the internet is written in their transliterated form instead of in their indigenous script. Due to this search engines receive a large number of transliterated search queries.This paper presents our approach to handle labelling of queries and ad hoc retrieval of documents based on these queries, as part of the FIRE2014 shared task on transliterated search. The content of each document is written in either the native Devanagari script or its transliterated form in Roman script or a combination of both. The queries to retrieve these documents can also be in mixed script. The task is challenging primarily due to the spelling variations that occur in the transliterated form of search queries. This particular problem is addressed by using back transliteration to reduce spelling variations, and a set of hand-tailored rules for consonant mapping. Sub-word indexing is done to take care of breaking and joining of transliterated words. Implementation of query labelling of the mixed script content was done using a supervised learning approach where an SVM classifier was trained using character n-grams as features for language identification. A Na\"{\i}ve Bayes classifier was used for classifying transliterated words that can belong to both Hindi and English when looked at individually.The 2 runs submitted by our team (BITS-Lipyantaran) performs best across all metrics for Subtask 2 among all the teams that participated, with a MRR score of 0.8171 and MAP score of 0.6421.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {86–90},
numpages = {5},
keywords = {Language Identification, Sub-word Indexing, Support Vector Machines, Na\"{\i}ve Bayes, Language Modelling, Machine Learning, Supervised learning, Natural Language Processing, Mixed-script information retrieval},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824874,
author = {Bankapur, Channa and Philip, Adithya Abraham and Heblikar, Saimadhav},
title = {Query Word Labeling Using Supervised Machine Learning: Shared Task Report by PESIT Team},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824874},
doi = {10.1145/2824864.2824874},
abstract = {We have participated in the first sub task of FIRE 2014 Shared Task on Transliterated Search where the Indian language is Hindi. The aim of this task is to identify words as belonging to an Indian language (L) or English (E) from sentences written in Roman script and if the word belongs to Indian language (L), transliterate the same to its Devanagari script equivalent. We have used a supervised machine learning approach to classify words as belonging to either L or E and a rule-based approach for transliterating words by first splitting them into consonant or vowel blocks.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {91–97},
numpages = {7},
keywords = {Bilingual classification, query word labeling, transliteration},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824881,
author = {Prabhakar, Dinesh Kumar and Dubey, Shantanu and Goel, Bharti and Pal, Sukomal},
title = {ISM@FIRE-2014: Named Entity Recognition for Indian Languages},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824881},
doi = {10.1145/2824864.2824881},
abstract = {This paper describes the approaches we have adopted to identify the classes of named entities and annotate them with proper tags. Named Entity Recognition (NER) is a subtask of information extraction that seeks to locate and classify the entity names (proper nouns) in a text. NER systems are useful in many Natural Language Processing (NLP) applications such as question answering, machine translation, information extraction and so on. IN this task, Conditional Random Field (CRF) is used for the classification where tokens are classified hierarchically. We discuss two approaches: one for the official submission and the other its extended version. The performance of our systems were moderate.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {98–102},
numpages = {5},
keywords = {Named Entity Recognition, Classification},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824882,
author = {Abinaya, N. and John, Neethu and Ganesh, Barathi H. B. and Kumar, Anand M. and Soman, K. P.},
title = {AMRITA_CEN@FIRE-2014: Named Entity Recognition for Indian Languages Using Rich Features},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824882},
doi = {10.1145/2824864.2824882},
abstract = {This paper aims at implementing Named Entity Recognition (NER) for four languages such as English, Tamil, Hindi and Malayalam. The results obtained from this work are submitted to a research evaluation workshop Forum for Information Retrieval and Evaluation (FIRE 2014). This system detects three levels of named entity tags which are referred as nested named entities. It is a multi-label problem solved using chain classifier method. In this work, Conditional Random Field (CRF) and Support Vector Machine (SVM) are used for implementing NER system. In FIRE 2014, we developed a English NER system using CRF and other NER system for Tamil, Hindi and Malayalam are based on SVM. The FIRE estimated the average precision for all the four languages as 41.93 for outermost level and 33.25 for inner level. In order to improve the performance of Indian languages, we implemented CRF based NER system for the same corpus in Tamil, Hindi and Malayalam. The average precision measure for these mentioned languages are 42.87 for outer level and 36.31 for inner level. The overall performance of the NER system improved by 2.24% for outer level and 9.20% for inner level.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {103–111},
numpages = {9},
keywords = {Natural Language Processing (NLP), Conditional Random Fields (CRF), Named Entity Recognition (NER), Support Vector Machine (SVM)},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824883,
author = {kumar, Anand M. and Soman, K. P.},
title = {AMRITA_CEN@FIRE-2014: Morpheme Extraction and Lemmatization for Tamil Using Machine Learning},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824883},
doi = {10.1145/2824864.2824883},
abstract = {This paper presents the method of Morpheme Extraction and lemmatization for Tamil language in Morpheme Extraction Task (MET) of FIRE-2014. Tamil is a morphologically rich and agglutinative language. Such a language needs deeper analysis at the word level to capture the meaning of the word from its morphemes and its categories. In this attempt, the methodology employed to extract Tamil morphemes and lemmas are based on a supervised machine learning algorithm for nouns and verbs and simple suffix stripping for pronouns and proper nouns. Morphemes are extracted for other Part-of-Speech categories using Tamil Part of Speech tagger. In supervised learning, Morphological analyzer problem is redefined as a classification problem. We decompose the problem of noun and verb morpheme extraction into two sub-problems: learning to perform morpheme identification of words in a text, and learning to perform morpheme tagging. In addition to the Morpheme extraction task results of FIRE-2014, we have carried out different experiments to show the effectiveness of the proposed method.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {112–120},
numpages = {9},
keywords = {Morpheme Extraction, Lemmatization, Support Vector Machines, Natural Language Processing, Machine Learning},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824869,
author = {Chakraborty, Anirban and Ghosh, Kripabandhu and Parui, Swapan Kumar},
title = {Building Test Collection from Old IR Literature},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824869},
doi = {10.1145/2824864.2824869},
abstract = {Standard test collections form the very basis of Information Retrieval research and evaluation. Important datasets have been created to promote empirical research and experimentation. In this paper, we describe our endeavour in creating a test collection from old, archived writings of IR stalwarts. The documents are created in text format from the scanned and OCRed version. The test collection consists of a set of documents in TREC format along with a set of expert queries and their relevance assessments. This dataset, though small in size, would be of paramount interest for researchers and students of IR since it contains valuable discourses on the discipline from its very inception. Also, to the best of our knowledge, no standard IR dataset has been built so far comprising old research articles. Furthermore, this is a dataset without the original error-free digital text version. So, the resulting collection would expect researchers to run retrieval experiments on the erroneous collection without the scope of error modeling. This would invite new research ideas.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {121–125},
numpages = {5},
keywords = {Test Collection, OCR Errors, Old Literature},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824865,
author = {Kanapala, Ambedkar and Pal, Sukomal},
title = {Test Collection for Legal IR from Online Discussion Forums},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824865},
doi = {10.1145/2824864.2824865},
abstract = {With proliferation of online discussion forums, legal data on the Web is increasing. A number of online sites provide platforms for discussion, counselling and assistance pertaining to legal problems where a lay person can ask questions and/or seek assistance and volunteers share their views, expert opinions. Although these forums can provide legal help at rudimentary level, increasing number of users consult and often their legal information need gets complemented by the forums. Lack of easy natural language search facility in these forums, however, deprives a novice user from quickly retrieving answers to similar questions asked in the past. Like all other empirical discipline, measurable technological progress in legal IR requires a quantitative evaluation framework that provides standard, well-defined experimental setup, benchmark datasets and evaluation metrics. The aim of building this test collection is to provide a credible testbed for legal IR from online discussion forums. The data has been collected by crawling a number of free online legal discussion forums such as lawguru.com, legalservice.co.in covering different type of legal cases like criminal law, consumer law, constitutional law etc.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {126–129},
numpages = {4},
keywords = {Information Retrieval, Corpus, Legal text},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824866,
author = {Li, Guofu and Ghosh, Aniruddha and Veale, Tony},
title = {Constructing A Corpus Of Figurative Language For a Tweet Classification and Retrieval Task},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824866},
doi = {10.1145/2824864.2824866},
abstract = {Twitter is an intriguing source of topical content for tasks involving the detection of phenomena such as sarcasm and metaphor. The hashtags that users employ to self-annotate their own micro-texts can often facilitate the targeted retrieval of texts with the desired characteristics. Though tweets tagged with #sarcasm are highly likely to be sarcastic, the lack of a topic model for sarcastic tweets makes it difficult to detect when such tags are used in the expected way, or indeed, to retrieve tweets that are not explicitly tagged in this way. In this study, we explore how a tweet-retrieval and classification system can benefit from a topic model when constructing a task-specific Twitter corpus, such as for irony, sarcasm or metaphor detection.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {130–133},
numpages = {4},
keywords = {Information retrieval, Figurative Language, Twitter, Latent Semantic Analysis, Query Expansion},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824867,
author = {Mertens, Laurent and Demeester, Thomas and Deleu, Johannes and Feys, Matthias and Develder, Chris},
title = {Entity Linking: Test Collections Revisited},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824867},
doi = {10.1145/2824864.2824867},
abstract = {This paper analyzes two important conditions that are usually taken for granted in the evaluation of information retrieval systems: the test queries should be representative for the intended application scenario, and a sufficient amount of queries are needed to robustly assess system performance, as well as discern performance differences between systems. Both issues have important consequences, as studied in this paper for the specific case of Entity Linking systems. We investigate two methods for automatic query generation, and show them to have a vast impact on evaluated system performance. We further demonstrate the effect a query set's size has on its ability to faithfully distinguish systems, and propose a method for assessing the possible impact on system performance that adding a specific number of queries to the set might have.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {134–137},
numpages = {4},
keywords = {Entity Linking, query selection, Evaluation},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824868,
author = {Feys, Matthias and Demeester, Thomas and Fortuna, Blaz and Deleu, Johannes and Develder, Chris},
title = {On the Robustness of Event Detection Evaluation: A Case Study},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824868},
doi = {10.1145/2824864.2824868},
abstract = {Research on evaluation of IR systems has led to the insight that a robust evaluation strategy requires tests on a large number of events/queries. However, especially for event detection, the number of manually labeled events may be limited. In this paper we investigate how to optimize the evaluation strategy in those cases to maximize robustness. We also introduce two new vector space models for event detection that aim to incorporate bursty information of terms and compare these with existing models. Experiments show that exploiting graded relevance levels reduces the impact of subjectivity and ambiguity of event detection evaluation. We also show that although user disagreement is significant, it has no real impact on result ranking.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {138–141},
numpages = {4},
keywords = {Event Detection, Vector Space Models, Evaluation},
location = {Bangalore, India},
series = {FIRE '14}
}

@inproceedings{10.1145/2824864.2824889,
author = {Yadav, Praveen and Pal, Sukomal and Kumar, Rishabh and Singh, Surender and Singh, Harsh},
title = {Popular Acronym Retrieval through Text Messaging},
year = {2014},
isbn = {9781450337557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2824864.2824889},
doi = {10.1145/2824864.2824889},
abstract = {We present a prototype system for providing quick information on common and popular abbreviations through text messaging. The system receives a text input of acronym (possibly wrongly typed) in Roman script. The application returns a very brief information from the first few lines of corresponding English Wikipedia page. The system is designed especially for low-cost mobile phones having text only messaging facility but without Internet and native language support. The target users are primarily semi-literate people who may not have sufficient knowledge of English. The output is translated to native language of user query (Hindi) as transliterated text.},
booktitle = {Proceedings of the Forum for Information Retrieval Evaluation},
pages = {142–145},
numpages = {4},
keywords = {Transliteration, Machine Translation, Web-scrapping},
location = {Bangalore, India},
series = {FIRE '14}
}

