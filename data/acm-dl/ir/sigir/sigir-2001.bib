@inproceedings{10.1145/383952.383953,
author = {Lam-Adesina, Adenike M. and Jones, Gareth J. F.},
title = {Applying Summarization Techniques for Term Selection in Relevance Feedback},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383953},
doi = {10.1145/383952.383953},
abstract = {Query-expansion is an effective Relevance Feedback technique for improving performance in Information Retrieval. In general query-expansion methods select terms from the complete contents of relevant documents. One problem with this approach is that expansion terms unrelated to document relevance can be introduced into the modified query due to their presence in the relevant documents and distribution in the document collection. Motivated by the hypothesis that query-expansion terms should only be sought from the most relevant areas of a document, this investigation explores the use of document summaries in query-expansion. The investigation explores the use of both context-independent standard summaries and query-biased summaries. Experimental results using the Okapi BM25  probabilistic retrieval model with the TREC-8 ad hoc retrieval task show that query-expansion using document summaries can be considerably more effective than using full-document expansion. The paper also presents a novel approach to term-selection that separates the choice of relevant documents from the selection of a pool of potential expansion terms. Again, this technique is shown to be more effective that standard methods.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1–9},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383954,
author = {Allan, James and Gupta, Rahul and Khandelwal, Vikas},
title = {Temporal Summaries of New Topics},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383954},
doi = {10.1145/383952.383954},
abstract = {We discuss technology to help a person monitor changes in news coverage over time.  We define temporal summaries of news stories as extracting a single sentence from each event within a news topic, where the stories are presented one at a time and sentences from a story must be ranked before the next story can be considered. We explain a method for evaluation, and describe an evaluation corpus that we have built.  We also propose several methods for constructing temporal summaries and evaluate their effectiveness in comparison to degenerate cases.  We show that simple approaches are effective, but that the problem is far from solved.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {10–18},
numpages = {9},
keywords = {summarization, metrics, experimental design},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383955,
author = {Gong, Yihong and Liu, Xin},
title = {Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383955},
doi = {10.1145/383952.383955},
abstract = {In this paper, we propose two generic text summarization methods that create text summaries by ranking and extracting sentences from the original documents. The first method uses standard IR methods to rank sentence relevances, while the second method uses the latent semantic analysis technique to identify semantically important sentences, for summary creations. Both methods strive to select sentences that are highly ranked and different from each other. This is an attempt to create a summary with a wider coverage of the document's main content and less redundancy. Performance evaluations on the two summarization methods are conducted by comparing their summarization outputs with the manual summaries generated by three independent human evaluators. The evaluations also study the influence of different VSM weighting schemes on the text summarization performances. Finally, the causes of the large disparities in the evaluators' manual summarization results are investigated, and discussions on human text summarization patterns are presented.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {19–25},
numpages = {7},
keywords = {relevance measure, generic text summarization, semantic analysis},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383956,
author = {Nomoto, Tadashi and Matsumoto, Yuji},
title = {A New Approach to Unsupervised Text Summarization},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383956},
doi = {10.1145/383952.383956},
abstract = {The paper presents a novel approach to unsupervised text summarization. The novelty lies in exploiting the diversity of concepts in text for summarization, which has not received much attention in the summarization literature. A diversity-based approach here is a principled generalization of Maximal Marginal Relevance criterion by Carbonell and Goldstein cite{carbonell-goldstein98}.We propose, in addition, aninformation-centricapproach to evaluation, where the quality of summaries is judged not in terms of how well they match human-created summaries but in terms of how well they represent their source documents in IR tasks such document retrieval and text categorization.To find the effectiveness of our approach under the proposed evaluation scheme, we set out to  examine how a system with the diversity functionality performs against one without, using the BMIR-J2 corpus, a test data developed by a Japanese research consortium. The results demonstrate a clear superiority of a diversity based approach to a non-diversity based approach.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {26–34},
numpages = {9},
keywords = {text summarization},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383957,
author = {Anh, Vo Ngoc and de Kretser, Owen and Moffat, Alistair},
title = {Vector-Space Ranking with Effective Early Termination},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383957},
doi = {10.1145/383952.383957},
abstract = {Considerable research effort has been invested in improving the effectiveness of information retrieval systems. Techniques such as relevance feedback, thesaural expansion, and pivoting all provide better quality responses to queries when tested in standard evaluation frameworks. But such enhancements can add to the cost of evaluating queries. In this paper we consider the pragmatic issue of how to improve the cost-effectiveness of searching. We describe a new inverted file structure using quantized weights that provides superior retrieval effectiveness compared to conventional inverted file structures when early termination heuristics are employed. That is, we are able to reach similar effectiveness levels with less computational cost, and so provide a better cost/performance compromise than previous inverted file organisations.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {35–42},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383958,
author = {Carmel, David and Cohen, Doron and Fagin, Ronald and Farchi, Eitan and Herscovici, Michael and Maarek, Yoelle S. and Soffer, Aya},
title = {Static Index Pruning for Information Retrieval Systems},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383958},
doi = {10.1145/383952.383958},
abstract = {We introduce static index pruning methods that significantly reduce the index size in information retrieval systems.We investigate uniform and term-based methods that each remove selected entries from the index and yet have only a minor effect on retrieval results. In uniform pruning, there is a fixed cutoff threshold, and all index entries whose contribution to relevance scores is bounded above by a given threshold are removed from the index. In term-based pruning, the cutoff threshold is determined for each term, and thus may vary from term to term. We give experimental evidence that for each level of compression, term-based pruning outperforms uniform pruning, under various measures of precision. We present theoretical and experimental evidence that under our term-based pruning scheme, it is possible to prune the index greatly and still get retrieval results that are almost as good as those based on the full index.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {43–50},
numpages = {8},
keywords = {compression, indexing},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383959,
author = {Saraiva, Patricia Correia and Silva de Moura, Edleno and Ziviani, Nivio and Meira, Wagner and Fonseca, Rodrigo and Ribeiro-Neto, Berthier},
title = {Rank-Preserving Two-Level Caching for Scalable Search Engines},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383959},
doi = {10.1145/383952.383959},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {51–58},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383960,
author = {Stent, Amanda and Loui, Alexander},
title = {Using Event Segmentation to Improve Indexing of Consumer Photographs},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383960},
doi = {10.1145/383952.383960},
abstract = {Automatic albuming --- the automatic organization of photographs, either as an end in itself or for use in other applications -- is an application that promises to be of great assistance to photographers.  Relatively sophisticated image content analysis techniques have been used for image indexing, organization and retrieval.  In this paper, we describe a method of organizing photographs into events using spoken photograph captions.  The results of this process can be used to improve image indexing and retrieval.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {59–65},
numpages = {7},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383961,
author = {Soboroff, Ian and Nicholas, Charles and Cahan, Patrick},
title = {Ranking Retrieval Systems without Relevance Judgments},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383961},
doi = {10.1145/383952.383961},
abstract = {The most prevalent experimental methodology for comparing the effectiveness of information retrieval systems requires a test collection, composed of a set of documents, a set of query topics, and a set of relevance judgments indicating which documents are relevant to which topics.  It is well known that relevance judgments are not infallible, but recent retrospective investigation into results from the Text REtrieval Conference (TREC) has shown that differences in human judgments of relevance do not affect the relative measured performance of retrieval systems.  Based on this result, we propose and describe the initial results of a new evaluation methodology which replaces human relevance judgments with a randomly selected mapping of documents to topics which we refer to aspseudo-relevance judgments.Rankings of systems with our methodology correlate positively with official TREC rankings, although the performance of the top systems is not predicted well.  The correlations are stable over a variety of pool depths and sampling techniques.  With improvements, such a methodology could be useful in evaluating systems such as World-Wide Web search engines, where the set of documents changes too often to make traditional collection construction techniques practical.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {66–73},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383963,
author = {Voorhees, Ellen M.},
title = {Evaluation by Highly Relevant Documents},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383963},
doi = {10.1145/383952.383963},
abstract = {Given the size of the web, the search engine industry has argued that engines should be evaluated by their ability to retrieve highly relevant pages rather than all possible relevant pages. To explore the role highly relevant documents play in retrieval system evaluation, assessors for the mbox{TREC-9} web track used a three-point relevance scale and also selected best pages for each topic. The relative effectiveness of runs evaluated by different relevant document sets differed, confirming the hypothesis that different retrieval techniques work better for retrieving highly relevant documents. Yet evaluating by highly relevant documents can be unstable since there are relatively few highly relevant documents. TREC assessors frequently disagreed in their selection of the best page, and subsequent evaluation by best page across different assessors varied widely. The discounted cumulative gain measure introduced by J"{a}rvelin and Kek"{a}l"{a}inen increases evaluation stability by incorporating all relevance judgments while still giving precedence to highly relevant documents.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {74–82},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383964,
author = {Jin, Rong and Falusos, Christos and Hauptmann, Alex G.},
title = {Meta-Scoring: Automatically Evaluating Term Weighting Schemes in IR without Precision-Recall},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383964},
doi = {10.1145/383952.383964},
abstract = {In this paper, we present a method that can automatically evaluate performance of different term weighting schemes in information retrieval without resorting to precision-recall based on human relevance judgments. Specifically, the problem is: given two document-term matrixes generated from two different term weighting schemes, can we tell which term weighting scheme will performance better than the other? We propose a meta-scoring function, which takes as input the document-term matrix generated by some term weighting scheme and computes a goodness score from the document-term matrix. In our experiments, we found out that this score is highly correlated with the precision-recall measurement for all the collections and term weighting schema we tried. Thus, we conclude that our meta-scoring function can be a substitute for the precision-recall measurement that needs relevance judgments of human subject. Furthermore, this meta-scoring function is not limited only to text information retrieval can be applied to fields such as image and DNA retrieval.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {83–89},
numpages = {7},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383965,
author = {Gollins, Tim and Sanderson, Mark},
title = {Improving Cross Language Retrieval with Triangulated Translation},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383965},
doi = {10.1145/383952.383965},
abstract = {Most approaches to cross language information retrieval assume that resources providing a direct translation between the query and document languages exist.  This paper presents research examining the situation where such an assumption is false.  Here, an intermediate (or pivot) language provides a means of transitive translation of the query language to that of the document via the pivot, at the cost, however, of introducing much error.  The paper reports the novel approach of translating in parallel across multiple intermediate languages and fusing the results.  Such a technique removes the error, raising the effectiveness of the tested retrieval system, up to and possibly above the level expected, had a direct translation route existed.  Across a number of retrieval situations and combinations of languages, the approach proves to be highly effective.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {90–95},
numpages = {6},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383966,
author = {Gao, Jianfeng and Nie, Jian-Yun and Xun, Endong and Zhang, Jian and Zhou, Ming and Huang, Changning},
title = {Improving Query Translation for Cross-Language Information Retrieval Using Statistical Models},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383966},
doi = {10.1145/383952.383966},
abstract = {Dictionaries have often been used for query translation in cross-language information retrieval (CLIR). However, we are faced with the problem of translation ambiguity, i.e. multiple translations are stored in a dictionary for a word. In addition, a word-by-word query translation is not precise enough. In this paper, we explore several methods to improve the previous dictionary-based query translation. First, as many as possible, noun phrases are recognized and translated as a whole by using statistical models and phrase translation patterns. Second, the best word translations are selected based on the cohesion of the translation words. Our experimental results on TREC English-Chinese CLIR collection show that these techniques result in significant improvements over the simple dictionary approaches, and achieve even better performance than a high-quality machine translation system.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {96–104},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383968,
author = {Xu, Jinxi and Weischedel, Ralph and Nguyen, Chanh},
title = {Evaluating a Probabilistic Model for Cross-Lingual Information Retrieval},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383968},
doi = {10.1145/383952.383968},
abstract = {This work proposes and evaluates a probabilistic cross-lingual retrieval system.  The system uses a generative model to estimate the probability that a document in one language is relevant, given a query in another language.  An important component of the model is translation probabilities from terms in documents to terms in a query.  Our approach is evaluated when 1) the only resource is a manually generated bilingual word list, 2) the only resource is a parallel corpus, and 3) both resources are combined in a mixture model.  The combined resources produce about 90% of monolingual performance in retrieving Chinese documents.  For Spanish the system achieves 85% of monolingual performance using only a pseudo-parallel Spanish-English corpus. Retrieval results are comparable with those of the structural query translation technique (Pirkola, 1998) when bilingual lexicons are used for query translation.  When parallel texts in addition to conventional lexicons are used, it achieves better retrieval results but requires more computation than the structural query translation technique.  It also produces slightly better results than using a machine translation system for CLIR, but the improvement over the MT system is not significant.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {105–110},
numpages = {6},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383970,
author = {Lafferty, John and Zhai, Chengxiang},
title = {Document Language Models, Query Models, and Risk Minimization for Information Retrieval},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383970},
doi = {10.1145/383952.383970},
abstract = {We present a framework for information retrieval that combines document models and query models using a probabilistic ranking function based on Bayesian decision theory. The framework suggests an operational retrieval model that extends recent developments in the language modeling approach to information retrieval.  A language model for each document is estimated, as well as a language model for each query, and the retrieval problem is cast in terms of risk minimization. The query language model can be exploited to model user preferences, the context of a query, synonomy and word senses. While recent work has incorporated word translation models for this purpose, we introduce a new method using Markov chains defined on a set of documents to estimate the query models.  The Markov chain method has connections to algorithms from link analysis and social networks.  The new approach is evaluated on TREC collections and compared to the basic language modeling approach and vector space models together with query expansion using Rocchio.  Significant improvements are obtained over standard query expansion methods for strong baseline TF-IDF systems, with the greatest improvements attained for short queries on Web data.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {111–119},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383972,
author = {Lavrenko, Victor and Croft, W. Bruce},
title = {Relevance Based Language Models},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383972},
doi = {10.1145/383952.383972},
abstract = {We explore the relation between classical probabilistic models of information retrieval and the emerging language modeling approaches. It has long been recognized that the primary obstacle to effective performance of classical models is the need to estimate arelevance model: probabilities of words in the relevant class. We propose a novel technique for estimating these probabilities using the query alone. We demonstrate that our technique can produce highly accurate relevance models, addressing important notions of synonymy and polysemy. Our experiments show relevance models outperforming baseline language modeling systems on TREC retrieval and TDT tracking tasks. The main contribution of this work is an effective formal method for estimating a relevance model with no training data.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {120–127},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383974,
author = {Joachims, Thorsten},
title = {A Statistical Learning Learning Model of Text Classification for Support Vector Machines},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383974},
doi = {10.1145/383952.383974},
abstract = {This paper develops a theoretical learning model of text classification for Support Vector Machines (SVMs). It connects the statistical properties of text-classification tasks with the generalization performance of a SVM in a quantitative way. Unlike conventional approaches to learning text classifiers, which rely primarily on empirical evidence, this model explains why and when SVMs perform well for text classification. In particular, it addresses the following questions: Why can support vector machines handle the large feature spaces in text classification effectively? How is this related to the statistical properties of text? What are sufficient conditions for applying SVMs to text-classification problems successfully?},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {128–136},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383975,
author = {Yang, Yiming},
title = {A Study of Thresholding Strategies for Text Categorization},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383975},
doi = {10.1145/383952.383975},
abstract = {Thresholding strategies in automated text categorization are an underexplored area of research.  This paper presents an examination of the effect of thresholding strategies on the performance of a classifier under various conditions.  Using k-Nearest Neighbor (kNN) as the classifier and five evaluation benchmark collections as the testbets, three common thresholding methods were investigated, including rank-based thresholding (RCut), proportion-based assignments (PCut) and score-based local optimization (SCut); in addition, new variants of these methods are proposed to overcome significant problems in the existing approaches.  Experimental results show that the choice of thresholding strategy can significantly influence the performance of kNN, and that the ``optimal'' strategy may vary by application.  SCut is potentially better for fine-tuning but risks overfitting.  PCut copes better with rare categories and exhibits a smoother trade-off in recall versus precision, but is not suitable for online decision making.  RCut is most natural for online response but is too coarse-grained for global or local optimization.  RTCut, a new method combining the strength of category ranking and scoring, outperforms both PCut and RCut significantly.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {137–145},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383976,
author = {Bekkerman, Ron and El-Yaniv, Ran and Tishby, Naftali and Winter, Yoad},
title = {On Feature Distributional Clustering for Text Categorization},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383976},
doi = {10.1145/383952.383976},
abstract = {We describe a text categorization approach that is based on a combination of feature distributional clusters with a support vector machine (SVM) classifier. Our feature selection approach employs distributional clustering of words via the recently introducedinformation bottleneck method, which generates a more efficientword-clusterrepresentation of documents. Combined with the classification power of an SVM, this method yields high performance text categorization that can outperform other recent methods in terms of categorization accuracy and representation efficiency. Comparing the accuracy of our method with other techniques, we observe significant dependency of the results on the data set. We discuss the potential reasons for this dependency.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {146–153},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383981,
author = {Ando, Rie Kubota and Lee, Lillian},
title = {Iterative Residual Rescaling},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383981},
doi = {10.1145/383952.383981},
abstract = {We consider the problem of creating document representations in which inter-document similarity measurements correspond to semantic similarity.  We first present a novelsubspace-basedframework for formalizing this task. Using this framework, we derive a new analysis ofLatent Semantic Indexing(LSI), showing a precise relationship between its performance and theuniformityof the underlying distribution of documents over topics.  This analysis helps explain the improvements gained by Ando's (2000)Iterative Residual Rescaling(ours) algorithm: ours can compensate for distributional non-uniformity.  A further benefit of our framework is that it provides a well-motivated, effective method for automatically determining the rescaling factor ours depends on, leading to further improvements.  A series of experiments over various settings and with several evaluation metrics validates our claims.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {154–162},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383982,
author = {Chinenyanga, Taurai Tapiwa and Kushmerick, Nicholas},
title = {Expressive Retrieval from XML Documents},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383982},
doi = {10.1145/383952.383982},
abstract = {The emergence of XML as a standard interchange format for structured documents/data has given rise to many XML query language proposals. However, some of these languages do not support information retrieval-style ranked queries based on textual similarity.  There have been several extensions to these query languages to support keyword search, but the resulting query languages cannot express queries such as``find books and CDs with similar titles''. Either these extensions use keywords as mere boolean filters, or similarities can be calculated only between data values and constants rather than two data values. We propose ELIXIR, an textbf{underline{e}}xpressive and textbf{underline{e}}fficienttextbf{underline{l}}anguage for textbf{underline{X}}ML textbf{underline{i}}nformation textbf{underline{r}}etrieval that extends the query language XML-QL cite{deutsch-www8,deutsch-deb99} with a textual similarity operator.  ELIXIR is a general-purpose XML information retrieval language, sufficiently expressive to handle the above query.  Our algorithm for answering ELIXIR queries rewrites the original ELIXIR query into a series of XML-QL queries that generate intermediate relational data, and uses relational database techniques to efficiently evaluate the similarity operators on this intermediate data, yielding an XML document with nodes ranked by similarity.  Our experiments demonstrate that our prototype scales well with the size of the XML data and complexity of the query.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {163–171},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383985,
author = {Fuhr, Norbert and Gro\ss{}johann, Kai},
title = {XIRQL: A Query Language for Information Retrieval in XML Documents},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383985},
doi = {10.1145/383952.383985},
abstract = {Based on the document-centric view of XML, we present the query language XIRQL.  Current proposals for XML query languages lack most IR-related features, which are weighting and ranking, relevance-oriented search, datatypes with vague predicates, and semantic relativism.  XIRQL integrates these features by using ideas from logic-based probabilistic IR models, in combination with concepts from the database area.  For processing XIRQL queries, a path algebra is presented, that also  serves as a starting point for query optimization.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {172–180},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383986,
author = {Rithven, Ian and Lalmas, Mounia and van Rijsbergen, Keith},
title = {Empirical Investigations on Query Modification Using Abductive Explanations},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383986},
doi = {10.1145/383952.383986},
abstract = {In this paper we report on a series of experiments designed to investigate query modification techniques motivated by the area of abductive reasoning. In particular we use the notion of abductive explanation, explanations being a description of data that highlight important features of the data. We describe several methods of creating abductive explanations, exploring term reweighting and query reformulation techniques and demonstrate their suitability for relevance feedback.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {181–189},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383987,
author = {Sakai, Tetsuya and Sparck-Jones, Karen},
title = {Generic Summaries for Indexing in Information Retrieval},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383987},
doi = {10.1145/383952.383987},
abstract = {This paper examines the use of generic summaries for indexing in information retrieval. Our main observations are that: (1) With or without pseudo-relevance feedback, a summary index may be as effective as the corresponding fulltext index forprecision-oriented search of highly relevant documents. %43 But a reasonably sophisticated summarizer, using a compression ratio of 10-30%, is desirable for this purpose. (2) In pseudo-relevance feedback, using a summary index at initial search and a fulltext index at final search is possibly effective for precision-oriented search, regardless of relevance levels. This strategy is significantly more effective than the one using the summary index only and probably more effective than using summaries as mere term  selection filters. %the use of summaries as mere term selection filters. %The summary quality is probably not a critical factor for this strategy, For this strategy, the summary quality is probably not a critical factor, and a compression ratio of 5-10% appears best.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {190–198},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383989,
author = {Zechner, Klaus},
title = {Automatic Generation of Concise Summaries of Spoken Dialogues in Unrestricted Domains},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383989},
doi = {10.1145/383952.383989},
abstract = {Automatic summarization of open domain spoken dialogues is a new research area. This paper introduces the task, the challenges involved, and presents an approach to obtain automatic extract summaries for multi-party dialogues of four different genres, without any restriction on domain. We address the following issues which are intrinsic to spoken dialogue summarization and typically can be ignored when summarizing written text such as newswire data: (i) detection and removal of speech disfluencies; (ii) detection and insertion of sentence boundaries; (iii) detection and linking of cross-speaker information units (question-answer pairs). A global system evaluation using a corpus of 23 relevance annotated dialogues containing 80 topical segments shows that for the two more informal genres, our summarization system using dialogue specific components significantly outperforms a baseline using TFIDF term weighting with maximum marginal relevance ranking (MMR).},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {199–207},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383990,
author = {Chakrabarti, Soumen and Joshi, Mukul and Tawde, Vivek},
title = {Enhanced Topic Distillation Using Text, Markup Tags, and Hyperlinks},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383990},
doi = {10.1145/383952.383990},
abstract = {Topic distillation is the analysis of hyperlink graph structure to identify mutually reinforcing authorities (popular pages) and hubs (comprehensive lists of links to authorities).  Topic distillation is becoming common in Web search engines, but the best-known algorithms model the Web graph at a coarse grain, with whole pages as single nodes.  Such models may lose vital details in the markup tag structure of the pages, and thus lead to a tightly linked irrelevant subgraph winning over a relatively sparse relevant subgraph, a phenomenon called topic drift or contamination. The problem gets especially severe in the face of increasingly complex pages with navigation panels and advertisement links.  We present an enhanced   topic distillation algorithm which analyzes text, the markup tag trees that constitute HTML pages, and hyperlinks between pages. It thereby identifies subtrees which have high text- and hyperlink-based coherence w.r.t. the query. These subtrees get preferential treatment in the mutual reinforcement process.  Using over 50 queries, 28 from earlier topic distillation work, we analyzed over 700,000 pages and obtained quantitative and anecdotal evidence that the new algorithm reduces topic drift.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {208–216},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383991,
author = {Muramatsu, Jack and Pratt, Wanda},
title = {Transparent Queries: Investigation Users' Mental Models of Search Engines},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383991},
doi = {10.1145/383952.383991},
abstract = {Typically, commercial Web search engines provide very little feedback to the user concerning how a particular query is processed and interpreted. Specifically, they apply key query transformations without the users knowledge. Although these transformations have a pronounced effect on query results, users have very few resources for recognizing their existence and understanding their practical importance. We conducted a user study to gain a better understanding of users knowledge of and reactions to the operation of several query transformations that web search engines automatically employ. Additionally, we developed and evaluated Transparent Queries, a software system designed to provide users with lightweight feedback about opaque query transformations. The results of the study suggest that users do indeed have difficulties understanding the operation of query transformations without additional assistance. Finally, although transparency is helpful and valuable, interfaces that allow direct control of query transformations might ultimately be more helpful for end-users.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {217–224},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383992,
author = {Turpin, Andrew H. and Hersh, William},
title = {Why Batch and User Evaluations Do Not Give the Same Results},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383992},
doi = {10.1145/383952.383992},
abstract = {Much system-oriented evaluation of information retrieval systems has used the Cranfield approach based upon queries run against test collections in a batch mode. Some researchers have questioned whether this approach can be applied to the real world, but little data exists for or against that assertion. We have studied this question in the context of the TREC Interactive Track. Previous results demonstrated that improved performance as measured by relevance-based metrics in batch studies did not correspond with the results of outcomes based on real user searching tasks. The experiments in this paper analyzed those results to determine why this occurred. Our assessment showed that while the queries entered by real users into systems yielding better results in batch studies gave comparable gains in ranking of relevant documents for those users, they did not translate into better performance on specific tasks. This was most likely due to users being able to adequately find and utilize relevant documents ranked further down the output list.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {225–231},
numpages = {7},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383993,
author = {McDonald, Sharon and Lai, Ting-Sheng and Tait, John},
title = {Evaluating a Content Based Image Retrieval System},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383993},
doi = {10.1145/383952.383993},
abstract = {Content Based Image Retrieval (CBIR) presents special challenges in terms of how image data is indexed, accessed, and how end systems are evaluated.   This paper discusses the design of a CBIR system that uses global colour as the primary indexing key, and a user centered evaluation of the systems visual search tools.  The results indicate that users are able to make use of a range of visual search tools, and that different tools are used at different points in the search process.  The results also show that the provision of a structured navigation and browsing tool can support image retrieval, particularly in situations in which the user does not have a target image in mind.   The results are discussed in terms of their implications for the design of visual search tools, and their implications for the use of user-centered evaluation for CBIR systems.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {232–240},
numpages = {9},
keywords = {general inage indexing, content based image retrieval, colour based indexing, user centered evaluation},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

@inproceedings{10.1145/383952.383995,
author = {Menczer, Filippo and Pant, Gautam and Srinivasan, Padmini and Ruiz, Miguel E.},
title = {Evaluating Topic-Driven Web Crawlers},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383995},
doi = {10.1145/383952.383995},
abstract = {Due to limited bandwidth, storage, and computational resources, and to the dynamic nature of the Web, search engines cannot index every Web page, and even the covered portion of the Web cannot be monitored continuously for changes.  Therefore it is essential to develop effective crawling strategies to prioritize the pages to be indexed. The issue is even more important for topic-specific search engines, where crawlers must make additional decisions based on the relevance of visited pages.  However, it is difficult to evaluate alternative crawling strategies because relevant sets are unknown and the search space is changing.  We propose three different methods to evaluate crawling strategies.  We apply the proposed metrics to compare three topic-driven crawling algorithms based on similarity ranking, link analysis, and adaptive agents.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {241–249},
numpages = {9},
keywords = {Web information retrieval, InfoSpiders, topic driven crawling, focused crawlers, best-first search, PageRank, performance metrics},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}

