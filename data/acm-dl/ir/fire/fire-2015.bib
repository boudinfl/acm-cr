@inproceedings{10.1145/2838706.2838708,
author = {Ganguly, Debasis and Leveling, Johannes and Jones, Gareth J. F.},
title = {Context-Driven Dimensionality Reduction for Clustering Text Documents},
year = {2015},
isbn = {9781450340045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838706.2838708},
doi = {10.1145/2838706.2838708},
abstract = {We investigate clustering documents based on automatically annotated potentially sensitive information extracted from a large collection of organizational data. The process of clustering in this particular use case is helpful to visualize and navigate through groups of documents with related content. However, the effectiveness and efficiency of document clustering is limited mainly due to the large dimensionality of the document vectors. To alleviate this problem we propose a dimensionality reduction approach which involves selecting terms with high tf-idf scores from the context of the automatically annotated sensitive regions of a document. Due to the unavailability of real organizational data for research purposes, we evaluate our approach on the standard 20 news-groups dataset. For evaluation purposes, the only sensitive information that we use from the documents of this dataset are the named entities, e.g. the names of persons and organizations. Experimental results show that our approach is able to achieve an almost perfect clustering with a purity value of 0.998 improving by 22.60% with respect to the purity value of 0.814 obtained without document dimensionality reduction.},
booktitle = {Proceedings of the 7th Forum for Information Retrieval Evaluation},
pages = {1–7},
numpages = {7},
keywords = {Dimensionality reduction, Document Clustering},
location = {Gandhinagar, India},
series = {FIRE '15}
}

@inproceedings{10.1145/2838706.2838710,
author = {Harth, Eric and Dugerdil, Philippe},
title = {Document Retrieval Metrics for Program Understanding},
year = {2015},
isbn = {9781450340045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838706.2838710},
doi = {10.1145/2838706.2838710},
abstract = {The need for domain knowledge representation for program comprehension is now widely accepted in the program comprehension community. The so-called "concept assignment problem" represents the challenge to locate domain concepts in the source code of programs. The vast majority of attempts to solve it are based on static source code search for clues to domain concepts. In contrast, our approach is based on dynamic analysis using information retrieval (IR) metrics. First we explain how we modeled the domain concepts and their role in program comprehension. Next we present how some of the popular IR metrics could be adapted to the "concept assignment problem" and the way we implemented the search engine. Then we present our own metric and the performance of these metrics to retrieve domain concepts in source code. The contribution of the paper is to show how the IR metrics could be applied to the "concept assignment problem" when the "documents" to retrieve are domain concepts structured in an ontology.},
booktitle = {Proceedings of the 7th Forum for Information Retrieval Evaluation},
pages = {8–15},
numpages = {8},
keywords = {Program comprehension, Dynamic analysis, Domain ontology, Document retrieval metrics},
location = {Gandhinagar, India},
series = {FIRE '15}
}

@inproceedings{10.1145/2838706.2838711,
author = {Rao, Pattabhi R. K. and Devi, Sobha Lalitha},
title = {Automatic Identification of Conceptual Structures Using Deep Boltzmann Machines},
year = {2015},
isbn = {9781450340045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838706.2838711},
doi = {10.1145/2838706.2838711},
abstract = {This paper presents an approach to automatically extract Conceptual Graphs (CGs) from patent documents using Over-Replicated Softmax model of Deep Boltzman Machines (DBMs). The main challenge in the extraction of conceptual graphs from the natural language texts is the automatic identification of concepts and conceptual relations. The text analyzed in this work are patent documents, focused mainly on the claim's section (Claim) of the documents. The task of automatically identifying the concept and conceptual relation becomes difficult due to the complexities in the writing style of these documents as they are technical as well as legal. The analysis we have done shows that the general in-depth parsers available in the open domain fail to parse the 'claims section' sentences in patent documents. The failure of in-depth parsers led us, to develop a methodology to extract CGs using shallow parsed text. Thus in the present work we came up with a methodology which uses shallow parsed text in conjunction with DBMs, a deep learning technique for extracting CGs from sentences in the claim/novelty section of patent documents. The results obtained in our experiments are encouraging with a significant improvement over the state -of-art and are discussed in detail in this paper. We have obtained a precision of 79.34 % and a recall of 72.54%.},
booktitle = {Proceedings of the 7th Forum for Information Retrieval Evaluation},
pages = {16–20},
numpages = {5},
keywords = {Conceptual Structures, Conceptual Graphs, Artificial Neural Networks, Deep Learning, Deep Boltzmann Machines, Machine Learning},
location = {Gandhinagar, India},
series = {FIRE '15}
}

@inproceedings{10.1145/2838706.2838709,
author = {Kabadjov, Mijail and Steinberger, Josef and Barker, Emma and Kruschwitz, Udo and Poesio, Massimo},
title = {OnForumS: The Shared Task on Online Forum Summarisation at MultiLing'15},
year = {2015},
isbn = {9781450340045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838706.2838709},
doi = {10.1145/2838706.2838709},
abstract = {In this paper we present the Online Forum Summarisation (OnForumS) pilot task at MultiLing'15. OnForumS is a pioneering attempt at encompassing automatic summarisation, argumentation mining and sentiment analysis into one shared task and at bringing crowdsourcing to the evaluation of systems for automatic summarisation and argument structure parsing. It covered two languages, English and Italian. Four research groups, each submitting two runs, participated in the task and these complemented with two baseline system runs were evaluated via crowdsourcing. Performance results are presented and briefly discussed. Being the first of its kind, we believe OnForumS'15 was a successful campaign and hope it will establish itself as a valuable exercise in advancing the state-of-the-art in this new emerging area. Current plans are to organise it again jointly with MultiLing in 2017 and to include more languages.},
booktitle = {Proceedings of the 7th Forum for Information Retrieval Evaluation},
pages = {21–26},
numpages = {6},
location = {Gandhinagar, India},
series = {FIRE '15}
}

@inproceedings{10.1145/2838706.2838715,
author = {Anand, Rajul and Kotov, Alexander},
title = {An Empirical Comparison of Statistical Term Association Graphs with DBpedia and ConceptNet for Query Expansion},
year = {2015},
isbn = {9781450340045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838706.2838715},
doi = {10.1145/2838706.2838715},
abstract = {Term graphs constructed from document collections as well as external resources, such as encyclopedias (DBpedia) and knowledge bases (ConceptNet), can be used as sources of semantically related terms for query expansion. Although these resources individually have been shown to be effective for IR, it is not known how their retrieval effectiveness compares with each other. In this work, we use standard TREC collections to perform systematic evaluation and empirical comparison of retrieval effectiveness of both types of term graphs for all and difficult queries. Our results indicate that of the term association graphs constructed automatically from document collection using information theoretic measures are more effective for Web collections, while the term graphs derived from DBpedia and ConceptNet are more effective for newswire collections.},
booktitle = {Proceedings of the 7th Forum for Information Retrieval Evaluation},
pages = {27–30},
numpages = {4},
keywords = {Query Expansion, Term Graphs, Knowledge Graphs},
location = {Gandhinagar, India},
series = {FIRE '15}
}

@inproceedings{10.1145/2838706.2838718,
author = {Koto, Fajri and Adriani, Mirna},
title = {HBE: Hashtag-Based Emotion Lexicons for Twitter Sentiment Analysis},
year = {2015},
isbn = {9781450340045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838706.2838718},
doi = {10.1145/2838706.2838718},
abstract = {In this paper we report the first effort of constructing emotion lexicon by utilizing Twitter as source of data. Specifically we used hashtag feature to obtain tweets with certain emotion label in English. There are eight emotion classes used in our work, comprising of angry, disgust, fear, joy, sad, surprise, trust and anticipation that refer to the Plutchik's wheel. To obtain the lexicon, we first ranked the words according to its term frequency. After that, we reduced some irrelevant words by removing words with low frequency. We also enriched the lexicon with the synonym and conducted filtering by utilizing sentiment lexicon (40,288 words). As result, we successfully constructed 4 Hashtag-Based Emotion (HBE) Lexicons through different procedures and called them as HBE-A1 (50,613 words), HBE-B1 (23,400 words), HBE-A2 (26,909 words) and HBE-B2 (14,905 words). In our experiment, we used the lexicons in investigating Twitter Sentiment Analysis and the result reveals that our proposed emotion lexicons can boost the accuracy and even improve over than NRC-Emotion lexicon. It is also worth noting that our construction idea is simple, automatic, inexpensive and suitable for Social Media analysis.},
booktitle = {Proceedings of the 7th Forum for Information Retrieval Evaluation},
pages = {31–34},
numpages = {4},
keywords = {polarity, twitter, sentiment analysis, emotion lexicon, hashtag, subjectivity},
location = {Gandhinagar, India},
series = {FIRE '15}
}

@inproceedings{10.1145/2838706.2838717,
author = {Agarwal, Amit and Gupta, Bhumika and Bhatt, Gaurav and Mittal, Ankush},
title = {Construction of a Semi-Automated Model for FAQ Retrieval via Short Message Service},
year = {2015},
isbn = {9781450340045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838706.2838717},
doi = {10.1145/2838706.2838717},
abstract = {Mobile phones, currently, are one of the most extensive medium for the communication of any kind of information to the general public. Being one of the fastest spreading technologies, even to the remotest of areas, this highly sought after contemporary resource has started seeking its application in areas like healthcare, education, banking and internet crime. On this account Short Message Service via mobile phones can aid as an efficient tool to retrieve answers to various Frequently Asked Questions (FAQs) in multiple domains. This application of text messages using mobile phones can be quite substantial only if the limitations that occur due to the large amount of noise in the SMS text can be eliminated. The solution proposed in this paper tries to effectively denoise the text using a similarity measure that aggregates results from prefix and suffix matching and a similarity ratio. To further refine these results supervised machine learning using Na\"{\i}ve Bayes theorem on the N-Gram Markov model is implemented. For this we use the training database of FAQs in various domains to compute probabilities of consecutive occurrence of bigrams of words. Further, using set operations like intersection and minus the corrected query is matched in the FAQ corpus to generate the most proximate questions corresponding to it. To demonstrate the accuracy of the proposed algorithm it was experimented upon a set of queries collected from some mobile phone users and the results were compared with that of certain existing methodologies.},
booktitle = {Proceedings of the 7th Forum for Information Retrieval Evaluation},
pages = {35–38},
numpages = {4},
keywords = {Suffix, Short Message Service (SMS), Similarity measure, Na\"{\i}ve Bayes, Prefix, Frequently Asked Question (FAQ) Noise removal, N-gram model},
location = {Gandhinagar, India},
series = {FIRE '15}
}

@inproceedings{10.1145/2838706.2838713,
author = {Ramrakhiyani, Nitin and Pawar, Sachin and Palshikar, Girish},
title = {Word2vec or JoBimText? A Comparison for Lexical Expansion of Hindi Words},
year = {2015},
isbn = {9781450340045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838706.2838713},
doi = {10.1145/2838706.2838713},
abstract = {Exploration of distributional semantics for NLP tasks in Indian languages has been scarce. This work carries out a comparative analysis of two recent and high performing distributional semantics techniques namely word2vec and JoBimText. The task of lexical expansion of words in Hindi is considered for the analysis. A manual similarity assessment of the lexical expansions of words is employed for evaluation of the techniques. It can be observed that word2vec framework performs better than the JoBimText for various corpus sizes. Analysis of the results also presents insights on performance of the systems on various word types.},
booktitle = {Proceedings of the 7th Forum for Information Retrieval Evaluation},
pages = {39–42},
numpages = {4},
location = {Gandhinagar, India},
series = {FIRE '15}
}

@inproceedings{10.1145/2838706.2838714,
author = {Prakash, Amit and Saha, Sujan Kumar},
title = {A Comparative Study on Different Translation Approaches for Query Formation in the Source Retrieval Task},
year = {2015},
isbn = {9781450340045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838706.2838714},
doi = {10.1145/2838706.2838714},
abstract = {The text reuse detection among documents in comparable corpora has become an important research topic due to its usages ranging from document linking to plagiarism detection. A text reuse detection system typically computes similarity between source document and the possibly reused document. Considering the real-world scenario where exhaustive comparison is not possible, the system must first retrieve a subset of documents that serves as the source of similarity computation. But the task becomes more challenging when the language of reused document differs from the source document. In this paper we present a comparative study of different translation approaches used to map the language barrier for cross-lingual retrieval of possible sources of text reuse. We perform our experiments on CL!NSS 2013 dataset. It is observed that machine translation based query formation approach retrieves sources with highest recall. Our system also outperforms the existing systems in the literature.},
booktitle = {Proceedings of the 7th Forum for Information Retrieval Evaluation},
pages = {43–46},
numpages = {4},
location = {Gandhinagar, India},
series = {FIRE '15}
}

@inproceedings{10.1145/2838706.2838716,
author = {Londhe, Nikhil and Gopalakrishnan, Vishrawas and Srihari, Rohini K. and Zhang, Aidong},
title = {MESS: A Multilingual Error Based String Similarity Measure for Transliterated Name Variants},
year = {2015},
isbn = {9781450340045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838706.2838716},
doi = {10.1145/2838706.2838716},
abstract = {Cross-lingual name matching is an important problem in the fields of machine translation and data mining. Though well studied, it lacks a generic solution largely due to issues like language specific nuances, resource scarcity, etc. Most of the proposed unsupervised approaches focus on a small subset of languages, mostly English and its derivatives, and employ specific handcrafted rules that do not port well to other languages. In this paper, we propose a generic multilingual solution that instead adds simple probabilistic extensions to existing string similarity methods. Not only does our solution depend only on freely available open source resources but we also demonstrate the superiority of our approach on 60 language pairs drawn across language families.},
booktitle = {Proceedings of the 7th Forum for Information Retrieval Evaluation},
pages = {47–50},
numpages = {4},
location = {Gandhinagar, India},
series = {FIRE '15}
}

