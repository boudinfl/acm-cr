@inbook{10.1145/191246.191249,
author = {Al-Anzi, Fawaz S. and Spooner, David L.},
title = {Modeling Behavior, a Step towards Defining Functionally Correct Views of Complex Objects in Concurrent Engineering},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191249},
abstract = {Multidisciplinary concurrent engineering needs to model and manage different views of complex designs. Previous attempts to address the problem of creating views of complex objects in object oriented database systems focus on the structure of complex objects; little attention is paid to how complex object behavior is effected when creating views. We believe that designing functionally correct behavior for a complex object should be a major consideration when defining a view to guarantee correctness of the derived classes.In this paper, we study the problem for designing functionally correct views of complex objects in concurrent engineering. View behavioral modeling requirements are presented. A behavior model that satisfies these requirements is presented. This model is demonstrated on an example complex object that represents process management.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {1–9},
numpages = {9}
}

@inproceedings{10.1145/191246.191252,
author = {Halper, Michael and Geller, James and Perl, Yehoshua and Klas, Wolfgang},
title = {Integrating a Part Relationship into an Open OODB System Using Metaclasses},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191252},
doi = {10.1145/191246.191252},
abstract = {The part-whole semantic relationship (the part relationship, for short) is an important modeling primitive in many advanced application domains such as manufacturing, design, and document processing. In this paper, we examine the problem of integrating such a construct into an OODB system. Specifically, two questions are addressed in this regard. This first is: Can a part relationship be made an intrinsic construct of an existing OODB system without having to rewrite a substantial portion of the system? The second: Can an “open” OODB system which claims to support such an integration really do so, and, more specifically, can the integration be done using a metaclass mechanism which purports to bring extensibility to the VODAK Model Language (VML)?To demonstrate that both questions can be answered “yes,” we introduce and discuss the details of a custom VML metaclass—the “HolonymicMeronymic” metaclass—which we have built. This metaclass comprises two items, an “instance” type and an “instance-instance” type. Together, the two endow the classes of a part hierarchy and their instances with structure and behavior consistent with our comprehensive part relationship model and the notions of “part” and “whole.” Complete descriptions of each of these two aspects of the metaclass are presented and their effect on schema construction and database usage is discussed.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {10–17},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191254,
author = {Rundensteiner, Elke Angelika},
title = {A Classification Algorithm for Supporting Object-Oriented Views},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191254},
doi = {10.1145/191246.191254},
abstract = {In recent years, object-oriented (OO) views have been recognized as a powerful mechanism for customizing the structural as well as behavioral aspects of interfaces to object-oriented databases (OODBs) for diverse users. In this context, classification is concerned with the integration of virtual classes derived using an OO query into one unifying schema. Existing approaches either require the user to explicitly specify the relationship between a virtual class and existing base classes, or they relate a virtual class directly with its source class(es) or with the root of the schema. In this paper, we propose a solution to this classification problem that accomplishes the following goals: (1) generate maximally informative, and thus comprehensible, schemas that explicitly model the subclass relationships between base and virtual classes, and (2) support efficient type resolution for shared property functions by supporting upwards inheritance for both base and virtual classes. Correctness and complexity of the classification algorithm are also discussed.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {18–25},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191256,
author = {B\"{o}hm, Klemens and Aberer, Karl},
title = {Storing HyTime Documents in an Object-Oriented Databases},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191256},
doi = {10.1145/191246.191256},
abstract = {An open hypermedia-document storage system has to meet requirements that are not satisfied by existing systems: it has to support non-generic hypermedia document types, i.e. document types enriched with application-specific semantics. It has to provide hypermedia-document access methods. Finally, it has to allow the exchange of hypermedia documents with other systems. On a technical level, an object-oriented database-management system, on a logical level, a well established ISO standard, namely HyTime, is used to satisfy the requirements mentioned above. By means of the example of documents incorporating hypertext structures we discuss the impact of taking such an approach on representation and processing within the database system.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {26–33},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191257,
author = {Chen, Yangjun and H\"{a}rder, Theo},
title = {An Optimal Graph Traversal Algorithm for Evaluating Linear Binary-Chain Programs},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191257},
doi = {10.1145/191246.191257},
abstract = {Grahne et al. have presented a graph algorithm for a subset of recursive queries. This method consists of two phases. In the first phase, the method transforms a linear binary-chain program into a set of equations over expressions containing predicate symbols. In the second phase, a graph is constructed from the equations and the answers are produced by traversing the relevant paths. Here we describe a new algorithm which requires less time than the algorithm of Grahne et al. The key idea of the improvement is to reduce the search space that will be traversed when a query is invoked. Further, we speed up the evaluation of cyclic data by generating most answers directly in terms of the answers already found and the associated “path information” instead of traversing the corresponding paths as usual. In this way, our algorithm achieves a linear time complexity for both cyclic and non-cyclic data.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {34–41},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191258,
author = {Marek, Robert and Rahm, Erhard},
title = {TID Hash Joins},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191258},
doi = {10.1145/191246.191258},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {42–49},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191259,
author = {Ambrosio, Ana Paula},
title = {Introducing Semantics in Conceptual Schema Reuse},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191259},
doi = {10.1145/191246.191259},
abstract = {Although standard components' manufacture and reuse is common practice in many engineering domains (e.g. electrical and mechanical engineering), this is not yet the case with respect to software development. Ironically, in such a highly “automated” domain, users still fail to find available components that match their needs faster than developing them again. The gap between what designers expect from reuse (and how it should be offered), and the actual reuse attempts remains the main barrier. This paper deals with conceptual schema construction in terms of reuse and the respective semantic requirements. It proposes a semantic retrieval mechanism based on imprecise queries for the reuse of conceptual schema components.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {50–56},
numpages = {7},
keywords = {reuse, conceptual modeling, semantic retrieval, imprecise querying},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191261,
author = {Woods, W. A. and Moser, H. D. and Frieder, O. and Kantor, Paul B.},
title = {A Case for Reconfigurable Parallel Architectures for Information Retrieval},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191261},
doi = {10.1145/191246.191261},
abstract = {As the volume of data and computational requirements of modern information retrieval systems continue to expand, it is inevitable that parallel systems will be necessary to meet these demands. In this research, we provide a conceptual model of a reconfigurable parallel information retrieval system in a multicomputer environment. We develop strategies for scheduling queries in such systems, provide simulation results for implementing these strategies under various different theoretical situations, and present an analytical model of the system behavior.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {57–63},
numpages = {7},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191260,
author = {Lee, Dik L. and Lee, Wang-chien},
title = {Using Path Information for Query Processing in Object-Oriented Database Systems},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191260},
doi = {10.1145/191246.191260},
abstract = {This paper argues that most queries in object-oriented databases require traversing from one object to another in the aggregation hierarchy. Thus, the connections between objects through object identifiers are essential to the efficiency of query processing and should be represented separately from the database. We introduce the concept of path dictionary and describe how it supports queries of different types. We evaluate the storage overhead, query and update costs of the path dictionary. Compared to the path index, the path dictionary has better overall query and update performance and lower storage overhead.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {64–71},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191262,
author = {Alhajj, Reda and Polat, Faruk},
title = {Closure Maintenance in an Object-Oriented Query Model},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191262},
doi = {10.1145/191246.191262},
abstract = {An object-algebra is presented as a formal query model for object-oriented data models. The algebra serves not only to access and manipulate the structure and behavior of objects, but it also supports the creation of new objects and the introduction of new relationships into the schema. It provides a more powerful and flexible tool than messages for effectively dealing with complex situations and meeting associative access requirements. Operands as well as the results of operations in the proposed algebra are formally characterized as a pair of sets—a set of objects capturing the states and a set of message expressions comprised of sequences of messages modeling the object behavior. The closure property is achieved in a natural way by letting the results of operations possess the same characteristics as the operands in an algebra expression. Some operators of the algebra resemble those of the relational algebra but with different syntax and semantics. Additional operators are introduced to complement them. A class is shown to posses the properties of an operand by defining a set of objects and deriving a set of message expressions for it. Furthermore, the result of an object algebra expression is shown to have the characteristics of a class whose superclass/subclass relationships with its operand class(es) can be established providing a mechanism to properly and persistently place it in the class lattice (schema).},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {72–79},
numpages = {8},
keywords = {object-oriented data model, database system, object algebra expression, closure, total instances, query model, message expression, object-oriented query language},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191263,
author = {Shrufi, Adel},
title = {Performances of Clustering Policies in Object Bases},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191263},
doi = {10.1145/191246.191263},
abstract = {In this paper, we address the problem of clustering graphs in object-oriented databases. Unlike previous studies which focused only on a workload consisting of a single operation, this study tackles the problem when the workload is a set of operations (method and queries) that occur with a certain probability. Thus, the goal is to minimize the expected cost of an operation in the workload, while maintaining a similarly low cost for each individual operation class.To this end, we present a new clustering policy based on the nearest-neighbor graph partitioning algorithm. We then demonstrate that this policy provides considerable gains when compared to a suite of well-known clustering policies proposed in the literature. Our results are based on two widely referenced object-oriented database benchmarks; namely, the Tektronix HyperModel and OO7.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {80–87},
numpages = {8},
keywords = {storage techniques, performance analysis, object-oriented database systems, graph partitioning},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191264,
author = {Chen, Weimin and Turau, Volker},
title = {An Optimized Implementation for VML Based on Pattern Matching and Dynamic Programming},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191264},
doi = {10.1145/191246.191264},
abstract = {In an object-oriented database system (OODBS), objects exist persistently and object I/O is transparent to the programmer. Therefore, some mechanism in the system must initiate I/O as the program runs. In this paper we present an approach based on pattern matching and dynamic programming that allows a program to interact efficiently with the runtime storage layer. We are interested in allowing programs to manipulate very large objects without necessarily reading them entirely. If a program touches only a small part of a large object, the problem is how to determine the part of the object needed. In this paper, we present an approach based on pattern matching and dynamic programming to resolve this problem.We discuss and solve this problem in the context of VML, a modeling language of an open object-oriented database language. The VML compiler translates VML programs into C++ programs which contain calls to the object manager. We provide a detailed description of our implementation with the hope that our approach will foster the development of object-oriented database systems based on C++.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {88–96},
numpages = {9},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191265,
author = {Karp, Peter D. and Paley, Suzanne M. and Greenberg, Ira},
title = {A Storage System for Scalable Knowledge Representation},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191265},
doi = {10.1145/191246.191265},
abstract = {Twenty years of AI research in knowledge representation has produced frame knowledge representation systems (FRSs) that incorporate a number of important advances. However, FRSs lack two important capabilities that prevent them from scaling up to realistic applications: they cannot provide high-speed access to large knowledge bases (KBs), and they do not support shared, concurrent KB access by multiple users. Our research investigates the hypothesis that one can employ an existing database management system (DBMS) as a storage subsystem for an FRS, to provide high-speed access to large, shared KBs. We describe the design and implementation of a general storage system that incrementally loads referenced frames from a DBMS, and saves modified frames back to the DBMS, for two different FRSs: LOOM and THEO. We also present experimental results showing that the performance of our prototype storage subsystem exceeds that of flat files for simulated applications that reference or update up to one third of the frames from a large LOOM KB.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {97–104},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191266,
author = {Lee, Eric and Whalen, Thom},
title = {Computer Image Retrieval by Features: Selecting the Best Facial Features for Suspect Identification Systems},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191266},
doi = {10.1145/191246.191266},
abstract = {Correct suspect identification of known offenders by witnesses deteriorates rapidly as more are examined in mugshot albums. Feature approaches, where mugshots are displayed in order of similarity to witnesses' descriptions, increase identification success by reducing this number. System performance depends on selection of system features. Four methods of selecting features are evaluated empirically: theory, random, hill-climbing algorithm, and hybrid. The theory asserts success depends on five properties of system features: informativeness, orthogonality, sufficiency, consistency, and observability. Comparing system performance on the best 10 features selected (from a pool of 90) by each method supports our contention. In four experimental tests of a system with 1000 official mugshots, over 90% of witness searches resulted in photos of target suspects retrieved in the first ten mugshots displayed for examination (using all 90 system features). On average, suspects were retrieved in the first 54, 7, 22, and 70 mugshots when using only the best 10 model features. Hybrid and hill-climbing algorithms did not improve on this performance, and performance of randomly selected sets of 10 features was poor.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {105–111},
numpages = {7},
keywords = {computer image retrieval, feature retrieval, information retrieval, suspect identification},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191267,
author = {de Ferreira Rezende, Fernando and H\"{a}rder, Theo},
title = {A Lock Method for KBMSs Using Abstraction Relationships' Semantics},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191267},
doi = {10.1145/191246.191267},
abstract = {Knowledge Base Management Systems (KBMSs) are a growing research area finding applicability in different domains. As a consequence, the demand for ever-larger knowledge bases (KBs) is growing more and more. Inside this context, knowledge sharing turns out to be a crucial point to be supported by KBMSs. In this paper, we propose a way of controlling knowledge sharing. We show how we obtain serializability of transactions providing many different locking granules, which are based on the semantics of the abstraction relationships. The main benefit of our technique is the high degree of potential concurrency, to be obtained through a logical partitioning of the KB graph and the provision of lock types used for each referenced partition. By this way, we capture more of the semantics contained in a KB graph, through an interpretation of its edges grounded in the abstraction relationships, and make feasible a full exploitation of all inherent parallelism in a knowledge representation approach.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {112–121},
numpages = {10},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191268,
author = {Chaudhri, Vinay K. and Hadzilacos, Vassos and Mylopoulos, John and Sevcik, Kenneth C.},
title = {Quantitative Evaluation of a Transaction Facility for Knowledge Base Management System},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191268},
doi = {10.1145/191246.191268},
abstract = {Large knowledge bases that are intended for applications such as CAD, corporate repositories or process control will have to be shared by multiple users. For these systems to scale up, to give acceptable performance and to exhibit consistent behavior, it is mandatory to synchronize user transactions using a concurrency control algorithm. In this paper, we examine a novel concurrency control policy called Dynamic Directed Graph (or DDG) policy that effectively exploits the rich semantic structure of a knowledge base.Our analysis is carried out in the context of a real knowledge based system application from which knowledge base structure and workload parameters are computed. These serve as a basis for studying the implementation alternatives that arise as a result of knowledge base characteristics. The implementation alternatives that we consider include selection of portions of the knowledge base structure to be exploited for concurrency control, and also the dependence of concurrency on the traversal strategy used to search through the knowledge base. We analyze the effects of various workload parameters and conclude that the DDG policy improves substantially the response time for short transactions when there is heavy data contention.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {122–131},
numpages = {10},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191269,
author = {Dattolo, Antonina and Gisolfi, Antonio},
title = {Analytical Version Control Management in a Hypertext System},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191269},
doi = {10.1145/191246.191269},
abstract = {In this paper it is shown how structural and cognitive versioning issues can be efficiently managed in a Petri nets based hypertextual model. The advantages of this formalism are enhanced by modular and structured modeling; modularity allows to focus the attention only on some modules, while giving the abstraction of the others. Each module owns metaknowledge that is useful in defining new layers and contexts.The central point of the data model is the formulation and resolution of three recurrence equations, effective in describing both the versioning and the derivation history; these equations permit to express in precise terms both the structural evolution (changes operated on specific nodes of the net) and the behavioral one (changes concerning browsing).},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {132–139},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191270,
author = {Kwon, Oh-Woog and Kim, Myoung-Cheol and Choi, Key-Sun},
title = {Query Expansion Using Domain-Adapted, Weighted Thesaurus in an Extended Boolean Model},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191270},
doi = {10.1145/191246.191270},
abstract = {In this paper, we address there important issues with query expansion using a thesaurus; how to give weights to the terms in expanded queries, how to select additional search terms in the thesaurus, and how to enrich the terms in the manual thesaurus (namely, thesaurus reconstruction). To weight the terms in expanded queries, we construct the weighted thesaurus that has a similarity value between the terms in the thesaurus, using statistical co-occurrence in a corpus. To enrich the terms in the manual thesaurus, domain dependent terms which occur in a corpus are inserted into the weighted thesaurus using the co-occurrence information. In this paper, the reconstructed thesaurus with weights is defined as a domain-adapted, weighted thesaurus. Then we explain query expansion using the domain-adapted, weighted thesaurus in an extended Boolean retrieval model. To select additional search terms during query expansion, our model uses semi-automatic query expansion and a restriction method. In the experiments, our system had almost twice the recall of the boolean retrieval system not using the thesaurus or the query expansion retrieval system using the original thesaurus. And also, the precision of our system was almost the same precision as the other systems.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {140–146},
numpages = {7},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191271,
author = {B\"{o}hm, Klemens and M\'{u}ller, Adrian and Neuhold, Erich},
title = {Structured Document Handling—a Case for Integrating Databases and Information Retrieval},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191271},
doi = {10.1145/191246.191271},
abstract = {In this paper we discuss the structured multimedia documents that will be, or already are, to some degree the communication backbone of the so-called superhighways. It will be shown that storage and retrieval of such documents will best be handled by an integration of database and information retrieval technologies. We assume documents to be structured with the help of standards like SGML/HyTime and represented by the multitude of formats currently used for multimedia data.Starting with an approach based on object-oriented database technology we extend both their functionality on the cost models for query evaluation on one side with multimedia features and on the other with logic-based models of information retrieval to truly combine structure and content information about the documents in question.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {147–154},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191272,
author = {Lamirel, Jean-Charles and Crehange, Marion},
title = {Application of a Symbolic-Connectionist Approach for the Design of a Highly Interactive Documentary Database Interrogation System with on-Line Learning Capabilities},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191272},
doi = {10.1145/191246.191272},
abstract = {The NOMAD system is a documentary database interrogration system based on a symbolico-connectionist approach.NOMAD makes use of the synthesis capabilities and flexibility inherent in this type of approach to increase its processing power as compared as compared to existing systems while proposing new operating modes directly accessible to a large number of users:•NOMAD manages multiple synthetic type views on its documentary contents in the form of neural topographies acting as case-memories as well as elaborated thematic browsing tools.•NOMAD manages a session memory based on the neural model of the novelty detector with the following three functions: cumulative recording of user need, managing user contradictions and proposing new orientations.•NOMAD also has extended learning capabilities, enabling it to improve its performance in the long term.In the introduction of this article, we justify the modelling choices made for NOMAD. The system will then be described in detailed, stressing the new possibilities regarding help in query formulation and improvement in retrieval performance. The evaluation campaign and the numerous evolution perspectives provided by the model are described in the final section.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {155–163},
numpages = {9},
keywords = {document classification, symbolico-connectionist model, information retrieval, unsupervised learning, case-based reasoning, hypertext generation, novelty detection, thematic browsing},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191273,
author = {Syu, Inien and Lang, S. D.},
title = {A Competition-Based Connectionist Model for Information Retrieval Using a Merged Thesaurus},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191273},
doi = {10.1145/191246.191273},
abstract = {This paper investigates a network-based information retrieval model using diagnostic inferencing techniques. A basic inference network in information retrieval consists of two component networks: the document component and the query component. In our approach, there is a layer of nodes corresponding to the documents, and a layer of nodes corresponding to the index terms extracted from the document set, with links connecting documents to the related index terms 1. A thesaurus is used to provide concept categories; these categories are represented by another layer of nodes, with links connecting the index terms and the related categories 2. The query component uses a symmetric structure. Each query causes markings of category nodes, hence markings of the related index term nodes, in the document component of the network. In our previous work, we adapted a competition-based connectionist model for diagnostic problem solving to information retrieval. In this model, documents are treated as “disorders” and user information needs, represented by the marked index term nodes, as “manifestations”. A competitive activation mechanism is then used which converges to a set of disorders that best explain the given manifestations. Our experiments showed that the retrieval performance of this model is comparable to or better than that of various information retrieval models reported in the literature. In this paper, we report further enhancements of the model by using a merged thesaurus.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {164–170},
numpages = {7},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191274,
author = {Chu, S. Iris and Winslett, Marianne},
title = {Minipage Locking Support for Object-Oriented Page-Server DBMS},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191274},
doi = {10.1145/191246.191274},
abstract = {Many object-oriented database systems are implemented using a page-server architecture for its performance advantages. Since the applications envisioned for object-oriented DBMSes typically spend a great deal of time processing data already in memory, fast in-memory access is very important. A page-server architecture will permit an implementation where most routine reference following (i.e., where the referenced data is in memory and appropriately locked) is handled by virtual memory hardware to eliminate expensive software overhead. One of the major drawbacks of this approach is that locking and authorization must be handled on a per-page basis, causing unacceptable low concurrency for high-contention data pages and difficulties in supporting fine-grained authorization. With hardware support on the client side for locks on minipages (subdivisions of a page), however, it is possible to have good improvements in concurrency for high-contention areas of the database, along with the ability to do fine-grained authorization. This paper presents a callback-read locking scheme that makes use of hardware-assisted locking of minipages and compares its performance with one that uses page protection under four different workloads. Minipages are already available in several commonly used platforms, but only at the internal levels of the operating system. We conclude that minipages improve performance significantly in high-contention workloads, with minimal performance impact under low-contention workloads, and that minipage facilities should be made visible to client DBMS code. We also discuss the application of our locking algorithms to page servers that supporting object-level locking.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {171–178},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191275,
author = {Pal, Shankar and Lanka, Sitaram},
title = {Isolation of Transaction Aborts in Object-Oriented Database Systems},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191275},
doi = {10.1145/191246.191275},
abstract = {We address the problem of recovery in object-oriented data-bases from considerations of the semantics of committed transactions and the efficiency of recovery procedures. The recovery strategy used is update-in-place. We show that efficient recovery procedures should allow transactions to abort independently of one another by executing inverse operations. We refer to these requirements collectively as the recovery isolation property. We present a formal definition of recovery isolation and prove that schedules possessing this property do not suffer from rollback dependencies. Recovery isolation is useful in constructing concurrency control protocols that go beyond commutativity, and in parallel and high performance database systems. We define the notion of strict schedules for object-oriented databases as an extension of the analogous definition for read and write operations. We show that strict schedules possess the recovery isolation property.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {179–186},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191276,
author = {Soparkar, Nandit and Levy, Eliezer and Korth, Henry F. and Silberschatz, Avi},
title = {Adaptive Commitment for Distributed Real-Time Transactions},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191276},
doi = {10.1145/191246.191276},
abstract = {Distributed real-time transaction systems are useful for both real-time and high-performance database applications. Standard transaction management approaches that use the two-phase commit protocol suffer from its high costs and blocking behavior which is problematic in real-time computing environments. Our approach in this paper is to identify ways in which a commit protocol can be made adaptive in the sense that under situations that demand it, such as a transient local overload, the system can dynamically change to a different commitment strategy. The decision to do so can be taken autonomously at any site. The different commitment strategies exploit a trade-off between the cost of commitment and the obtained degree of atomicity. Our protocols are based on optimistic  commitment strategies, and they rely on local compensatory actions to recover from non-atomic executions. We provide the necessary framework to study the logical and temporal correctness criteria, and we describe examples to illustrate the use of our strategies.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {187–194},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191278,
author = {Pu, Calton and Tsang, Miu K. and Wu, Kun-Lung and Yu, Philip S.},
title = {Multiversion Divergence Control of Time Fuzziness},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191278},
doi = {10.1145/191246.191278},
abstract = {Epsilon Serializability (ESR) has been proposed to manage and control inconsistency in extending the classic transaction processing. ESR increases system concurrency by tolerating a bounded amount of inconsistency. In this paper, we present multiversion divergence control (mvDC) algorithms that support ESR with not only value but also time fuzziness in multiversion databases. Unlike value fuzziness, accumulating time fuzziness is semantically different. A simple summation of the length of two time intervals may either underestimate the total time fuzziness, resulting in incorrect execution, or overestimate the total time fuzziness, unnecessarily degrading the effectiveness of mvESR. We present a new operation, called TimeUnion, to accurately accumulate the total time fuzziness. Because of the accurate control of time and value fuzziness by the mvDC algorithm, mvESR is very suitable for the use of multiversion databases for real-time applications that may tolerate a limited degree of data inconsistency but prefer more data recency.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {195–202},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191280,
author = {Jain, Neel K.},
title = {Group Formation Mechanisms for Transactions in Isis},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191280},
doi = {10.1145/191246.191280},
abstract = {Distributed toolkits like Isis provide means of replicating data but not means for making it persistent. This makes the use of transactions desirable, even in non-database applications. Using Isis can alleviate the programming cost of distributed transaction processing the multi-phase commit protocols. Using the Isis transaction tool, however, imposes additional cost, and we examine the effect of group formation strategies on the overhead. The paper presents three different group formation mechanisms in Isis and compares the costs associated with them.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {203–210},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191281,
author = {Helal, Abdelsalam and Ku, Tung-Hui and Fortner, Jud},
title = {Quasi-Dynamic Two-Phase Locking},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191281},
doi = {10.1145/191246.191281},
abstract = {Among the plethora of concurrency control algorithms that have been proposed and analyzed, two-phase locking (2PL) has been adapted as the industry de facto standard concurrency control. In accord, current research in concurrency control is focusing on enhancing the scalability of 2PL performance in highly concurrent and contentious environments. This is especially needed in future on-line transaction processing systems, where thousand Transaction Per Second performance will be required.Static locking (SL) and dynamic locking (DL) are two famous adaptations of 2PL that are used under different degrees of data contention. In this paper, we offer our observation that 2PL is indeed a family of methods, of which SL and DL are extreme case members. Further, we argue for and verify the existence of other 2PL member methods that, under variable conditions, outperform SL and DL. We propose two novel schemes which we categorize as quasi-dynamic two-phase locking on account of their behavior in comparison with dynamic/static two-phase locking. We present a simulation study of the performance of the proposed schemes and their comparison to dynamic and static locking methods.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {211–218},
numpages = {8},
keywords = {resource contention, data contention, pre-declaration, two-phase locking, adaptability},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191283,
author = {Borgida, Alex},
title = {On the Relationship between Description Logic and Predicate Logic Queries},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191283},
doi = {10.1145/191246.191283},
abstract = {Description languages form the basis of several object-centered knowledge base management systems developed in recent years, including ones in industrial use. Originally used for conceptual modeling (to define views), DLs are seeing increased use as query languages for retrieving information. This paper, aimed at a general audience that includes database researchers, considers the relationship between the expressive power of DLs and that of query languages based on Predicate Calculus.We show that all descriptions built using constructors currently considered in the literature can be expressed as formulae of the First Order Predicate Calculus (FOPC) with at most three variable symbols, though we have to allow numeric quantifiers and infinitary disjunction in order to handle some special constructors. Conversely, we show that all first-order queries (formulae with one free variable) built up from unary and binary predicates using at most three variables can be expressed as descriptions. We conclude by exhibiting queries that cannot be expressed as DL concepts, and reflecting briefly on consequences.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {219–225},
numpages = {7},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191285,
author = {Han, Jiawei and Liu, Ling and Xie, Zhaohui},
title = {LogicBase: A Deductive Database System Prototype},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191285},
doi = {10.1145/191246.191285},
abstract = {A deductive database system prototype, LogicBase, has been developed, with an emphasis on efficient compilation and query evaluation of application-oriented recursions in deductive databases. The system identifies different classes of recursions and compiles recursions into chain or psuedo-chain forms when appropriate. Queries posed to the compiled recursions are analyzed systematically with efficient evaluation plans generated and executed, mainly based on a chained-based query evaluation method. The system has been tested using sophisticated recursions and queries with satisfactory performance. This paper introduces the general design principles and implementation techniques of the system and discusses its strength and limitations.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {226–233},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191287,
author = {Liu, Hong-Chen and Ramamohanarao, K.},
title = {Algebraic Equivalences among Nested Relational Expressions},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191287},
doi = {10.1145/191246.191287},
abstract = {Algebraic optimization is both theoretically and practically important for query processing in (nested) relational databases. In this paper, we consider this issue and investigate some algebraic properties concerning the nested relational operators. We also outline a heuristic optimization algorithm for nested relational expressions by adopting algebraic transformation rules developed in this paper and previous related work.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {234–243},
numpages = {10},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191288,
author = {Yoon, Suk-Chung and Song, Il-Yeol and Park, E. K.},
title = {Intelligent Query Answering in Deductive and Object-Oriented Databases},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191288},
doi = {10.1145/191246.191288},
abstract = {In the near future, we believe that we will need much more sophisticated answer-finding schemes in an object-oriented database in order to satisfy the needs of truly intelligent information system. In this paper, we introduce a method to apply the intensional query processing techniques of deductive databases to object-oriented databases. So, we can generate intensional answers to represent answer-set abstractly for a given query in object-oriented databases.Our approach consists of four steps: rule generation, pre-resolution, resolution, and post-resolution. In rule generation, we generate a set of deductive rules based on an object-oriented database schema. In pre-resolution, rule transformation is done to get unique intensional literals and extended term-restricted rules. In resolution, we identify rules that are potentially relevant to a query. In post-resolution, we find relevant resolvents as candidates for intensional answers among potentially relevant resolvents. We also use the notion of potentially relevant resolvents and relevant resolvents to avoid generating certain meaningless intensional answers.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {244–251},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191290,
author = {Johannesson, Paul},
title = {Linguistic Instruments and Qualitative Reasoning for Schema Integration},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191290},
doi = {10.1145/191246.191290},
abstract = {Two major problems in schema integration are to identify correspondences between different conceptual schemas and to verify that the proposed correspondences are consistent with the semantics of the schemas. We propose a heuristic method, based on the use of Galois lattices, for identifying schema correspondences. We show how the results of this method can be checked for correctness by introducing a number of necessary conditions for schema mergeability. These conditions are formulated in the context of a semantically rich modelling formalism, the distinguishing feature of which is the use of case grammar.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {252–262},
numpages = {11},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191291,
author = {Vidal, V\^{a}nia M. P. and Winslett, Marianne},
title = {Preserving Update Semantics in Schema Integration},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191291},
doi = {10.1145/191246.191291},
abstract = {In this paper, we propose a methodology for schema integration where the semantics of updates is preserved during the view integration process. We propose to divide view integration into three steps: combination, restructuring, and optimization. In the view combination step, we define the combined schema that contains all original views, plus a new set of constraints that express how data in distinct views are interrelated. The restructuring step is devoted to normalizing the views so that merging becomes possible. The optimization step tries to reduce redundancy and the size of the schema. Our methodology defines a set of transformation primitives that allows schema integration to be realized in a safe (information preserving) and algorithmic way. In the proposed transformation primitives, the relationship between the original and transformed schema is formally specified by the instance and update mappings. We introduce the notion of an update semantics preserving transformation, which guarantees that the relationships between each view and the global schema, originated during the view integration process, reflect exactly the relationships between the views as defined by the combined schema. In our approach, the view definition mappings and the view update translator can be directly defined from the instance and update mappings between the different intermediate schemas generated during the view integration process.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {263–271},
numpages = {9},
keywords = {semantic model, schema integration, update translation, user views, schema mapping},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191293,
author = {Lehmann, Fritz and Cohn, Anthony G.},
title = {The EGG/YOLK Reliability Hierarchy: Semantic Data Integration Using Sorts with Prototypes},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191293},
doi = {10.1145/191246.191293},
abstract = {Integration of disparate heterogeneous databases requires translation of types. Because a type in one system often has no exact counterpart in the others, fully reliable integration requires deep understanding of the subject domain, with conceptual analysis of type meanings. So far, reliable translation has had to be done by hand. In practice, few types are so crucial as to require full reliability. The EGG/YOLK hierarchy ranks types by the tolerable rashness in translation, based on prototypes in each type. Each defined class (EGG) has a subclass of typical members (YOLK) defined. We exploit Cui, Cohn and Randell's Qualitative Spatial Simulation program to create the hierarchy of all possible relations between source and target EGG/YOLK types, ranked by reliability. Our eventual ranking is based on a poset combining four different preference criteria.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {272–279},
numpages = {8},
keywords = {spatial reasoning, prototypes, data translation, formal concept lattice, database integration, order theory, poset, semantic integration, view integration},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191294,
author = {Pissinou, Niki and Makki, Kia and Park, E. K.},
title = {Towards a Framework for Integrating Multilevel Secure Models and Temporal Data Models},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191294},
doi = {10.1145/191246.191294},
abstract = {Within many organizations the number of databases containing classified or otherwise sensitive data is increasing rapidly. Access to these databases must be restricted and controlled to limit the unauthorized disclosure, or malicious modification of data contained in them. However, the conventional models of authorization that have been designed for database systems supporting the hierarchical, network and relational models of data do not provide adequate mechanisms to support controlled access of temporal objects and context based temporal information. In this paper we extend the multilevel secure relational model to capture the functionality required of a temporal database, i.e. a database that supports some aspect of time, not counting user-defined time. In particular we assign class access to bitemporal timestamped attributes, and give explicit security classifications to temporal elements.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {280–287},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191296,
author = {Cheng, Tsz S. and Gadia, Shashi K.},
title = {A Pattern Matching Language for Spatio-Temporal Databases},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191296},
doi = {10.1145/191246.191296},
abstract = {We propose a pattern matching language for spatio-temporal databases. The matching process in time dimension is based upon the evolutionary nature of time, but in spatial dimension it is based on placement, shape and sizes of regions. The concept of pattern matching introduced in this paper is independent of the choice of the underlying model for spatio-temporal databases. In particular, the pattern matching language seamlessly extends our SQL-like query language ParaSQL for spatio-temporal databases. The pattern matching language would also have application in active databases, because patterns can be used as triggers.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {288–295},
numpages = {8},
keywords = {object identity, temporal data, spatial data, active databases, dimension alignment, parametric data, spatio-temporal data, pattern matching, time cursor, SQL, object-relational databases},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191298,
author = {Kouramajian, Vram and Kamel, Ibrahim and Elmasri, Ramez and Waheed, Syed},
title = {The Time Index<sup>+</sup>: An Incremental Access Structure for Temporal Databases},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191298},
doi = {10.1145/191246.191298},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {296–303},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191300,
author = {Ghandeharizadeh, Shahram and Ierardi, Douglas J.},
title = {Management of Disk Space with REBATE},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191300},
doi = {10.1145/191246.191300},
abstract = {The past decade has witnessed a proliferation of respositories whose workload consists of queries that retrieve information. These repositories provide on-line access to vast amount of data and serve as an integral component of many application domains (e.g., library information systems, scientific applications, entertainment industry). Their storage subsystem is expected to be hierarchical consisting of memory, disk drives, and one or more tertiary storage devices. The database resides permanently on the tertiary storage devices and objects are swapped onto the magnetic disk drives on demand (and deleted once the disk storage capacity is exhausted). This may fragment the disk space over a period of time, resulting in a non-contiguous layout of an object across the surface of a disk drive. This is undesirable because, once the object is referenced, the disk drive is required to reposition its read head multiple times (incur seek operations) when retrieving the object, resulting in a low performance.This paper presents the design of REBATE. REBATE ensures the contiguous layout of each object across the surface of a disk drive by partitioning the available disk space into regions where each region manages objects of approximately the same size. We describe the tradeoffs of using REBATE and its possible limitations.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {304–311},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191302,
author = {Keen, John S. and Dally, William J.},
title = {XEL: Extended Ephemeral Logging for Log Storage Management},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191302},
doi = {10.1145/191246.191302},
abstract = {Extended ephemeral logging (XEL) is a more general variation of the ephemeral logging (EL) technique for managing a log of database activity on disk; it does not require a timestamp to be maintained with each object in the database. XEL does not require periodic checkpoints and does not abort lengthy transactions as frequently as traditional firewall logging for the same amount of disk space. Therefore, it is well suited for concurrent databases and applications which have a wide distribution of transaction lifetimes.Simulation results indicate that XEL can offer significant savings in disk space, at the expense of slightly higher bandwidth for logging and more main memory. The reduced size of the log permits much faster recovery after a crash as well as cost savings.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {312–321},
numpages = {10},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191303,
author = {\"{O}zden, Banu and Rastogi, Rajeev and Silberschatz, Avi},
title = {On the Storage and Retrieval of Continuous Media Data},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191303},
doi = {10.1145/191246.191303},
abstract = {Continuous media applications, which require a guaranteed transfer rate of the data, are becoming an integral part of daily computational life. However, conventional file systems do not provide rate guarantees, and are therefore not suitable for the storage and retrieval of continuous media data (e.g., audio, video). To meet the demands of these new applications, continuous media file systems, which provide rate guarantees by managing critical storage resources such as memory and disks, must be designed.In this paper, we highlight the issues in the storage and retrieval of continuous media data. We first present a simple scheme for concurrently retrieving multiple continuous media streams from disks. We then introduce a a clever allocation technique for storing continuous media data that eliminates disk latency and thus, drastically reduces RAM requirements. We present, for video data, schemes for implementing the operations fast-forward, rewind and pause. Finally, we conclude by outlining directions for future research in the storage and retrieval of continuous media data.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {322–328},
numpages = {7},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191305,
author = {Liu, Ling},
title = {Design and Evaluation Rules for Building Adaptive Schema in an Object-Oriented Data and Knowledge Base System},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191305},
doi = {10.1145/191246.191305},
abstract = {We develop a selection of design and evaluation rules for building an adaptive schema in an object-oriented data and knowledge base system. This set of style rules include not only those which we use to preserve validity and minimality of an object-oriented schema, but also those which help us to promote extensibility, reusability and adaptiveness of an object-oriented schema against future requirement changes. We encourage to use the set of style rules proposed as a means for validating quality of a schema, and for transforming an object-oriented schema into a better style regarding to adaptiveness and robustness, rather than as a user-oriented method solely for designing the schema.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {329–336},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191306,
author = {Goh, Cheng Hian and Madnick, Stuart E. and Siegel, Michael D.},
title = {Context Interchange: Overcoming the Challenges of Large-Scale Interoperable Database Systems in a Dynamic Environment},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191306},
doi = {10.1145/191246.191306},
abstract = {Research in database interoperability has primarily focused on circumventing schematic and semantic incompatibility arising from autonomy of the underlying databases. We argue that, while existing integration strategies might provide satisfactory support for small or static systems, their inadequacies rapidly become evident in large-scale interoperable database systems operating in a dynamic environment. This paper highlights the problem of receiver heterogeneity, scalability, and evolution which have received little attention in the literature, provides an overview of the Context Interchange approach to interoperability, illustrates why this is able to better circumvent the problems identified, and forges the connections to other works by suggesting how the context interchange framework differs from other integration approaches in the literature.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {337–346},
numpages = {10},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191307,
author = {Aref, Walid G. and Samet, Hanan},
title = {Hashing by Proximity to Process Duplicates in Spatial Databases},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191307},
doi = {10.1145/191246.191307},
abstract = {In a spatial database, an object may extend arbitrarily in space. As a result, many spatial data structures (e.g., the quadtree, the cell tree, the R+-tree) represent an object by partitioning it into multiple, yet simple, pieces, each of which is stored separately inside the data structure. Many operations on these data structures are likely to produce duplicate results because of the multiplicity of object pieces. A novel approach for duplicate processing based on proximity of spatial objects is presented. This is different from conventional duplicate elimination in database systems because, with spatial databases, different pieces of the same object can span multiple buckets of the underlying data structure. Example algorithms are presented to perform duplicate processing using proximity for quadtree representation of line segments and arbitrary rectangles. The complexity of the algorithms is seen to depend on a geometric classification of different instances of the spatial objects. By using proximity and the spatial properties of the objects, the number of disk-I/O requests as well as the run-time storage during duplicate processing can be reduced.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {347–354},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191308,
author = {Zhao, J. Leon and Zaki, Ahmed},
title = {Spatial Data Traversal in Road Map Databases: A Graph Indexing Approach},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191308},
doi = {10.1145/191246.191308},
abstract = {Spatial data are found in geographic information systems such as digital road map databases where city and road attributes are associated with nodes and links in a directed graph. Queries on spatial data are expensive because of the recursive property of graph traversal. We propose a graph indexing technique to expedite spatial queries where the graph topology remains relatively stationary. Using a probabilistic analysis, this paper shows that the graph indexing technique significantly improves the efficiency of constrained spatial queries.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {355–362},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191309,
author = {Kashyap, Vipul and Sheth, Amit},
title = {Semantics-Based Information Brokering},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191309},
doi = {10.1145/191246.191309},
abstract = {The rapid advances in computer and communication technologies, and their merger, is leading to a global information market place. It will consist of federations of very large number of information systems that will cooperate to varying extents to support the users' information needs. We discuss an approach to information brokering in the above environment. We discuss two of its tasks: information resource discovery, which identifies relevant information sources for a given query, and query processing, which involves the generation of appropriate mapping from relevant but structurally heterogeneous objects. Query processing consists of information focusing and information correlation.Our  approach is based on: semantic proximity, which represents semantic similarities based on the context of comparison, and schema correspondences which are used to represent structural mappings and are associated with the context. The context of comparison of the two objects is the primary vehicle to represent the semantics for determining semantic proximity. Specifically, we use a partial context representation to capture the semantics in terms of the assumptions in the intended use of the objects and the intended meaning of the user query. Information focusing is supported by subsequent context comparison. The same mechanism can be used to support information resource discovery. Context  comparison leads to changes in schema correspondences  that are used to support information correlation.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {363–370},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191310,
author = {Pitoura, Evaggelia and Bhargava, Bharat},
title = {Building Information Systems for Mobile Environments},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191310},
doi = {10.1145/191246.191310},
abstract = {It is expected that in the near future, tens of millions of users will have access to distributed information systems through wireless connections. The technical characteristics of the wireless medium and the resulting mobility of both data resources and data consumers raise new challenging questions regarding the development of information systems appropriate for mobile environments. In this paper, we report on the development of such a system. First, we describe the general architecture of the information system and the main considerations of our design. Then, based on these considerations, we present our system support for maintaining the consistency of replicated data and for providing transaction schemas that account for the frequent but predictable disconnections, the mobility, and the vulnerability of the wireless environment.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {371–378},
numpages = {8},
keywords = {information systems, new applications, transaction management, mobile computing, consistency},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191311,
author = {Flater, David and Yesha, Yelena},
title = {The Role of the Database Community in the National Information Infrastructure},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191311},
doi = {10.1145/191246.191311},
abstract = {The computer science community is increasingly focusing its efforts on tasks related to the National Information Infrastructure. Hardware and software advances are being sought to make wide-area networks and technological resources in general more useful to mainstream society. With this transition comes a set of unavoidable political and social issues that have never been satisfactorily dealt with in the past. Grand challenges face the computer science community on both the technical and the socio-political sides of this “major upgrade.” This paper discusses various aspects of the enormous coordination problem that faces all of us and considers what the database community can do to help.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {379–383},
numpages = {5},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191312,
author = {Kirsche, Thomas and Lenz, Richard and Schuster, Hans},
title = {Functionality and Architecture of a Cooperative Database System: A Vision},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191312},
doi = {10.1145/191246.191312},
abstract = {A database system fostering the cooperative usage and modification of a common data pool should provide standard database functionality (e.g. application-independent correctness criteria and data modelling) plus means for a step-wise, cooperative refinement of data over a long period of time. Key ingredients are a hierarchical organization of work, a sound data model covering cooperative uncertainly, and support for long-living cooperative processes. Furthermore, mechanisms for data passing and hiding, negotiation means, and notification are of prominent importance. In the paper, the rationale behind such a functionality is described. Furthermore, a proposal is made for the software architecture of what is called a cooperative database system (CDBMS).},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {384–391},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191313,
author = {Hu, Xiaohua and Cercone, Nick},
title = {Discovery of Decision Rules in Relational Databases: A Rough Set Approach},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191313},
doi = {10.1145/191246.191313},
abstract = {We develop an attribute-oriented rough set approach for the discovery of decision rules in relational databases. Our approach combines machine learning techniques and rough set theory. We consider a learning procedure to consist of the two phases data generalization and data reduction. In the data generalization phase, utilizing knowledge about concept hierarchies and relevance of the data, an attribute-oriented induction is performed attribute by attribute. Some undesirable attributes of the discovery task are removed and the primitive data in the databases are generalized to the desirable level; this process greatly decreases the number of tuples which must be examined for the discovery task and substantially reduces the computational complexity of the database learning processes. Subsequently, in data reduction phase, rough set theory is applied to the generalized relation; the cause-effect relationships among the condition and decision attributes in the databases are analyzed and the non-essential or irrelevant attributes to the discovery task are eliminated without losing information of the original database system. This process further reduces the generalized relation. Thus very concise and more accurate decision rules for each class in the decision attribute with little or no redundancy information, can be extracted automatically from the reduced relation during the learning process. Our study shows that attribute-oriented induction combined with rough set theory provide an efficient and effective mechanism for discovering decision rules in database systems.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {392–400},
numpages = {9},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191314,
author = {Klemettinen, Mika and Mannila, Heikki and Ronkainen, Pirjo and Toivonen, Hannu and Verkamo, A. Inkeri},
title = {Finding Interesting Rules from Large Sets of Discovered Association Rules},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191314},
doi = {10.1145/191246.191314},
abstract = {Association rules, introduced by Agrawal, Imielinski, and Swami, are rules of the form “for 90% of the rows of the relation, if the row has value 1 in the columns in set W, then it has 1 also in column B”. Efficient methods exist for discovering association rules from large collections of data. The number of discovered rules can, however, be so large that browsing the rule set and finding interesting rules from it can be quite difficult for the user. We show how a simple formalism of rule templates makes it possible to easily describe the structure of interesting rules. We also give examples of visualization of rules, and show how a visualization tool interfaces with rule templates.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {401–407},
numpages = {7},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191315,
author = {Rose, J. Royce and Gasteiger, Johann},
title = {Hierarchical Classification as an Aid to Database and Hit-List Browsing},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191315},
doi = {10.1145/191246.191315},
abstract = {A navigational aid for databases that relies on unsupervised hierarchical classification is presented. The approach to hierarchical classification, based on both functional and topological features, supports the creation of deep hierarchies in which succeeding levels represent increasing degrees of abstraction. This allows the user to quickly evaluate the result of a query and to locate interesting items and classes of items by performing a tree traversal rather than a sequential perusal of a hit list or a series of ad hoc query refinements. In very large databases where classical querying methods are increasingly inadequate such as chemical reaction databases, such a browsing method is required in order to manage the flood of information with which the user is confronted.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {408–414},
numpages = {7},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191316,
author = {Heinlein, C. and Kuhn, K. and Dadam, P.},
title = {Representation of Medical Guidelines Using a Classification-Based System},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191316},
doi = {10.1145/191246.191316},
abstract = {Medical guidelines play an increasing role in selecting diagnostic and therapeutic steps under the aspects of effectiveness, invasiveness, and costs. To work directly on patient data already available in electronic form, they should be integrated into a medical information system. In order to develop a “medical guideline module” (MGM) managing and applying guidelines to patients, a “knowledge level” representation of guidelines is necessary which reflects the structure of medical knowledge and matches medical processes. Furthermore, a direct transformation to the “symbol level” is needed. We use a nested, frame-like structure on the knowledge level and show that a classification-based knowledge representation system (CBKRS) is principally well suited for the symbol level. To facilitate the usage and to be independent of a particular CBKRS, we introduce an intermediate level called “intelligent object system” (IOS). It is developed by augmenting a simple data model for describing complex objects with prototypes and implications as a means to classify objects and to draw inferences based on this classification. Finally, the transformation of guidelines to prototypes and implications is described.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {415–422},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191317,
author = {Davidson, S. B. and Kosky, A. S. and Eckman, B.},
title = {Facilitating Transformations in a Human Genome Project Database},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191317},
doi = {10.1145/191246.191317},
abstract = {Human Genome Project databases present a confluence of interesting database challenges: rapid schema and data evolution, complex data entry and constraint management, and the need to integrate multiple data sources and software systems which range over a wide variety of models and formats. While these challenges are not necessarily unique to biological databases, their combination, intensity and complexity are unusual and make automated solutions imperative. We illustrate these problems in the context of the Philadelphia Genome Center for Human Chromosome 22, and describe a new approach to a solution for these problems, by means of a deductive language for expressing database transformations and constraints.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {423–432},
numpages = {10},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191318,
author = {Arens, Yigal and Knoblock, Craig A.},
title = {Intelligent Caching: Selecting, Representing, and Reusing Data in an Information Server},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191318},
doi = {10.1145/191246.191318},
abstract = {Accessing information sources to retrieve data requested by a user can be expensive, especially when dealing with distributed information sources. One way to reduce this cost is to cache the results of queries, or related classes of data. This paper presents an approach to caching and addresses the issues of which information to cache, how to describe what has been cached, and how to use the cached information to answer future queries. We consider these issues in the context of the SIMS information server, which is a system for retrieving information from multiple heterogeneous and distributed information sources. The design of this information server is ideal for representing and reusing cached information since each class of cached information is simply viewed as another information source that is available for answering future queries.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {433–438},
numpages = {6},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191319,
author = {Jiang, Yin-he and Liu, Xiangning and Bhargava, Bharat},
title = {Re-Evaluating Indexing Schemes for Nested Objects},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191319},
doi = {10.1145/191246.191319},
abstract = {Performance is a major issue in the acceptance of object-oriented database management systems (OODBMS). The nested index and path index schemes have been criticized for their heavy costs and poor handling of update operations. This paper re-evaluates three index schemes (nested index, path index, and multi-index) applicable to queries on nested attributes. Among these, we found that a multi-index scheme is best supported in the object-oriented or extended relational DBMS environment. Multi-index schemes not only provide a better balance between retrieval and update costs than do the nested or path indices, but they also scale well for update when the number of indices increases. In this paper, we propose a multi-index design that reuses the single-table index structures already present in a DBMS. Our performance study extends previous models by permitting attributes to be multi-valued as well as single-valued. We also suggest that a combination of nested index and multi-index schemes offers a feasible solution to the support of queries on nested objects.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {439–446},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191320,
author = {Labrou, Yannis and Finin, Tim},
title = {A Semantics Approach for KQML—a General Purpose Communication Language for Software Agents},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191320},
doi = {10.1145/191246.191320},
abstract = {We investigate the semantics for Knowledge Query Manipulation Language (KQML) and we propose a semantic framework for the language. KQML is a language and a protocol to support communication between software agents. Based on ideas from speech act theory, we propose a semantic description for KQML that associates descriptions of the cognitive states of agents with the use of the language's primitives (performatives). We use this approach to describe the semantics for the basic set of KQML performatives. We also investigate implementation issues related to our semantic approach. We suggest that KQML can offer an all purpose communication language for software agents that requires no limiting pre-commitments on the agents' structure and implementation. KQML can provide the Distributed AI, Cooperative Distributed Problem Solving and Software Agents communities with an all purpose language and environment for intelligent inter-agent communication.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {447–455},
numpages = {9},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

@inproceedings{10.1145/191246.191322,
author = {Finin, Tim and Fritzson, Richard and McKay, Don and McEntire, Robin},
title = {KQML as an Agent Communication Language},
year = {1994},
isbn = {0897916743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/191246.191322},
doi = {10.1145/191246.191322},
abstract = {This paper describes the design of and experimentation with the Knowledge Query and Manipulation Language (KQML), a new language and protocol for exchanging information and knowledge. This work is part of a larger effort, the ARPA Knowledge Sharing Effort which is aimed at developing techniques and methodology for building large-scale knowledge bases which are sharable and reusable. KQML is both a message format and a message-handling protocol to support run-time knowledge sharing among agents. KQML focuses on an extensible set of performatives, which defines the permissible “speech acts” agents may use and comprise a substrate on which to develop higher-level models of interagent interaction such as contract nets and negotiation. In addition, KQML provides a basic architecture for knowledge sharing through a special class of agent called communication facilitors which coordinate the interactions of other agents. The ideas which underlie the evolving design of KQML are currently being explored through experimental prototype systems which are being used to support several testbeds in such areas as concurrent engineering, intelligent design and intelligent planning and scheduling.},
booktitle = {Proceedings of the Third International Conference on Information and Knowledge Management},
pages = {456–463},
numpages = {8},
location = {Gaithersburg, Maryland, USA},
series = {CIKM '94}
}

