@inproceedings{10.1145/3250786,
author = {Heuser, Carlos A.},
title = {Session Details: XML Query Processing (DB)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250786},
doi = {10.1145/3250786},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321445,
author = {Brodianskiy, Tali and Cohen, Sara},
title = {Self-Correcting Queries for Xml},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321445},
doi = {10.1145/1321440.1321445},
abstract = {It has been observed that queries over XML data sources are often unsatisfiable. Unsatisfiability may stem from several different sources, e.g., the user may be insufficiently familiar with the labels appearing the documents, or may not be intimately aware of the hierarchical structure of the documents. This difficulty may be compounded by the fact that errors in query formulation lead to an empty answer, and not to some sort of compilation error.To deal with query and document mismatches, previous research has considered returning answers that maximally satisfy (in some sense) the query, instead of only returning strictly satisfying answers. However, this breaks the golden database rule that only strictly satisfying answers are returned when querying. Indeed, the relationship between the query and answers is no longer clear, when unsatisfying answers are returned. To revive the golden database rule, this paper proposes a framework for deriving self-correcting queries over XML. This framework generates similar satisfiable queries, when the user query is unsatisfiable. The user can then choose a satisfiable query of interest, and receive exactly satisfying answers to this query.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {11–20},
numpages = {10},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321446,
author = {Souldatos, Stefanos and Wu, Xiaoying and Theodoratos, Dimitri and Dalamagas, Theodore and Sellis, Timos},
title = {Evaluation of Partial Path Queries on Xml Data},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321446},
doi = {10.1145/1321440.1321446},
abstract = {XML query languages typically allow the specification of structural patterns of elements. Finding the occurrences of such patterns in an XML tree is the key operation in XML query processing. Many algorithms have been presented for this operation. These algorithms focus mainly on the evaluation of path-pattern or tree-pattern queries. In this paper, we define a partial path-pattern query language, and we address the problem of its efficient evaluation on XML data.In order to process partial path-pattern queries, we introduce a set of sound and complete inference rules to characterize structural relationship derivation. We provide necessary and sufficient conditions for detecting query unsatisfiability and node redundancy. We show how partial path-pattern queries can be equivalently put in a canonical directed acyclic graph form. We developed two stack-based algorithms for the evaluation of partial path-pattern queries, PartialMJ and PartialPathStack. PartialMJ computes answers to the query by merge-joining the results of the root-to-leaf paths of a spanning tree of the query. PartialPathStack exploits a topological order of the nodes of the query graph to match the query pattern as a whole to the XML tree. The experimental evaluation of our algorithms shows that PartialPathStack is independent of intermediate results and largely outperforms PartialMJ.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {21–30},
numpages = {10},
keywords = {tree-structured data, partial path-pattern query, query evaluation},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321447,
author = {Li, Guoliang and Feng, Jianhua and Wang, Jianyong and Zhou, Lizhu},
title = {Effective Keyword Search for Valuable Lcas over Xml Documents},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321447},
doi = {10.1145/1321440.1321447},
abstract = {In this paper, we study the problem of effective keyword search over XML documents. We begin by introducing the notion of Valuable Lowest Common Ancestor (VLCA) to accurately and effectively answer keyword queries over XML documents. We then propose the concept of Compact VLCA (CVLCA) and compute the meaningful compact connected trees rooted as CVLCAs as the answers of keyword queries. To efficiently compute CVLCAs, we devise an effective optimization strategy for speeding up the computation, and exploit the key properties of CVLCA in the design of the stack-based algorithm for answering keyword queries. We have conducted an extensive experimental study and the experimental results show that our proposed approach achieves both high efficiency and effectiveness when compared with existing proposals.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {31–40},
numpages = {10},
keywords = {lca, information retrieval, xml keyword search, vlca, cvlca},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250787,
author = {Go\~{n}i, Alfredo},
title = {Session Details: Semantic Annotation (KM)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250787},
doi = {10.1145/3250787},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321449,
author = {Wu, Fei and Weld, Daniel S.},
title = {Autonomously Semantifying Wikipedia},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321449},
doi = {10.1145/1321440.1321449},
abstract = {Berners-Lee's compelling vision of a Semantic Web is hindered by a chicken-and-egg problem, which can be best solved by a bootstrapping method - creating enough structured data to motivate the development of applications. This paper argues that autonomously "Semantifying Wikipedia" is the best way to solve the problem. We choose Wikipedia as an initial data source, because it is comprehensive, not too large, high-quality, and contains enough manually-derived structure to bootstrap an autonomous, self-supervised process. We identify several types of structures which can be automatically enhanced in Wikipedia (e.g., link structure, taxonomic data, infoboxes, etc.), and we describea prototype implementation of a self-supervised, machine learning system which realizes our vision. Preliminary experiments demonstrate the high precision of our system's extracted data - in one case equaling that of humans.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {41–50},
numpages = {10},
keywords = {semantic web, wikipedia, information extraction},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321450,
author = {Fanizzi, Nicola and d'Amato, Claudia and Esposito, Floriana},
title = {Randomized Metric Induction and Evolutionary Conceptual Clustering for Semantic Knowledge Bases},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321450},
doi = {10.1145/1321440.1321450},
abstract = {We present an evolutionary clustering method which can be applied to multi-relational knowledge bases storing semantic resource annotations expressed in the standard languages for the Semantic Web. The method exploits an effective and language-independent semi-distance measure defined for the space of individual resources, that is based on a finite number of dimensions corresponding to a committee of features represented by a group of concept descriptions (discriminating features). We show how to obtain a maximally discriminating group of features through a feature construction method based on genetic programming. The algorithm represents the possible clusterings as strings of central elements (medoids, w.r.t. the given metric) of variable length. Hence, the number of clusters is not needed as a parameter since the method can optimize it by means of the mutation operators and of a proper fitness function. We also show how to assign each cluster with a newly constructed intensional definition in the employed concept language. An experimentation with some ontologies proves the feasibility of our method and its effectiveness in terms of clustering validity indices.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {51–60},
numpages = {10},
keywords = {conceptual clustering, description logics, unsupervised learning, metric learning, randomized optimization, genetic programming, evolutionary algorithms},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321451,
author = {Doran, Paul and Tamma, Valentina and Iannone, Luigi},
title = {Ontology Module Extraction for Ontology Reuse: An Ontology Engineering Perspective},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321451},
doi = {10.1145/1321440.1321451},
abstract = {Problems resulting from the management of shared, distributed knowledge has led to ontologies being employed as a solution, in order to effectively integrate information across applications. This is dependent on having ways to share and reuse existing ontologies; with the increased availability of ontologies on the web, some of which include thousands of concepts, novel and more efficient methods for reuse are being devised. One possible way to achieve efficient ontology reuse is through the process of ontology module extraction. A novel approach to ontology module extraction is presented that aims to achieve more efficient reuse of very large ontologies; the motivation is drawn from an Ontology Engineering perspective. This paper provides a definition of ontology modules from the reuse perspective and an approach to module extraction based on such a definition. An abstract graph model for module extraction has been defined, along with a module extraction algorithm. The novel contribution of this paper is a module extraction algorithm that is independent of the language in which the ontology is expressed. This has been implemented in ModTool; a tool that produces ontology modules via extraction. Experiments were conducted to compare ModTool to other modularisation methods.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {61–70},
numpages = {10},
keywords = {ontology module extraction, ontology engineering},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250788,
author = {Milic-Frayling, Natasa},
title = {Session Details: Natural Language I (IR)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250788},
doi = {10.1145/3250788},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321453,
author = {Roussinov, Dmitri and Turetken, Ozgur},
title = {Semantic Verification in an Online Fact Seeking Environment},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321453},
doi = {10.1145/1321440.1321453},
abstract = {Many artificial intelligence tasks, such as automated question answering, reasoning or heterogeneous database integration, involve verification of a semantic category (e.g. "coffee" is a drink, "red" is a color, while "steak" is not a drink and "big" is not a color). We present a novel algorithm to automatically validate a semantic category. Contrary to the methods suggested earlier, our approach does not rely on any manually codified knowledge but instead capitalizes on the diversity of topics and word usage on the World Wide Web. We have tested our approach within our online fact-seeking (question answering) environment. When tested on the TREC questions that expect the answer to belong to a specific semantic category, our approach has improved the accuracy by up to 14% depending on the model and metrics used.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {71–78},
numpages = {8},
keywords = {question answering, artificial intelligence, online search engines},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321454,
author = {Ouyang, You and Li, Sujian and Li, Wenjie},
title = {Developing Learning Strategies for Topic-Based Summarization},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321454},
doi = {10.1145/1321440.1321454},
abstract = {Most up-to-date well-behaved topic-based summarization systems are built upon the extractive framework. They score the sentences based on the associated features by manually assigning or experimentally tuning the weights of the features. In this paper, we discuss how to develop learning strategies in order to obtain the optimal feature weights automatically, which can be used for assigning a sound score to a sentence characterized with a set of features. The two fundamental issues are about training data and learning models. To save the costly manual annotation time and effort, we construct the training data by labeling the sentence with a "true" score calculated according to human summaries. The Support Vector Regression (SVR) model is then used to learn how to relate the "true" score of the sentence to its features. Once the relations have been mathematically modeled, SVR is able to predict the "estimated" score for any given sentence. The evaluations by ROUGE-2 criterion on DUC 2006 and DUC 2005 document sets demonstrate the competitiveness and the adaptability of the proposed approaches.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {79–86},
numpages = {8},
keywords = {support vector regression, document summarization},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321455,
author = {Pa\c{s}ca, Marius},
title = {Lightweight Web-Based Fact Repositories for Textual Question Answering},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321455},
doi = {10.1145/1321440.1321455},
abstract = {Since answers to fact-seeking questions usually reside within small factual text nuggets, often "hidden" within full-length documents, their relevance to a question is not necessarily correlated to the relevance of the full-length document to the question. Yet previous approaches to open-domain textual question answering from large document collections quasi-unanimously employ a document retrieval stage, in order to apply widely different, often expensive answer mining techniques to only a small subset of documents. Depending on the collection size, 95% or more of the documents in the collection (much more in the case of the Web) are left out of the selected subset for any given query, and thus become invisible to subsequent processing stages for actual answer mining. This paper introduces a new model for answer retrieval for question answering. The collection is distilled offline into large repositories of facts. Each fact constitutes a potential direct answer to questions seeking a particular kind of entity or relation, such as questions asking about the date of particular events. Question answering becomes equivalent to online fact retrieval, which greatly simplifies the de-facto system architecture for fact-seeking question answering. In addition to simplicity, experiments on a fact repository acquired from approximately a billion Web documents illustrate the impact of fact repositories in extracting accurate answers to a standard evaluation set of open-domain test questions and additional sets of domain-specific questions.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {87–96},
numpages = {10},
keywords = {lightweight text analysis, web information retrieval, fact extraction, question answering},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250789,
author = {Tinoco, Lucio},
title = {Session Details: Enterprise Information Management (IND)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250789},
doi = {10.1145/3250789},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321457,
author = {Lempel, Ronny and Mass, Yosi and Ofek-Koifman, Shila and Sheinwald, Dafna and Petruschka, Yael and Sivan, Ron},
title = {Just in Time Indexing for up to the Second Search},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321457},
doi = {10.1145/1321440.1321457},
abstract = {E-commerce and intranet search systems require newly arriving content to be indexed and made available for search within minutes or hours of arrival. Applications such as file system and email search demand even faster turnaround from search systems, requiring new content to become available for search almost instantaneously. However, incrementally updating inverted indices, which are the predominant datastructure used in search engines, is an expensive operation that most systems avoid performing at high rates.We present JiTI, a Just-in-Time Indexing component that allows searching over incoming content (nearly) as soon as that content reaches the system. JiTI's main idea is to invest less in the preprocessing of arriving data, at the expense of a tolerable latency in query response time. It is designed for deployment in search systems that maintain a large main index and that rebuild smaller stop-press indices once or twice an hour. JiTI augments such systems with instant retrieval capabilities over content arriving in between the stop-press builds. A main design point is for JiTI to demand few computational resources, in particular RAM and I/O.Our experiments consisted of injecting several documents and queries per second concurrently into the system over half-hour long periods. We believe that there are search applications for which the combination of the workloads we experimented with and the response times we measured present a viable solution to a pressing problem.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {97–106},
numpages = {10},
keywords = {incremental indexing, search engines, inverted indices},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321458,
author = {Drumm, Christian and Schmitt, Matthias and Do, Hong-Hai and Rahm, Erhard},
title = {Quickmig: Automatic Schema Matching for Data Migration Projects},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321458},
doi = {10.1145/1321440.1321458},
abstract = {A common task in many database applications is the migration of legacy data from multiple sources into a new one. This requires identifying semantically related elements of the source and target systems and the creation of mapping expressions to transform instances of those elements from the source format to the target format. Currently, data migration is typically done manually, a tedious and timeconsuming process, which is difficult to scale to a high number of data sources. In this paper, we describe QuickMig, a new semi-automatic approach to determining semantic correspondences between schema elements for data migration applications. QuickMig advances the state of the art with a set of new techniques exploiting sample instances, domain ontologies, and reuse of existing mappings to detect not only element correspondences but also their mapping expressions. QuickMig further includes new mechanisms to effectively incorporate domain knowledge of users into the matching process. The results from a comprehensive evaluation using real-world schemas and data indicate the high quality and practicability of the overall approach.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {107–116},
numpages = {10},
keywords = {schema matching, schema mapping, mapping discovery, data transformation, data migration},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321459,
author = {Park, Youngja},
title = {Automatic Call Section Segmentation for Contact-Center Calls},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321459},
doi = {10.1145/1321440.1321459},
abstract = {This paper presents a SVM (Support Vector Machine) classification system which divides contact-center call transcripts into "Greeting", "Question", "Refine", "Research", "Resolution", "Closing" and "Out-of-topic" sections. This call section segmentation is useful to improve search and retrieval functions and to provide more detailed statistics on calls. We use an off-the-shelf automatic speech recognition (ASR) system to generate call transcripts from recorded calls between customers and service representatives.We first classify an individual utterance into a call section by applying the SVM classifier and then merge adjacent utterances classified into a same call section. We experiment with the proposed system on 100 automatically transcribed calls. The 10-fold cross validation shows 87.2% classification accuracy. We also compare the proposed algorithm with two other approaches - the most frequent section only method and a maximum entropy-based segmentation. The evaluation shows that our system's accuracy is 12% higher than the first baseline system and 6% higher than the second baseline system respectively.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {117–126},
numpages = {10},
keywords = {classification, call section segmentation, natural language processing, contact center analytics, text mining, speech analytics},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250790,
author = {Rodrigues, Eduarda Mendes},
title = {Session Details: Classification and Clustering I (KM)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250790},
doi = {10.1145/3250790},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321461,
author = {Ertekin, Seyda and Huang, Jian and Bottou, Leon and Giles, Lee},
title = {Learning on the Border: Active Learning in Imbalanced Data Classification},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321461},
doi = {10.1145/1321440.1321461},
abstract = {This paper is concerned with the class imbalance problem which has been known to hinder the learning performance of classification algorithms. The problem occurs when there are significantly less number of observations of the target concept. Various real-world classification tasks, such as medical diagnosis, text categorization and fraud detection suffer from this phenomenon. The standard machine learning algorithms yield better prediction performance with balanced datasets. In this paper, we demonstrate that active learning is capable of solving the class imbalance problem by providing the learner more balanced classes. We also propose an efficient way of selecting informative instances from a smaller pool of samples for active learning which does not necessitate a search through the entire dataset. The proposed method yields an efficient querying system and allows active learning to be applied to very large datasets. Our experimental results show that with an early stopping criteria, active learning achieves a fast solution with competitive prediction performance in imbalanced data classification.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {127–136},
numpages = {10},
keywords = {imbalanced data, support vector machines, active learning},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321462,
author = {Wang, Jun and Lee, Meng Chen},
title = {Reconstructing Ddc for Interactive Classification},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321462},
doi = {10.1145/1321440.1321462},
abstract = {The automated text categorization (TC) has made prominent progress in recent years. However, seldom work is done on automatic classification with library classification systems, the largest and most sophisticated classification systems people ever built, such as the Dewey Decimal classification (DDC). The library classification is a very laborious and time-consuming job that requires qualification and good training.The large-scale classification schemes, such as the DDC, impose several obstacles to the state-of-art TC technologies, including very deep hierarchy, data sparseness, and skewed category distribution. These problems characterize large corpora of real-world applications and it is very hard, if not impossible, to obtain satisfactory results. In this paper, we propose a novel algorithm to reconstruct classification schemes according to the document density and category distribution, and to transform the category hierarchy into a balanced virtual taxonomy by merging sparse categories, lopping dense branches and flattening the hierarchy. To make the classification performance acceptable to real-world applications, we also propose an interactive classification model that only needs two or three times of user interaction. Extensive experiments are conducted on a 10-year bibliographic data collection of the Library of Congress to verify the proposed methodology.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {137–146},
numpages = {10},
keywords = {taxonomy reconstruction, dewey decimal classification, hierarchical classification, bibliographic data, trimming machine, interactive classification},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321463,
author = {Li, Tao and Anand, Sarabjot Singh},
title = {Diva: A Variance-Based Clustering Approach for Multi-Type Relational Data},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321463},
doi = {10.1145/1321440.1321463},
abstract = {Clustering is a common technique used to extract knowledge from a dataset in unsupervised learning. In contrast to classical propositional approaches that only focus on simple and flat datasets, relational clustering can handle multi-type interrelated data objects directly and adopt semantic information hidden in the linkage structure to improve the clustering result. However, exploring linkage information will greatly reduce the scalability of relational clustering. Moreover, some characteristics of vector data space utilized to accelerate the propositional clustering procedure are no longer valid in relational data space. These two disadvantages restrain the relational clustering techniques from being applied to very large datasets or in time-critical tasks, such as online recommender systems. In this paper we propose a new variance-based clustering algorithm to address the above difficulties. Our algorithm combines the advantages of divisive and agglomerative clustering paradigms to improve the quality of cluster results. By adopting the idea of Representative Object, it can be executed with linear time complexity. Experimental results show our algorithm achieves high accuracy, efficiency and robustness in comparison with some well-known relational clustering approaches.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {147–156},
numpages = {10},
keywords = {relational, multi-type, clustering},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250791,
author = {Giles, Lee},
title = {Session Details: Web Retrieval I (IR)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250791},
doi = {10.1145/3250791},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321465,
author = {Najork, Marc A.},
title = {Comparing the Effectiveness of Hits and Salsa},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321465},
doi = {10.1145/1321440.1321465},
abstract = {This paper compares the effectiveness of two well-known query-dependent link-based ranking algorithms, "Hyperlink-Induced Topic Search" (HITS) and the "Stochastic Approach for Link-Structure Analysis" (SALSA). The two algorithms are evaluated on a very large web graph induced by 463 million crawled web pages and a set of 28,043 queries and 485,656 results labeled by human judges. We employed three different performance measures - mean average precision (MAP), mean reciprocal rank (MRR), and normalized discounted cumulative gain (NDCG). We found that as an isolated feature, SALSA substantially outperforms HITS. This is quite surprising, given that the two algorithms operate over the same neighborhood graph induced by the query result set. We also studied the combination of SALSA and HITS with BM25F, a state-of-the-art text-based scoring function that incorporates anchor text. We found that the combination of SALSA and BM25F outperforms the combination of HITS and BM25F. Finally, we broke down our query set by query specificity, and found that SALSA (and to a lesser extent HITS) is most effective for general queries.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {157–164},
numpages = {8},
keywords = {hits, salsa, link-based ranking, web search, retrieval performance},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321466,
author = {Fernandes, David and de Moura, Edleno S. and Ribeiro-Neto, Berthier and da Silva, Altigran S. and Gon\c{c}alves, Marcos Andr\'{e}},
title = {Computing Block Importance for Searching on Web Sites},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321466},
doi = {10.1145/1321440.1321466},
abstract = {In this paper we consider the problem of using the block structure of a Web page to improve ranking results when searching for information on Web sites. Given the block structure of the Web pages as input, we propose a method for computing the importance of each block (in the form of block weights) in a Web collection. As we show through experiments, the deployment of our method may allow a significant improvement in the quality of search results. We ran experiments to compare the quality of search results when using our method to the quality obtained when using no structure information. When compared to a ranking method that considered pages as monolithic units, our block-based ranking method led to improvements in the quality of search results in experiments with two sites with heterogeneous structures. Further, our method does not increase the cost of processing queries when compared to the systems using no structural information.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {165–174},
numpages = {10},
keywords = {block class, block importance, page segmentation},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321467,
author = {Piwowarski, Benjamin and Zaragoza, Hugo},
title = {Predictive User Click Models Based on Click-through History},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321467},
doi = {10.1145/1321440.1321467},
abstract = {Web search engines consistently collect information about users interaction with the system: they record the query they issued, the URL of presented and selected documents along with their ranking. This information is very valuable: It is a poll over millions of users on the most various topics and it has been used in many ways to mine users interests and preferences. Query logs have the potential to partially alleviate the search engines from thousand of searches by providing a way to predict answers for a subset of queries and users without knowing the content of a document. Even if the predicted result is at rank one, this analysis might be of interest: If there is enough confidence on a user's click, we might redirect the user directly to the page whose link would be clicked. In this paper, we present three different models for predicting user clicks, ranging from most specific ones (using only past user history for the query) to very general ones (aggregating data over all users for a given query). The former model has a very high precision at low recall values, while the latter can achieve high recalls. We show that it is possible to combine the different models to predict with high accuracy (over 90%) a high subset of query sessions (24% of all the sessions).},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {175–182},
numpages = {8},
keywords = {web retrieval, re-finding, query log analysis, user modelling, repeat queries},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250792,
author = {Jomier, Genevi\`{e}ve},
title = {Session Details: Spatio-Temporal Databases and Time Series Streams (DB)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250792},
doi = {10.1145/3250792},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321469,
author = {Praing, Reasey and Schneider, Markus},
title = {Modeling Historical and Future Movements of Spatio-Temporal Objects in Moving Objects Databases},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321469},
doi = {10.1145/1321440.1321469},
abstract = {Spatio-temporal databases deal with geometries changing over time. In general, geometries do not only change discretely but continuously; hence we are dealing with moving objects. In the past, a few moving object data models and query languages have been proposed. Each of them supports either historical movements or future movements but not both together. Consequently, queries that start in the past and extend into the future cannot be supported. To model both historical and future movements of an object, two separate concepts with different properties are required, and extra attention is necessary to avoid their conflicts. Furthermore, current definitions of moving objects are too general and vague. It is unclear how a moving object is allowed to move through space and time. For instance, the continuity or discontinuity of motion is not specified. In this paper, we propose a new moving object data model called Balloon model which provides integrated support for both historical and future movements of moving objects. As part of the model, we provide formal definitions of moving objects with respect to their past and future movements. All kinds of queries including past queries, future queries, and queries that start in the past and end in the future are supported in our model.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {183–192},
numpages = {10},
keywords = {gis, continuity of evolutions, historical movements, predictive movements, moving objects databases, spatio-temporal data model, balloon model},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321470,
author = {Combi, Carlo and Montanari, Angelo and Pozzi, Giuseppe},
title = {The T4sql Temporal Query Language},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321470},
doi = {10.1145/1321440.1321470},
abstract = {Time characterizes every aspect of our life and its management when storing and querying data is very important. In this paper we propose a new temporal query language, called T4SQL, supporting multiple temporal dimensions of data. Besides the well-known valid and transaction times, it encompasses two additional temporal dimensions, namely, availability and event times. The availability time records when information is known and treated as true by the information system; the event times record the occurrence times of both the event that starts the valid time and the event that ends it. T4SQL is capable to deal with different temporal semantics (atemporal aka non-sequenced, current, sequenced, next) with respect to every temporal dimension. Moreover, T4SQL provides a novel temporal grouping clause and an orthogonal management of temporal properties when defining the selection condition(s) and the schema for the output relation.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {193–202},
numpages = {10},
keywords = {temporal query language, sql},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321471,
author = {Zhang, Tiancheng and Yue, Dejun and Gu, Yu and Yu, Ge},
title = {Boolean Representation Based Data-Adaptive Correlation Analysis over Time Series Streams},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321471},
doi = {10.1145/1321440.1321471},
abstract = {Correlation analysis is a basic problem in the field of data stream mining. Typical approaches add sliding window to data streams to get the recent results, but the window length defined by users is always fixed which is not suitable for the changing stream environment. We propose a Boolean representation based data-adaptive method for correlation analysis among a large number of time series streams. The periodical trends of each stream series to are monitored to choose the most suitable window size and group the series with the same trends together. Instead of adopting complex pair-wise calculation, we can also quickly get the correlation pairs of series at the optimal window sizes. All the processing is realized by simple Boolean operations. Both the theory analysis and the experimental evaluations show that our method has good computation efficiency with high accuracy.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {203–212},
numpages = {10},
keywords = {boolean representation, time series streams, correlation analysis, data-adaptive},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250793,
author = {Falc\~{a}o, Andr\'{e} O.},
title = {Session Details: Explanation, Knowledge Provenance and Synthesis (KM)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250793},
doi = {10.1145/3250793},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321473,
author = {Don, Anthony and Zheleva, Elena and Gregory, Machon and Tarkan, Sureyya and Auvil, Loretta and Clement, Tanya and Shneiderman, Ben and Plaisant, Catherine},
title = {Discovering Interesting Usage Patterns in Text Collections: Integrating Text Mining with Visualization},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321473},
doi = {10.1145/1321440.1321473},
abstract = {This paper addresses the problem of making text mining results more comprehensible to humanities scholars, journalists, intelligence analysts, and other researchers, in order to support the analysis of text collections. Our system, FeatureLens1, visualizes a text collection at several levels of granularity and enables users to explore interesting text patterns. The current implementation focuses on frequent itemsets of n-grams, as they capture the repetition of exact or similar expressions in the collection. Users can find meaningful co-occurrences of text patterns by visualizing them within and across documents in the collection. This also permits users to identify the temporal evolution of usage such as increasing, decreasing or sudden appearance of text patterns. The interface could be used to explore other text features as well. Initial studies suggest that FeatureLens helped a literary scholar and 8 users generate new hypotheses and interesting insights using 2 text collections.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {213–222},
numpages = {10},
keywords = {frequent closed itemsets, n-grams, digital humanities, text mining, user interface},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321474,
author = {Yu, Jonathan and Thom, James A. and Tam, Audrey},
title = {Ontology Evaluation Using Wikipedia Categories for Browsing},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321474},
doi = {10.1145/1321440.1321474},
abstract = {Ontology evaluation is a maturing discipline with methodologies and measures being developed and proposed. However, evaluation methods that have been proposed have not been applied to specific examples. In this paper, we present the state-of-the-art in ontology evaluation - current methodologies, criteria and measures, analyse appropriate evaluations that are important to our application - browsing in Wikipedia, and apply these evaluations in the context of ontologies with varied properties. Specifically, we seek to evaluate ontologies based on categories found in Wikipedia.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {223–232},
numpages = {10},
keywords = {ontology evaluation, user studies, wikipedia, browsing},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321475,
author = {Mihalcea, Rada and Csomai, Andras},
title = {Wikify! Linking Documents to Encyclopedic Knowledge},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321475},
doi = {10.1145/1321440.1321475},
abstract = {This paper introduces the use of Wikipedia as a resource for automatic keyword extraction and word sense disambiguation, and shows how this online encyclopedia can be used to achieve state-of-the-art results on both these tasks. The paper also shows how the two methods can be combined into a system able to automatically enrich a text with links to encyclopedic knowledge. Given an input document, the system identifies the important concepts in the text and automatically links these concepts to the corresponding Wikipedia pages. Evaluations of the system show that the automatic annotations are reliable and hardly distinguishable from manual annotations.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {233–242},
numpages = {10},
keywords = {keyword extraction, wikipedia, word sense disambiguation, semantic annotation},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321476,
author = {Hu, Meiqun and Lim, Ee-Peng and Sun, Aixin and Lauw, Hady Wirawan and Vuong, Ba-Quy},
title = {Measuring Article Quality in Wikipedia: Models and Evaluation},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321476},
doi = {10.1145/1321440.1321476},
abstract = {Wikipedia has grown to be the world largest and busiest free encyclopedia, in which articles are collaboratively written and maintained by volunteers online. Despite its success as a means of knowledge sharing and collaboration, the public has never stopped criticizing the quality of Wikipedia articles edited by non-experts and inexperienced contributors. In this paper, we investigate the problem of assessing the quality of articles in collaborative authoring of Wikipedia. We propose three article quality measurement models that make use of the interaction data between articles and their contributors derived from the article edit history. Our B<scp>asic</scp> model is designed based on the mutual dependency between article quality and their author authority. The P<scp>eer</scp>R<scp>eview</scp> model introduces the review behavior into measuring article quality. Finally, our P<scp>rob</scp>R<scp>eview</scp> models extend P<scp>eer</scp>R<scp>eview</scp> with partial reviewership of contributors as they edit various portions of the articles. We conduct experiments on a set of well-labeled Wikipedia articles to evaluate the effectiveness of our quality measurement models in resembling human judgement.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {243–252},
numpages = {10},
keywords = {article quality, collaborative authoring, wikipedia, authority, peer review},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250794,
author = {Lalmas, Mounia},
title = {Session Details: IR Modeling (IR)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250794},
doi = {10.1145/3250794},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321478,
author = {Metzler, Donald A.},
title = {Automatic Feature Selection in the Markov Random Field Model for Information Retrieval},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321478},
doi = {10.1145/1321440.1321478},
abstract = {Previous applications of the Markov random field model for information retrieval have used manually chosen features. However, it is often difficult or impossible to know, a priori, the best set of features to use for a given task or data set. Therefore, there is a need to develop automatic feature selection techniques. In this paper we describe a greedy procedure for automatically selecting features to use within the Markov random field model for information retrieval. We also propose a novel, robust method for describing classes of textual information retrieval features. Experimental results, evaluated on standard TREC test collections, show that our feature selection algorithm produces models that are either significantly more effective than, or equally effective as, models with manually selected features, such as those used in the past.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {253–262},
numpages = {10},
keywords = {feature selection, parameter estimation, markov random field model},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321479,
author = {He, Ben and Ounis, Iadh},
title = {Parameter Sensitivity in the Probabilistic Model for Ad-Hoc Retrieval},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321479},
doi = {10.1145/1321440.1321479},
abstract = {The term frequency normalisation parameter sensitivity is an important issue in the probabilistic model for Information Retrieval. A high parameter sensitivity indicates that a slight change of the parameter value may considerably affect the retrieval performance. Therefore, a weighting model with a high parameter sensitivity is not robust enough to provide a consistent retrieval performance across different collections and queries. In this paper, we suggest that the parameter sensitivity is due to the fact that the query term weights are not adequate enough to allow informative query terms to differ from non-informative ones. We show that query term reweighing, which is part of the relevance feedback process, can be successfully used to reduce the parameter sensitivity. Experiments on five Text REtrieval Conference (TREC) collections show that the parameter sensitivity does remarkably decrease when query terms are reweighed.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {263–272},
numpages = {10},
keywords = {query term reweighing, parameter sensitivity, relevance feedback},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321480,
author = {Melucci, Massimo and White, Ryen W.},
title = {Utilizing a Geometry of Context for Enhanced Implicit Feedback},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321480},
doi = {10.1145/1321440.1321480},
abstract = {Implicit feedback algorithms utilize interaction between searchers and search systems to learn more about users' needs and interests than expressed in query statements alone. This additional information can be used to formulate improved queries or directly improve retrieval performance. In this paper we present a geometric framework that utilizes multiple sources of evidence present in this interaction context (e.g., display time, document retention) to develop enhanced implicit feedback models personalized for each user and tailored for each search task. We use rich interaction logs (and associated metadata such as relevance judgments), gathered during a longitudinal user study, as relevance stimuli to compare an implicit feedback algorithm developed using the framework with alternative algorithms. Our findings demonstrate both the effectiveness of our proposed algorithm and the potential value of incorporating multiple sources of interaction evidence when developing implicit feedback algorithms.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {273–282},
numpages = {10},
keywords = {implicit relevance feedback, geometry of information retrieval},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250795,
author = {Gon\c{c}alves, Marcos A.},
title = {Session Details: Record Linkage and Approximate Matching (DB)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250795},
doi = {10.1145/3250795},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321482,
author = {Kim, Hung-sik and Lee, Dongwon},
title = {Parallel Linkage},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321482},
doi = {10.1145/1321440.1321482},
abstract = {We study the parallelization of the (record) linkage problem - i.e., to identify matching records between two collections of records, A and B. One of main idiosyncrasies of the linkage problem, compared to Database join, is the fact that once two records a in A and b in B are matched and merged to c, c needs to be compared to the rest of records in A and B again since it may incur new matching. This re-feeding stage of the linkage problem requires its solution to be iterative, and complicates the problem significantly. Toward this problem, we first discuss three plausible scenarios of inputs - when both collections are clean, only one is clean, and both are dirty. Then, we show that the intricate interplay between match and merge can exploit the characteristics of each scenario to achieve good parallelization. Our parallel algorithms achieve 6.55-7.49 times faster in speedup compared to sequential ones with 8 processors, and 11.15-18.56% improvement in efficiency compared to P-Swoosh.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {283–292},
numpages = {10},
keywords = {record linkage, parallel linkage},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321483,
author = {Leit\~{a}o, Lu\'{\i}s and Calado, P\'{a}vel and Weis, Melanie},
title = {Structure-Based Inference of Xml Similarity for Fuzzy Duplicate Detection},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321483},
doi = {10.1145/1321440.1321483},
abstract = {Fuzzy duplicate detection aims at identifying multiple representations of real-world objects stored in a data source, and is a task of critical practical relevance in data cleaning, data mining, or data integration. It has a long history for relational data stored in a single table (or in multiple tables with equal schema). Algorithms for fuzzy duplicate detection in more complex structures, e.g., hierarchies of a data warehouse, XML data, or graph data have only recently emerged. These algorithms use similarity measures that consider the duplicate status of their direct neighbors, e.g., children in hierarchical data, to improve duplicate detection effectiveness. In this paper, we propose a novel method for fuzzy duplicate detection in hierarchical and semi-structured XML data. Unlike previous approaches, it not only considers the duplicate status of children, but rather the probability of descendants being duplicates. Probabilities are computed efficiently using a Bayesian network. Experiments show the proposed algorithm is able to maintain high precision and recall values, even when dealing with data containing a high amount of errors and missing information. Our proposal is also able to outperform a state-of-the-art duplicate detection system on three different XML databases.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {293–302},
numpages = {10},
keywords = {duplicate detection, bayesian networks, xml},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321484,
author = {Dorneles, Carina F. and Heuser, Carlos A. and Orengo, Viviane Moreira and da Silva, Altigran S. and de Moura, Edleno S.},
title = {A Strategy for Allowing Meaningful and Comparable Scores in Approximate Matching},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321484},
doi = {10.1145/1321440.1321484},
abstract = {The goal of approximate data matching is to assess whether two distinct data instances represent the same real world object. This is usually achieved through the use of a similarity function, which returns a score that defines how similar two data instances are. If this score surpasses a given threshold, both data instances are considered as representing the same real world object. The score values returned by a similarity function depend on the algorithm that implements the function and have no meaning to the user (apart from the fact that a higher similarity value means that two data instances are more similar). In this paper, we propose that instead of defining the threshold in terms of the scores returned by a similarity function, the user specifies the precision that is expected from the matching process. Precision is a well known quality measure and has a clear interpretation from the user's point of view. Our approach relies on mapping between similarity scores and precision values based on a training data set. Experimental results show the training may be executed against a representative data set, and reused for other databases from the same domain.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {303–312},
numpages = {10},
keywords = {data cleaning, data integration, similarity querying},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250796,
author = {Najork, Marc},
title = {Session Details: Miscellaneous (IR)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250796},
doi = {10.1145/3250796},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321486,
author = {Cormack, Gordon V. and G\'{o}mez Hidalgo, Jos\'{e} Mar\'{\i}a and S\'{a}nz, Enrique Puertas},
title = {Spam Filtering for Short Messages},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321486},
doi = {10.1145/1321440.1321486},
abstract = {We consider the problem of content-based spam filtering for short text messages that arise in three contexts: mobile (SMS) communication, blog comments, and email summary information such as might be displayed by a low-bandwidth client. Short messages often consist of only a few words, and therefore present a challenge to traditional bag-of-words based spam filters. Using three corpora of short messages and message fields derived from real SMS, blog, and spam messages, we evaluate feature-based and compression-model-based spam filters. We observe that bag-of-words filters can be improved substantially using different features, while compression-model filters perform quite well as-is. We conclude that content filtering for short messages is surprisingly effective.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {313–320},
numpages = {8},
keywords = {email, filtering, sms, spam, blog, classification},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321487,
author = {Paltoglou, Georgios and Salampasis, Michail and Satratzemi, Maria},
title = {Hybrid Results Merging},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321487},
doi = {10.1145/1321440.1321487},
abstract = {The problem of results merging in distributed information retrieval environments has been approached by two different directions in research. Estimation approaches attempt to calculate the relevance of the returned documents through ad-hoc methodologies (weighted score merging, regression etc) while download approaches, download all the documents locally, partially or completely, in order to estimate "first hand" their relevance. Both have their advantages and disadvantages. It is assumed that download algorithms are more effective but they are very expensive in terms of time and bandwidth. Estimation approaches on the other hand, usually rely on document relevance scores being returned by the remote collections in order to achieve maximum performance. In addition to that, regression algorithms, which have proved to be more effective than weighted scores merging, rely on a significant number of overlap documents in order to function effectively, practically requiring multiple interactions with the remote collections. The new algorithm that is introduced reconciles the above two approaches, combining their strengths, while minimizing their weaknesses. It is based on downloading a limited, selected number of documents from the remote collections and estimating the relevance of the rest through regression methodologies. The proposed algorithm is tested in a variety of settings and its performance is found to be better than estimation approaches, while approximating that of download.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {321–330},
numpages = {10},
keywords = {distributed information retrieval, results merging},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321488,
author = {Anagnostopoulos, Aris and Broder, Andrei Z. and Gabrilovich, Evgeniy and Josifovski, Vanja and Riedel, Lance},
title = {Just-in-Time Contextual Advertising},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321488},
doi = {10.1145/1321440.1321488},
abstract = {Contextual Advertising is a type of Web advertising, which, given the URL of a Web page, aims to embed into the page (typically via JavaScript) the most relevant textual ads available. For static pages that are displayed repeatedly, the matching of ads can be based on prior analysis of their entire content; however, ads need to be matched also to new or dynamically created pages that cannot be processed ahead of time. Analyzing the entire body of such pages on-the-fly entails prohibitive communication and latency costs. To solve the three-horned dilemma of either low-relevance or high-latency or high-load, we propose to use text summarization techniques paired with external knowledge (exogenous to the page) to craft short page summaries in real time. Empirical evaluation proves that matching ads on the basis of such summaries does not sacrifice relevance, and is competitive with matching based on the entire page content. Specifically, we found that analyzing a carefully selected 5% fraction of the page text sacrifices only 1%-3% in ad relevance. Furthermore, our summaries are fully compatible with the standard JavaScript mechanisms used for ad placement: they can be produced at ad-display time by simple additions to the usual script, and they only add 500-600 bytes to the usual request.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {331–340},
numpages = {10},
keywords = {text summarization, text classification},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250797,
author = {Nicholas, Charles},
title = {Session Details: Query Expansion (IR)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250797},
doi = {10.1145/3250797},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321490,
author = {Macdonald, Craig and Ounis, Iadh},
title = {Expertise Drift and Query Expansion in Expert Search},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321490},
doi = {10.1145/1321440.1321490},
abstract = {Pseudo-relevance feedback, or query expansion, has been shown to improve retrieval performance in the adhoc retrieval task. In such a scenario, a few top-ranked documents are assumed to be relevant, and these are then used to expand and refine the initial user query, such that it retrieves a higher quality ranking of documents. However, there has been little work in applying query expansion in the expert search task. In this setting, query expansion is applied by assuming a few top-ranked candidates have relevant expertise, and using these to expand the query. Nevertheless, retrieval is not improved as expected using such an approach. We show that the success of the application of query expansion is hindered by the presence of topic drift within the profiles of experts that the system considers. In this work, we demonstrate how topic drift occurs in the expert profiles, and moreover, we propose three measures to predict the amount of drift occurring in an expert's profile. Finally, we suggest and evaluate ways of enhancing query expansion in expert search using our new insights. Our results show that, once topic drift has been anticipated, query expansion can be successfully applied in a general manner in the expert search task.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {341–350},
numpages = {10},
keywords = {expertise modelling, expert search information retrieval, expert finding, topic drift, query expansion},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321491,
author = {Cao, Guihong and Gao, Jianfeng and Nie, Jian-Yun and Bai, Jing},
title = {Extending Query Translation to Cross-Language Query Expansion with Markov Chain Models},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321491},
doi = {10.1145/1321440.1321491},
abstract = {Dictionary-based approaches to query translation have been widely used in Cross-Language Information Retrieval (CLIR) experiments. However, translation has been not only limited by the coverage of the dictionary, but also affected by translation ambiguities. In this paper we propose a novel method of query translation that combines other types of term relation to complement the dictionary-based translation. This allows extending the literal query translation to related words, which produce a beneficial effect of query expansion in CLIR. In this paper, we model query translation by Markov Chains (MC), where query translation is viewed as a process of expanding query terms to their semantically similar terms in a different language. In MC, terms and their relationships are modeled as a directed graph, and query translation is performed as a random walk in the graph, which propagates probabilities to related terms. This framework allows us to incorporating different types of term relation, either between two languages or within the source or target languages. In addition, the iterative training process of MC allows us to attribute higher probabilities to the target terms more related to the original query, thus offers a solution to the translation ambiguity problem. We evaluated our method on three CLIR benchmark collections, and obtained significant improvements over traditional dictionary-based approaches.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {351–360},
numpages = {10},
keywords = {query translation, markov chain, random walk, cross-language information retrieval, query expansion},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321492,
author = {Yan, Rong and Hauprmann, Alexander},
title = {Query Expansion Using Probabilistic Local Feedback with Application to Multimedia Retrieval},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321492},
doi = {10.1145/1321440.1321492},
abstract = {As one of the most effective query expansion approaches, local feedback is able to automatically discover new query terms and improve retrieval accuracy for different retrieval models. However, the performance of local feedback is heavily dependent on the assumption that most top-ranked documents are relevant to the query topic. Although this assumption might be sensible for ad-hoc text retrieval, it is usually violated in many other retrieval tasks such as multimedia retrieval. In this paper, we develop a robust local analysis approach called probabilistic local feedback (PLF) based on a discriminative probabilistic retrieval framework. The proposed model is effective for improving retrieval accuracy without assuming the most top-ranked documents are relevant. It also provides a sound probabilistic interpretation and a convergence guarantee on the iterative result updating process. Although derived from variational techniques, this approach only involves an iterative process of simple operations on ranking features and thus can be computed efficiently in practice. Our multimedia retrieval experiments on TRECVID'03-'05 collections have demonstrated the advantage of the proposed PLF approaches which can achieve noticeable gains in terms of mean average precision over various baseline methods and PRF-augmented results.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {361–370},
numpages = {10},
keywords = {probabilistic local feedback, multimedia retrieval, query expansion},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250798,
author = {Lee, Dongwon},
title = {Session Details: Query Processing (DB)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250798},
doi = {10.1145/3250798},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321494,
author = {Chan, Edward P. F. and Zhang, Jie},
title = {A Fast Unified Optimal Route Query Evaluation Algorithm},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321494},
doi = {10.1145/1321440.1321494},
abstract = {We investigate the problem of how to evaluate, fast and efficiently, classes of optimal route queries on a massive graph in a unified framework. To evaluate a route query effectively, a large network is partitioned into a collection of fragments, and distances of some optimal routes in the network are pre-computed. Under such a setting, we find a unified algorithm that can evaluate classes of optimal route queries. The classes that can be processed efficiently are called constraint preserving (CP) which include, among others, shortest path, forbidden edges, forbidden nodes and α-autonomy optimal route query classes. We prove the correctness of the unified algorithm. We then turn our attention to the optimization of the proposed algorithm. Several pruning and optimization techniques are derived that minimize the search time and I/O accesses. We show empirically that these techniques are effective. The proposed optimal route query evaluation algorithm, with all these techniques incorporated, is compared with a main-memory and a disk-based brute-force CP algorithms. We show experimentally that the proposed unified algorithm outperforms the brute-force algorithms, both in term of CPU time and I/O cost, by a wide margin.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {371–380},
numpages = {10},
keywords = {route query evaluation, distance materialization, optimal route queries},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321495,
author = {Tamashiro, Manuel and Thomo, Alex and Venkatesh, Srinivasan},
title = {Towards Practically Feasible Answering of Regular Path Queries in Lav Data Integration},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321495},
doi = {10.1145/1321440.1321495},
abstract = {Regular path queries (RPQ's) are given by means of regular expressions and ask for matching patterns on labeled graphs. RPQ's have received great attention in the context of semistructured data, which are data whose structure is irregular, partially known, or subject to frequent changes. One of the most important problems in databases today is the integration of semistructured data from multiple sources modeled as views. The well-know paradigm of computing first a view-based rewriting of the query, and then evaluating the rewriting on the view extensions is indeed possible for RPQ's. However, computing the rewriting is computationally hard as it can only be done (in the worst case) in not less than 2EXPTIME. In this paper, we provide practical evidence that computing the rewriting is hard on the average as well. On the positive side, we propose automata-theoretic techniques, which efficiently compute and utilize instead the complement of the rewriting. Notably using the latter, it is possible to answer a query, and this makes the view-based answering of RPQ's fairly feasible in practice.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {381–390},
numpages = {10},
keywords = {semistructured data, data integration, regular path queries},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321496,
author = {Fu, Tao-Young and Peng, Wen-Chih and Lee, Wang-Chien},
title = {Optimizing Parallel Itineraries for Knn Query Processing in Wireless Sensor Networks},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321496},
doi = {10.1145/1321440.1321496},
abstract = {Spatial queries for extracting data from wireless sensor networks are important for many applications, such as environmental monitoring and military surveillance. One such query is K Nearest Neighbor (KNN) query that facilitates sampling of monitored sensor data in correspondence with a given query location. Recently, itinerary-based KNN query processing techniques, that propagate queries and collect data along a pre-determined itinerary, have been developed concurrently [12] [14]. These research works demonstrate that itinerary-based KNN query processing algorithms are able to achieve better energy efficiency than other existing algorithms. However, how to derive itineraries based on different performance requirements remains a challenging problem. In this paper, we propose a new itinerary-based KNN query processing technique, called PCIKNN, that derives different itineraries aiming at optimizing two performance criteria, response latency and energy consumption. The performance of PCIKNN is analyzed mathematically and evaluated through extensive experiments. Experimental results show that PCIKNN has better performance and scalability than the state-of-the-art.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {391–400},
numpages = {10},
keywords = {k nearest neighbor query, wireless sensor networks},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250799,
author = {Calado, Pavel},
title = {Session Details: Classification and Clustering II (KM)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250799},
doi = {10.1145/3250799},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321498,
author = {Jiang, Jing and Zhai, ChengXiang},
title = {A Two-Stage Approach to Domain Adaptation for Statistical Classifiers},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321498},
doi = {10.1145/1321440.1321498},
abstract = {In this paper, we consider the problem of adapting statistical classifiers trained from some source domains where labeled examples are available to a target domain where no labeled example is available. One characteristic of such a domain adaptation problem is that the examples in the source domains and the target domain are known to follow different distributions. Thus a regular classification method would tend to overfit the source domains. We present a two-stage approach to domain adaptation, where at the first generalization stage, we look for a set of features generalizable across domains, and at the second adaptation stage, we pick up useful features specific to the target domain. Observing that the exact objective function is hard to optimize, we then propose a number of heuristics to approximately achieve the goal of generalization and adaptation. Our experiments on gene name recognition using a real data set show the effectiveness of our general framework and the heuristics.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {401–410},
numpages = {8},
keywords = {semi-supervised learning, classification, logistic regression, feature selection, domain adaptation},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321499,
author = {Rosenfeld, Benjamin and Feldman, Ronen},
title = {Clustering for Unsupervised Relation Identification},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321499},
doi = {10.1145/1321440.1321499},
abstract = {Unsupervised Relation Identification is the task of automatically discovering interesting relations between entities in a large text corpora. Relations are identified by clustering the frequently co-occurring pairs of entities in such a way that pairs occurring in similar contexts end up belonging to the same clusters. In this paper we compare several clustering setups, some of them novel and others already tried. The setups include feature extraction and selection methods and clustering algorithms. In order to do the comparison, we develop a clustering evaluation metric, specifically adapted for the relation identification task. Our experiments demonstrate significant superiority of the single-linkage hierarchical clustering with the novel threshold selection technique over the other tested clustering algorithms. Also, the experiments indicate that for successful relation identification it is important to use rich complex features of two kinds: features that test both relation slots together ("relation features"), and features that test only one slot each ("entity features"). We have found that using both kinds of features with the best of the algorithms produces very high-precision results, significantly improving over the previous work.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {411–418},
numpages = {8},
keywords = {clustering, unsupervised relation identification, relation learning, information extraction},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321500,
author = {Bechchi, Mounir and Raschia, Guillaume and Mouaddib, Noureddine},
title = {Merging Distributed Database Summaries},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321500},
doi = {10.1145/1321440.1321500},
abstract = {The database summarization system coined S<scp>aint</scp>E<scp>ti</scp>Q provides multi-resolution summaries of structured data stored into acentralized database. Summaries are computed online with a conceptual hierarchical clustering algorithm. However, most companies work in distributed legacy environments and consequently the current centralized version of S<scp>aint</scp>E<scp>ti</scp>Q is either not feasible (privacy preserving) or not desirable (resource limitations).To address this problem, we propose new algorithms to generate a single summary hierarchy given two distinct hierarchies, without scanning the raw data. The Greedy Merging Algorithm (GMA) takes all leaves of both hierarchies and generates the optimal partitioning for the considered data set with regards to a cost function (compactness and separation). Then, a hierarchical organization of summaries is built by agglomerating or dividing clusters such that the cost function may emphasize local or global patterns in the data. Thus, we obtain two different hierarchies according to the performed optimisation. However, this approach breaks down due to its exponential time complexity.Two alternative approaches with constant time complexity w.r.t. the number of data items, are proposed to tackle this problem. The first one, called Merge by Incorporation Algorithm (MIA), relies on the S<scp>aint</scp>E<scp>ti</scp>Q engine whereas the second approach, named Merge by Alignment Algorithm (MAA), consists in rearranging summaries by levels in a top-down manner.Then, we compare those approaches using an original quality measure in order to quantify how good our merged hierarchies are. Finally, an experimental study, using real data sets, shows that merging processes (MIA and MAA) are efficient in terms of computational time.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {419–428},
numpages = {10},
keywords = {tree alignment, database summary, distributed clustering},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250800,
author = {Melucci, Massimo},
title = {Session Details: Semantic IR (IR)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250800},
doi = {10.1145/3250800},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321502,
author = {Price, Susan L. and Nielsen, Marianne Lykke and Delcambre, Lois M. L. and Vedsted, Peter},
title = {Semantic Components Enhance Retrieval of Domain-Specific Documents},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321502},
doi = {10.1145/1321440.1321502},
abstract = {We seek to leverage knowledge about information organization in a domain to effectively and efficiently meet targeted information needs of expert users. The semantic components model represents document content in a manner that is complementary to full text and keyword indexing. Semantic component instances are segments of text about a particular aspect of the main topic of the document and may not correspond to structural elements in the document. This paper describes the semantic components model and presents experimental evidence from a large interactive searching study showing that semantic components, used to supplement full text and keyword indexing and to extend the query language, enhanced the retrieval of domain-specific documents in response to realistic queries posed by real users.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {429–438},
numpages = {10},
keywords = {semantic components},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321503,
author = {Pham, Trong-Ton and Maillot, Nicolas Eric and Lim, Joo-Hwee and Chevallet, Jean-Pierre},
title = {Latent Semantic Fusion Model for Image Retrieval and Annotation},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321503},
doi = {10.1145/1321440.1321503},
abstract = {This paper studies the effect of Latent Semantic Analysis (LSA) on two different tasks: multimedia document retrieval (MDR) and automatic image annotation (AIA). The contributions of this paper are twofold. First, to the best of our knowledge, this work is the first study of the influence of LSA on the retrieval of a significant number of multimedia documents (i.e. collection of 20000 tourist images). Second, it shows how different image representations (region-based and keypoint-based) can be combined by LSA to improve automatic image annotation. The document collections used for these experiments are the Corel photo collection and ImageCLEF 2006 collection.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {439–444},
numpages = {6},
keywords = {latent semantic indexing, image indexing and retrieval, automatic annotation, multimedia fusion},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321504,
author = {Milne, David N. and Witten, Ian H. and Nichols, David M.},
title = {A Knowledge-Based Search Engine Powered by Wikipedia},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321504},
doi = {10.1145/1321440.1321504},
abstract = {This paper describes Koru, a new search interface that offers effective domain-independent knowledge-based information retrieval. Koru exhibits an understanding of the topics of both queries and documents. This allows it to (a) expand queries automatically and (b) help guide the user as they evolve their queries interactively. Its understanding is mined from the vast investment of manual effort and judgment that is Wikipedia. We show how this open, constantly evolving encyclopedia can yield inexpensive knowledge structures that are specifically tailored to expose the topics, terminology and semantics of individual document collections. We conducted a detailed user study with 12 participants and 10 topics from the 2005 TREC HARD track, and found that Koru and its underlying knowledge base offers significant advantages over traditional keyword search. It was capable of lending assistance to almost every query issued to it; making their entry more efficient, improving the relevance of the documents they return, and narrowing the gap between expert and novice seekers.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {445–454},
numpages = {10},
keywords = {information retrieval, query expansion, wikipedia, thesauri, data mining},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250801,
author = {Lim, Ee-Peng},
title = {Session Details: OLAP and Multi-Dimensional Databases (DB)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250801},
doi = {10.1145/3250801},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321506,
author = {Inokuchi, Akihiro and Takeda, Koichi},
title = {A Method for Online Analytical Processing of Text Data},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321506},
doi = {10.1145/1321440.1321506},
abstract = {There are increasingly visible demands for structured/ unstructured information integration and advanced analytics. However, conventional database technology has not been able to present a robust and practical implementation of a truly integrated architecture for such purposes. After working on several industrial applications (in particular, in the healthcare and life sciences area), we have identified fundamental issues and technical approaches to tackle the issues. In this paper, we propose data representations and algebraic operations for integrating semantic information (e.g., ontologies) into OLAP systems, which allow us to analyze a huge set of textual documents with their underlying semantic information. The performance of the prototype implementation has been evaluated using real world datasets, and the high scalability and flexibility of our approach have been confirmed with respect to the computation time.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {455–464},
numpages = {10},
keywords = {olap, text mining},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321507,
author = {Eavis, Todd and Taleb, Ahmad},
title = {Mapgraph: Efficient Methods for Complex Olap Hierarchies},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321507},
doi = {10.1145/1321440.1321507},
abstract = {Online Analytical Processing is a database paradigm that provides for the rich analysis of multi-dimensional data. OLAP is often supported by a logical structure known as the Cube. However, supporting efficient OLAP query resolution in enterprise scale environments is an issue of considerable complexity. In practice, the difficulty of the problem is exacerbated by the existence of dimension hierarchies that sub-divide core dimensions into aggregation layers of varying granularity. Common hierarchy-sensitive query operations such as Rollup and Drilldown can be very costly on large cubes. Moreover, facilities for the representation of more complex hierarchical relationships are not well supported by conventional techniques. This paper presents a robust hierarchy infrastructure called mapGraph that supports the efficient and transparent manipulation of attribute hierarchies within OLAP environments. Experimental results verify that, when compared to the alternatives, very little additional overhead is introduced, even when advanced functionality is exploited.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {465–474},
numpages = {10},
keywords = {data cube, attribute hierarchies, olap},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321508,
author = {Eavis, Todd and Lopez, Alex},
title = {Rk-Hist: An r-Tree Based Histogram for Multi-Dimensional Selectivity Estimation},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321508},
doi = {10.1145/1321440.1321508},
abstract = {Database query engines typically rely upon query size estimators in order to evaluate the potential cost of alternate query plans. In multi-dimensional database systems, such as those typically found in large data warehousing environments, these selectivity estimators often take the form of multi-dimensional histograms. But while single dimensional histograms have proven to be quite accurate, even in the presence of data skew, the multi-dimensional variations have generally been far less reliable. In this paper, we present a new histogram model that is based upon an r-tree space partitioning. The localization of the r-tree boxes is in turn controlled by a Hilbert space filling curve, while a series of efficient area equalization heuristics restructures the initial boxes to provide improved bucket representation. Experimental results demonstrate significantly improved estimation accuracy relative to state of the art alternatives, as well as superior consistency across a variety of record distributions.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {475–484},
numpages = {10},
keywords = {multi-dimensional selectivity estimation, data warehousing},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250802,
author = {Mitra, Prasenjit},
title = {Session Details: Information Extraction, Conceptual Clustering, and Prioritization (KM)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250802},
doi = {10.1145/3250802},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321510,
author = {Pa\c{s}ca, Marius and Van Durme, Benjamin and Garera, Nikesh},
title = {The Role of Documents vs. Queries in Extracting Class Attributes from Text},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321510},
doi = {10.1145/1321440.1321510},
abstract = {Challenging the implicit reliance on document collections, this paper discusses the pros and cons of using query logs rather than document collections, as self-contained sources of data in textual information extraction. The differences are quantified as part of a large-scale study on extracting prominent attributes or quantifiable properties of classes (e.g., top speed, price and fuel consumption for CarModel) from unstructured text. In a head-to-head qualitative comparison, a lightweight extraction method produces class attributes that are 45% more accurate on average, when acquired from query logs rather than Web documents.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {485–494},
numpages = {10},
keywords = {knowledge acquisition, textual data sources, query logs, class attribute extraction},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321511,
author = {Chevallet, Jean-Pierre and Lim, Joo-Hwee and Le, Diem Thi Hoang},
title = {Domain Knowledge Conceptual Inter-Media Indexing: Application to Multilingual Multimedia Medical Reports},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321511},
doi = {10.1145/1321440.1321511},
abstract = {Conceptual Indexing is a way to produce only one index for many multilingual documents. Inter-Media conceptual indexing promotes the use of common concepts between two media in order to use a single index for several media. In this paper we explore such an advance indexing point of view. We show the benefit of an automatic conceptual indexing for texts and its extension for text and image documents. Tests are conducted on the multilingual image and text medical document corpus of the CLEF initiative, where we obtain best results on text in 2005 and 2006, and show promising results on images, and best results for the combination of image and text.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {495–504},
numpages = {10},
keywords = {conceptual indexing, inter-media indexing, image indexing, multilingual text indexing},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321512,
author = {Chu-Carroll, Jennifer and Prager, John},
title = {An Experimental Study of the Impact of Information Extraction Accuracy on Semantic Search Performance},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321512},
doi = {10.1145/1321440.1321512},
abstract = {Researchers have shown that various natural language processing techniques can be used in document analysis to impact search performance. For the most part, they examined how an analysis system with certain performance characteristics can be leveraged to improve document and/or passage search results. We have previously shown that semantic queries which utilize named entity and relation information extracted from the corpus can lead to significant improvement in search performance. In this paper, we extend our previous efforts and examine how search performance degrades in the face of imperfect named entity and relation extraction. Our study was carried out by developing gold standard annotated corpora and applying different error models to the gold standard annotations to simulate errors made by automatic recognizers. We identify automatic recognizer characteristics that make them more amenable to our search tasks, show that recognizer recall has more significant impact on semantic search performance than its precision, and demonstrate that significant improvement in both MAP and Exact Precision scores can be achieved by adopting automatic named entity and relation recognizers with near state-of-the-art performance.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {505–514},
numpages = {10},
keywords = {relationship recognition, semantic search, named entity recognition},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321513,
author = {Li, Lida and Muller, Michael J. and Geyer, Werner and Dugan, Casey and Brownholtz, Beth and Millen, David R.},
title = {Predicting Individual Priorities of Shared Activities Using Support Vector Machines},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321513},
doi = {10.1145/1321440.1321513},
abstract = {Activity-centric collaboration environments help knowledge workers to manage the context of their shared work activities by providing a representation for an activity and its resources. Activity management systems provide more structure and organization than email to execute the shared activity but, as the number of shared activities increases, it becomes more and more difficult for users to focus on important activities that need their attention. This paper describes a personalized activity prioritization approach implemented on top of the Lotus Connections Activities management system. Our prototype implementation allows each user to view activities ordered by her/his predicted priorities. The predictions are made using a ranking Support Vector Machine model trained with the user's past interactions with the activities system. We describe the prioritization interface and the results of an offline experiment based on data from 13 users over 6-months. Our results show that our feature set derived from shared activity structures can significantly increase prediction accuracy compared to a recency baseline.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {515–524},
numpages = {10},
keywords = {support vector machine, knowledge worker, activity flood, ranking, activity-centric collaboration, prioritization},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250803,
author = {de Moura, Edleno Silva},
title = {Session Details: Web Retrieval II (IR)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250803},
doi = {10.1145/3250803},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321515,
author = {Sieg, Ahu and Mobasher, Bamshad and Burke, Robin},
title = {Web Search Personalization with Ontological User Profiles},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321515},
doi = {10.1145/1321440.1321515},
abstract = {Every user has a distinct background and a specific goal when searching for information on the Web. The goal of Web search personalization is to tailor search results to a particular user based on that user's interests and preferences. Effective personalization of information access involves two important challenges: accurately identifying the user context and organizing the information in such a way that matches the particular context. We present an approach to personalized search that involves building models of user context as ontological profiles by assigning implicitly derived interest scores to existing concepts in a domain ontology. A spreading activation algorithm is used to maintain the interest scores based on the user's ongoing behavior. Our experiments show that re-ranking the search results based on the interest scores and the semantic evidence in an ontological user profile is effective in presenting the most relevant results to the user.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {525–534},
numpages = {10},
keywords = {ontological user profiles, search personalization, user context},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321516,
author = {Tan, Qingzhao and Mitra, Prasenjit and Giles, C. Lee},
title = {Designing Clustering-Based Web Crawling Policies for Search Engine Crawlers},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321516},
doi = {10.1145/1321440.1321516},
abstract = {The World Wide Web is growing and changing at an astonishing rate. Web information systems such as search engines have to keep up with the growth and change of the Web. Due to resource constraints, search engines usually have difficulties keeping the local database completely synchronized with the Web. In this paper, we study how tomake good use of the limited system resource and detect as many changes as possible. Towards this goal, a crawler for the Web search engine should be able to predict the change behavior of the webpages. We propose applying clustering-based sampling approach. Specifically, we first group all the local webpages into different clusters such that each cluster contains webpages with similar change pattern. We then sample webpages from each cluster to estimate the change frequency of all the webpages in that cluster. Finally, we let the crawler re-visit the cluster containing webpages with higher change frequency with a higher probability. To evaluate the performance of an incremental crawler for a Web search engine, we measure both the freshness and the quality of the query results provided by the search engine. We run extensive experiments on a real Web data set of about 300,000 distinct URLs distributed among 210 websites. The results demonstrate that our clustering algorithm effectively clusters the pages with similar change patterns, and our solution significantly outperforms the existing methods in that it can detect more changed webpages and improve the quality of the user experience for those who query the search engine.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {535–544},
numpages = {10},
keywords = {refresh policy, incremental crawler, sampling, web search engine, clustering},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321517,
author = {Park, Laurence A. F. and Ramamohanarao, Kotagiri},
title = {Mining Web Multi-Resolution Community-Based Popularity for Information Retrieval},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321517},
doi = {10.1145/1321440.1321517},
abstract = {The PageRank algorithm is used in Web information retrieval to calculate a single list of popularity scores for each page in the Web. These popularity scores are used to rank query results when presented to the user. By using the structure of the entire Web to calculate one score per document, we are calculating a general popularity score, not particular to any community. Therefore, the PageRank scores are more suited to general queries. In this paper, we introduce a more general form of PageRank, using Web multi-resolution community-based popularity scores, where each document obtains a popularity score dependent on a given Web community. When a query is related to a specific community, we choose the associated set of popularity scores and order the query results accordingly. Using Web-community based popularity scores, we achieved an 11% increase in precision over PageRank.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {545–554},
numpages = {10},
keywords = {pagerank, symmetric non-negative matrix factorisation},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321518,
author = {Wang, Changhu and Jing, Feng and Zhang, Lei and Zhang, Hong-Jiang},
title = {Learning Query-Biased Web Page Summarization},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321518},
doi = {10.1145/1321440.1321518},
abstract = {Query-biased Web page summarization is the summarization of a Web page reflecting the relevance of it to a specific query. It plays an important role in search results representation of Web search engines. In this paper, we propose a learning-based query-biased Web page summarization method. The summarization problem is solved within the typical sentence selection framework. Different from existing Web page summarization methods that use page content or link context alone, both of them are considered as the sources of sentences in this work. Most of existing learning-based summarization methods treat summarization as a sentence classification problem and train a classifier to discriminate between extracted sentences and non-extracted sentences of all training documents. The basic assumption of these methods is that sentences from different documents are comparable with respect to the class information. In contrast to the classification scheme, a ranking scheme is introduced to rank extracted sentences higher than non-extracted sentences of each training document. The underlying assumption that sentences within a document are comparable is weaker and more reasonable than the assumption of classification-based scheme. Extensive results using intrinsic evaluation metrics gauge many aspects of the proposed method.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {555–562},
numpages = {8},
keywords = {ranking, classification, support vector machines, query-biased web page summarization},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250804,
author = {Oliveira, Arlindo},
title = {Session Details: Graph Based Retrieval (IR)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250804},
doi = {10.1145/3250804},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321520,
author = {Vieira, Monique V. and Fonseca, Bruno M. and Damazio, Rodrigo and Golgher, Paulo B. and Reis, Davi de Castro and Ribeiro-Neto, Berthier},
title = {Efficient Search Ranking in Social Networks},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321520},
doi = {10.1145/1321440.1321520},
abstract = {In social networks such as Orkut, www.orkut.com, a large portion of the user queries refer to names of other people. Indeed, more than 50% of the queries in Orkut are about names of other users, with an average of 1.8 terms per query. Further, the users usually search for people with whom they maintain relationships in the network. These relationships can be modelled as edges in a friendship graph, a graph in which the nodes represent the users. In this context, search ranking can be modelled as a function that depends on the distances among users in the graph, more specifically, of shortest paths in the friendship graph. However, application of this idea to ranking is not straightforward because the large size of modern social networks (dozens of millions of users) prevents efficient computation of shortest paths at query time. We overcome this by designing a ranking formula that strikes a balance between producing good results and reducing query processing time. Using data from the Orkut social network, which includes over 40 million users, we show that our ranking, augmented by this new signal, produces high quality results, while maintaining query processing time small.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {563–572},
numpages = {10},
keywords = {social networks, graphs, shortest path},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321521,
author = {Mart\'{\i}nez-Bazan, Norbert and Munt\'{e}s-Mulero, Victor and G\'{o}mez-Villamor, Sergio and Nin, Jordi and S\'{a}nchez-Mart\'{\i}nez, Mario-A. and Larriba-Pey, Josep-L.},
title = {Dex: High-Performance Exploration on Large Graphs for Information Retrieval},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321521},
doi = {10.1145/1321440.1321521},
abstract = {Link and graph analysis tools are important devices to boost the richness of information retrieval systems. Internet and the existing social networking portals are just a couple of situations where the use of these tools would be beneficial and enriching for the users and the analysts. However, the need for integrating different data sources and, even more important, the need for high performance generic tools, is at odds with the continuously growing size and number of data repositories.In this paper we propose and evaluate DEX, a high performance graph database querying system that allows for the integration of multiple data sources. DEX makes graph querying possible in different flavors, including link analysis, social network analysis, pattern recognition and keyword search. The richness of DEX shows up in the experiments that we carried out on the Internet Movie Database (IMDb). Through a variety of these complex analytical queries, DEX shows to be a generic and efficient tool on large graph databases.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {573–582},
numpages = {10},
keywords = {data representation, social networks, graph databases, query performance, information retrieval},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321522,
author = {Wang, Xuanhui and Sun, Jian-Tao and Chen, Zheng},
title = {Shine: Search Heterogeneous Interrelated Entities},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321522},
doi = {10.1145/1321440.1321522},
abstract = {Heterogeneous entities or objects are very common and are usually interrelated with each other in many scenarios. For example, typical Web search activities involve multiple types of interrelated entities such as end users, Web pages, and search queries. In this paper, we define and study a novel problem: <ul>S</ul>earch <ul>H</ul>eterogeneous <ul>IN</ul>terrelated <ul>E</ul>ntities (SHINE). Given a SHINE-query which can be any type(s) of entities, the task of SHINE is to retrieve multiple types of related entities to answer this query. This is in contrast to the traditional search,which only deals with a single type of entities (e.g., Web pages). The advantages of SHINE include: (1) It is feasible for end users to specify their information need along different dimensions by accepting queries with different types. (2) Answering a query by multiple types of entities provides informative context for users to better understand the search results and facilitate their information exploration. (3) Multiple relations among heterogeneous entities can be utilized to improve the ranking of any particular type of entities. To attain the goal of SHINE, we propose to represent all entities in a unified space through utilizing their interaction relationships. Two approaches, M-LSA and E-VSM, are discussed and compared in this paper. The experiments on 3 data sets (i.e., a literature data set, a search engine log data set, and a recommendation data set) show the effectiveness and flexibility of our proposed methods.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {583–592},
numpages = {10},
keywords = {search, heterogeneous interrelated entities, shine, multiple types},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250805,
author = {Frieder, Ophir},
title = {Session Details: Data Exploration and Discovery (KM)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250805},
doi = {10.1145/3250805},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321524,
author = {Schockaert, Steven and De Cock, Martine},
title = {Reasoning about Vague Topological Information},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321524},
doi = {10.1145/1321440.1321524},
abstract = {Topological information plays a fundamental role in the human perception of spatial configurations and is thereby one of the most prominent geographical features in natural language. As vagueness abounds in geography, flexible formalisms with the ability to capture vague topological information are often needed in practice. While such formalisms have already been introduced by various authors, complete reasoning procedures are usually not discussed. In this paper, we show how many interesting reasoning tasks, such as consistency checking and entailment checking, can be supported in a generalization of the well-known RCC-8 calculus. In particular, we present decision procedures based on linear programming, solving all reasoning tasks of interest. We furthermore show how deciding the consistency of vague topological information can be reduced to the consistency problem of the original RCC-8.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {593–602},
numpages = {10},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321525,
author = {Yang, Di and Rundensteiner, Elke A. and Ward, Matthew O.},
title = {Nugget Discovery in Visual Exploration Environments by Query Consolidation},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321525},
doi = {10.1145/1321440.1321525},
abstract = {Queries issued by casual users or specialists exploring a dataset often point us to important subsets of the data, be it clusters, outliers or other meaningful features. Capturing and caching such queries (henceforth called nuggets) has many potential benefits, including the optimization of the system performance and the search experience of users. Unfortunately, current visual exploration systems have not yet tapped into this potential resource of identifying and sharing important queries. In this paper, we introduce a query consolidation strategy aimed at solving the general problem of isolating important queries from the potentially huge amount of queries submitted. Our solution clusters redundant queries caused by exploration-style query specification, which is prevalent in data exploration systems. To measure the similarity between queries, we designed an effective distance metric that incorporates both the query specification and the actual query result. To overcome its high complexity when comparing queries with large result sets, we designed an approximation method, which is efficient while still providing excellent accuracy. A user study conducted on multivariate data sets comparing our proposed technique to others in the literature confirms that the proposed distance metric indeed matches well with users' intuition. As proof of feasibility, we integrated our proposed query consolidation solution into the Nugget Management System (NMS) framework [22], which is based on a visual exploration system XmdvTool. A second user study indicates that both the efficiency and accuracy of users' visual exploration are enhanced when supported by NMS.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {603–612},
numpages = {10},
keywords = {query redundancy, query consolidation, distance metrics},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321526,
author = {Lang, Kevin J. and Andersen, Reid},
title = {Finding Dense and Isolated Submarkets in a Sponsored Search Spending Graph},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321526},
doi = {10.1145/1321440.1321526},
abstract = {Methods for improving sponsored search revenue are often tested or deployed within a small submarket of the larger marketplace. For many applications, the ideal submarket contains a small number of nodes, a large amount of spending within the submarket, and a small amount of spending leaving the submarket. We introduce an efficient algorithm for finding submarkets that are optimal for a user-specified tradeoff between these three quantities. We apply our algorithm to find submarkets that are both dense and isolated in a large spending graph from Yahoo! sponsored search.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {613–622},
numpages = {10},
keywords = {sparse cuts, dense subgraphs, pareto-optimality, sponsored search, e-commerce, vector-valued optimization, parametric flow},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250806,
author = {Clarke, Charles},
title = {Session Details: IR Evaluation (IR)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250806},
doi = {10.1145/3250806},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321528,
author = {Smucker, Mark D. and Allan, James and Carterette, Ben},
title = {A Comparison of Statistical Significance Tests for Information Retrieval Evaluation},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321528},
doi = {10.1145/1321440.1321528},
abstract = {Information retrieval (IR) researchers commonly use three tests of statistical significance: the Student's paired t-test, the Wilcoxon signed rank test, and the sign test. Other researchers have previously proposed using both the bootstrap and Fisher's randomization (permutation) test as non-parametric significance tests for IR but these tests have seen little use. For each of these five tests, we took the ad-hoc retrieval runs submitted to TRECs 3 and 5-8, and for each pair of runs, we measured the statistical significance of the difference in their mean average precision. We discovered that there is little practical difference between the randomization, bootstrap, and t tests. Both the Wilcoxon and sign test have a poor ability to detect significance and have the potential to lead to false detections of significance. The Wilcoxon and sign tests are simplified variants of the randomization test and their use should be discontinued for measuring the significance of a difference between means.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {623–632},
numpages = {10},
keywords = {hypothesis test, sign, wilcoxon, bootstrap, student's t-test, randomization, statistical significance, permutation},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321529,
author = {Aslam, Javed A. and Yilmaz, Emine},
title = {Inferring Document Relevance from Incomplete Information},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321529},
doi = {10.1145/1321440.1321529},
abstract = {Recent work has shown that average precision can be accurately estimated from a small random sample of judged documents. Unfortunately, such "random pools" cannot be used to evaluate retrieval measures in any standard way. In this work, we show that given such estimates of average precision, one can accurately infer the relevances of the remaining unjudged documents, thus obtaining a fully judged pool that can be used in standard ways for system evaluation of all kinds. Using TREC data, we demonstrate that our inferred judged pools are well correlated with assessor judgments, and we further demonstrate that our inferred pools can be used to accurately infer precision recall curves and all commonly used measures of retrieval performance.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {633–642},
numpages = {10},
keywords = {incomplete judgments, relevance judgments, average precision},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321530,
author = {Carterette, Ben and Smucker, Mark D.},
title = {Hypothesis Testing with Incomplete Relevance Judgments},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321530},
doi = {10.1145/1321440.1321530},
abstract = {Information retrieval experimentation generally proceeds in a cycle of development, evaluation, and hypothesis testing. Ideally, the evaluation and testing phases should be short and easy, so as to maximize the amount of time spent in development. There has been recent work on reducing the amount of assessor effort needed to evaluate retrieval systems, but it has not, for the most part, investigated the effects of these methods on tests of significance. In this work, we explore in detail the effects of reduced sets of judgments on the sign test. We demonstrate both analytically and empirically the relationship between the power of the test, the number of topics evaluated, and the number of judgments available. Using these relationships, we can determine the number of topics and judgments needed for the least-cost but highest-confidence significance evaluation. Specifically, testing pairwise significance over 192 topics with fewer than 5 judgments for each is as good as testing significance over 25 topics with an average of 166 judgments for each - 85% less effort producing no additional errors.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {643–652},
numpages = {10},
keywords = {information retrieval, hypothesis testing, test collections, evaluation},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250807,
author = {Couto, Francisco},
title = {Session Details: Performance Issues (DB)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250807},
doi = {10.1145/3250807},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321532,
author = {Botelho, Fabiano C. and Ziviani, Nivio},
title = {External Perfect Hashing for Very Large Key Sets},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321532},
doi = {10.1145/1321440.1321532},
abstract = {We present a simple and efficient external perfect hashing scheme (referred to as EPH algorithm) for very large static key sets. We use a number of techniques from the literature to obtain a novel scheme that is theoretically well-understood and at the same time achieves an order-of-magnitude increase in the size of the problem to be solved compared to previous "practical" methods. We demonstrate the scalability of our algorithm by constructing minimum perfect hash functions for a set of 1.024 billion URLs from the World Wide Web of average length 64 characters in approximately 62 minutes, using a commodity PC. Our scheme produces minimal perfect hash functions using approximately 3.8 bits per key. For perfect hash functions in the range {0,...,2n - 1} the space usage drops to approximately 2.7 bits per key. The main contribution is the first algorithm that has experimentally proven practicality for sets in the order of billions of keys and has time and space usage carefully analyzed without unrealistic assumptions.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {653–662},
numpages = {10},
keywords = {functions, perfect, key sets, minimal, hash, large},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321533,
author = {Rao, Weixiong and Chen, Lei and Fu, Ada Wai-Chee and Bu, YingYi},
title = {Optimal Proactive Caching in Peer-to-Peer Network: Analysis and Application},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321533},
doi = {10.1145/1321440.1321533},
abstract = {As a promising new technology with the unique properties like high efficiency, scalability and fault tolerance, Peer-to-Peer (P2P) technology is used as the underlying network to build new Internet-scale applications. However, one of the well known issues in such an application (for example WWW) is that the distribution of data popularities is heavily tailed with a Zipf-like distribution. With consideration of the skewed popularity we adopt a proactive caching approach to handle the challenge, and focus on two key problems: where (i.e. the placement strategy: where to place the replicas) and how (i.e. the degree problem: how many replicas are assigned to one specific content)? For the where problem, we propose a novel approach which can be generally applied to structured P2P networks. Next, we solve two optimization objectives related to the how problem: MAX_PERF and MIN_COST. Our solution is called <b>PoPCache</b>, and we discover two interesting properties: (1) the number of replicas assigned to each content is proportional to its popularity; (2) the derived optimal solutions are related to the entropy of popularity. To our knowledge, none of the previous works has mentioned such results. Finally, we apply the results of PoPCache to propose a P2P base web caching, called as Web-PoPCache. By means of web cache trace driven simulation, our extensive evaluation results demonstrate the advantages of PoPCache and Web-PoPCache.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {663–672},
numpages = {10},
keywords = {placement strategy, peer-to-peer, web caching},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321534,
author = {Bhowmick, Sourav S. and Leonardi, Erwin and Sun, Hongmei},
title = {Efficient Evaluation of High-Selective Xml Twig Patterns with Parent Child Edges in Tree-Unaware Rdbms},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321534},
doi = {10.1145/1321440.1321534},
abstract = {Recent study showed that native twig join algorithms and tree-aware relational framework significantly outperform tree-unaware approaches in evaluating structural relationships in XML twig queries. In this paper, we present an efficient strategy to evaluate high-selective twig queries containing only parent-child relationships in a tree-unaware relational environment. Our scheme is built on top of our S<scp>UCXENT</scp>++ system. We show that by exploiting the encoding scheme of S<scp>UCXENT</scp>++, we can devise efficient strategy for evaluating such twig queries. Extensive performance studies on various data sets and queries show that our approach performs better than a representative tree-unaware approach (G<scp>LOBAL</scp>-O<scp>RDER</scp>) and a state-of-the-art native twig join algorithm (TJF<scp>AST</scp>) on all benchmark queries with the highest observed gain factors being 243 and 95, respectively. Additionally, our approach reduces significantly the performance gap between tree-aware and tree-unaware approaches and even outperforms a tree-aware approach(M<scp>ONET</scp>DB/XQ<scp>UERY</scp>) for certain high-selective twig queries. We also report our insights to the plan choices a relational optimizer made during twig query evaluation by visually characterizing its behavior over the relational selectivity space.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {673–682},
numpages = {10},
keywords = {parent child edges, xml, twig query evaluation, tree-unaware rdbms},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250808,
author = {Marx, Maarten},
title = {Session Details: Information Representation and Integration (KM)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250808},
doi = {10.1145/3250808},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321536,
author = {Pa\c{s}ca, Marius},
title = {Weakly-Supervised Discovery of Named Entities Using Web Search Queries},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321536},
doi = {10.1145/1321440.1321536},
abstract = {A seed-based framework for textual information extraction allows for weakly supervised extraction of named entities from anonymized Web search queries. The extraction is guided by a small set of seed named entities, without any need for handcrafted extraction patterns or domain-specific knowledge, allowing for the acquisition of named entities pertaining to various classes of interest to Web search users. Inherently noisy search queries are shown to be a highly valuable, albeit little explored, resource for Web-based named entity discovery.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {683–690},
numpages = {8},
keywords = {weakly supervised information extraction, named entities, query logs, unstructured text, knowledge acquisition},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321537,
author = {von Brzeski, Vadim and Irmak, Utku and Kraft, Reiner},
title = {Leveraging Context in User-Centric Entity Detection Systems},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321537},
doi = {10.1145/1321440.1321537},
abstract = {A user-centric entity detection system is one in which the primary consumer of the detected entities is a person who can perform actions on the detected entities (e.g. perform a search, view a map, shop, etc.). We contrast this with machine-centric detection systems where the primary consumer of the detected entities is a machine. Machine-centric detection systems typically focus on the quantity of detected entities, measured by precision and recall metrics, with the goal of correctly identifying every single entity in a document.However, the simple precision/recall scores of machine-centric entity detection systems fail to accurately reflect the quality of detected entities in user-centric systems, where users may not necessarily want to "see" every possible entity. We posit that not all of the detected entities in a given piece of text are necessarily relevant to the main topic of the text, nor are they necessarily interesting enough to the user to warrant further action. In fact, presenting all of the detected entities to a user may annoy the user to the point where he decides to turn this capability off completely, an undesirable outcome. Therefore, we propose to measure the quality and utility of user-centric entity detection systems in three core dimensions: the accuracy, the interestingness, and the relevance of the entities it presents to the user. We show that leveraging surrounding context can greatly improve the performance of such systems in all three dimensions by employing novel algorithms for generating a concept vector and for finding concept extensions using search query logs.We extensively evaluate the proposed algorithms within Contextual Shortcuts - a large-scale user-centric entity detection platform - using 1,586 entities detected over 1,519 documents. The results confirm the importance of using context within user-centric entity detection systems, and validate the usefulness of the proposed algorithms by showing how they improve the overall entity detection quality within Contextual Shortcuts.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {691–700},
numpages = {10},
keywords = {contextual shortcuts, entity detection, information extraction, context, content syndication, contextual},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321538,
author = {Prager, John and Luger, Sarah and Chu-Carroll, Jennifer},
title = {Type Nanotheories: A Framework for Term Comparison},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321538},
doi = {10.1145/1321440.1321538},
abstract = {We present in this paper Type Nanotheories (TN), a framework for representing the knowledge necessary for performing similarity comparisons between pairs of terms of the same type. TN itself uses another methodology, namely Support Outcomes, which is also introduced. Many IR and NLP applications use redundancy as a factor to increase confidence, and TN-based comparisons can determine redundancy better than simple string comparisons. Results include a showing of a 14% increase in Confidence-Weighted Score for an end-to-end QA system and an up to 68% improvement over baseline in an answer-key equivalencing experiment.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {701–710},
numpages = {10},
keywords = {term comparison, type systems, question answering},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250809,
author = {Shanahan, James G.},
title = {Session Details: Natural Language II (IR)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250809},
doi = {10.1145/3250809},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321540,
author = {Zhang, Wei and Liu, Shuang and Yu, Clement and Sun, Chaojing and Liu, Fang and Meng, Weiyi},
title = {Recognition and Classification of Noun Phrases in Queries for Effective Retrieval},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321540},
doi = {10.1145/1321440.1321540},
abstract = {It has been shown that using phrases properly in the document retrieval leads to higher retrieval effectiveness. In this paper, we define four types of noun phrases and present an algorithm for recognizing these phrases in queries. The strengths of several existing tools are combined for phrase recognition. Our algorithm is tested using a set of 500 web queries from a query log, and a set of 238 TREC queries. Experimental results show that our algorithm yields high phrase recognition accuracy. We also use a baseline noun phrase recognition algorithm to recognize phrases from the TREC queries. A document retrieval experiment is conducted using the TREC queries (1) without any phrases, (2) with the phrases recognized from a baseline noun phrase recognition algorithm, and (3) with the phrases recognized from our algorithm respectively. The retrieval effectiveness of (3) is better than that of (2), which is better than that of (1). This demonstrates that utilizing phrases in queries does improve the retrieval effectiveness, and better noun phrase recognition yields higher retrieval performance.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {711–720},
numpages = {10},
keywords = {dictionary phrase, information retrieval, complex phrase, simple phrase, feedback, noun phrases, verification, proper noun},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321541,
author = {Lad, Abhimanyu and Yang, Yiming},
title = {Generalizing from Relevance Feedback Using Named Entity Wildcards},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321541},
doi = {10.1145/1321440.1321541},
abstract = {Traditional adaptive filtering systems learn the user's interests in a rather simple way - words from relevant documents are favored in the query model, while words from irrelevant documents are down-weighted. This biases the query model towards specific words seen in the past, causing the system to favor documents containing relevant but redundant information over documents that use previously unseen words to denote new facts about the same news event. This paper proposes news ways of generalizing from relevance feedback by augmenting the traditional bag-of-words query model with named entity wildcards that are anchored in context. The use of wildcards allows generalization beyond specific words, while contextual restrictions limit the wildcard-matching to entities related to the user's query. We test our new approach in a nugget-level adaptive filtering system and evaluate it in terms of both relevance and novelty of the presented information. Our results indicate that higher recall is obtained when lexical terms are generalized using wildcards. However, such wildcards must be anchored to their context to maintain good precision. How the context of a wildcard is represented and matched against a given document also plays a crucial role in the performance of the retrieval system.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {721–730},
numpages = {10},
keywords = {adaptive filtering, relevance feedback, named entities},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321542,
author = {Petkova, Desislava and Croft, W. Bruce},
title = {Proximity-Based Document Representation for Named Entity Retrieval},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321542},
doi = {10.1145/1321440.1321542},
abstract = {One aspect in which retrieving named entities is different from retrieving documents is that the items to be retrieved - persons, locations, organizations - are only indirectly described by documents throughout the collection. Much work has been dedicated to finding references to named entities, in particular to the problems of named entity extraction and disambiguation. However, just as important for retrieval performance is how these snippets of text are combined to build named entity representations.We focus on the TREC expert search task where the goal is to identify people who are knowledgeable on a specific topic. Existing language modeling techniques for expert finding assume that terms and person entities are conditionally independent given a document. We present theoretical and experimental evidence that this simplifying assumption ignores information on how named entities relate to document content. To address this issue, we propose a new document representation which emphasizes text in proximity to entities and thus incorporates sequential information implicit in text. Our experiments demonstrate that the proposed model significantly improves retrieval performance. The main contribution of this work is an effective formal method for explicitly modeling the dependency between the named entities and terms which appear in a document.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {731–740},
numpages = {10},
keywords = {expert finding, ne (named entity) retrieval, language models, proximity kernels},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250810,
author = {Ziviani, Nivio},
title = {Session Details: Indexing (IR)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250810},
doi = {10.1145/3250810},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321544,
author = {Cai, Deng and He, Xiaofei and Zhang, Wei Vivian and Han, Jiawei},
title = {Regularized Locality Preserving Indexing via Spectral Regression},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321544},
doi = {10.1145/1321440.1321544},
abstract = {We consider the problem of document indexing and representation. Recently, Locality Preserving Indexing (LPI) was proposed for learning a compact document subspace. Different from Latent Semantic Indexing (LSI) which is optimal in the sense of global Euclidean structure, LPI is optimal in the sense of local manifold structure. However, LPI is not efficient in time and memory which makes it difficult to be applied to very large data set. Specifically, the computation of LPI involves eigen-decompositions of two dense matrices which is expensive. In this paper, we propose a new algorithm called Regularized Locality Preserving Indexing (RLPI). Benefit from recent progresses on spectral graph analysis, we cast the original LPI algorithm into a regression framework which enable us to avoid eigen-decomposition of dense matrices. Also, with the regression based framework, different kinds of regularizers can be naturally incorporated into our algorithm which makes it more flexible. Extensive experimental results show that RLPI obtains similar or better results comparing to LPI and it is significantly faster, which makes it an efficient and effective data preprocessing method for large scale text clustering, classification and retrieval.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {741–750},
numpages = {10},
keywords = {regularized locality preserving indexing, dimensionality reduction, document representation and indexing},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321545,
author = {Guo, Ruijie and Cheng, Xueqi and Xu, Hongbo and Wang, Bin},
title = {Efficient On-Line Index Maintenance for Dynamic Text Collections by Using Dynamic Balancing Tree},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321545},
doi = {10.1145/1321440.1321545},
abstract = {Previous on-line index maintenance strategies are mainly designed for document insertions without considering document deletions. In a truly dynamic search environment, however, documents may be added to and removed from the collection at any point in time. In this paper, we examine issues of on-line index maintenance with support for instantaneous document deletions and insertions. We present a DBT Merge strategy that can dynamically adjust the sequence of sub-index merge operations during index construction, and offers better query processing performance than previous methods, while providing an equivalent level of index maintenance performance when document insertions and deletions exist in parallel. Using experiments on 426 GB of web data we demonstrate the efficiency of our method in practice, showing that on-line index construction for dynamic text collections can be performed efficiently and almost as fast as for growing text collections.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {751–760},
numpages = {10},
keywords = {index maintenance, on-line index, garbage collection, information retrieval, dynamic text collections},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321546,
author = {B\"{u}ttcher, Stefan and Clarke, Charles L. A.},
title = {Index Compression is Good, Especially for Random Access},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321546},
doi = {10.1145/1321440.1321546},
abstract = {Index compression techniques are known to substantially decrease the storage requirements of a text retrieval system. As a side-effect, they may increase its retrieval performance by reducing disk I/O overhead. Despite this advantage, developers sometimes choose to store index data in uncompressed form, in order to not obstruct random access into each index term's postings list.In this paper, we show that index compression does not harm random access performance. In fact, we demonstrate that, in some cases, random access into a term's postings list may be realized more efficiently if the list is stored in compressed form instead of uncompressed. This is regardless of whether the index is stored on disk or in main memory, since both types of storage - hard drives and RAM - do not support efficient random access in the first place.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {761–770},
numpages = {10},
keywords = {random access, main memory, ram, index compression},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321547,
author = {Zhu, Mingjie and Shi, Shuming and Li, Mingjing and Wen, Ji-Rong},
title = {Effective Top-k Computation in Retrieving Structured Documents with Term-Proximity Support},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321547},
doi = {10.1145/1321440.1321547},
abstract = {Modern web search engines are expected to return top-k results efficiently given a query. Although many dynamic index pruning strategies have been proposed for efficient top-k computation, most of them are prone to ignore some especially important factors in ranking functions, e.g. term proximity (the distance relationship between query terms in a document). The inclusion of term proximity breaks the monotonicity of ranking functions and therefore leads to additional challenges for efficient query processing. This paper studies the performance of some existing top-k computation approaches using term-proximity-enabled ranking functions. Our investigation demonstrates that, when term proximity is incorporated into ranking functions, most existing index structures and top-k strategies become quite inefficient. According to our analysis and experimental results, we propose two index structures and their corresponding index pruning strategies: Structured and Hybrid, which performs much better on the new settings. Moreover, the efficiency of index building and maintenance would not be affected too much with the two approaches.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {771–780},
numpages = {10},
keywords = {top-k, hybrid index structure, document structure, term proximity, dynamic index pruning},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250811,
author = {Park, EK},
title = {Session Details: Data Mining (KM)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250811},
doi = {10.1145/3250811},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321549,
author = {Xu, Yue and Li, Yuefeng},
title = {Generating Concise Association Rules},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321549},
doi = {10.1145/1321440.1321549},
abstract = {Association rule mining has made many achievements in the area of knowledge discovery. However, the quality of the extracted association rules is a big concern. One problem with the quality of the extracted association rules is the huge size of the extracted rule set. As a matter of fact, very often tens of thousands of association rules are extracted among which many are redundant thus useless. Mining non-redundant rules is a promising approach to solve this problem. The Min-max exact basis proposed by Pasquier et al [Pasquier05] has showed exciting results by generating only non-redundant rules. In this paper, we first propose a relaxing definition for redundancy under which the Min-max exact basis still contains redundant rules; then we propose a condensed representation called Reliable exact basis for exact association rules. The rules in the Reliable exact basis are not only non-redundant but also more succinct than the rules in Min-max exact basis. We prove that the redundancy eliminated by the Reliable exact basis does not reduce the belief to the Reliable exact basis. The size of the Reliable exact basis is much smaller than that of the Min-max exact basis. Moreover, we prove that all exact association rules can be deduced from the Reliable exact basis. Therefore the Reliable exact basis is a lossless representation of exact association rules. Experimental results show that the Reliable exact basis significantly reduces the number of non-redundant rules.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {781–790},
numpages = {10},
keywords = {redundant association rules, closed itemsets, generators},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321550,
author = {Angiulli, Fabrizio and Fassetti, Fabio},
title = {Very Efficient Mining of Distance-Based Outliers},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321550},
doi = {10.1145/1321440.1321550},
abstract = {In this work a novel algorithm, named DOLPHIN, for detecting distance-based outliers is presented.The proposed algorithm performs only two sequential scans of the dataset. It needs to store into main memory a portion of the dataset, to efficiently search for neighbors and early prune inliers. The strategy pursued by the algorithm allows to keep this portion very small. Both theoretical justification and empirical evidence that the size of the stored data amounts only to a few percent of the dataset are provided.Another important feature of DOLPHIN is that the memory-resident data are indexed by using a suitable proximity search approach. This allows to search for nearest neighbors looking only at a small subset of the main memory stored data.Temporal and spatial cost analysis show that the novel algorithm achieves both near linear CPU and I/O cost. DOLPHIN has been compared with state of the art methods, showing that it outperforms existing ones.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {791–800},
numpages = {10},
keywords = {data mining, distance-based outliers, anomaly detection},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321551,
author = {Park, Nam Hun and Lee, Won Suk},
title = {Grid-Based Subspace Clustering over Data Streams},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321551},
doi = {10.1145/1321440.1321551},
abstract = {A real-life data stream usually contains many dimensions and some dimensional values of its data elements may be missing. In order to effectively extract the on-going change of a data stream with respect to all the subsets of the dimensions of the data stream, a grid-based subspace clustering algorithm is proposed in this paper. Given an n-dimensional data stream, the on-going distribution statistics of data elements in each one-dimension data space is firstly monitored by a list of grid-cells called a sibling list. Once a dense grid-cell of a first-level sibling list becomes a dense unit grid-cell, new second-level sibling lists are created as its child nodes in order to trace any cluster in all possible two-dimensional rectangular subspaces. In such a way, a sibling tree grows up to the nth level at most and a k-dimensional subcluster can be found in the kth level of the sibling tree. The proposed method is comparatively analyzed by a series of experiments to identify its various characteristics.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {801–810},
numpages = {10},
keywords = {data mining, grid-based clustering, clustering, adaptive memory utilization, data streams},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321552,
author = {Angiulli, Fabrizio and Fassetti, Fabio},
title = {Detecting Distance-Based Outliers in Streams of Data},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321552},
doi = {10.1145/1321440.1321552},
abstract = {In this work a method for detecting distance-based outliers in data streams is presented. We deal with the sliding window model, where outlier queries are performed in order to detect anomalies in the current window. Two algorithms are presented. The first one exactly answers outlier queries, but has larger space requirements. The second algorithm is directly derived from the exact one, has limited memory requirements and returns an approximate answer based on accurate estimations with a statistical guarantee. Several experiments have been accomplished, confirming the effectiveness of the proposed approach and the high quality of approximate solutions.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {811–820},
numpages = {10},
keywords = {distance-based outliers, data streams, anomaly detection},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250812,
author = {Zaragoza, Hugo},
title = {Session Details: Natural Language III (IR)},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250812},
doi = {10.1145/3250812},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321554,
author = {Feng, Ao and Allan, James},
title = {Finding and Linking Incidents in News},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321554},
doi = {10.1145/1321440.1321554},
abstract = {News reports are being produced and disseminated in overwhelming volume, making it difficult to keep up with the newest information. Most previous research in automatic news organization treated news topics as a flat list, ignoring the intrinsic connection among individual reports. We argue that more contextual information within and across the topics will benefit users in their news understanding process.A news organization infrastructure, incident threading, is proposed in this article. All text snippets describing the occurrence of a real-world happening are combined into a news incident, and a network is composed of incidents that are interconnected by links in certain types. A limited vocabulary of connection types is defined and corresponding rules are established based upon the human experience of news understanding.The incident threading system is implemented with two different algorithms. One starts from clustering of text passages and then creates links with pre-built rules. The other method defines a global score function over the whole collection and solves the optimization problem with simulated annealing. The former achieves higher accuracy in the identification of incidents and the latter generates better links, which is preferred since the links are more important for the formation of the incident network.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {821–830},
numpages = {10},
keywords = {global optimization, automatic news organization, threading rules, simulated annealing, incident threading},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321555,
author = {Zhang, Wei and Yu, Clement and Meng, Weiyi},
title = {Opinion Retrieval from Blogs},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321555},
doi = {10.1145/1321440.1321555},
abstract = {Opinion retrieval is a document retrieval process, which requires documents to be retrieved and ranked according to their opinions about a query topic. A relevant document must satisfy two criteria: relevant to the query topic, and contains opinions about the query, no matter if they are positive or negative. In this paper, we describe an opinion retrieval algorithm. It has a traditional information retrieval (IR) component to find topic relevant documents from a document set, an opinion classification component to find documents having opinions from the results of the IR step, and a component to rank the documents based on their relevance to the query, and their degrees of having opinions about the query. We implemented the algorithm as a working system and tested it using TREC 2006 Blog Track data in automatic title-only runs. Our result showed 28% to 32% improvements in MAP score over the best automatic runs in this 2006 track. Our result is also 13% higher than a state-of-art opinion retrieval system, which is tested on the same data set.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {831–840},
numpages = {10},
keywords = {information retrieval, blog retrieval, trec, opinion retrieval},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321556,
author = {Feuer, Alan and Savev, Stefan and Aslam, Javed A.},
title = {Evaluation of Phrasal Query Suggestions},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321556},
doi = {10.1145/1321440.1321556},
abstract = {This paper evaluates the uptake and efficacy of a unified approach to phrasal query suggestions in the context of a high-precision search engine. The search engine performs ranked extended-Boolean searches with the proximity operator <scp>NEAR</scp> being the default operation. Suggestions are offered to the searcher when the length of the result list falls outside predefined bounds. If the list is too long, the engine suggests narrowing the query through the use of super phrases; if the list is too short, the engine suggests broadening the query through the use of proximal subphrases.We evaluated uptake of phrasal query suggestions by analyzing search log data from before and after the suggestion feature was added to a commercial version of the search engine. We looked at approximately 1.5 million queries and found that, after they were added, suggestions represented nearly 30% of the total queries.We evaluated efficacy through a controlled study of 24 participants performing nine searches using three different search engines. We found that the engine with phrase suggestions had better high-precision recall than both the same search engine without suggestions and a search engine with a similar interface but using an Okapi BM25 ranking algorithm.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {841–848},
numpages = {8},
keywords = {proximity search, web search, query Log analysis, user study},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/3250813,
author = {Laender, Alberto H.F.},
title = {Session Details: Poster Session},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250813},
doi = {10.1145/3250813},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321558,
author = {Altingovde, Ismail Sengor and Ozcan, Rifat and Cetintas, Suleyman and Yilmaz, Hakan and Ulusoy, \"{O}zg\"{u}r},
title = {An Automatic Approach to Construct Domain-Specific Web Portals},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321558},
doi = {10.1145/1321440.1321558},
abstract = {We describe the architecture of an automatic domain-specific Web portal construction system. The system has three major components: i) a focused crawler that collects the domain-specific pages on the Web, ii) an information extraction engine that extracts useful fields from these Web pages, and iii) a query engine that allows both typical keyword based queries on the pages and advanced queries on the extracted data fields. We present a prototype system that works for the course homepages domain on the Web. A user study with the prototype system shows that our approach produces high quality results and achieves better precision figures than the typical keyword based search.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {849–852},
numpages = {4},
keywords = {querying, information extraction, focused crawling},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321559,
author = {Bache, Richard and Baillie, Mark and Crestani, Fabio},
title = {Language Models, Probability of Relevance and Relevance Likelihood},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321559},
doi = {10.1145/1321440.1321559},
abstract = {This paper proposes a measure of relevance likelihood derived specifically for language models. Such a measure may be used to guide a user on how far to browse through the list of retrieved items or for pseudo-relevance feedback. To derive this measure, it is necessary to make the assumption that a user is seeking an ideal (usually non-existent) document and the actual relevant documents in the collection will contain fragments of this ideal document. Thus, in deriving this measure we propose a novel way of capturing relevance in Language Modelling.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {853–856},
numpages = {4},
keywords = {language modelling, relevance likelihood, ranking function},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321560,
author = {Bast, Holger and Majumdar, Debapriyo and Weber, Ingmar},
title = {Efficient Interactive Query Expansion with Complete Search},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321560},
doi = {10.1145/1321440.1321560},
abstract = {We present an efficient realization of the following interactive search engine feature: as the user is typing the query, words that are related to the last query word and that would lead to good hits are suggested, as well as selected such hits. The realization has three parts: (i) building clusters of related terms, (ii) adding this information as artificial words to the index such that (iii) the described feature reduces to an instance of prefix search and completion. An efficient solution for the latter is provided by the CompleteSearch engine, with which we have integrated the proposed feature. For building the clusters of related terms we propose a variant of latent semantic indexing that, unlike standard approaches, is completely transparent to the user. By experiments on two large test-collections, we demonstrate that the feature is provided at only a slight increase in query processing time and index size.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {857–860},
numpages = {4},
keywords = {wordnet, query expansion, wikipedia, interactive, index building, synsets},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321561,
author = {Bloehdorn, Stephan and Moschitti, Alessandro},
title = {Structure and Semantics for Expressive Text Kernels},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321561},
doi = {10.1145/1321440.1321561},
abstract = {Several Text Categorization applications require a representation beyond the standard bag-of-words paradigm. Kernel-based learning has approached this problem by (i) considering information about syntactic structure or by (ii) incorporating knowledge about the semantic similarity of term features. We propose a generalized framework consisting of a family of kernels that jointly incorporate syntactic and semantic similarity and demonstrate the power of this approach in a series of experiments.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {861–864},
numpages = {4},
keywords = {text classification, machine learning, kernel methods},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321562,
author = {Breitman, Karin K. and Barbosa, Simone D. J. and Casanova, Marco A. and Furtado, Antonio L.},
title = {Conceptual Modeling by Analogy and Metaphor},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321562},
doi = {10.1145/1321440.1321562},
abstract = {Metaphor is not merely a rhetorical device, characteristic of language alone, but rather a fundamental feature of the human conceptual system. A metaphor is understood by finding an analogy mapping between two domains. This paper argues that analogy mappings facilitate conceptual modeling by allowing the designer to reinterpret fragments of familiar conceptual models in other contexts. The contributions of the paper are expressed within the tradition of the Entity-Relation model.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {865–868},
numpages = {4},
keywords = {analogy, metaphor, entity-relationship model},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321563,
author = {Brinkmeier, Michael and Werner, Jeremias and Recknagel, Sven},
title = {Communities in Graphs and Hypergraphs},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321563},
doi = {10.1145/1321440.1321563},
abstract = {In this paper we define a type of cohesive subgroups - called communities - in hypergraphs, based on the edge connectivity of subhypergraphs. We describe a simple algorithm for the construction of these sets and show, based on examples from image segmentation and information retrieval, that these groups may be useful for the analysis and accessibility of large graphs and hypergraphs.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {869–872},
numpages = {4},
keywords = {document collection, components, edge-connectivity, cohesive group, community, graph, image segmentation, hypergraph},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321564,
author = {Carterette, Ben and Allan, James},
title = {Semiautomatic Evaluation of Retrieval Systems Using Document Similarities},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321564},
doi = {10.1145/1321440.1321564},
abstract = {Semiautomatic evaluation of retrieval systems using document similarities.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {873–876},
numpages = {4},
keywords = {evaluation, clustering, information retrieval, test collections},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321565,
author = {Fortuna, Blaz and Rodrigues, Eduarda Mendes and Milic-Frayling, Natasa},
title = {Improving the Classification of Newsgroup Messages through Social Network Analysis},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321565},
doi = {10.1145/1321440.1321565},
abstract = {Improving the classification of newsgroup messages through social network analysis.In this paper, we focus on automatic classification of message replies into several types. For representing messages we consider rich feature sets that combine the standard author reply-to network properties with features derived from four additional structures identified in the data: 1) a network of authors who participate in the same threads, 2) network of authors who post similar content, 3) network of threads sharing common authors, and 4) network of content-related threads.For selected newsgroups we train linear SVM classifiers to identify agreement and disagreement with the original message, and question and answer patterns in the threads. We show that the use of newly defined features substantially improves classification of messages in comparison with the SVM model based only on the standard reply-to network.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {877–880},
numpages = {4},
keywords = {communities, social networks, newsgroups, message classification},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321566,
author = {Fu, Yupeng and Xiang, Rongjing and Liu, Yiqun and Zhang, Min and Ma, Shaoping},
title = {A CDD-Based Formal Model for Expert Finding},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321566},
doi = {10.1145/1321440.1321566},
abstract = {Searching an organization's document repositories for experts is a frequently faced problem in intranet information management. This paper proposes a candidate-centered model which is referred as Candidate Description Document (CDD)-based retrieval model. The expertise evidence about an expert candidate scattered over repositories is mined and aggregated automatically to form a profile called the candidate's CDD, which represents his knowledge. We present the model from its foundations through its logical development and argue in favor of this model for expert finding. We devise and compare the different strategies for exploring a variety of expertise evidence. The experiments on TREC enterprise corpora demonstrate that the CDD-based model achieves significant and consistent improvement on performance through comparative studies with non-CDD methods.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {881–884},
numpages = {4},
keywords = {expertise modeling, knowledge management, expert finding},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321567,
author = {Garakani, Vahid and Izadi, Sayyed Kamyar and Haghjoo, Mostafa and Harizi, Mohammad},
title = {Ntjfsat¬: A Novel Method for Query with Not-Predicates on Xml Data},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321567},
doi = {10.1145/1321440.1321567},
abstract = {Previous researches, in the filed of XML databases, have been done to evaluate XML queries with AND-branches. However, as far as we know, very little work has examined the efficient processing of XML queries with NOT-predicates. Also these methods have to process all of the query nodes in the document when dealing with queries with NOT-branch. In this paper, with some modification in TJFast method, we propose a new manner for answering to various NOT-queries. This method processes nodes efficiently, in a way that in the ideal state, we obtain part of the answer after the process of each node, and we don't have any unreasonable processing of each node.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {885–888},
numpages = {4},
keywords = {tjfast, not-predicates, dewey encoding, xml, twig joins},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321568,
author = {Goldstein, Jade and Ciany, Gary M. and Carbonell, Jaime G.},
title = {Genre Identification and Goal-Focused Summarization},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321568},
doi = {10.1145/1321440.1321568},
abstract = {In this paper, we present a novel technique of first performing document genre identification, then utilizing the genre for producing tailored summaries based on a user's information seeking needs - genre oriented goal-focused summarization - such as a plot or opinion summary of a movie review. We create a test corpus to determine genre classification accuracy for 16 genres, and examine performance on various amounts of training data for machine learning algorithms - Random Forests, SVM light and Na\"{\i}ve Bayes. Results show that Random Forests outperforms SVM light and Na\"{\i}ve Bayes. The genre tag is used to inform a downstream summarization engine. We define types of summaries for 7 genres, create a ground truth corpus and analyze the results of genre oriented goal-focused summarization, showing that this type of user based summarization requires different algorithms than the leading sentence baseline which is known to perform well in the case of news articles.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {889–892},
numpages = {4},
keywords = {genre identification, evaluation, data mining, text classification, text categorization, task-based information retrieval, metadata extraction, summarization, machine learning},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321569,
author = {Hlaoua, Lobna and Boughanem, Mohand and Pinel-Sauvagnat, Karen},
title = {Combination of Evidences in Relevance Feedback for Xml Retrieval},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321569},
doi = {10.1145/1321440.1321569},
abstract = {The main objective in XML Retrieval is to select the relevant elements of XML document instead of the whole document. Many open issues appear when considering Relevance Feedback (RF) in XML documents. They are mainly related to the form of XML documents, which mix content and structure information and to the new information granularity. In this paper, a new flexible method of relevance feedback in XML retrieval using two sources of evidence is described. We propose to use the context criterion to select terms to extend the initial query and to use generative structures to express structural constraints. Both approaches are applied in different combined forms. Experiments are carried out with the INEX evaluation campaign and results show the effectiveness of our approach.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {893–896},
numpages = {4},
keywords = {combined evidences, context, structure, relevance feedback, xml retrieval},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321570,
author = {Houle, Michael Edward and Grira, Nizar},
title = {A Correlation-Based Model for Unsupervised Feature Selection},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321570},
doi = {10.1145/1321440.1321570},
abstract = {We propose a new model for feature evaluation and selection that assesses the propensity of the features to support two-set classification. For each item of the data set, the collection of features induce a ranking (ordered list) of the remaining items. The evaluation criterion favors features that result in the most consistent discrimination between relevant and non-relevant items within these ranked lists. The discrimination boundaries within a single list are determined combinatorially, according to the degree of correlation among the relevant sets of its members. The model makes no special assumptions on the nature of the data. A selection heuristic based on the model is also proposed using sequential forward generation, and an experimental comparison is made with other unsupervised feature selection methods.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {897–900},
numpages = {4},
keywords = {clustering, feature selection},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321571,
author = {Hu, Meishan and Sun, Aixin and Lim, Ee-Peng},
title = {Comments-Oriented Blog Summarization by Sentence Extraction},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321571},
doi = {10.1145/1321440.1321571},
abstract = {Much existing research on blogs focused on posts only, ignoring their comments. Our user study conducted on summarizing blog posts, however, showed that reading comments does change one's understanding about blog posts. In this research, we aim to extract representative sentences from a blog post that best represent the topics discussed among its comments. The proposed solution first derives representative words from comments and then selects sentences containing representative words. The representativeness of words is measured using ReQuT (i.e., Reader, Quotation, and Topic). Evaluated on human labeled sentences, ReQuT together with summation-based sentence selection showed promising results.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {901–904},
numpages = {4},
keywords = {blog, requt, sentence selection, comments},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321572,
author = {Jiang, Fianny Ming-fei and Pei, Jian and Fu, Ada Wai-chee},
title = {Ix-Cubes: Iceberg Cubes for Data Warehousing and Olap on Xml Data},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321572},
doi = {10.1145/1321440.1321572},
abstract = {With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {905–908},
numpages = {4},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321573,
author = {Jones, Rosie and Kumar, Ravi and Pang, Bo and Tomkins, Andrew},
title = {"I Know What You Did Last Summer": Query Logs and User Privacy},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321573},
doi = {10.1145/1321440.1321573},
abstract = {We investigate the subtle cues to user identity that may be exploited in attacks on the privacy of users in web search query logs. We study the application of simple classifiers to map a sequence of queries into the gender, age, and location of the user issuing the queries. We then show how these classifiers may be carefully combined at multiple granularities to map a sequence of queries into a set of candidate users that is 300-600 times smaller than random chance would allow. We show that this approach remains accurate even after removing personally identifiable information such as names/numbers or limiting the size of the query log.We also present a new attack in which a real-world acquaintance of a user attempts to identify that user in a large query log, using personal information. We show that combinations of small pieces of information about terms a user would probably search for can be highly effective in identifying the sessions of that user.We conclude that known schemes to release even heavily scrubbed query logs that contain session information have significant privacy risks.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {909–914},
numpages = {6},
keywords = {privacy, query log analysis, k-anonymity},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321574,
author = {Jouini, Khaled and Jomier, Genevi\`{e}ve},
title = {Indexing Multiversion Databases},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321574},
doi = {10.1145/1321440.1321574},
abstract = {An efficient management of multiversion data with branched evolution is crucial for many applications. It requires database designers aware of tradeoffs among index structures and policies. This paper defines a framework and an analysis method for understanding the behavior of different indexing policies. Given data and query characteristics the analysis allows determining the most suitable index structure. The analysis is validated by an experimental study.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {915–918},
numpages = {4},
keywords = {performance analysis, branched and multiversion data, comparison, index structures},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321575,
author = {Jurczyk, Pawel and Agichtein, Eugene},
title = {Discovering Authorities in Question Answer Communities by Using Link Analysis},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321575},
doi = {10.1145/1321440.1321575},
abstract = {Question-Answer portals such as Naver and Yahoo! Answers are quickly becoming rich sources of knowledge on many topics which are not well served by general web search engines. Unfortunately, the quality of the submitted answers is uneven, ranging from excellent detailed answers to snappy and insulting remarks or even advertisements for commercial content. Furthermore, user feedback for many topics is sparse, and can be insufficient to reliably identify good answers from the bad ones. Hence, estimating the authority of users is a crucial task for this emerging domain, with potential applications to answer ranking, spam detection, and incentive mechanism design. We present an analysis of the link structure of a general-purpose question answering community to discover authoritative users, and promising experimental results over a dataset of more than 3 million answers from a popular community QA site. We also describe structural differences between question topics that correlate with the success of link analysis for authority discovery.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {919–922},
numpages = {4},
keywords = {question-answer portals, link analysis},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321576,
author = {Kumaran, Giridhar and Allan, James},
title = {Selective User Interaction},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321576},
doi = {10.1145/1321440.1321576},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {923–926},
numpages = {4},
keywords = {query potential, query expansion, user interaction, query relaxation},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321577,
author = {Liu, Feng and Tian, Fengzhan and Zhu, Qiliang},
title = {Ensembling Bayesian Network Structure Learning on Limited Data},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321577},
doi = {10.1145/1321440.1321577},
abstract = {In recent years, Bagging method has been applied to learn Bayesian networks (BNs), especially on limited datasets. However, the BNs learned using Bagging method from limited datasets can be biased towards complex models. We present an efficient approach to produce more accurate BNs from limited datasets. Based on the Markov condition of BN learning, we proposed a novel sampling method, called Root Nodes based Sampling (RNS), and a BNs fusion method. The experimental results reveal that our ensemble method can achieve more accurate results in terms of accuracy on limited datasets.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {927–930},
numpages = {4},
keywords = {fusion method, ensemble method, bayesian network, sampling method},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321578,
author = {Lv, Jianming and Cheng, Xueqi},
title = {CTO: Concept Tree Based Semantic Overlay for Pure Peer-to-Peer Information Retrieval},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321578},
doi = {10.1145/1321440.1321578},
abstract = {Inspired by how search behavior works in human society, we propose CTO, a self-organized semantic overlay based on concept tree for P2P IR infrastructure, which is efficient for full text search in pure P2P environment without any central control or powerful peer as hub node. Especially, CTO performs very well on searching the unpopular resources shared by a few peers. In our experiment, while searching for the scarce documents shared by the peers, CTO achieves about 80% recall rate when the search covers less than 5% peers in the overlay. The search latency of CTO is also very low, which is controlled in the range about 5~12 hops.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {931–934},
numpages = {4},
keywords = {concept tree, keyword co-occurrences, peer clustering, p2p ir},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321579,
author = {Marin, Mauricio and Gil-Costa, Veronica},
title = {High-Performance Distributed Inverted Files},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321579},
doi = {10.1145/1321440.1321579},
abstract = {We present a general method of parallel query processing that allows scalable performance on distributed inverted files. The method allows the realization of a hybrid that combines the advantages of the document and term partitioned inverted files.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {935–938},
numpages = {4},
keywords = {parallel and distributed computing, inverted files},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321580,
author = {Namata, Galileo Mark and Staats, Brian and Getoor, Lise and Shneiderman, Ben},
title = {A Dual-View Approach to Interactive Network Visualization},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321580},
doi = {10.1145/1321440.1321580},
abstract = {Visualizing network data, from tree structures to arbitrarily connected graphs, is a difficult problem in information visualization. A large part of the problem is that in network data, users not only have to visualize the attributes specific to each data item, but also the links specifying how those items are connected to each other. Past approaches to resolving these difficulties focus on zooming, clustering, filtering and applying various methods of laying out nodes and edges. Such approaches, however, focus only on optimizing a network visualization in a single view, limiting the amount of information that can be shown and explored in parallel. Moreover, past approaches do not allow users to cross reference different subsets or aspects of large, complex networks. In this paper, we propose an approach to these limitations using multiple coordinated views of a given network. To illustrate our approach, we implement a tool called DualNet and evaluate the tool with a case study using an email communication network. We show how using multiple coordinated views improves navigation and provides insight into large networks with multiple node and link properties and types.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {939–942},
numpages = {4},
keywords = {information visualization, interactive graph visualization, social networks, coordinated views},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321581,
author = {Pera, Maria S. and Ng, Yiu-Kai},
title = {Using Word Similarity to Eradicate Junk Emails},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321581},
doi = {10.1145/1321440.1321581},
abstract = {Emails are one of the most commonly used modern communication media these days; however, unsolicited emails obstruct this otherwise fast and convenient technology for information exchange and jeopardize the continuity of this popular communication tool. Waste of valuable resources and time and exposure to offensive content are only a few of the problems that arise as a result of junk emails. In addition, the monetary cost of processing junk emails reaches billions of dollars per year and is absorbed by public users and Internet service providers. Even though there has been extensive work in the past dedicated to eradicate junk emails, none of the existing junk email detection approaches has been highly successful in solving these problems, since spammers have been able to infiltrate existing detection techniques. In this paper, we present a new tool, JunEX, which relies on the content similarity of emails to eradicate junk emails. JunEX compares each incoming email to a core of emails marked as junk by each individual user to identify unwanted emails while reducing the number of legitimate emails treated as junk, which is critical. Conducted experiments on JunEX verify its high accuracy.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {943–946},
numpages = {4},
keywords = {word correlation, similarity detection, junk email},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321582,
author = {Quian\'{e}-Ruiz, Jorge-Arnulfo and Lamarre, Philippe and Cazalens, Sylvie and Valduriez, Patrick},
title = {Satisfaction Balanced Mediation},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321582},
doi = {10.1145/1321440.1321582},
abstract = {We consider a distributed information system that allows autonomous consumers to query autonomous providers. We focus on the problem of query allocation from a new point of view, by considering consumers and providers' satisfaction in addition to query load. We define satisfaction as a long-run notion based on the consumers and providers' preferences. We propose and validate a mediation process, called SBMediation, which is compared to Capacity based query allocation. The experimental results show that SBMediation significantly outperforms Capacity based when confronted to autonomous participants.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {947–950},
numpages = {4},
keywords = {participants' satisfaction, imposition, query allocation, autonomous participants, payment},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321583,
author = {Raghuveer, Aravindan and Jindal, Meera and Mokbel, Mohamed F. and Debnath, Biplob and Du, David},
title = {Towards Efficient Search on Unstructured Data: An Intelligent-Storage Approach},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321583},
doi = {10.1145/1321440.1321583},
abstract = {Applications that create and consume unstructured data have grown both in scale of storage requirements and complexity of search primitives. We consider two such applications: exhaustive search and integration of structured and unstructured data. Current block-based storage systems are either incapable or inefficient to address the challenges bought forth by the above applications. We propose a storage framework to efficiently store and search unstructured and structured data while controlling storage management costs. Experimental results based on our prototype show that the proposed system can provide impressive performance and feature benefits.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {951–954},
numpages = {4},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321584,
author = {Rull, Guillem and Farr\'{e}, Carles and Teniente, Ernest and Urp\'{\i}, Toni},
title = {Computing Explanations for Unlively Queries in Databases},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321584},
doi = {10.1145/1321440.1321584},
abstract = {A query is unlively if it always returns an empty answer. Debugging a database schema requires not only determining unlively queries, but also fixing them. To the best of our knowledge, the existing methods do not provide the designer with an explanation of why a query is not lively. In this paper, we propose a method for computing explanations that is independent of the particular method used to determine liveliness. It provides three levels of search: one explanation, a maximal set of non-overlapping explanations, and all explanations. The first two levels require only a linear number of calls to the underlying method. We also propose a filter to reduce the number of these calls, and experimentally compare our method with the best known method for finding unsatisfiable subsets of constraints.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {955–958},
numpages = {4},
keywords = {query liveliness, database schema validation},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321585,
author = {Sarmento, Luis and Jijkuon, Valentin and de Rijke, Maarten and Oliveira, Eugenio},
title = {"More like These": Growing Entity Classes from Seeds},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321585},
doi = {10.1145/1321440.1321585},
abstract = {We present a corpus-based approach to the class expansion task. For a given set of seed entities we use co-occurrence statistics taken from a text collection to define a membership function that is used to rank candidate entities for inclusion in the set. We describe an evaluation framework that uses data from Wikipedia. The performance of our class extension method improves as the size of the text collection increases.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {959–962},
numpages = {4},
keywords = {list expansion, lexical acquisition},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321586,
author = {Shin, Se Jung and Lee, Won Suk},
title = {An On-Line Interactive Method for Finding Association Rules Data Streams},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321586},
doi = {10.1145/1321440.1321586},
abstract = {In order to trace the changes of association rules over an online data stream efficiently, this paper proposes two different methods of generating all association rules directly over the changing set of currently frequent itemsets. While all of the currently frequent itemsets are monitored by the estDec method, all the association rules of every frequent itemset in the prefix tree of the estDec method are generated. For this purpose, a traversal stack is introduced to efficiently enumerate all association rules. These online methods can avoid the drawbacks of the conventional two-step approach. In an on-line environment, a user may be interested in finding those association rules whose antecedents or consequents are fixed to be a specific itemset. Since generating all the association rules may take too long to produce them timely, two additional methods, namely Assoc-X and Assoc-Y, are introduced. Finally, the proposed methods are compared by a series of experiments to identify their various characteristics.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {963–966},
numpages = {4},
keywords = {frequent itemsets, data streams, data mining, association rules},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321587,
author = {Song, Shaoxu and Chen, Lei},
title = {Probabilistic Correlation-Based Similarity Measure of Unstructured Records},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321587},
doi = {10.1145/1321440.1321587},
abstract = {Computing the similarity between unstructured records is a fundamental function in multiple applications. Approximate string matching and full text retrieval techniques do not show the best performance when applied directly, since the information are limited in unstructured records of short record length. In this paper, we propose a novel probabilistic correlation-based similarity measure. Rather than simply conducting the exact matching tokens of two records, our similarity evaluation enriches the information of records by considering the correlations of tokens. We define the probabilistic correlation between tokens as the probability that these tokens appear in the same records. Then we compute the weight of tokens and discover the correlations of records based on the probabilistic correlations of tokens. Finally, we present extensive experimental results to demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {967–970},
numpages = {4},
keywords = {probabilistic correlation, record similarity},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321588,
author = {Song, Xiaodan and Chi, Yun and Hino, Koji and Tseng, Belle},
title = {Identifying Opinion Leaders in the Blogosphere},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321588},
doi = {10.1145/1321440.1321588},
abstract = {Opinion leaders are those who bring in new information, ideas, and opinions, then disseminate them down to the masses, and thus influence the opinions and decisions of others by a fashion of word of mouth. Opinion leaders capture the most representative opinions in the social network, and consequently are important for understanding the massive and complex blogosphere. In this paper, we propose a novel algorithm called InfluenceRank to identify opinion leaders in the blogosphere. The InfluenceRank algorithm ranks blogs according to not only how important they are as compared to other blogs, but also how novel the information they can contribute to the network. Experimental results indicate that our proposed algorithm is effective in identifying influential opinion leaders.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {971–974},
numpages = {4},
keywords = {network summarization, blog ranking, opinion leader},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321589,
author = {Song, Yangqiu and Zhang, Bin and Yin, Wenjun and Zhang, Changshui and Dong, Jin},
title = {Ranking with Semi-Supervised Distance Metric Learning and Its Application to Housing Potential Estimation},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321589},
doi = {10.1145/1321440.1321589},
abstract = {This paper proposes a semi-supervised distance metric learning algorithm for the ranking problem. Instead of giving the computer what are the important factors that affect the final rank value, we only give several most certainly ranked points which implicitly contain the knowledge of the ranking factors. Then the computer can automatically use the most certain points and plenty of unlabeded data to learn an informative metric for ranking. This metric not only can help to regress an order in the observed data, but also can be used to retrieve the data by querying new test points. Moreover, the lower-rank distance metric can be used to visualize high-dimensional data. We also present an application to the housing potential estimation problem. It is shown that the algorithm is efficient to help consultants to refine their consulting work.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {975–978},
numpages = {4},
keywords = {dimensionality reduction, information retrieval, ranking, metric learning, semi-supervised learning},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321590,
author = {Tan, Songbo and Wu, Gaowei and Tang, Huifeng and Cheng, Xueqi},
title = {A Novel Scheme for Domain-Transfer Problem in the Context of Sentiment Analysis},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321590},
doi = {10.1145/1321440.1321590},
abstract = {In this work, we attempt to tackle domain-transfer problem by combining old-domain labeled examples with new-domain unlabeled ones. The basic idea is to use old-domain-trained classifier to label some informative unlabeled examples in new domain, and retrain the base classifier over these selected examples. The experimental results demonstrate that proposed scheme can significantly boost the accuracy of the base sentiment classifier on new domain.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {979–982},
numpages = {4},
keywords = {opinion mining, information retrieval, sentiment classification},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321591,
author = {Trotman, Andrew and Subramanya, Vikram},
title = {Sigma Encoded Inverted Files},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321591},
doi = {10.1145/1321440.1321591},
abstract = {Compression of term frequency lists and very long document-id lists within an inverted file search engine are examined. Several compression schemes are compared including Elias γ and δ codes, Golomb Encoding, Variable Byte Encoding, and a class of word-based encoding schemes including Simple-9, Relative-10 and Carryover-12. It is shown that these compression methods are not well suited to compressing these kinds of lists of numbers. Of those tested, Carryover-12 is preferred because it is both effective at compression and fast at decompression.A novel technique, Sigma Encoding prior to compression, is proposed and tested. Sigma Encoding utilizes a parameterized dictionary to reduce the number of bits necessary to store an integer. This method shows an about 0.3 bit per integer improvement over Carryover-12 while costing only about 3 extra clock cycles per integer to decompress.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {983–986},
numpages = {4},
keywords = {compression, inverted files},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321592,
author = {Tsegay, Yohannes and Turpin, Andrew and Zobel, Justin},
title = {Dynamic Index Pruning for Effective Caching},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321592},
doi = {10.1145/1321440.1321592},
abstract = {RAM and dynamic pruning schemes to reduce query evaluation times. While only a small portion of lists are processed with dynamic pruning, current systems still store the entire inverted list in cache. In this paper we investigate caching only the pieces of the inverted lists that are actually used to answer a query during dynamic pruning. We examine an LRU cache model, and two recently proposed models. We also introduce a new dynamic pruning scheme for impact-ordered inverted lists.Using two large web collections and corresponding query logs we show that, using an LRU cache, our new pruning scheme reduces the number of disk accesses during query processing time by 7%-15% over the state-of-the-art impact-ordered baseline, without reducing answer quality. Surprisingly, however, we discover that using our new pruning scheme makes little difference to disk traffic when the more sophisticated caching schemes are employed.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {987–990},
numpages = {4},
keywords = {inverted lists, search engines, caching, efficiency},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321593,
author = {Wang, Xuanhui and Fang, Hui and Zhai, ChengXiang},
title = {Improve Retrieval Accuracy for Difficult Queries Using Negative Feedback},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321593},
doi = {10.1145/1321440.1321593},
abstract = {How to improve search accuracy for difficult topics is an under-addressed, yet important research question. In this paper, we consider a scenario when the search results are so poor that none of the top-ranked documents is relevant to a user's query, and propose to exploit negative feedback to improve retrieval accuracy for such difficult queries. Specifically, we propose to learn from a certain number of top-ranked non-relevant documents to rerank the rest unseen documents. We propose several approaches to penalizing the documents that are similar to the known non-relevant documents in the language modeling framework. To evaluate the proposed methods, we adapt standard TREC collections to construct a test collection containing only difficult queries. Experiment results show that the proposed approaches are effective for improving retrieval accuracy of difficult queries.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {991–994},
numpages = {4},
keywords = {difficult queries, negative feedback, language modeling},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321594,
author = {Wang, Yong and Gong, Shaogang},
title = {Translating Topics to Words for Image Annotation},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321594},
doi = {10.1145/1321440.1321594},
abstract = {One of the classic techniques for image annotation is the language translation model. It views an image as a document, i.e., a set of visual words which are obtained by vector quatitizing the image regions generated by unsupervised image segmentation. Annotating images are achieved by translating visual words to textual words, just like translating a document in English to a document in French. In this paper, we also view an image as a document, but we view the annotation processes as two consecutive processes, i.e., document summarization and translation. In the document summarization process, an image document is firstly summarized into its own visual language, which we called visual topics. The translation process translates these visual topics to textual words. Compared to the original translation model, our visual topics learned by the probabilistic latent semantic analysis (PLSA) approach provide an intermediate abstract level of visual description. We show improved annotation performance on the Corel image dataset.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {995–998},
numpages = {4},
keywords = {translation model, probabilistic latent sematic analysis, automatic image annotation},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inbook{10.1145/1321440.1321595,
author = {Wu, Youzheng and Hu, Xinhui and Kashioka, Hideki},
title = {Mining Redundancy in Candidate-Bearing Snippets to Improve Web Question Answering},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321595},
abstract = {Conventional question answering (QA) techniques independently process candidate-bearing snippets to select an exact answer to a question from candidate answers. This paper presents two novel ways of utilizing redundancy in candidate-bearing snippets to help select an exact answer to a question in our Web QA system, i.e., cluster-based language model (CLM-M) and unsupervised SVM classifier (U-SVM) techniques. The comparative experiments demonstrate that the proposed methods significantly outperform the language model-based (LM-M) and supervised SVM-based (S-SVM) techniques that do not utilize this redundancy in the candidate-bearing snippets. Using the CLM-M, the top_1 score is increased from 36.03% (LM-M) to 46.96%; and the top_1 improvement in the U-SVM over the S-SVM is about 23%. Moreover, a cross-model comparison shows that the performance ranking of these models is: U-SVM &gt; CLM-LM &gt; LM-M &gt; S-SVM &gt; R-M (the retrieval-based model).},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {999–1002},
numpages = {4}
}

@inproceedings{10.1145/1321440.1321596,
author = {Xu, Shengliang and Bao, Shenghua and Cao, Yunbo and Yu, Yong},
title = {Using Social Annotations to Improve Language Model for Information Retrieval},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321596},
doi = {10.1145/1321440.1321596},
abstract = {This poster is concerned with the problem of exploring the use of social annotations for improving language models for information retrieval (denoted as LMIR). Two properties of social annotations, namely keyword property and structure property are studied for this aim. The keyword property improves LMIR by concatenating all the annotations of a document to generate a summary of the document. The structure property can boost LMIR further when similarity among annotations and similarity among documents are taken into consideration simultaneously. The two properties of social annotations are leveraged for the use of language modeling with a mixture model named as "Language Annotation Model" (denoted as LAM). Evaluations using del.icio.us data show that LAM outperforms the traditional LMIR approaches significantly.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {1003–1006},
numpages = {4},
keywords = {information retrieval, social annotation, language model},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321597,
author = {Xu, Yu and Papakonstantinou, Yannis},
title = {Efficient LCA Based Keyword Search in Xml Data},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321597},
doi = {10.1145/1321440.1321597},
abstract = {Keyword search in XML documents based on the notion of lowest common ancestors (LCAs) and modifications of it has recently gained research interest [2, 3, 4]. In this paper we propose an efficient algorithm called Indexed Stack to find answers to keyword queries based on XRank's semantics to LCA [2]. The complexity of the Indexed Stack algorithm is O(kd|S1|log|S|) where k is the number of keywords in the query, d is the depth of the tree and |S1 | (|S|) is the occurrence of the least (most) frequent keyword in the query. In comparison, the best worst case complexity of the core algorithms in [2] is O(kd|S|). We analytically and experimentally evaluate the Indexed Stack algorithm and the two core algorithms in [2]. The results show that the Indexed Stack algorithm outperforms in terms of both CPU and I/O costs other algorithms by orders of magnitude when the query contains at least one low frequency keyword along with high frequency keywords.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {1007–1010},
numpages = {4},
keywords = {search, xml, LCA, keyword},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321598,
author = {Yang, Lei and Qi, Lei and Zhao, Yan-Ping and Gao, Bin and Liu, Tie-Yan},
title = {Link Analysis Using Time Series of Web Graphs},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321598},
doi = {10.1145/1321440.1321598},
abstract = {Link analysis is a key technology in contemporary web search engines. Most of the previous work on link analysis only used information from one snapshot of web graph. Since commercial search engines crawl the Web periodically, they will naturally obtain time series data of web graphs. The historical information contained in the series of web graphs can be used to improve the performance of link analysis. In this paper, we argue that page importance should be a dynamic quantity, and propose defining page importance as a function of both PageRank of the current web graph and accumulated historical page importance from previous web graphs. Specifically, a novel algorithm named TemporalRank is designed to compute the proposed page importance. We try to use a kinetic model to interpret this page importance and show that it can be regarded as the solution to an ordinary differential equation. Experiments on link analysis using web graph data in five snapshots show that the proposed algorithm can outperform PageRank in many measures, and can effectively filter out newly appeared link spam websites.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {1011–1014},
numpages = {4},
keywords = {page importance, search engine, pagerank, link analysis, temporal information},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321599,
author = {Zaragoza, Hugo and Rode, Henning and Mika, Peter and Atserias, Jordi and Ciaramita, Massimiliano and Attardi, Giuseppe},
title = {Ranking Very Many Typed Entities on Wikipedia},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321599},
doi = {10.1145/1321440.1321599},
abstract = {We discuss the problem of ranking very many entities of different types. In particular we deal with a heterogeneous set of types, some being very generic and some very specific. We discuss two approaches for this problem: i) exploiting the entity containment graph and ii) using a Web search engine to compute entity relevance. We evaluate these approaches on the real task of ranking Wikipedia entities typed with a state-of-the-art named-entity tagger. Results show that both approaches can greatly increase the performance of methods based only on passage retrieval.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {1015–1018},
numpages = {4},
keywords = {entity retrieval, wikipedia, entity graphs},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321600,
author = {Zhang, Duo and Tang, Jie and Li, Juanzi and Wang, Kehong},
title = {A Constraint-Based Probabilistic Framework for Name Disambiguation},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321600},
doi = {10.1145/1321440.1321600},
abstract = {This paper is concerned with the problem of name disambiguation. By name disambiguation, we mean distinguishing persons with the same name. It is a critical problem in many knowledge management applications. Despite much research work has been conducted, the problem is still not resolved and becomes even more serious, in particular with the popularity of Web 2.0. Previously, name disambiguation was often undertaken in either a supervised or unsupervised fashion. This paper first gives a constraint-based probabilistic model for semi-supervised name disambiguation. Specifically, we focus on investigating the problem in an academic researcher social network (http://arnetminer.org). The framework combines constraints and Euclidean distance learning, and allows the user to refine the disambiguation results. Experimental results on the researcher social network show that the proposed framework significantly outperforms the baseline method using unsupervised hierarchical clustering algorithm.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {1019–1022},
numpages = {4},
keywords = {digital library, name disambiguation, social network analysis, semi-supervised clustering},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321601,
author = {Zhang, Qi and Wang, Wei},
title = {An Efficient Algorithm for Approximate Biased Quantile Computation in Data Streams},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321601},
doi = {10.1145/1321440.1321601},
abstract = {We propose an efficient algorithm for approximate biased quantile computation in large data streams. Our algorithm computes decomposable biased quantile summaries on fixed sized blocks and dynamically maintains the biased quantile summary for the entire stream as the exponential histogram over the block-wise quantile summaries. The algorithm is computationally efficient and achieves an amortized computational cost of O(log(1⁄∈log(∈n))) and a space requirement of O(log3∈n↬∈). Our algorithm does not assume prior knowledge of the stream sizes or the range of data values in the streams. In practice, our algorithm is able to efficiently maintain summaries over large data streams with over tens of millions of observations and achieves significant performance improvement over prior algorithms.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {1023–1026},
numpages = {4},
keywords = {streaming algorithms, biased quantiles},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@inproceedings{10.1145/1321440.1321602,
author = {Zhou, Xiaohua and Hu, Xiaohua and Zhang, Xiaodan and Shen, Xiajiong},
title = {A Segment-Based Hidden Markov Model for Real-Setting Pinyin-to-Chinese Conversion},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321602},
doi = {10.1145/1321440.1321602},
abstract = {Hidden markov model (HMM) is frequently used for Pinyin-to-Chinese conversion. But it only captures the dependency with the preceding character. Higher order markov models can bring higher accuracy, but are computationally unaffordable to average PC settings. We propose a segment-based hidden markov model (SHMM), which has the same magnitude of complexity as first-order HMM, but generates higher decoding accuracy. SHMM tells a word from a bigram connecting two words, and assigns a reasonable probability to words as a whole. It is more powerful than HMM to decode words containing over two characters. We conduct a comprehensive Pinyin-to-Chinese conversion evaluation on Lancaster corpus. The experiment shows the perfect sentence accuracy is improved from 34.7% (HMM) to 43.3% (SHMM). The one-error sentence accuracy is increased from 72.7% to 78.3%. Furthermore, SHMM can seamlessly integrate with pinyin typing correction, acronym pinyin input, user-defined words, and self-adaptive learning all of which are a must for a commercial Pinyin-to-Chinese conversion product in order to improve the efficiency of pinyin input.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {1027–1030},
numpages = {4},
keywords = {chinese input, segment-based hidden markov model, pinyin},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

