@inproceedings{10.1145/502585.502587,
author = {Ferhatosmanoglu, Hakan and Agrawal, Divyakant and Abbadi, Amr El},
title = {Efficient Processing of Conical Queries},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502587},
doi = {10.1145/502585.502587},
abstract = {Conical queries are a novel type of query with an increasing number of applications. Traditional index structures and retrieval mechanisms, in general, have been optimized for rectangular and circular queries, rather than conical queries. In this paper, we focus on conical queries which can be defined as a multi-dimensional cone in a multi-dimensional data space. We develop a model for expressing such queries and suggest efficient techniques for evaluating them. In particular, we explore the retrieval problem in the context of conical query processing and develop multi-disk allocation methods specifically for processing conical queries.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {1–8},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502588,
author = {Kim, Sang-Wook and Aggarwal, Charu C. and Yu, Philip S.},
title = {Effective Nearest Neighbor Indexing with the Euclidean Metric},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502588},
doi = {10.1145/502585.502588},
abstract = {The nearest neighbor search is an important operation widely-used in multimedia databases. In higher dimensions, most of previous methods for nearest neighbor search become inefficient and require to compute nearest neighbor distances to a large fraction of points in the space. In this paper, we present a new approach for processing nearest neighbor search with the Euclidean metric, which searches over only a small subset of the original space. This approach effectively approximates clusters by encapsulating them into geometrically regular shapes and also computes better upper and lower bounds of the distances from the query point to the clusters. For showing the effectiveness of the proposed approach, we perform extensive experiments. The results reveal that the proposed approach significantly outperforms the X-tree as well as the sequential scan.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {9–16},
numpages = {8},
keywords = {high dimensional indexes, Euclidean metric, nearest neighbor queries, multimedia databases, similarity search},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502589,
author = {Tombros, Anastasios and van Rijsbergen, C. J.},
title = {Query-Sensitive Similarity Measures for the Calculation of Interdocument Relationships},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502589},
doi = {10.1145/502585.502589},
abstract = {The application of document clustering to information retrieval has been motivated by the potential effectiveness gains postulated by the Cluster Hypothesis. The hypothesis states that relevant documents tend to be highly similar to each other, and therefore tend to appear in the same clusters. In this paper we propose that, for any given query, pairs of relevant documents will exhibit an inherent similarity which is dictated by the query itself. Our research describes an attempt to devise means by which this similarity can be detected. We propose the use of query-sensitive similarity measures that bias interdocument relationships towards pairs of documents that jointly possess attributes that are expressed in a query. We experimentally tested query-sensitive measures against conventional ones that do not take the context of the query into account. We calculated interdocument relationships for varying numbers of top-ranked documents for five document collections. Our results show a consistent and significant increase in the number of relevant documents that become nearest neighbours of any given relevant document when query-sensitive measures are used. These results suggest that the effectiveness of a cluster-based IR system has the potential to increase through the use of query-sensitive similarity measures.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {17–24},
numpages = {8},
keywords = {query-sensitive similarity measures, information retrieval, cluster hypothesis, document clustering},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502591,
author = {Zha, Hongyuan and He, Xiaofeng and Ding, Chris and Simon, Horst and Gu, Ming},
title = {Bipartite Graph Partitioning and Data Clustering},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502591},
doi = {10.1145/502585.502591},
abstract = {Many data types arising from data mining applications can be modeled as bipartite graphs, examples include terms and documents in a text corpus, customers and purchasing items in market basket analysis and reviewers and movies in a movie recommender system. In this paper, we propose a new data clustering method based on partitioning the underlying bipartite graph. The partition is constructed by minimizing a normalized sum of edge weights between unmatched pairs of vertices of the bipartite graph. We show that an approximate solution to the minimization problem can be obtained by computing a partial singular value decomposition (SVD) of the associated edge weight matrix of the bipartite graph. We point out the connection of our clustering algorithm to correspondence analysis used in multivariate analysis. We also briefly discuss the issue of assigning data objects to multiple clusters. In the experimental results, we apply our clustering algorithm to the problem of document clustering to illustrate its effectiveness and efficiency.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {25–32},
numpages = {8},
keywords = {document clustering, bipartite graph, singular value decomposition, correspondence analysis, spectral relaxation, graph partitioning},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502592,
author = {Leuski, Anton},
title = {Evaluating Document Clustering for Interactive Information Retrieval},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502592},
doi = {10.1145/502585.502592},
abstract = {We consider the problem of organizing and browsing the top ranked portion of the documents returned by an information retrieval system. We study the effectiveness of a document organization in helping a user to locate the relevant material among the retrieved documents as quickly as possible. In this context we examine a set of clustering algorithms and experimentally show that a clustering of the retrieved documents can be significantly more effective than traditional ranked list approach. We also show that the clustering approach can be as effective as the interactive relevance feedback based on query expansion while retaining an important advantage -- it provides the user with a valuable sense of control over the feedback process.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {33–40},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502593,
author = {Azcarraga, Arnulfo P. and Yap, Teddy N.},
title = {Extracting Meaningful Labels for WEBSOM Text Archives},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502593},
doi = {10.1145/502585.502593},
abstract = {Self-Organizing Maps, being used mainly with data that are not pre-labeled, need automatic procedures for extracting keywords as labels for each of the map units. The WEBSOM methodology for building very large text archives has a very slow method for extracting such unit labels. It computes the relative frequencies of all the words of all the documents associated to each unit and then compares these to the relative frequencies of all the words of all the other units of the map. Since maps may have more than 100,000 units and the archive may contain up to 7 million documents, the existing WEBSOM method is not practical. This paper describes how the meaningful labels per map unit can be deduced by analyzing the relative weight distribution of the SOM weight vectors and by taking advantage of some characteristics of the random projection method used in dimensionality reduction. The effectiveness of this technique is demonstrated on archives of the well studied Reuters and CNN collections. Comparisons with the WEBSOM method are provided.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {41–48},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502595,
author = {Haase, Oliver and Henrich, Andreas},
title = {Exposing the Vagueness of Query Results on Partly Inaccessible Databases},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502595},
doi = {10.1145/502585.502595},
abstract = {Query processing on partly inaccessible databases generally does not yield exact, but vague result sets. A good notion of vague sets fulfills two aims: It keeps the degree of vagueness of the query result as small as possible, and it clarifies the degree of and the reasons for the vagueness to the end user. The first goal requires a good internal representation, while the second goal requires a good external representation of a vague set. In this paper, we present a novel calculus for expressive vague sets that meets both requirements. This is the first approach that is well suited for both internal and external representation of vagueness induced by partial inaccessibility. It consists of a data representation that is capable of holding all the necessary information. Complementary, we have accordingly adapted the usual query language operations. These adaptations are independent of a concrete query language, to make them applicable to most existing query languages. The adapted operations minimize the vagueness of the result, propagate the reasons of uncertainty of the individual vague candidates, and compute an expressive description of the missing elements.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {49–56},
numpages = {8},
keywords = {inaccessibility, query processing, vagueness},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502596,
author = {Jamil, Hasan M. and Modica, Giovanni A. and Teran, Maria A.},
title = {Towards a Visual Query Interface for Phylogenetic Databases},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502596},
doi = {10.1145/502585.502596},
abstract = {Querying and visualization of phylogenetic databases remain a great challenge due to their complex tree type semi structured nature. Naturally, successful phylogenetic databases such as the Tree of Life database at the University of Arizona are implemented as Web documents in HTML. While Web implementation of such databases facilitate the representation, and in part, visualization of their contents, querying remains an issue. The interoperability of Web-based phylogenetic databases with other similar databases such as TreeBase and RDB which are implemented using traditional database management systems, has not been possible due to the impedance mismatch between the underlying query and data representation framework. In this paper, we present a novel approach to phylogentic database management using existing database technologies without compromising potential opportunities for visualization and interoperability. We present a Web-based tool for the creation, querying and visualization of phylogenetic databases. We demonstrate the functional capabilities and strengths of our system by recreating the Tree of Life database in our system and performing queries that are not possible in the original Tree of Life database.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {57–64},
numpages = {8},
keywords = {information retrieval, tree and semi structured data, phylogenies, visualization, query language, relational database, web},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502597,
author = {Wyss, Catharine and Van Gucht, Dirk},
title = {A Relational Algebra for Data/Metadata Integration in a Federated Database System},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502597},
doi = {10.1145/502585.502597},
abstract = {The need for interoperability among databases has increased dramatically with the proliferation of readily available DBMS and application software. Even within a single organization, data from disparate relational databases must be integrated. A framework for interoperability in a federated system of relational databases should be inherently relational, so that it can use existing techniques for query evaluation and optimization where possible and retain the key features of SQL, such as a modest complexity and ease of query formulation. Our contribution is a logspace relational algebra, the Meta-Algebra (MA), for data/metadata integration among relational databases containing semantically similar information in schematically disparate formats. The MA is a simple yet powerful extension of the classical relational algebra (RA). The MA has a natural declarative counterpart, the Meta-Query Language (MQL), which we briefly describe. We state a result showing MQL and the MA are computationally equivalent, which enables us to algebratize MQL queries in fundamentally the same way as ordinary SQL queries. This algebratization in turn enables us to use MA equivalences to facilitate the application of known query optimization techniques to MQL query evaluation.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {65–72},
numpages = {8},
keywords = {federated information system (FIS), federated data-base systems, multidatabase, database integration, schema transparency, interoperability, query languages, relation-al algebra, metadata, database schema integration, metaquery},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502599,
author = {Vilares, M. and Ribadas, F. J. and Gra\~{n}a, J.},
title = {Approximately Common Patterns in Shared-Forests},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502599},
doi = {10.1145/502585.502599},
abstract = {We present a proposal intended to demonstrate the applicability of tabulation techniques for detecting approximately common patterns when dealing with structures sharing some common parts. This sharing saves on the space needed to represent the structures and also on their later processing, by factorizing the filtering of substructure matching. As a consequence, preliminary experimental tests indicate a reduction of the running time.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {73–80},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502600,
author = {Pinto, Helen and Han, Jiawei and Pei, Jian and Wang, Ke and Chen, Qiming and Dayal, Umeshwar},
title = {Multi-Dimensional Sequential Pattern Mining},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502600},
doi = {10.1145/502585.502600},
abstract = {Sequential pattern mining, which finds the set of frequent subsequences in sequence databases, is an important data-mining task and has broad applications. Usually, sequence patterns are associated with different circumstances, and such circumstances form a multiple dimensional space. For example, customer purchase sequences are associated with region, time, customer group, and others. It is interesting and useful to mine sequential patterns associated with multi-dimensional information.In this paper, we propose the theme of multi-dimensional sequential pattern mining, which integrates the multidimensional analysis and sequential data mining. We also thoroughly explore efficient methods for multi-dimensional sequential pattern mining. We examine feasible combinations of efficient sequential pattern mining and multi-dimensional analysis methods, as well as develop uniform methods for high-performance mining. Extensive experiments show the advantages as well as limitations of these methods. Some recommendations on selecting proper method with respect to data set properties are drawn.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {81–88},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502601,
author = {Wang, Ke and He, Yu and Cheung, David W.},
title = {Mining Confident Rules without Support Requirement},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502601},
doi = {10.1145/502585.502601},
abstract = {An open problem is to find all rules that satisfy a minimum confidence but not necessarily a minimum support. Without the support requirement, the classic support-based pruning strategy is inapplicable. The problem demands a confidence-based pruning strategy. In particular, the following monotonicity of confidence, called the universal-existential upward closure, holds: if a rule of size k is confident (for the given minimum confidence), for every other attribute not in the rule, some specialization of size k+1 using the attribute must be confident. Like the support-based pruning, the bottleneck is at the memory that often is too small to store the candidates required for search. We implement this strategy on disk and study its performance.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {89–96},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502603,
author = {Al-Kofahi, Khalid and Tyrrell, Alex and Vachher, Arun and Travers, Tim and Jackson, Peter},
title = {Combining Multiple Classifiers for Text Categorization},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502603},
doi = {10.1145/502585.502603},
abstract = {A major problem facing online information services is how to index and supplement large document collections with respect to a rich set of categories. We focus upon the routing of case law summaries to various secondary law volumes in which they should be cited. Given the large number (&gt; 13,000) of closely related categories, this is a challenging task that is unlikely to succumb to a single algorithmic solution. Our fully implemented and recently deployed system shows that a superior classification engine for this task can be constructed from a combination of classifiers. The multi-classifier approach helps us leverage all the relevant textual features and meta data, and appears to generalize to related classification tasks.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {97–104},
numpages = {8},
keywords = {multi-classifier, document classification},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502604,
author = {Toutanova, Kristina and Chen, Francine and Popat, Kris and Hofmann, Thomas},
title = {Text Classification in a Hierarchical Mixture Model for Small Training Sets},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502604},
doi = {10.1145/502585.502604},
abstract = {Documents are commonly categorized into hierarchies of topics, such as the ones maintained by Yahoo! and the Open Directory project, in order to facilitate browsing and other interactive forms of information retrieval. In addition, topic hierarchies can be utilized to overcome the sparseness problem in text categorization with a large number of categories, which is the main focus of this paper. This paper presents a hierarchical mixture model which extends the standard naive Bayes classifier and previous hierarchical approaches. Improved estimates of the term distributions are made by differentiation of words in the hierarchy according to their level of generality/specificity. Experiments on the Newsgroups and the Reuters-21578 dataset indicate improved performance of the proposed classifier in comparison to other state-of-the-art methods on datasets with a small number of positive examples.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {105–113},
numpages = {9},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502605,
author = {Zelikovitz, Sarah and Hirsh, Haym},
title = {Using LSI for Text Classification in the Presence of Background Text},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502605},
doi = {10.1145/502585.502605},
abstract = {This paper presents work that uses Latent Semantic Indexing (LSI) for text classification. However, in addition to relying on labeled training data, we improve classification accuracy by also using unlabeled data and other forms of available "background" text in the classification process. Rather than performing LSI's singular value decomposition (SVD) process solely on the training data, we instead use an expanded term-by-document matrix that includes both the labeled data as well as any available and relevant background text. We report the performance of this approach on data sets both with and without the inclusion of the background text, and compare our work to other efforts that can incorporate unlabeled data and other background text in the classification process.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {113–118},
numpages = {6},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502607,
author = {Jones, William and Bruce, Harry and Dumais, Susan},
title = {Keeping Found Things Found on the Web},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502607},
doi = {10.1145/502585.502607},
abstract = {This paper describes the results of an observational study into the methods people use to manage web information for re-use. People observed in our study used a diversity of methods and associated tools. For example, several participants emailed web addresses (URLs) along with comments to themselves and to others. Other methods observed included printing out web pages, saving web pages to the hard drive, pasting the address for a web page into a document and pasting the address into a personal web site. Ironically, two web browser tools that have been explicitly developed to help users track web information - the bookmarking tool and the history list - were not widely used by participants in this study. A functional analysis helps to explain the observed diversity of methods. Methods vary widely in the functions they provide. For example, a web address pasted into a self-addressed email can provide an important reminding function together with a context of relevance: The email arrives in an inbox which is checked at regular intervals and the email can include a few lines of text that explain the URL's relevance and the actions to be taken. On the other hand, for most users in the study, the bookmarking tool ("Favorites" or "Bookmarks" depending on the browser) provided neither a reminding function nor a context of relevance. The functional analysis can help to assess the likely success of various tools, current and proposed.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {119–126},
numpages = {8},
keywords = {world wide web use, personal information management, human-computer interaction, information retrieval},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502608,
author = {Tsikrika, Theodora and Lalmas, Mounia},
title = {Merging Techniques for Performing Data Fusion on the Web},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502608},
doi = {10.1145/502585.502608},
abstract = {Data fusion on the Web refers to the merging, into a unified single list, of the ranked document lists, which are retrieved in response to a user query by more than one Web search engine. It is performed by metasearch engines and their merging algorithms utilise the information present in the ranked lists of retrieved documents provided to them by the underlying search engines, such as the rank positions of the retrieved documents and their retrieval scores. In this paper, merging techniques are introduced that take into account not only the rank positions, but also the title and the summary accompanying the retrieved documents. Furthermore, the data fusion process is viewed as being similar to the combination of belief in uncertain reasoning and is modelled using Dempster-Shafer's theory of evidence. Our evaluation experiments indicate that the above merging techniques yield improvements in the effectiveness and that their effectiveness is comparable to that of the approach that merges the ranked lists by downloading and analysing the Web documents.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {127–134},
numpages = {8},
keywords = {Dempster-Shafer's theory of evidence, web data fusion, information retrieval},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502609,
author = {Hansen, Mark H. and Shriver, Elizabeth},
title = {Using Navigation Data to Improve IR Functions in the Context of Web Search},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502609},
doi = {10.1145/502585.502609},
abstract = {As part of the process of delivering content, devices like proxies and gateways log valuable information about the activities and navigation patterns of users on the Web. In this study, we consider how this navigation data can be used to improve Web search. A query posted to a search engine together with the set of pages accessed during a search task is known as a search session. We develop a mixture model for the observed set of search sessions, and propose variants of the classical EM algorithm for training. The model itself yields a type of navigation-based query clustering. By implicitly borrowing strength between related queries, the mixture formulation allows us to identify the "highly relevant" URLs for each query cluster. Next, we explore methods for incorporating existing labeled data (the Yahoo! directory, for example) to speed convergence and help resolve low-traffic clusters. Finally, the mixture formulation also provides for a simple, hierarchical display of search results based on the query clusters. The effectiveness of our approach is evaluated using proxy access logs for the outgoing Lucent proxy.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {135–142},
numpages = {8},
keywords = {query clustering, web searching, model-based clustering, proxy access logs, expectation-maximization algorithm},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502610,
author = {Radev, Dragomir R. and Qi, Hong and Zheng, Zhiping and Blair-Goldensohn, Sasha and Zhang, Zhu and Fan, Weiguo and Prager, John},
title = {Mining the Web for Answers to Natural Language Questions},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502610},
doi = {10.1145/502585.502610},
abstract = {The web is now becoming one of the largest information and knowledge repositories. Many large scale search engines (Google, Fast, Northern Light, etc.) have emerged to help users find information. In this paper, we study how we can effectively use these existing search engines to mine the Web and discover the "correct" answers to factual natural language questions.We propose a probabilistic algorithm called QASM (Question Answering using Statistical Models) that learns the best query paraphrase of a natural language question. We validate our approach for both local and web search engines using questions from the TREC evaluation. We also show how this algorithm can be combined with another algorithm (AnSel) to produce precise answers to natural language questions.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {143–150},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502612,
author = {Jeong, Euna and Hsu, Chun-Nan},
title = {Induction of Integrated View for XML Data with Heterogeneous DTDs},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502612},
doi = {10.1145/502585.502612},
abstract = {This paper proposes a novel approach to integrating heterogeneous XML DTDs. With this approach, an information agent can be easily extended to integrate heterogeneous XML-based contents and perform federated search. Based on a tree grammar inference technique, this approach derives an integrated view of XML DTDs in an information integration framework. The derivation takes advantages of naming and structural similarities among DTDs in similar domains. The complete approach consists of three main steps. (1) DTD clustering clusters DTDs in similar domains into classes. (2) Schema learning applies a tree grammar inference technique to generate a set of tree grammar rules from the DTDs in a class from the previous step. (3) Minimization optimizes the rules generated in the previous step and transforms them into an integrated view. We have implemented the proposed approach into a system called DEEP and tested the system on artificial and real domains. The experimental results reveal that this system can effectively and efficiently integrate radically different DTDs.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {151–158},
numpages = {8},
keywords = {semistructured data, federated search, intelligent agent, mark-up schemes, XML DTD, distributed databases},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502613,
author = {Sankey, Jason and Wong, Raymond K.},
title = {Structural Inference for Semistructured Data},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502613},
doi = {10.1145/502585.502613},
abstract = {Semistructured data presents many challenges, mainly due to its lack of a strict schema. These challenges are further magnified when large amounts of data are gathered from heterogeneous sources. We address this by investigation and development of methods to automatically infer structural information from example data. Using XML as a reference format, we approach the schema generation problem by application of inductive inference theory. In doing so, we review and extend results relating to the search spaces of grammatical inferences. We then adapt a method for evaluating the result of an inference process from computational linguistics. Further, we combine several inference algorithms, including both new techniques introduced by us and those from previous work. Comprehensive experimentation reveals our new hybrid method, based upon recently developed optimisation techniques, to be the most effective.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {159–166},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502614,
author = {Li, Ying Guang and Bressan, St\'{e}phane and Dobbie, Gillian and Lacroix, Zo\'{e} and Lee, Mong Li and Nambiar, Ullas and Wadhwa, Bimlesh},
title = {XOO7: Applying OO7 Benchmark to XML Query Processing Tool},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502614},
doi = {10.1145/502585.502614},
abstract = {If XML is to play the critical role of the lingua franca for Internet data interchange that many predict, it is necessary to start designing and adopting benchmarks allowing the comparative performance analysis of the tools being developed and proposed. The effectiveness of existing XML query languages has been studied by many, with a focus on the comparison of linguistic features, implicitly reflecting the fact that most XML tools exist only on paper. In this paper, with a focus on efficiency and concreteness, we propose a pragmatic first step toward the systematic benchmarking of XML query processing platforms with an initial focus on the data (versus document) point of view. We propose XOO7, an XML version of the OO7 benchmark. We discuss the applicability of XOO7, its strengths, limitations and the extensions we are considering. We illustrate its use by presenting and discussing the performance comparison against XOO7 of three different query processing platforms for XML.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {167–174},
numpages = {8},
keywords = {XML management systems, native-XML database, XML aware database, XML benchmarks, XOO7},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502615,
author = {Barg, Michael and Wong, Raymond K.},
title = {Structural Proximity Searching for Large Collections of Semi-Structured Data},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502615},
doi = {10.1145/502585.502615},
abstract = {The richness of the XML data format allows data to be structured in a way which precisely captures the semantics required by the author. It is the structure of the data, however, which forms the basis of all XML query languages. Without at least some notion of the structure, a user cannot meaningfully query the data. This problem is compounded when one considers that heterogeneous data adhering to different schema are likely to exist in the database(s) being queried. This paper proposes a solution based on an efficient proximity index. In particular, we describe a family of encoding and compression schemes which enable us to build an index to efficiently implement the proximity search. Our index is extremely small, and can reflect updates in the underlying database in modest time. Experiments show that our algorithm and implementation are fast and scale well.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {175–182},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502617,
author = {Ogilvie, Paul and Callan, Jamie},
title = {The Effectiveness of Query Expansion for Distributed Information Retrieval},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502617},
doi = {10.1145/502585.502617},
abstract = {Query expansion has been shown effective for both single database retrieval and for distributed information retrieval where complete collection information is available. One might expect that query expansion would then work for distributed information retrieval when complete collection information is not available. However, this does not appear to be the case. When using local context analysis for query expansion in distributed retrieval with partial information, the most significant reason query expansion does not work is that merging scores of documents retrieved by expanded queries is very difficult. However, we have found that using sampled information for query expansion can give boosts in a single database environment, and that when more information is available, query expansion can work in distributed environments. We also show that most of the benefit of query expansion in distributed retrieval comes from finding good documents, and not from selecting good databases.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {183–190},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502618,
author = {Rasolofo, Yves and Abbaci, Fa\"{\i}za and Savoy, Jacques},
title = {Approaches to Collection Selection and Results Merging for Distributed Information Retrieval},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502618},
doi = {10.1145/502585.502618},
abstract = {We have investigated two major issues in Distributed Information Retrieval (DIR), namely: collection selection and search results merging. While most published works on these two issues are based on pre-stored metadata, the approaches described in this paper involve extracting the required information at the time the query is processed. In order to predict the relevance of collections to a given query, we analyse a limited number of full documents (e.g., the top five documents) retrieved from each collection and then consider term proximity within them. On the other hand, our merging technique is rather simple since input only requires document scores and lengths of results lists. Our experiments evaluate the retrieval effectiveness of these approaches and compare them with centralised indexing and various other DIR techniques (e.g., CORI). We conducted our experiments using two testbeds: one containing news articles extracted from four different sources (2 GB) and another containing 10 GB of Web pages. Our evaluations demonstrate that the retrieval effectiveness of our simple approaches is worth considering.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {191–198},
numpages = {8},
keywords = {evaluation, distributed information retrieval, search results, merging, collection selection},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502619,
author = {French, James C. and Powell, Allison L. and Gey, Fredric and Perelman, Natalia},
title = {Exploiting a Controlled Vocabulary to Improve Collection Selection and Retrieval Effectiveness},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502619},
doi = {10.1145/502585.502619},
abstract = {Vocabulary incompatibilities arise when the terms used to index a document collection are largely unknown, or at least not well-known to the users who eventually search the collection. No matter how comprehensive or well-structured the indexing vocabulary, it is of little use if it is not used effectively in query formulation. This paper demonstrates that techniques for mapping user queries into the controlled indexing vocabulary have the potential to radically improve document retrieval performance. We also show how the use of controlled indexing vocabulary can be employed to achieve performance gains for collection selection. Finally, we demonstrate the potential benefit of combining these two techniques in an interactive retrieval environment. Given a user query, our evaluation approach simulates the human user's choice of terms for query augmentation given a list of controlled vocabulary terms suggested by a system. This strategy lets us evaluate interactive strategies without the need for human subjects.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {199–206},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502621,
author = {Blok, Henk Ernst and Hiemstra, Djoerd and Choenni, Sunil and de Jong, Franciska and Blanken, Henk M. and Apers, Peter M.G.},
title = {Predicting the Cost-Quality Trade-off for Information Retrieval Queries: Facilitating Database Design and Query Optimization},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502621},
doi = {10.1145/502585.502621},
abstract = {Efficient, flexible, and scalable integration of full text information retrieval (IR) in a DBMS is not a trivial case. This holds in particular for query optimization in such a context. To facilitate the bulk-oriented behavior of database query processing, a priori knowledge of how to limit the data efficiently prior to query evaluation is very valuable at optimization time. The usually imprecise nature of IR querying provides an extra opportunity to limit the data by a trade-off with the quality of the answer. In this paper we present a mathematically derived model to predict the quality implications of neglecting information before query execution. In particular we investigate the possibility to predict the retrieval quality for a document collection for which no training information is available, which is usually the case in practice. Instead, we construct a model that can be trained on other document collections for which the necessary quality information is available, or can be obtained quite easily. We validate our model for several document collections and present the experimental results. These results show that our model performs quite well, even for the case were we did not train it on the test collection itself.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {207–214},
numpages = {8},
keywords = {information retrieval, efficiency, databases, quality, Zipf, trade-off, fragmentation},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502622,
author = {Hergula, Klaudia and H\"{a}rder, Theo},
title = {How Foreign Function Integration Conquers Heterogeneous Query Processing},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502622},
doi = {10.1145/502585.502622},
abstract = {With the emergence of application systems which encapsulate databases and related application components, pure data integration using, for example, a federated database system is not possible anymore. Instead, access via predefined functions is the only way to get data from an application system. As a result, retrieval of such heterogeneous and encapsulated data sources needs the combination of generic query as well as predefined function access. In this paper, we present a middleware approach supporting such a novel and extended kind of integration. Starting with the overall architecture, we explain the functionality and cooperation of its core components: a federated database system and a workflow management system connected via a wrapper. Afterwards, we concentrate on essential aspects of query processing across these heterogeneous components focusing on the impact of the functions included. We discuss the operations the wrapper should provide in order to extend the workflow system's native functionality. In addition to selection and projection, these operations could include aggregation and the support of subqueries. Moreover, we point out modifications to the traditional cost model needed to consider the cost estimates for the function calls as well.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {215–222},
numpages = {8},
keywords = {cost model, workflow management system, federated database system, heterogeneous query processing, function integration, wrapper},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502623,
author = {Nie, Zaiqing and Kambhampati, Subbarao},
title = {Joint Optimization of Cost and Coverage of Query Plans in Data Integration},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502623},
doi = {10.1145/502585.502623},
abstract = {Existing approaches for optimizing queries in data integration use decoupled strategies--attempting to optimize coverage and cost in two separate phases. Since sources tend to have a variety of access limitations, such phased optimization of cost and coverage can unfortunately lead to expensive planning as well as highly inefficient plans. In this paper we present techniques for joint optimization of cost and coverage of the query plans. Our algorithms search in the space of parallel query plans that support multiple sources for each subgoal conjunct. The refinement of the partial plans takes into account the potential parallelism between source calls, and the binding compatibilities between the sources included in the plan. We start by introducing and motivating our query plan representation. We then briefly review how to compute the cost and coverage of a parallel plan. Next, we provide both a System-R style query optimization algorithm as well as a greedy local search algorithm for searching in the space of such query plans. Finally we present a simulation study that demonstrates that the plans generated by our approach will be significantly better, both in terms of planning cost, and in terms of plan execution cost, compared to the existing approaches.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {223–230},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502625,
author = {Chen, Hung-Chen and Chen, Arbee L. P.},
title = {A Music Recommendation System Based on Music Data Grouping and User Interests},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502625},
doi = {10.1145/502585.502625},
abstract = {With the growth of the World Wide Web, a large amount of music data is available on the Internet. In addition to searching expected music objects for users, it becomes necessary to develop a recommendation service. In this paper, we design the Music Recommendation System (MRS) to provide a personalized service of music recommendation. The music objects of MIDI format are first analyzed. For each polyphonic music object, the representative track is first determined, and then six features are extracted from this track. According to the features, the music objects are properly grouped. For users, the access histories are analyzed to derive user interests. The content-based, collaborative and statistics-based recommendation methods are proposed, which are based on the favorite degrees of the users to the music groups. A series of experiments are carried out to show that our approach is feasible.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {231–238},
numpages = {8},
keywords = {access histories, perceptual properties, user profiles, music recommendation, recommendation methods},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502626,
author = {Yu, Kai and Xu, Xiaowei and Ester, Martin and Kriegel, Hans-Peter},
title = {Selecting Relevant Instances for Efficient and Accurate Collaborative Filtering},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502626},
doi = {10.1145/502585.502626},
abstract = {Collaborative filtering uses a database about consumers' preferences to make personal product recommendations and is achieving widespread success in both E-Commerce and Information Filtering Applications nowadays. However, the traditional collaborative filtering algorithms do not scale well to the ever-growing number of consumers. The quality of the recommendation also needs to be improved in order to gain more trust from the consumers. In this paper, we present a novel method to improve the scalability and the accuracy of the collaborative filtering algorithm. We introduce an information theoretic approach to measure the relevance of a consumer (instance) for predicting the preference for the given product (target concept). The proposed method reduces the training data set by selecting only highly relevant instances. Our experimental evaluation on the well-known EachMovie data set shows that our method doesn't only significantly speed up the prediction, but also results in a better accuracy.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {239–246},
numpages = {8},
keywords = {data mining, collaborative filtering, instance selection},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502627,
author = {Karypis, George},
title = {Evaluation of Item-Based Top-<i>N</i> Recommendation Algorithms},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502627},
doi = {10.1145/502585.502627},
abstract = {The explosive growth of the world-wide-web and the emergence of e-commerce has led to the development of recommender systems---a personalized information filtering technology used to identify a set of N items that will be of interest to a certain user. User-based Collaborative filtering is the most successful technology for building recommender systems to date, and is extensively used in many commercial recommender systems. Unfortunately, the computational complexity of these methods grows linearly with the number of customers that in typical commercial applications can grow to be several millions. To address these scalability concerns item-based recommendation techniques have been developed that analyze the user-item matrix to identify relations between the different items, and use these relations to compute the list of recommendations.In this paper we present one such class of item-based recommendation algorithms that first determine the similarities between the various items and then used them to identify the set of items to be recommended. The key steps in this class of algorithms are (i) the method used to compute the similarity between the items, and (ii) the method used to combine these similarities in order to compute the similarity between a basket of items and a candidate recommender item. Our experimental evaluation on five different datasets show that the proposed item-based algorithms are up to 28 times faster than the traditional user-neighborhood based recommender systems and provide recommendations whose quality is up to 27% better.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {247–254},
numpages = {8},
keywords = {collaborative filtering, recommender system},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502629,
author = {Park, Sanghyun and Kim, Sang-Wook and Cho, June-Suh and Padmanabhan, Sriram},
title = {Prefix-Querying: An Approach for Effective Subsequence Matching under Time Warping in Sequence Databases},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502629},
doi = {10.1145/502585.502629},
abstract = {This paper discusses an index-based subsequence matching that supports time warping in large sequence databases. Time warping enables finding sequences with similar patterns even when they are of different lengths. In our earlier work, we suggested an efficient method for whole matching under time warping. This method constructs a multi-dimensional index on a set of feature vectors, which are invariant to time warping, from data sequences. For filtering at feature space, it also applies a lower-bound function, which consistently underestimates the time warping distance as well as satisfies the triangular inequality.In this paper, we incorporate the prefix-querying approach based on sliding windows into the earlier approach. For indexing, we extract a feature vector from every subsequence inside a sliding window and construct a multi-dimensional index using a feature vector as indexing attributes. For query processing, we perform a series of index searches using the feature vectors of qualifying query prefixes. Our approach provides effective and scalable subsequence matching even with a large volume of a database. We also prove that our approach does not incur false dismissal. To verify the superiority of our method, we perform extensive experiments. The results reveal that our method achieves significant speedup with real-world S&amp;P 500 stock data and with very large synthetic data.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {255–262},
numpages = {8},
keywords = {subsequence matching, indexing technique, time warping, prefix querying, sequence database, similarity search},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502630,
author = {Lee, Chang-Hung and Lin, Cheng-Ru and Chen, Ming-Syan},
title = {Sliding-Window Filtering: An Efficient Algorithm for Incremental Mining},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502630},
doi = {10.1145/502585.502630},
abstract = {We explore in this paper an effective sliding-window filtering (abbreviatedly as SWF) algorithm for incremental mining of association rules. In essence, by partitioning a transaction database into several partitions, algorithm SWF employs a filtering threshold in each partition to deal with the candidate itemset generation. Under SWF, the cumulative information of mining previous partitions is selectively carried over toward the generation of candidate itemsets for the subsequent partitions. Algorithm SWF not only significantly reduces I/O and CPU cost by the concepts of cumulative filtering and scan reduction techniques but also effectively controls memory utilization by the technique of sliding-window partition. Algorithm SWF is particularly powerful for efficient incremental mining for an ongoing time-variant transaction database. By utilizing proper scan reduction techniques, only one scan of the incremented dataset is needed by algorithm SWF. The I/O cost of SWF is, in orders of magnitude, smaller than those required by prior methods, thus resolving the performance bottleneck. Experimental studies are performed to evaluate performance of algorithm SWF. It is noted that the improvement achieved by algorithm SWF is even more prominent as the incremented portion of the dataset increases and also as the size of the database increases.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {263–270},
numpages = {8},
keywords = {data mining, incremental mining, association rules, time-variant database},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502631,
author = {Man, Polly Wan Po and Wong, Man Hon},
title = {Efficient and Robust Feature Extraction and Pattern Matching of Time Series by a Lattice Structure},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502631},
doi = {10.1145/502585.502631},
abstract = {The efficiency of searching scaling-invariant and shifting-invariant shapes in a set of massive time series data can be improved if searching is performed on an approximated sequence which involves less data but contains all the significant features. However, commonly used smoothing techniques, such as moving averages and best-fitting polylines, usually miss important peaks and troughs and deform the time series. In addition, these techniques are not robust, as they often requires users to supply a set of smoothing parameters which has direct effect on the resultant approximation pattern. To address these problems, an algorithm to construct a lattice structure as an underlying framework for pattern matching is proposed in this paper. As inputs, the algorithm takes a time series and users' requirements of level of detail. The algorithm then identifies all the important peaks and troughs (known as controlm points) in the time series and classifies the points into appropriate layers of the lattice structure. The control points in each layer of the structure form an approximation pattern an yet preserve the overall shape of the original series with approximation error lies within certain bound. The lower the layer, the more precise the approximation pattern is. Putting in another way, the algorithm takes different levels of data smoothing into account. Also, the lattice structure can be indexed to further improve the performance of pattern matching.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {271–278},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502633,
author = {Ghani, Rayid and Jones, Rosie and Mladeni\'{c}, Dunja},
title = {Mining the Web to Create Minority Language Corpora},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502633},
doi = {10.1145/502585.502633},
abstract = {The Web is a valuable source of language specific resources but the process of collecting, organizing and utilizing these resources is difficult. We describe CorpusBuilder, an approach for automatically generating Web-search queries for collecting documents in a minority language. It differs from pseudo-relevance feedback in that retrieved documents are labeled by an automatic language classifier as relevant or irrelevant, and this feedback is used to generate new queries. We experiment with various query-generation methods and query-lengths to find inclusion/exclusion terms that are helpful for retrieving documents in the target language and find that using odds-ratio scores calculated over the documents acquired so far was one of the most consistently accurate query-generation methods. We also describe experiments using a handful of words elicited from a user instead of initial documents and show that the methods perform similarly. Experiments applying the same approach to multiple languages are also presented showing that our approach generalizes to a variety of languages.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {279–286},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502634,
author = {Conrad, Jack G. and Dabney, Daniel P.},
title = {Automatic Recognition of Distinguishing Negative Indirect History Language in Judicial Opinions},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502634},
doi = {10.1145/502585.502634},
abstract = {We describe a model-based filtering application that generates candidate case-to-case distinguishing citations. We developed the system to aid editors in identifying indirect relationships among judicial opinions in a database of over 5 million documents. Using a training collection of approximately 30,000 previously edited cases, the filter application provides ranked sets of textual evidence for current case law documents in the editorial process. These sets contain judicial language with a strong probability of containing distinguishing relationships. Integrating this application into the editorial review environment has greatly improved the coverage and efficiency of the work flow to identify and generate new distinguishing relationship entries.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {287–294},
numpages = {8},
keywords = {legal research, case law analysis, citation loci, distinguished cases},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502635,
author = {Aljlayl, Mohammed and Frieder, Ophir},
title = {Effective Arabic-English Cross-Language Information Retrieval via Machine-Readable Dictionaries and Machine Translation},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502635},
doi = {10.1145/502585.502635},
abstract = {In Cross-Language Information Retrieval (CLIR), queries in one language retrieve relevant documents in other languages Machine-Readable Dictionary (MRD) and Machine Translation (MT) are important resources for query translation in CLIR. We investigate MT and MRD to Arabic-English CLIR. The translation ambiguity associated with these resources is the key problem. We present three methods of query translation using a bilingual dictionary for Arabic-English CLIR. First, we present the Every-Match (EM) method. This method yields ambiguous translations since many extraneous terms are added to the original query. To disambiguate the query translation, we present the First-Match (FM) method that considers the first match in the dictionary as the candidate term. Finally, we present the Two-Phase (TP) method. We show that good retrieval effectiveness can be achieved without complex resources using the Two-Phase method for Arabic-English CLIR. We also empirically evaluate the effectiveness of the MT-based method using short, medium, and long queries from TREC. The effects of the query length on the quality of the MT-based CLIR are investigated.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {295–302},
numpages = {8},
keywords = {machine translation, cross-language, two-phase, machine-readable dictionary},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502637,
author = {Hsu, Chih-Hao and Lee, Guanling and Chen, Arbee L. P.},
title = {A near Optimal Algorithm for Generating Broadcast Programs on Multiple Channels},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502637},
doi = {10.1145/502585.502637},
abstract = {In a wireless environment, the bandwidth of the channels and the energy of the portable devices are limited. Data broadcast has become an excellent method for efficient data dissemination. In this paper, the problem for generating a broadcast program of a set of data items with the associated access frequencies on multiple channels is explored. In our approach, an expected average access time of the broadcast data items is first derived. The broadcast program is then generated, which minimizes the expected average access time. Simulation is performed to compare the performance of our approach with two existing approaches. The result of the experiments shows that our approach outperforms others and is in fact close to the optimal.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {303–309},
numpages = {7},
keywords = {wireless environment, multiple broadcast channels, data allocation, broadcast program},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502638,
author = {Aberer, Karl and Despotovic, Zoran},
title = {Managing Trust in a Peer-2-Peer Information System},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502638},
doi = {10.1145/502585.502638},
abstract = {Managing trust is a problem of particular importance in peer-to-peer environments where one frequently encounters unknown agents. Existing methods for trust management, that are based on reputation, focus on the semantic properties of the trust model. They do not scale as they either rely on a central database or require to maintain global knowledge at each agent to provide data on earlier interactions. In this paper we present an approach that addresses the problem of reputation-based trust management at both the data management and the semantic level. We employ at both levels scalable data structures and algorithms that require no central control and allow to assess trust by computing an agents reputation from its former interactions with other agents. Thus the meethod can be implemented in a peer-to-peer environment and scales well for very large numbers of participants. We expect that scalable methods for trust management are an important factor, if fully decentralized peer-to-peer systems should become the platform for more serious applications than simple file exchange.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {310–317},
numpages = {8},
keywords = {trust management, reputation, peer-to-peer information systems, decentralized databases},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502639,
author = {Eisenstein, J. and Ghandeharizadeh, S. and Shahabi, C. and Shanbhag, G. and Zimmermann, R.},
title = {Alternative Representations and Abstractions for Moving Sensors Databases},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502639},
doi = {10.1145/502585.502639},
abstract = {Moving sensors refers to an emerging class of data intensive applications that inpacts disciplines such as communication, health-care, scientific applications, etc. These applications consist of a fixed number of sensors that move and produce streams of data as a function of time. They may require the system to match these streams against stored streams to retrieve relevant data (patterns). With communication, for example, a speaking impaired individual might utilize a haptic glove that translates hand signs into written (spoken) words. The glove consists of sensors for different finger joints. These sensors report their location and values as a function of time, producing streams of data. These streams are matched against a repository of spatio-temporal streams to retrieve the corresponding English character or word.The contributions of this study are two fold. First, it introduces a framework to store and retrieve "moving sensors" data. The framework advocates physical data independence and software-reuse. Second, we investigate alternative representations for storage and retrieve of data in support of query processing. We quantify the tradeoff associated with these alternatives using empirical data RoboCup soccer matches.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {318–325},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502640,
author = {Couchot, Alain},
title = {Termination Analysis of Active Rules Modular Sets},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502640},
doi = {10.1145/502585.502640},
abstract = {This paper presents an algorithm for static termination analysis of active rules in a context of modular design. Several recent works have suggested proving termination by using the concept of triggering graph. We propose here an original approach, based on these works, and that allows to guarantee the termination of a set of rules, conceived by several designers, even when none of the designers knows the set of the active rules. We introduce the notions of private event and of public event, and we refine the notion of triggering graph (by enclosing also events in graphs). We replace then the notion of cycle (which is no more relevant in a context of modular design) by the notion of maximal private path preceding a rule. By means of these tools, we show that it is possible to prove termination of active rules modular sets.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {326–333},
numpages = {8},
keywords = {static analysis, termination, active database systems},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502642,
author = {Qiu, Shi Guang and Ling, Tok Wang},
title = {Index Filtering and View Materialization in ROLAP Environment},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502642},
doi = {10.1145/502585.502642},
abstract = {Using materialized view to accelerate OLAP queries is one of the most common methods used in ROLAP systems. However, high storage and computation cost make this method very difficult to be implemented in the actual environment. Among various issues associated with this, index selection and view materialization are two of the top challenges. In this paper, we propose to build indexes on subsets of the primary keys rather than the full sets if the index selectivity for these smaller indexes can be maintained above the required level. Based on that we propose an index filtering rule, Dominant Prime (DPrime) Index Set Filter, to filter out candidate indexes that have insufficient index selectivity or have cheaper alternatives. In the second part, we propose a view materialization method, Nested Relation Approach, to group tuples with the same value for index attributes into one super tuple using a nested relation and implement this method using Oracle VARRAY. In performance tests, our method outperforms others significantly.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {334–340},
numpages = {7},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502643,
author = {Cho, Kyoosang and Han, Yijie and Lee, Yugyung and Park, E. K.},
title = {Dynamic and Hierarchical Spatial Access Method Using Integer Searching},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502643},
doi = {10.1145/502585.502643},
abstract = {Dynamic and complex computation in the area of Geographic Information System (GIS) or Mobile Computing System involves huge amount of spatial objects such as points, boxes, polygons, etc and requires a scalable data structure and an efficient management tool for this information. In this paper, for a dynamic management of spatial objects, we construct a hierarchical dynamic data structure, called an IST/OPG hierarchy, which may overcome some limitations of existing Spatial Access Methods (SAMs). The hierarchy is constructed by combining three primary components: (1) Minimum Boundary Rectangle (MBR), which is the most widely used method among SAMs; (2) the population-based domain slicing, which is modified from the Grid File [14]; (3) extended optimal Integer Searching algorithm [4]. For dynamic management of spatial objects in the IST/OPG hierarchy, a number of primary and supplementary operations are introduced. This paper includes a comparative analysis of our approach with previous SAMs, such as R-Tree, R+-Tree and R*-Tree and QSF-Tree. The results of analysis show that our approach is better than other SAMs in construction and query time and space requirements. Specifically, for a given search domain with n objects, our query operations yield $O($ scriptsize $sqrt {frac {log n} {loglog n}}$normalsize $)$ compared to $O(log n)$ of the fast SAM and an IST/OPG hierarchy containing $n$ objects can be constructed in $O(n$ scriptsize $sqrt {frac {log n}{loglog n}}$normalsize $)$ time and O(n) space.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {341–348},
numpages = {8},
keywords = {spatial access method, grid file, integer searching algorithm, dynamic and hierarchical structure},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502644,
author = {Lee, Ki Yong and Son, Jin Hyun and Kim, Myoung Ho},
title = {Efficient Incremental View Maintenance in Data Warehouses},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502644},
doi = {10.1145/502585.502644},
abstract = {In the data warehouse environment, the concept of a materialized view is nowadays common and important in an objective of efficiently supporting OLAP query processing. Materialized views are generally derived from select-project-join of several base relations. These materialized views need to be updated when the base relations change. Since the propagation of updates to the views may impose a significant overhead, it is very important to update the warehouse views efficiently. Though various view maintenance strategies have been discussed so far, they typically require too much access to base relations, resulting in the performance degradation.In this paper we propose an efficient incremental view maintenance strategy called delta propagation that can minimize the total size of base relations accessed by analyzing the properties of base relations. We first define the delta expression and a delta propagation tree which are core concepts of the strategy. Then, a dynamic programming algorithm that can find the optimal delta expression are proposed. We also present various experimental results that show the usefulness and efficiency of the strategy.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {349–356},
numpages = {8},
keywords = {materialized view, view maintenance, data warehouse},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502646,
author = {Collins-Thompson, Kevyn and Schweizer, Charles and Dumais, Susan},
title = {Improved String Matching under Noisy Channel Conditions},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502646},
doi = {10.1145/502585.502646},
abstract = {Many document-based applications, including popular Web browsers, email viewers, and word processors, have a 'Find on this Page' feature that allows a user to find every occurrence of a given string in the document. If the document text being searched is derived from a noisy process such as optical character recognition (OCR), the effectiveness of typical string matching can be greatly reduced. This paper describes an enhanced string-matching algorithm for degraded text that improves recall, while keeping precision at acceptable levels. The algorithm is more general than most approximate matching algorithms and allows string-to-string edits with arbitrary costs. We develop a method for evaluating our technique and use it to examine the relative effectiveness of each sub-component of the algorithm. Of the components we varied, we find that using confidence information from the recognition process lead to the largest improvements in matching accuracy.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {357–364},
numpages = {8},
keywords = {approximate string matching, noisy channel model, information retrieval evaluation, optical character recognition},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502647,
author = {Kolcz, Aleksander and Prabakarmurthi, Vidya and Kalita, Jugal},
title = {Summarization as Feature Selection for Text Categorization},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502647},
doi = {10.1145/502585.502647},
abstract = {We address the problem of evaluating the effectiveness of summarization techniques for the task of document categorization. It is argued that for a large class of automatic categorization algorithms, extraction-based document categorization can be viewed as a particular form of feature selection performed on the full text of the document and, in this context, its impact can be compared with state-of-the-art feature selection techniques especially devised to provide good categorization performance. Such a framework provides for a better assessment of the expected performance of a categorizer if the compression rate of the summarizer is known.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {365–370},
numpages = {6},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502648,
author = {Golgher, Paulo B. and da Silva, Altigran S. and Laender, Alberto H. F. and Ribeiro-Neto, Berthier},
title = {Bootstrapping for Example-Based Data Extraction},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502648},
doi = {10.1145/502585.502648},
abstract = {The effortless generation of wrappers for Web data sources is a crucial task if proper access to the huge amount of semi-structured data on the Web is to be granted. In particular, the development of strategies for wrapper generation based on user-given examples is currently one of the most promising research directions in Web data extraction. In this paper we show how to use a pre-existing data repository to automatically generate examples and allow full automated example-based data extraction. To demonstrate the feasibility of our approach we provide a number of results obtained from experiments we carried out and discuss how our ideas can be used to improve extraction rates and for providing resilience and adaptiveness for example-based generated wrappers.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {371–378},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502650,
author = {Sattler, Kai-Uwe and Dunemann, Oliver},
title = {SQL Database Primitives for Decision Tree Classifiers},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502650},
doi = {10.1145/502585.502650},
abstract = {Scalable data mining in large databases is one of today's challenges to database technologies. Thus, substantial effort is dedicated to a tight coupling of database and data mining systems leading to database primitives supporting data mining tasks. In order to support a wide range of tasks and to be of general usage these primitives should be rather building blocks than implementations of specific algorithms. In this paper, we describe primitives for building and applying decision tree classifiers. Based on the analysis of available algorithms and previous work in this area we have identified operations which are useful for a number of classification algorithms. We discuss the implementation of these primitives on top of a commercial DBMS and present experimental results demonstrating the performance benefit.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {379–386},
numpages = {8},
keywords = {query operators, data mining primitives, SQL-aware data mining},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502651,
author = {Nottelmann, Henrik and Fuhr, Norbert},
title = {Learning Probabilistic Datalog Rules for Information Classification and Transformation},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502651},
doi = {10.1145/502585.502651},
abstract = {Probabilistic Datalog is a combination of classical Datalog (i.e., function-free Horn clause predicate logic) with probability theory. Therefore, probabilistic weights may be attached to both facts and rules. But it is often impossible to assign exact rule weights or even to construct the rules themselves. Instead of specifying them manually, learning algorithms can be used to learn both rules and weights. In practice, these algorithms are very slow because they need a large example set and have to test a high number of rules. We apply a number of extensions to these algorithms in order to improve efficiency. Several applications demonstrate the power of learning probabilistic Datalog rules, showing that learning rules is suitable for low dimensional problems (e.g., schema mapping) but inappropriate for higher dimensions like e.g. in text classification.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {387–394},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502652,
author = {Goh, King-Shy and Chang, Edward and Cheng, Kwang-Ting},
title = {SVM Binary Classifier Ensembles for Image Classification},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502652},
doi = {10.1145/502585.502652},
abstract = {We study how the SVM-based binary classifiers can be effectively combined to tackle the multi-class image classification problem. We study several ensemble schemes, including OPC (one per class), PWC (pairwise coupling), and ECOC (error-correction output coding), that aim to achieve good error correction capability through redundancy. To enhance these ensemble schemes' accuracy, we propose methods that on the one hand boost the margins (i.e., confidence) of the SVM-based binary classifiers, and, on the other hand, remove the noise of irrelevant classifiers from class prediction. From empirical study we show that our margin boosting and noise reduction methods lead to higher classification accuracy than ensemble schemes that are solely designed for maximum error correction capability.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {395–402},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502654,
author = {Zhai, Chengxiang and Lafferty, John},
title = {Model-Based Feedback in the Language Modeling Approach to Information Retrieval},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502654},
doi = {10.1145/502585.502654},
abstract = {The language modeling approach to retrieval has been shown to perform well empirically. One advantage of this new approach is its statistical foundations. However, feedback, as one important component in a retrieval system, has only been dealt with heuristically in this new retrieval approach: the original query is usually literally expanded by adding additional terms to it. Such expansion-based feedback creates an inconsistent interpretation of the original and the expanded query. In this paper, we present a more principled approach to feedback in the language modeling approach. Specifically, we treat feedback as updating the query language model based on the extra evidence carried by the feedback documents. Such a model-based feedback strategy easily fits into an extension of the language modeling approach. We propose and evaluate two different approaches to updating a query language model based on feedback documents, one based on a generative probabilistic model of feedback documents and one based on minimization of the KL-divergence over feedback documents. Experiment results show that both approaches are effective and outperform the Rocchio feedback approach.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {403–410},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502655,
author = {Grabs, Torsten and B\"{o}hm, Klemens and Schek, Hans-J\"{o}rg},
title = {PowerDB-IR: Information Retrieval on Top of a Database Cluster},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502655},
doi = {10.1145/502585.502655},
abstract = {Our current concern is a scalable infrastructure for information retrieval (IR) with up-to-date retrieval results in the presence of frequent, continuous updates. Timely processing of updates is important with novel application domains, e.g., e-commerce. We want to use off-the-self hardware and software as much as possible. These issues are challenging, given the additional requirement that the resulting system must scale well. We have built PowerDB-IR, a system that has the characteristics sought. This paper describes its design, implementation, and evaluation. PowerDB-IR is a coordination layer for a database cluster. The rationale behind a database cluster is to 'scale-out', i.e., to add further cluster nodes, whenever necessary for better performance. We build on IR-to-database mappings and service decomposition to support high-level parallelism. We follow a three-tier architecture with the database cluster as the bottom layer for storage management. The middle tier provides IR-specific processing and update services. PowerDB-IR has the following features: It allows to insert and retrieve documents concurrently, and it ensures freshness with almost no overhead. Alternative physical data organization schemes provide adequate performance for different workloads. Query processing techniques for the different data organizations efficiently integrate the ranked retrieval results from the cluster nodes. We have run extensive experiments with our prototype using commercial database systems and middleware software products. The main result is that PowerDB-IR shows surprisingly ideal scalability and low response times.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {411–418},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502656,
author = {Cai, D. and van Rijsbergen, C. J. and Jose, J. M.},
title = {Automatic Query Expansion Based on Divergence},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502656},
doi = {10.1145/502585.502656},
abstract = {In this paper we are mainly concerned with discussion of a formal model, based on the basic concept of divergence from information theory, for automatic query expansion. The basic principles and ideas on which our study is based are described. A theoretical framework is established, which allows the comparison and evaluation of different term scoring functions for identifying good terms for query expansion. The approaches proposed in this paper have been implemented and evaluated on collections from TREC. Preliminary results show that our approaches are viable and worthy of continued investigation.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {419–426},
numpages = {8},
keywords = {relative entropy, divergence, scoring functions, query quality, automatic query expansion, term selection},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502657,
author = {Montague, Mark and Aslam, Javed A.},
title = {Relevance Score Normalization for Metasearch},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502657},
doi = {10.1145/502585.502657},
abstract = {Given the ranked lists of documents returned by multiple search engines in response to a given query, the problem of  metasearch is to combine these lists in a way which optimizes the performance of the combination. This problem can be naturally decomposed into three subproblems: (1) normalizing the relevance scores given by the input systems, (2) estimating relevance scores for unretrieved documents, and (3) combining the newly-acquired scores for each document into one, improved score.Research on the problem of metasearch has historically concentrated on algorithms for combining (normalized) scores. In this paper, we show that the techniques used for normalizing relevance scores and estimating the relevance scores of unretrieved documents can have a significant effect on the overall performance of metasearch. We propose two new normalization/estimation techniques and demonstrate empirically that the performance of well known metasearch algorithms can be significantly improved through their use.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {427–433},
numpages = {7},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502659,
author = {Huang, Jiun-Long and Peng, Wen-Chih and Chen, Ming-Syan},
title = {Binary Interpolation Search for Solution Mapping on Broadcast and On-Demand Channels in a Mobile Computing Environment},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502659},
doi = {10.1145/502585.502659},
abstract = {We explore in this paper the problem of dynamic data and channel allocations with the number of communication channels and the number of data items given. It is noted that the combined use of broadcast and on-demand channels can utilize the bandwidth effectively for data dissemination in a mobile computing environment. We first derive the an-alytical models of the expected delays when the data are requested through the broadcast and on-demand channels. Then, we transform this problem into to a guided search problem. In light of the theoretical properties derived, we devise an algorithm based on binary interpolation search, referred to as algorithm BIS, to obtain solutions of high quality efficiently. In essence, algorithm BIS is guided to explore the solution space with higher likelihood to be the optimal first, thereby leading to an efficient and effective search. It is shown by our simulation results that the solution obtained by algorithm BIS is of very high quality and is in fact very close to the optimal one. Sensitivity analysis on several parameters, including the number of data items and the number of communication channels, is conducted.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {434–441},
numpages = {8},
keywords = {dynamic data and channel allocation, mobile computing, data dissemination},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502660,
author = {Mazumdar, Subhasish and Pietrzyk, Mateusz and Chrysanthis, Panos},
title = {Caching Constrained Mobile Data},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502660},
doi = {10.1145/502585.502660},
abstract = {As mobile devices get ubiquitous and grow in computational power, their management of interdependent data also becomes increasingly important. The mobile environment exhibits all the characteristics of a distributed database plus the feature of whimsical connectivity. Consequently, transactions respecting data consistency can suffer unbounded and unpredictable delays at both mobile and stationary nodes. The currently popular multi-tier model, in which mobile devices are in one end and always-connected stationary servers in the other, has certain practical advantages. However, it assumes that all integrity constraints are evaluated at the servers and hence relies on the semantics of operations for any autonomy enhancement of the mobile devices. In this paper, we examine the idea of constraint localization in cases where two mobile nodes each own data that share a constraint. It relies on reformulation of a constraint into more flexible local constraints that give more autonomy to the mobile nodes. The scheme also involves dynamic changes of these local constraints through negotiation, which we call re-localization. To overcome the problem of simultaneous requests for such re-localization, we give algorithms along with experimental results indicating their effectiveness.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {442–449},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502661,
author = {Yee, Wai Gen and Donahoo, Michael J. and Omiecinski, Edward and Navathe, Shamkant B.},
title = {Scaling Replica Maintenance in Intermittently Synchronized Mobile Databases},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502661},
doi = {10.1145/502585.502661},
abstract = {To avoid the high cost of continuous connectivity, a class of mobile applications employs replicas of shared data that are periodically updated. Updates to these replicas are typically performed on a client-by-client basis--that is, the server individually computes and transmits updates to each client--limiting scalability. By basing updates on replica groups (instead of clients), however, update generation complexity is no longer bound by client population size. Clients then download updates of pertinent groups. Proper group design reduces redundancies in server processing, disk usage and bandwidth usage, and dimininishes the tie between the complexity of updating replicas and the size of the client population. In this paper, we expand on previous work done on group design, include a detailed I/O cost model for update generation, and propose a heuristic-based greedy algorithm for group computation. Experimental results with an adapted commercial replication system demonstrate a significant increase in overall scalability over the client-centric approach.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {450–457},
numpages = {8},
keywords = {distributed databases, mobile databases, intermittent synchronization},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502662,
author = {Hou, Wen-Chi and Su, Meng and Zhang, Hongyan and Wang, Hong},
title = {An Optimal Construction of Invalidation Reports for Mobile Databases},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502662},
doi = {10.1145/502585.502662},
abstract = {Mobile computing is characterized by frequent disconnection, limited communication capability, narrow bandwidth, etc. Caching can play a vital role in mobile computing by reducing the amount of data transferred. In order to reuse caches after short disconnections, invalidation reports are broadcasted to clients to help update/invalidate their caches. Detailed reports may not be desirable because they can be very long and consume large bandwidth. On the other hand, false invalidations may set in if detailed timing information of updates is not provided in the report. In this research, we aim to reduce the false invalidation rates of the reports. It is found that false invalidation rates are closely related to clients' reconnection patterns (i.e., the distribution of the time spans between disconnections and reconnections). By using Newton's method, we show how a report with a minimal false invalidation rate can be constructed for any given disconnection pattern.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {458–465},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502664,
author = {Relue, Richard and Wu, Xindong and Huang, Hao},
title = {Efficient Runtime Generation of Association Rules},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502664},
doi = {10.1145/502585.502664},
abstract = {Mining frequent patterns in transaction databases has been a popular subject in data mining research. Common activities include finding patterns in database transactions, times-series, and exceptions. The Apriori algorithm is a widely accepted method of generating frequent patterns. The algorithm can require many scans of the database and can seriously tax resources. New methods of finding association rules, such as the Frequent Pattern Tree (FP-Tree) have improved performance, but still have problems when new data becomes available and require two scans of the database.This paper proposes a new method, which requires only one scan of the database and supports update of patterns when new data becomes available. We design a new structure called Pattern Repository (PR), which stores all of the relevant information in a highly compact form and allows direct derivation of the FP-Tree and association rules quickly with a minimum of resources. In addition, it supports run-time generation of association rules by considering only those patterns that meet on-line data requirements.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {466–473},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502665,
author = {Das, Amitabha and Ng, Wee-Keong and Woon, Yew-Kwong},
title = {Rapid Association Rule Mining},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502665},
doi = {10.1145/502585.502665},
abstract = {Association rule mining is a well-researched area where many algorithms have been proposed to improve the speed of mining. In this paper, we propose an innovative algorithm called Rapid Association Rule Mining (RARM) to once again break this speed barrier. It uses a versatile tree structure known as the Support-Ordered Trie Itemset (SOTrieIT) structure to hold pre-processed transactional data. This allows RARM to generate large 1-itemsets and 2-itemsets quickly without scanning the database and without candidate 2-itemset generation. It achieves significant speed-ups because the main bottleneck in association rule mining using the Apriori property is the generation of candidate 2-itemsets. RARM has been compared with the classical mining algorithm Apriori and it is found that it outperforms Apriori by up to two orders of magnitude (100 times), much more than what recent mining algorithms are able to achieve.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {474–481},
numpages = {8},
keywords = {association rule mining, index data structure, electronic commerce},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502666,
author = {Nanavati, Amit A. and Chitrapura, Krishna P. and Joshi, Sachindra and Krishnapuram, Raghu},
title = {Mining Generalised Disjunctive Association Rules},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502666},
doi = {10.1145/502585.502666},
abstract = {This paper introduces generalised disjunctive association rules such as "People who buy bread also buy butter jam", and "People who buy either raincoats or umbrellas also buy flashlights". A generalised disjunctive association rule allows the disjunction of conjuncts, "People who buy jackets also buy bow ties or neckties and tiepins". Such rules capture contextual inter-relationships among items.Given a context (antecedent), there may be a large number of generalised disjunctive association rules that satisfy the minsupp and minconf constraints. It is computationally expensive to find all such rules. We present algorithm thrifty traverse which borrows concepts such as subsumption from propositional logic to mine a subset of such rules in a computationally feasible way. We experimented with our algorithm on US census data as well as transaction data from a grocery superstore to demonstrate its computational feasibility, utility and scalability.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {482–489},
numpages = {8},
keywords = {knowledge discovery, data mining, disjunctive association rules, association rules},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502668,
author = {Ponceleon, Dulce and Srinivasan, Savitha},
title = {Automatic Discovery of Salient Segments in Imperfect Speech Transcripts},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502668},
doi = {10.1145/502585.502668},
abstract = {This paper addresses the problem of automatic detection of salient video segments for real-world applications such as corporate training based on associated speech transcriptions. We present a novel segmentation algorithm based on automatic speech recognition (ASR) applied to the audio track of the video. Our feature set consists of word n-grams extracted from the imperfect speech transcriptions. We use a two-pass algorithm that combines a boundary-based method with a content-based method. In the first pass, we analyze the temporal distribution and the rate of arrival of features to compute an initial segmentation. In the second pass, we detect changes in content-bearing words by using the content-bearing features as queries in an information retrieval system. The content-based second pass validates the initial segments and merges them as needed. Variations in the structure of the audio/video content, and the accuracy of ASR have an impact on the feasibility of the segmentation task. For realistic data we observe that we can identify content-rich segments of the audio. In the best scenario a high-level table-of-contents is generated and in the worse scenario a single salient segment is identified. We illustrate the algorithm in detail with some examples and validate the data with manual segmentation boundaries.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {490–497},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502669,
author = {Shen, Heng Tao},
title = {Finding Similar Images Quicky Using Object Shapes},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502669},
doi = {10.1145/502585.502669},
abstract = {Retrieving images from a large image collection has been an active area of research. Most of the existing works have focused on content representation. In this paper, we address the issue of identifying relevant images quickly. This is important in order to meet the users' performance requirements. We propose a framework for fast image retrieval based on object shapes extracted from objects within images. The framework builds a hierarchy of approximations on object shapes such that shape representation at a higher level is a coarser representation of a shape at the lower level. In other words, multiple shapes at a lower level can be mapped into a single shape at a higher level. In this way, the hierarchy serves to partition the database at various granularities. Given a query shape, by searching only the relevant paths in the hierarchy, a large portion of the database can thus be pruned away. We propose the angle mapping (AM) method to transform a shape from one level to another (higher) level. AM essentially replaces some edges of a shape by a smaller number of edges based on the angles between the edges, thus reducing the complexity of the original shape. Based on the framework, we also propose two hierarchical structures to facilitate speedy retrieval. The first, called Hierarchical Partitioning on Shape Representation (HPSR), uses the shape representation as the indexing key. The second, called Hierarchical Partitioning on Angle Vector (HPAV), captures the angle information from the shape representation. We conducted an extensive study on both methods to see their quality and efficiency. Our experiments on sets of images, each of which has objects around from 1 to 30, showed that the framework can provide speedy image retrieval without sacrificing on the quality. Both proposed schemes can improve the efficiency by as much as hundreds of times to sequential scanning. The improvement grows as image database size, objects per image or object dimension increase.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {498–506},
numpages = {9},
keywords = {shape indexing, partitioning, shape representation, shape-based image retrieval},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502670,
author = {Liu, Chih-Chin and Tsai, Po-Jun},
title = {Content-Based Retrieval of MP3 Music Objects},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502670},
doi = {10.1145/502585.502670},
abstract = {In recent years, the searching and indexing techniques for multimedia data are getting more attention in the area of multimedia databases. As many research works were done on the content-based retrieval of image and video data, less attention was received to the content-based retrieval of audio data. In this paper, we propose an approach to retrieve MP3 music objects based on their content. In our approach, the coefficients extracting from the output of the polyphase filters are used to compute the MP3 features for indexing the MP3 objects. We also propose an MP3 similarity measuring function to provide users the ability to approximately retrieve the desired MP3 objects. Experiments are performed and analyzed to show the efficiency and the effectiveness of the proposed method.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {506–511},
numpages = {6},
keywords = {music databases, MP3 databases, content-based music data retrieval, MP3 indexing, music feature extraction},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502671,
author = {Mokbel, Mohamed F. and Aref, Walid G.},
title = {Irregularity in Multi-Dimensional Space-Filling Curves with Applications in Multimedia Databases},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502671},
doi = {10.1145/502585.502671},
abstract = {A space-filling curve is a way of mapping the multi-dimensional space into the one-dimensional space. It acts like a thread that passes through every cell element (or pixel) in the N-dimensional space so that every cell is visited at least once. Thus, a space-filling curve imposes a linear order of the cells in the N-dimensional space. There are numerous kinds of space-filling curves. The difference between such curves is in their way of mapping to the one-dimensional space. Selecting the appropriate curve for any application requires a brief knowledge of the mapping scheme provided by each space-filling curve. Irregularity is proposed as a quantitative measure of the quality of the mapping of the space-filling curve. Closed formulas are developed to compute the irregularity for any general dimension D with N points in each dimension for different space-filling curves.A comparative study of different space-filling curves with respect to irregularity is conducted and results are presented and discussed. The applicability of this research is the area of multimedia databases is illustrated with a discussion of the problems that arise.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {512–519},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502673,
author = {Ford, Daniel A. and Ruvolo, Joann and Edlund, Stefan and Myllymaki, Jussi and Kaufman, James and Jackson, Jared and Gerlach, Martin},
title = {Tempus Fugit: A System for Making Semantic Connections},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502673},
doi = {10.1145/502585.502673},
abstract = {Tempus Fugit ("Time Flies") is the first of a new generation of Personal Information Management (PIM) systems. A PIM system incorporates an electronic calendar, "to-do" list and address book. The premise behind Tempus Fugit is that information stored in electronic calendars, to-do lists and address books can be given richer semantic interpretation and automatically processed to make its users more effective. Tempus Fugit also tracks the physical and virtual locations of users and uses this information to predict meeting attendance and help them as they travel during their day.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {520–522},
numpages = {3},
keywords = {PIM, location based services, electronic calendar, personal information management, mobile productivity, active calendar, software agents, electronic address book, location tracking},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502674,
author = {Ong, Hwee-Leng and Tan, Ah-Hwee and Ng, Jamie and Pan, Hong and Li, Qiu-Xiang},
title = {FOCI: Flexible Organizer for Competitive Intelligence},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502674},
doi = {10.1145/502585.502674},
abstract = {This paper describes how an integrated web-based application, code-named FOCI (Flexible Organizer for Competitive Intelligence), can help the knowledge worker in the gathering, organizing, tracking, and dissemination of competitive intelligence or knowledge bases on the web. It shows how text mining techniques including a novel user-configurable clustering, trend analysis and visualization techniques can be used synergistically to address the problem of managing information gathered from the web. FOCI allows a user to define and personalize the organization of the information clusters according to their needs and preferences into portfolios. Predefined sections for organizing information in specific domains is also supported. The personalized portfolios created can be saved and subsequently tracked and shared with other users. In addition, FOCI is designed to handle multilingual documents.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {523–525},
numpages = {3},
keywords = {web mining, text mining, clustering, personalization, visualization, trend analysis, competitive intelligence},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502675,
author = {Brown, Eric and Srinivasan, Savitha and Coden, Anni and Ponceleon, Dulce and Cooper, James and Amir, Arnon and Pieper, Jan},
title = {Towards Speech as a Knowledge Resource},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502675},
doi = {10.1145/502585.502675},
abstract = {Speech is a tantalizing mode of human communication. On the one hand, humans understand speech with ease and use speech to express complex ideas, information, and knowledge. On the other hand, automatic speech recognition with computers is still very hard, and extracting knowledge from speech is even harder. In this paper we motivate the study of speech as a knowledge resource and briefly survey a family of related applications and systems being developed at IBM Research aimed towards the goal of exploiting speech as a knowledge resource.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {526–528},
numpages = {3},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502677,
author = {Mani, Inderjeet},
title = {Recent Developments in Text Summarization},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502677},
doi = {10.1145/502585.502677},
abstract = {With the explosion in the quantity of on-line text and multimedia information in recent years, demand for text summarization technology is growing. Increased pressure for technology advances is coming from users of the web, on-line information sources, and new mobile devices, as well as from the need for corporate knowledge management. Commercial companies are increasingly starting to offer text summarization capabilities, often bundled with information retrieval tools. In this paper, I will discuss the significance of some recent developments in summarization technology.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {529–531},
numpages = {3},
keywords = {text summarization, information and knowledge management, information reduction, evaluation, abstracts, extracts},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502678,
author = {Farrell, Robert and Fairweather, Peter G. and Snyder, Kathleen},
title = {Summarization of Discussion Groups},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502678},
doi = {10.1145/502585.502678},
abstract = {In this paper, we describe an algorithm to generate textual summaries of discussion groups. Our system combines sentences extracted from individual postings into variable-length summaries by utilizing the hierarchical discourse context provided by discussion threads. We have incorporated this algorithm into a Web-based application called IDS (Interactive Discussion Summarizer).},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {532–534},
numpages = {3},
keywords = {summarization, hierarchical, discourse, text, extract, discussion},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502679,
author = {Voorhees, Ellen M.},
title = {Question Answering in TREC},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502679},
doi = {10.1145/502585.502679},
abstract = {Traditional text retrieval systems return a ranked list of documents in response to a user's request. While a ranked list of documents can be an appropriate response for the user, frequently it is not. Usually it would be better for the system to provide the answer itself instead of requiring the user to search for the answer in a set of documents. The Text REtrieval Conference (TREC) is sponsoring a question answering "track" to foster research on the problem of retrieving answers rather than document lists.TREC is a workshop series sponsored by the National Institute of Standards and Technology and the U.S. Department of Defense [7]. The purpose of the conference series is to encourage research on text retrieval for realistic applications by providing large test collections, uniform scoring procedures, and a forum for organizations interested in comparing results. The conference has focused primarily on the traditional IR problem of retrieving a ranked list of documents in response to a statement of information need, but has also included other tasks, called tracks, that focus on new areas or particularly difficult aspects of information retrieval. A question answering track was introduced in TREC-8 1999. The track has generated wide-spread interest in the QA problem [2, 3, 4], and has documented significant improvements in question answering system effectiveness in its two-year history.This paper provides a brief summary of the findings of the TREC question answering track to date and discusses the future directions of the track. The paper is extracted from a fuller description of the track given in "The TREC Question Answering Track" [8]. Complete details about the TREC question answering track can be found in the TREC proceedings.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {535–537},
numpages = {3},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502681,
author = {Papakonstantinou, Yannis and Vassalos, Vasilis},
title = {The Enosys Markets Data Integration Platform: Lessons from the Trenches},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502681},
doi = {10.1145/502585.502681},
abstract = {Enosys Markets offers a state-of-the-art data integration software platform to support the development of the next generation of eBusiness applications that deliver value by providing new levels of function for customer relationship management, e-commerce, supply chain management, and decision support. These applications require that data be integrated from information sources that exist both within and across organizational boundaries. The Enosys Markets data integration architecture and product family provides a complete end-to-end XML-based solution for integrating and querying distributed information sources. It incorporates advanced research into XML and database technology. We present the product architecture and components, discuss the key technical challenges, and outline the technical concepts and innovations employed in the Enosys platform.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {538–540},
numpages = {3},
keywords = {XML, integration, mediators, query forms, distributed querying, semistructured data, reports, heterogeneous databases},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502682,
author = {Zilio, Daniel and Lightstone, Sam and Lyons, Kelly and Lohman, Guy},
title = {Self-Managing Technology in IBM DB2 Universal Database},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502682},
doi = {10.1145/502585.502682},
abstract = {As the cost of both hardware and software falls due to technological advancements and economies of scale, the cost of ownership for database applications is increasingly dominated by the cost of people to manage them. Databases are growing rapidly in scale and complexity, while skilled database administrators (DBAs) are becoming rarer and more expensive. The scope of responsibility of DBAs is indeed daunting. This paper describes the self-managing technology in IBM DB2 Universal Database to illustrate how self-managing technology can enhance the usability of enterprise middleware and reduce the total cost of ownership (TCO).},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {541–543},
numpages = {3},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502683,
author = {Rosenthal, Arnon and Wiederhold, Gio},
title = {Document Release versus Data Access Controls: Two Sides of the Same Coin?},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502683},
doi = {10.1145/502585.502683},
abstract = {The database and document worlds have traditionally had different approaches to security. Databases provide access controls on structured data, while document security interrogates the outgoing information, based on document markings and actual contents. For the emerging world in which many documents are generated from structured data (and vice versa), the separation can cause failure, implementation-dependence, inconsistency, and wasted effort. After comparing approaches and mechanisms in the two areas, we identify issues in security administration and implementation in military and medical applications. We then present elements of a unifying model.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {544–546},
numpages = {3},
keywords = {access control, data security, release control, information release, protection of privacy, boundary guard, document security},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502685,
author = {Schallehn, Eike and Sattler, Kai-Uwe and Saake, Gunter},
title = {Advanced Grouping and Aggregation for Data Integration},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502685},
doi = {10.1145/502585.502685},
abstract = {New applications from the areas of analytical data processing and data integration require powerful features to condense and reconcile available data. As outlined in [1], the general concept of grouping and aggregation appears to be a fitting paradigm for a number of these issues, but in its common form of equality based groups or with current extensions like simple user-defined functions to derive group-by values on a per tuple basis and restricted aggregate functions a number of problems remain unsolved. We describe two extensions to the grouping mechanism, a generic one to support holistic user-defined grouping functions and higher level construct that provides similarity based grouping suitable in a number of applications like duplicate detection and elimination.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {547–549},
numpages = {3},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502686,
author = {Xia, Ying and Kim, Sung-Hee and Cho, Sook-Kyoung and Rim, Kee-Wook and Bae, Hae-Young},
title = {Dynamic Versioning Concurrency Control for Index-Based Data Access in Main Memory Database Systems},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502686},
doi = {10.1145/502585.502686},
abstract = {We present a concurrency control scheme using dynamic versioning for index-based data access in main memory database systems. This scheme enables read-only transactions read correct version without holding any locks or latches, while update transactions only obtain a few locks or latches without deadlocks. Efficient version management is designed to support high concurrency level and low space overhead. The interaction between dynamic versioning and indexing is considered so that all available versions can be accessed through indexing. Experiment results show that dynamic versioning can improve the performance in concurrent environment significantly.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {550–552},
numpages = {3},
keywords = {dynamic versioning scheme, version list},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502687,
author = {Kim, SungSuk and Lee, SangKeun and Jung, SoonYoung and Hwang, Chong-Sun},
title = {<i>O-PreH</i>: Optimistic Transaction Processing Algorithm Based on Pre-Reordering in Hybrid Broadcast Environments},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502687},
doi = {10.1145/502585.502687},
abstract = {In recent years, there has been a lot of research effort in the periodic push model where the server repetitively disseminates information without explicit request. We call the broadcast model supporting backchannel as hybrid broadcast. In this paper, we devise a new transaction processing algorithm called O-PreH, which is based on the notion of  pre-reordering. If one or more conflicts for mobile transactions are found from server's periodic invalidation report, conflict orders are determined not to violate the consistency( pre-reordering) and then the remaining operations have to be executed pessimistically.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {553–555},
numpages = {3},
keywords = {invalidation report, hybrid broadcast, reordering, transaction},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502688,
author = {Yan, Men Hin and Fu, Ada Wai-chee},
title = {Algorithm for Discovering Multivalued Dependencies},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502688},
doi = {10.1145/502585.502688},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {556–558},
numpages = {3},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502689,
author = {Wu, Kesheng and Otoo, Ekow J. and Shoshani, Arie},
title = {A Performance Comparison of Bitmap Indexes},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502689},
doi = {10.1145/502585.502689},
abstract = {We present a comparison of two new word-aligned schemes with some schemes for compressing bitmap indexes, including the well-known byte-aligned bitmap code (BBC). On both synthetic data and real application data, the new word-aligned schemes use only 50% more space, but perform logical operations on compressed data 12 times faster than BBC. The new schemes achieve this performance advantage by guaranteeing that during logical operations every machine instruction performs useful work on words rather than on bytes or bits as in BBC.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {559–561},
numpages = {3},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502690,
author = {Bruno, Jeanette},
title = {Facilitating Knowledge Flow through the Enterprise},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502690},
doi = {10.1145/502585.502690},
abstract = {This paper is concerned with the issues we encountered when attempting to achieve enterprise level knowledge reuse. We present 3 pilot studies where new visualization techniques were used to allow manufacturing and service operations take advantage of engineering knowledge embodied in 3D models. Though all these studies showed dramatic productivity increases, only one business unit from the studies is currently working to achieve the reuse. There are a number of reasons why this is so, but the key underlying theme is a lack of enterprise level commitment to knowledge sharing and a lack of an adequate knowledge architecture for sharing knowledge across organizational boundaries. We conclude with an approach for facilitating knowledge flow across functional units.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {562–564},
numpages = {3},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502691,
author = {Blanzieri, Enrico and Giorgini, Paolo and Recla, Sabrina and Massa, Paolo},
title = {Information Access in Implicit Culture Framework},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502691},
doi = {10.1145/502585.502691},
abstract = {The goal of a System for Implicit Culture Support (SICS) is to establish an implicit culture phenomenon, namely when the elements of a set behave according to the culture of a generally different group of agents. Earlier work claimed that Implicit Culture support can be seen as a generalization of Collaborative Filtering. In this paper, we recall the concept of Implicit Culture, show how it is useful for automatically exploit tacit knowledge and we present an implementation of a System for Implicit Culture Support.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {565–567},
numpages = {3},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502693,
author = {Bauer, Travis and Leake, David B.},
title = {Real Time User Context Modeling for Information Retrieval Agents},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502693},
doi = {10.1145/502585.502693},
abstract = {The success of personal information agents depends on their ability to provide task-relevant information. This paper presents WordSieve, a new algorithm that generates context descriptions to guide document indexing and retrieval. WordSieve exploits information about the sequence of accessed documents to identify words which indicate a shift in context. We have tested WordSieve in a personal information agent, Calvin, which monitors a user's document access, generates a representation of the user's task context, indexes the resources consulted, and presents recommendations for other resources that were consulted in similar prior contexts. In initial experiments, WordSieve outperforms term frequency/inverse document frequency at matching documents to hand-coded vector representations of the task contexts in which they were originally consulted, where the task context representations are term vectors representing a specific search task given to the user.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {568–570},
numpages = {3},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502694,
author = {Krishna, K. and Krishnapuram, Raghu},
title = {A Clustering Algorithm for Asymmetrically Related Data with Applications to Text Mining},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502694},
doi = {10.1145/502585.502694},
abstract = {Clustering techniques find a collection of subsets of a data set such that the collection satisfies a criterion that is dependent on a relation defined on the data set. The underlying relation is traditionally assumed to be symmetric. However, there exist many practical scenarios where the underlying relation is asymmetric. One example of an asymmetric relation in text analysis is the inclusion relation, i.e., the inclusion of the meaning of a block of text in the meaning of another block. In this paper, we consider the general problem of clustering of asymmetrically related data and propose an algorithm to cluster such data. To demonstrate its usefulness, we consider two applications in text mining: (1) summarization of short documents, and (2) generation of a concept hierarchy from a set of documents. Our experiments show that the performance of the proposed algorithm is superior to that of more traditional algorithms.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {571–573},
numpages = {3},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502695,
author = {Si, Luo and Callan, Jamie},
title = {A Statistical Model for Scientific Readability},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502695},
doi = {10.1145/502585.502695},
abstract = {In this paper, we present a new method of using statistical models to estimate readability [1]. Language Model is used to capture the content information. It is combined with linguistic feature model by a linear form. Experiments show that this new method has a better performance than the widely used Flesch-Kincaid readability formula.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {574–576},
numpages = {3},
keywords = {EM, Flesch-Kincaid, readability, linear interpolation, unigram language model},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502696,
author = {Liu, King-Lup and Santoso, Adrain and Yu, Clement and Meng, Weiyi},
title = {Discovering the Representative of a Search Engine},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502696},
doi = {10.1145/502585.502696},
abstract = {Given a large number of search engines on the Internet, it is difficult for a person to determine which search engines could serve his/her information needs. A common solution is to construct a metasearch engine on top of the search engines. Upon receiving a user query, the metasearch engine sends it to those underlying search engines which are likely to return the desired documents for the query. The selection algorithm used by a metasearch engine to determine whether a search engine should be sent the query typically makes the decision based on the search-engine representative, which contains characteristic information about the database of a search engine. However, an underlying search engine may not be willing to provide the needed information to the metasearch engine. This paper shows that the needed information can be estimated from an uncooperative search engine with good accuracy. Two pieces of information which permit accurate search engine selection are the number of documents indexed by the search engine and the maximum weight of each term. In this paper, we present techniques for the estimation of these two pieces of information.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {577–579},
numpages = {3},
keywords = {search engine, term weight, database size, metasearch engine},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502697,
author = {Amir, Arnon and Efrat, Alon and Srinivasan, Savitha},
title = {Advances in Phonetic Word Spotting},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502697},
doi = {10.1145/502585.502697},
abstract = {Phonetic speech retrieval is used to augment word based retrieval in spoken document retrieval systems, for in and out of vocabulary words. In this paper, we present a new indexing and ranking scheme using metaphones and a Bayesian phonetic edit distance. We conduct an extensive set of experiments using a hundred hours of HUB4 data with ground truth transcript and twenty-four thousands query words. We show improvement of up to 15% in precision compare to results obtained speech recognition alone, at a processing time of 0.5 Sec per query.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {580–582},
numpages = {3},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502698,
author = {Fu, Yongjian and Creado, Mario and Ju, Chunhua},
title = {Reorganizing Web Sites Based on User Access Patterns},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502698},
doi = {10.1145/502585.502698},
abstract = {In this paper, an approach for reorganizing Web sites based on user access patterns is proposed. The approach consists of three steps: preprocessing, page classification, and site reorganization. In preprocessing, pages on a Web site are processed to create an internal representation of the site, and page access information of its users is extracted from its server log. In page classification, the Web pages on the site are classified into two categories, index pages and content pages, based on the page access information. After the pages are classified, in site reorganization, the Web site is examined to find better ways to organize and arrange the pages on the site. Our experiments on a large real data set show that the approach is efficient and practical for adaptive Web sites.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {583–585},
numpages = {3},
keywords = {page classification, access patterns, site reorganization, web usage mining, adaptive web wites},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502699,
author = {Feldman, Ronen and Aumann, Yonatan and Liberzon, Yair and Ankori, Kfir and Schler, Jonathan and Rosenfeld, Benjamin},
title = {A Domain Independent Environment for Creating Information Extraction Modules},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502699},
doi = {10.1145/502585.502699},
abstract = {Text-Mining is a growing area of interest within the field of Data Mining and Knowledge Discovery. Given a collection of text documents, most approaches to Text Mining perform knowledge-discovery operations either on external tags associated with each document, or on the set of all words within each document. Both approaches suffer from limitations. This paper focuses on an intermediate approach, one that we call text mining via information extraction, in which knowledge discovery takes place on focused, relevant terms, phrases and facts, as extracted from the documents.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {586–588},
numpages = {3},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502700,
author = {Marcus, Andrian and Maletic, Jonathan I. and Lin, King-Ip},
title = {Ordinal Association Rules for Error Identification in Data Sets},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502700},
doi = {10.1145/502585.502700},
abstract = {A new extension of the Boolean association rules, ordinal association rules, that incorporates ordinal relationships among data items, is introduced. One use for ordinal rules is to identify possible errors in data. A method that finds these rules and identifies potential errors in data is proposed.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {589–591},
numpages = {3},
keywords = {data cleansing, association rules, ordinal rules, data mining},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502702,
author = {Neuhold, Erich J.},
title = {XML, the Web and Database Functionality?},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502702},
doi = {10.1145/502585.502702},
abstract = {With the advance of XML, DTD's, XML-Schema, X-Path, X-Link, XML-Query, XML-Protocol, RDF, Semantic WEB and other information description tools and mechanisms the WEB develops more and more into a huge structured information and data network that becomes computer processable via "semi"-automatic "intelligent" means.However, most of these multimedia information resources are kept in hierarchical file-type systems or relatively unstructured relational systems with none or very little of what is commonly known as "database functionality" for the various information types represented. Do we still need transaction properties, interoperability, security, reliability, scalability and others?If yes, then why are these concepts not really taken into the WEB-world?Are the database people too far away to be able to adopt their existing solutions?Are the information system people too process-oriented to care about data?Are the knowledge handlers too occupied with semantic representations and ontology discussions?Does the document community even see the problem?And do the WEB people care whether human resources are wasted through attempts to achieve database functionality through careful "behaviour"?The panel will offer opinions about these aspects but will also want to raise these issues with the audience.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {592},
numpages = {1},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

@inproceedings{10.1145/502585.502703,
author = {Rosenthal, Arnon},
title = {What Can Researchers Do to Improve Security of Data and Documents? (Panel Description)},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502703},
doi = {10.1145/502585.502703},
abstract = {Data security (protection of information rather than systems) goes far beyond the traditional questions of RDBMS grant/revole, or security markings on documents. We will discuss what the new research agenda should be to impact the masses of systems.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {593},
numpages = {1},
keywords = {database, information release, application server, security, document server, intellectual property},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}

