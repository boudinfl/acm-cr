@inproceedings{10.1145/1031171.1031176,
author = {Fan, Wenfei and Garofalakis, Minos and Xiong, Ming and Jia, Xibei},
title = {Composable XML Integration Grammars},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031176},
doi = {10.1145/1031171.1031176},
abstract = {The proliferation of XML as a standard for data representation and exchange in diverse, next-generation Web applications has created an emphatic need for effective XML data-integration tools. For several real-life scenarios, such XML data integration needs to be <i>DTD-directed</i> -- in other words, the target, integrated XML database must conform to a prespecified, user- or application-defined DTD. In this paper, we propose a novel formalism, <i>XML Integration Grammars (XIGs)</i>, for specifying DTD-directed integration of XML data. Abstractly, an XIG maps data from multiple XML sources to a target XML document that conforms to a predefined DTD. An XIG extracts source XML data via queries expressed in a fragment of XQuery, and controls target document generation with tree-valued attributes and the target DTD. The novelty of XIGs consists in not only their automatic support for DTD-conformance but also in their: an XIG may embed local and remote XIGs in its definition, and invoke these XIGs during its evaluation. This yields an important modularity property for our XIGs that allows one to divide a complex integration task into manageable sub-tasks and conquer each of them separately. To efficiently evaluate XIGs we provide algorithms for merging XML queries in an XIG and for scheduling queries and embedded XIGs. These lead to an effective framework, as well as a design tool for XQuery, for effectively specifying and computing complex, DTD-directed XML integration.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {2–11},
numpages = {10},
keywords = {XML, data integration, grammar},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031177,
author = {He, Qi and Ling, Tok Wang},
title = {Extending and Inferring Functional Dependencies in Schema Transformation},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031177},
doi = {10.1145/1031171.1031177},
abstract = {We study the representation, derivation and utilization of a special kind of constraints in multidatabase systems. A major challenge is when component database schemas are <i>schematic discrepant</i> from each other, i.e., data values of one database correspond to schema labels of another. We propose "qualified functional dependencies" (or qualified FDs), an extension to conventional FDs to formalize integrity constraints in multidatabase systems. We first give inference rules to derive qualified FDs in fixed schemas, then study the derivation of qualified FDs during the transformations between schematic discrepant schemas. Propagation rules are given to derive qualified FDs of transformed schemas from qualified FDs of original schemas. Our work can be used in different stages of building and accessing a multidatabase system, e.g., to detect and resolve value inconsistency in schema integration, to verify lossless schema transformations, to normalize integrated schemas, to verify the integrity of data, and to optimize queries at an integration level. In particular, as an application of our theory, we will use FDs to check the validity of SchemaSQL views (SchemaSQL is a powerful multidatabase language).},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {12–21},
numpages = {10},
keywords = {functional dependency, multidatabase, schema integration, schematic discrepancy},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031178,
author = {He, Bin and Tao, Tao and Chang, Kevin Chen-Chuan},
title = {Organizing Structured Web Sources by Query Schemas: A Clustering Approach},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031178},
doi = {10.1145/1031171.1031178},
abstract = {In the recent years, the Web has been rapidly "deepened" with the prevalence of databases online. On this deep Web, many sources are <i>structured</i> by providing structured query interfaces and results. Organizing such structured sources into a domain hierarchy is one of the critical steps toward the integration of heterogeneous Web sources. We observe that, for structured Web sources, query schemas <i>ie</i>, attributes in query interfaces) are discriminative representatives of the sources and thus can be exploited for source characterization. In particular, by viewing query schemas as a type of categorical data, we abstract the problem of source organization into the clustering of categorical data. Our approach hypothesizes that "homogeneous sources" are characterized by the same hidden generative models for their schemas. To find clusters governed by such statistical distributions, we propose a new objective function, <i>model-differentiation</i>, which employs principled hypothesis testing to maximize statistical heterogeneity among clusters. Our evaluation over hundreds of real sources indicates that (1) the schema-based clustering accurately organizes sources by object domains <i>eg</i>, Books, Movies), and (2) on clustering Web query schemas, the model-differentiation function outperforms existing ones, such as likelihood, entropy, and context linkages, with the hierarchical agglomerative clustering algorithm.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {22–31},
numpages = {10},
keywords = {deep Web, hierarchical agglomerative clustering, data integration},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@dataset{10.1145/review-1031171.1031178_R38564,
author = {Artz, John M.},
title = {Review ID:R38564 for DOI: 10.1145/1031171.1031178},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1031171.1031178_R38564}
}

@inproceedings{10.1145/1031171.1031180,
author = {Si, Luo and Callan, Jamie},
title = {Unified Utility Maximization Framework for Resource Selection},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031180},
doi = {10.1145/1031171.1031180},
abstract = {This paper presents a unified utility framework for resource selection of distributed text information retrieval. This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases. With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications. Specifically, when used for database recommendation, the selection is optimized for the goal of high-recall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents). This new model provides a more solid framework for distributed information retrieval. Empirical studies show that it is at least as effective as other state-of-the-art algorithms.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {32–41},
numpages = {10},
keywords = {distributed information retrieval, resource selection},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031181,
author = {Robertson, Stephen and Zaragoza, Hugo and Taylor, Michael},
title = {Simple BM25 Extension to Multiple Weighted Fields},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031181},
doi = {10.1145/1031171.1031181},
abstract = {This paper describes a simple way of adapting the BM25 ranking formula to deal with structured documents. In the past it has been common to compute scores for the individual fields (e.g. title and body) independently and then combine these scores (typically linearly) to arrive at a final score for the document. We highlight how this approach can lead to poor performance by breaking the carefully constructed non-linear saturation of term frequency in the BM25 function. We propose a much more intuitive alternative which weights term frequencies <i>before</i> the non-linear term frequency saturation function is applied. In this scheme, a structured document with a title weight of two is mapped to an unstructured document with the title content repeated twice. This more verbose unstructured document is then ranked in the usual way. We demonstrate the advantages of this method with experiments on Reuters Vol1 and the TREC dotGov collection.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {42–49},
numpages = {8},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031182,
author = {Terra, Egidio and Clarke, Charles L.A.},
title = {Scoring Missing Terms in Information Retrieval Tasks},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031182},
doi = {10.1145/1031171.1031182},
abstract = {An usual approach to address mismatching vocabulary problem is to augment the original query using dictionaries and other lexical resources and/or by looking at pseudo-relevant documents. Either way, terms are added to form a new query that will be used to score all documents in a subsequent retrieval pass, and as consequence the original query's focus may drift because of the newly added terms. We propose a new method to address the mismatching vocabulary problem, expanding original query terms only when necessary and complementing the user query for missing terms while scoring documents. It allows related semantic aspects to be included in a conservative and selective way, thus reducing the possibility of query drift. Our results using replacements for the <i>missing query terms</i> in modified document and passages retrieval methods show significant improvement over the original ones.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {50–58},
numpages = {9},
keywords = {passage retrieval, automatic query expansion, document retrieval},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031184,
author = {Siersdorfer, Stefan and Sizov, Sergej and Weikum, Gerhard},
title = {Goal-Oriented Methods and Meta Methods for Document Classification and Their Parameter Tuning},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031184},
doi = {10.1145/1031171.1031184},
abstract = {Automatic text classification methods come with various calibration parameters such as thresholds for probabilities in Bayesian classifiers or for hyperplane distances in SVM classifiers. In a given application context these parameters should be set so as to meet the relative importance of various result quality metrics such as precision versus recall. In this paper we consider classifiers that can accept a document for a topic, reject it, or abstain. We aim to meet the application's goals in terms of accuracy (i.e., avoid false acceptances or rejections) and loss (i.e., limit the fraction of documents for which no decision is made). To this end we investigate restrictive forms of Support Vector Machine classifiers and we develop meta methods that split the training data into subsets for independently trained classifiers and then combine the results of these classifiers. These techniques tend to improve accuracy at the expense of document loss. We develop estimators that help to predict the accuracy and loss for a given setting of the methods' tuning parameters, and a methodology for efficiently deriving a setting that meets the application's goals. Our experiments confirm the practical viability of the approach.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {59–68},
numpages = {10},
keywords = {meta classification, restrictive classification},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031185,
author = {Mekhaldi, Dalila and Lalanne, Denis and Ingold, Rolf},
title = {Using Bi-Modal Alignment and Clustering Techniques for Documents and Speech Thematic Segmentations},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031185},
doi = {10.1145/1031171.1031185},
abstract = {In this paper, we describe a new method for a simultaneous thematic segmentation of the meeting dialogs and the documents discussed or visible throughout the meeting. This bi-modal method is suitable for multimodal applications that are centered on documents, such as meetings and lectures, where documents can be aligned with meeting dialogs. Bringing into play this alignment, our bi-modal segmentation method first transforms its results into a set of nodes in a 2D graph space, where the two axes represent respectively the document units and the meeting dialogs units. Secondly, via a clustering method, the most connected regions in the constituted bi-graph are detected. Finally, the denser clusters are projected on the two axes. The two sequences of segments, obtained on both axes, represent the thematic structure of the document and of the meeting dialogs respectively. We present in this article this bi-modal segmentation technique and its performance compared with two mono-modal segmentation methods.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {69–77},
numpages = {9},
keywords = {thematic alignment, k-means clustering, thematic segmentation},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031186,
author = {Cai, Lijuan and Hofmann, Thomas},
title = {Hierarchical Document Categorization with Support Vector Machines},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031186},
doi = {10.1145/1031171.1031186},
abstract = {Automatically categorizing documents into pre-defined topic hierarchies or taxonomies is a crucial step in knowledge and content management. Standard machine learning techniques like Support Vector Machines and related large margin methods have been successfully applied for this task, albeit the fact that they ignore the inter-class relationships. In this paper, we propose a novel hierarchical classification method that generalizes Support Vector Machine learning and that is based on discriminant functions that are structured in a way that mirrors the class hierarchy. Our method can work with arbitrary, not necessarily singly connected taxonomies and can deal with task-specific loss functions. All parameters are learned jointly by optimizing a common objective function corresponding to a regularized upper bound on the empirical loss. We present experimental results on the WIPO-alpha patent collection to show the competitiveness of our approach.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {78–87},
numpages = {10},
keywords = {SVM, hierarchical loss, taxonomy, document categorization, subspace optimization, class relationship},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031188,
author = {Wu, Kun-Lung and Chen, Shyh-Kwei and Yu, Philip S.},
title = {Interval Query Indexing for Efficient Stream Processing},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031188},
doi = {10.1145/1031171.1031188},
abstract = {A large number of continual range queries can be issued against a data stream. Usually, a main memory-based query index with a small storage cost and a fast search time is needed, especially if the stream is rapid. In this paper, we present a CEI-based query index that meets both criteria for efficient processing of continual interval queries in a streaming environment. This new query index is centered around a set of predefined virtual <i>containment-encoded intervals</i>, or CEIs. The CEIs are used to first decompose query intervals and then perform efficient search operations. The CEIs are defined and labeled such that containment relationships among them are encoded in their IDs. The containment encoding makes decomposition and search operations efficient because integer additions and logical shifts can be used to carry out most of the operations. Simulations are conducted to evaluate the effectiveness of the CEI-based query index and to compare it with alternative approaches. The results show that the CEI-based query index significantly outperforms existing approaches in terms of both storage cost and search time.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {88–97},
numpages = {10},
keywords = {data streams, continual queries, query monitoring, query indexing, interval indexing},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031189,
author = {Ding, Luping and Rundensteiner, Elke A.},
title = {Evaluating Window Joins over Punctuated Streams},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031189},
doi = {10.1145/1031171.1031189},
abstract = {We explore join optimizations in the presence of both time-based constraints (sliding windows) and value-based constraints (punctuations). We present the first join solution named PWJoin that exploits such combined constraints to shrink the runtime join state and to propagate punctuations to benefit downstream operators. We design a state structure for PWJoin that facilitates the exploitation of both constraint types. We also explore optimizations enabled by the interactions between window and punctuation, e.g., early punctuation propagation. The costs of the PWJoin are analyzed using a cost model. We also conduct an experimental study using CAPE continuous query system. The experimental results show that in most cases, by exploiting punctuations, PWJoin outperforms the pure window join with regard to both memory overhead and throughput. Our technique complements the joins in the literature, such as symmetric hash join or window join, to now require less runtime resources without compromising the accuracy of the result.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {98–107},
numpages = {10},
keywords = {sliding window, punctuation, join algorithm, streaming data processing},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031190,
author = {Chen, Yi and Mihaila, George A. and Davidson, Susan B. and Padmanabhan, Sriram},
title = {EXPedite: A System for Encoded XML Processing},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031190},
doi = {10.1145/1031171.1031190},
abstract = {As XML becomes an increasingly popular format for information exchange, the efficient processing of broadcast XML data on a constrained device (for example, a cell phone or a PDA) becomes a critical task. In this paper we present the EXPedite system: a new model of data processing in an information exchange environment, which "migrates" the power of the data-sending server to receivers for efficient processing. It consists of a simple and general encoding scheme for servers, and streaming query processing algorithms on encoded XML stream for data receivers with constrained computing abilities. Experiments show the impressive performance of EXPedite.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {108–117},
numpages = {10},
keywords = {binary encoding, XML, query processing, XPath},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031192,
author = {Xue, Gui-Rong and Zeng, Hua-Jun and Chen, Zheng and Yu, Yong and Ma, Wei-Ying and Xi, WenSi and Fan, WeiGuo},
title = {Optimizing Web Search Using Web Click-through Data},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031192},
doi = {10.1145/1031171.1031192},
abstract = {The performance of web search engines may often deteriorate due to the diversity and noisy information contained within web pages. User click-through data can be used to introduce more accurate description (metadata) for web pages, and to improve the search performance. However, noise and incompleteness, sparseness, and the volatility of web pages and queries are three major challenges for research work on user click-through log mining. In this paper, we propose a novel iterative reinforced algorithm to utilize the user click-through data to improve search performance. The algorithm fully explores the interrelations between queries and web pages, and effectively finds "virtual queries" for web pages and overcomes the challenges discussed above. Experiment results on a large set of MSN click-through log data show a significant improvement on search performance over the naive query log mining algorithm as well as the baseline search engine.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {118–126},
numpages = {9},
keywords = {click-through data, log mining, search engine, iterative algorithm},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031193,
author = {Chuang, Shui-Lung and Chien, Lee-Feng},
title = {A Practical Web-Based Approach to Generating Topic Hierarchy for Text Segments},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031193},
doi = {10.1145/1031171.1031193},
abstract = {It is crucial in many information systems to organize short text segments, such as keywords in documents and queries from users, into a well-formed topic hierarchy. In this paper, we address the problem of generating topic hierarchies for diverse text segments with a general and practical approach that uses the Web as an additional knowledge source. Unlike long documents, short text segments typically do not contain enough information to extract reliable features. This work investigates the possibilities of using highly ranked search-result snippets to enrich the representation of text segments. A hierarchical clustering algorithm is then applied to create the hierarchical topic structure of text segments. Different from traditional clustering algorithms, which tend to produce cluster hierarchies with a very unnatural shape, the approach tries to produce a more natural and comprehensive hierarchy. Extensive experiments were conducted on different domains of text segments. The obtained results have shown the potential of the proposed approach, which is believed able to benefit many information systems.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {127–136},
numpages = {10},
keywords = {clustering, partitioning, topic hierarchy generation, search-result snippet, web data mining, text segment},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031194,
author = {Pasca, Marius},
title = {Acquisition of Categorized Named Entities for Web Search},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031194},
doi = {10.1145/1031171.1031194},
abstract = {The recognition of names and their associated categories within unstructured text traditionally relies on semantic lexicons and gazetteers. The amount of effort required to assemble large lexicons confines the recognition to either a limited domain (e.g., <i>medical imaging</i>), or a small set of pre-defined, broader categories of interest (e.g., <i>persons</i>, <i>countries</i>, <i>organizations</i>, <i>products</i>). This constitutes a serious limitation in an information seeking context. In this case, the categories of potential interest to users are more diverse (<i>universities</i>, <i>agencies</i>, <i>retailers</i>, <i>celebrities</i>), often refined (e.g., <i>SLR digital cameras</i>, <i>programming languages</i>, <i>multinational oil companies</i>), and usually overlapping (e.g., the same entity may be concurrently a <i>brand name</i>, a <i>technology company</i>, and an <i>industry leader</i>). We present a lightly supervised method for acquiring named entities in arbitrary categories. The method applies lightweight lexico-syntactic extraction patterns to the unstructured text of Web documents. The method is a departure from traditional approaches to named entity recognition in that: 1) it does not require any start-up seed names or training; 2) it does not encode any domain knowledge in its extraction patterns; 3) it is only lightly supervised, and data-driven; 4) it does not impose any a-priori restriction on the categories of extracted names. We illustrate applications of the method in Web search, and describe experiments on 500 million Web documents and news articles.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {137–145},
numpages = {9},
keywords = {named entity extraction, related names and categories, information integration, lightweight text processing, web information retrieval},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@dataset{10.1145/review-1031171.1031194_R39466,
author = {Gelbukh, Alexander},
title = {Review ID:R39466 for DOI: 10.1145/1031171.1031194},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1031171.1031194_R39466}
}

@inproceedings{10.1145/1031171.1031196,
author = {Song, Yang and Bhowmick, Sourav S.},
title = {BioDIFF: An Effective Fast Change Detection Algorithm for Genomic and Proteomic Data},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031196},
doi = {10.1145/1031171.1031196},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {146–147},
numpages = {2},
keywords = {biological data, biological data integration, change detection, algorithm},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031197,
author = {Aghili, S. Alireza and Agrawal, Divyakant and El Abbadi, Amr},
title = {Protein Structure Alignment Using Geometrical Features},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031197},
doi = {10.1145/1031171.1031197},
abstract = {A novel approach for similarity search on protein structure databases is proposed which incorporates the three dimensional coordinates of the main atoms of each amino acid and extracts a geometrical signature along with the direction of the given amino acid. As a result, each protein is presented by a series of feature vectors representing local geometry, shape, direction, and secondary structure assignment of its amino acid constituents. Furthermore, a residue-to-residue distance matrix is calculated and is incorporated into a local alignment dynamic programming algorithm to find the similar portions of two given proteins and finally a sequence alignment step is used as the last filtration step. The optimal superimposition of the detected similar regions is used to assess the quality of the results. The proposed algorithm is fast and accurate and hence could be used for the analysis of large protein structure similarity.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {148–149},
numpages = {2},
keywords = {shape similarity, biological data mining, biological databases, protein structure alignment},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031198,
author = {Chung, Seokkyung and Jun, Jongeun and McLeod, Dennis},
title = {Mining Gene Expression Datasets Using Density-Based Clustering},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031198},
doi = {10.1145/1031171.1031198},
abstract = {Given the recent advancement of microarray technologies, we present a density-based clustering approach for the purpose of co-expressed gene cluster identification. The underlying hypothesis is that a set of co-expressed gene clusters can be used to reveal a common biological function. By addressing the strengths and limitations of previous density-based clustering approaches, we present a novel clustering algorithm that utilizes a neighborhood defined by <i>k</i>-nearest neighbors. Experimental results indicate that the proposed method identifies biologically meaningful and co-expressed gene clusters.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {150–151},
numpages = {2},
keywords = {density-based clustering, gene expression analysis, microarray analysis},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031199,
author = {Li, Tao and Ogihara, Mitsunori},
title = {Semi-Supervised Learning for Music Artists Style Identification},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031199},
doi = {10.1145/1031171.1031199},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {152–153},
numpages = {2},
keywords = {content, artist style, lyrics, semi-supervised learning},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inbook{10.1145/1031171.1031200,
author = {Shen, Jialie and Shepherd, John and Ngu, Anne H.H.},
title = {Integrating Heterogeneous Reatures for Efficient Content Based Music Retrieval},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031200},
abstract = {In this paper, we present a novel feature extraction method facilitating efficient content-based music retrieval and classification, called <i>InMAF</i>. The goal of our approach is to allow straightforward incorporation of multiple musical features, such as timbral texture, pitch and rhythm structure, into a single low dimensional vector that is effective for retrieval and classification. Unlike earlier approaches that used only acoustic properties as the basis for retrieval, our approach can easily incoporate human music perception to improve accuracy of retrieval and classification process. The superiority of our method is demonstrated by comparing it with state-of-the-art approaches in the areas of music classification (using a variety of machine learning algorithms), query effectiveness and robustness against audio distortion.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {154–155},
numpages = {2}
}

@inproceedings{10.1145/1031171.1031201,
author = {Si, Luo and Jin, Rong},
title = {Unified Filtering by Combining Collaborative Filtering and Content-Based Filtering via Mixture Model and Exponential Model},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031201},
doi = {10.1145/1031171.1031201},
abstract = {Collaborative filtering and content-based filtering are two types of information filtering techniques. Combining these two techniques can improve the recommendation effectiveness. The main problem with previous research is that the content information and the rating information are not combined in an integrated way. This paper presents a unified probabilistic framework that allows the mutual interaction between these two types of information. Experiments have shown that the new unified filtering algorithm outperforms a pure collaborative filtering approach, a pure content-based filtering approach and another unified filtering algorithm.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {156–157},
numpages = {2},
keywords = {unified filtering},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031202,
author = {Ma, Yiming and Zhong, Qi and Mehrotra, Sharad and Seid, Dawit Yimam},
title = {A Framework for Refining Similarity Queries Using Learning Techniques},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031202},
doi = {10.1145/1031171.1031202},
abstract = {In numerous applications that deal with similarity search, a user may not have an exact idea of his information need and/or may not be able to construct a query that exactly captures his notion of similarity. A promising approach to mitigate this problem is to enable the user to submit a rough approximation of the desired query and use the feedback on the relevance of the retrieved objects to refine the query. In this paper, we explore such a refinement strategy for a general class of SQL similarity queries. Our approach casts the refinement problem as that of learning concepts using examples. This is achieved by viewing the tuples on which a user provides feedback as a labeled training set for a learner. Under this setup, SQL query refinement consists of two learning tasks, namely learning the structure of the SQL query and learning the relative importance of the query components. The paper develops appropriate machine learning approaches suitable for these two learning tasks. The primary contribution of the paper is a general refinement framework that decides when each learner is invoked in order to quickly learn the user query. Experimental analyses over many real life datasets and queries show that our strategy outperforms the existing approaches significantly in terms of retrieval accuracy and query simplicity.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {158–159},
numpages = {2},
keywords = {learning, refinement, relevance feedback, structured data},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031203,
author = {Megalooikonomou, Vasileios and Li, Guo and Wang, Qiang},
title = {A Dimensionality Reduction Technique for Efficient Similarity Analysis of Time Series Databases},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031203},
doi = {10.1145/1031171.1031203},
abstract = {Efficiently searching for similarities among time series and discovering interesting patterns is an important and non-trivial problem with applications in many domains. The high dimensionality of the data makes the analysis very challenging. To solve this problem, many dimensionality reduction methods have been proposed. PCA (Piecewise Constant Approximation) and its variant have been shown efficient in time series indexing and similarity retrieval. However, in certain applications, too many false alarms introduced by the approximation may reduce the overall performance dramatically. In this paper, we introduce a new piecewise dimensionality reduction technique that is based on Vector Quantization. The new technique, PVQA (Piecewise Vector Quantized Approximation), partitions each sequence into equi-length segments and uses vector quantization to represent each segment by the closest (based on a distance metric) codeword from a codebook of key-sequences. The efficiency of calculations is improved due to the significantly lower dimensionality of the new representation. We demonstrate the utility and efficiency of the proposed technique on real and simulated datasets. By exploiting prior knowledge about the data, the proposed technique generally outperforms PCA and its variants in similarity searches.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {160–161},
numpages = {2},
keywords = {time series, dimensionality reduction, data mining},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031204,
author = {Zhang, Baoping and Gon\c{c}alves, Marcos Andr\'{e} and Fan, Weiguo and Chen, Yuxin and Fox, Edward A. and Calado, P\'{a}vel and Cristo, Marco},
title = {Combining Structural and Citation-Based Evidence for Text Classification},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031204},
doi = {10.1145/1031171.1031204},
abstract = {This paper discusses how citation-based information and structural content (e.g., title, abstract) can be combined to improve classification of text documents into predefined categories. We evaluate different measures of similarity derived from the citation structure and the structural content of the collection, and determine how they can be fused to improve classification effectiveness. To discover the best fusion framework, we apply Genetic Programming (GP) techniques. Our empirical experiments using documents from the ACM Digital Library and the ACM Computing Classification System show that we can discover similarity functions that work better than using evidence in isolation and whose combined performance through a simple majority voting is comparable to that of Support Vector Machine classifiers.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {162–163},
numpages = {2},
keywords = {classification, document similarity, GP, citation analysis},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031205,
author = {Ma, Ling and Goharian, Nazli},
title = {Using Relevance Feedback to Detect Misuse for Information Retrieval Systems},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031205},
doi = {10.1145/1031171.1031205},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {164–165},
numpages = {2},
keywords = {detection, security, misuse, relevance feedback},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031206,
author = {Chen, Jianwen and Zhang, Yan},
title = {An Extended Logic Programming Based Multi-Agent System Formalization in Mobile Environments},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031206},
doi = {10.1145/1031171.1031206},
abstract = {In this paper, we propose an extended logic programming based formalization for a multi-agent system in mobile environments. Such a system consists of a number of agents connected via wire or wireless communication channels. Our formalization is knowledge oriented and has declarative semantics inherited from extended logic programming. This model can be used to study the details of knowledge transaction in mobile environments. },
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {166–167},
numpages = {2},
keywords = {extended logic programming, mobile environments, multi-agent system},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031208,
author = {Gollapudi, Sreenivas and Sivakumar, D.},
title = {Framework and Algorithms for Trend Analysis in Massive Temporal Data Sets},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031208},
doi = {10.1145/1031171.1031208},
abstract = {Mining massive temporal data streams for significant trends, emerging buzz, and unusually high or low activity is an important problem with several commercial applications. In this paper, we propose a framework based on relational records and metric spaces to study such problems. Our framework provides the necessary mathematical underpinnings for this genre of problems, and leads to efficient algorithms in the stream/sort model of massive data sets (where the algorithm makes passes over the data, computes a new stream on the fly, and is allowed to sort the intermediate data). Our algorithm makes novel use of metric approximations in the data stream context, and highlights the role of hierarchical organization of large data sets in designing efficient algorithms in the stream/sort model.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {168–177},
numpages = {10},
keywords = {taxonomies, data stream algorithms, trend analysis, metric approximations, hierarchically partitioned data},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031209,
author = {Wang, Ke and Xu, Yabo and Yu, Jeffrey Xu},
title = {Scalable Sequential Pattern Mining for Biological Sequences},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031209},
doi = {10.1145/1031171.1031209},
abstract = {Biosequences typically have a small alphabet, a long length, and patterns containing gaps (i.e., "don't care") of arbitrary size. Mining frequent patterns in such sequences faces a different type of explosion than in transaction sequences primarily motivated in market-basket analysis. In this paper, we study how this explosion affects the classic sequential pattern mining, and present a scalable two-phase algorithm to deal with this new explosion. The <i>Segment Phase</i> first searches for short patterns containing no gaps, called <i>segments</i>. This phase is efficient. The <i>Pattern Phase</i> searches for long patterns containing multiple segments separated by variable length gaps. This phase is time consuming. The purpose of two phases is to exploit the information obtained from the first phase to speed up the pattern growth and matching and to prune the search space in the second phase. We evaluate this approach on synthetic and real life data sets.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {178–187},
numpages = {10},
keywords = {sequential pattern, pruning technique, algorithm, bioinformatics, sequence, frequent pattern},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031210,
author = {Zhao, Qiankun and Bhowmick, Sourav S. and Mohania, Mukesh and Kambayashi, Yahiko},
title = {Discovering Frequently Changing Structures from Historical Structural Deltas of Unordered XML},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031210},
doi = {10.1145/1031171.1031210},
abstract = {Recently, a large amount of work has been done in XML data mining. However, we observed that most of the existing works focus on the snapshot XML data, while XML data is dynamic in real applications. To the best of our knowledge, none of the existing works has addressed the issue of mining the history of changes to XML documents. Such mining results can be useful in many applications such as XML change detection, XML indexing, association rule mining, and classification etc. In this paper, we propose a novel approach to discover the <i>frequently changing structures</i> from the sequence of historical <i>structural deltas</i> of unordered XML. To make the structure discovering process efficient, an expressive and compact data model, <b>H</b>istorical-<b>D</b>ocument <b>O</b>bject <b>M</b>odel (<b>H-DOM</b>), is proposed. Using this model, two basic algorithms, which can discover all the <i>frequently changing structures</i> with only two scans of the XML sequence, are presented. Experimental results show that our algorithms, together with the optimization techniques, are efficient and scalable.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {188–197},
numpages = {10},
keywords = {data mining, XML},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031212,
author = {Hore, Bijit and Hacigumus, Hakan and Iyer, Bala and Mehrotra, Sharad},
title = {Indexing Text Data under Space Constraints},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031212},
doi = {10.1145/1031171.1031212},
abstract = {An important class of queries is the LIKE predicate in SQL. In the absence of an index, LIKE queries are subject to performance degradation. The notion of indexing on substrings (or <i>q</i>-grams) has been explored earlier without sufficient consideration of efficiency. <i>q</i>-grams are used to prune away rows that do not qualify for the query. The problem is to identify a finite number of grams subject to storage constraint that gives maximal pruning for a given query workload. Our contributions include: i) a formal problem definition, that produces results within a provable error bound, ii) performance evaluation of the application of the novel method to real data, and iii) parallelization of the algorithm, scaling considerations and a proposal to handle scaling issues.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {198–207},
numpages = {10},
keywords = {q-grams, SQL, B-tree, like queries, index},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031213,
author = {Lv, Qin and Charikar, Moses and Li, Kai},
title = {Image Similarity Search with Compact Data Structures},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031213},
doi = {10.1145/1031171.1031213},
abstract = {The recent theoretical advances on compact data structures (also called "sketches") have raised the question of whether they can effectively be applied to content-based image retrieval systems. The main challenge is to derive an algorithm that achieves high-quality similarity searches while using compact metadata. This paper proposes a new similarity search method consisting of three parts. The first is a new region feature representation with weighted $=<i></i><inf>1</inf> distance function, and EMD* match, an improved EMD match, to compute image similarity. The second is a thresholding and transformation algorithm to convert feature vectors into very compact data structures. The third is an EMD embedding based filtering method to speed up the query process. We have implemented a prototype system with the proposed method and performed experiments with a 10,000 image database. Our results show that the proposed method can achieve more effective similarity searches than previous approaches with metadata 3 to 72 times more compact than previous systems. The experiments also show that our EMD embedding based filtering technique can speed up the query process by a factor of 5 or more with little loss in query effectiveness.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {208–217},
numpages = {10},
keywords = {search, compact data structures, image similarity},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031214,
author = {Pisharath, Jayaprakash and Choudhary, Alok and Kandemir, Mahmut},
title = {Energy Management Schemes for Memory-Resident Database Systems},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031214},
doi = {10.1145/1031171.1031214},
abstract = {With the tremendous growth of system memories, memory-resident databases are increasingly becoming important in various domains. Newer memories provide a structured way of storing data in multiple chips, with each chip having a bank of memory modules. Current memory-resident databases are yet to take full advantage of the banked storage system, which offers a lot of room for performance and energy optimizations. In this paper, we identify the implications of a banked memory environment in supporting memory-resident databases, and propose hardware (memory-directed) and software (query-directed) schemes to reduce the energy consumption of queries executed on these databases. Our results show that high-level query-directed schemes (hosted in the query optimizer) better utilize the low-power modes in reducing the energy consumption than the respective hardware schemes (hosted in the memory controller), due to their complete knowledge of query access patterns. We extend this further and propose a query restructuring scheme and a multi-query optimization. Queries are restructured and regrouped based on their table access patterns to maximize the likelihood that data accesses are clustered. This helps increase the inter-access idle times of memory modules, which in turn enables a more effective control of their energy behavior. This heuristic is eventually integrated with our hardware optimizations to achieve maximum savings. Our experimental results show that the memory energy reduces by 90% if query restructuring method is applied along with basic energy optimizations over the unoptimized version. The system-wide performance impact of each scheme is also studied simultaneously.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {218–227},
numpages = {10},
keywords = {DRAM, multiquery optimization, hardware energy scheme, database, power consumption, energy, query-directed energy management},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031216,
author = {Liu, Bin and Rundensteiner, Elke A. and Finkel, David},
title = {Restructuring Batch View Maintenance Efficiently},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031216},
doi = {10.1145/1031171.1031216},
abstract = {Materialized views defined over distributed data sources are a well recognized technology for modern applications. State-of the-art incremental view maintenance requires O(<i>n</i><sup>2</sup>) or more maintenance queries to remote data sources with <i>n</i> being the number of data sources in the view definition. In this poster, we illustrate basic ideas of novel view maintenance strategies that dramatically reduce the number of maintenance queries. Such reduction brings the tradeoff between the number of maintenance queries and the complexity of each query. These algorithms have been implemented in a working prototype system. Experimental studies illustrate major performance improvement in terms of total processing time compared with existing batch algorithms.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {228–229},
numpages = {2},
keywords = {grouping maintenance, batch maintenance},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031217,
author = {Kumaran, A. and Haritsa, Jayant R.},
title = {On Semantic Matching of Multilingual Attributes in Relational Systems},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031217},
doi = {10.1145/1031171.1031217},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {230–231},
numpages = {2},
keywords = {multilingual databases, semantic information retrieval, ontology-based, query processing},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031218,
author = {Huang, Weiyun and Omiecinski, Edward and Mark, Leo},
title = {Compression Schemes for Differential Categorical Stream Clustering},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031218},
doi = {10.1145/1031171.1031218},
abstract = {Stream data analysis differs significantly from traditional data processing. To process the data online the algorithm has to work in one pass, incorporating new data into a model maintained in main memory. Storing a model or synopsis of processed data in the memory, which we call "data compression", is an important technique in both incremental and differential stream mining. This paper proposes several data compression schemes in one-pass categorical data clustering, and demonstrates their performance on synthetic and real data. Our compression schemes can efficiently generate compact representations of original data, so as to enable the algorithm to process streams at high speed and detect the changes in underlying data. The example algorithm based on these compression schemes achieves good accuracy in short execution time.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {232–233},
numpages = {2},
keywords = {categorical, stream, compression schemes, differential},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031219,
author = {Zou, Qinghua and Liu, Shaorong and Chu, Wesley W.},
title = {Using a Compact Tree to Index and Query XML Data},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031219},
doi = {10.1145/1031171.1031219},
abstract = {Indexing XML is crucial for efficient XML query processing. We propose a compact tree (Ctree) for XML indexing, which provides not only concise path summaries at group level but also detailed child-parent relationships at element level. Based on Ctree, we are able to measure how well XML data is structured. We also propose a three-step query processing method. Its efficiency is achieved by: (1) summarizing large XML data structures into a condensed Ctree; (2) pruning irrelevant groups to significantly reduce the search space; (3) eliminating join operations between the matches for value predicates and those for structure constraints and (4) using Ctree properties such as regular groups to reduce query processing time. Our experiments reveal that Ctree is an effective data structure for managing XML data.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {234–235},
numpages = {2},
keywords = {path summary, XQuery processing, compact tree, early pruning, XML index, inverted file clustering},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031220,
author = {Cronen-Townsend, Steve and Zhou, Yun and Croft, W. Bruce},
title = {A Framework for Selective Query Expansion},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031220},
doi = {10.1145/1031171.1031220},
abstract = {Query expansion is a well-known technique that has been shown to improve <i>average</i> retrieval performance. This technique has not been used in many operational systems because of the fact that it can greatly degrade the performance of some individual queries. We show how comparison between language models of the unexpanded and expanded retrieval results can be used to predict when the expanded retrieval has strayed from the original sense of the query. In these cases, the unexpanded results are used while the expanded results are used in the remaining cases (where such straying is not detected). We evaluate this method on a wide variety of TREC collections.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {236–237},
numpages = {2},
keywords = {language modeling, query expansion, clarity},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031221,
author = {Ravindran, Devanand and Gauch, Susan},
title = {Exploiting Hierarchical Relationships in Conceptual Search},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031221},
doi = {10.1145/1031171.1031221},
abstract = {As the number of available Web pages grows, users experience increasing difficulty finding documents relevant to their interests. One of the underlying reasons for this is that most search engines find matches based on keywords, regardless of their meanings. To provide the user with more useful information, we need a system that disambiguates queries by including information about the user's conceptual framework. This is the goal of KeyConcept, a conceptual search engine. During indexing, KeyConcept automatically classifies documents into concepts selected from a reference concept hierarchy. During retrieval, KeyConcept ranks documents based on a combination of keyword and conceptual similarity. This paper describes the system architecture and discusses the results of experiments that evaluate the effect of exploiting the hierarchical relationships between concepts during retrieval. Our results confirm that conceptual match significantly improves the precision of the search results over keyword match alone. In addition, the use of the concept hierarchy to prune irrelevant search results also significantly increases precision.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {238–239},
numpages = {2},
keywords = {text classification, personalized search, ontologies, conceptual search},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031222,
author = {Xue, Gui-Rong and Zeng, Hua-Jun and Chen, Zheng and Yu, Yong and Ma, Wei-Ying and Xi, WenSi and Fox, Edward},
title = {MRSSA: An Iterative Algorithm for Similarity Spreading over Interrelated Objects},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031222},
doi = {10.1145/1031171.1031222},
abstract = {We introduce the Multiple Relationship Similarity Spreading Algorithm (MRSSA) to enhance IR effectiveness. This method has similarity computed in an iterative "spreading" fashion for multiple object types, combining both inter- and intra-object relationships. We demonstrate the value of this approach in the context of the WWW, where the key objects are web pages and queries, Relationships considered are derived from hyperlinks (in- and out-links) and click-through logs. },
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {240–241},
numpages = {2},
keywords = {interrelated, mutual reinforcement, similarity spreading},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031223,
author = {Wang, Xuanhui and Shen, Dou and Zeng, Hua-Jun and Chen, Zheng and Ma, Wei-Ying},
title = {Web Page Clustering Enhanced by Summarization},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031223},
doi = {10.1145/1031171.1031223},
abstract = {Traditional Web page clustering algorithms use the full-text in the documents to generate feature vectors. Such methods often produce unsatisfactory results because there is much noisy information, such as decoration, interaction, and advertisement, in Web pages. The varying-length problem of the Web pages is also a significant negative factor affecting the performance. In this paper, we investigate the use of several summarization techniques to tackle these issues when clustering Web pages. Compared with the full-text representation of the Web pages, our experimental results indicate that our proposed approach effectively solves the problems of noisy information and varying-length, and thus significantly boosts the clustering performance.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {242–243},
numpages = {2},
keywords = {summarization, luhn, latent semantic analysis, web page clustering},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031224,
author = {Srinivasan, Savitha and Amir, Arnon and Deshpande, Prasad and Zbarsky, Vladimir},
title = {Grammar-Based Task Analysis of Web Logs},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031224},
doi = {10.1145/1031171.1031224},
abstract = {The daily use of Internet-based services is involved with hundreds of different tasks being performed by multiple users. A single task is typically involved with a sequence of Web URLs invocation. We study the problem of pattern detection in Web logs to identify tasks performed by users, and analyze task trends over time using a grammar-based framework. Our results are demonstrated on a corporate Intranet portal application with 7000 users over a 6 week period and demonstrate compelling business value from this high-level task analysis.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {244–245},
numpages = {2},
keywords = {web logs analysis, regular grammar, pattern detection, data mining},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031225,
author = {Zhao, Ying and Karypis, George},
title = {Soft Clustering Criterion Functions for Partitional Document Clustering: A Summary of Results},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031225},
doi = {10.1145/1031171.1031225},
abstract = {Recently published studies have shown that partitional clustering algorithms that optimize certain criterion functions, which measure key aspects of inter- and intra-cluster similarity, are very effective in producing hard clustering solutions for document datasets and outperform traditional partitional and agglomerative algorithms. In this paper we study the extent to which these criterion functions can be modified to include soft membership functions and whether or not the resulting soft clustering algorithms can further improve the clustering solutions. Specifically, we focus on four of these hard criterion functions, derive their soft-clustering extensions, and present an experimental evaluation involving twelve different datasets. Our results show that introducing softness into the criterion functions tends to lead to better clustering results for most datasets.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {246–247},
numpages = {2},
keywords = {soft clustering, document clustering},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031226,
author = {Tomita, Junji and Nakawatase, Hidekazu and Ishii, Megumi},
title = {Calculating Similarity between Texts Using Graph-Based Text Representation Model},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031226},
doi = {10.1145/1031171.1031226},
abstract = {Knowledge discovery from a large volumes of texts usually requires many complex analysis steps. The graph-based text representation model has been proposed to simplify the steps. The model represents texts in a formal manner, Subject Graphs, and provides text handling operations whose inputs and outputs are identical in form, i.e. a set of subject graphs, so they can be combined in any order. A subject graph uses node weight to represent the significance of each term, and link weight to represent that of each term-term association. This paper concentrates on the algorithms for making subject graphs and calculating the similarity between them. An evaluation shows that Subject Graphs can calculate the similarity between texts more precisely than term vectors, since they incorporate the significance of association between terms.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {248–249},
numpages = {2},
keywords = {subject graphs, similarity calculation},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031228,
author = {Fradkin, Dmitriy and Kantor, Paul},
title = {A Design Space Approach to Analysis of Information Retrieval Adaptive Filtering Systems},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031228},
doi = {10.1145/1031171.1031228},
abstract = {In this paper we suggest a new approach to analysis and design of IR systems. We argue for design space exploration in constructing IR systems and in analyzing the effects of individual modules and parameters. We present results of experiments with parametric interpolation, or "homotopy", between two systems, and show, incidentally, that the best results are not achieved at the endpoints, and may lie outside the bounding hypercube defined by our choice of parameterization. Three distinct classes of interpolation are introduced to deal with the complexities of the specific example.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {251–260},
numpages = {10},
keywords = {rocchio, adaptive filtering, homotopy},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031229,
author = {Lynam, Thomas R. and Buckley, Chris and Clarke, Charles L. A. and Cormack, Gordon V.},
title = {A Multi-System Analysis of Document and Term Selection for Blind Feedback},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031229},
doi = {10.1145/1031171.1031229},
abstract = {Experiments were conducted to explore the impact of combining various components of eight leading information retrieval systems. Each system demonstrated improved effectiveness with the use of <i>blind feedback</i>, in which the results of a preliminary retrieval step were used to augment the efficacy of a secondary retrieval step. The hybrid combination of primary and secondary retrieval steps from different systems in a number of cases yielded better effectiveness than either of the constituent systems alone. This positive combining effect was observed when entire documents were passed between the two retrieval steps, but not when only the expansion terms were passed. Several combinations of primary and secondary retrieval steps were fused using the CombMNZ algorithm; all yielded significant effectiveness improvement over the individual systems, with the best yielding a an improvement of 13% (<i>p</i> = 10<sup>-6</sup>) over the best individual system and an improvement of 4% (<i>p</i> = 10<sup>-5</sup>) over a simple fusion of the eight systems.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {261–269},
numpages = {9},
keywords = {fusion, blind feedback, pseudo-relevance feedback},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031230,
author = {Bot, Razvan Stefan and Wu, Yi-fang Brook},
title = {Improving Document Representations Using Relevance Feedback: The RFA Algorithm},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031230},
doi = {10.1145/1031171.1031230},
abstract = {In this paper we present a document representation improvement technique, named the Relevance Feedback Accumulation (RFA) algorithm. Using prior relevance feedback assessments and a data mining measure called "support", the algorithm's learning function gradually improves document representations, over time and across users. Results show that the modified document representations yield lower dimensionality while improving retrieval effectiveness. The algorithm is efficient and scalable, suited for retrieval systems managing large document collections.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {270–278},
numpages = {9},
keywords = {information retrieval, relevance feedback, document oriented view, document representation},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031232,
author = {Ren, Dongmei and Rahal, Imad and Perrizo, William and Scott, Kirk},
title = {A Vertical Distance-Based Outlier Detection Method with Local Pruning},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031232},
doi = {10.1145/1031171.1031232},
abstract = {"One person's noise is another person's signal". Outlier detection is used to clean up datasets and also to discover useful anomalies, such as criminal activities in electronic commerce, computer intrusion attacks, terrorist threats, agricultural pest infestations, etc. Thus, outlier detection is critically important in the information-based society. This paper focuses on finding outliers in large datasets using distance-based methods. First, to speedup outlier detections, we revise Knorr and Ng's distance-based outlier definition; second, a vertical data structure, instead of traditional horizontal structures, is adopted to facilitate efficient outlier detection further. We tested our methods against national hockey league dataset and show an order of magnitude of speed improvement compared to the contemporary distance-based outlier detection approaches.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {279–284},
numpages = {6},
keywords = {neighborhood search, p-tree, distance-based, pruning, outlier detection},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031233,
author = {Chen, Keke and Liu, Ling},
title = {ClusterMap: Labeling Clusters in Large Datasets via Visualization},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031233},
doi = {10.1145/1031171.1031233},
abstract = {With the rapid increase of data in many areas, clustering on large datasets has become an important problem in data analysis. Since cluster analysis is a highly iterative process, cluster analysis on large datasets prefers short iteration on a relatively small representative set. Thus, a two-phase framework "sampling/summarization - iterative cluster analysis" is often applied in practice. Since the clustering result only labels the small representative set, there are problems with extending the result to the entire large dataset, which are almost ignored by the traditional clustering research. This extending is often named as labeling process. Labeling irregular shaped clusters, distinguishing outliers and extending cluster boundary are the main problems in this stage. We address these problems and propose a visualization-based approach to dealing with them precisely. This approach partially involves human into the process of defining and refining the structure "ClusterMap". Based on this structure, the ClusterMap algorithm scans the large dataset to adapt the boundary extension and generate the cluster labels for the entire dataset. Experimental result shows that ClusterMap can preserve cluster quality considerably with low computational cost, compared to the distance-comparison-based labeling algorithms.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {285–293},
numpages = {9},
keywords = {cluster labeling, data clustering, cluster visualization, human factors in clustering},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031234,
author = {Li, Tao and Ogihara, Mitsunori and Ma, Sheng},
title = {On Combining Multiple Clusterings},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031234},
doi = {10.1145/1031171.1031234},
abstract = {Many problems can be reduced to the problem of combining multiple clusterings. In this paper, we first summarize different application scenarios of combining multiple clusterings and provide a new perspective of viewing the problem as a categorical clustering problem. We then show the connections between various consensus and clustering criteria and discuss the complexity results of the problem. Finally we propose a new method to determine the final clustering. Experiments on kinship terms and clustering popular music from heterogeneous feature sets show the effectiveness of combining multiple clusterings.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {294–303},
numpages = {10},
keywords = {categorical, combining, multiple clusterings},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031236,
author = {Banaei-Kashani, Farnoush and Shahabi, Cyrus},
title = {SWAM: A Family of Access Methods for Similarity-Search in Peer-to-Peer Data Networks},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031236},
doi = {10.1145/1031171.1031236},
abstract = {Peer-to-peer Data Networks (PDNs) are large-scale, self-organizing, distributed query processing systems. Familiar examples of PDN are peer-to-peer file-sharing networks, which support exact-match search queries to locate user-requested files. In this paper, we formalize the more general problem of <i>similarity-search</i> in PDNs, and propose a <i>family</i> of distributed access methods, termed <i>Small-World Access Methods (SWAM)</i>, for efficient execution of various similarity-search queries, namely exact-match, range, and k-nearest-neighbor queries. Unlike its predecessors, i.e., LH* and DHTs, SWAM does not control the assignment of data objects to PDN nodes; each node autonomously stores its own data. Besides, SWAM supports all similarity-search queries on multiple attributes. SWAM guarantees that the query object will be found (if it exists in the network) in average time logarithmically proportional to the network size. Moreover, once the query object is found, all the similar objects would be in its proximate network neighborhood and hence enabling efficient range and k-nearest-neighbor queries.As a specific instance of SWAM, we propose <i>SWAM-V</i>, a Voronoi-based SWAM that indexes PDNs with multi-attribute data objects. For a PDN with <i>N</i> nodes SWAM-V has query time, communication cost, and computation cost of <i>O</i>(log <i>N</i>) for exact-match queries, and <i>O</i>(log <i>N</i> + <b>s</b><i>N</i>) and <i>O</i>(log <i>N</i> + <b>k</b>) for range queries (with selectivity <b>s</b>) and <b>k</b>NN queries, respectively. Our experiments show that SWAM-V consistently outperforms a similarity-search enabled version of CAN in query time and communication cost by a factor of 2 to 3.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {304–313},
numpages = {10},
keywords = {peer-to-peer networks, distributed hash table (DHT), similarity search, small-world},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@dataset{10.1145/review-1031171.1031236_R38570,
author = {Safar, Maytham Hassan},
title = {Review ID:R38570 for DOI: 10.1145/1031171.1031236},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1031171.1031236_R38570}
}

@inproceedings{10.1145/1031171.1031237,
author = {Jing, Qiang and Yang, Rui and Kalnis, Panos and Tung, Anthony K. H.},
title = {Localized Signature Table: Fast Similarity Search on Transaction Data},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031237},
doi = {10.1145/1031171.1031237},
abstract = {Recently, techniques for supporting efficient similarity search over huge transaction datasets have emerged as an important research area. Several indexing schemes have been proposed towards this direction. Typically, these schemes provide a tradeoff between searching efficiency and indexing overhead in terms of space.In this paper, we propose a novel indexing scheme for similarity search on transaction data. Based on well-studied clustering techniques, we develop a construction algorithm for the proposed index and a branch-and-bound searching strategy for answering similarity search. Unlike previous techniques, our indexing scheme exhibits high search efficiency and low space requirements by trading-off the pre-computation time. This behavior is ideal for applications with low update but high read volume <i>e.g.</i>, data warehousing, collaborative filtering, <i>etc.</i>). Moreover, our experimental results illustrate that our method is robust to the varying characteristics of the datasets.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {314–323},
numpages = {10},
keywords = {transaction data, similarity search, indexing, data mining},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031238,
author = {Wu, Yi and Chang, Edward Y.},
title = {Distance-Function Design and Fusion for Sequence Data},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031238},
doi = {10.1145/1031171.1031238},
abstract = {Sequence-data mining plays a key role in many scientific studies and real-world applications such as bioinformatics, data stream, and sensor networks, where sequence data are processed and their semantics interpreted. In this paper we address two relevant issues: sequence-data representation, and representation-to-semantics mapping. For representation, since the best one is dependent upon the application being used and even the type of query, we propose representing sequence data in multiple views. For each representation, we propose methods to construct a <i>valid kernel</i> as the distance function to measure <i>similarity</i> between sequences. For mapping, we then find the best combination of the individual distance functions, which measure similarity of different views, to depict the target semantics. We propose a <i>super-kernel function-fusion</i> scheme to achieve the optimal mapping. Through theoretical analysis and empirical studies on UCI and real world datasets, we show our approach of multi-view representation and fusion to be mathematically valid and very effective for practical purposes.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {324–333},
numpages = {10},
keywords = {super-kernel fusion, multiple-view representation, sequence-data representation, representation-to-semantics mapping, sequence-data mining},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031240,
author = {Liu, Ning and Zhang, Benyu and Yan, Jun and Yang, Qiang and Yan, Shuicheng and Chen, Zheng and Bai, Fengshan and Ma, Wei-Ying},
title = {Learning Similarity Measures in Non-Orthogonal Space},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031240},
doi = {10.1145/1031171.1031240},
abstract = {Many machine learning and data mining algorithms crucially rely on the similarity metrics. The Cosine similarity, which calculates the inner product of two normalized feature vectors, is one of the most commonly used similarity measures. However, in many practical tasks such as text categorization and document clustering, the Cosine similarity is calculated under the assumption that the input space is an orthogonal space which usually could not be satisfied due to <i>synonymy</i> and <i>polysemy</i>. Various algorithms such as Latent Semantic Indexing (LSI) were used to solve this problem by projecting the original data into an orthogonal space. However LSI also suffered from the high computational cost and data sparseness. These shortcomings led to increases in computation time and storage requirements for large scale realistic data. In this paper, we propose a novel and effective similarity metric in the non-orthogonal input space. The basic idea of our proposed metric is that the similarity of features should affect the similarity of objects, and vice versa. A novel iterative algorithm for computing non-orthogonal space similarity measures is then proposed. Experimental results on a synthetic data set, a real MSN search click-thru logs, and 20NG dataset show that our algorithm outperforms the traditional Cosine similarity and is superior to LSI.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {334–341},
numpages = {8},
keywords = {latent semantic indexing (LSI), similarity measures (SM), non-orthogonal space (NOS), vector space model (VSM)},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031241,
author = {Wang, Gang and Lochovsky, Frederick H.},
title = {Feature Selection with Conditional Mutual Information Maximin in Text Categorization},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031241},
doi = {10.1145/1031171.1031241},
abstract = {Feature selection is an important component of text categorization. This technique can both increase a classifier's computation speed, and reduce the overfitting problem. Several feature selection methods, such as information gain and mutual information, have been widely used. Although they greatly improve the classifier's performance, they have a common drawback, which is that they do not consider the mutual relationships among the features. In this situation, where one feature's predictive power is weakened by others, and where the selected features tend to bias towards major categories, such selection methods are not very effective. In this paper, we propose a novel feature selection method for text categorization called <i>conditional mutual information maximin</i> (CMIM). It can select a set of individually discriminating and weakly dependent features. The experimental results show that CMIM can perform much better than traditional feature selection methods.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {342–349},
numpages = {8},
keywords = {Na\"{\i}ve Bayes, conditional mutual information, SVMs, classification, feature selection, text categorization},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@dataset{10.1145/review-1031171.1031241_R38828,
author = {State, Luminita},
title = {Review ID:R38828 for DOI: 10.1145/1031171.1031241},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1031171.1031241_R38828}
}

@inproceedings{10.1145/1031171.1031242,
author = {Kang, Feng and Jin, Rong and Chai, Joyce Y.},
title = {Regularizing Translation Models for Better Automatic Image Annotation},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031242},
doi = {10.1145/1031171.1031242},
abstract = {The goal of automatic image annotation is to automatically generate annotations for images to describe their content. In the past, statistical machine translation models have been successfully applied to automatic image annotation task [8]. It views the process of annotating images as a process of translating the content from a 'visual language' to textual words. One problem with the existing translation models is that common words are usually associated with too many different image regions. As a result, uncommon words have little chance to be used for annotating images. Uncommon words are important for automatic image annotation because they are often used in the queries. In this paper, we propose two modified translation models for automatic image annotation, namely the normalized translation model and the regularized translation model, that specifically address the problem of common annotated words. The basic idea is to raise the number of blobs that are associated with uncommon words. The normalized translation model realizes this by scaling translation probabilities of different words with different factors. The same goal is achieved in the regularized translation model through the introduction of a special Dirichlet prior. Empirical study with the Corel dataset has shown that both two modified translation models outperform the original translation model and several existing approaches for automatic image annotation substantially.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {350–359},
numpages = {10},
keywords = {automatic image annotation, regularized translation model, normalized translation model, translation model},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031244,
author = {Andrus, D. Calvin and Bernholz, David and Scoggins, James and Thomsen, Erik},
title = {Key Problems in Integrating Structured and Unstructured Information},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031244},
doi = {10.1145/1031171.1031244},
abstract = {This forum will host a discussion among the panelists - and with the audience - on key trends in information technology, as well as on industry and government problems in search of technology solutions. The panel will focus on the core problems inherent in integrating structured and unstructured textual information, moving from application-centric to user-centric software, and merging knowledge management with data management.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {360},
numpages = {1},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031246,
author = {Piwowarski, Benjamin and Lalmas, Mounia},
title = {Providing Consistent and Exhaustive Relevance Assessments for XML Retrieval Evaluation},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031246},
doi = {10.1145/1031171.1031246},
abstract = {Comparing retrieval approaches requires test collections, which consist of documents, queries and relevance assessments. Obtaining consistent and exhaustive relevance assessments is crucial for the appropriate comparison of retrieval approaches. Whereas the evaluation methodology for flat text retrieval approaches is well established, the evaluation of XML retrieval approaches is a research issue. This is because XML documents are composed of nested components that cannot be considered independent in terms of relevance. This paper describes the methodology adopted in INEX (the INitiative for the Evaluation of XML Retrieval) to ensure consistent and exhaustive relevance assessments.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {361–370},
numpages = {10},
keywords = {XML, evaluation, relevance assessment process, INEX},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031247,
author = {Sigurbj\"{o}rnsson, B\"{o}rkur and Kamps, Jaap and de Rijke, Maarten},
title = {Processing Content-Oriented XPath Queries},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031247},
doi = {10.1145/1031171.1031247},
abstract = {Document-centric XML collections contain text-rich documents, marked up with XML tags that add lightweight semantics to the text. Querying such collections calls for a hybrid query language: the text-rich nature of the documents suggests a content-oriented (IR) approach, while the mark-up allows users to add structural constraints to their IR queries. Hybrid queries tend to be more expressive, which should lead---in principle---to better retrieval performance. In practice, the processing of these hybrid queries within an IR systems turns out to be far from trivial, because a delicate balance between structural and content information needs to be sought. We propose an approach to processing such hybrid content-and-structure queries that decomposes a query into multiple content-only queries whose results are then combined in ways determined by the structural constraints of the original query. We evaluate our methods using the INEX 2003 test-suite, and show (1) that effective ways of processing of content-oriented XPath queries are non-trivial, (2) that there are differences in the effectiveness for different topics types, but (3) that with appropriate processing methods retrieval effectiveness can improve.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {371–380},
numpages = {10},
keywords = {XML retrieval, XPath, content and structure},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031248,
author = {Chen, Yen-Yu and Gan, Qingqing and Suel, Torsten},
title = {Local Methods for Estimating Pagerank Values},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031248},
doi = {10.1145/1031171.1031248},
abstract = {The Google search engine uses a method called PageRank, together with term-based and other ranking techniques, to order search results returned to the user. PageRank uses link analysis to assign a global importance score to each web page. The PageRank scores of all the pages are usually determined off-line in a large-scale computation on the entire hyperlink graph of the web, and several recent studies have focused on improving the efficiency of this computation, which may require multiple hours on a workstation.However, in some scenarios, such as online analysis of link evolution and mining of large web archives such as the Internet Archive, it may be desirable to quickly approximate or update the PageRanks of individual nodes without performing a large-scale computation on the entire graph. We address this problem by studying several methods for efficiently estimating the PageRank score of a particular web page using only a small subgraph of the entire web. In our model, we assume that the graph is accessible remotely via a link database (such as the AltaVista Connectivity Server) or is stored in a relational database that performs lookups on disks to retrieve node and connectivity information. We show that a reasonable estimate of the PageRank value of a node is possible in most cases by retrieving only a moderate number of nodes in the local neighborhood of the node.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {381–389},
numpages = {9},
keywords = {out-of-core, search engines, external memory algorithms, pagerank, link-based ranking, link database},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031250,
author = {Efron, Miles},
title = {The Liberal Media and Right-Wing Conspiracies: Using Cocitation Information to Estimate Political Orientation in Web Documents},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031250},
doi = {10.1145/1031171.1031250},
abstract = {This paper introduces a simple method for estimating <i>cultural orientation</i>, the affiliation of online entities in a polarized field of discourse. In particular, cocitation information is used to estimate the political orientation of hypertext documents. A type of cultural orientation, the political orientation of a document is the degree to which it participates in traditionally left- or right-wing beliefs. Estimating documents' political orientation is of interest for personalized information retrieval and recommender systems. In its application to politics, the method uses a simple probabilistic model to estimate the strength of association between a document and left- and right-wing communities. The model estimates the likelihood of cocitation between a document of interest and a small number of documents of known orientation. The model is tested on three sets of data, 695 partisan web documents, 162 political weblogs, and 72 non-partisan documents. Accuracy above 90% is obtained from the cocitation model, outperforming lexically based classifiers at statistically significant levels.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {390–398},
numpages = {9},
keywords = {personalization, PMI-IR, politics, cocitation, cultural orientation, opinion mining},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031251,
author = {Takaki, Toru and Fujii, Atsushi and Ishikawa, Tetsuya},
title = {Associative Document Retrieval by Query Subtopic Analysis and Its Application to Invalidity Patent Search},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031251},
doi = {10.1145/1031171.1031251},
abstract = {We propose an associative document retrieval method, in which a document is used as a query to search for other similar documents. Because a long document usually includes more than one topic, we first analyze a query document to extract multiple subtopics. For each subtopic element, a sub-query is produced and similar documents are retrieved with a relevance score. The relevance scores are weighted by the importance of each subtopic element and are integrated to determine the final relevant documents. In the calculation of the subtopic importance, the specificity of a query term is evaluated using entropy, which is the deviation degree of the appearances of the term in each subtopic element. We apply this method to an invalidity patent search. By exploiting certain unique features of Japanese patent claims, we use features distinguishing the preamble and the essential portion in a query patent claim. To demonstrate the effectiveness of our method, we experimentally evaluated our associative document retrieval method on five years of patent documents.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {399–405},
numpages = {7},
keywords = {invalidity patent search, associative document retrieval, query subtopic analysis, subtopics},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031252,
author = {Ziegler, Cai-Nicolas and Lausen, Georg and Schmidt-Thieme, Lars},
title = {Taxonomy-Driven Computation of Product Recommendations},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031252},
doi = {10.1145/1031171.1031252},
abstract = {Recommender systems have been subject to an enormous rise in popularity and research interest over the last ten years. At the same time, very large taxonomies for product classification are becoming increasingly prominent among e-commerce systems for diverse domains, rendering detailed machine-readable content descriptions feasible. Amazon.com makes use of an entire plethora of hand-crafted taxonomies classifying books, movies, apparel, and various other goods. We exploit such taxonomic background knowledge for the computation of personalized recommendations. Hereby, relationships between super-concepts and sub-concepts constitute an important cornerstone of our novel approach, providing powerful inference opportunities for profile generation based upon the classification of products that customers have chosen. Ample empirical analysis, both offline and online, demonstrates our proposal's superiority over common existing approaches when user information is sparse and implicit ratings prevail.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {406–415},
numpages = {10},
keywords = {recommender systems, taxonomies, machine learning},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031254,
author = {Chomicki, Jan and Marcinkowski, Jerzy and Staworko, Slawomir},
title = {Computing Consistent Query Answers Using Conflict Hypergraphs},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031254},
doi = {10.1145/1031171.1031254},
abstract = {A consistent query answer in a possibly inconsistent database is an answer which is true in every (minimal) repair of the database. We present here a practical framework for computing consistent query answers for large, possibly inconsistent relational databases. We consider relational algebra queries without projection, and denial constraints. Because our framework handles union queries, we can effectively (and efficiently) extract indefinite disjunctive information from an inconsistent database. We describe a number of novel optimization techniques applicable in this context and summarize experimental results that validate our approach.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {417–426},
numpages = {10},
keywords = {integrity constraints, query processing, inconsistency},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031255,
author = {Gedik, Bugra and Wu, Kun-Lung and Yu, Philip and Liu, Ling},
title = {Motion Adaptive Indexing for Moving Continual Queries over Moving Objects},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031255},
doi = {10.1145/1031171.1031255},
abstract = {This paper describes a <i>motion adaptive</i> indexing scheme for efficient evaluation of moving continual queries (MCQs) over moving objects. It uses the concept of <i>motion-sensitive bounding boxes</i> (<i>MSB</i>s) to model moving objects and moving queries. These bounding boxes automatically adapt their sizes to the dynamic motion behaviors of individual objects. Instead of indexing frequently changing object positions, we index less frequently changing object and query <i>MSB</i>s, where updates to the bounding boxes are needed only when objects and queries move across the boundaries of their boxes. This helps decrease the number of updates to the indexes. More importantly, we use <i>predictive query results</i> to optimistically precalculate query results, decreasing the number of searches on the indexes. Motion-sensitive bounding boxes are used to incrementally update the predictive query results. Our experiments show that the proposed motion adaptive indexing scheme is efficient for the evaluation of moving continual range queries.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {427–436},
numpages = {10},
keywords = {continual queries, moving object databases},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031256,
author = {Chundi, Parvathi and Rosenkrantz, Daniel J.},
title = {On Lossy Time Decompositions of Time Stamped Documents},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031256},
doi = {10.1145/1031171.1031256},
abstract = {Constructing time decompositions of time stamped documents is an important first step in extracting temporal information from a document set. Efficient algorithms are described for computing optimal lossy decompositions for a given document set, where the loss of information is constrained to be within a specified bound. A novel and efficient algorithm is proposed for computing information loss values required to construct optimal lossy decompositions. Experimental results are reported comparing optimal lossy decompositions and equal length decompositions in terms of a number of parameters such as information loss. In particular, our results show that optimal lossy decompositions outperform equal length decompositions by preserving more of the information content of the underlying document set. The results also demonstrate that permitting even small amounts of variability in the length of the subintervals of a decomposition results in capturing more of the temporal information content of a document set when compared to equal length decompositions. This paper builds upon our earlier work on time decompositions where the problem of computing optimal lossy decomposition of the time period associated with a document set was first formulated.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {437–445},
numpages = {9},
keywords = {time decompositions, lossy decompositions, information loss},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031258,
author = {Nallapati, Ramesh and Feng, Ao and Peng, Fuchun and Allan, James},
title = {Event Threading within News Topics},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031258},
doi = {10.1145/1031171.1031258},
abstract = {With the overwhelming volume of online news available today, there is an increasing need for automatic techniques to analyze and present news to the user in a meaningful and efficient manner. Previous research focused only on organizing news stories by their topics into a flat hierarchy. We believe viewing a news topic as a flat collection of stories is too restrictive and inefficient for a user to understand the topic quickly. In this work, we attempt to capture the rich structure of events and their dependencies in a news topic through our event models. We call the process of recognizing events and their dependencies <i>event threading</i>. We believe our perspective of modeling the structure of a topic is more effective in capturing its semantics than a flat list of on-topic stories.We formally define the novel problem, suggest evaluation metrics and present a few techniques for solving the problem. Besides the standard word based features, our approaches take into account novel features such as temporal locality of stories for event recognition and time-ordering for capturing dependencies. Our experiments on a manually labeled data sets show that our models effectively identify the events and capture dependencies among them.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {446–453},
numpages = {8},
keywords = {threading, event, clustering, dependency},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031259,
author = {Clarke, Charles L. A. and Terra, Egidio L.},
title = {Approximating the Top-m Passages in a Parallel Question Answering System},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031259},
doi = {10.1145/1031171.1031259},
abstract = {We examine the problem of retrieving the top-<i>m</i> ranked items from a large collection, randomly distributed across an <i>n</i>-node system. In order to retrieve the top <i>m</i> overall, we must retrieve the top <i>m</i> from the subcollection stored on each node and merge the results. However, if we are willing to accept a small probability that one or more of the top-<i>m</i> items may be missed, it is possible to reduce computation time by retrieving only the top <i>k &lt; m</i> from each node. In this paper, we demonstrate that this simple observation can be exploited in a realistic application to produce a substantial efficiency improvement without compromising the quality of the retrieved results. To support our claim, we present a statistical model that predicts the impact of the optimization. The paper is structured around a specific application~---~passage retrieval for question answering~---~but the primary results are more broadly applicable.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {454–462},
numpages = {9},
keywords = {ranking queries, parallel information retrieval, question answering},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031260,
author = {Maguitman, Ana and Leake, David and Reichherzer, Thomas and Menczer, Filippo},
title = {Dynamic Extraction Topic Descriptors and Discriminators: Towards Automatic Context-Based Topic Search},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031260},
doi = {10.1145/1031171.1031260},
abstract = {Effective knowledge management may require going beyond initial knowledge capture, to support decisions about how to extend previously-captured knowledge. Electronic <i>concept maps,</i> interlinked with other concept maps and multimedia resources, can provide rich <i>knowledge models</i> for human knowledge capture and sharing. This paper presents research on methods for supporting experts as they extend these knowledge models, by searching the Web for new context-relevant topics as candidates for inclusion. This topic search problem presents two challenges: First, how to formulate queries to seek topics that reflect the context of the current knowledge model, and, second, how to identify candidate topics with the right balance of novelty and relevance. More generally, this problem raises the broad question of the interaction of topic information from the local analysis space (a collected set of documents) and the global search space (the Web). The paper develops a framework for understanding this interaction, and proposes and evaluates techniques for addressing the query formation and topic identification questions by dynamically extracting topic descriptors and discriminators from a knowledge model, to characterize information needs for retrieval and filtering of relevant material. Using these techniques, we have developed a support tool that starts from a knowledge model under construction and automatically produces a set of suggestions for topics to include, proactively supporting users as they extend knowledge models.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {463–472},
numpages = {10},
keywords = {acquisition tools, knowledge, context, knowledge management, automatic topic search, information retrieval, concept mapping},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031262,
author = {Singhal, Anoop},
title = {Design of a Data Warehouse System for Network/Web Services},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031262},
doi = {10.1145/1031171.1031262},
abstract = {This paper describes the architecture and design of a data warehouse for AT&amp;T Business Services. The main purpose of our system is to generate reports about the performance and reliability of the network. We describe the architecture of our system and discuss some open research problems in this area.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {473–476},
numpages = {4},
keywords = {data warehouse, data mining, computer networks},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031263,
author = {Zhang, Li and Liu, ShiXia and Pan, Yue and Yang, LiPing},
title = {InfoAnalyzer: A Computer-Aided Tool for Building Enterprise Taxonomies},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031263},
doi = {10.1145/1031171.1031263},
abstract = {In this paper we study the problem of collecting training samples for building enterprise taxonomies. We develop a computer-aided tool named InfoAnalyzer, which can effectively assist the enterprise to prepare large set of samples used for machine learning in text categorization. In our system, the enterprise category tree is initially defined by some keywords, then the Google search engine is used to construct a small set of labeled documents, and topic tracking algorithm based on document length normalization is applied to enlarge the training corpus on the bases of the seed stories. Furthermore, we design a method to check the consistency of the training corpus. Experiments show that the training corpus is good enough for statistical classification methods and meets human's requirements as well.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {477–483},
numpages = {7},
keywords = {shannon entropy measure, document length normalization, topic tracking, relevance feedback, support vector machine},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031264,
author = {Ma, Li and Su, Zhong and Pan, Yue and Zhang, Li and Liu, Tao},
title = {RStar: An RDF Storage and Query System for Enterprise Resource Management},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031264},
doi = {10.1145/1031171.1031264},
abstract = {Modern corporations operate in an extremely complex environment and strongly depend on all kinds of information resources across the enterprise. Unfortunately, with the growth of an enterprise, its information resources are not only heterogeneous but also distributed in physically different systems and databases. How to effectively exploit information across the enterprise is becoming a critical but hard problem. In recent years, metadata which is the detailed description of the data is used to efficiently exploit information resources in the web. The World Wide Web Consortium (W3C) recommends the resource description framework (RDF) as a standard for the definition and use of metadata descriptions of resources in the web. In this paper, we present an RDF storage and query system called RStar for enterprise resource management. RStar uses a relational database as the persistent data store and defines RStar Query Language (RSQL) for resource retrieval. Currently, most of existing RDF storage and query systems are evaluated on small data sets and no detailed performance analysis is given for such systems. Therefore, we conduct extensive experiments on a large scale data set to investigate the performance problem in RDF storage. Such analysis will be helpful for designing RDF storage and query systems as well as for understanding not well-solved issues in RDF based enterprise resource management. In addition, experiences and lessons learned in our implementation are presented for further research and development.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {484–491},
numpages = {8},
keywords = {RDF query language, RDF storage, metadata, resource management, ontology},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031265,
author = {Knabe, Frederick and Tunkelang, Daniel},
title = {Processing Search Queries in a Distributed Environment},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031265},
doi = {10.1145/1031171.1031265},
abstract = {Endeca's approach to processing search queries in a distributed computing environment is predicated on concerns of correctness, scalability, and flexibility in deployment. Using a master-slave architecture, we are able to support classic search as well as more advanced features. We avoid bottlenecking the master with excessive computation or communication by limiting the information from the slaves in the expected case.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {492–494},
numpages = {3},
keywords = {information retrieval, distributed systems, search},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031266,
author = {Clark, Alan and Filev, Dimitar},
title = {Intelligent Agent for Automated Manufacturing Rule Generation},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031266},
doi = {10.1145/1031171.1031266},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {495–500},
numpages = {6},
keywords = {latent semantic indexing, intelligent agent, clustering algorithms, knowledge management, knowledge extraction, KBE},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031267,
author = {Niu, Zheng-Yu and Ji, Dong-Hong and Tan, Chew-Lim},
title = {Document Clustering Based on Cluster Validation},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031267},
doi = {10.1145/1031171.1031267},
abstract = {This paper presents a cluster validation based document clustering algorithm, which is capable of identifying both important feature words and true model order (cluster number). Important feature subset is selected by optimizing a cluster validity criterion subject to some constraint. For achieving model order identification capability, this feature selection procedure is conducted for each possible value of cluster number. The feature subset and cluster number which maximize the cluster validity criterion are chosen as our answer. We have applied our algorithm to several datasets from 20Newsgroup corpus. Experimental results show that our algorithm can find important feature subset, estimate the model order and yield higher micro-averaged precision than other four document clustering algorithms which require cluster number to be provided.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {501–506},
numpages = {6},
keywords = {feature selection, cluster number estimation, cluster validation, document clustering},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031268,
author = {Sano, Makoto and Evans, David A.},
title = {Circumstance-Based Categorization Analysis of Knowledge Management Systems for the Japanese Market},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031268},
doi = {10.1145/1031171.1031268},
abstract = {We conducted a survey of thirty of the approximately 1,700 customers of Justsystem Corporation's knowledge-management applications. Our goal was to discover the kinds of functions that customers hoped to address in their next-generation use of knowledge management technology and to assess the core processes that we will need to deploy in our products to address their desired solutions. In particular, we sought to analyze our customers' requirements along dimensions that take account of both the context of use of the application and its stage in the cycle of knowledge creation and use. As part of our analysis, we were able to classify all customer cases as focused by one or more of three Goals, supported by one or more of eleven technology Means. To establish appropriate categories of use, we exploited the stages of the <i>SECI Model</i>, several other transactional categories of knowledge use, and whether activities were targeted at internal or external users. Through the analysis, we found the typical technology components (Means) for each stage of knowledge creation and use associated with each set of goals. We consider such analysis essential to the task of designing next-generation knowledge-management applications and critical to overcoming the unfortunate tendency of developers to devise solutions that bear little relation to the true needs of users.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {507–514},
numpages = {8},
keywords = {knowledge management technology, decision support, natural language processing, ontology, information retrieval, knowledge management, SECI model, affect analysis},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031269,
author = {Duckstein, Ralf and B\"{o}hm, Klemens},
title = {Database Support for Species Extraction from the Biosystematics Literature: A Feasibility Demonstration},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031269},
doi = {10.1145/1031171.1031269},
abstract = {A part of the biosystematics literature is currently being digitized and manually marked up with XML. Fast search on such documents shall be feasible. But marking up such documents incurs high costs, and biologists would like to know the value of such an activity in advance. Deploying standard XML database technology in a straightforward way is not feasible, because of two characteristics of biosystematics documents. The first one is that descriptions of taxa are related, i.e., a more specific taxon should inherit from a more general one. The combination of inheritance with information-retrieval mechanisms gives rise to difficulties addressed in this article. The second issue is the frequent occurrence of very specific technical terms in such documents, i.e., geographical information or biological terms. To investigate the characteristics of the search in the presence of such difficulties, we have designed and implemented a respective system, based on relational database technology. We use a collection of XML documents that mimics the characteristics of biosystematics documents, as we will explain. We propose two query-evaluation alternatives and compare them by means of performance experiments. It turns out that our techniques can administer the envisioned corpus of documents efficiently and cope with those problems at the same time.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {515–522},
numpages = {8},
keywords = {biosystematics},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031271,
author = {Yang, Beverly and Fontoura, Marcus and Shekita, Eugene and Rajagopalan, Sridhar and Beyer, Kevin},
title = {Virtual Cursors for XML Joins},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031271},
doi = {10.1145/1031171.1031271},
abstract = {Structural joins are a fundamental operation in XML query processing and a large body of work has focused on index-based algorithms for executing them. In this paper, we describe how two well-known index features -- path indices and ancestor information -- can be combined in a novel way to replace one or more of the physical index cursors in a structural join with <i>virtual cursors</i>. The position of a virtual cursor is derived from the path and ancestor information of a physical cursor. Implementation results are provided to show that, by eliminating index I/O, virtual cursors can improve the performance of structural joins by an order of magnitude or more.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {523–532},
numpages = {10},
keywords = {evaluation, indexing, join operator, XML},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031272,
author = {Lu, Jiaheng and Chen, Ting and Ling, Tok Wang},
title = {Efficient Processing of XML Twig Patterns with Parent Child Edges: A Look-Ahead Approach},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031272},
doi = {10.1145/1031171.1031272},
abstract = {With the growing importance of semi-structure data in information exchange, much research has been done to provide an effective mechanism to match a twig query in an XML database. A number of algorithms have been proposed recently to process a twig query holistically. Those algorithms are quite efficient for quires with only ancestor-descendant edges. But for queries with mixed ancestor-descendant and parent-child edges, the previous approaches still may produce large intermediate results, even when the input and output size are more manageable. To overcome this limitation, in this paper, we propose a novel holistic twig join algorithm, namely <i>TwigStackList</i>. Our main technique is to look-ahead read some elements in input data steams and cache limited number of them to <i>lists</i> in the main memory. The number of elements in any list is bounded by the length of the longest path in the XML document. We show that <i>TwigStackList</i> is I/O optimal for queries with only ancestor-descendant relationships below branching nodes. Further, even when queries contain parent-child relationship below branching nodes, the set of intermediate results in <i>TwigStackList</i> is guaranteed to be a subset of that in previous algorithms. We complement our experimental results on a range of real and synthetic data to show the significant superiority of <i>TwigStackList</i> over previous algorithms for queries with <i>parent</i>-<i>child</i> relationships.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {533–542},
numpages = {10},
keywords = {holistic twig pattern matching, XML},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031273,
author = {Luo, Bo and Lee, Dongwon and Lee, Wang-Chien and Liu, Peng},
title = {QFilter: Fine-Grained Run-Time XML Access Control via NFA-Based Query Rewriting},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031273},
doi = {10.1145/1031171.1031273},
abstract = {At present, most of the state-of-the-art solutions for XML access controls are either (1) document-level access control techniques that are too limited to support fine-grained security enforcement; (2) view-based approaches that are often expensive to create and maintain; or (3) impractical proposals that require substantial security-related support from underlying XML databases. In this paper, we take a different approach that assumes no security support from underlying XML databases and examine three alternative fine-grained XML access control solutions, namely <i>primitive, pre-processing</i> and <i>post-processing</i> approaches. In particular, we advocate a pre-processing method called <i>QFilter</i> that uses Non-deterministic Finite Automata (NFA) to rewrite user's query such that any parts violating access control rules are pruned. We show the construction and execution of a QFilter and demonstrate its superiority to other competing methods.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {543–552},
numpages = {10},
keywords = {XML security, data security and privacy, query rewriting},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031275,
author = {Petinot, Yves and Giles, C. Lee and Bhatnagar, Vivek and Teregowda, Pradeep B. and Han, Hui and Councill, Isaac},
title = {CiteSeer-API: Towards Seamless Resource Location and Interlinking for Digital Libraries},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031275},
doi = {10.1145/1031171.1031275},
abstract = {We introduce CiteSeer-API, a public API to CiteSeer-like services. CiteSeer-API is SOAP/WSDL based and allows for easy programmatical access to all the specific functionalities offered by CiteSeer services, including full text search of documents and citations and citation-based document discovery. In order to enable operability and interlinking with arbitrary software agents and digital library systems, CiteSeer-API uses digital content signatures to create system-independent handles for the Document, Citation and Group resources of CiteSeer servers. We discuss specific functionalities of CiteSeer-API that take advantage of these handlers in order to enable seamless location of CiteSeer resources. Finally we argue that the digital signature scheme used by CiteSeer-API is well suited for the creation of machine-usable semantic descriptions of digital library services which is the key toward seamless discovery and integration of services such as CiteSeer-API. CiteSeer-API is currently showcased on CiteSeer.IST, the CiteSeer server of the School of Information Science and Technology at the Pennsylvania State University.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {553–561},
numpages = {9},
keywords = {interoperability, digital libraries, interlink, citeseer, services, interfaces, semantic web, citeceer-API},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031276,
author = {Renda, M. Elena and Callan, Jamie},
title = {The Robustness of Content-Based Search in Hierarchical Peer to Peer Networks},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031276},
doi = {10.1145/1031171.1031276},
abstract = {Hierarchical <i>peer to peer</i> networks with multiple directory services are an important architecture for large-scale file sharing due to their effectiveness and efficiency. Recent research argues that they are also an effective method of providing large-scale content-based federated search of text-based digital libraries. In both cases the directory services are critical resources that are subject to attack or failure, but the latter architecture may be particularly vulnerable because content is less likely to be replicated throughout the network.This paper studies the robustness, effectiveness and efficiency of content-based federated search in hierarchical <i>peer to peer</i> networks when directory services fail unexpectedly. Several recovery methods are studied using simulations with varying failure rates. Experimental results show that quality of service and efficiency degrade gracefully as the number of directory service failures increases. Furthermore, they show that content-based search mechanisms are more resilient to failures than the match-based search techniques.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {562–570},
numpages = {9},
keywords = {hierarchical, peer to peer, robustness, retrieval, search, content-based},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031277,
author = {Jung, Seikyung and Harris, Kevin and Webster, Janet and Herlocker, Jonathan L.},
title = {SERF: Integrating Human Recommendations with Search},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031277},
doi = {10.1145/1031171.1031277},
abstract = {Today's university library has many digitally accessible resources, both indexes to content and considerable original content. Using off-the-shelf search technology provides a single point of access into library resources, but we have found that such full-text indexing technology is not entirely satisfactory for library searching. In response to this, we report initial usage results from a prototype of an entirely new type of search engine - The System for Electronic Recommendation Filtering (SERF) - that we have designed and deployed for the Oregon State University (OSU) Libraries. SERF encourages users to enter longer and more informative queries, and collects ratings from users as to whether search results meet their information need or not. These ratings are used to make recommendations to later users with similar needs. Over time, SERF learns from the users what documents are valuable for what information needs.In this paper, we focus on understanding whether such recommendations can increase other users' search efficiency and effectiveness in library website searching. Based on examination of three months of usage as an alternative search interface available to all users of the Oregon State University Libraries website (http://osulibrary.oregonstate.edu/), we found strong evidence that the recommendations with human evaluation could increase the efficiency as well as effectiveness of the library website search process. Those users who received recommendations needed to examine fewer results, and recommended documents were rated much higher than documents returned by a traditional search engine.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {571–580},
numpages = {10},
keywords = {digital libraries, information retrieval, collaborative filtering, user studies, web search},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031279,
author = {Zhang, Zhu},
title = {Weakly-Supervised Relation Classification for Information Extraction},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031279},
doi = {10.1145/1031171.1031279},
abstract = {This paper approaches the relation classification problem in information extraction framework with bootstrapping on top of Support Vector Machines. A new bootstrapping algorithm is proposed and empirically evaluated on the ACE corpus. We show that the supervised SVM classifier using various lexical and syntactic features can achieve promising classification accuracy. More importantly, the proposed <i>BootProject</i> algorithm based on random feature projection can significantly reduce the need for labeled training data with only limited sacrifice of performance.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {581–588},
numpages = {8},
keywords = {SVM, classification, relation, information extraction, bootstrapping, co-training},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031280,
author = {Rosenfeld, Benjamin and Feldman, Ronen and Fresko, Moshe and Schler, Jonathan and Aumann, Yonatan},
title = {TEG: A Hybrid Approach to Information Extraction},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031280},
doi = {10.1145/1031171.1031280},
abstract = {This paper describes a hybrid statistical and knowledge-based information extraction model, able to extract entities and relations at the sentence level. The model attempts to retain and improve the high accuracy levels of knowledge-based systems while drastically reducing the amount of manual labor by relying on statistics drawn from a training corpus. The implementation of the model, called TEG (Trainable Extraction Grammar), can be adapted to any IE domain by writing a suitable set of rules in a SCFG (Stochastic Context Free Grammar) based extraction language, and training them using an annotated corpus. The system does not contain any purely linguistic components, such as PoS tagger or parser. We demonstrate the performance of the system on several named entity extraction and relation extraction tasks. The experiments show that our hybrid approach outperforms both purely statistical and purely knowledge-based systems, while requiring orders of magnitude less manual rule writing and smaller amount of training data. The improvement in accuracy is slight for named entity extraction task and more pronounced for relation extraction.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {589–596},
numpages = {8},
keywords = {rules based system, information extraction, text mining, HMM},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031281,
author = {Chitrapura, Krishna P. and Kashyap, Srinivas R.},
title = {Node Ranking in Labeled Directed Graphs},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031281},
doi = {10.1145/1031171.1031281},
abstract = {Our work is motivated by the problem of ranking hyper-linked documents for a given query. Given an arbitrary directed graph with edge and node labels, we present a new flow-based model and an efficient method to dynamically rank the nodes of this graph with respect to any of the original labels. Ranking documents for a given query in a hyper-linked document set and ranking of authors/articles for a given topic in a citation database are some typical applications of our method. We outline the structural conditions that the graph must satisfy for our ranking to be different from the traditional <i>PageRank</i>.We have built a system using two indices that is capable of dynamically ranking documents for any given query. We validate our system and method using experiments on a few datasets: a crawl of the IBM Intranet (12 million pages), a crawl of the <b>www</b> (30 million pages) and the DBLP citation dataset. We compare our method to existing schemes for topic-biased ranking that require a classifier and the traditional <i>PageRank</i>. In these experiments, we demonstrate that our method is well suited for fine-grained ranking and that our method performs better than the existing schemes. We also demonstrate that our system can obtain an improved ranking with very little impact on query time.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {597–606},
numpages = {10},
keywords = {random surfer model, pagerank, search, model, link structure, intranet search, flow-based, context-sensitive ranking, citation graph, web graph, search in context},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031283,
author = {Lita, Lucian Vlad and Carbonell, Jaime},
title = {Unsupervised Question Answering Data Acquisition from Local Corpora},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031283},
doi = {10.1145/1031171.1031283},
abstract = {Data-driven approaches in question answering (QA) are increasingly common. Since availability of training data for such approaches is very limited, we propose an unsupervised algorithm that generates high quality question-answer pairs from local corpora. The algorithm is ontology independent, requiring very small seed data as its starting point. Two alternating views of the data make learning possible: 1) question types are viewed as relations between entities and 2) question types are described by their corresponding question-answer pairs. These two aspects of the data allow us to construct an unsupervised algorithm that acquires high precision question-answer pairs. We show the quality of the acquired data for different question types and perform a task-based evaluation. With each iteration, pairs acquired by the unsupervised algorithm are used as training data to a simple QA system. Performance increases with the number of question-answer pairs acquired confirming the robustness of the unsupervised algorithm. We introduce the notion of <i>semantic drift</i> and show that it is a desirable quality in training data for question answering systems.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {607–614},
numpages = {8},
keywords = {data acquisition, question answering, semantic drift, unsupervised learning},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031284,
author = {Lavelli, Alberto and Sebastiani, Fabrizio and Zanoli, Roberto},
title = {Distributional Term Representations: An Experimental Comparison},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031284},
doi = {10.1145/1031171.1031284},
abstract = {A number of content management tasks, including term categorization, term clustering, and automated thesaurus generation, view natural language <i>terms</i> (e.g. words, noun phrases) as first-class objects, i.e. as objects endowed with an internal representation which makes them suitable for explicit manipulation by the corresponding algorithms. The information retrieval (IR) literature has traditionally used an extensional (aka <i>distributional</i>) representation for terms according to which a term is represented by the "bag of documents" in which the term occurs. The computational linguistics (CL) literature has independently developed an alternative distributional representation for terms, according to which a term is represented by the "bag of terms" that co-occur with it in some document. This paper aims at discovering which of the two representations is most effective, i.e. brings about higher effectiveness once used in tasks that require terms to be explicitly represented and manipulated. We carry out experiments on (i) a term categorization task, and (ii) a term clustering task; this allows us to compare the two different representations in closely controlled experimental conditions. We report the results of experiments in which we categorize/cluster under 42 different classes the terms extracted from a corpus of more than 65,000 documents. Our results show a substantial difference in effectiveness between the two representation styles; we give both an intuitive explanation and an information-theoretic justification for these different behaviours.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {615–624},
numpages = {10},
keywords = {term clustering, term classification, term representation},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031285,
author = {Korenius, Tuomo and Laurikkala, Jorma and J\"{a}rvelin, Kalervo and Juhola, Martti},
title = {Stemming and Lemmatization in the Clustering of Finnish Text Documents},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031285},
doi = {10.1145/1031171.1031285},
abstract = {Stemming and lemmatization were compared in the clustering of Finnish text documents. Since Finnish is a highly inflectional and agglutinative language, we hypothesized that lemmatization, involving splitting of the compound words, would be more appropriate normalization approach than the straightforward stemming. The relevance of the documents were evaluated with a four-point relevance assessment scale, which was collapsed into binary one by considering all the relevant and only the highly relevant documents relevant, respectively. Experiments with four hierarchical clustering methods supported the hypothesis. The stringent relevance scale showed that lemmatization allowed the single and complete linkage methods to recover especially the highly relevant documents better than stemming. In comparison with stemming, lemmatization together with the average linkage and Ward's methods produced higher precision. We conclude that lemmatization is a better word normalization method than stemming, when Finnish text documents are clustered for information retrieval.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {625–633},
numpages = {9},
keywords = {stemming, normalization, lemmatization, clustering},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031287,
author = {Krishna, Vikas and Deshpande, Prasad M. and Srinivasan, Savitha},
title = {Towards Smarter Documents},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031287},
doi = {10.1145/1031171.1031287},
abstract = {Document analysis research typically focuses on document image understanding or classic problems in text classification, clustering, summarization and discovery. While that is an important aspect of document management, in practice, documents lifecycles are often determined by the context of the business process that they are relevant to. It therefore becomes necessary for the document analysis techniques to recognize and leverage the contextual information provided by a supporting schema and business process. This paper presents an intelligent document management framework with relevant document analysis, metadata extraction, and business process association algorithms and methodology. The architecture supporting this framework seamlessly integrates a runtime environment with an authoring environment by combining relational data modeling tools with document classification techniques. The runtime environment accepts incoming documents, classifies the document, extracts metadata and executes customized business logic. The authoring environment supports the association of a class of documents with a relational document schema, identification of attribute values that must be extracted automatically, generation of relevant business logic, and deployment of authoring artifacts into the runtime architecture. We demonstrate the use of this framework with representative real-world document transformative applications.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {634–641},
numpages = {8},
keywords = {workflow, processes, content, classification},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031288,
author = {Mason, Paul and Cosh, Ken and Vihakapirom, Pulyamon},
title = {On Structuring Formal, Semi-Formal and Informal Data to Support Traceability in Systems Engineering Environments},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031288},
doi = {10.1145/1031171.1031288},
abstract = {The development of large, complex systems poses a number of challenges for systems engineers, not least of which is the ability to ensure user requirements have been satisfied. Effective requirements management - an amalgam of information capture, information <i>storage</i> and <i>management</i>, and information <i>dissemination</i> activities - is crucial in that respect. In this paper we concentrate on one of the core issues of <i>information management</i> in a requirements management context - namely <i>traceability</i>.Traceability is the common term for mechanisms to record and navigate relationships between artifacts produced by development processes. However, realising effective traceability in systems engineering environments is complicated by the fact that engineers use a range of notations to describe complex systems. These range from natural language (informal), to graphical notations such as Statecharts (semi-formal) to languages with a well defined (formal) semantics such as VDM-SL and SPARK Ada. Most have tool support, although a lack of well-defined approaches to integration leads to inconsistencies and limits traceability between their respective data sets (internal models). This paper demonstrates an approach based on meta-modelling that enables traceability links to be established and consistency maintained between tools.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {642–651},
numpages = {10},
keywords = {meta-modelling, systems engineering data, traceability},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

@inproceedings{10.1145/1031171.1031289,
author = {Ding, Li and Finin, Tim and Joshi, Anupam and Pan, Rong and Cost, R. Scott and Peng, Yun and Reddivari, Pavan and Doshi, Vishal and Sachs, Joel},
title = {Swoogle: A Search and Metadata Engine for the Semantic Web},
year = {2004},
isbn = {1581138741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031171.1031289},
doi = {10.1145/1031171.1031289},
abstract = {Swoogle is a crawler-based indexing and retrieval system for the Semantic Web. It extracts metadata for each discovered document, and computes relations between documents. Discovered documents are also indexed by an information retrieval system which can use either character N-Gram or URIrefs as keywords to find relevant documents and to compute the similarity among a set of documents. One of the interesting properties we compute is <i>ontology rank</i>, a measure of the importance of a Semantic Web document.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
pages = {652–659},
numpages = {8},
keywords = {search, metadata, crawler, semantic web, rank},
location = {Washington, D.C., USA},
series = {CIKM '04}
}

