@inproceedings{10.1145/319950.319951,
author = {Giles, C. Lee},
title = {Searching the Web (Keynote Address): Can You Find What You Want?},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319951},
doi = {10.1145/319950.319951},
abstract = {The World Wide Web has revolutionized communication and information distribution, storage, and access. Its impact has been felt everywhere - e.g. science and technology, commerce and business, education, government, religion, law, entertainment, health care. Even so, there are many ways the web can be improved. We discuss what the web consists of and how it has changed, what is the size of the web, and what is covered. Results for the publicly indexable web show that the web though Terabytes in size and growing is still less than large commercial databases and the Library of Congress. Though the web started out as an academic-government endeavor, it is now primarily commerce. Furthermore, the major web search engines cover only a fraction of the publicly indexable web and appear to base their indexing strategy on the popularity of information. Since current search on the web is primarily done with the search engines, what would be the economic, political and scientific implications of these results?},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {1},
numpages = {1},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319952,
author = {Scoggins, Jim},
title = {A Practitioner's View of Techniques Used in Data Warehousing for Sifting through Data to Provide Information (Keynote Address)},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319952},
doi = {10.1145/319950.319952},
abstract = {Over the past 10 years data warehousing evolved from providing 'nice to know' data to 'need to know' information. The decision support systems providing summarized reports to executives have advanced to integrated information factories providing vital information to the desktops of knowledge workers. Data warehousing has benefited through the use advanced techniques of sifting through data to produce information. This discussion will cover data mining techniques used in specific business cases as well as attempt to describe problems that still exist (and could be researched) in the business intelligence arena.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {3},
numpages = {1},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319953,
author = {Yu, Byunggu and Orlandic, Ratko and Evens, Martha},
title = {Simple QSF-Trees: An Efficient and Scalable Spatial Access Method},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319953},
doi = {10.1145/319950.319953},
abstract = {The development of high-performance spatial access methods that can support complex operations of large spatial databases continues to attract considerable attention. This paper introduces QSF-trees, an efficient and scalable structure for indexing spatial objects, which has some important advantages over R*-trees. QSF-trees eliminate overlapping of index regions without forcing object clipping or sacrificing the selectivity of spatial operations. The method exploits the semantics of topological relations between spatial objects to further reduce the number of index nodes visited during the search. A series of experiments involving randomly-generated spatial objects was conducted to compare the structure with two variations of R*-trees. The experiments show QSF-trees to be more efficient and more scalable to the increase in the data-set size, the size of spatial objects, and the number of dimensions of the spatial universe.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {5–14},
numpages = {10},
keywords = {database management, point access methods, spatial access methods, spatial database, topological relations},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319954,
author = {Song, Ju-Won and Whang, Kyu-Young and Lee, Young-Koo and Lee, Min-Jae and Kim, Sang-Wook},
title = {Transformation-Based Spatial Join},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319954},
doi = {10.1145/319950.319954},
abstract = {Spatial join finds pairs of spatial objects having a specific spatial relationship in spatial database systems. A number of spatial join algorithms have recently been proposed in the literature. Most of them, however, perform the join in the original space. Joining in the original space has a drawback of dealing with sizes of objects and thus has difficulty in developing a formal algorithm that does not rely on heuristics. In this paper, we propose a spatial join algorithm based on the transformation technique. An object having a size in the two-dimensional original space is transformed into a point in the four-dimensional transform space, and the join is performed on these point objects. This can be easily extended to n-dimensional cases. We show the excellence of the proposed approach through analysis and extensive experiments. The results show that the proposed algorithm has a performance generally better than that of the R*-based algorithm proposed by Brinkhoff et al. This is a strong indicating that corner transformation preserves clustering among objects and that spatial operations can be performed better in the transform space than in the original space. This reverses the common belief that transformation will adversely affect clustering. We believe that our result will provide a new insight towards transformation-based spatial query processing.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {15–26},
numpages = {12},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319955,
author = {Vasilis, Delis and Thanasis, Hadzilacos},
title = {Binary String Relations: A Foundation for Spatiotemporal Knowledge Representation},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319955},
doi = {10.1145/319950.319955},
abstract = {The paper is concerned with the qualitative representation of spatiotemporal relations. We initially propose a multiresolution framework for the representation of relations among 1D intervals, based on a binary string encoding. We subsequently extend this framework to multiple dimensions, thus allowing the description of spatiotemporal relations at various contexts. The feasible relations at a particular resolution level are inherently permeated by a poset structure, called conceptual neighbourhood, upon which we propose efficient relation inferencing mechanisms. Finally, we discuss the application of our model to spatiotemporal reasoning, which refers to the classic problems of satisfiability and deductive closure of a set of spatiotemporal assertions.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {27–37},
numpages = {11},
keywords = {spatiotemporal relations, conceptual neighbourhoods, spatiotemporal reasoning},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319956,
author = {Swan, Russell and Allan, James},
title = {Extracting Significant Time Varying Features from Text},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319956},
doi = {10.1145/319950.319956},
abstract = {We propose a simple statistical model for the frequency of occurrence of features in a stream of text. Adoption of this model allows us to use classical significance tests to filter the stream for interesting events. We tested the model by building a system and running it on a news corpus. By a subjective evaluation, the system worked remarkably well: almost all of the groups of identified tokens corresponded to news stories and were appropriately placed in time. A preliminary objective evaluation was also used to measure the quality of the system and it showed some of the weaknesses and the power of our approach.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {38–45},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.323229,
author = {Kanada, Yasusi},
title = {A Method of Geographical Name Extraction from Japanese Text for Thematic Geographical Search},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.323229},
doi = {10.1145/319950.323229},
abstract = {A text retrieval method called the thematic geographical search method has been developed and applied to a Japanese encyclopedia called the World Encyclop\ae{}dia. In this method, the user specifies a search theme using free words, then obtains a sorted list of excerpts and hyperlinks to encyclopedia sentences that contain geographical names. Using this list, the user can also open maps that indicate the locations of the names. To generate an index of names for this searching, a method of extracting geographical names has been developed. In this method, geographical names are extracted, matched to names in a geographical name database, and identified. Geographical names, however, often have several types of ambiguities. Ambiguities are resolved by using non-local context analysis, which uses a stack and several other techniques. As a result, the precision of extracted names is more than 96% on average. This method depends on features of the Japanese language, but the strategy and most of the techniques can be applied to texts in English or other languages.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {46–54},
numpages = {9},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319957,
author = {Lin, Chin-Yew},
title = {Training a Selection Function for Extraction},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319957},
doi = {10.1145/319950.319957},
abstract = {In this paper we compare performance of several heuristics in generating informative generic/query-oriented extracts for newspaper articles in order to learn how topic prominence affects the performance of each heuristic. We study how different query types can affect the performance of each heuristic and discuss the possibility of using machine learning algorithms to automatically learn good combination functions to combine several heuristics. We also briefly describe the design, implementation, and performance of a multilingual text summarization system SUMMARIST.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {55–62},
numpages = {8},
keywords = {topic extraction, automated text summarization, summary evaluation},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319958,
author = {Pr\"{o}ll, Birgit and Starck, Heinrich and Retschitzegger, Werner and Sighart, Harald},
title = {Ready for Prime Time: Pre-Generation of Web Pages in TIScover},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319958},
doi = {10.1145/319950.319958},
abstract = {In large data- and access-intensive web sites, efficient and reliable access is hard to achieve. This situation gets even worse for web sites providing precise structured query facilities and requiring topicality of the presented information even in face of a highly dynamic content. The achievement of these partly conflicting goals is strongly influenced by the approach chosen for page generation, ranging from composing a web page upon a user's request to its generation in advance. The official Austrian web-based tourism information and booking system TIScover tries to reconcile these goals by employing a hybrid approach of page generation. In TIScover, web pages are not only generated on request in order to support precise structured queries on the content managed by a database system. Rather, the whole web site is also pre-generated out of the extremely dynamic content and synchronized with the database on the basis of metadata. Thus, topicality of information is guaranteed, while ensuring efficient and reliable access. This paper discusses the hybrid approach as realized in TIScover, focussing in particular on the concepts used for pre-generation.1},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {63–68},
numpages = {6},
keywords = {WWW, tourism information system, page generation, optimization, reliability},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319959,
author = {Wu, Kun-Lung and Yu, Philip S.},
title = {Local Replication for Proxy Web Caches with Hash Routing},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319959},
doi = {10.1145/319950.319959},
abstract = {This paper studies controlled local replication for hash routing, such as CARP, among a collection of loosely-coupled proxy web cache servers. Hash routing partitions the entire URL space among the shared web caches, creating a single logical cache. Each partition is assigned to a cache server. Duplication of cache contents is eliminated and total incoming traffic to the shared web caches is minimized. Client requests for non-assigned-partition objects are forwarded to sibling caches. However, request forwarding increases not only inter-cache traffic but also cpu utilization, thus slows the client response time. We propose a controlled local replication of non-assigned-partition objects in each cache server to effectively reduce the inter-cache traffic. We use a multiple-exit LRU to implement controlled local replication. Trace-driven simulations are conducted to study the performance impact of local replication. The results show that (1) regardless of cache sizes, with a controlled local replication, the average response time, inter-cache traffic and CPU overhead can be effectively reduced without noticeable increases in incoming traffic; (2) for very large cache sizes, a larger amount of local replication can be allowed to reduce inter-cache traffic without increasing incoming traffic; and (3) local replication is effective even if clients are dynamically assigned to different cache servers.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {69–76},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319960,
author = {Lee, Dongwon and Chu, Wesley W.},
title = {Semantic Caching via Query Matching for Web Sources},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319960},
doi = {10.1145/319950.319960},
abstract = {A semantic caching scheme suitable for wrappers wrapping web sources is presented. Since the web sources have typically weaker querying capabilities than conventional databases, existing semantic caching schemes cannot be applied directly. A seamlessly integrated query translation and capability mapping between the wrappers and web sources in semantic caching is described. In addition, an analysis on the match types between the user's input query and cached queries is presented. Semantic knowledge acquired from the data can be used to avoid unnecessary access to the web sources by transforming the cache miss to the cache hit. A polynomial time algorithm based on the proposed query matching technique is presented to find the best matched query in the cache. Experimental results reveal the effectiveness of the proposed semantic caching scheme.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {77–85},
numpages = {9},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319961,
author = {Liddle, Stephen W. and Campbell, Douglas M. and Crawford, Chad},
title = {Automatically Extracting Structure and Data from Business Reports},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319961},
doi = {10.1145/319950.319961},
abstract = {A considerable amount of clean semistructured data is internally available to companies in the form of business reports. However, business reports are untapped for data mining, data warehousing, and querying because they are not in relational form. Business reports have a regular structure that can be reconstructed. We present algorithms that automatically infer the regular structure underlying business reports and automatically generate wrappers to extract relational data.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {86–93},
numpages = {8},
keywords = {business reports, data and information extraction, automatic wrapper generation, report structure, regular expressions},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319962,
author = {Ribeiro-Neto, Berthier and Laender, Alberto H. F. and da Silva, Altigran S.},
title = {Extracting Semi-Structured Data through Examples},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319962},
doi = {10.1145/319950.319962},
abstract = {In this paper, we describe an innovative approach to extracting semi-structured data from Web sources. The idea is to collect a couple of example objects from the user and to use this information to extract new objects from new pages or texts. To perform the extraction of new objects, we introduce a bottom-up extration strategy and, through experimentation, demonstrate that it works quite effectively with distinct Web sources, even if only a few examples are provided by the user.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {94–101},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319963,
author = {Shyu, Mei-Ling and Chen, Shu-Ching and Kashyap, R. L.},
title = {Discovering Quasi-Equivalence Relationships from Database Systems},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319963},
doi = {10.1145/319950.319963},
abstract = {Association rule mining has recently attracted strong attention and proven to be a highly successful technique for extracting useful information from very large databases. In this paper, we explore a generalized affinity-based association mining which discovers quasi-equivalent media objects in a distributed information-providing environment consisting of a network of heterogeneous databases which could be relational databases, hierarchical databases, object-oriented databases, multimedia databases, etc. Online databases, consisting of millions of media objects, have been used in business management, government administration, scientific and engineering data management, and many other applications owing to the recent advances in high-speed communication networks and large-capacity storage devices. Because of the navigational characteristic, queries in such an information-providing environment tend to traverse equivalent media objects residing in different databases for the related data records. As the number of databases increases, query processing efficiency depends heavily on the capability to discover the equivalence relationships of the media objects from the network of databases. Theoretical terms along with an empirical study of real databases are presented.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {102–108},
numpages = {7},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319964,
author = {Matsuda, Katsushi and Fukushima, Toshikazu},
title = {Task-Oriented World Wide Web Retrieval by Document Type Classification},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319964},
doi = {10.1145/319950.319964},
abstract = {This paper proposes a novel approach to accurately searching Web pages for relevant information in problem solving by specifying a Web document category instead of the user's task. Accessing information from World Wide Web pages as an approach to problem solving has become commonplace. However, such a search is difficult with current search services, since these services only provide keyword-based search methods that are equivalent to narrowing down the target references according to domains. However, problem solving usually involves both a domain and a task. Accordingly, our approach is based on problem solving tasks. To specify a user's problem solving task, we introduce the concept of document types that directly relate to the problem solving tasks; with this approach, users can easily designate problem solving tasks. We implemented PageTypeSearch system based on our approach. Classifier of PageTypeSearch classifies Web pages into the document types by comparing their pages with typical structural characteristics of the types. We compare PageTypeSearch using the document typeindices with a conventional keyword-based search system in experiments. The average precision of the document type-based search is 88.9%, while the average precision of the keyword-based search is 31.2%. Moreover, the number of irrelevant references gathered by our system is about one-thirteenth that of traditional keyword-based search systems. Our approach has practical advantages for problem solving by introducing the viewpoint of tasks to achieve higher performance.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {109–113},
numpages = {5},
keywords = {classification, WWW, document type, information retrieval},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319965,
author = {Hsu, Wen-Lin and Lang, Sheau-Dong},
title = {Classification Algorithms for NETNEWS Articles},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319965},
doi = {10.1145/319950.319965},
abstract = {We propose several algorithms using the vector space model to classify the news articles posted on the NETNEWS according to the newsgroup categories. The baseline method combines the terms of all the articles of each newsgroup in the training set to represent the newsgroups as single vectors. After training, the incoming news articles are classified based on their similarity to the existing newsgroup categories. We propose to use the following techniques to improve the classification performance of the baseline method: (1) use routing (classification) accuracy and the similarity values to refine the training set; (2) update the underlying term structures periodically during testing; and (3) apply k-means clustering to partition the newsgroup articles and represent each newsgroup by k vectors. Our test collection consists of the real news articles and the 519 subnewsgroups under the REC newsgroup of NETNEWS in a period of 3 months. Our experimental results demonstrate that the technique of refining the training set reduces from one-third to two-thirds of the storage. The technique of periodical updates improves the routing accuracy ranging from 20% to 100% but incurs runtime overhead. Finally, representing each newsgroup by k vectors (with k = 2 or 3) using clustering yields the most significant improvement in routing accuracy, ranging from 60% to 100%, while causing only slightly higher storage requirements.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {114–121},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319966,
author = {Li, Hang and Yamanishi, Kenji},
title = {Text Classification Using ESC-Based Stochastic Decision Lists},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319966},
doi = {10.1145/319950.319966},
abstract = {We propose a new method of text classification using stochastic decision lists. A stochastic decision list is an ordered sequence of IF-THEN rules, and our method can be viewed as a rule-based method for text classification having advantages of readability and refinability of acquired knowledge. Our method is unique in that decision lists are automatically constructed on the basis of the principle of minimizing Extended Stochastic Complexity (ESC), and with it we are able to construct decision lists that have fewer errors in classification. The accuracy of classification achieved with our method appears better than or comparable to those of existing rule-based methods.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {122–130},
numpages = {9},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319968,
author = {Wieczerzycki, Waldemar},
title = {Database Model for Web-Based Cooperative Applications},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319968},
doi = {10.1145/319950.319968},
abstract = {In this paper we propose a model of a database that could become a kernel of cooperative database applications. First, we propose a new data model CDM (Collaborative Data Model) that is oriented for the specificity of multiuser environments, in particular: cooperation scenarios, cooperation techniques and cooperation management. Second, we propose to apply to databases supporting collaboration so called multiuser transactions. Multiuser transactions are flat transactions in which, in comparison to classical ACID transactions, the isolation property is relaxed.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {131–138},
numpages = {8},
keywords = {trasaction model, object-oriented databases, CSCW, data model},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319970,
author = {Lawrence, Steve and Bollacker, Kurt and Giles, C. Lee},
title = {Indexing and Retrieval of Scientific Literature},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319970},
doi = {10.1145/319950.319970},
abstract = {The web has greatly improved access to scientific literature. However, scientific articles on the web are largely disorganized, with research articles being spread across archive sites, institution sites, journal sites, and researcher homepages. No index covers all of the available literature, and the major web search engines typically do not index the content of Postscript/PDF documents at all. This paper discusses the creation of digital libraries of scientific literature on the web, including the efficient location of articles, full-text indexing of the articles, autonomous citation indexing, information extraction, display of query-sensitive summaries and citation context, hubs and authorities computation, similar document detection, user profiling, distributed error correction, graph analysis, and detection of overlapping documents. The software for the system is available at no cost for non-commercial use.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {139–146},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319971,
author = {Allen, Robert B. and Schalow, John},
title = {Metadata and Data Structures for the Historical Newspaper Digital Library},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319971},
doi = {10.1145/319950.319971},
abstract = {We examine metadata and data-structure issues for the Historical Newspaper Digital Library. This project proposes to digitize and then do OCR and linguisting processing on several years worth of historical newspapers. Newspapers are very complex information objects so developing a rich description of their content is challenging. In addition to frameworks for the logical structure and physical layout, we propose metadata relevant to the image processing and to the historians who will use this collection. Finally, we consider how the metadata infrastructure might be managed as it evolves with improved text processing capabilities and how an infrastructure might be developed to support a community of users.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {147–153},
numpages = {7},
keywords = {digital libraries, history, metadata, newspapers, OCR},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319972,
author = {Noaman, Amin Y. and Barker, Ken},
title = {A Horizontal Fragmentation Algorithm for the Fact Relation in a Distributed Data Warehouse},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319972},
doi = {10.1145/319950.319972},
abstract = {Data warehousing is one of the major research topics of appliedside database investigators. Most of the work to date has focused on building large centralized systems that are integrated repositories founded on pre-existing systems upon which all corporate-wide data are based. Unfortunately, this approach is very expensive and tends to ignore the advantages realized during the past decade in the area of distribution and support for data localization in a geographically dispersed corporate structure. This research investigates building distributed data warehouses with particular emphasis placed on distribution design for the data warehouse environment. The article provides an architectural model for a distributed data warehouse, the formal definition of the relational data model for data warehouse and a methodology for distributed data warehouse design along with a “horizontal” fragmentation algorithm for the fact relation.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {154–161},
numpages = {8},
keywords = {distributed data warehouse design, distributed data warehouse architecture, horizontal fragmentation},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319974,
author = {Cheung, David W. and Zhou, Bo and Kao, Ben and Lu, Hongjun and Lam, Tak Wah and Ting, Hing Fung},
title = {Requirement-Based Data Cube Schema Design},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319974},
doi = {10.1145/319950.319974},
abstract = {On-line analytical processing (OLAP) requires efficient processing of complex decision support queries over very large databases. It is well accepted that pre-computed data cubes can help reduce the response time of such queries dramatically. A very important design issue of an efficient OLAP system is therefore the choice of the right data cubes to materialize. We call this problem the data cube schema design problem. In this paper we show that the problem of finding an optimal data cube schema for an OLAP system with limited memory is NP-hard. As a more computationally efficient alternative, we propose a greedy approximation algorithm cMP and its variants. Algorithm cMP consists of two phases. In the first phase, an initial schema consisting of all the cubes required to efficiently answer the user queries is formed. In the second phase, cubes in the initial schema are selectively merged to satisfy the memory constraint. We show that cMP is very effective in pruning the search space for an optimal schema. This leads to a highly efficient algorithm. We report the efficiency and the effectiveness of cMP via an empirical study using the TPC-D benchmark. Our results show that the data cube schemas generated by cMP enable very efficient OLAP query processing.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {162–169},
numpages = {8},
keywords = {DSS, data cube schema design, OLAP, data cubes},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319975,
author = {Johnson, Theodore and Chatziantoniou, Damianos},
title = {Extending Complex Ad-Hoc OLAP},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319975},
doi = {10.1145/319950.319975},
abstract = {Large scale data analysis and mining activities require sophisticated information extraction queries. Many queries require complex aggregation, and many of these aggregates are non-distributive. Conventional solutions to this problem involve defining User Defined Aggregate Functions (UDAFs). However, the use of UDAFs entails several problems. Defining a new UDAF can be a significant burden for the user, and optimizing queries involving UDAFs is difficult because of the “black box” nature of the UDAF.In this paper, we present a method for expressing nested aggregates in a declarative way. A nested aggregate, which is a rollup of another aggregated value, expresses a wide range of useful non-distributive aggregation. For example, most frequent type aggregation can be naturally expressed using nested aggregation, e.g. “For each product, report its total sales during the month with the largest total sales of the product”. By expressing compex aggregates declaratively, we relieve the user of the burden of defining UDAFs, and allow the evalution of the complex aggregates to be optimized.We use the Extended Multi-Feature (EMF) syntax as the basis for expressing nested aggregation. An advantage of this approach is that EMF SQL can already express a wide range of complex aggregation in a succinct way, and EMF SQL is easily optimized into efficient query plans. We show that nested aggregation queries can be evaluated efficiently by using a small extension to the EMF SQL query evaluation algorithm. A side effect of this extension is to extend EMF SQL to permit complex aggregation of data from multiple sources.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {170–179},
numpages = {10},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319976,
author = {Labrou, Yannis and Finin, Tim},
title = {Yahoo! As an Ontology: Using Yahoo! Categories to Describe Documents},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319976},
doi = {10.1145/319950.319976},
abstract = {We suggest that one (or a collection) of names of Yahoo! (or any other WWW indexer's) categories can be used to describe the content of a document. Such categories offer a standardized and universal way for referring to or describing the nature of real world objects, activities, documents and so on, and may be used (we suggest) to semantically characterize the content of documents. WWW indices, like Yahoo! provide a huge hierarchy of categories (topics) that touch every aspect of human endeavors. Such topics can be used as descriptors, similarly to the way librarians use for example, the Library of Congress cataloging system to annotate and categorize books.In the course of investigating this idea, we address the problem of automatic categorization of webpages in the Yahoo! directory. We use Telltale as our classifier; Telltale uses n-grams to compute the similarity between documents. We experiment with various types of descriptions for the Yahoo! categories and the webpages to be categorized. Our findings suggest that the best results occur when using the very brief descriptions of the Yahoo! categorized entries; these brief descriptions are provided either by the entries' submitters or by the Yahoo! human indexers and accompany most Yahoo!-indexed entries.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {180–187},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.329374,
author = {Zhu, Xiaolan and Gauch, Susan and Gerhard, Lutz and Kral, Nicholas and Pretschner, Alexander},
title = {Ontology-Based Web Site Mapping for Information Exploration},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.329374},
doi = {10.1145/319950.329374},
abstract = {Centralized search process requires that the whole collection reside at a single site. This imposes a burden on both the system storage of the site and the network traffic near the site. It thus comes to require the search process to be distributed. Recently, more and more Web sites provide the ability to search their local collection of Web pages. Query brokering systems are used to direct queries to the promising sites and merge the results from these sites. Creation of meta-information of the sites plays an important role in such systems. In this article, we introduce an ontology-based web site mapping method used to produce conceptual meta-information, the Vector Space approach, and present a serial of experiments comparing it with Na\"{\i}ve-Bayes approach. We found that the Vector Space approach produces better accuracy in ontology-based web site mapping.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {188–194},
numpages = {7},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319978,
author = {Geffner, S. and Agrawal, D. and Abbadi, A. El and Smith, T.},
title = {Browsing  Large Digital Library Collections Using Classification Hierarchies},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319978},
doi = {10.1145/319950.319978},
abstract = {Summarization of intermediary query result sets plays an important role when users browse through digital library collections. Summarization enables users to quickly digest the results of their queries, and provides users with important information they can use to narrow their search interactively. Techniques from the field of data analysis may be applied to the problem of generating summaries of query results efficiently. Such techniques should permit the incorporation of classification hierarchies in order to provide powerful browsing environments for digital library users.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {195–201},
numpages = {7},
keywords = {classification, summarization, browsing, digital libraries, searching, aggregation},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319979,
author = {Lin, Yong and Xu, Jian and Lim, Ee-Peng and Ng, Wee-Keong},
title = {ZBroker: A Query Routing Broker for Z39.50 Databases},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319979},
doi = {10.1145/319950.319979},
abstract = {A query routing broker is a software agent that determines from a large set of accessing information sources the ones most relevant to a user's information need. As the number of information sources on the Internet increases dramatically, future users will have to rely on query routing brokers to decide a small number of information sources to query without incurring too much query processing overheads. In this paper, we describe a query routing broker known as ZBroker developed for bibliographic database servers that support the Z39.50 protocol. ZBroker samples the content of each bibliographic database by using training queries and their results, and summarizes the bibliographic database content into a knowledge base. We present the design and implementation of ZBroker and describe its Web-based user interface.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {202–209},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319980,
author = {Glover, Eric J. and Lawrence, Steve and Birmingham, William P. and Giles, C. Lee},
title = {Architecture of a Metasearch Engine That Supports User Information Needs},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319980},
doi = {10.1145/319950.319980},
abstract = {When a query is submitted to a metasearch engine, decisions are made with respect to the underlying search engines to be used, what modifications will be made to the query, and how to score the results. These decisions are typically made by considering only the user's keyword query, neglecting the larger information need. Users with specific needs, such as “research papers” or “homepages,” are not able to express these needs in a way that affects the decisions made by the metasearch engine. In this paper, we describe a metasearch engine architecture that considers the user's information need for each decision. Users with different needs, but the same keyword query, may search different sub-search engines, have different modifications made to their query, and have results ordered differently. Our architecture combines several powerful approaches together in a single general purpose metasearch engine.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {210–216},
numpages = {7},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320005,
author = {Yu, Clement and Meng, Weiyi and Liu, King-Lup and Wu, Wensheng and Rishe, Naphtali},
title = {Efficient and Effective Metasearch for a Large Number of Text Databases},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320005},
doi = {10.1145/319950.320005},
abstract = {Metasearch engines can be used to facilitate ordinary users for retrieving information from multiple local sources (text databases). In a metasearch engine, the contents of each local database is represented by a representative. Each user query is evaluated against the set of representatives of all databases in order to determine the appropriate databases to search. When the number of databases is very large, say in the order of tens of thousands or more, then a traditional metasearch engine may become inefficient as each query needs to be evaluated against too many database representatives. Furthermore, the storage requirement on the site containing the metasearch engine can be very large. In this paper, we propose to use a hierarchy of database representatives to improve the efficiency. We provide an algorithm to search the hierarchy. We show that the retrieval effectiveness of our algorithm is the same as that of evaluating the user query against all database representatives. We also show that our algorithm is efficient. In addition, we propose an alternative way of allocating representatives to sites so that the storage burden on the site containing the metasearch engine is much reduced.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {217–224},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320007,
author = {Feng, Ling and Lu, Hongjun and Yu, Jeffrey Xu and Han, Jiawei},
title = {Mining Inter-Transaction Associations with Templates},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320007},
doi = {10.1145/319950.320007},
abstract = {Multi-dimensional, inter-transaction association rules extend the traditional association rules to describe more general associations among items with multiple properties cross transactions. “After McDonald and Burger King open branches, KFC will open a branch two months later and one mile away” is an example of such rules. Since the number of potential inter-transaction association rules tends to be extremely large, mining inter-transaction associations poses more challenges on efficient processing than mining intra-transaction associations. In order to make such association mining truly practical and computationally tractable, in this study, we present a template model to help users declare the interesting inter-transaction associations to be mined. With the guidance of templates, several optimization techniques are devised to speed up the discovery of inter-transaction association rules. We show, through a series of experiments, that these optimization techniques can yield significant performance benefits.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {225–233},
numpages = {9},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319981,
author = {Holt, John D. and Chung, Soon M.},
title = {Efficient Mining of Association Rules in Text Databases},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319981},
doi = {10.1145/319950.319981},
abstract = {In this paper, we propose two new algorithms for mining association rules between words in text databases. The characteristics of text databases are quite different from those of retail transaction databases, and existing mining algorithms cannot handle text databases efficiently because of the large number of itemsets (i.e., words) that need to be counted. Two well-known mining algorithms, Apriori algorithm and Direct Hashing and Pruning (DHP) algorithm, are evaluated in the context of mining text databases, and are compared with the new proposed algorithms named Multipass-Apriori (M-Apriori) and Multipass-DHP (M-DHP). It has been shown that the proposed algorithms have better performance for large text databases.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {234–242},
numpages = {9},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320008,
author = {Yoon, Suk-Chung and Henschen, Lawrence J. and Park, E. K. and Makki, Sam},
title = {Using Domain Knowledge in Knowledge Discovery},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320008},
doi = {10.1145/319950.320008},
abstract = {With the explosive growth of the size of databases, many knowledge discovery applications deal with large quantities of data. There is an urgent need to develop methodologies which will allow the applications to focus search to a potentially interesting and relevant portion of the data, which can reduce the computational complexity of the knowledge discovery process and improve the interestingness of discovered knowledge. Previous work on semantic query optimization, which is an approach to take advantage of domain knowledge for query optimization, has demonstrated that significant cost reduction can be achieved by reformulating a query into a less expensive yet equivalent query which produces the same answer as the original one. In this paper, we introduce a method to utilize three types of domain knowledge in reducing the cost of finding a potentially interesting and relevant portion of the data while improving the quality of discovered knowledge. In addition, we propose a method to select relevant domain knowledge without an exhaustive search of all domain knowledge. The contribution of this paper is that we lay out a general framework for using domain knowledge in the knowledge discovery process effectively by providing guidelines.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {243–250},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320010,
author = {Parthasarathy, S. and Zaki, M. J. and Ogihara, M. and Dwarkadas, S.},
title = {Incremental and Interactive Sequence Mining},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320010},
doi = {10.1145/319950.320010},
abstract = {The discovery of frequent sequences in temporal databases is an important data mining problem. Most current work assumes that the database is static, and a database update requires rediscovering all the patterns by scanning the entire old and new database. In this paper, we propose novel techniques for maintaining sequences in the presence of a) database updates, and b) user interaction (e.g. modifying mining parameters). This is a very challenging task, since such updates can invalidate existing sequences or introduce new ones. In both the above scenarios, we avoid re-executing the algorithm on the entire dataset, thereby reducing execution time. Experimental results confirm that our approach results in execution time improvements of up to several orders of magnitude in practice.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {251–258},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320011,
author = {Lee, Jeong-Oog and Baik, Doo-Kwon},
title = {SemQL: A Semantic Query Language for Multidatabase Systems},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320011},
doi = {10.1145/319950.320011},
abstract = {An essential prerequisite to achieving interoperability in multidatabase systems is to be able to identify semantically equivalent or related data items in component databases. Another problem in multidatabase systems is allowing users to handle information from different databases that refer to the same realworld entity. In this paper, we provide semantic networks so that multidatabase systems can detect and resolve semantic heterogeneities among component databases. And we provide a semantic query language, SemQL, to capture the concepts about what users want. It enables users to issue queries to a large number of autonomous databases without prior knowledge of their schemas.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {259–266},
numpages = {8},
keywords = {wordnet, semantic query language, semantic network, multidatabase},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320012,
author = {Warshaw, Lane B. and Miranker, Daniel P.},
title = {Rule-Based Query Optimization, Revisited},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320012},
doi = {10.1145/319950.320012},
abstract = {We present the architecture and a performance assessment of an extensible query optimizer written in Venus. Venus is a general-purpose active-database rule language embedded in C++. Following the developments in extensible database query optimizers, first in rule-based form, followed by optimizers written as object-oriented programs, the Venus-based optimizer avails to the advantages of both. Venus' modular structure allows us to go a step further and provide extensibility in search by defining parameterized search components in a declarative form that has the additional effect of integrating heuristic and cost-based optimization. We compare optimizers developed with Volcano, OPT++ and Venus. Venus' optimizing compiler yields code whose performance is comparable with Volcano and OPT++ on smaller queries. The ability to introduce additional pruning heuristics yields better scalability on larger queries. Evaluation of the system using quantitative software metrics supports a claim that the Venus-based optimizer is more easily maintained and extended than are its predecessors.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {267–275},
numpages = {9},
keywords = {rule-based optimizers, extensible query optimization, rule-based programming, heuristics, object oriented optimizers, search, rule modules, declarative programming},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320013,
author = {Lim, Andrew and Kwan, Jennifer Lai-Pheng and Oon, Wee-Chong},
title = {Page Access Scheduling in Join Processing},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320013},
doi = {10.1145/319950.320013},
abstract = {The join relational operation is one of the most expensive among database operations. In this study, we consider the problem of scheduling page accesses in join processing. This raises two interesting problems: 1) determining a page access sequence that uses the minimum number of buffer pages without any page reaccesses, and 2) determining a page access sequence that minimizes the number of page reaccesses for a given buffer size. We use a graph model to represent the pages from the relations that contain tuples to be joined, and present new heuristics for the two problems based on the sort-merge join and the simple TID algorithm. Our experimental results show that the new heuristics perform well.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {276–283},
numpages = {8},
keywords = {page access scheduling, join processing, heuristics, graph models},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320015,
author = {Tanzer, David and Shasha, Dennis},
title = {Queryable Acyclic Production Systems},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320015},
doi = {10.1145/319950.320015},
abstract = {We pose a query problem about the behavior of a consultation system S: given a constraint formula q and a potential conclusion c for S, determine if there is a user input binding that satisfies q and causes S to conclude c. Existing rule-based expert systems, both forward and backward chaining[3], implement a consultation mechanism S, but are not designed for these queries about S. For general production systems, the queries are undecidable. Here we solve the problem for useful sublanguages of acyclic production systems.We implement a query tool in a Datalog + constraints framework, and optimize for “embedded decision trees” in the rule system. Our data complexity is Θ(n·undefined(n)) in the size of the embedded trees, versus Θ(n·undefined(n) + n2) for existing datalog evaluation algorithms, where undefined(n) is the cost of destructively conjoining a constraint of unit size into a conjunction of n constraints.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {284–291},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320018,
author = {Samtani, S. and Kumar, V. and Mohania, M.},
title = {Self Maintenance of Multiple Views in Data Warehousing},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320018},
doi = {10.1145/319950.320018},
abstract = {Materialized views (MV) at the data warehouse (DW) can be kept up to date in response to changes in data sources without accessing data sources for additional information. This process is usually refered to as “self maintenance of views”. A number of algorithms have been proposed for self maintenance of views, which use auxiliary views (AV) to keep some additional information in DW. In this paper we propose an algorithm for self maintainability of multiple MVs using the above approach. Our algorithm generates a simple maintenance query to incrementally maintain an MV along with its AV at DW. The algorithm maintains these views by minimizing the number and the size of the AVs. Our approach provides better insight into view maintenance issues by exploiting the dependencies and constraints that might exist in the data sources and multiple MVs at DW.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {292–299},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320020,
author = {Kao, Ben and Lam, K. Y. and Adelberg, Brad and Cheng, Reynold and Lee, Tony},
title = {Updates and View Maintenance in Soft Real-Time Database Systems},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320020},
doi = {10.1145/319950.320020},
abstract = {A database system contains base data items which record and model a physical, real world environment. For better decision support, base data items are summarized and correlated to derive views. These base data and views are accessed by application transactions to generate the ultimate actions taken by the system. As the environment changes, updates are applied to the base data, which subsequently trigger view recomputations. There are thus three types of activities: base data update, view recomputation, and transaction execution. In a real-time system, two timing constrains need to be enforced. We require transactions meet their deadlines (transaction timeliness) and read fresh data (data timeliness). In this paper we define the concept of absolute and relative temporal consistency from the perspective of transactions. We address the important issue of transaction scheduling among the three types of activities such that the two timing requirements can be met. We also discuss how a real-time database system should be designed to enforce different levels of temporal consistency.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {300–307},
numpages = {8},
keywords = {view maintenance, temporal consistency, real-time database, updates, transaction scheduling},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320021,
author = {Smith, John R. and Li, Chung-Sheng},
title = {An Adaptive View Element Framework for Multi-Dimensional Data Management},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320021},
doi = {10.1145/319950.320021},
abstract = {We present an adaptive wavelet view element framework for managing different types of multi-dimensional data in storage and retrieval applications. We consider the problems of multi-dimensional data compression, multi-resolution subregion access, selective materialization, progressive retrieval and similarity searching. The framework uses wavelets to partition the multi-dimensional data into view elements that form the building blocks for synthesizing views of the data. The view elements are organized and managed using different view element graphs. The graphs are used to guide cost-based view element selection algorithms for optimizing compression, access, retrieval and search performance.We present the adaptive wavelet view element framework and describe its application in managing multi-dimensional data such as 1-D time series data, 2-D images, video sequences, and multi-dimensional data cubes. We present experimental results that demonstrate that the adaptive wavelet view element framework improves performance of compressing, accessing, and retrieving multi-dimensional data compared to non-adaptive methods.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {308–315},
numpages = {8},
keywords = {OLAP, data management, multimedia database systems, digital libraries, information retrieval, content-based search, data cubes},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320022,
author = {Song, Fei and Croft, W. Bruce},
title = {A General Language Model for Information Retrieval},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320022},
doi = {10.1145/319950.320022},
abstract = {Statistical language modeling has been successfully used for speech recognition, part-of-speech tagging, and syntactic parsing. Recently, it has also been applied to information retrieval. According to this new paradigm, each document is viewed as a language sample, and a query as a generation process. The retrieved documents are ranked based on the probabilities of producing a query from the corresponding language models of these documents. In this paper, we will present a new language model for information retrieval, which is based on a range of data smoothing techniques, including the Good-Turning estimate, curve-fitting functions, and model combinations. Our model is conceptually simple and intuitive, and can be easily extended to incorporate probabilities of phrases such as word pairs and word triples. The experiments with the Wall Street Journal and TREC4 data sets showed that the performance of our model is comparable to that of INQUERY and better than that of another language model for information retrieval. In particular, word pairs are shown to be useful in improving the retrieval performance.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {316–321},
numpages = {6},
keywords = {good-turing estimate, model combinations, curve-fitting functions, statistical language modeling},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320023,
author = {Dolin, R. and Pierre, J. and Butler, M. and Avedon, R.},
title = {Practical Evaluation of IR within Automated Classification Systems},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320023},
doi = {10.1145/319950.320023},
abstract = {This paper describes some of the work we have done to evaluate and compare the use of three IR systems (Verity, LSI, and SMART) as black boxes within an automated classification environment. We use automated classification to make a quantitative comparison of the effectiveness of the systems within this context. In so doing, we also develop criteria for the construction of a useful training set. These results lead to metrics useful in the integration of IR systems into larger applications. We conclude with an initial API for an IR component within an automated classification architecture.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {322–329},
numpages = {8},
keywords = {training sets, automated classification, IR evaluation},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320024,
author = {McCabe, M. Catherine and Chowdhury, Abdur and Grossman, David A. and Frieder, Ophir},
title = {A Unified Environment for Fusion of Information Retrieval Approaches},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320024},
doi = {10.1145/319950.320024},
abstract = {Prior work has shown that combining results of various retrieval approaches and query representations can improve search effectiveness. Today, many meta-search engines exist which combine the results of various search engines in the hopes of improving overall effectiveness. However, the combination of results from different search engines masks variations in parsers, and other indexing techniques (stemming, stop words, etc.) This makes it difficult to assess the utility of the fusion technique. We have implemented the two most prevalent retrieval strategies: probabilistic and vector space using the same parser and the same relational retrieval engine. First, we identified a model that enables the fusion of an arbitrary number of sources. Next, we tested various linear combinations of these two methods as well as various thresholds for identifying retrieved documents. Our results show some improvement of effectiveness, but they also provide us for a baseline from which we can continue with other retrieval strategies and test the effect of fusing these strategies.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {330–334},
numpages = {5},
keywords = {retrieval, information retrieval, fusion, metasearch, text},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320025,
author = {Kang, Myoung-Ah and Servigne, Sylvie and Li, Ki-Joune Joune and Laurini, Robert},
title = {Indexing Field Values in Field Oriented Systems: Interval Quadtree},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320025},
doi = {10.1145/319950.320025},
abstract = {With the extension of spatial database applications, field oriented systems emerge as an important research issue in order to deal with continuous natural phenomena during the last years. It however has a large volume of data and efficient indexing methods for field data are necessary to overcome the performance obstacle. In special, we introduce indexing methods for field value queries (i.e. searching some regions where the temperature is more 20 degrees). We introduce the concept of subfield and show how we make use of this concept to index field values in field oriented systems. We present two implementation methods based on Quadtree space subdivision. We modify traditional linear quadtree implementation method for field value query processing using subfields. We analyze the performance of our methods. Experimentation with real terrain data shows that proposed indexing methods improve the query processing time of field value queries in comparison with the case of no indexing method.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {335–342},
numpages = {8},
keywords = {indexing method, subfield, field values, field oriented systems},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320026,
author = {Ferhatosmanoglu, Hakan and Agrawal, Divyakant and El Abbadi, Amr},
title = {Clustering Declustered Data for Efficient Retrieval},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320026},
doi = {10.1145/319950.320026},
abstract = {Modern databases increasingly integrate new kinds of information, such as multimedia information in the form of image, video, and audio data. Both the dimensionality and the amount of data that need to be processed is increasing rapidly, increasing the demand for the efficient retrieval of large amounts of multi-dimensional data. Declustering techniques for multi-disk architectures have been effectively used for storage. In this paper, we first establish that besides exploiting the parallelism, a careful organization of each disk must be considered for fast searching. We introduce the notion of page allocation and data space mapping which can be used to organize and retrieve multidimensional data. We develop these notions based on three different partitioning strategies: regular grid partitioning, concentric hypercubes and hyperpyramids. We develop techniques that satisfy efficient retrieval by optimizing the number of buckets retrieved by the query, disk arm movement and I/O parallelism. We prove that concentric hypercube-based mapping satisfies the optimal clustering and optimal parallelism. We develop a technique based on hyperpyramid partitioning that reduces the number of buckets retrieved by the query and has efficient inter- and intra-disk organizations. We evaluate the performance of proposed techniques by comparing them with the current approaches. The new techniques lead to very significant improvement over the existing techniques, and result in fast retrieval of multi-dimensional data.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {343–350},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320027,
author = {Hu, Quinglong and Lee, Wang-Chien and Lee, Dik Lun},
title = {Indexing Techniques for Wireless Data Broadcast under Data Clustering and Scheduling},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320027},
doi = {10.1145/319950.320027},
abstract = {This paper investigates power conserving indexing techniques for data disseminated on a broadcast channel. A hybrid indexing method combining strengths of the signature and the index tree techniques is presented. Different from previous studies, our research takes into consideration two important data organization factors, namely, clustering and scheduling. Cost models for index, signature and hybrid methods are derived by taking into account various data organizations accommodating these two factors. Based on our analytical comparisons, the signature and the hybrid indexing techniques are the best choices for power conserving indexing of various data organizations on wireless broadcast channels.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {351–358},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320028,
author = {Ravat, Franck and Teste, Olivier and Zurfluh, Giles},
title = {Towards Data Warehouse Design},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320028},
doi = {10.1145/319950.320028},
abstract = {This paper focuses on data warehouse modelling. The conceptual model we defined, is based on object concepts extended with specific concepts like generic classes, temporal classes and archive classes. The temporal classes are used to store the detailed evolutions and the archive classes store the summarised data evolutions. We also provide a flexible concept allowing the administrator to define historised parts and non-historised parts into the warehouse schema. Moreover, we introduce constraints which configure the data warehouse behaviour and these various parts. To validate our propositions, we describe a prototype dedicated to the data warehouse design.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {359–366},
numpages = {8},
keywords = {conceptual data warehouse model, object modelling, temporal data},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320029,
author = {Gal, Avigdor},
title = {Obsolescent Materialized Views in Query Processing of Enterprise Information Systems},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320029},
doi = {10.1145/319950.320029},
abstract = {In recent years, query processing has become more complex as data sources are frequently replicated and data are periodically processed and embedded within several data sources simultaneously. These trends have necessitated the optimization of techniques for query processing in order to exploit these new alternatives. Accordingly, this paper introduces an improved query optimization technique, which is capable of assessing query plans that use both current and obsolescent data. In particular, we provide a cost model by which the trade-offs of using obsolescent materialized views can be evaluated and we also discuss the method's applicability to contemporary query optimization techniques.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {367–374},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320030,
author = {Wang, Hui and Orlowska, Maria and Liang, Weifa},
title = {Efficient Refreshment of Materialized Views with Multiple Sources},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320030},
doi = {10.1145/319950.320030},
abstract = {A data warehouse collects and maintains a large amount of data from multiple distributed and autonomous data sources. Often the data in it is stored in the form of materialized views in order to provide fast access to the integrated data. However, maintaining a certain level consistency of warehouse data with the source data is challenging in a distributed multiple source environment. Transactions containing multiple updates at one or more sources further complicate the consistency issue.Following the four level consistency definition of view in a warehouse, we first present a complete consistency algorithm for maintaining SPJ-type materialized views incrementally. Our algorithm speed-ups the view refreshment time, provided that some extra moderate space in the warehouse is available. We then give a variant of the proposed algorithm by taking the update frequencies of sources into account. We finally discuss the relationship between a view's certain level consistency and its refresh time. It is difficult to propose an incremental maintenance algorithm such that the view is always kept at a certain level consistency with the source data and the view's refresh time is as fast as possible. We trade-off these two factors by giving an algorithm with faster view refresh time, while the view maintained by the algorithm is strong consistency rather than complete consistency with the source data.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {375–382},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320031,
author = {Jang, Hyunchi and Kim, Youngil and Shin, Dongwook},
title = {An Effective Mechanism for Index Update in Structured Documents},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320031},
doi = {10.1145/319950.320031},
abstract = {Indexing and retrieval of structured documents have been drawing attention increasingly since they enable to retrieve and access a certain part of a document easily. So far, several methods have been proposed in the setting that documents are rarely changed. These can be applied for the books or journals possessed in libraries, but hardly work for the documents that are subject to change frequently in the business domain. This paper aims at enabling incremental update of indices whenever parts of documents are changed. For this, it employs the index-organized table that has been developed for the full-text retrieval in Oracle. It creates several index-organized tables that are essential in implementing the Bottom Up Scheme strategy, which has been developed for manipulating structured documents efficiently.Along with an experiment, the technique presented here does not add much index overhead to the original one taken to the index organized table. In addition, the updates of indices are performed quickly as soon as parts of documents are changed.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {383–390},
numpages = {8},
keywords = {indexing, XML, incremental update, SGML, retrieval},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320032,
author = {Chang, Conrad T. K. and Schatz, Bruce R.},
title = {Performance and Implications of Semantic Indexing in a Distributed Environment},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320032},
doi = {10.1145/319950.320032},
abstract = {A research prototype is presented for semantic indexing and retrieval in Information Retrieval. The prototype is motivated by a desire to provide a more efficient and effective information retrieval system compared to the current state of the art. An overview of the Interspace architecture layers is discussed. An object model supporting semantic operations is developed. The model contains a rich set of classes and relationships of the data for the semantic indexing module. The basis of our semantic indexing is done by the creation of concept space. A concept space is an index of a collection that uses document statistics to capture the relationships between concepts. It is useful for boosting text search, by term suggestion of alternative terms semantically related to query terms. Over the years, we have developed generic technology for concept spaces computation on large collections across many subjects. Recent computations on discipline-scale collections have been made on high-end supercomputers. This paper describes our implementation and implications of the computation in a distributed computing environment. Experimental results using different collection sizes and number of processes are presented to show the feasibility of this approach. We also show that laboratory and community collections are already easily computable using a group of PCs in a lab via a message-passing model. We conclude that PC clusters will shortly be able to compute semantic indexes for any real collections.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {391–398},
numpages = {8},
keywords = {semantic indexing, concept space, information retrieval, distributed computing},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320034,
author = {Rolker, Claudia and Kramer, Ralf},
title = {Quality of Service Transferred to Information Retrieval: The Adaptive Information Retrieval System},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320034},
doi = {10.1145/319950.320034},
abstract = {Users often quit an information retrieval system (IR system) very frustrated, because they cannot find the information matching their information needs. We have identified the following two main reasons: too high expectations and wrong use of the system. Our approach which addresses both issues is based on the transfer of the concept of Quality of Service to IR systems: The user first negotiates the retrieval success rates with the IR system, so he knows what to expect from the system in advance. Second, by dynamic adaptation to the retrieval context, the IR system tries to improve the user's queries and thereby tries to exploit the underlying information source as best as possible.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {399–404},
numpages = {6},
keywords = {recall, precision, query reformulation, adaptation, information retrieval, retrieval success rates, quality of service (QoS)},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.323230,
author = {Widyantoro, Dwi H. and Ioerger, Thomas R. and Yen, John},
title = {An Adaptive Algorithm for Learning Changes in User Interests},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.323230},
doi = {10.1145/319950.323230},
abstract = {In this paper, we describe a new scheme to learn dynamic user's interests in an automated information filtering and gathering system running on the Internet. Our scheme is aimed to handle multiple domains of long-term and short-term user's interests simultaneously, which is learned through positive and negative user's relevance feedback. We developed a 3-descriptor approach to represent the user's interest categories. Using a learning algorithm derived for this representation, our scheme adapts quickly to significant changes in user interest, and is also able to learn exceptions to interest categories.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {405–412},
numpages = {8},
keywords = {intelligent agents, information filtering},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.323231,
author = {Adar, Eytan and Karger, David and Stein, Lynn Andrea},
title = {Haystack: Per-User Information Environments},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.323231},
doi = {10.1145/319950.323231},
abstract = {Traditional Information Retrieval (IR) systems are designed to provide uniform access to centralized corpora by large numbers of people. The Haystack project emphasizes the relationship between a particular individual and his corpus. An individual's own haystack priviliges information with which that user interacts, gathers data about those interactions, and uses this metadata to further personalize the retrieval process. This paper describes the prototype Haystack system.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {413–422},
numpages = {10},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.323232,
author = {Lee, Thomas and Chams, Melanie and Nado, Robert and Siegel, Michael and Madnick, Stuart},
title = {Information Integration with Attribution Support for Corporate Profiles},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.323232},
doi = {10.1145/319950.323232},
abstract = {The proliferation of electronically available data within large organizations as well as publicly available data (e.g. over the World Wide Web) poses challenges for users who wish to efficiently interact with and integrate multiple heterogeneous sources. This paper presents CI3, a corporate information integrator, which applies XML as a tool to facilitate data mediation and integration amongst heterogeneous sources in the context of financial analysts creating corporate profiles. Sources include Lotus Notes, relational databases, and the World Wide Web. CI3 applies a unified XML data model to automate integration. By preserving metadata about the source of each datum in the integrated result set, CI3 supports source attribution. Users may trace the attribution metadata from the result back to the underlying sources and leverage their expertise in interpreting the data and, if necessary, use their judgment in assessing the authenticity and veracity of results. We present a functional overview of CI3, its system architecture including the XML data model, and the integration procedures. We conclude by reflecting on lessons learned.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {423–429},
numpages = {7},
keywords = {attribution, XML, metadata, data mediation, data integration},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320048,
author = {Weber, R. and Bollinger, J. and Gross, T. and Schek, H.-J.},
title = {Architecture of a Networked Image Search and Retrieval System},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320048},
doi = {10.1145/319950.320048},
abstract = {Large scale networked image retrieval systems face a number of problems that are not fully satisfied by current systems. On one hand, integrated solutions that store all image data centrally are often limited in terms of scalability and autonomy of data providers. On the other hand, WWW-based search engines proved to be fairly scalable, and data providers retain their autonomy. However, such engines often confront users with links to servers that are not available or to images that no longer exist, i.e., they are unable to keep their meta-database consistent with the repositories' contents. Furthermore, existing solutions often neglect the cost of image delivery. The considerable variations in the effective bandwidth in today's Internet lead to highly unpredictable response times, which are often intolerable from the user's point of view.This paper presents the architecture of Chariot, a networked image search and retrieval system that tackles these concerns. With respect to scalability and autonomy, Chariot follows the approach of WWW-based search engines by maintaining only the meta-data in a central database. Various specialized components (feature extraction, indexes, images servers) are coordinated by a middleware component that employs transactional process management to enforce consistency between the meta-data and all components. Moreover, Chariot incorporates mechanisms to provide more predictable response times for the image delivery over the Internet by employing network-aware image servers. These servers trade off the quality of the images to be delivered with the bandwidth required to transmit the images.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {430–441},
numpages = {12},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320049,
author = {Ghandeharizadeh, Shahram and Kim, Seon Ho},
title = {A Comparison of Alternative Continuous Display Techniques with Heterogeneous Multi-Zone Disks},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320049},
doi = {10.1145/319950.320049},
abstract = {A number of recent technological trends have made data intensive applications such as continuous media (audio and video) servers a reality. These servers are expected to play an important role in applications such as video-on-demand, digital library, news-on-demand, distance learning, etc. Continuous media applications are data intensive and might require storage subsystems that consist of hundreds of (multi-zone) disk drives. With the current technological trends, a homogeneous disk subsystem might evolve to consist of a heterogeneous collection of disk drives. Given such a storage subsystem, the system must continue to support a hiccup-free display of audio and video clips. This study describes extensions of four continuous display techniques for multi-zone disk drives to a heterogeneous platform. These techniques include IBM's Logical Track [21], HP's Track Pairing [4], and USC's FIXB [9] and deadline driven techniques [10]. We quantify the performance tradeoff associated with these techniques using analytical models and simulation studies. The obtained results demonstrate tradeoffs between the cost per simultaneous stream supported by a technique, the wasted disk space, and the incurred startup latency.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {442–449},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320050,
author = {Kim, Yeon-Jung and Sim, Choon-Bo and Chang, Jae-Woo},
title = {Spatial Match Representation Scheme Supporting Ranking in Iconic Images Databases},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320050},
doi = {10.1145/319950.320050},
abstract = {Because content-based image retrieval is essential to retrieve relevant multimedia documents, we represent images as a set of recognizable symbols, i.e., icon objects, and do indexing by regarding the icon object as a representative of a given document. When users request content-based image retrieval, we convert a query image into icon objects and retrieve relevant images in the database. In this paper, we propose a new spatial-match representation scheme, called SRR(Spatial-match Representation supporting Ranking) scheme, which combine directional operators with positional operators. Therefore, our SRR scheme can represent spatial relationships between icon objects precisely and can provide ranking for the retrieved images. In addition, we compare our scheme with the conventional 9DLT and SMR schemes in terms of retrieval effectiveness. Finally, we show from our experiment that our SRR scheme holds about 25% higher recall and about 10% higher precision, compared with the 9DLT and the SMR.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {450–457},
numpages = {8},
keywords = {iconic image database, spatial match representation},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320051,
author = {Chi, Chi-Hung and Ding, Chen and Lim, Andrew},
title = {Word Segmentation and Recognition for Web Document Framework},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320051},
doi = {10.1145/319950.320051},
abstract = {It is observed that a better approach to Web information understanding is to base on its document framework, which is mainly consisted of (i) the title and the URL name of the page, (ii) the titles and the URL names of the Web pages that it points to, (iii) the alternative information source for the embedded Web objects, and (iv) its linkage to other Web pages of the same document. Investigation reveals that a high percentage of words inside the document framework are “compound words” which cannot be understood by ordinary dictionaries. They might be abbreviations or acronyms, or concatenations of several (partial) words. To recover the content hierarchy of Web documents, we propose a new word segmentation and recognition mechanism to understand the information derived from the Web document framework. A maximal bi-directional matching algorithm with heuristic rules is used to resolve ambiguous segmentation and meaning in compound words. An adaptive training process is further employed to build a dictionary of recognisable abbreviations and acronyms. Empirical results show that over 75% of the compound words found in the Web document framework can be understood by our mechanism. With the training process, the success rate of recognising compound words can be increased to about 90%.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {458–465},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320052,
author = {Lim, Seung-Jin and Ng, Yiu-Kai},
title = {An Automated Approach for Retrieving Hierarchical Data from HTML Tables},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320052},
doi = {10.1145/319950.320052},
abstract = {Among the HTML elements, HTML tables [RHJ98] encapsulate hierarchically structured data (hierarchical data in short) in a tabular structure. HTML tables do not come with a rigid schema and almost any forms of two-dimensional tables are acceptable according to the HTML grammar. This relaxation complicates the process of retrieving hierarchical data from HTML tables. In this paper, we propose an automated approach for retrieving hierarchical data from HTML tables. The proposed approach constructs the content tree of an HTML table, which captures the intended hierarchy of the data content of the table, without requiring the internal structure of the table to be known beforehand. Also, the user of the content tree does not deal with HTML tags while retrieving the desired data from the content tree. Our approach can be employed by (i) a query language written for retrieving hierarchically structured data, extracted from either the contents of HTML tables or other sources, (ii) a processor for converting HTML tables to XML documents, and (iii) a data warehousing repository for collecting hierarchical data from HTML tables and storing materialized views of the tables. The time complexity of the proposed retrieval approach is proportional to the number of HTML elements in an HTML table.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {466–474},
numpages = {9},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320053,
author = {G\"{o}vert, Norbert and Lalmas, Mounia and Fuhr, Norbert},
title = {A Probabilistic Description-Oriented Approach for Categorizing Web Documents},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320053},
doi = {10.1145/319950.320053},
abstract = {The automatic categorisation of web documents is becoming crucial for organising the huge amount of information available in the Internet. We are facing a new challenge due to the fact that web documents have a rich structure and are highly heterogeneous. Two ways to respond to this challenge are (1) using a representation of the content of web documents that captures these two characteristics and (2) using more effective classifiers.Our categorisation approach is based on a probabilistic description-oriented representation of web documents, and a probabilistic interpretation of the k-nearest neighbour classifier. With the former, we provide an enhanced document representation that incorporates the structural and heterogeneous nature of web documents. With the latter, we provide a theoretical sound justification for the various parameters of the k-nearest neighbour classifier.Experimental results show that (1) using an enhanced representation of web documents is crucial for an effective categorisation of web documents, and (2) a theoretical interpretation of the k-nearest neighbour classifier gives us improvement over the standard k-nearest neighbour classifier.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {475–482},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320054,
author = {Wang, Ke and Xu, Chu and Liu, Bing},
title = {Clustering Transactions Using Large Items},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320054},
doi = {10.1145/319950.320054},
abstract = {In traditional data clustering, similarity of a cluster of objects is measured by pairwise similarity of objects in that cluster. We argue that such measures are not appropriate for transactions that are sets of items. We propose the notion of large items, i.e., items contained in some minimum fraction of transactions in a cluster, to measure the similarity of a cluster of transactions. The intuition of our clustering criterion is that there should be many large items within a cluster and little overlapping of such items across clusters. We discuss the rationale behind our approach and its implication on providing a better solution to the clustering problem. We present a clustering algorithm based on the new clustering criterion and evaluate its effectiveness.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {483–490},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320055,
author = {Epter, Scott and Krishnamoorthy, Mukkai},
title = {A Multiple-Resolution Method for Edge-Centric Data Clustering},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320055},
doi = {10.1145/319950.320055},
abstract = {Recent works in spatial data clustering view the input data set in terms of inter-point edge lengths rather than the points themselves. Cluster detection in such a system is a matter of finding connected paths of edges whose weight is no greater than some user input threshold or cutoff value. The SMTIN algorithm[9] is one such system that uses Delaunay triangulation to compute the set of nearest neighbor edges quickly and efficiently. Experiments demonstrate a substantial performance and accuracy improvement using SMTIN in comparison to other clustering systems.The resolution of the clusters discovered in the SMTIN system is directly related to the choice of a cutoff threshold, which makes SMTIN perform poorly for input sets with clusters at multiple resolutions. In this work we introduce an edge-centric clustering method that detects clusters at multiple resolutions. Our algorithm detects differences in density among groups of points and uses multiple cutoff points in order to account for clusters at different resolutions. One of the main benefits of the multi-resolution approach of our system is the ability to accurately cluster points that other systems would consider to be noise. Experiments indicate a substantial improvement in the clustering quality of our system in comparison to SMTIN as well as the removal of the requirement of an input distance-threshold, achieved with comparable theoretical as well as actual runtime performance. We present promising directions for this new algorithm.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {491–498},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320056,
author = {Lawrie, Dawn and Rus, Daniela},
title = {A Self-Organized File Cabinet},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320056},
doi = {10.1145/319950.320056},
abstract = {The self-organizing file cabinet is an information retrieval system associated with a user's physical file cabinet. It enhances a physical file cabinet with electronic information about the papers in it. It can remember, organize, update, and help the user find documents contained in the physical file cabinet. The system consists of a module for extracting electronic information about the papers stored in the file cabinet, a module for representing and storing this information in multiple views, and a module that allows a user to interact with this information. The focus of this paper is on the design and evaluation of the self-organized file cabinet.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {499–506},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320057,
author = {van Bommel, M. F. and Beck, T. J.},
title = {Incremental Encoding of Multiple Inheritance Hierarchies},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320057},
doi = {10.1145/319950.320057},
abstract = {Incremental updates to multiple inheritance hierarchies are becoming more prevalent with the increasing number of persistent applications supporting complex objects, making the efficient computation of lattice operations such as greatest lower bound (GLB), least upper bound (LUB), and subsumption more and more important. General techniques for the compact encoding of a hierarchy are presented. One such method is to plunge the given ordering into a boolean lattice of binary words, leading to an almost constant-time complexity of the lattice operations. The method is based on an inverted version of the encoding of Ait-Kaci et al. to allow incremental update. Simple grouping is used to reduce the code space while keeping the lattice operations efficient. Comparisons are made to an incremental version of the range compression scheme of Agrawal et al., where each class is assigned an interval, and relationships are based on containment in the interval. The result is two encoding methods which have their relative merits. The former being better for smaller, more structured hierarchies, and the latter for larger, less organized hierarchies.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {507–513},
numpages = {7},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320058,
author = {Tamzalit, Dalila and Oussalah, Chabane},
title = {From Object Evolution to Object Emergence},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320058},
doi = {10.1145/319950.320058},
abstract = {Database applications which model aspects of the real world should be able to express as accurately as possible the different nuances of reality; that includes the need to evolve internally in response to signals of updates coming from the environment. These updates are not always supplied in an ideal and complete manner and are not always predefined or precisely defined. In practice, requirements for evolution generally occur during the manipulation of objects while running the database. It is frequently necessary to change individual objects, less frequently the database schema. Database systems need to have mechanisms capable, whenever and as well as possible, of assimilating this new information correctly and diagnosing and implementing the changes necessary.This paper concerns the evolution of objects inside databases. Our two main objectives are:to allow objects to evolve their structures dynamically during database maintenance and use, with all necessary impacts on the database schema;to allow, similarly, the creation and display of different plans for evolving the design, like ways of schema evolution, giving in this way a simulation tool for database design and maintenance.So, we propose a Genetic Evolution Object Model developed to have inherent capabilities for auto-adaptation between classes and instances.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {514–521},
numpages = {8},
keywords = {development and emergence processes, class-GTYPE, object-PTYPE, crossing-over, design evolution},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320059,
author = {Radev, Ivan and Pissinou, Niki and Makki, Kia and Park, E. K.},
title = {Graph-Based Object-Oriented Approach for Structural and Behavioral Representation of Multimedia Data},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320059},
doi = {10.1145/319950.320059},
abstract = {The management of multimedia information poses special requirements for multimedia information systems. Both representation and retrieval of the complex and multifaceted multimedia data are not easily handled with the flat relational model and require new data models. In the last several years, object-oriented and graph-based data models are actively pursued approaches for handling the multimedia information. In this paper the characteristics of the novel graph-based object-oriented data model are presented. This model represents the structural and behavioral aspects of data that form multimedia information systems. It also provides for handling the continuously changing user requirements and the complexity of the schema and data representation in multimedia information systems using the schema versioning approach and perspective version abstraction.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {522–530},
numpages = {9},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320060,
author = {Apon, Amy W. and Wagner, Thomas D. and Dowdy, Lawrence W.},
title = {A Learning Approach to Processor Allocation in Parallel Systems},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320060},
doi = {10.1145/319950.320060},
abstract = {Given a typical parallel system and a collection of applications that are to execute on the system, a common problem is determining an effective allocation of processors among the applications. In this paper a learning approach is applied to processor allocation. The approach is to use a stochastic learning automaton (SLA) as a decision tool. An SLA uses values of the current state description, makes an allocation decision, evaluates its decision at some later time, modifies its decision making process, and tries to find the best allocation strategy by learning from its previous mistakes. The method is applied to the problem of allocating processors to parallel applications in a distributed system such as a cluster of workstations, and is validated through simulation. The result of this study show that a learning approach that utilizes a stochastic learning automaton is effective at making processor allocation decisions in a parallel system.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {531–537},
numpages = {7},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320061,
author = {Lanquillon, Carsten and Renz, Ingrid},
title = {Adaptive Information Filtering: Detecting Changes in Text Streams},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320061},
doi = {10.1145/319950.320061},
abstract = {The task of information filtering is to classify documents from a stream as either relevant or non-relevant according to a particular user interest with the objective to reduce information load. When using an information filter in an environment that is changing with time, methods for adapting the filter should be considered in order to retain classification accuracy. We favor a methodology that attempts to detect changes and adapts the information filter only if inevitable in order to minimize the amount of user feedback for providing new training data. Yet, detecting changes may require costly user feedback as well. This paper describes two methods for detecting changes without user feedback. The first method is based on evaluating an expected error rate, while the second one observes the fraction of classification decisions made with a confidence below a given threshold. Further, a heuristics for automatically determining this threshold is suggested and the performance of this approach is experimentally explored as a function of the threshold parameter. Some empirical results show that both methods work well in a simulated change scenario with real world data.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {538–544},
numpages = {7},
keywords = {text classification, change detection, information filtering, statistical quality control},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320062,
author = {Arapis, Constantin},
title = {Archiving Telemeetings},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320062},
doi = {10.1145/319950.320062},
abstract = {This paper presents a prototype system for modeling and managing the complete life-time of telemeetings/teleconferences. The system provides services for modeling telemeetings, storing telemeetings in a telemeeting database, annotating telemeetings and querying the telemeeting database.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {545–552},
numpages = {8},
keywords = {teleconference management, telemeeting archiving},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

