@inproceedings{10.1145/319950.319951,
author = {Giles, C. Lee},
title = {Searching the Web (Keynote Address): Can You Find What You Want?},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319951},
doi = {10.1145/319950.319951},
abstract = {The World Wide Web has revolutionized communication and information distribution, storage, and access. Its impact has been felt everywhere - e.g. science and technology, commerce and business, education, government, religion, law, entertainment, health care. Even so, there are many ways the web can be improved. We discuss what the web consists of and how it has changed, what is the size of the web, and what is covered. Results for the publicly indexable web show that the web though Terabytes in size and growing is still less than large commercial databases and the Library of Congress. Though the web started out as an academic-government endeavor, it is now primarily commerce. Furthermore, the major web search engines cover only a fraction of the publicly indexable web and appear to base their indexing strategy on the popularity of information. Since current search on the web is primarily done with the search engines, what would be the economic, political and scientific implications of these results?},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {1},
numpages = {1},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319952,
author = {Scoggins, Jim},
title = {A Practitioner's View of Techniques Used in Data Warehousing for Sifting through Data to Provide Information (Keynote Address)},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319952},
doi = {10.1145/319950.319952},
abstract = {Over the past 10 years data warehousing evolved from providing 'nice to know' data to 'need to know' information. The decision support systems providing summarized reports to executives have advanced to integrated information factories providing vital information to the desktops of knowledge workers. Data warehousing has benefited through the use advanced techniques of sifting through data to produce information. This discussion will cover data mining techniques used in specific business cases as well as attempt to describe problems that still exist (and could be researched) in the business intelligence arena.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {3},
numpages = {1},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319953,
author = {Yu, Byunggu and Orlandic, Ratko and Evens, Martha},
title = {Simple QSF-Trees: An Efficient and Scalable Spatial Access Method},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319953},
doi = {10.1145/319950.319953},
abstract = {The development of high-performance spatial access methods that can support complex operations of large spatial databases continues to attract considerable attention. This paper introduces QSF-trees, an efficient and scalable structure for indexing spatial objects, which has some important advantages over R*-trees. QSF-trees eliminate overlapping of index regions without forcing object clipping or sacrificing the selectivity of spatial operations. The method exploits the semantics of topological relations between spatial objects to further reduce the number of index nodes visited during the search. A series of experiments involving randomly-generated spatial objects was conducted to compare the structure with two variations of R*-trees. The experiments show QSF-trees to be more efficient and more scalable to the increase in the data-set size, the size of spatial objects, and the number of dimensions of the spatial universe.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {5–14},
numpages = {10},
keywords = {database management, spatial database, spatial access methods, point access methods, topological relations},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319954,
author = {Song, Ju-Won and Whang, Kyu-Young and Lee, Young-Koo and Lee, Min-Jae and Kim, Sang-Wook},
title = {Transformation-Based Spatial Join},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319954},
doi = {10.1145/319950.319954},
abstract = {Spatial join finds pairs of spatial objects having a specific spatial relationship in spatial database systems. A number of spatial join algorithms have recently been proposed in the literature. Most of them, however, perform the join in the original space. Joining in the original space has a drawback of dealing with sizes of objects and thus has difficulty in developing a formal algorithm that does not rely on heuristics. In this paper, we propose a spatial join algorithm based on the transformation technique. An object having a size in the two-dimensional original space is transformed into a point in the four-dimensional transform space, and the join is performed on these point objects. This can be easily extended to n-dimensional cases. We show the excellence of the proposed approach through analysis and extensive experiments. The results show that the proposed algorithm has a performance generally better than that of the R*-based algorithm proposed by Brinkhoff et al. This is a strong indicating that corner transformation preserves clustering among objects and that spatial operations can be performed better in the transform space than in the original space. This reverses the common belief that transformation will adversely affect clustering. We believe that our result will provide a new insight towards transformation-based spatial query processing.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {15–26},
numpages = {12},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319955,
author = {Vasilis, Delis and Thanasis, Hadzilacos},
title = {Binary String Relations: A Foundation for Spatiotemporal Knowledge Representation},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319955},
doi = {10.1145/319950.319955},
abstract = {The paper is concerned with the qualitative representation of spatiotemporal relations. We initially propose a multiresolution framework for the representation of relations among 1D intervals, based on a binary string encoding. We subsequently extend this framework to multiple dimensions, thus allowing the description of spatiotemporal relations at various contexts. The feasible relations at a particular resolution level are inherently permeated by a poset structure, called conceptual neighbourhood, upon which we propose efficient relation inferencing mechanisms. Finally, we discuss the application of our model to spatiotemporal reasoning, which refers to the classic problems of satisfiability and deductive closure of a set of spatiotemporal assertions.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {27–37},
numpages = {11},
keywords = {conceptual neighbourhoods, spatiotemporal relations, spatiotemporal reasoning},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319956,
author = {Swan, Russell and Allan, James},
title = {Extracting Significant Time Varying Features from Text},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319956},
doi = {10.1145/319950.319956},
abstract = {We propose a simple statistical model for the frequency of occurrence of features in a stream of text. Adoption of this model allows us to use classical significance tests to filter the stream for interesting events. We tested the model by building a system and running it on a news corpus. By a subjective evaluation, the system worked remarkably well: almost all of the groups of identified tokens corresponded to news stories and were appropriately placed in time. A preliminary objective evaluation was also used to measure the quality of the system and it showed some of the weaknesses and the power of our approach.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {38–45},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.323229,
author = {Kanada, Yasusi},
title = {A Method of Geographical Name Extraction from Japanese Text for Thematic Geographical Search},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.323229},
doi = {10.1145/319950.323229},
abstract = {A text retrieval method called the thematic geographical search method has been developed and applied to a Japanese encyclopedia called the World Encyclop\ae{}dia. In this method, the user specifies a search theme using free words, then obtains a sorted list of excerpts and hyperlinks to encyclopedia sentences that contain geographical names. Using this list, the user can also open maps that indicate the locations of the names. To generate an index of names for this searching, a method of extracting geographical names has been developed. In this method, geographical names are extracted, matched to names in a geographical name database, and identified. Geographical names, however, often have several types of ambiguities. Ambiguities are resolved by using non-local context analysis, which uses a stack and several other techniques. As a result, the precision of extracted names is more than 96% on average. This method depends on features of the Japanese language, but the strategy and most of the techniques can be applied to texts in English or other languages.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {46–54},
numpages = {9},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319957,
author = {Lin, Chin-Yew},
title = {Training a Selection Function for Extraction},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319957},
doi = {10.1145/319950.319957},
abstract = {In this paper we compare performance of several heuristics in generating informative generic/query-oriented extracts for newspaper articles in order to learn how topic prominence affects the performance of each heuristic. We study how different query types can affect the performance of each heuristic and discuss the possibility of using machine learning algorithms to automatically learn good combination functions to combine several heuristics. We also briefly describe the design, implementation, and performance of a multilingual text summarization system SUMMARIST.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {55–62},
numpages = {8},
keywords = {topic extraction, summary evaluation, automated text summarization},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319958,
author = {Pr\"{o}ll, Birgit and Starck, Heinrich and Retschitzegger, Werner and Sighart, Harald},
title = {Ready for Prime Time: Pre-Generation of Web Pages in TIScover},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319958},
doi = {10.1145/319950.319958},
abstract = {In large data- and access-intensive web sites, efficient and reliable access is hard to achieve. This situation gets even worse for web sites providing precise structured query facilities and requiring topicality of the presented information even in face of a highly dynamic content. The achievement of these partly conflicting goals is strongly influenced by the approach chosen for page generation, ranging from composing a web page upon a user's request to its generation in advance. The official Austrian web-based tourism information and booking system TIScover tries to reconcile these goals by employing a hybrid approach of page generation. In TIScover, web pages are not only generated on request in order to support precise structured queries on the content managed by a database system. Rather, the whole web site is also pre-generated out of the extremely dynamic content and synchronized with the database on the basis of metadata. Thus, topicality of information is guaranteed, while ensuring efficient and reliable access. This paper discusses the hybrid approach as realized in TIScover, focussing in particular on the concepts used for pre-generation.1},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {63–68},
numpages = {6},
keywords = {tourism information system, reliability, WWW, page generation, optimization},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319959,
author = {Wu, Kun-Lung and Yu, Philip S.},
title = {Local Replication for Proxy Web Caches with Hash Routing},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319959},
doi = {10.1145/319950.319959},
abstract = {This paper studies controlled local replication for hash routing, such as CARP, among a collection of loosely-coupled proxy web cache servers. Hash routing partitions the entire URL space among the shared web caches, creating a single logical cache. Each partition is assigned to a cache server. Duplication of cache contents is eliminated and total incoming traffic to the shared web caches is minimized. Client requests for non-assigned-partition objects are forwarded to sibling caches. However, request forwarding increases not only inter-cache traffic but also cpu utilization, thus slows the client response time. We propose a controlled local replication of non-assigned-partition objects in each cache server to effectively reduce the inter-cache traffic. We use a multiple-exit LRU to implement controlled local replication. Trace-driven simulations are conducted to study the performance impact of local replication. The results show that (1) regardless of cache sizes, with a controlled local replication, the average response time, inter-cache traffic and CPU overhead can be effectively reduced without noticeable increases in incoming traffic; (2) for very large cache sizes, a larger amount of local replication can be allowed to reduce inter-cache traffic without increasing incoming traffic; and (3) local replication is effective even if clients are dynamically assigned to different cache servers.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {69–76},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319960,
author = {Lee, Dongwon and Chu, Wesley W.},
title = {Semantic Caching via Query Matching for Web Sources},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319960},
doi = {10.1145/319950.319960},
abstract = {A semantic caching scheme suitable for wrappers wrapping web sources is presented. Since the web sources have typically weaker querying capabilities than conventional databases, existing semantic caching schemes cannot be applied directly. A seamlessly integrated query translation and capability mapping between the wrappers and web sources in semantic caching is described. In addition, an analysis on the match types between the user's input query and cached queries is presented. Semantic knowledge acquired from the data can be used to avoid unnecessary access to the web sources by transforming the cache miss to the cache hit. A polynomial time algorithm based on the proposed query matching technique is presented to find the best matched query in the cache. Experimental results reveal the effectiveness of the proposed semantic caching scheme.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {77–85},
numpages = {9},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319961,
author = {Liddle, Stephen W. and Campbell, Douglas M. and Crawford, Chad},
title = {Automatically Extracting Structure and Data from Business Reports},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319961},
doi = {10.1145/319950.319961},
abstract = {A considerable amount of clean semistructured data is internally available to companies in the form of business reports. However, business reports are untapped for data mining, data warehousing, and querying because they are not in relational form. Business reports have a regular structure that can be reconstructed. We present algorithms that automatically infer the regular structure underlying business reports and automatically generate wrappers to extract relational data.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {86–93},
numpages = {8},
keywords = {business reports, report structure, regular expressions, automatic wrapper generation, data and information extraction},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319962,
author = {Ribeiro-Neto, Berthier and Laender, Alberto H. F. and da Silva, Altigran S.},
title = {Extracting Semi-Structured Data through Examples},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319962},
doi = {10.1145/319950.319962},
abstract = {In this paper, we describe an innovative approach to extracting semi-structured data from Web sources. The idea is to collect a couple of example objects from the user and to use this information to extract new objects from new pages or texts. To perform the extraction of new objects, we introduce a bottom-up extration strategy and, through experimentation, demonstrate that it works quite effectively with distinct Web sources, even if only a few examples are provided by the user.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {94–101},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319963,
author = {Shyu, Mei-Ling and Chen, Shu-Ching and Kashyap, R. L.},
title = {Discovering Quasi-Equivalence Relationships from Database Systems},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319963},
doi = {10.1145/319950.319963},
abstract = {Association rule mining has recently attracted strong attention and proven to be a highly successful technique for extracting useful information from very large databases. In this paper, we explore a generalized affinity-based association mining which discovers quasi-equivalent media objects in a distributed information-providing environment consisting of a network of heterogeneous databases which could be relational databases, hierarchical databases, object-oriented databases, multimedia databases, etc. Online databases, consisting of millions of media objects, have been used in business management, government administration, scientific and engineering data management, and many other applications owing to the recent advances in high-speed communication networks and large-capacity storage devices. Because of the navigational characteristic, queries in such an information-providing environment tend to traverse equivalent media objects residing in different databases for the related data records. As the number of databases increases, query processing efficiency depends heavily on the capability to discover the equivalence relationships of the media objects from the network of databases. Theoretical terms along with an empirical study of real databases are presented.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {102–108},
numpages = {7},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319964,
author = {Matsuda, Katsushi and Fukushima, Toshikazu},
title = {Task-Oriented World Wide Web Retrieval by Document Type Classification},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319964},
doi = {10.1145/319950.319964},
abstract = {This paper proposes a novel approach to accurately searching Web pages for relevant information in problem solving by specifying a Web document category instead of the user's task. Accessing information from World Wide Web pages as an approach to problem solving has become commonplace. However, such a search is difficult with current search services, since these services only provide keyword-based search methods that are equivalent to narrowing down the target references according to domains. However, problem solving usually involves both a domain and a task. Accordingly, our approach is based on problem solving tasks. To specify a user's problem solving task, we introduce the concept of document types that directly relate to the problem solving tasks; with this approach, users can easily designate problem solving tasks. We implemented PageTypeSearch system based on our approach. Classifier of PageTypeSearch classifies Web pages into the document types by comparing their pages with typical structural characteristics of the types. We compare PageTypeSearch using the document typeindices with a conventional keyword-based search system in experiments. The average precision of the document type-based search is 88.9%, while the average precision of the keyword-based search is 31.2%. Moreover, the number of irrelevant references gathered by our system is about one-thirteenth that of traditional keyword-based search systems. Our approach has practical advantages for problem solving by introducing the viewpoint of tasks to achieve higher performance.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {109–113},
numpages = {5},
keywords = {document type, WWW, information retrieval, classification},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319965,
author = {Hsu, Wen-Lin and Lang, Sheau-Dong},
title = {Classification Algorithms for NETNEWS Articles},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319965},
doi = {10.1145/319950.319965},
abstract = {We propose several algorithms using the vector space model to classify the news articles posted on the NETNEWS according to the newsgroup categories. The baseline method combines the terms of all the articles of each newsgroup in the training set to represent the newsgroups as single vectors. After training, the incoming news articles are classified based on their similarity to the existing newsgroup categories. We propose to use the following techniques to improve the classification performance of the baseline method: (1) use routing (classification) accuracy and the similarity values to refine the training set; (2) update the underlying term structures periodically during testing; and (3) apply k-means clustering to partition the newsgroup articles and represent each newsgroup by k vectors. Our test collection consists of the real news articles and the 519 subnewsgroups under the REC newsgroup of NETNEWS in a period of 3 months. Our experimental results demonstrate that the technique of refining the training set reduces from one-third to two-thirds of the storage. The technique of periodical updates improves the routing accuracy ranging from 20% to 100% but incurs runtime overhead. Finally, representing each newsgroup by k vectors (with k = 2 or 3) using clustering yields the most significant improvement in routing accuracy, ranging from 60% to 100%, while causing only slightly higher storage requirements.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {114–121},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319966,
author = {Li, Hang and Yamanishi, Kenji},
title = {Text Classification Using ESC-Based Stochastic Decision Lists},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319966},
doi = {10.1145/319950.319966},
abstract = {We propose a new method of text classification using stochastic decision lists. A stochastic decision list is an ordered sequence of IF-THEN rules, and our method can be viewed as a rule-based method for text classification having advantages of readability and refinability of acquired knowledge. Our method is unique in that decision lists are automatically constructed on the basis of the principle of minimizing Extended Stochastic Complexity (ESC), and with it we are able to construct decision lists that have fewer errors in classification. The accuracy of classification achieved with our method appears better than or comparable to those of existing rule-based methods.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {122–130},
numpages = {9},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319968,
author = {Wieczerzycki, Waldemar},
title = {Database Model for Web-Based Cooperative Applications},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319968},
doi = {10.1145/319950.319968},
abstract = {In this paper we propose a model of a database that could become a kernel of cooperative database applications. First, we propose a new data model CDM (Collaborative Data Model) that is oriented for the specificity of multiuser environments, in particular: cooperation scenarios, cooperation techniques and cooperation management. Second, we propose to apply to databases supporting collaboration so called multiuser transactions. Multiuser transactions are flat transactions in which, in comparison to classical ACID transactions, the isolation property is relaxed.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {131–138},
numpages = {8},
keywords = {CSCW, trasaction model, object-oriented databases, data model},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319970,
author = {Lawrence, Steve and Bollacker, Kurt and Giles, C. Lee},
title = {Indexing and Retrieval of Scientific Literature},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319970},
doi = {10.1145/319950.319970},
abstract = {The web has greatly improved access to scientific literature. However, scientific articles on the web are largely disorganized, with research articles being spread across archive sites, institution sites, journal sites, and researcher homepages. No index covers all of the available literature, and the major web search engines typically do not index the content of Postscript/PDF documents at all. This paper discusses the creation of digital libraries of scientific literature on the web, including the efficient location of articles, full-text indexing of the articles, autonomous citation indexing, information extraction, display of query-sensitive summaries and citation context, hubs and authorities computation, similar document detection, user profiling, distributed error correction, graph analysis, and detection of overlapping documents. The software for the system is available at no cost for non-commercial use.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {139–146},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319971,
author = {Allen, Robert B. and Schalow, John},
title = {Metadata and Data Structures for the Historical Newspaper Digital Library},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319971},
doi = {10.1145/319950.319971},
abstract = {We examine metadata and data-structure issues for the Historical Newspaper Digital Library. This project proposes to digitize and then do OCR and linguisting processing on several years worth of historical newspapers. Newspapers are very complex information objects so developing a rich description of their content is challenging. In addition to frameworks for the logical structure and physical layout, we propose metadata relevant to the image processing and to the historians who will use this collection. Finally, we consider how the metadata infrastructure might be managed as it evolves with improved text processing capabilities and how an infrastructure might be developed to support a community of users.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {147–153},
numpages = {7},
keywords = {newspapers, OCR, digital libraries, history, metadata},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319972,
author = {Noaman, Amin Y. and Barker, Ken},
title = {A Horizontal Fragmentation Algorithm for the Fact Relation in a Distributed Data Warehouse},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319972},
doi = {10.1145/319950.319972},
abstract = {Data warehousing is one of the major research topics of appliedside database investigators. Most of the work to date has focused on building large centralized systems that are integrated repositories founded on pre-existing systems upon which all corporate-wide data are based. Unfortunately, this approach is very expensive and tends to ignore the advantages realized during the past decade in the area of distribution and support for data localization in a geographically dispersed corporate structure. This research investigates building distributed data warehouses with particular emphasis placed on distribution design for the data warehouse environment. The article provides an architectural model for a distributed data warehouse, the formal definition of the relational data model for data warehouse and a methodology for distributed data warehouse design along with a “horizontal” fragmentation algorithm for the fact relation.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {154–161},
numpages = {8},
keywords = {horizontal fragmentation, distributed data warehouse design, distributed data warehouse architecture},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319974,
author = {Cheung, David W. and Zhou, Bo and Kao, Ben and Lu, Hongjun and Lam, Tak Wah and Ting, Hing Fung},
title = {Requirement-Based Data Cube Schema Design},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319974},
doi = {10.1145/319950.319974},
abstract = {On-line analytical processing (OLAP) requires efficient processing of complex decision support queries over very large databases. It is well accepted that pre-computed data cubes can help reduce the response time of such queries dramatically. A very important design issue of an efficient OLAP system is therefore the choice of the right data cubes to materialize. We call this problem the data cube schema design problem. In this paper we show that the problem of finding an optimal data cube schema for an OLAP system with limited memory is NP-hard. As a more computationally efficient alternative, we propose a greedy approximation algorithm cMP and its variants. Algorithm cMP consists of two phases. In the first phase, an initial schema consisting of all the cubes required to efficiently answer the user queries is formed. In the second phase, cubes in the initial schema are selectively merged to satisfy the memory constraint. We show that cMP is very effective in pruning the search space for an optimal schema. This leads to a highly efficient algorithm. We report the efficiency and the effectiveness of cMP via an empirical study using the TPC-D benchmark. Our results show that the data cube schemas generated by cMP enable very efficient OLAP query processing.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {162–169},
numpages = {8},
keywords = {DSS, OLAP, data cube schema design, data cubes},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319975,
author = {Johnson, Theodore and Chatziantoniou, Damianos},
title = {Extending Complex Ad-Hoc OLAP},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319975},
doi = {10.1145/319950.319975},
abstract = {Large scale data analysis and mining activities require sophisticated information extraction queries. Many queries require complex aggregation, and many of these aggregates are non-distributive. Conventional solutions to this problem involve defining User Defined Aggregate Functions (UDAFs). However, the use of UDAFs entails several problems. Defining a new UDAF can be a significant burden for the user, and optimizing queries involving UDAFs is difficult because of the “black box” nature of the UDAF.In this paper, we present a method for expressing nested aggregates in a declarative way. A nested aggregate, which is a rollup of another aggregated value, expresses a wide range of useful non-distributive aggregation. For example, most frequent type aggregation can be naturally expressed using nested aggregation, e.g. “For each product, report its total sales during the month with the largest total sales of the product”. By expressing compex aggregates declaratively, we relieve the user of the burden of defining UDAFs, and allow the evalution of the complex aggregates to be optimized.We use the Extended Multi-Feature (EMF) syntax as the basis for expressing nested aggregation. An advantage of this approach is that EMF SQL can already express a wide range of complex aggregation in a succinct way, and EMF SQL is easily optimized into efficient query plans. We show that nested aggregation queries can be evaluated efficiently by using a small extension to the EMF SQL query evaluation algorithm. A side effect of this extension is to extend EMF SQL to permit complex aggregation of data from multiple sources.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {170–179},
numpages = {10},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319976,
author = {Labrou, Yannis and Finin, Tim},
title = {Yahoo! As an Ontology: Using Yahoo! Categories to Describe Documents},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319976},
doi = {10.1145/319950.319976},
abstract = {We suggest that one (or a collection) of names of Yahoo! (or any other WWW indexer's) categories can be used to describe the content of a document. Such categories offer a standardized and universal way for referring to or describing the nature of real world objects, activities, documents and so on, and may be used (we suggest) to semantically characterize the content of documents. WWW indices, like Yahoo! provide a huge hierarchy of categories (topics) that touch every aspect of human endeavors. Such topics can be used as descriptors, similarly to the way librarians use for example, the Library of Congress cataloging system to annotate and categorize books.In the course of investigating this idea, we address the problem of automatic categorization of webpages in the Yahoo! directory. We use Telltale as our classifier; Telltale uses n-grams to compute the similarity between documents. We experiment with various types of descriptions for the Yahoo! categories and the webpages to be categorized. Our findings suggest that the best results occur when using the very brief descriptions of the Yahoo! categorized entries; these brief descriptions are provided either by the entries' submitters or by the Yahoo! human indexers and accompany most Yahoo!-indexed entries.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {180–187},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.329374,
author = {Zhu, Xiaolan and Gauch, Susan and Gerhard, Lutz and Kral, Nicholas and Pretschner, Alexander},
title = {Ontology-Based Web Site Mapping for Information Exploration},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.329374},
doi = {10.1145/319950.329374},
abstract = {Centralized search process requires that the whole collection reside at a single site. This imposes a burden on both the system storage of the site and the network traffic near the site. It thus comes to require the search process to be distributed. Recently, more and more Web sites provide the ability to search their local collection of Web pages. Query brokering systems are used to direct queries to the promising sites and merge the results from these sites. Creation of meta-information of the sites plays an important role in such systems. In this article, we introduce an ontology-based web site mapping method used to produce conceptual meta-information, the Vector Space approach, and present a serial of experiments comparing it with Na\"{\i}ve-Bayes approach. We found that the Vector Space approach produces better accuracy in ontology-based web site mapping.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {188–194},
numpages = {7},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319978,
author = {Geffner, S. and Agrawal, D. and Abbadi, A. El and Smith, T.},
title = {Browsing  Large Digital Library Collections Using Classification Hierarchies},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319978},
doi = {10.1145/319950.319978},
abstract = {Summarization of intermediary query result sets plays an important role when users browse through digital library collections. Summarization enables users to quickly digest the results of their queries, and provides users with important information they can use to narrow their search interactively. Techniques from the field of data analysis may be applied to the problem of generating summaries of query results efficiently. Such techniques should permit the incorporation of classification hierarchies in order to provide powerful browsing environments for digital library users.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {195–201},
numpages = {7},
keywords = {classification, aggregation, searching, digital libraries, browsing, summarization},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319979,
author = {Lin, Yong and Xu, Jian and Lim, Ee-Peng and Ng, Wee-Keong},
title = {ZBroker: A Query Routing Broker for Z39.50 Databases},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319979},
doi = {10.1145/319950.319979},
abstract = {A query routing broker is a software agent that determines from a large set of accessing information sources the ones most relevant to a user's information need. As the number of information sources on the Internet increases dramatically, future users will have to rely on query routing brokers to decide a small number of information sources to query without incurring too much query processing overheads. In this paper, we describe a query routing broker known as ZBroker developed for bibliographic database servers that support the Z39.50 protocol. ZBroker samples the content of each bibliographic database by using training queries and their results, and summarizes the bibliographic database content into a knowledge base. We present the design and implementation of ZBroker and describe its Web-based user interface.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {202–209},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.319980,
author = {Glover, Eric J. and Lawrence, Steve and Birmingham, William P. and Giles, C. Lee},
title = {Architecture of a Metasearch Engine That Supports User Information Needs},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.319980},
doi = {10.1145/319950.319980},
abstract = {When a query is submitted to a metasearch engine, decisions are made with respect to the underlying search engines to be used, what modifications will be made to the query, and how to score the results. These decisions are typically made by considering only the user's keyword query, neglecting the larger information need. Users with specific needs, such as “research papers” or “homepages,” are not able to express these needs in a way that affects the decisions made by the metasearch engine. In this paper, we describe a metasearch engine architecture that considers the user's information need for each decision. Users with different needs, but the same keyword query, may search different sub-search engines, have different modifications made to their query, and have results ordered differently. Our architecture combines several powerful approaches together in a single general purpose metasearch engine.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {210–216},
numpages = {7},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320005,
author = {Yu, Clement and Meng, Weiyi and Liu, King-Lup and Wu, Wensheng and Rishe, Naphtali},
title = {Efficient and Effective Metasearch for a Large Number of Text Databases},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320005},
doi = {10.1145/319950.320005},
abstract = {Metasearch engines can be used to facilitate ordinary users for retrieving information from multiple local sources (text databases). In a metasearch engine, the contents of each local database is represented by a representative. Each user query is evaluated against the set of representatives of all databases in order to determine the appropriate databases to search. When the number of databases is very large, say in the order of tens of thousands or more, then a traditional metasearch engine may become inefficient as each query needs to be evaluated against too many database representatives. Furthermore, the storage requirement on the site containing the metasearch engine can be very large. In this paper, we propose to use a hierarchy of database representatives to improve the efficiency. We provide an algorithm to search the hierarchy. We show that the retrieval effectiveness of our algorithm is the same as that of evaluating the user query against all database representatives. We also show that our algorithm is efficient. In addition, we propose an alternative way of allocating representatives to sites so that the storage burden on the site containing the metasearch engine is much reduced.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {217–224},
numpages = {8},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

@inproceedings{10.1145/319950.320007,
author = {Feng, Ling and Lu, Hongjun and Yu, Jeffrey Xu and Han, Jiawei},
title = {Mining Inter-Transaction Associations with Templates},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320007},
doi = {10.1145/319950.320007},
abstract = {Multi-dimensional, inter-transaction association rules extend the traditional association rules to describe more general associations among items with multiple properties cross transactions. “After McDonald and Burger King open branches, KFC will open a branch two months later and one mile away” is an example of such rules. Since the number of potential inter-transaction association rules tends to be extremely large, mining inter-transaction associations poses more challenges on efficient processing than mining intra-transaction associations. In order to make such association mining truly practical and computationally tractable, in this study, we present a template model to help users declare the interesting inter-transaction associations to be mined. With the guidance of templates, several optimization techniques are devised to speed up the discovery of inter-transaction association rules. We show, through a series of experiments, that these optimization techniques can yield significant performance benefits.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {225–233},
numpages = {9},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}

